<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,200.12,181.23,202.76,15.48">Answer Validation Exercise</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,272.59,218.86,57.82,8.64"><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
							<email>ingo.gloeckner@fernuni-hagen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Hagen at QA@CLEF</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Intelligent Information and Communication Systems (IICS</orgName>
								<orgName type="institution">University of Hagen (FernUniversität in Hagen</orgName>
								<address>
									<postCode>58084</postCode>
									<settlement>Hagen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,200.12,181.23,202.76,15.48">Answer Validation Exercise</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4137ED09614CDED5C91A3E342E5A332B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing-Linguistic processing</term>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Information filtering, Selection process</term>
					<term>H.3.4 [Information Storage and Retrieval]: Systems and Software-Performance evaluation (efficiency and effectiveness)</term>
					<term>I.2.3 [Artificial Intelligence]: Deduction and Theorem Proving-Inference engines</term>
					<term>I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-Predicate logic, Semantic networks</term>
					<term>I.2.7 [Artificial Intelligence]: Natural Language Processing Answer Validation, Question Answering, Recognising Textual Entailment (RTE), Robustness, MultiNet</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper features MAVE (MultiNet answer verification), a system for validating results of question-answering systems which serves as a testbed for robust knowledge processing based on multi-layered extended semantic networks (MultiNet [4]). The system utilizes logical inference rather than graph matching for recognising textual entailment, which makes it easily extensible by further knowledge encoded in logical axioms. In order to ensure robustness, the prover is embedded in a relaxation loop which subsequently skips 'critical' literals until a proof of the reduced query succeeds. MAVE uses the number of skipped literals as a robust indicator of logical entailment. Although the detection of 'critical' literals does not necessarily result in the minimal number of non-provable literals, the skipped literal indicator performed very well in the AVE experiments. MAVE parses the hypothesis strings and is therefore sensitive to syntactic errors in hypothesis generation. A regular correction grammar is applied to alleviate this problem. The system also parses the original question (when available) and tries to prove it from the snippet representation. The skipped-literal count for the question is then combined with a similarity index for hypothesis vs. found answer. This method boosts recall by up to 13%. Additional indicators are used for recognizing false positives. The filter increases precision by up to 14% without compromising recall of the system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction 1.System description</head><p>MAVE is a knowledge-based system for answer validation which uses deep linguistic processing and logical inference for recognising textual entailment. The system assumes XML input as specified in the AVE 2006 description. A validation candidate p consists of a hypothesis string constructed by substituting the answer of the QA system into the original question, and by the extracted snippet which contains the relevant text passage. Optionally, the question string can also be given. MAVE uses a standard cascaded architecture for processing validation candidates. The sequence of processing steps involves preprocessing, linguistic analysis, and indicator extraction (the results of logical inference are also represented by indicators). The final aggregation step determines the YES/NO decision and confidence score from a combination of the indicators. These processing stages and the involved components of MAVE are detailed in the following.</p><p>Preprocessing The hypotheses are very often syntactically ill-formed or semantically defective due to the primitive mechanism used for generating hypotheses from fixed templates. For example, the hypothesis might contain the phrase 'im Jahre 26. April 1986' (En: 'in the year April 26, 1986') rather than listing the date only. Sequences of two prepositions are also often found, e.g. 'in in Lillehammer' rather than 'in Lillehammer'. These errors deteriorate the results of syntactic parsing, and a set of regular expressions is therefore applied in the preprocessing stage which corrects many of them. For example, sequences of local prepositions are reduced to a single local preposition. The snippet strings often need correction as well. Syntactic errors in the snippets mostly result from wrong XML-text conversion (contents of XML elements are concatenated without proper punctuation), from wrong segmentation (snippet extraction disregards sentence boundaries), from named entity recognizers which collapse multi-word units into new tokens like 'Gianni Versace', and from primitive mechanisms for reference resolution which simply replace pronouns by a named entity they most likely refer to. Some of these errors can be fixed by regular correction patterns, but the detoriation of extracted snippets by artefacts of the QA systems remains an issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linguistic analysis</head><p>The WOCADI parser [1] is used for natural-language analysis. Linguistic processing starts with a morphological and lexical analysis, i.e. lemmatization, decomposition of noun compounds, and assignment of possible word meanings. These word meanings are expressed by lexical concept constants from the HaGenLex computational lexicon for German [3]. WOCADI then attempts a deep syntactic analysis, but it is robust against non-grammatical input and often returns a chunk parse (incomplete parse) in the event of such syntactic errors. The parser also constructs a semantic representation of the natural language expression, which is expressed in the MultiNet formalism [4] (a modern dialect of semantic networks). This process includes the disambiguation of word meanings, semantic role labeling, and facticity labeling of all discourse entities (which helps to discern real situations from hypothetical or negated cases). WOCADI handles intrasentential coreference resolution and also prepares the necessary data for a subsequent intersentential coreference resolution (e.g. for interpreting referring pronouns). The output format of the WOCADI parser, which contains the results of all analysis levels, is documented in [2].</p><p>Post-processing of representations and query construction Snippets are allowed to span multiple sentences. The representations of individual snippet sentences must hence be integrated into a global meaning representation of the snippet text [5]. This assimilation process involves a resolution of intersentential anaphora, which is based on the coreference information generated by WOCADI. The post-processing further involves a simplification and normalization of the generated semantic networks. A list of synonyms and near-synonyms is used for replacing all lexical concept constants with the canonical synset representative. This eliminates the need of handling synonyms on the level of knowledge processing. The resulting semantic network for a snippet is represented as a logical conjunction of elementary facts which correspond to the edges of the network and to additional node attributes. While the snippet facts are dynamically added to the knowledge base, the hypothesis representation (or the question representation) must be turned into a logical query, i.e. a conjunction of literals in which all entities mentioned in the hypothesis are represented by variables. A proof of such a literal will bind the variables to the appropriate discourse entities mentioned in the snippet or to a skolem term built from such entities. The resulting logical query is further refined by eliminating complicated constructions. This process eliminates modalities 'müssen' (En: 'must') and 'können' (En: 'can'), for example. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robust entailment proving</head><p>The most important indicators for deciding answer correctness are determined by the MAVE prover, an improved version of the resolution-based inference engine for MultiNet described in [5]. The theorem prover expects queries to be expressed as a conjunctive list of literals. The knowledge base is comprised by variable-free facts (these describe the MultiNet representations of the snippet, as well as lexico-semantic relationships and bindings of rule schemata), and by implicative rules which derive a conjunction of conclusion literals from a conjunction of premise literals. All variables which only occur in the conclusion are considered existentially quantified. Because all such variables are replaced with skolem terms, application of an axiom will always result in a fully instantiated (variable-free) conclusion. The prover uses iterative deepening to find answers, i.e. a bounded depth first search with depth increment after each iterative deepening cycle. The iterative deepening process is stopped when a given maximum depth limit of 5 is reached or when a time limit is exceeded (20 seconds in the experiments). Performance of the prover is optimized by term indexing techniques and by literals sorting. Thus, the literals are assigned costs according to the estimated effort for proving the literal given the current instantiation pattern of the literal, e.g. pattern (sub var non-var) for the literal (sub X lesen.1.1). The literals are then ordered in increasing order of estimated effort, where the estimate for each literal L i takes into account the instantiation pattern of bound vs. variable arguments which results from proving literals L 1 , . . . , L i-1 . In order to achieve robust logical inference, the theorem prover is embedded in a relaxation loop which drops part of the conjunctive query when a proof of the current query fragment fails or can not been found within the time limit. By subsequently removing 'critical' literals from the query, this process always finds a (possibly empty) query fragment provable from the assumed knowledge. The number of skipped query literals which are not contained in the provable fragment will be used as a numeric indicator of logical entailment which is robust against slight errors in the logical representation of hypothesis and snippet and also against gaps in knowledge modeling. The selection of the 'critical' literal (which will be skipped in the next iteration of the relaxation loop), is based on the least-effort ordering of the literals. The prover keeps track of the longest prefix L 1 , . . . , L k of the ordered literal list for which a proof was found. When a full proof fails, the literal L k+1 will be skipped and a new proof of the smaller fragment will be started. Notice that literals L k+2 , . . . , L N must be re-ordered because the instantiation patterns can change when deleting L k+1 from the literal list. The relaxation loop does not necessarily determine the smallest possible number of non-provable literals. This will not affect the success of the approach in practice, however (see Sect. 3).</p><p>Aggregation of indicators An indicator evaluation language was developed which makes it easy to experiment with scoring criteria. In this language, all indicators can be treated like variables which are automatically bound to the appropriate value for each considered item in the test collection. In addition, aggregation functions like maximum, average etc. are available in the scoring language. The language also offers means for dealing with undefined indicator values. Expressions in this scoring language can then be translated into active scoring functions (which are ordinary functions in the Scheme programming language). Both the YES/NO decision of the run and the confidence score are computed from expressions in this scoring language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Knowledge resources</head><p>The background knowledge base of the first MAVE prototype comprises 2,484 basic facts which interrelate 4,666 lexicalized concepts. About 2,000 of these lexico-semantic relationships were supplied by the computational lexicon HaGenLex. A typical example is the MultiNet relation CHEA, which holds between events described by verbs like 'aktivieren' (En: 'to activate') and the corresponding abstractum 'Aktivierung' (En: 'activation'). The remaining 500 facts capture important relationships not covered by HaGenLex, e.g. about the connection between states ('Spain'), state adjectives ('Spanish'), and inhabitant names ('Spaniard'). Apart from the facts, MAVE uses 103 implicative rules. 38 of these rules are so-called 'R-axioms', which define key properties of the MultiNet relations. The remaining 65 rules include meaning postulates for frequent verbs not covered by the HaGenLex relations, and also a rudimentary domain modelling which covers a few important topics like the birth and death of a person, being awarded a prize, and being a member or a leader of an organization. These topics were identified based on the known questions of QA@CLEF 2004 and 2005. HaGenLex is also the source of more than 1,000 high-quality synonyms which are used for normalizing concept names. 525 additional synonyms or near-synonyms for frequent terms were manually compiled from OpenThesaurus synsets. The resulting system comprises 636 synonym classes. Not only the representations of query and snippet, but also the fact knowledge base and the implicative axioms are automatically normalized by replacing known synonyms with a canonical representative for the synset. This normalization increases coverage of this model because distinct lexical constants are mapped to a single canonical representation. The background knowledge which refers to the canonical representation is then applicable to all lexical variants of expressing the underlying concept. Due to lack of time, it was not possible to integrate additional resources like GermaNet. This means that knowledge modeling in the first MAVE prototype is rather sparse. However, most of the hypothesis-snippet entailments or question-snippet entailments in the AVE test set are not very knowledge intensive, and the robust inference mechanism of MAVE is specifically designed for absorbing a few gaps in knowledge modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Indicators and scoring function</head><p>Defining the scoring criterion The following basic scoring criterion is used by MAVE. The returned number reflects the imperfection score of the considered hypothesis-snippet pair. A perfect logical entailment is scored 0, while an entailment relationship with slight errors (detailed below) will be scored by a small integer depending on the number of detected mismatches. A large value of 1000 will be returned to indicate that the pair should be rejected in any case. The imperfection score imp-score(p) of hypothesis pair p is given by the expression: Indicator evaluation was implemented in the Scheme programming language which explains the prefix notation of operators in the above expression. Note that the indicators can be undefined in certain cases. The functions max, +, -and * are assumed to be defined only if all arguments are defined. The special expression (? A B) is used to handle undefined values. It returns A if A is defined and B otherwise. The basic indicators on which the computation of the imp-score is based are defined as follows.</p><p>• hypo-triviality -Lemma repetition ratio.</p><p>This criterion serves to identify trivial hypotheses like 'Gianni Versace was Gianni Versace' which are logically valid but not acceptable as an answer. In order to detect such wrong positives, the hypo-triviality criterion measures the ratio of lemmata which occur an even number of times. <ref type="bibr" coords="5,508.82,557.76,3.69,6.39" target="#b0">1</ref> Trivial hypotheses of the above kind result in a high repetition score. Empirically, a threshold of hypo-triviality ≥ 0.8 was found to identify such hypotheses reliably. In the case that there is no sufficient data for computing the hypo-triviality indicator (because the syntactic processing failed completely), the assumption of a non-trivial hypothesis is a reasonable default. This assumption is captured by the expression (? (&gt;= hypo-triviality 0.8) 0), i.e. no error condition by default.</p><p>• hypo-num-sentences -The number of sentences in the hypothesis.</p><p>Due to odd punctuation and syntax errors, the parser might wrongly consider a hypothesis as consisting of two sentences although in fact we only have single-sentence hypothesis. We therefore know that there is something wrong with the analysis of the parser if hypo-num-sentences = 2. Such hypotheses get the default evaluation NO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• hypo-collapse -Query extraction control index</head><p>When there is no perfect parse of a sentence, the result of semantic analysis can be incomplete. Compared to the correct query representation, the constructed logical query will contain too few literals in this case and thus be underspecific. In order to detect this cause of false positives, we define a query extraction control measure hypo-collapse = #hypothesis-literals #hypo-content-words which relates the size of the constructed query (number of literals in the representation of the hypothesis) and the number of content words in the hypothesis string. A value of hypo-collapse &lt; 0.7 was found to be a reliable indicator for the case that the logical representation is too small compared to the size of the hypothesis. A proof of the logical hypothesis is not useful for judging answer correctness in this case because it likely corresponds to a false positive.</p><p>• hypo-lexical-focus -Hint at potential false positive</p><p>This binary criterion identifies a situation where the logical hypothesis representation is too permissive compared to the intended meaning of the hypothesis. This problem in query construction affects mainly hypotheses involving measurable properties (like '8585 meters high'). A modification of the parser will eliminate the need to consider this case in the future.</p><p>• hypo-missing-constraints -Control index for numerical constraints</p><p>Apart from regular words, the hypothesis can also contain numbers. These numbers must occur as restrictions in the constructed logical representations (for example, a restriction of the year to 1994). Although the syntactic-semantic parser normally treats these numeric constraints as expected, it turned out that they are occasionally dropped when a complete parse of the sentence fails. The indicator hypo-missing-constraints was introduced to recognize this case. It is defined as hypo-missing-constraints = #numerical-tokens-in-hypothesis -#numerical-constraints-in-logical-hypothesis.</p><p>The imp-score criterion will treat each missing constraint like a skipped literal which could not be established by logical inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• hypo-failed-literals -Number of non-provable literals</head><p>As explained above, the prover is embedded in a relaxation loop which skips non-provable literals until a proof of the rest literals succeeds. In this way, we obtain the indicator hypo-failed-literals (number of non-provable literals in the hypothesis representation). A value of 0 means strict logical entailment, while a small integer indicates a few mismatches or knowledge gaps.</p><p>• question-failed-literals -Number of non-provable literals for the question</p><p>The MAVE system also tries to prove the original question (rather than the hypothesis) from the snippet. The indicator question-failed-literals counts the number of literals in the logical question representation which had to be skipped in order to prove the remaining question literals.</p><p>• question-missing-constraints -Control index for numerical constraints in the question</p><p>The indicator counts the number of numeric tokens in the original question which are not expressed in the logical representation of the question. The evaluation criterion imp-score treats these missing constraints like a skipped literal which cannot be established in the logical proof. See indicator hypo-missing-constraints for motivation.</p><p>• question-lexical-focus -Hint at potential false positive</p><p>Due to a bug in semantics construction, the logical representation of questions involving measurable adjectives is too permissive in a few cases, which means a higher risk of a false positive. These situations are detected by the binary indicator question-lexical-focus, the analogue of hypo-lexical-focus for questions. The problem will be fixed in future revisions of MAVE and the indicators can then be omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• hypo-delta-concepts -Number of answer words in the hypothesis</head><p>A proof of the original question will only support the given hypothesis if the answer from which the hypothesis has been built is similar to the answer found in the proof of the question. As the basis for establishing such a similarity, we extract the answer part of the hypothesis (knowing that the hypothesis has been constructed from the question and from the original answer determined by the QA system). hypo-delta-concepts counts the number of content-bearing tokens in the hypothesis which likely stem from the original answer of the QA system and not from the question.</p><p>• hypo-delta-matched -Number of matched answer words</p><p>In order to relate the answer of the QA system to a proof of the original question based on the snippet, we use the matching coefficient hypo-delta-matched. Based on the proof of the logical question representation over the snippet and the known origin of literals in one of the sentences of the snippet, we first identify that sentence in the snippet which contains most of the literals supporting the proof. The content-bearing and numeric tokens in the answer part of the hypothesis are then matched against the lexical concepts and cardinality specifications in the MultiNet representation of this central sentence. <ref type="bibr" coords="7,211.28,460.46,3.69,6.39" target="#b1">2</ref> In this way, we obtain the number of those answer words which can be matched with the selected snippet sentence. The focus on a most relevant sentence of the snippet helps detect the case that a hypothesis refers to parts of the snippet not related to the answer.</p><p>Defining the selection criterion A simple threshold θ is used to determine YES/NO decisions from the basic evaluation score as defined above, i.e.</p><formula xml:id="formula_0" coords="7,204.69,551.81,186.29,26.67">select(p) = YES : imp-score(p) ≤ θ NO : else .</formula><p>Thus all hypothesis-snippet pairs with an imperfection score imp-score ≤ θ will be evaluated YES, while those items with more than θ imperfections will be rejected.</p><p>Defining the confidence ranking The assignment of confidence scores is based on the assumption that for YES decisions, the results with the least number of imperfections (as expressed by the evaluation criterion) are most reliable. Specifically, results with an imperfection score of 0 (no mismatches at all) should be fully reliable with score 1, and all other YES decisions should get smaller but positive confidence evaluation which depends on the number of detected errors. A simple linear function is used to obtain this behaviour.</p><p>As to the NO decisions, a larger number of imperfections (non-provable literals) means more evidence that the answer is indeed false. Assuming that σ or more non-provable literals (and other imperfections) mean 'totally wrong', we capture this as follows,</p><formula xml:id="formula_1" coords="8,137.14,124.96,322.53,47.13">confidence(p) =        1 -η imp-score(p) θ + 1 : select(p) = YES min 1, imp-score(p) -θ -1 σ -θ -1 : else</formula><p>where 0 ≤ θ &lt; σ -1. The parameter 0 &lt; η &lt; 1 + 1 θ can be used to balance the YES scale of confidence scores with the NO scale of confidence scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Description of Runs</head><p>The two submitted runs both instantiate the general scheme for selecting valid hypothesis and for assigning confidence scores.</p><p>• In the recall-oriented first run, θ = 2 mismatches ('imperfections') were allowed for YES decisions.</p><p>• In the more precision-oriented second run, θ = 1 mismatch was allowed for YES decisions.</p><p>We chose σ = 8 in both runs, i.e. the confidence score for NO decisions will reach 1 if the imp-score detects 8 or more mismatches. It was intended to use η = 1 in all runs. However, the confidence scores in the submitted run #1 were based on η = 1.2 by mistake.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>The viability of the basic approach of robust inferential entailment checking is witnessed by Table <ref type="table" coords="8,481.31,442.05,4.98,8.64" target="#tab_0">1</ref> which lists the results of MAVE in the answer validation exercise. 'Accuracy' measures the proportion of correct decision, and CWS is the confidence-weighted score. <ref type="bibr" coords="8,304.20,469.76,3.69,6.39" target="#b2">3</ref> Although the approach does not search for a minimal set of failed literals, but only skips literals according to a simple 'longest partial proof' strategy, the computed failed literal counts are obviously significant with respect to the validity of entailment: There is a consistent decrease in precision if more imperfections θ (usually skipped literals) are allowed, but also the intended boost in recall. The table further reveals that the two submitted runs (which admit one or two mismatches) represent the best compromise of all evaluation measures. Assuming provability of all literals (θ = 0) is too restrictive in terms of recall, while admitting θ = 3 or more mismatches considerably lowers precision.</p><p>MAVE features the use of question proofs for validating hypotheses, by comparing the answer part of the hypothesis with the lexical concepts in that snippet sentence which contributes most to the proof of the question. This mechanism is intended to increase recall when the hypothesis is syntactically ill-formed. In this case, no proof of the hypothesis is possible because no logical representation can be constructed. However, the question can usually be parsed so that a proof of the question can be tried. The hypothesis will then be compared with the answer determined by the proof of the question in a way which does not assume a parse of the hypothesis (i.e. only based on results of lexical analysis). To see the benefits of this technique, consider Table <ref type="table" coords="8,193.08,694.06,4.98,8.64" target="#tab_1">2</ref> which lists the results when no question-related indicators are used. <ref type="bibr" coords="8,465.00,692.13,3.69,6.39" target="#b3">4</ref> For θ = 1, the recall decreases by 9.6% when no question-related information is available; for θ = 2, the question indicators even contribute 13.0% of recall. Precision is lowered somewhat when using question data (by 11.2% for θ = 1 and 10.0% for θ = 2) but remains on a high level, and we get an overall improvement as witnessed by the higher f-measure and accuracy in the submitted runs. Table <ref type="table" coords="9,400.24,426.49,4.98,8.64" target="#tab_2">3</ref> shows that this increase in performance can only be achieved when both hypothesis and question indicators are combined. The table lists the results of MAVE when no information about hypothesis provability is used. <ref type="bibr" coords="9,419.44,454.21,3.69,6.39" target="#b4">5</ref> In fact, there is a clear decay in recall when deciding the validity of a hypothesis only based on the proof of the question and a subsequent matching with the hypothesis. Interestingly, the simplistic method of validating the hypothesis from a proof of the question and a comparison with the relevant snippet segment does not result in a loss of precision. The definition of imp-score(p) also involves indicators (like hypo-triviality, hypo-collapse or question-lexical-focus) which serve to detect false positives. Table <ref type="table" coords="9,399.21,545.09,4.98,8.64" target="#tab_3">4</ref> shows the results obtained when omitting these indicators from the scoring function. Specifically, it demonstrates that the proposed indicators are effective in removing false positives, because precision drops by 14.4 percent when the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have presented the MAVE system for robust inferential answer validation in the MultiNet framework. Despite its small knowledge base, the system works reasonably well in the AVE exercise -mainly because most negative cases are strikingly false and easily recognized. Moreover, a relevant number of the positive cases need limited inference or are even trivial (i.e. constructed from a substring of the snippet). Thus AVE systems can be pretty simple-minded as long as QA systems use simple techniques, and they must become smarter only as the QA systems which generate the answers become smarter as well. Apart from that, the relative success of the MAVE prototype is explained by its robust proving strategy which tolerates a few mismatches and gaps in the coded knowledge. The most obvious improvement of MAVE will be the integration of large lexico-semantic resources like Germanet; this step is necessary to boost recall beyond 50%. Now that the annotated AVE 2006 corpus for German is available as a development collection, the design of improved scoring metrics and training of statistical classificators also become an option.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,90.00,319.24,423.00,9.03;3,90.00,334.45,423.00,8.64;3,90.00,349.28,422.99,8.70;3,90.00,364.10,423.00,8.64;3,90.00,378.93,423.00,8.64;3,90.00,393.75,423.00,8.70;3,90.00,408.57,423.00,8.64;3,90.00,423.40,423.00,8.64;3,90.00,438.22,282.34,8.64"><head></head><label></label><figDesc>Indicator extraction A total of 46 indicators is extracted from the hypothesis, the snippet and from the question string, which include character-based indicators (like the length of the snippet or the proportion of non-alphabetic characters), lemma-based indicators (like the hypo-triviality ratio described below), concept-based indicators (like the presence of concepts which indicate negative polarity contexts), syntaxbased indicators (e.g. number of sentences in the snippet), indicators computed from the semantic network representation (like the indicator hypo-lexical-focus described below), and finally proof-based and matching-based indicators. It was not yet possible to consider all of these indicators in the scoring function of MAVE because there was no development collection for German. The eleven indicators which were chosen for the scoring criterion are explained at the end of this section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,94.71,281.17,18.83,7.47;5,99.41,294.76,329.52,7.47;5,99.41,308.35,18.83,7.47;5,104.12,321.94,296.56,7.47;5,141.78,335.53,235.37,7.47;5,108.83,349.12,23.54,7.47;5,104.12,362.71,301.27,7.47;5,141.78,376.30,127.10,7.47;5,155.90,389.89,131.81,7.47;5,155.90,403.47,207.12,7.47;5,108.83,417.06,32.95,7.47"><head></head><label></label><figDesc>max (? (&gt;= hypo-triviality 0.8) 0) (= hypo-num-sentences 2)) 1000) (min (? (max (* (max (&lt; hypo-collapse 0.7) hypo-lexical-focus) 1000) (+ hypo-missing-constraints hypo-failed-literals)) 1000) (? (max (* question-lexical-focus (= hypo-delta-matched 0) 1000) (+ question-failed-literals question-missing-constraints (-hypo-delta-concepts hypo-delta-matched))) 1000)))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,100.86,592.53,256.56,8.55"><head></head><label></label><figDesc><ref type="bibr" coords="9,100.86,592.53,2.99,5.18" target="#b4">5</ref> i.e. when omitting the first argument of min in the definition of imp-score(p).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,160.30,110.36,282.40,231.38"><head>Table 1 :</head><label>1</label><figDesc>Results of the MAVE system for the AVE-2006 test collection</figDesc><table coords="9,162.36,110.36,278.28,231.38"><row><cell>θ (run)</cell><cell cols="4">Precision Recall F measure Accuracy CWS</cell></row><row><cell>0</cell><cell>0.8496</cell><cell>0.2720 0.4120</cell><cell>0.8051</cell><cell>0.8505</cell></row><row><cell cols="2">1 (run #2) 0.7238</cell><cell>0.3711 0.4906</cell><cell>0.8085</cell><cell>0.8457</cell></row><row><cell cols="2">2 (run #1) 0.5878</cell><cell>0.4929 0.5362</cell><cell>0.7859</cell><cell>0.8331</cell></row><row><cell>3</cell><cell>0.4880</cell><cell>0.5779 0.5292</cell><cell>0.7418</cell><cell>0.8134</cell></row><row><cell>4</cell><cell>0.4107</cell><cell>0.6771 0.5112</cell><cell>0.6750</cell><cell>0.7883</cell></row><row><cell>5</cell><cell>0.3658</cell><cell>0.7337 0.4882</cell><cell>0.6138</cell><cell>0.7638</cell></row><row><cell>6</cell><cell>0.3289</cell><cell>0.7705 0.4610</cell><cell>0.5477</cell><cell>0.7429</cell></row><row><cell cols="5">θ Precision Recall F measure Accuracy CWS</cell></row><row><cell cols="2">0 0.9452</cell><cell>0.1955 0.3239</cell><cell>0.7952</cell><cell>0.8418</cell></row><row><cell cols="2">1 0.8361</cell><cell>0.2748 0.4137</cell><cell>0.8044</cell><cell>0.8413</cell></row><row><cell cols="2">2 0.6882</cell><cell>0.3626 0.4750</cell><cell>0.7987</cell><cell>0.8375</cell></row><row><cell cols="2">3 0.5597</cell><cell>0.4646 0.5077</cell><cell>0.7738</cell><cell>0.8280</cell></row><row><cell cols="2">4 0.4508</cell><cell>0.5326 0.4883</cell><cell>0.7198</cell><cell>0.8138</cell></row><row><cell cols="2">5 0.3963</cell><cell>0.6062 0.4793</cell><cell>0.6693</cell><cell>0.7987</cell></row><row><cell cols="2">6 0.3615</cell><cell>0.6544 0.4657</cell><cell>0.6230</cell><cell>0.7832</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,190.23,364.80,222.54,8.64"><head>Table 2 :</head><label>2</label><figDesc>MAVE results based on hypothesis proofs only</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,179.11,623.57,244.77,124.81"><head>Table 3 :</head><label>3</label><figDesc>MAVE results based on question proofs only</figDesc><table coords="9,179.11,623.57,244.77,93.11"><row><cell cols="4">θ Precision Recall F measure Accuracy CWS</cell></row><row><cell>0 0.8571</cell><cell>0.2380 0.3725</cell><cell>0.7987</cell><cell>0.8504</cell></row><row><cell>1 0.7468</cell><cell>0.3343 0.4618</cell><cell>0.8044</cell><cell>0.8481</cell></row><row><cell>2 0.6031</cell><cell>0.4476 0.5138</cell><cell>0.7873</cell><cell>0.8393</cell></row><row><cell>3 0.5081</cell><cell>0.5326 0.5201</cell><cell>0.7532</cell><cell>0.8244</cell></row><row><cell>4 0.4379</cell><cell>0.6487 0.5228</cell><cell>0.7027</cell><cell>0.8046</cell></row><row><cell>5 0.3910</cell><cell>0.7167 0.5060</cell><cell>0.6486</cell><cell>0.7822</cell></row><row><cell>6 0.3451</cell><cell>0.7450 0.4717</cell><cell>0.5811</cell><cell>0.7602</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,90.00,226.53,423.00,70.33"><head>Table 4 :</head><label>4</label><figDesc>MAVE results without false positive tests false positive indicators are ignored. This effect is most marked when the recall base is small (most false positives like 'Gianni Versace is Gianni Versace' are tautologies and thus provable for θ = 0). Recall is not altered significantly, i.e. the indicators are sufficiently selective for removing mostly false examples.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,104.35,747.12,303.87,7.05"><p>Certain lemmata like German 'sein' (En: 'be') will be filtered out before determining this ratio.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,104.35,747.00,385.94,6.91"><p>Matching is performed on the level of lexical concepts in order to profit from lemmatization and synonym normalization.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="8,104.35,727.73,408.65,7.33;8,90.00,737.20,406.29,7.33"><p>For computing the confidence weighted score (CWS), σ = 8 and η = 1 were used throughout. Notice that for θ = 2, exactly the same results are obtained for η = 1 and η = 1.2, i.e. the numbers shown for θ = 2 indeed correspond to the submitted first run.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,106.60,562.42,406.40,8.82;10,106.60,577.42,100.74,8.64" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,181.94,562.42,221.25,8.59">Hybrid Disambiguation in Natural Language Analysis</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Der Andere Verlag</publisher>
			<pubPlace>Osnabrück, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,106.60,600.22,406.40,8.64;10,106.60,615.04,223.84,8.64" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Helbig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Constantin</forename><surname>Jenge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rainer</forename><surname>Osswald</surname></persName>
		</author>
		<title level="m" coord="10,421.14,600.22,91.87,8.64;10,106.60,615.04,17.71,8.64">BenToWeb deliverable D6.2</title>
		<imprint>
			<publisher>FernUniversität in Hagen</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="10,106.60,637.83,406.40,8.64;10,106.60,652.48,406.40,8.82;10,106.60,667.48,42.34,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,343.63,637.83,169.37,8.64;10,106.60,652.66,211.80,8.64">The semantically based computer lexicon HaGenLex -Structure and technological environment</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Helbig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rainer</forename><surname>Osswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,325.55,652.48,141.78,8.59">Traitement automatique des langues</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="81" to="105" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,106.60,690.10,406.40,8.82;10,106.60,705.10,22.42,8.64" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,177.76,690.10,262.18,8.59">Knowledge Representation and the Semantics of Natural Language</title>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Helbig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,106.60,727.90,406.40,8.64;10,106.60,742.72,264.98,8.64" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,159.73,727.90,349.09,8.64">Untersuchungen zur Assimilation größerer Wissensbestände aus textueller Information</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marthen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Hagen, Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>FernUniversität in Hagen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
