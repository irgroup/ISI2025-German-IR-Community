<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,181.20,74.45,232.84,12.58;1,211.32,90.53,172.72,12.58">INAOE at CLEF 2006: Experiments in Spanish Question Answering</title>
				<funder ref="#_UPT8Y4f">
					<orgName type="full">CONACYT</orgName>
				</funder>
				<funder ref="#_7sfxu2M">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.48,119.58,106.41,9.02"><forename type="first">Antonio</forename><surname>Juárez-Gonzalez</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratorio de Tecnologías del Lenguaje Instituto Nacional de Astrofísica</orgName>
								<orgName type="laboratory" key="lab2">Óptica y Electrónica (INAOE)</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.06,119.58,91.29,9.02"><forename type="first">Alberto</forename><surname>Téllez-Valero</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratorio de Tecnologías del Lenguaje Instituto Nacional de Astrofísica</orgName>
								<orgName type="laboratory" key="lab2">Óptica y Electrónica (INAOE)</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,353.13,119.58,100.84,9.02"><forename type="first">Claudia</forename><surname>Denicia-Carral</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratorio de Tecnologías del Lenguaje Instituto Nacional de Astrofísica</orgName>
								<orgName type="laboratory" key="lab2">Óptica y Electrónica (INAOE)</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,191.82,131.04,106.08,9.02"><forename type="first">Manuel</forename><surname>Montes-Y-Gómez</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratorio de Tecnologías del Lenguaje Instituto Nacional de Astrofísica</orgName>
								<orgName type="laboratory" key="lab2">Óptica y Electrónica (INAOE)</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.40,131.04,98.05,9.02"><forename type="first">Luis</forename><surname>Villaseñor-Pineda</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratorio de Tecnologías del Lenguaje Instituto Nacional de Astrofísica</orgName>
								<orgName type="laboratory" key="lab2">Óptica y Electrónica (INAOE)</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,181.20,74.45,232.84,12.58;1,211.32,90.53,172.72,12.58">INAOE at CLEF 2006: Experiments in Spanish Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6C2AA25F8E03902C661F3EBC996F212C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the system developed by the Language Technologies Lab at INAOE for the Spanish Question Answering task at CLEF 2006. The presented system is centered in a full datadriven architecture that uses machine learning and text mining techniques to identify the most probable answers to factoid and definition questions respectively. Its major quality is that it mainly relies on the use of lexical information and avoids applying any complex language processing resource such as named entity classifiers, parsers or ontologies. Experimental results show that the proposed architecture can be a practical solution for monolingual question answering reaching an answer precision as high as 51%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Current information requirements claim for efficient mechanisms capable of interact with users in a more natural way. Question Answering (QA) systems has been proposed as a feasible option for the creation of such mechanisms <ref type="bibr" coords="1,97.63,360.24,10.64,9.02" target="#b0">[1]</ref>. Recent developments in QA use a variety of linguistic resources to help in understanding the questions and the documents. The most common linguistic resources include: part-of-speech taggers, parsers, named entity extractors, dictionaries, and WordNet <ref type="bibr" coords="1,254.40,383.22,10.87,9.02" target="#b1">[2,</ref><ref type="bibr" coords="1,268.80,383.22,7.54,9.02" target="#b2">3,</ref><ref type="bibr" coords="1,279.90,383.22,7.50,9.02" target="#b3">4,</ref><ref type="bibr" coords="1,290.93,383.22,7.19,9.02" target="#b4">5]</ref>. Despite of the promising results of these approaches, they have two main inconveniences. On the one hand, the construction of such linguistic resources is a very complex task. On the other hand, their performance rates are usually not optimal.</p><p>In this paper we present a QA system that allows answering factoid and definition questions. This system is based on a full data-driven approach that requires a minimum knowledge about the lexicon and the syntax of the specified language. It is basically supported on the idea that the questions and their answers are commonly expressed using the same set of words. Therefore, it simply uses lexical information to identify the relevant document passages and to extract the candidate answers.</p><p>The prototype presented this year by our group continues our last year work <ref type="bibr" coords="1,396.25,475.20,10.82,9.02" target="#b5">[6]</ref>: it is also based on a lexical full data-driven approach. However, it presents two important modifications. First, it applies a supervised approach instead of a statistical method for answering factoid questions. Second, it answers definition questions by applying lexical patterns that were automatically constructed rather manually defined.</p><p>The following sections give some details on the proposed system. In particular, section 2 describes the method for answering factoid questions, section 3 explains the method for answering definition questions, and section 4 discusses the results achieved by our system in the Spanish Question Answering task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Answering Factoid Questions</head><p>Figure <ref type="figure" coords="1,100.56,585.48,5.01,9.02" target="#fig_1">1</ref> shows the general process for answering factoid questions. It considers three main modules: passage retrieval, where the passages with the major probability to contain the answer are recovered from the document collection; question classification, where the type of expected answer is determined; and answer extraction, where candidate answers are selected using a machine-learning approach, and the final answer recommendation of the system is produced. The following sections describe each of these modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Passage Retrieval</head><p>The passage retrieval (PR) method is specially suited for the QA task <ref type="bibr" coords="1,356.35,672.48,10.64,9.02" target="#b6">[7]</ref>. It allows retrieving the passages with the highest probability to contain the answer instead of simply recover the passages sharing a subset of words with the question.</p><p>Given a user question, the PR method finds the passages with the relevant terms (non-stopwords) using a classical information retrieval technique based on the vector space model. Then, it measures the similarity between the n-gram sets of the passages and the user question in order to obtain the new weights for the passages. The weight of a passage is related to the largest n-gram structure of the question that can be found in the passage itself. The larger the n-gram structure, the greater the weight of the passage. Finally, it returns to the user the passages with the new weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Similarity measure</head><p>The similarity between a passage d and a question q is defined by <ref type="bibr" coords="2,335.65,274.02,10.65,9.02" target="#b0">(1)</ref>.</p><formula xml:id="formula_0" coords="2,238.62,286.68,278.97,59.49">( ) ( ) ( ) ∑ ∑ ∑ ∑ = ∈ ∀ = ∈ ∀ = n j Q x j n j Q x j j j Q j x h D j x h q d sim 1 1 ), ( ), ( ,<label>(1)</label></formula><p>Where sim(d, q) is a function which measures the similarity of the set of n-grams of the question q with the set of n-grams of the passage d. Q j is the set of j-grams that are generated from the question q and D j is the set of jgrams of the passage d. That is, Q 1 will contain the question unigrams whereas D 1 will contain the passage unigrams, Q 2 and D 2 will contain the question and passage bigrams respectively, and so on until Q n and D n . In both cases, n is the number of question terms.</p><p>The result of ( <ref type="formula" coords="2,139.77,410.94,3.90,9.02" target="#formula_0">1</ref>) is equal to 1 if the longest n-gram of the question is in the set of passage n-grams. The function h(x(j), D j ) measures the relevance of the j-gram x(j) with respect to the set of passage j-grams, whereas the function h(x(j), Q j ) is a factor of normalization 1 . The function h assigns a weight to every question ngram as defined in (2).</p><p>( )</p><formula xml:id="formula_1" coords="2,224.88,458.72,291.03,40.34">⎪ ⎩ ⎪ ⎨ ⎧ ∈ = ∑ = otherwise D j x if w D j x h j j k x j k 0 ) ( ), ( 1 ) 1 ( ˆ (2)</formula><p>Where the notation ) 1 ( ˆk x indicates the k-th unigram included in the j-gram x, and ) 1 ( ˆk x w specifies the associated weight to this unigram. This weight gives an incentive to the terms -unigrams-that appear rarely in the document collection. Moreover, this weight should also discriminate the relevant terms against those (e.g. stopwords) which often occur in the document collection.</p><p>The weight of a unigram is calculated by (3):</p><p>( )</p><formula xml:id="formula_2" coords="2,255.30,572.04,259.59,26.25">( ) N n w k k x x log 1 log 1 ) 1 ( ) 1 ( ˆ+ - =<label>(3)</label></formula><p>Where , and N is the total number of passages in the collection. We assume that the stopwords occur in every passage (i.e., n takes the value of N). For instance, if the term appears once in the passage collection, its weight will be equal to 1 (the maximum weight), whereas if the term is a stopword, then its weight will be the lowest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question Classification</head><p>This module is responsible of the definition of the semantic class of the answer expected to respond to the given question. The idea is to know in advance the type of the expected answer in order to reduce the searching space 1 We introduce the notation x(n) for the sake of simplicity. In this case x(n) indicates the n-gram x of size n.  Our prototype implements this module following a direct approach based on regular expressions. It only considers three general semantic classes for the type of expected answer: date, quantity and name (i.e., a proper noun).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Extraction</head><p>Answer extraction aims to establish the best answer for a given question. It is based on a supervised machine learning approach. It consists of two main modules, one for attribute extraction and other one for answer selection.</p><p>Attribute extraction. First, the set of recovered passages are processed. The purpose is to identify all text fragments related to the semantic class of the expected answer. This process is done using a set of regular expression that allows identifying proper names, dates and quantities. Each identified text fragment is considered a "candidate answer".</p><p>In a second step, the lexical context of each candidate answer is analyzed with the aim of constructing its formal representation. In particular, each candidate answer is represented by a set of 17 attributes, clustered in the following groups:</p><p>1. Attributes that describe the complexity of the question. For instance, the length of the question (number of non-stopwords). 2. Attributes that measure the similarity between the context of the candidate answer and the given question.</p><p>Basically, these attributes considers the number of common words, word lemmas and named entities (proper names) between the context of the candidate answer and the question. They also take into consideration the density of the question words in the answer context. 3. Attributes that indicate the relevance of the candidate answer in accordance with the set of recovered passages. For instance, the relative position of passage that contains the candidate answer as well as the redundancy of the answer in the whole set of passages.</p><p>Answer Selection. This module selects from the set of candidate answers the one with the maximum probability of being the correct answer. This selection is done by a machine learning method, in particular, by a Naïve Bayes classifier.</p><p>It is important to mention that the classification model (actually, we have three classifiers, one for each kind of answer) was constructed using as a training set the questions and documents from previous CLEFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Answering Definition Questions</head><p>Figure <ref type="figure" coords="3,100.25,472.02,5.01,9.02" target="#fig_3">2</ref> shows the general scheme of our method for answering definition questions<ref type="foot" coords="3,417.24,471.76,3.00,5.40" target="#foot_0">2</ref> . It consists of three main modules: a module for the discovery of definition patterns, a module for the construction of a general definition catalog, and a module for the extraction of the candidate answer. The following sections describe in detail these modules.</p><p>It is important to mention that this method is specially suited for answering definition questions as delimited in the CLEF. That is, questions asking for the position of a person, e.g., Who is Vicente Fox?, and for the description of concept, e.g., What is the CERN? or What is Linux?.</p><p>It is also important to notice that the processes of pattern discovery and catalog construction are done offline, while the answer extraction is done online, and that different to traditional QA approaches, the proposed method does not consider any module for document or passage retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pattern Discovery</head><p>The module for pattern discovery uses a small set of concept-description pairs to collect from the Web an extended set of definition instances. Then, it applies a text mining method on the collected instances to discover a set of definition surface patterns. The idea is to capture the definition conventions through their repetition. This module considers two main subtasks:</p><p>Definition searching. This task is triggered by a small set of empirically defined concept-description pairs. The pairs are used to retrieve a number of usage examples from the Web<ref type="foot" coords="3,378.30,673.72,3.00,5.40" target="#foot_1">3</ref> . Each usage example represents a definition instance. To be relevant, a definition instance must contain the concept and its description in one single phrase.</p><p>Pattern mining. It is divided in three main steps: data preparation, data mining and pattern filtering. The purpose of the data preparation phase is to normalize the input data. It transforms all definition instances into the same format using special tags for the concepts and their descriptions. It also indicates with a special tag the concepts expressing proper names.</p><p>In the data mining phase, a sequence mining algorithm <ref type="bibr" coords="4,304.61,393.54,11.72,9.02" target="#b8">[9]</ref> is used to obtain all maximal frequent sequences of words, punctuation marks and tags from the set of definition instances. The sequences express lexicographic patterns highly related to concept definitions.</p><p>Finally, the pattern-filtering phase allows choosing the more discriminative patterns. It selects the patterns satisfying the following general regular expressions: Figure <ref type="figure" coords="4,111.13,557.10,5.01,9.02" target="#fig_4">3</ref> illustrates the information treatment through the pattern discovery process. The idea is to obtain several surface definition patterns starting up with a small set of concept-description example pairs. First, using a small set of concept description seeds, for instance, "Wolfgang Clement -German Federal Minister of Economics and Labor" and "Vicente Fox -President of Mexico", we obtained a set of definition instances. One example of these instances is "…meeting between the Cuban leader and the president of Mexico, Vicente Fox.". Then, the instances were normalized, and finally a sequence-mining algorithm was used to obtain some lexical patterns highly related to concept definitions. The figure shows two example patterns: ", the &lt;DESCRIPTION&gt;, &lt;CONCEPT&gt;, says" and "the &lt;DESCRIPTION&gt; &lt;PROPER_NAME_CONCEPT&gt;". It is important to notice that the discovered patterns may include words, punctuation marks as well as proper name tags as frontier elements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Catalog Construction</head><p>In this module, the definition patterns discovered in the previous stage (i.e., in the pattern discovery module) are applied over the target document collection. The result is a set of matched text segments that presumably contain a concept and its description. The definition catalog is created gathering all matched segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer Extraction</head><p>This module handles the extraction of the answer for a given definition question. Its purpose is to find the more adequate description for a requested concept from the definition catalog. The definition catalog may contain a huge diversity of information, including incomplete and incorrect descriptions for many concepts. However, it is expected that the correct information will be more abundant than the incorrect one. This expectation supports the idea of using a frequency criterion and a text mining technique to distinguish between the adequate and the improbable answers to a given question. This module considers the following steps:</p><p>Description filtering. Given a specific question, this procedure extracts from the definition catalog all descriptions corresponding to the requested concept. As we mentioned, these "presumable" descriptions may include incomplete and incorrect information. However, it is expected that many of them will contain, maybe as a substring, the required answer.</p><p>Answer selection. This process aims to detect a single answer to the given question from the set of extracted descriptions. It is divided in two main phases: data preparation and data mining.</p><p>The data preparation phase focuses on homogenizing the descriptions related to the requested concept. The main action is to convert these descriptions to a lower case format. In the data mining phase, a sequence mining algorithm <ref type="bibr" coords="5,112.46,602.09,11.69,9.02" target="#b8">[9]</ref> is used to obtain all maximal frequent word sequences from the set of descriptions. Then, the most frequent sequence is selected as the correct answer.</p><p>Figure <ref type="figure" coords="5,111.12,625.07,5.01,9.02" target="#fig_5">4</ref> shows the process of answer extraction for the question "Who is Diego Armando Maradona?". First, we obtained all descriptions associated with the requested concept. It is clear that there are erroneous or incomplete descriptions (e.g. "Argentina soccer team"). However, most of them contain a partially satisfactory explanation of the concept. Actually, we detected correct descriptions such as "captain of the Argentine soccer team" and "Argentine star". Then, a mining process allowed detecting a set of maximal frequent sequences. Each sequence was considered a candidate answer. In this case, we detected three sequences: "argentine", "captain of the Argentine soccer team" and "supposed overuse of Ephedrine by the star of the Argentine team". Finally, the candidate answers were ranked based on the frequency of occurrence of its subsequences in the whole description set. In this way, we took advantage of the incomplete descriptions of the concept. The selected answer was "captain of the Argentine national football soccer team", since it was conformed from frequent subsequences such as "captain of the", "soccer team" and "Argentine".  It is important to clarify that a question may have several correct answers. In accordance with the CLEF, an answer is correct if there is a passage that supports it. Therefore, for the question at hand there are other correct answers such as "ex capitán de la selección argentina de futbol" and "astro argentino".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Results</head><p>This section describes the experimental results related to our participation at QA@CLEF2006 monolingual track for Spanish. It is important to remember that this year the question type (e.g., factoid, definition, temporal or list) was not included as a data field on the question test file. Therefore, each participant had to automatically determine the kind of question.</p><p>Our system prototype, as described in the previous sections, only can deal with factoid and definition questions. In particular, from the 200 test questions, it treats 144 as factoid questions and the rest of them as definition questions. Table <ref type="table" coords="6,157.01,500.82,5.01,9.02" target="#tab_2">1</ref> details our results on answering factoid questions.  On the other hand, the method for answering definition questions was used to respond 56 questions; from them 28 questions asked for the position of a person (who questions) and 28 asked for the description of a concept (what questions). Table <ref type="table" coords="7,167.62,96.24,5.01,9.02" target="#tab_3">2</ref> resumes the assessed results from this kind of questions. In addition to the outstanding results obtained by this method, it was very interesting to notice that it replies very exact answers most of the times. Nevertheless, it has the inconvenience of constructing an enormous definition catalog (1,772,918 for concept expansion and 3,525,632 for persons positions) containing a huge quantity of incorrect/incomplete registers. This characteristic was the origin of most of our wrong answers, since noisy information was more redundant that correct one.</p><p>Lastly, it is important to mention that the overall evaluation of this year exercise (51%) was 10-points over our last year result <ref type="bibr" coords="7,147.63,306.54,10.64,9.02" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>This paper presented a question answering system that allows answering factoid and definition questions. This system is based on a lexical data-driven approach. Its main idea is that the questions and their answers are commonly expressed using almost the same set of words, and therefore, it simply uses lexical information to identify the relevant passages as well as the candidate answers.</p><p>The answer extraction for factoid questions is based on a machine learning method. Each candidate answer (uppercase word, date or quantity) is represented by a set of lexical attributes and a classifier determines the most probable answer for the given question. The method achieved good results, however it has two significant disadvantages: (i) it requires a lot of training data, (ii) the detection of the candidate answers is not always (not for all cases, nor for all languages) an easy -high precision-task.</p><p>On the other hand, the answer extraction for definition questions is based on a text mining approach. The proposed method uses a text mining technique (namely, a sequence mining algorithm) to discover a set of definition patterns from the Web as well as to determine -with finer precision-the answer to a given question. The achieved results were especially good, and they evidenced that a non-standard QA approach, which does not contemplate an IR phase, can be a good scheme for answering definitions questions.</p><p>As future work we plan to improve the final answer selection by applying an answer validation method. The purpose is to reduce the dependence of our current methods to the answer redundancy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,199.32,202.56,197.54,9.02"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Process for answering factoid questions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,130.56,456.70,345.57,7.07;4,130.56,467.92,345.57,7.07;4,98.16,479.14,410.36,7.07;4,98.16,490.30,410.36,7.07;4,138.66,501.52,329.37,7.07;4,135.96,512.68,334.77,7.07;4,181.86,523.90,242.98,7.07;4,179.16,535.12,248.38,7.07"><head></head><label></label><figDesc>&lt;left-string&gt; DESCRIPTION &lt;middle-string&gt; CONCEPT &lt;right-string&gt; &lt;left-string&gt; CONCEPT &lt;middle-string&gt; DESCRIPTION &lt;right-string&gt; &lt;left-string&gt; DESCRIPTION &lt;middle-string&gt; PROPER_NAME_CONCEPT &lt;right-string&gt; &lt;left-string&gt; PROPER_NAME_CONCEPT &lt;middle-string&gt; DESCRIPTION &lt;right-string&gt; &lt;left-string&gt; DESCRIPTION &lt;middle-string&gt; PROPER_NAME_CONCEPT PROPER_NAME_CONCEPT &lt;middle-string&gt; DESCRIPTION &lt;right-string&gt; &lt;left-string&gt; DESCRIPTION PROPER_NAME_CONCEPT PROPER_NAME_CONCEPT DESCRIPTION &lt;right-string&gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,193.32,315.00,208.64,9.02"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Process for answering definition questions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,203.46,324.78,207.57,9.02"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Data flow in the pattern discovery process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,191.04,337.62,209.16,9.02"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Data flow in the answer extraction process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,135.89,89.31,318.60,542.76"><head>Table 1 .</head><label>1</label><figDesc>Accuracy on answering factoid questions (run inao061eses)</figDesc><table coords="6,182.64,545.43,227.87,86.64"><row><cell></cell><cell>Overall</cell><cell cols="3">Evaluation by answer type</cell></row><row><cell></cell><cell>Evaluation</cell><cell>Quantity</cell><cell>Date</cell><cell>Name</cell></row><row><cell>Right</cell><cell>59</cell><cell>13</cell><cell>9</cell><cell>37</cell></row><row><cell>Wrong</cell><cell>75</cell><cell>13</cell><cell>10</cell><cell>52</cell></row><row><cell>Inexact</cell><cell>2</cell><cell>0</cell><cell>1</cell><cell>1</cell></row><row><cell>Unsupported</cell><cell>8</cell><cell>0</cell><cell>5</cell><cell>3</cell></row><row><cell>Accuracy</cell><cell>40.9%</cell><cell>50%</cell><cell>36%</cell><cell>39.7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,169.08,124.89,257.22,97.74"><head>Table 2 .</head><label>2</label><figDesc>Accuracy on answering definition questions (run inao061eses)</figDesc><table coords="7,191.88,138.45,213.84,84.18"><row><cell></cell><cell>Overall</cell><cell>Person's</cell><cell>Concept's</cell></row><row><cell></cell><cell>Evaluation</cell><cell>Positions</cell><cell>Descriptions</cell></row><row><cell>Right</cell><cell>43</cell><cell>19</cell><cell>24</cell></row><row><cell>Wrong</cell><cell>11</cell><cell>7</cell><cell>4</cell></row><row><cell>Inexact</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>Unsupported</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>Accuracy</cell><cell>76.7%</cell><cell>67.8%</cell><cell>85.7%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,76.20,731.97,241.94,8.10"><p>This method is an adaptation of the one previously proposed in<ref type="bibr" coords="3,305.51,731.97,9.47,8.10" target="#b7">[8]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,76.20,742.95,198.00,8.10"><p>At present we are using Google for searching the Web.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was done under partial support of <rs type="funder">CONACYT</rs> (Project Grants: <rs type="grantNumber">43990</rs> and <rs type="grantNumber">U39957-Y</rs>). We also like to thanks to the CLEF organizing committee as well as to the <rs type="institution">EFE agency</rs> for the resources provided.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_UPT8Y4f">
					<idno type="grant-number">43990</idno>
				</org>
				<org type="funding" xml:id="_7sfxu2M">
					<idno type="grant-number">U39957-Y</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,74.69,612.12,449.88,9.02;7,88.92,623.58,435.69,9.02;7,88.92,635.10,315.31,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,88.92,623.58,276.24,9.02">Overview of the CLEF 2004 Multilingual Question Answerig Track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Erbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,384.72,623.58,139.89,9.02;7,88.92,635.10,197.34,9.02">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2004)</title>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">September 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,74.69,646.62,449.80,9.02;7,88.92,658.08,435.62,9.02;7,88.92,669.60,164.51,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,475.88,646.62,48.62,9.02;7,88.92,658.08,89.93,9.02">Spanish QA System at CLEF-2005</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ferrández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ferrández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tomás</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Aliqan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,201.30,658.08,323.24,9.02;7,88.92,669.60,21.53,9.02">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2005)</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,74.69,681.12,449.79,9.02;7,88.92,692.58,435.59,9.02;7,88.92,704.10,340.27,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,453.42,681.12,71.06,9.02;7,88.92,692.58,275.26,9.02">Monolingual and Cross-language QA using a QA-oriented Passage Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gómez-Soriano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bisbal-Asensi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanchos-Arnal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,384.06,692.58,140.45,9.02;7,88.92,704.10,197.34,9.02">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2005)</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,74.69,715.62,449.87,9.02;7,88.92,727.08,435.61,9.02;7,88.92,738.60,355.76,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,124.38,727.08,267.62,9.02">MIRACLE&apos;s 2005 Approach to Cross-Lingual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>De-Pablo-Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>González-Ledesma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Martinez-Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Guirao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,410.27,727.08,114.26,9.02;7,88.92,738.60,212.92,9.02">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2005)</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,74.69,73.26,449.86,9.02;8,88.92,84.72,435.74,9.02;8,88.92,96.24,126.12,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,388.14,73.26,136.42,9.02;8,88.92,84.72,56.46,9.02">The TALP-QA System for Spanish at CLEF-2005</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrés</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kanaan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ageno</forename><surname>Al</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,165.53,84.72,338.13,9.02">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2005)</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,74.69,107.76,449.80,9.02;8,88.92,119.22,435.58,9.02;8,88.92,130.74,435.49,9.02;8,88.92,142.26,67.82,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,130.74,119.22,393.76,9.02;8,88.92,130.74,11.51,9.02">INAOE-UPV Joint Participation at CLEF 2005: Experiments in Monolingual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pérez-Coutiño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gómez-Soriano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanchis-Arnal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,118.29,130.74,332.75,9.02">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2005)</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,74.69,153.72,449.80,9.02;8,88.92,165.24,435.59,9.02;8,88.92,176.76,435.50,9.02;8,88.92,188.22,22.58,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,484.50,153.72,40.00,9.02;8,88.92,165.24,228.31,9.02">Language Independent Passage Retrieval for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gómez-Soriano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanchis-Arnal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,338.20,165.24,186.31,9.02;8,88.92,176.76,241.36,9.02">Proceedings for the Fourth Mexican International Conference on Artificial Intelligence (MICAI 2005)</title>
		<meeting>for the Fourth Mexican International Conference on Artificial Intelligence (MICAI 2005)<address><addrLine>Monterrey, Nuevo León, México</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">November 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,74.69,199.74,449.78,9.02;8,88.92,211.20,435.63,9.02;8,88.92,222.72,333.42,9.02" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,463.62,199.74,60.85,9.02;8,88.92,211.20,182.65,9.02">A Text Mining Approach for Definition Question Answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Denicia-Carral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>García-Hernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,332.45,211.20,192.10,9.02;8,88.92,222.72,207.70,9.02">Proceedings for the Fifth International Conference on Natural Language Processing (FinTal 2006)</title>
		<meeting>for the Fifth International Conference on Natural Language Processing (FinTal 2006)<address><addrLine>Turku, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-08">August 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,74.69,234.24,449.79,9.02;8,88.92,245.70,435.60,9.02;8,88.92,257.22,435.67,9.02;8,88.92,268.74,22.58,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,366.24,234.24,158.25,9.02;8,88.92,245.70,223.67,9.02">A New Algorithm for Fast Discovery of Maximal Sequential Patterns in a Document Collection</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>García-Hernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Martínez-Trinidad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Carrasco-Ochoa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,333.42,245.70,191.10,9.02;8,88.92,257.22,300.12,9.02">Proceedings for the Seventh International Conference on Computational Linguistics and text Processing (CICLing 2006)</title>
		<meeting>for the Seventh International Conference on Computational Linguistics and text Processing (CICLing 2006)<address><addrLine>Mexico City, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-02">February 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
