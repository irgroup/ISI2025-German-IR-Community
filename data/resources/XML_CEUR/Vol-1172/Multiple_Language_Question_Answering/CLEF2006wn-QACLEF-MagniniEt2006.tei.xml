<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,92.16,74.11,409.92,12.65">Over view of the CLEF 2006 Multilingual Question Answer ing Tr ack</title>
				<funder ref="#_gGSQUjX">
					<orgName type="full">German Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
				<funder>
					<orgName type="full">POSI</orgName>
				</funder>
				<funder ref="#_y2scDN2">
					<orgName type="full">Portuguese Fundação para a Ciência e Tecnologia</orgName>
				</funder>
				<funder ref="#_ued2HJg">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,70.80,146.37,77.68,9.16"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
							<email>magnini@itc.it</email>
							<affiliation key="aff0">
								<orgName type="institution">ITCIrst</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,160.56,146.37,84.96,9.16"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
							<email>giampiccolo@celct.it</email>
							<affiliation key="aff1">
								<orgName type="institution">CELCT</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,256.80,146.37,62.64,9.16"><forename type="first">Pamela</forename><surname>Forner</surname></persName>
							<email>forner@celct.it</email>
							<affiliation key="aff1">
								<orgName type="institution">CELCT</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,331.68,146.37,75.76,9.16"><forename type="first">Christelle</forename><surname>Ayache</surname></persName>
							<email>ayache@elda.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">ELDA/ELRA</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,419.76,146.37,69.84,9.16"><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
							<email>jijkoun@science.uva.nl</email>
							<affiliation key="aff3">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,502.08,146.37,21.76,9.16;1,70.80,157.89,34.72,9.16"><forename type="first">Petya</forename><surname>Osenova</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">BTB</orgName>
								<address>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,114.24,157.89,62.00,9.16"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
							<affiliation key="aff5">
								<orgName type="department">Departamento de Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,184.56,157.89,50.56,9.16"><forename type="first">Paulo</forename><surname>Rocha</surname></persName>
							<email>paulo.rocha@alfa.di.uminho.pt</email>
							<affiliation key="aff6">
								<orgName type="department">Linguateca</orgName>
								<orgName type="institution">SINTEF ICT and Portugal</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.60,157.89,73.92,9.16"><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
							<email>bogdan.sacaleanu@dfki.de</email>
							<affiliation key="aff7">
								<orgName type="institution">DFKI</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,342.96,157.89,68.31,9.16"><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
							<email>richard.sutcliffe@ul.ie</email>
							<affiliation key="aff8">
								<orgName type="department">DLTG</orgName>
								<orgName type="institution">University of Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,92.16,74.11,409.92,12.65">Over view of the CLEF 2006 Multilingual Question Answer ing Tr ack</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F3231D83376EC1DCAEA0A82167197E20</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Having being proposed for the fourth time, the QA at CLEF track has confirmed a still raising interest from the research community, recording a constant increase both in the number of participants and submissions. In 2006, two pilot tasks, WiQA and AVE, were proposed beside the main tasks, representing two promising experiments for the future of QA. Also in the main task some significant innovations were introduced, namely list questions and requiring text snippet(s) to support the exact answers. Although this had an impact on the work load of the organizers both to prepare the question sets and especially to evaluate the submitted runs, it had no significant influence on the performance of the systems, which registered a higher Best accuracy than in the previous campaign, both in monolingual and bilingual tasks. In this paper the preparation of the test set and the evaluation process are described, together with a detailed presentation of the results for each of the languages. The pilot tasks WiQA and AVE will be presented in dedicated articles.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.2" lry="841.68"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction</p><p>Inspired by previous TREC evaluation campaigns, QA tracks have been proposed at CLEF since 2003. During these years, the effort of the organisers has been focused on two main issues. One issue was to offer an evaluation exercise characterised by crosslinguality, covering as many languages as possible. From this perspective, major attention has been given to European languages, adding at least one new language each year, but keeping the offer open to languages from allover the world, as the use of Indonesian shows. The other important issue was to maintain a balance between the established procedure inherited by the TREC campaigns and innovation. This allowed newcomers to join the competition and, at the same time, offered "veterans" more challenges. Following these principles, in QA@CLEF 2006 two pilot tasks, namely WiQA and Answer Validation Exercise (AVE), were proposed together with a main task. As far as the latter is concerned, the most significant innovation was the introduction of lIST questions, which had also been considered for previous competitions, but had previously been avoided due to the problems that their selection and assessment implied. Other important innovations consisted in the possibility to return more than one answer per question, and by the request to provide text snippets together with the docid to support the exact answer. All these changes implied the necessity of introducing new evaluation measures, which would account also for List and multiple answers. Nevertheless, the evaluation process proved to be more complicated than expected, partly because of the excessive workload that multiple answers represented for groups already in charge for a larger number of runs.</p><p>As a consequence, some groups, like the Spanish and the English ones, could only correct one answer per question, which decreased the possibility of comparisons between runs. As a general remark, it can be said that the positive trend in participation registered in the previous campaigns was confirmed, and 10 new participants joined the competition from Europe, Asia and America. As reflected in the results, systems' performance improved considerably, with the Best Accuracy increasing from 64% to 68% in the monolingual tasks, and, more significantly, from 39% to 49% in the bilingual ones. This paper describes the preparation process and presents the results of the QA track at CLEF 2006. In section 2, the task is described in detail. The different phases of the Gold Standard preparation are exposed in section 3. After a quick presentation of the participants in section 4, the evaluation procedure and the results are reported respectively in section 5 and 6. In section 7, some final considerations are given about this campaign and the future of QA@CLEF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks</head><p>In 2006 campaign, the procedure consolidated in previous competitions was used. Accordingly, there was a main task (which was comprehensive of a monolingual task and several crosslanguage subtasks), and two pilot tasks described below:</p><p>1. WiQA: developed by Maarten de Rijke. The purpose of the WiQA pilot is to see how IR and NLP techniques can be effectively used to help readers and authors of Wikipages access information spread throughout Wikipedia rather than stored locally on the pages. <ref type="bibr" coords="2,351.52,316.53,12.32,9.16">[2]</ref> 2. Answer Validation Exercise (AVE): A voluntary exercise to promote the development and evaluation of subsystems aimed at validating the correctness of the answers given by a QA system. The basic idea is that once a pair [answer + snippet] is returned by a QA system, a hypothesis is built by turning the pair [question + answer] into the affirmative form. If the related text (a snippet or a document) semantically entails this hypothesis, then the answer is expected to be correct. <ref type="bibr" coords="2,420.72,385.65,11.75,9.16">[3]</ref> Two specific papers in the present Working Notes are dedicated to these pilot tasks. More detailed information, together with the results, can be found there.</p><p>In addition to the tasks proposed during the actual competition, a "timeconstrained" QA exercise will be proposed by the University of Alicante during the CLEF 2006 Workshop. In order to evaluate the ability of QA systems to retrieve answers in real time, the participants will be given a time limit (e.g. one or two hours) in which to answer a set of questions. These question sets are different and smaller than those provided in the main task (e.g. 1525 questions). The initiative is aimed towards providing a more realistic scenario for a QA exercise.</p><p>The main task was basically the same as in previous campaigns. Some new ideas were implemented in order to make the competition more challenging. The participating systems were fed a set of 200 questions, which could be about:</p><p>• facts or events (Factoid questions); • definitions of people, things or organisations (Definition questions); • lists of people, objects or data (List questions).</p><p>The systems were then asked to return from one to ten exact answers. "Exact" meant that neither more nor less than the information required is given. The answer needed to be supported by the docid of the document(s) in which the exact answer was found, and by one to ten text snippets which gave the actual context of it. The text snippets were to be put one next to the other, separated by a tab. The snippets were substrings of the specified documents. They should provide enough context to justify the exact answer suggested. Snippets for a given response had to be a set of sentences of not more than 500 bytes in total (although for example the Portuguese group accepted -and actually preferred -length to be specified in sentences). There were no particular restrictions on the length of an answerstring, but unnecessary pieces of information were penalized, since the answer was marked as ineXact. Since Definition questions may have long strings as answers, they were (subjectively) assessed mainly on their informativity and usefulness, and not on exactness. The tasks were both:</p><p>• monolingual, where the language of the question (Source language) and the language of the news collection (Target language) were the same; • crosslingual, where the questions were formulated in a language different from that of the news collection. Eleven source languages were considered, namely, Bulgarian, Dutch , English, French, German, Indonesian, Italian, Polish , Portuguese, Romanian and Spanish. Note the loss of Finnish, and the introduction of Polish and Romanian with respect to last year. All these languages were also considered as target languages, except for Indonesian, Polish and Romanian. These three languages had no news collection available for the queries. As was done for Indonesian in the previous two campaigns, the English question set was translated into Indonesian (IN), Polish (PL) and Romanian (RO), and the German question set into Romanian (RO). Only the bilingual tasks INEN, PLEN, ROEN and RODE were activated. In the case of INEN, PLEN, and ROEN, the questions were posed in the respective language (i.e. IN, PL, RO), while the answers were retrieved from the English collection. In the RODE case, the question was made in Romanian, whilst the answer was retrieved from the German collection.</p><p>As shown in Table <ref type="table" coords="3,148.80,463.17,3.66,9.16" target="#tab_0">1</ref>, 24 tasks were proposed and divided in:  As customary in recent campaigns, a monolingual English (EN) task was not available as it seems to have been already thoroughly investigated in TREC campaigns, even though English was both source and target language in the crosslanguage tasks.</p><p>Although the task was not radically changed with regard to previous campaigns, some new elements were introduced. The most important one was the addition of List questions to the question sets, which implied some major issues. For this first year of QA@CLEF, we were not too strict on the definition of lists, using both questions asking for a specific finite number of answers (that could be called "closed lists") e.g.:</p><p>Q: What are the names of the two lovers from Verona separated by family issues in one of Shakespeare's plays? A: Romeo and Juliet. and open lists ,where as many correct answers could be returned, e.g. Q: Name books by Jules Verne. and let organizing groups decide on how to assess the answers to these different kinds of questions.</p><p>Other innovations were:</p><p>• the input format, where the type of question (F,D,L) was no longer indicated; • and the result format, where up to a maximum of ten answers per question was allowed, with one to ten text snippets supporting the exact answer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Test Set Preparation</head><p>Following the procedure established in previous campaigns, initially each organising group (one for each Target language) was assigned a number of topics taken from the CLEF IR track on which candidates' questions were based. This choice was originally made to reduce the number of duplicates in the multilingual question set. As the number of new topics introduced in 2006 was small, old topics were simply reassigned to different groups. Some groups questioned this methodology, preferring to produce questions with other methods instead of following particular topics. The topics, and hence the questions, were aimed at data collections composed of news articles provided by ELRA/ELDA dating back to 1994/1995; with the exception of Bulgarian, which dated back to 2000 (see Table <ref type="table" coords="5,168.96,130.29,3.56,9.16" target="#tab_2">3</ref>). The choice of a different collection was a matter for long discussion; copyright issues remaining a major obstacle. A step towards a possible solution was nevertheless made by the proposal of the WiQA pilot task, which represents a first attempt to set the QA competitions in their natural context, i.e. the Internet. The last two categories were especially added to reduce the numbers of definition questions which may be answered very easily (such as acronyms concerning organizations, which are usually answered rendering the abbreviation in full, and people's jobdescription, which are usually found as appositions of proper names in news text).</p><p>As mentioned above, questions that require a list of items as answers, were introduced for the first time. (e.g.</p><p>Name works by Tolstoy).</p><p>Among these three categories, a number of NIL questions, i.e. questions that do not have any known answer in the target document collection, were distributed. They are important because a good QA system should identify them, instead of returning wrong answers. Three different types of temporal restriction -a temporal specification that provides important information for the retrieval of the correct answer, were associated to a certain number of F, D, L, more specifically:</p><p>-restriction by DATE (e.g. "Who was the US president in 1962?"; "Who was Berlusconi in 1994?")</p><p>-restriction by PERIOD (e.g. "How many cars were sold in Spain between 1980 and 1995?") -restriction by EVENT (e.g. "Where did Michael Milken study before enrolling in the university of Pennsylvania?")</p><p>The distribution of the questions among these categories is described in Table <ref type="table" coords="6,384.72,72.69,3.78,9.16" target="#tab_3">4</ref>. Each of the question sets was then translated into English, so that each group could choose additional 100 questions from those proposed by the others and translate them in their own languages. At the end, each source language had 200 questions, which were collected in an XML document. Unlike in the previous campaigns, the questions were not translated in all the languages due to time constraints, and the Gold Standard contained questions in multiple languages only for activated tasks. Since Indonesian, Polish and Romanian did not have a data collection of their own, the English question set was translated, so that the crosslingual subtasks INEN, PLEN and ROEN were made available. As not all questions had been previously translated, a translation of the target language question sets into the source languages was needed for crosslanguage subtasks which had at least one registered participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participants</head><p>The number of participants has constantly grown over the years [see Table <ref type="table" coords="6,372.48,236.13,3.56,9.16" target="#tab_4">5</ref>]. In fact, about ten new groups have joined the competition each year, and in 2006 a total of 30 participants was reached. The increase in the number of submitted runs corresponded to that of the participants. Of higher significance is the slight decrease registered in monolingual subtasks to the advantage of bilingual ones. This indicates that QA@CLEF is becoming increasingly crosslingual, as it was originally set out to be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>The introduction of list questions, the possibility to return multiple answers, and the requirement of supporting the answers with snippets of texts from the relevant documents made the evaluation process more difficult. Moreover, in some languages the large amount of data requiring assessment made it impossible for the judging panels to correct more than one answer per question. Therefore, only the first answers were evaluated in runs that had English and Spanish as a target. In all other cases at least the first three answers were evaluated.</p><p>Considering these issues, it was decided to follow the procedure utilised during the previous campaign. The files submitted by the participants in all tasks were manually judged by native speakers. Each language coordination group guaranteed the evaluation of at least one answer per question. If a group decided to assess more than one answer per question, the answers were assessed in the order they occurred in the submission file and the same number was applied to all questions, and all the runs assessed by the group. The exact answer (i.e. the shortest string of words which is supposed to provide the exact amount of information to answer the question) was assessed as:</p><formula xml:id="formula_0" coords="7,88.80,165.57,113.92,33.64">• R (Right) if correct; • W (Wrong) if incorrect; • X (ineXact)</formula><p>if contained less or more information than that required by the query; • U (Unsupported) if either the docid was missing or wrong, or the supporting snippet did not contain the exact answer.</p><p>Most assessorgroups managed to guarantee a second judgement of all the runs, with a good average inter assessor agreement. As far as the evaluation measures are concerned, the list questions had to be scored separately, and different groups returned a different number of answers for originally meant Factoid and Definition questions. As a consequence, we decided to provide the following measures:</p><p>• accuracy, as the main evaluation score, defined as the average of SCORE(q) over all 200 questions q; • the mean reciprocal rank (MRR) over N assessed answers per question. That is, the mean of the reciprocal of the rank of the first correct label over all questions; • the K1 measure used in earlier QA@CLEF campaigns [2]</p><p>• the Confident Weighted Score (CWS) designed for systems that give only one answer per question.</p><p>Answers are in a decreasing order of confidence and CWS rewards systems that give correct answers at the top of the ranking [2]</p><p>Although some other kinds of measures have been proposed and used in CLEF 2005, such as a more detailed analysis/breakdown of bad answers by the Portuguese group . <ref type="bibr" coords="7,318.46,404.61,10.01,9.16" target="#b4">[7]</ref>, they were not considered this year. Also, issues like providing more accurate description of what X means: too much or too little were only distinguished by the Portuguese assessors, argued for i.a. in Rocha and Santos <ref type="bibr" coords="7,302.40,427.65,10.70,9.16" target="#b1">[4]</ref>. As far as accuracy is concerned, a general improvement has been noticed, as Figure <ref type="figure" coords="8,425.76,72.69,5.04,9.16" target="#fig_1">1</ref> shows. In detail, Best accuracy in the monolingual task improved by 6.9%, passing from last year's 64.5% to 68.95%, while Best accuracy in crosslanguage tasks passed from 39.5% to 49.47%, recording an increment of 25.2%. As far as average performances are concerned, a slight decrease has been recorded in the monolingual tasks, which went from 29.36% to 27.94%. This probably was due to the number of newcomers which tested their systems for the first time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>41</head><p>As a general remark, best performances has been quite stable, with most languages registering similar or better scores than last campaigns (see Figure <ref type="figure" coords="8,227.04,153.33,3.56,9.16" target="#fig_2">2</ref>). Although also in 2006 campaign self confidence score was not returned by all systems, data about the confidence were plentiful, and allowed to consider the additional evaluation measures, i.e. K1, CWS and MMR. Generally speaking, systems with high accuracy scored accordingly well also with these measures, implying that best systems provide high self confidence, as Table <ref type="table" coords="8,260.40,382.05,5.04,9.16" target="#tab_5">7</ref> shows. Here below a more detailed analyses of the results in each language follows, giving more specific information on the performances of systems in the single subtasks and on the different types of questions, providing the relevant statistics and comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Bulgarian as Target</head><p>At CLEF 2006 Bulgarian was addressed as a target language for the second time. This year there was no change in the number of the participants again two groups took part in the monolingual evaluation task with Bulgarian as a target language: BTB at Linguistic Modelling Laboratory, Sofia and The Joint Research Centre (JRC), Ispra. Three runs altogether were submitted -one by the first group and two by the second group with insignificant difference between them. The 2006 results are presented in Table <ref type="table" coords="9,337.68,143.97,5.04,9.16" target="#tab_6">8</ref> below. First, the correct answers in numbers and percentage are given (Right) per run. Then the wrong (W), inexact (X) and unsupported answers (U) are shown in numbers. Further, the number of the factoids (F), temporally restricted questions (T), definitions (D) and list questions (L) are given. Also, the percentage of the correct answers per each type is registered in Table <ref type="table" coords="9,70.80,190.05,3.78,9.16" target="#tab_6">8</ref>. NIL questions are presented as the number of correctly and wrongly returned answers by the systems with NIL marking. It is obvious that the systems returned NIL answer also when they could not detect a possibly existing answer in the corpus themselves. In our opinion, the present NIL marking might be divided into two labels: NIL = no answer in the corpus is existing and CANNOT = the system itself cannot find an answer. In this way the evaluation would be more realistic. Main reciprocal rank score is provided in the last column of the table.</p><p>As it can be seen, this year the first system performs better. The only outperforming results in comparison with the last year are the following: the improvement of the definition type answers (from 42 % to 55.81 %) and the raise of the main reciprocal rank score (from 0.160 to 0.2660).</p><p>The introduction of the snippet support proved out to be a good idea. There was only 1 unsupported answer in all three runs. The interannotator agreement was very high due to two reasons: first, the number of the answered questions was not very high, and second, there were strict guidelines for the interpretation of the answers, based on our last year experience.</p><p>In spite of the somewhat controversial results from the participating systems this year, there is a lot of potential in the task of Bulgarian as a target language in several aspects: investing in the development of the present systems and creating new systems. We hope that Bulgarian will become even more attractive as an EU language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Dutch as Target</head><p>This year three teams that took part in the CLEF QA track used Dutch as the target language: the University of Amsterdam, the University of Groningen and the University of Roma -3, with six runs submitted in total: three Dutch monolingual and three crosslingual (English to Dutch). All runs were assessed by two assessors, with the overall interassessor agreement 0.96. For creating the gold standard for Dutch, the assessments were automatically reconciled in favour of more lenient assessments: for example, in case the same answer was assessed as W (incorrect) by one assessor and as X (inexact) by another, the X judgement was included in the gold standard. The results of the evaluation of the six runs are provided in Tables <ref type="table" coords="9,398.64,713.73,5.04,9.16" target="#tab_7">9</ref> and<ref type="table" coords="9,423.12,713.73,8.24,9.16" target="#tab_8">10</ref>. The columns labelled Right, W, X and U give the results for factoid, definition and temporally restricted questions. An interesting thing to notice about this year's task is that the overall scores of the systems are lower, compared to the last year's numbers (44% and 50% of correct answers to factoid questions last year). This year's questions were created by annotators who were explicitly instructed to think of "harder" questions, that is, involving paraphrases and some limited general knowledge reasoning. It would be interesting to compare the performance of this year's systems on last year's questions to the previous results of the campaign. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">English as Target</head><p>Cr eation of Questions. The question for creation of the questions was very similar to last year and is now a well understood procedure. This year it was required to store supporting snippets for the reference answers but this was not difficult and is well worth the trouble. As previously, we were requested to set Temporarily Restricted questions and to distribute these in a prescribed way over the various Factoid question types (PERSON, LOCATION etc). We achieved our quotas but this was extremely difficult to accomplish and we do not feel the time spent is worthwhile as the addition of temporal restrictions more than doubles the time taken to generate the questions. On the other hand, as the restrictions are frequently synthetic in nature, our knowledge of how to solve these important questions does not necessarily advance from year to year. Searching for Definition questions (or indeed any questions beyond Factoids) is always very interesting work but the method of evaluation was not clarified this year. So, while the topics we selected do follow the guidelines, we were not required to (or indeed able to) state at generation time exactly what a complete and correct answer should look like. In consequence we can not conclude much from an analysis of the answers returned by systems to such questions.</p><p>Summar y Statistics for all the Runs. Overall, thirteen crosslingual runs with English as a target were submitted. The results are shown in. Ten groups participated in seven languages, French, German, Indonesian, Italian, Romanian, Polish and Spanish. There were three groups for French, two for Spanish and one for all the rest.</p><p>Results Analysis. There were three main types of question this year, Factoids, Definitions and Lists and we consider the results over these types as well as considering the best scores overall. The most indicative measure overall is a simple count of correct answers and this is what we have used. For the 150 Factoids the best system was utjp061plen (PolishEnglish) with 132 correct. This is by far the best and is vastly higher than last year. By comparison, the top five are utjp061plen (132), lire062fren (39), lire061fren (33), dltg061fren (32) and aliv061esen (29). The other results are not greatly different from last year. The top result of 132/150 amounts to 88%. The next best result of 39/150 is 26%.</p><p>For the 40 definitions, the picture is similar. The top five results are utjp061plen (32), aliv062esen (11), lire061fren <ref type="bibr" coords="10,118.08,717.57,15.26,9.16" target="#b7">(10)</ref>, aliv061esen (9), lire062fren (9) and dfki061deen <ref type="bibr" coords="10,338.64,717.57,10.70,9.16" target="#b5">(8)</ref>. Again, the top result is far higher than the rest amounting to 32/40 i.e. 80% with the next being 11/40 i.e. 28%. For each of the ten list questions, a system could return up to ten candidate answers. Considering both a simple count of correct answers and the P@N score achieved, the top five results by count are utjp061plen (18, 0.65), uaic061roen (10, 0.11), lire061fren (9, 0.09), irst061iten (8, 0.16), lire062fren (8, 0.08) and dfki061deen (6, 0.2). By either score, utjp061plen is the best while the ordering of the rest differs for the P@N score: utjp061plen (18, 0.65), dfki061deen (6, 0.2) irst061iten (8, 0.16), uaic061roen (10, 0.11), lire061fren (9, 0.09) and lire062fren (8, 0.08).</p><p>Assessment Pr ocedur e. This approach to assessment was broadly similar to that of last year. However, as the format of the runs had changed, we decided not to use the NIST software but to work with the bare text files instead. It had been intended to doublejudge all the questions but unexpectedly and at the last moment this proved not to be possible due to the absence of an assessor. There were 200 questions in all. One assessor judged all answers to questions 1100 while the other two judged all answers to questions 101200.</p><p>There were considerable practical problems with the assessment of runs this year. Firstly, several runs used invalid run tags. Secondly two of the runs were answering the questions in a completely different order! Thirdly, one question in these two runs was different from the question being answered by the other systems in that position. Fourthly, one run had the fields in the wrong order. Fifthly one run used NULL instead of NIL while another run used nil. Luckily we spotted problems 2 and 3 and were able to correct them and indeed all the others but this was extremely time consuming and difficult.</p><p>As in all previous years the runs were anonymised by a third party so none of the assessors knew either the origin of a run or the original source language.</p><p>This year it had been decided to allow multiple answers to Factoid and Definition questions (up to ten per question). The rationale for this was never quite clear since the whole objective of Question Answering (as against Information Retrieval) is to return only the right answer. Even in cases where there are genuinely several right answers (a rare situation in our carefully designed question sets) a system should still return a correct answer in the first place. For this reason and due to our limited time and resources, we only judged the first answer returned to Factoid and Definition questions. For List questions, all candidate answers were judged, as is normal at TREC.</p><p>For the questions double judged, we measured the agreement level. There were 149 differences over thirteen runs of 100 questions. This amounts to 149/1300 i.e. 11% disagreement or 89% agreement. The overall figure for last year was 93%.</p><p>Concerning the judgement process itself, Factoids and Lists did not present a problem as we were very familiar with them. On the other hand Definitions were in the same state as last year in that they had been included in the task without a suitable evaluation prodedure having been defined. In consequence we used the same approach as last year: If an answer contained information relevant to the question and also contained no irrelevant information, it was judged R if supported, and U otherwise. If both relevant and irrelevant information was present it was judged X. Finally, if no relevant information was present, the answer was judged W.</p><p>Comment and Conclusions. The number of runs judged (13) was similar to last year (12). However, three source languages were introduced: Indonesian, Polish and Romanian. The results themselves were also broadly similar with the exception of the Polish run which was vastly higher on all question types.</p><p>Definition questions remained in the same unspecified state as previously. This means that we have not been successful in stretching the boundaries of question answering beyond Factoids which are now very well understood. This is a great pity as the extraction of useful 'definition type' information on a topic is a very useful task for groups to study but it is one which needs to be carefully quantified. The introduction of snippets was very helpful at question generation time and also invaluable for judging the answers. Snippets are a great step forward for CLEF and are the most significant development for the QA Track this year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">French as Target</head><p>This year (as last year) seven groups took part in evaluation tasks using French as target language: four French groups: Laboratoire d'Informatique d'Avignon (LIA), CEAList, Université de Nantes (LINA) and Synapse Développement; one Spanish group: Universitat Politécnica de Valencia; one Japanese group; and one American group: LCC.</p><p>In total, 15 runs have been returned by the participants: eight monolingual runs (FRtoFR) and seven bilingual runs (6 ENtoFR, 1 PTtoFR). It appears that the number of participants for the French task is the same that last year but it's the first time there are nonEuropean participants. This shows there is a new major interest for the French as target language.</p><p>Two groups submitted four runs, two other groups submitted two runs and three groups submitted only one run. This year and for the first time, the participants could return up to 10 answers per question. A major part of participants returned only one answer per question, only three groups returned more than one answer per question. For these three groups, ELDA (Evaluation and Language resources Distribution Agency) assessed the three first answers for Factual, Definition and Temporally restricted questions. Table <ref type="table" coords="13,96.24,72.69,10.08,9.16" target="#tab_10">12</ref> shows the results of the assessment of each run for each participant and for the two tasks. Figure <ref type="figure" coords="13,99.60,321.81,5.04,9.16" target="#fig_3">3</ref> shows the best and the average scores for systems using French as target in the last three CLEF QA campaigns.</p><p>For both monolingual and bilingual tasks, the best results were obtained by a French group, Synapse Développement. Another French group, LIA, reached the 2nd position for the two tasks.</p><p>For the monolingual task, the systems returned between 27 and 129 correct answers in 1 st rank.</p><p>For the bilingual task, the systems returned between 19 and 86 correct answers.</p><p>The test set was composed of 190 Factual (F), Definition (D) and Temporally restricted (T) questions, and 10 List questions.</p><p>The accuracy has been calculated over all the first answers of F, D, T questions and also the Confidence Weighted Score (CWS), the Mean Reciprocal Rank score (MRR) and the K1 measure.</p><p>For the List questions, the P@N has been calculated.</p><p>For the monolingual task, the best system returned 67.89 % of correct answers (overall accuracy in 1st rank). We can observe this system obtained better results for definition questions (83.33 %) than for Factoid questions (63.51 %). The LIA' system, which reached the second position in this task, returned 46.32 % of correct answers (overall accuracy in 1st rank). We can also observe the difference between the results for the Factual questions and the results for the Definition questions: 37.84 % of correct answers for the Factual and 76.19 % for the Definition questions.</p><p>For the bilingual task, the best system obtained 45.26 % of correct answers as opposed to 34.74 % of correct answers for the LIA' system. We can remark that the best system for the bilingual task (ENtoFR) obtained worse results than the second system for the monolingual task.</p><p>This year, before the assessment, the French assessors determined some rules to face up to problems encountered the last year. Concerning Temporally restricted questions for example, to assess an answer as "Correct", the date, the period or the event had to be present in the document returned by the systems. They decided also to check separately, at the end of the assessment, some questions which seemed difficult to them, to make sure that each answer had received the same "treatment" during the evaluation.</p><p>The main problem encountered this year, was related to the assessment of the List questions. This was a new kind of questions this year and participants followed different ways to answer to these questions. Some systems returned a list of answers in a same line; others returned an answer per line. ELDA evaluated these answers according to each run (if a line contained one of correct answers or all the correct answers, these answers had been assessed as "Correct".</p><p>The best system obtained 5 correct answers out of 10 List questions in total. We can observe that the results for the List questions were not very relevant because of not much questions and not much rules.</p><p>In conclusion, this year, a system obtained "excellent" results. Synapse Développement obtained 129 correct answers out of 200 (as opposed to 128 last year). This system is the best system for the French language. This year, it's again the dominant system.</p><p>In addition, we can observe the same great interest in Question Answering from the European (and now non European) research community for the tasks using French as target language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">German as Target</head><p>Three research groups submitted runs for evaluation in the track having German as target language: The German Research Center for Artificial Intelligence (DFKI), FernUniversität Hagen (FUHA) and The Institute for Natural Language Processing in Stuttgart (IMS). All of them provided system runs for the monolingual scenario and just one group (DFKI) submitted runs for the crosslanguage EnglishGerman scenario. Two assessors with different profiles conducted the evaluation: a native German speaker with little knowledge of QA systems and a researcher with advanced knowledge of QA systems and a good command of German. Compared to the previous editions of the evaluation forum, this year an increase in the performance of an aggregated virtual system for both monolingual and crosslanguage tasks was registered, as well as for the crosslanguage best system's result (Figure <ref type="figure" coords="14,103.20,318.93,3.56,9.16">4</ref>). Given the increased complexity of the task (no question type provided, supporting snippets required) and of questions (definition and list), the stability of the best monolingual results can be considered also a gain in terms of performance. Except for FUHA, the other two groups provided more than one possible answer per question, of which only the first three were manually evaluated. In order to come up with a measure of performance for systems providing several answers per question, Mean Reciprocal Rank (MRR) over right answers has been considered for this purpose. Two things can be concluded from the answer distribution of Table <ref type="table" coords="15,349.20,335.97,8.59,9.16" target="#tab_3">14</ref>: first, there are a fair number of inexact and unsupported answers that show performance could be improved with a better answer extraction; second, the fair number of right answers among the second and third ranked positions indicate that there is still place for improvements with a more focused answer selection. The details of systems' results can be seen in Table <ref type="table" coords="15,283.68,594.69,8.40,9.16" target="#tab_12">15</ref>, in which the performance measures has been computed only for the first ranked answers to each question, except for the list questions. Interesting to observe is that none of the systems managed to correctly respond any temporal question. Table <ref type="table" coords="15,96.96,629.01,10.08,9.16" target="#tab_13">16</ref> describes the interrater disagreement on the assessment of answers in terms of question and answer disagreement. Question disagreement reflects the number of questions on which the assessors delivered different judgments and answer disagreement is a figure of the total number of answers disagreed on. Along the total figures for both types of disagreement, a breakdown at the question type level (Factoid, Definition, List) and at the assessment value level (ineXact, Unsupported, Wrong/Right) is listed. The answer disagreements of type Wrong/Right are trivial errors during the assessment process when a right answers was considered wrong by mistake and the other way around, while those of type X or U reflect different judgments whereby an assessor considered an answer inexact or unsupported while the other marked it as right or wrong. For the first time a crosslanguage task with Italian as target was chosen to test a participating system. The best performance in the monolingual task was obtained by the UPV, which achieved an accuracy of 28.19%. Almost the same result was recorded last year (see Figure <ref type="figure" coords="16,304.32,650.61,3.56,9.16">4</ref>). The average accuracy in the monolingual task was 26.41%, which is an improvement of more than 2% with respect to last year's results. The accuracy in the bilingual task was 17.02%, achieved by both submitted runs.</p><p>During the years the overall accuracy has steadily decreased starting from a 25.17% in the 2004, we reached a 24.08% in the 2005 and 22.06% this year. This could be partly due to newcomers -who usually get lower scores -and first experiments with bilingual tasks. From the results shown in Table <ref type="table" coords="17,202.80,311.25,8.40,9.16" target="#tab_14">17</ref>, it can be seen that the Universidad Politécnica de Valencia (UPV) submitted two runs in the monolingual task and achieved the best overall performance. The accuracy over Definition and Factoid questions ranged from 26.83% to 29.27%. ITCIrst submitted one run, and achieved much better accuracy over Factoid questions (25.00%) than over Definition questions (17.07%). As previously mentioned, the Università La Sapienza di Roma submitted two runs in the crosslanguage ENIT tasks, performing much better in the Definition questions (24.39%) than in the Factoid questions (15.28%).</p><p>As far as List questions are concerned, all participating systems performed rather poorly, with a P@N ranging from 0.08 to 0.17. This implies that a more indepth research on these questions and the measures for their evaluation is still needed. Temporally restricted questions represented a challenge for the systems, which generally achieved a lower than average accuracy in this subcategory. The Universidad Politécnica de Valencia achieved the best performance of 23.68% (see Table <ref type="table" coords="17,158.88,568.29,7.98,9.16" target="#tab_15">18</ref>).</p><p>The evaluation process did not presented particular problems, although it was more demanding than usual because of the necessity to check the supporting text snippet. All runs were anyway assessed by two judges. The interassessor agreement was averagely 90,14 %, most disagreement being between U and X. A couple of cases of disagreement between R and W were due just to trivial mistakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Portuguese as Target</head><p>This year five research groups took part in tasks with Portuguese as target language, submitting ten runs: seven in the monolingual task, two with English as source, and one with Spanish. Two new groups joined for Portuguese: University of Porto, and Brazilian NILC, while LCC participated with an EnglishPortuguese run only. Universidade de Évora did not participate this year. difference -all answers, assessing as right (or partially right) if any answer, irrespective of position, was right (or partially right). We have also distinguished inexact answers (X) between too little and too much information, respectively coded as X and X+. Just like last year, Priberam achieved the best results by a clear margin. Also, their SpanishPortuguese run, prib061espt, despite using a different (closely related) language as source, managed to achieve the second best result. On the other hand, overall results for both Priberam and Esfinge show but a small improvement compared to 2005. It remains to be seen whether this year's questions displayed a higher difficulty or whether the systems themselves were subject to few changes.</p><p>Table <ref type="table" coords="18,103.68,182.61,8.59,9.16" target="#tab_7">19</ref>: Results of the r uns with Por tuguese as tar get for nonlist questions: fir st answer s only, and all answer s (marked with *). There were only 18 NIL questions in the Por tuguese dataset. Confidence weighted scor e is computed for nonlist questions only</p><p>We also provide in Table <ref type="table" coords="18,185.52,462.93,10.08,9.16" target="#tab_17">20</ref> the overall accuracy considering (and evaluating) independently all different answers provided by the systems. Table <ref type="table" coords="19,97.44,72.69,10.08,9.16" target="#tab_18">21</ref> shows the results for each answer type (loc ≡ location, mea ≡ measure, or g ≡ organisation, per ≡ person, man ≡ manner, obj ≡ object, oth ≡ other, tim ≡ time). In parentheses we display the subset of temporallyrestricted questions, and we add the list questions, in order to provide the full picture. A virtual run, called combination, was included in Table <ref type="table" coords="19,302.16,400.29,10.08,9.16" target="#tab_18">21</ref> and computed as follows: if any of the participating systems found a right answer, it is considered right in the combination run. Ideally, this combination run measures the potential achievement of cooperation among all participants. However, for Portuguese this combination does not significantly outperform the best performance: Priberam alone corresponds to 92.4% of the combination run.</p><p>We have also analysed the size in words of both answers and justification snippets, as displayed in Table <ref type="table" coords="19,70.80,469.17,8.40,9.16" target="#tab_19">22</ref>. (Computations were made excluding NIL answers.) Interestingly, Priberam provided the shortest justifications. In Table <ref type="table" coords="19,107.28,719.25,8.24,9.16" target="#tab_20">23</ref>, we compare the accuracy of the systems for the 22 temporally restricted questions in the Portuguese question set with their scores for nontemporally restricted ones and their overall performance. Finally, a total of twelve questions were defined by the organization as requiring a list as proper answer. The fact that the systems had to find out whether multiple or single answers were expected was a new feature this year and was not conveniently handled by most systems. In fact, two systems (Priberam and NILC) completely ignored this and provided a single answer to every question, while two other systems, although attempting to deal with list questions, seemed to fail in appropriately identifying them: RAPOSA (UPorto) provided multiple answers only to nonlist questions, and Esfinge produced 12 answers for ten questions. In fact, only LCC presented multiple answers systematically, yielding an average of 7.32 answers per question, while no other group exceeded 1.1.</p><p>We believe further study should be devoted to the list questions for the next years, since a distinction between closed lists and open lists, although acknowledged, was not properly taken into consideration. We have thus chosen to handle all these questions alike, assigning them the following accuracy score: number of correct answers (where X counted as ½) divided by the sum of the number of existing answers in the collections and the number of wrong distinct answers provided by the system. The results are displayed in Table <ref type="table" coords="20,445.20,443.49,8.24,9.16" target="#tab_21">24</ref>.</p><p>For the case of closed lists (where "one" answer might bring all answers, such as "Lituânia, Estónia e Letónia"), we still counted the number of answers individually (3). The participation at the Spanish as Target subtask is still growing. Nine groups, two more than the last year, submitted 17 runs: 12 monolingual, 3 from English, 1from French and 1 from Portuguese. Table <ref type="table" coords="20,459.12,727.65,10.08,9.16" target="#tab_22">25</ref> and Table <ref type="table" coords="20,513.84,727.65,10.08,9.16" target="#tab_23">26</ref> show the summary of systems results for monolingual and crosslingual respectively. The number of Right, Wrong (W), Inexact (X) and Unsupported (U) answers. Tables show also the accuracy (in percentage) of factoids (F), factoids with temporal restriction (T), definitions (D) and list questions (L). Best values are marked in bold face. Best performing systems have improved their performance (as seen in Figure <ref type="figure" coords="21,459.36,95.73,3.56,9.16">5</ref>), mainly with respect to factoids. However, performance when the question has a temporal restriction didn't vary significantly. Last year, the answering of definitions with respect to persons and organizations was almost solved. In spite of the fact that this year the set of definition questions was more realistic systems have improved slightly their performance. List questions have been introduced this year so they deserve some attention regarding their evaluation. We have differentiated two types of list questions: conjunctive and disjunctive (as presented in <ref type="bibr" coords="21,437.04,400.77,10.44,9.16" target="#b0">[1]</ref>). Conjunctive list questions are asking for a set of items and they are Right if all the items are present in the answer. For example, "Nombre los tres Beatles que siguen vivos" (Name the three Beatles alive). Disjunctive list questions are asking for an undetermined number of items. For example, "Nombre luchadores de Sumo" (Name Sumo fighters).</p><p>Only the first answer of each system has been evaluated in both cases. Regarding the NIL questions, Table <ref type="table" coords="21,218.88,604.77,10.08,9.16" target="#tab_22">25</ref> and 26 show the harmonic mean (F) of precision (P) and recall (R). The best performing systems have increased again their performance (see Table <ref type="table" coords="21,374.40,616.29,8.96,9.16" target="#tab_24">27</ref>) in NIL questions. The correlation efficient r between the selfscore and the correctness of the answers has been increased in the majority of systems, although results are not good enough yet.</p><p>This year a supporting text snippet was requested. For this reason, we have evaluated the systems capability to extract the answer when the snippet contains it. The last column of Tables <ref type="table" coords="21,380.64,673.89,10.08,9.16" target="#tab_22">25</ref> and<ref type="table" coords="21,412.08,673.89,10.08,9.16" target="#tab_23">26</ref> shows the percentage of cases where the correct answer was correctly extracted. This information is very useful to diagnose if the lack of performance is due to the passage retrieval or to the answer extraction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>The QA track at CLEF 2006 has once again demonstrated the interest for Question Answering in languages other than English. In fact, both the number of participants and runs submitted has grown, following the positive trends of the previous campaign. Equally positive was the fact that, despite the loss of Finnish, two additional languages from Eastern Europe have been added, strengthening the crosslinguality of QA@CLEF. The balance between tradition and innovations -i.e the introduction of list questions and supporting text snippets has proved to be a good solution, which allows both newcomers and veterans to test their systems against adequately challenging tasks and, at the same time, to make a comparison with previous exercises. Generally speaking, the results recorded an improvement in performance, with best accuracy significantly higher than in previous campaigns both in monolingual and bilingual tasks.</p><p>As far as the organisation of the campaign is concerned, the introduction of new elements such as list questions and supporting snippets has implied a significant increase of work both in the question collection and in the evaluation phase, which was particularly demanding for language groups which had a great number of participants. A better distribution of the workload and solutions to speed up the evaluation process, also with automatic assessment of part of the submissions will be essential in next campaigns. A future perspective of QA is certainly outlined by the two pilot tasks offered in 2006i.e. AVE and WiQa, the latter in particular representing a significant step toward a more realistic scenario, where queries are carried out on the Web. For these reasons, a quick integration of these experiments into the main task is hoped for.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,88.80,475.41,435.48,9.16;3,106.80,486.93,85.12,9.16;3,88.80,499.17,87.00,9.16"><head>• 7</head><label>7</label><figDesc>Monolingual i.e. Bulgarian (BG), German (DE), Spanish (ES), French (FR), Italian (IT), Dutch (NL), and Portuguese (PT); • 17 Crosslingual.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,170.64,724.53,253.44,9.16;7,70.80,741.81,224.28,9.16"><head>Figur e 1 :</head><label>1</label><figDesc>Figur e 1: Best and average scor es in CLEF QA campaigns* *Note: the figure does not consider the run utjp061plen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,200.88,639.33,192.84,9.16"><head>Figur e 2 :</head><label>2</label><figDesc>Figur e 2: Best results in 2004, 2005 and 2006.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="13,118.80,291.76,357.31,8.29"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Best and aver age scor es for systems using French as tar get in CLEF QA campaigns</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="14,240.24,575.01,114.00,9.16"><head></head><label></label><figDesc>Figur e 4: Results evolution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,196.78,72.93,201.86,256.86"><head>Table 1 :</head><label>1</label><figDesc>Task activated in 2006</figDesc><table coords="3,196.78,95.07,201.86,234.72"><row><cell></cell><cell>TARGET LANGUAGES (corpus and answers)</cell></row><row><cell></cell><cell>BG DE EN ES FR IT NL PT</cell></row><row><cell></cell><cell>BG</cell></row><row><cell></cell><cell>DE</cell></row><row><cell>SOURCE LANGUAGES (questions)</cell><cell>EN ES FR IN IT NL PT</cell></row><row><cell></cell><cell>PL</cell></row><row><cell></cell><cell>RO</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,140.88,528.21,312.84,144.04"><head>Table 2 :</head><label>2</label><figDesc>Tasks chosen by at least 1 par ticipant in QA@CLEF campaigns.</figDesc><table coords="3,150.00,558.21,273.28,114.04"><row><cell></cell><cell>MONOLINGUAL</cell><cell>CROSSLINGUAL</cell></row><row><cell>CLEF 2003</cell><cell></cell><cell></cell></row><row><cell>CLEF 2004</cell><cell>6</cell><cell>13</cell></row><row><cell>CLEF 2005</cell><cell>8</cell><cell>15</cell></row><row><cell>CLEF 2006</cell><cell>7</cell><cell>17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,143.04,293.44,310.32,388.19"><head>Table 3 :</head><label>3</label><figDesc>Document collections used in CLEF 2006.</figDesc><table coords="4,143.04,310.06,310.32,371.57"><row><cell>TARGET LANG..</cell><cell>COLLECTION</cell><cell>PERIOD</cell><cell>SIZE</cell></row><row><cell>Bulgar ian (BG)</cell><cell>Sega</cell><cell>2002</cell><cell>120 MB (33,356 docs)</cell></row><row><cell></cell><cell>Standart</cell><cell>2002</cell><cell>93 MB (35,839 docs)</cell></row><row><cell></cell><cell>Frankfurter Rundschau</cell><cell>1994</cell><cell>320 MB (139,715 docs)</cell></row><row><cell>Ger many (DE)</cell><cell>Der Spiegel</cell><cell>1994/1995</cell><cell>63 MB (13,979 docs)</cell></row><row><cell></cell><cell>German SDA</cell><cell>1994</cell><cell>144 MB (71,677 docs)</cell></row><row><cell></cell><cell>German SDA</cell><cell>1995</cell><cell>141 MB (69,438 docs)</cell></row><row><cell>English (EN)</cell><cell>Los Angeles Times</cell><cell>1994</cell><cell>425 MB (113,005 docs)</cell></row><row><cell></cell><cell>Glasgow Herald</cell><cell>1995</cell><cell>154 MB (56,472 docs)</cell></row><row><cell>Spanish (ES)</cell><cell>EFE</cell><cell>1994</cell><cell>509 MB (215,738 docs)</cell></row><row><cell></cell><cell>EFE</cell><cell>1995</cell><cell>577 MB (238,307 docs)</cell></row><row><cell></cell><cell>Le Monde</cell><cell>1994</cell><cell>157 MB (44,013 docs)</cell></row><row><cell>Fr ench (FR)</cell><cell>Le Monde</cell><cell>1995</cell><cell>156 MB (47,646 docs)</cell></row><row><cell></cell><cell>French SDA</cell><cell>1994</cell><cell>86 MB (43,178 docs)</cell></row><row><cell></cell><cell>French SDA</cell><cell>1995</cell><cell>88 MB (42,615 docs)</cell></row><row><cell>Italian (IT)</cell><cell>La Stampa</cell><cell>1994</cell><cell>193 MB (58,051 docs)</cell></row><row><cell></cell><cell>Itallian SDA</cell><cell>1994</cell><cell>85 MB (50,527 docs)</cell></row><row><cell></cell><cell>Itallian SDA</cell><cell>1995</cell><cell>85 MB (50,527 docs)</cell></row><row><cell>Dutch (NL)</cell><cell>NRC Handelsblad</cell><cell>1994/1995</cell><cell>299 MB (84,121 docs)</cell></row><row><cell></cell><cell>Algemeen Dagblad</cell><cell>1994/1995</cell><cell>241 MB (106,483 docs)</cell></row><row><cell></cell><cell>Público</cell><cell>1994</cell><cell>164 MB (51,751 docs)</cell></row><row><cell>Por tuguese (PT)</cell><cell>Público</cell><cell>1995</cell><cell>176 MB (55,070 docs)</cell></row><row><cell></cell><cell>Folha de São Paulo</cell><cell>1994</cell><cell>108 MB (51,875 docs)</cell></row><row><cell></cell><cell>Folha de São Paulo</cell><cell>1995</cell><cell>116 MB (52,038 docs)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,70.80,194.13,453.60,372.17"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="5,70.80,194.13,453.60,372.17"><row><cell></cell><cell cols="4">: Test set breakdown accor ding to question type</cell><cell></cell></row><row><cell></cell><cell>F (150)</cell><cell>D (40)</cell><cell>L (10)</cell><cell>T (40)</cell><cell>NIL (20)</cell></row><row><cell>BG</cell><cell>145</cell><cell>43</cell><cell>12</cell><cell>26</cell><cell>17</cell></row><row><cell>DE</cell><cell>152</cell><cell>38</cell><cell>10</cell><cell>39</cell><cell>20</cell></row><row><cell>EN</cell><cell>150</cell><cell>40</cell><cell>10</cell><cell>40</cell><cell>18</cell></row><row><cell>ES</cell><cell>148</cell><cell>42</cell><cell>10</cell><cell>40</cell><cell>21</cell></row><row><cell>FR</cell><cell>148</cell><cell>42</cell><cell>10</cell><cell>40</cell><cell>20</cell></row><row><cell>IT</cell><cell>147</cell><cell>41</cell><cell>12</cell><cell>38</cell><cell>20</cell></row><row><cell>NL</cell><cell>147</cell><cell>40</cell><cell>13</cell><cell>30</cell><cell>20</cell></row><row><cell>PT</cell><cell>143</cell><cell>47</cell><cell>9</cell><cell>23</cell><cell>18</cell></row><row><cell cols="6">Initially, 100 questions were selected in each of the source languages, distributed between Factoid, Definition</cell></row><row><cell>and List questions.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Factoid questions are factbased questions, asking for the name of a person, a location, the extent of something,</cell></row><row><cell cols="6">the day on which something happened, etc. The following 6 answer types for factoids were considered:</cell></row><row><cell cols="4">-PERSON (e.g. "Who was Lisa Marie Presley's father?")</cell><cell></cell><cell></cell></row><row><cell cols="4">-TIME (e.g. "What year did the Second World War finish?")</cell><cell></cell><cell></cell></row><row><cell cols="3">-LOCATION (e.g. "What is the capital of Japan?")</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">-ORGANIZATION (e.g. "What party did Hitler belong to?")</cell><cell></cell><cell></cell></row><row><cell cols="5">-MEASURE (e.g. "How many monotheistic religions are there in the world?")</cell><cell></cell></row><row><cell cols="6">-OTHER, i.e. everything else that does not fit into the other five categories (e.g. "What is the mostread</cell></row><row><cell cols="2">Italian daily newspaper?")</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Definition questions, i.e. questions like "What/Who is X?", were divided into the following categories:</cell></row><row><cell cols="6">• PERSON i.e. questions asking for the role, job, and/or important information about someone (e.g.</cell></row><row><cell cols="2">"Who is Lisa Marie Presley?");</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">• ORGANIZATION i.e. questions asking for the mission, full name, and/or important information about</cell></row><row><cell cols="5">an organization (e.g. "What is Amnesty International?" or "What is the FDA?");</cell><cell></cell></row><row><cell cols="6">• OBJECT i.e. questions asking for the description or function of objects (e.g. "What is a Swiss army</cell></row><row><cell cols="2">knife?", "What is a router?");</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">• OTHER i.e. question asking for the description of natural phenomena, technologies, legal procedures</cell></row><row><cell cols="5">etc. (e.g. "What is a tsunami?", "What is DSL?", "What is impeachment?").</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,70.80,276.93,453.28,303.88"><head>Table 5 :</head><label>5</label><figDesc>Number of participating gr oups</figDesc><table coords="6,70.80,295.41,453.28,285.40"><row><cell></cell><cell cols="3">America Europe Asia Australia</cell><cell>TOTAL</cell><cell cols="2">Registered participants</cell><cell>New groups</cell><cell cols="2">Veterans Absent veterans</cell></row><row><cell>CLEF 2003</cell><cell>3</cell><cell>5</cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CLEF 2004</cell><cell>1</cell><cell>17</cell><cell></cell><cell>18 (+125% )</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CLEF 2005</cell><cell>1</cell><cell>22</cell><cell>1</cell><cell>24(+33% )</cell><cell></cell><cell>27</cell><cell>9</cell><cell>15</cell><cell>4</cell></row><row><cell>CLEF 2006</cell><cell>4</cell><cell>24</cell><cell>2</cell><cell>30 (+25% )</cell><cell></cell><cell>36</cell><cell>10</cell><cell>20</cell><cell>4</cell></row><row><cell cols="10">For the record, the number of groups which registered for the competition but did not actually participate in it</cell></row><row><cell cols="10">was six, while four groups which took part in QA2005 did not show up in 2006. From a geographical</cell></row><row><cell cols="10">perspective, most groups came from Europe, but in 2006 there was an increase in participants from both Asia</cell></row><row><cell cols="2">and America [see Table 5].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Table 6. Number of submitted r uns</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Number of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>submitted runs</cell><cell cols="2">Monolingual</cell><cell cols="2">Crosslingual</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>#</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">CLEF 2003</cell><cell>17</cell><cell>6</cell><cell></cell><cell cols="2">11</cell><cell></cell></row><row><cell></cell><cell cols="2">CLEF 2004</cell><cell>48</cell><cell>20</cell><cell></cell><cell cols="2">28</cell><cell></cell></row><row><cell></cell><cell cols="2">CLEF 2005</cell><cell>67</cell><cell>43</cell><cell></cell><cell cols="2">24</cell><cell></cell></row><row><cell></cell><cell cols="2">CLEF 2006</cell><cell>77</cell><cell>42</cell><cell></cell><cell cols="2">35</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,105.84,182.61,362.16,149.12"><head>Table 7 :</head><label>7</label><figDesc>Best accur acy scor es compar ed with K1, MRR, and CWS</figDesc><table coords="8,105.84,200.56,362.16,131.17"><row><cell>FILE NAME</cell><cell>OVERALL</cell><cell>K1</cell><cell>MRR</cell><cell>CWS</cell></row><row><cell></cell><cell>ACCURACY</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BEST</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>syna061fr fr .txt</cell><cell>68.95%</cell><cell>0.2832</cell><cell>0.6895</cell><cell>0.56724</cell></row><row><cell>inao061eses.txt</cell><cell>52.63%</cell><cell>0.0716</cell><cell>0.5263</cell><cell>0.43387</cell></row><row><cell>ulia061frfr.txt</cell><cell>46.32%</cell><cell>0.0684</cell><cell>0.4632</cell><cell>0.46075</cell></row><row><cell>ulia062frfr.txt</cell><cell>45.79%</cell><cell>0.0579</cell><cell>0.4579</cell><cell>0.45546</cell></row><row><cell>vein061eses.txt</cell><cell>42.11%</cell><cell>0.0657</cell><cell>0.4211</cell><cell>0.33582</cell></row><row><cell>alia061eses.txt</cell><cell>37.89%</cell><cell>0.1232</cell><cell>0.3763</cell><cell>0.23630</cell></row><row><cell>upv_061eses.txt</cell><cell>36.84%</cell><cell>0.0014</cell><cell>0.3684</cell><cell>0.22530</cell></row><row><cell>ulia061enfr .txt</cell><cell>35.26%</cell><cell>0.1684</cell><cell>0.3526</cell><cell>0.34017</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,70.80,259.17,453.60,195.88"><head>Table 8 :</head><label>8</label><figDesc>However, its overall accuracy is slightly worse with respect to the 2005 best accuracy result, achieved then by IRST, Trento. Now it is 26.60 %, while in 2005 it was 27.50 %. However, the BTB 2005 year result was significantly improved. Both systems 'crashed' at temporally restricted questions with no single match (see the empty slots in the table). It is a step back from 2005, when both systems had some hits, best of which scored 17.65 %. List questions are also very poorly answered (1 correct answer per run). Results at the Bulgar ian as tar get, monolingual</figDesc><table coords="9,76.56,371.01,440.64,84.04"><row><cell></cell><cell></cell><cell>Right</cell><cell>W</cell><cell>X U</cell><cell>% F</cell><cell>% T</cell><cell>% D</cell><cell>% L</cell><cell cols="2">NIL [16]</cell><cell></cell></row><row><cell>Run</cell><cell>#</cell><cell>%</cell><cell>#</cell><cell># #</cell><cell>[119]</cell><cell>[26 ]</cell><cell>[43]</cell><cell>[12]</cell><cell>right</cell><cell>wrong</cell><cell>r</cell></row><row><cell>btb061</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">50 26.60</cell><cell cols="2">132 4 1</cell><cell>17.93</cell><cell></cell><cell>55.81</cell><cell>0.0833</cell><cell>11</cell><cell>120</cell><cell>0.2660</cell></row><row><cell>jrc061</cell><cell cols="2">22 11.70</cell><cell cols="2">162 4 0</cell><cell>6.90</cell><cell></cell><cell>27.91</cell><cell>0.0833</cell><cell>12</cell><cell>155</cell><cell>0.1170</cell></row><row><cell>jrc062</cell><cell cols="2">22 11.70</cell><cell cols="2">160 6 0</cell><cell>6.90</cell><cell></cell><cell>27.91</cell><cell>0.0833</cell><cell>13</cell><cell>154</cell><cell>0.1170</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="10,76.32,72.93,414.24,90.76"><head>Table 9 :</head><label>9</label><figDesc>Results at the Dutch as tar get, monolingual</figDesc><table coords="10,76.32,98.85,414.24,64.84"><row><cell></cell><cell></cell><cell>Right</cell><cell cols="3">W X U % F % T % D P@N (lists) Accur acy NIL</cell><cell>MRR</cell></row><row><cell>Run</cell><cell>#</cell><cell>%</cell><cell># # # [146] [0] [40]</cell><cell>[13]</cell><cell>[10]</cell><cell>[187]</cell></row><row><cell cols="4">Gron061nlnl 58 31.02 115 11 3 27.40 0.00 45.00</cell><cell>23.08</cell><cell>0</cell><cell>0.3460</cell></row><row><cell cols="4">Isla061nlnl 40 21.39 141 4 2 21.23 0.00 22.50</cell><cell>0.00</cell><cell>0.1346</cell><cell>0.2341</cell></row><row><cell cols="4">Isla062nlnl 41 21.93 139 4 3 21.92 0.00 22.50</cell><cell>0.00</cell><cell>0.1346</cell><cell>0.2357</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="10,74.40,253.65,416.16,90.52"><head>Table 10 :</head><label>10</label><figDesc>Results at the Dutch as tar get, cr osslingual (English to Dutch)</figDesc><table coords="10,74.40,279.57,416.16,64.60"><row><cell></cell><cell></cell><cell>Right</cell><cell cols="3">W X U % F % T % D P@N (lists) Accur acy NIL</cell><cell>MRR</cell></row><row><cell>Run</cell><cell>#</cell><cell>%</cell><cell># # # [146] [0] [40]</cell><cell>[13]</cell><cell>[10]</cell><cell>[187]</cell></row><row><cell cols="4">Gron061ennl 38 20.32 139 7 3 18.37 0.00 28.21</cell><cell>6.15</cell><cell>0.1481</cell><cell>0.2239</cell></row><row><cell cols="4">Roma061ennl 25 13.37 150 6 3 11.56 0.00 20.51</cell><cell>17.95</cell><cell>0.0769</cell><cell>0.1430</cell></row><row><cell cols="4">Roma062ennl 25 13.37 149 7 3 11.56 0.00 20.51</cell><cell>15.38</cell><cell>0.0769</cell><cell>0.1529</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="11,120.72,72.93,353.36,236.44"><head>Table 11 :</head><label>11</label><figDesc>Results of English r uns</figDesc><table coords="11,120.72,97.41,353.36,211.96"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>OVERALL</cell></row><row><cell></cell><cell cols="2">Right W</cell><cell>X</cell><cell>U</cell><cell cols="2">% F % D P@N for L</cell><cell>ACCURACY</cell></row><row><cell>Run</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell cols="2"># [150] [40]</cell><cell>[10]</cell><cell>%</cell></row><row><cell>aliv061esen</cell><cell>38</cell><cell>142</cell><cell>4</cell><cell cols="2">6 19.33 22.50</cell><cell>0.0411</cell><cell>20.00</cell></row><row><cell>aliv062esen</cell><cell>29</cell><cell>156</cell><cell>3</cell><cell cols="2">2 12.00 27.50</cell><cell>0.0200</cell><cell>15.26</cell></row><row><cell>aske061esen</cell><cell>10</cell><cell>134</cell><cell>11</cell><cell cols="2">34 6.67 0.00</cell><cell>0</cell><cell>5.26</cell></row><row><cell>aske061fren</cell><cell>7</cell><cell>135</cell><cell>10</cell><cell cols="2">37 3.33 5.00</cell><cell>0.0100</cell><cell>3.68</cell></row><row><cell>dfki061deen</cell><cell>34</cell><cell>147</cell><cell>9</cell><cell cols="2">0 17.33 20.00</cell><cell>0.2000</cell><cell>17.89</cell></row><row><cell>dltg061fren</cell><cell>36</cell><cell>138</cell><cell>14</cell><cell cols="2">2 21.33 10.00</cell><cell>0.2000</cell><cell>18.95</cell></row><row><cell>irst061iten</cell><cell>24</cell><cell>152</cell><cell>3</cell><cell cols="2">11 16.00 0.00</cell><cell>0.1600</cell><cell>12.63</cell></row><row><cell>lire061fren</cell><cell>43</cell><cell>138</cell><cell>2</cell><cell cols="2">7 22.00 25.00</cell><cell>0.0900</cell><cell>22.63</cell></row><row><cell>lire062fren</cell><cell>48</cell><cell>130</cell><cell>2</cell><cell cols="2">10 26.00 22.50</cell><cell>0.0800</cell><cell>25.26</cell></row><row><cell>uaic061roen</cell><cell>25</cell><cell>150</cell><cell>7</cell><cell cols="2">8 15.33 5.00</cell><cell>0.1131</cell><cell>13.16</cell></row><row><cell>uaic062roen</cell><cell>18</cell><cell>171</cell><cell>1</cell><cell cols="2">0 12.00 0.00</cell><cell>0.0800</cell><cell>9.47</cell></row><row><cell>uind061inen</cell><cell>14</cell><cell>159</cell><cell>4</cell><cell cols="2">13 9.33 0.00</cell><cell>0</cell><cell>7.37</cell></row><row><cell>(utjp061plen</cell><cell>164</cell><cell>14</cell><cell>5</cell><cell cols="2">7 88.00 80.00</cell><cell>0.6500</cell><cell>86.32)* 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="12,72.96,518.37,446.88,234.56"><head>Table 12 :</head><label>12</label><figDesc>Results of the monolingual and bilingual Fr ench r uns.</figDesc><table coords="12,72.96,538.35,446.88,214.58"><row><cell>Id Par ticipant</cell><cell>Assessed Answer s (#)</cell><cell>Right answer s (#)</cell><cell>Wr ong answer s (#)</cell><cell>ineXact answer s (#)</cell><cell>U answer s (#)</cell><cell>Over all Accur acy (% )</cell><cell>Accur acy over F (% )</cell><cell>Accur acy over D (% )</cell><cell>MRR (F, D, T)</cell><cell>CWS (F, D, T)</cell><cell>K1 Measur e</cell><cell>P@N (L)</cell></row><row><cell>aske061frfr</cell><cell>635</cell><cell>27</cell><cell>138</cell><cell>12</cell><cell>12</cell><cell>14.21</cell><cell>16.89</cell><cell cols="3">4.76 0.1974 0.14211</cell><cell></cell><cell>0.0900</cell></row><row><cell>lcea061frfr</cell><cell>589</cell><cell>30</cell><cell>151</cell><cell>6</cell><cell>3</cell><cell>15.79</cell><cell>10.14</cell><cell cols="3">35.71 0.1907 0.15789</cell><cell></cell><cell>0.1633</cell></row><row><cell>lina061frfr</cell><cell>207</cell><cell>56</cell><cell>114</cell><cell>18</cell><cell>2</cell><cell>29.47</cell><cell>27.70</cell><cell cols="5">35.71 0.2947 0.25517 0.3777 0.3651</cell></row><row><cell cols="2">syna061fr fr 200</cell><cell>129</cell><cell>50</cell><cell>9</cell><cell>2</cell><cell>67.89</cell><cell>63.51</cell><cell cols="5">83.33 0.6789 0.55685 0.2729 0.5000</cell></row><row><cell>ulia061frfr</cell><cell>200</cell><cell>88</cell><cell>93</cell><cell>7</cell><cell>2</cell><cell>46.32</cell><cell>37.84</cell><cell cols="5">76.19 0.4632 0.46075 0.0684 0.5000</cell></row><row><cell>ulia062frfr</cell><cell>200</cell><cell>86</cell><cell>89</cell><cell>9</cell><cell>6</cell><cell>45.26</cell><cell>36.49</cell><cell cols="2">76.16 0.4501</cell><cell cols="3">0.45016 0.0474 0.2000</cell></row><row><cell>upv061frfr</cell><cell>200</cell><cell>60</cell><cell>119</cell><cell>10</cell><cell>1</cell><cell>31.58</cell><cell>31.08</cell><cell cols="5">6 33.33 0.3158 0.16389 0.0047 0.3000</cell></row><row><cell>upv062frfr</cell><cell>200</cell><cell>47</cell><cell>124</cell><cell>18</cell><cell>1</cell><cell>24.74</cell><cell>26.35</cell><cell cols="5">19.05 0.2474 0.10883 0.0931 0.2000</cell></row><row><cell cols="2">aske061enfr 640</cell><cell>19</cell><cell>157</cell><cell>6</cell><cell>8</cell><cell>10.00</cell><cell>12.16</cell><cell cols="5">2.38 0.1445 0.01662 0.2797 0.0633</cell></row><row><cell>lcc061enfr</cell><cell>578</cell><cell>40</cell><cell>125</cell><cell>23</cell><cell>2</cell><cell>21.05</cell><cell>25.00</cell><cell cols="2">7.14 0.2623</cell><cell></cell><cell></cell><cell>0.3967</cell></row><row><cell>syna061enf</cell><cell>200</cell><cell>86</cell><cell>97</cell><cell>6</cell><cell>1</cell><cell>45.26</cell><cell>37.16</cell><cell cols="3">0.04856 73.81 0.4526 0.45263</cell><cell>0.1816</cell><cell>0.2000</cell></row><row><cell cols="2">r syna062enfr 200</cell><cell>63</cell><cell>120</cell><cell>6</cell><cell>1</cell><cell>33.16</cell><cell>25.68</cell><cell cols="3">59.52 0.3316 0.33158</cell><cell></cell><cell>0.1000</cell></row><row><cell>ulia061enfr</cell><cell>200</cell><cell>66</cell><cell>114</cell><cell>7</cell><cell>3</cell><cell>34.74</cell><cell>26.35</cell><cell cols="2">64.29 0.347</cell><cell>0.33478</cell><cell></cell><cell>0</cell></row><row><cell>ulia062enfr</cell><cell>200</cell><cell>66</cell><cell>111</cell><cell>9</cell><cell>4</cell><cell>34.74</cell><cell>26.35</cell><cell cols="5">4 64.29 0.3474 0.3474 0.1789 0.1000 0.1789</cell></row><row><cell cols="2">syna061ptfr 200</cell><cell>94</cell><cell>90</cell><cell>4</cell><cell>2</cell><cell>49.47</cell><cell>41.50</cell><cell cols="3">76.74 0.4947 0.49474</cell><cell></cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="14,122.16,673.65,339.44,75.16"><head>Table 13 :</head><label>13</label><figDesc>Best and Aggregated Mono; Best and Aggregated Cr ossTable13resumes the distribution of the right, inexact and unsupported answers over the first three ranked positions as delivered by the systems, as well as the accuracy and MRR for each of the runs.</figDesc><table coords="14,122.16,699.57,339.44,49.24"><row><cell>Year</cell><cell>Best Mono</cell><cell cols="2">Aggr egated Mono Best Cr oss</cell><cell>Aggr egated Cr oss</cell></row><row><cell>2006</cell><cell>42.33</cell><cell>64.02</cell><cell>32.98</cell><cell>33.86</cell></row><row><cell>2005</cell><cell>43.5</cell><cell>58.5</cell><cell>23</cell><cell>28</cell></row><row><cell>2004</cell><cell>34.01</cell><cell>43.65</cell><cell>0</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="15,111.60,399.81,370.80,169.51"><head>Table 15 :</head><label>15</label><figDesc>System Per for mance -Details</figDesc><table coords="15,111.60,423.81,370.80,145.51"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>P@N</cell><cell></cell></row><row><cell>Run ID</cell><cell>Right</cell><cell cols="2">W X U % F % T % D</cell><cell>L</cell><cell></cell><cell>NIL [20]</cell><cell>CWS K1</cell></row><row><cell></cell><cell># %</cell><cell cols="3"># # # [113] [39] [37] [9]</cell><cell>F</cell><cell>P</cell><cell>R</cell></row><row><cell cols="7">dfki061dede M 80 42.32 95 6 8 37.82 0 63.64 25.93 0.35 0.28 0.45 0</cell><cell>0</cell></row><row><cell cols="7">dfki062dede M 63 33.33 114 4 8 30.13 0 48.48 33.33 0.32 0.27 0.4 0</cell><cell>0</cell></row><row><cell cols="7">fuha061dede M 61 32.27 124 0 4 31.41 0 36.36 11.11 0.23 0.13 0.95 0.3 0.18</cell></row><row><cell cols="7">fuha062dede M 64 33.86 120 1 4 32.69 0 39.39 11.11 0.24 0.14 0.95 0.32 0.19</cell></row><row><cell cols="7">ims061dede M 25 13.22 156 0 8 14.1 0 9.09 25.42 0.2 0.12 0.55 0.07 0.33</cell></row><row><cell cols="7">ims062dede M 23 12.16 158 0 8 12.82 0 9.09 26.43 0.19 0.12 0.5 0.06 0.33</cell></row><row><cell cols="7">dfki061ende C 62 32.8 117 3 6 28.21 0 56.25 10 0.31 0.21 0.6 0</cell><cell>0</cell></row><row><cell cols="3">dfki062ende C 50 26.45 130 5 3 21.79 0</cell><cell>50</cell><cell cols="3">10 0.33 0.22 0.65 0</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="16,70.80,72.93,453.36,535.00"><head>Table 16 :</head><label>16</label><figDesc>Inter Assessor Agreement/Disagreement (breakdown) Italian as Target Two groups participated in the Italian monolingual task, ITCirst and the Universidad Politécnica de Valencia (UPV); while one group, the Università La Sapienza di Roma, participated in the crosslanguage ENIT task. In total, five runs were submitted. Best and Aver age perfor mance in the Monolingual and Bilingual tasks</figDesc><table coords="16,70.80,97.65,417.60,510.28"><row><cell>Run ID</cell><cell cols="2"># Questions # Answer s</cell><cell cols="7"># QDisagreements Total F D L Total X U W/R # ADisagr eements</cell></row><row><cell>dfki061dede M</cell><cell>198</cell><cell>437</cell><cell>35</cell><cell cols="2">28 7</cell><cell>0</cell><cell>44</cell><cell>20 16</cell><cell>8</cell></row><row><cell>dfki062dede M</cell><cell>198</cell><cell>476</cell><cell>28</cell><cell cols="2">19 6</cell><cell>3</cell><cell>40</cell><cell>13 19</cell><cell>8</cell></row><row><cell>fuha061dede M</cell><cell>198</cell><cell>198</cell><cell>12</cell><cell>8</cell><cell>4</cell><cell>0</cell><cell>11</cell><cell>3 2</cell><cell>6</cell></row><row><cell>fuha062dede M</cell><cell>198</cell><cell>198</cell><cell>13</cell><cell>8</cell><cell>5</cell><cell>0</cell><cell>12</cell><cell>4 2</cell><cell>6</cell></row><row><cell>ims061dede M</cell><cell>198</cell><cell>432</cell><cell>15</cell><cell cols="2">13 0</cell><cell>2</cell><cell>30</cell><cell>13 9</cell><cell>8</cell></row><row><cell>ims062dede M</cell><cell>198</cell><cell>436</cell><cell>17</cell><cell cols="2">15 0</cell><cell>2</cell><cell>28</cell><cell>5 14</cell><cell>9</cell></row><row><cell>dfki061ende C</cell><cell>198</cell><cell>405</cell><cell>26</cell><cell cols="2">20 5</cell><cell>1</cell><cell>33</cell><cell>12 16</cell><cell>5</cell></row><row><cell>dfki062ende C</cell><cell>198</cell><cell>402</cell><cell>27</cell><cell cols="2">21 6</cell><cell>0</cell><cell>35</cell><cell>21 10</cell><cell>4</cell></row><row><cell cols="5">6.6 28,19 25 24,08 27,5 30</cell><cell>26,41</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>17,02</cell><cell></cell><cell></cell></row><row><cell>15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mono</cell><cell></cell><cell>Bilingual</cell><cell></cell><cell>Mono</cell><cell></cell><cell></cell><cell>Bilingual</cell><cell>Best</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Average</cell></row><row><cell></cell><cell>CLEF05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CLEF06</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figur e 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="17,163.44,72.93,267.92,9.16"><head>Table 17 :</head><label>17</label><figDesc>Results of the monolingual and bilingual Italian r uns</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="17,85.92,432.45,422.88,98.44"><head>Table 18 :</head><label>18</label><figDesc>Tempor ally Restr icted Questions: Right, Unsuppor ted and Wr ong Answer and Accur acy</figDesc><table coords="17,210.72,450.21,186.96,80.68"><row><cell></cell><cell>R</cell><cell cols="2">U W Accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell>%</cell></row><row><cell>ir st06itit</cell><cell>6</cell><cell>6 26</cell><cell>15.79</cell></row><row><cell>Roma061enit</cell><cell>2</cell><cell>4 32</cell><cell>5.26</cell></row><row><cell>Roma062enit</cell><cell>2</cell><cell>4 32</cell><cell>5.26</cell></row><row><cell>upv_061itit</cell><cell>8</cell><cell>0 30</cell><cell>21.05</cell></row><row><cell>upv_062itit</cell><cell>9</cell><cell>0 29</cell><cell>23.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="17,70.80,122.56,453.60,629.62"><head></head><label></label><figDesc>Table19presents the overall results concerning the 188 nonlist questions. We present values both taking into account only the first answer to each question, and -for the only system where this makes any</figDesc><table coords="17,76.56,122.56,441.33,170.29"><row><cell>Run Name</cell><cell>Right answe r s (#)</cell><cell>Wr ong answe r s (#)</cell><cell>ineXac t answe r s (#)</cell><cell>Unsupp or ted answers (#)</cell><cell>Over all Accur acy (% )</cell><cell>Accur a cy over F (% )</cell><cell>Accur acy over D (% )</cell><cell>P@N for L</cell><cell cols="2">NIL Accur acy</cell><cell>Confiden ce weighted Scor e</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Precisio n</cell><cell>Recall</cell></row><row><cell>ir st06itit</cell><cell>43</cell><cell>121</cell><cell>10</cell><cell>13</cell><cell>22.87</cell><cell>25.0</cell><cell>17.07</cell><cell>0.1528</cell><cell></cell><cell></cell><cell>0.19602</cell></row><row><cell>upv_061itit</cell><cell>53</cell><cell>124</cell><cell>6</cell><cell>5</cell><cell>28.19</cell><cell>28.47</cell><cell>26.83</cell><cell>0.0833</cell><cell></cell><cell></cell><cell>0.12330</cell></row><row><cell>upv_062itit</cell><cell>53</cell><cell>127</cell><cell>4</cell><cell>3</cell><cell>28.19</cell><cell>27.78</cell><cell>29.27</cell><cell>0.1667</cell><cell></cell><cell></cell><cell>0.13209</cell></row><row><cell cols="2">Roma061enit 32</cell><cell>141</cell><cell>4</cell><cell>11</cell><cell>17.02</cell><cell>15.28</cell><cell>24.39</cell><cell>0.1000</cell><cell></cell><cell></cell><cell>0.08433</cell></row><row><cell cols="2">Roma062enit 32</cell><cell>141</cell><cell>4</cell><cell>11</cell><cell>17.02</cell><cell>15.28</cell><cell>24.39</cell><cell>0.1500</cell><cell></cell><cell></cell><cell>0.08433</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="18,98.88,237.39,400.62,491.04"><head>Table 20 :</head><label>20</label><figDesc>Results of the r uns with Por tuguese as tar get: all answer s</figDesc><table coords="18,447.12,238.83,48.60,7.20"><row><cell>NIL Accur acy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="19,74.64,125.01,441.76,248.48"><head>Table 21 :</head><label>21</label><figDesc>Results of the assessment of the monolingual Por tuguese r uns: fir st answer s only, except for lists, for which (for this table) one cor rect member of the list made the answer to be considered corr ect</figDesc><table coords="19,74.64,158.08,422.64,215.41"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">cor r ect answer s</cell><cell></cell></row><row><cell></cell><cell cols="4">Definition (#, 48)</cell><cell></cell><cell></cell><cell cols="3">Factoid (#) (t.r .q. + list) (152)</cell><cell>Total</cell></row><row><cell></cell><cell>ob</cell><cell>or</cell><cell>ot</cell><cell>pe</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>j</cell><cell>g</cell><cell>h</cell><cell>r</cell><cell cols="2">loc mea</cell><cell>or g</cell><cell>oth</cell><cell>per</cell><cell>tim</cell><cell>#</cell></row><row><cell></cell><cell cols="4">7 8 24 9</cell><cell>25</cell><cell>21</cell><cell cols="4">23 (4+3) 30 (5+3) 34 (10+3) 19 (0+1)</cell><cell>200</cell></row><row><cell>Run</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1+0</cell><cell>2+0</cell><cell></cell><cell></cell><cell></cell><cell>(22+10)</cell><cell>%</cell></row><row><cell>esfg061ptpt</cell><cell cols="4">2 4 5 2</cell><cell>9</cell><cell>3</cell><cell>2 (0+0)</cell><cell>7 (1+1)</cell><cell>13 (3+0)</cell><cell>3(0+0)</cell><cell>50 (4+1) 25.0</cell></row><row><cell>esfg062ptpt</cell><cell cols="4">2 4 6 2</cell><cell>8</cell><cell>3</cell><cell>1(0+0)</cell><cell>7 (1+1)</cell><cell>10 (3+0)</cell><cell>3 (0+0)</cell><cell>46(4+1) 24.5</cell></row><row><cell>nilc061ptpt</cell><cell cols="4">0 0 0 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.0</cell></row><row><cell>nilc062ptpt</cell><cell cols="4">0 0 0 0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1 (1+0)</cell><cell>0</cell><cell>1(0+0)</cell><cell>3(1+0)</cell><cell>1.5</cell></row><row><cell>prib061ptpt</cell><cell cols="4">6 7 15 8</cell><cell>18</cell><cell>12</cell><cell cols="4">14 (0+2) 18 (1+2) 22 (2+2) 14 (0+1) 134 (3+7) 67.0</cell></row><row><cell cols="5">uporto061ptpt 1 0 2 5</cell><cell>3</cell><cell>2</cell><cell>0)</cell><cell>3 (1+1)</cell><cell>6 (3+0)</cell><cell>1(0+0)</cell><cell>23(4+1) 11.5</cell></row><row><cell cols="5">uporto062ptpt 1 0 2 5</cell><cell>4</cell><cell>1</cell><cell>0</cell><cell>5 (1+1)</cell><cell>5 (3+0)</cell><cell>3(0+0)</cell><cell>26(4+1) 13.0</cell></row><row><cell>combination</cell><cell cols="4">6 8 18 8</cell><cell>19</cell><cell>12</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>14</cell><cell>145</cell><cell>72.5</cell></row><row><cell>esfg061enpt</cell><cell cols="4">1 3 2 2</cell><cell>7</cell><cell>2</cell><cell>1 (0+0)</cell><cell>5 (1+1)</cell><cell>5 (2+0)</cell><cell>1(0+0)</cell><cell>19(3+1) 15.4</cell></row><row><cell>lcc_061enpt</cell><cell cols="4">1 2 2 0</cell><cell>2</cell><cell>1</cell><cell>1(0+0)</cell><cell>0 (0+1)</cell><cell>2 (1+0)</cell><cell>5(0+0)</cell><cell>18(1+1)</cell><cell>9.0</cell></row><row><cell>pribe061espt</cell><cell cols="4">3 4 12 7</cell><cell>7</cell><cell>8</cell><cell>5(0+0)</cell><cell>7 (1+0)</cell><cell>7 (2+0)</cell><cell>7(0+0)</cell><cell>67(3+0) 35.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" coords="19,83.52,509.97,420.88,192.54"><head>Table 22 :</head><label>22</label><figDesc>Size of justifying snippets, in wor ds</figDesc><table coords="19,83.52,527.79,420.88,174.72"><row><cell></cell><cell></cell><cell>NonNIL</cell><cell>Aver age</cell><cell>Aver age answer</cell><cell>Aver age</cell><cell>Aver age</cell></row><row><cell>Run name</cell><cell>Answer s (#)</cell><cell>Answer s</cell><cell>answer</cell><cell>size</cell><cell>snippet</cell><cell>snippet size</cell></row><row><cell></cell><cell></cell><cell>(#)</cell><cell>size</cell><cell>(R only)</cell><cell>size</cell><cell>(R only)</cell></row><row><cell>esfg061ptpt</cell><cell>208</cell><cell>105</cell><cell>3.4</cell><cell>3.2</cell><cell>108.8</cell><cell>108.5</cell></row><row><cell>esfg062ptpt</cell><cell>204</cell><cell>98</cell><cell>3.8</cell><cell>3.5</cell><cell>109.1</cell><cell>105.5</cell></row><row><cell>nilc061ptpt</cell><cell>200</cell><cell>200</cell><cell>5.7</cell><cell></cell><cell>5.7</cell><cell></cell></row><row><cell>nilc062ptpt</cell><cell>200</cell><cell>165</cell><cell>4.9</cell><cell></cell><cell>4.4</cell><cell></cell></row><row><cell>prib061ptpt</cell><cell>200</cell><cell>170</cell><cell>3.7</cell><cell>3.8</cell><cell>31.5</cell><cell>30.3</cell></row><row><cell>uporto061ptpt</cell><cell>210</cell><cell>29</cell><cell>3.1</cell><cell>3.2</cell><cell>39.7</cell><cell>32.7</cell></row><row><cell>uporto062ptpt</cell><cell>216</cell><cell>59</cell><cell>3.0</cell><cell>2.8</cell><cell>43.1</cell><cell>33.7</cell></row><row><cell>esfg061enpt</cell><cell>202</cell><cell>61</cell><cell>3.5</cell><cell>3.5</cell><cell>95.3</cell><cell>106.1</cell></row><row><cell>Lcc_061enpt</cell><cell>1463</cell><cell>1449</cell><cell>5.2</cell><cell>4.1</cell><cell>35.2</cell><cell>34.6</cell></row><row><cell>prib061espt</cell><cell>200</cell><cell>166</cell><cell>3.5</cell><cell>4.3</cell><cell>31.3</cell><cell>29.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20" coords="20,90.00,72.93,414.96,215.58"><head>Table 23 :</head><label>23</label><figDesc>Accur acy of tempor ally restr icted questions (all answer s consider ed), compar ed to non tempor ally r estr icted ones, and to over all accur acy</figDesc><table coords="20,146.40,113.55,308.04,174.96"><row><cell></cell><cell>Questions with at</cell><cell>Accuracy for</cell><cell>Accuracy for</cell><cell>Total accuracy</cell></row><row><cell>Run name</cell><cell>least one cor r ect</cell><cell>T.R.Q.</cell><cell>nonT.R.Q</cell><cell>(%)</cell></row><row><cell></cell><cell>answer (#)</cell><cell>(%)</cell><cell>(%)</cell><cell></cell></row><row><cell>esfg061ptpt</cell><cell>4</cell><cell>18.18</cell><cell>24.73</cell><cell>24.04</cell></row><row><cell>esfg062ptpt</cell><cell>4</cell><cell>18.18</cell><cell>23.08</cell><cell>22.55</cell></row><row><cell>nilc061ptpt</cell><cell>0</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>nilc062ptpt</cell><cell>1</cell><cell>4.55</cell><cell>1.12</cell><cell>1.50</cell></row><row><cell>prib061ptpt</cell><cell>7</cell><cell>31.82</cell><cell>71.35</cell><cell>67.00</cell></row><row><cell>uporto061ptpt</cell><cell>4</cell><cell>18.18</cell><cell>10.11</cell><cell>10.95</cell></row><row><cell>uporto062ptpt</cell><cell>4</cell><cell>18.18</cell><cell>11.34</cell><cell>12.04</cell></row><row><cell>esfg061enpt</cell><cell>3</cell><cell>13.64</cell><cell>14.44</cell><cell>14.36</cell></row><row><cell>lcc_061enpt</cell><cell>5</cell><cell>2.82</cell><cell>4.35</cell><cell>4.17</cell></row><row><cell>prib061espt</cell><cell>3</cell><cell>13.64</cell><cell>35.96</cell><cell>33.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21" coords="20,70.80,495.57,442.96,206.86"><head>Table 24 :</head><label>24</label><figDesc>Results for List questions</figDesc><table coords="20,70.80,513.15,442.96,189.28"><row><cell></cell><cell>Known</cell><cell>esfg</cell><cell>esfg</cell><cell>nilc</cell><cell>nilc</cell><cell>prib</cell><cell>uporto</cell><cell>uporto</cell><cell>esfg</cell><cell>lcc</cell><cell>esfg</cell></row><row><cell>Question</cell><cell>answers</cell><cell>061ptpt</cell><cell>062ptpt</cell><cell>061ptpt</cell><cell>062ptpt</cell><cell>061ptpt</cell><cell>061ptpt</cell><cell>062ptpt</cell><cell>061enpt</cell><cell>061enpt</cell><cell>061espt</cell></row><row><cell>205</cell><cell>3</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/10</cell><cell>0/1</cell></row><row><cell>399</cell><cell>3</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>3/3</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>3/9</cell><cell>0/1</cell></row><row><cell>400</cell><cell>3</cell><cell>0.5/3</cell><cell>0.5/3</cell><cell>0/1</cell><cell>0/1</cell><cell>3/3</cell><cell>0/1</cell><cell>0/1</cell><cell>0.5/3</cell><cell>0/8</cell><cell>3/3</cell></row><row><cell>759</cell><cell>3</cell><cell>0/1</cell><cell>0/1</cell><cell>0.5/1</cell><cell>0.5/1</cell><cell>1/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0.5/10</cell><cell>0/1</cell></row><row><cell>770</cell><cell>3</cell><cell>0.5/1</cell><cell>0.5/1</cell><cell>0/1</cell><cell>0/1</cell><cell>1/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>2/10</cell><cell>0/1</cell></row><row><cell>784</cell><cell>5</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>1/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>1/9</cell><cell>0/1</cell></row><row><cell>785</cell><cell>3</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0.5/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/10</cell><cell>0/1</cell></row><row><cell>786</cell><cell>3</cell><cell>0/1</cell><cell>0/1</cell><cell>0.5/1</cell><cell>0.5/1</cell><cell>1/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>2/10</cell><cell>0/1</cell></row><row><cell>795</cell><cell>5</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>1/1</cell><cell>0/1</cell><cell>0/1</cell><cell>0/1</cell><cell>2/7</cell><cell>0/1</cell></row><row><cell>score</cell><cell></cell><cell>0.030</cell><cell>0.030</cell><cell>0.037</cell><cell>0.037</cell><cell>0.396</cell><cell>0</cell><cell>0</cell><cell>0.019</cell><cell>0.113</cell><cell>0.011</cell></row><row><cell cols="3">6.8 Spanish as Target</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22" coords="21,70.08,171.09,454.53,204.08"><head>Table 25 :</head><label>25</label><figDesc>Results at the Spanish as tar get, monolingual</figDesc><table coords="21,70.08,195.04,454.53,180.13"><row><cell></cell><cell>Right</cell><cell>W</cell><cell>X</cell><cell cols="2">U %</cell><cell cols="2">% T % D % L</cell><cell>NIL</cell><cell></cell><cell>%</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F</cell><cell></cell><cell></cell><cell>[20]</cell><cell></cell><cell>Answer</cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell>#</cell><cell>%</cell><cell>#</cell><cell>#</cell><cell cols="2"># [108] [40]</cell><cell>[42]</cell><cell>[10]</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>r</cell><cell>Extr a</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ction</cell></row><row><cell>pribe061</cell><cell>105</cell><cell cols="2">52,50 86</cell><cell>4</cell><cell cols="5">5 55,56 30,00 69,05 40,00 0,44</cell><cell>0,34</cell><cell>0,60</cell><cell></cell><cell>84,68</cell></row><row><cell>inao061</cell><cell>102</cell><cell cols="2">51,00 86</cell><cell>3</cell><cell cols="5">9 47,22 35,00 83,33 20,00 0,46</cell><cell>0,38</cell><cell cols="3">0,60 0,216 86,44</cell></row><row><cell>vein061</cell><cell>80</cell><cell cols="3">40,00 112 3</cell><cell cols="3">5 32,41 25,00 83,33</cell><cell></cell><cell>0,34</cell><cell>0,21</cell><cell cols="3">0,80 0,133 86,02</cell></row><row><cell>alia061</cell><cell>72</cell><cell cols="6">36,00 105 15 8 38,89 22,50 50,00</cell><cell></cell><cell>0,34</cell><cell>0,22</cell><cell cols="3">0,75 0,322 69,23</cell></row><row><cell>upv_061</cell><cell>70</cell><cell cols="3">35,00 119 5</cell><cell cols="3">6 37,04 25,00 47,62</cell><cell></cell><cell>0,43</cell><cell>0,33</cell><cell cols="3">0,65 0,194 70,71</cell></row><row><cell>upv_062</cell><cell>57</cell><cell cols="6">28,50 123 6 14 27,78 25,00 40,48</cell><cell></cell><cell>0,41</cell><cell>0,32</cell><cell cols="3">0,60 0,163 66,28</cell></row><row><cell>aliv061</cell><cell>56</cell><cell cols="6">28,00 123 8 13 29,63 22,50 35,71</cell><cell></cell><cell>0,34</cell><cell>0,33</cell><cell cols="3">0,35 0,190 65,12</cell></row><row><cell>aliv062</cell><cell>56</cell><cell cols="3">28,00 132 6</cell><cell cols="3">6 26,85 25,00 40,48</cell><cell></cell><cell>0,33</cell><cell>0,26</cell><cell cols="3">0,45 0,153 72,73</cell></row><row><cell>mira062</cell><cell>41</cell><cell cols="3">20,50 148 4</cell><cell cols="5">7 21,30 17,50 23,81 10,00 0,35</cell><cell>0,35</cell><cell cols="3">0,35 0,145 43,62</cell></row><row><cell>sinaiBruja06</cell><cell>39</cell><cell cols="3">19,50 146 6</cell><cell cols="3">9 16,67 17,50 33,33</cell><cell></cell><cell>0,23</cell><cell>0,13</cell><cell>0,90</cell><cell></cell><cell>79,59</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0,119</cell></row><row><cell>mira061</cell><cell>37</cell><cell cols="3">18,50 154 3</cell><cell cols="5">6 21,30 15,00 16,67 10,00 0,34</cell><cell>0,26</cell><cell cols="3">0,50 0,136 51,39</cell></row><row><cell>aske061</cell><cell>27</cell><cell cols="8">13,50 143 1 29 15,74 12,50 9,52 10,00 0,08</cell><cell>0,20</cell><cell cols="3">0,05 0,199 62,79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23" coords="21,78.96,475.89,438.72,115.52"><head>Table 26 :</head><label>26</label><figDesc>Results at the Spanish as tar get, Cr osslingual</figDesc><table coords="21,78.96,502.48,438.72,88.93"><row><cell></cell><cell></cell><cell>Right</cell><cell cols="2">W X U % F</cell><cell>% T</cell><cell cols="2">% D % L</cell><cell></cell><cell>NIL [20]</cell><cell></cell><cell></cell><cell>% Answer</cell></row><row><cell>Run</cell><cell>#</cell><cell>%</cell><cell>#</cell><cell># # [108]</cell><cell>[40]</cell><cell cols="2">[42] [10]</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>r</cell><cell>Extr action</cell></row><row><cell cols="11">pribe061ptes 72 36,00 123 3 2 39,81 27,50 38,10 20 0,29 0,29 0,30</cell><cell></cell><cell>78,26</cell></row><row><cell>alia061enes</cell><cell cols="6">41 20,50 134 9 16 17,59 12,50 40,48</cell><cell></cell><cell cols="4">0,31 0,19 0,80 0,142</cell><cell>65,08</cell></row><row><cell>lcc_061enes</cell><cell cols="6">38 19,00 141 14 7 20,37 25,00 14,29</cell><cell></cell><cell cols="4">0,35 0,35 0,35 0,067</cell><cell>55,07</cell></row><row><cell>aske061fres</cell><cell cols="6">23 11,50 162 15 13,89 10,00 7,14</cell><cell cols="5">10 0,08 0,17 0,05 0,302</cell><cell>53,49</cell></row><row><cell>aske061enes</cell><cell cols="4">12 6,00 178 10 6,48</cell><cell>2,50</cell><cell>7,14</cell><cell cols="5">10 0,10 1,00 0,05 0,091</cell><cell>40,00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24" coords="22,70.80,81.51,453.60,405.95"><head>Table 27 :</head><label>27</label><figDesc>Evolution of best per for ming systems 20032006Regarding CrossLingual runs, it is worth to mention that Priberam has achieved in the Portuguese to Spanish task a result comparable to the monolingual runs. Evolution of best r esults in NIL questions All the answers have been assessed anonymously considering all systems' answers simultaneously question by question. The interannotator agreement was evaluated over 985 answers assessed by the two judges. Only a 2.5% of the judgements were different and the resulting kappa value was 0.93.</figDesc><table coords="22,150.48,81.51,295.31,358.91"><row><cell>100,00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>90,00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80,00</cell><cell>83,33</cell></row><row><cell>80,00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70,00</cell></row><row><cell>70,00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>60,00 50,00</cell><cell>42,00</cell><cell>52,5</cell><cell></cell><cell>55,56</cell><cell>2003 2004 2005</cell></row><row><cell>40,00</cell><cell>32,50</cell><cell></cell><cell>31,11</cell><cell>29,66</cell><cell>2006</cell></row><row><cell>30,00</cell><cell>24,50</cell><cell></cell><cell>24,50</cell><cell></cell></row><row><cell>20,00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10,00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0,00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Best Overall Acc. %</cell><cell cols="2">Best in Factoids %</cell><cell>Best in Definitions %</cell></row><row><cell></cell><cell cols="4">Figur e 5: Year Fmeasur e</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2003</cell><cell>0,25</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2004</cell><cell>0,30</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2005</cell><cell>0,38</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2006</cell><cell>0,46</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="11,76.80,747.33,145.56,9.16"><p>This result is still under verification.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank <rs type="person">Donna Harman</rs> for her valuable feedback and advice, and <rs type="person">Diana Santos</rs> for her precious contribution in the organization of the campaign and the revision of this paper. <rs type="person">Paulo Rocha</rs> is thankful to the many useful comments and overall discussion with <rs type="person">Diana Santos</rs> for the Portuguese part. <rs type="person">Paulo Rocha</rs> was supported by the <rs type="funder">Portuguese Fundação para a Ciência e Tecnologia</rs> within the <rs type="projectName">Linguateca</rs> project, through grant <rs type="grantNumber">POSI/PLP/43931/2001</rs>, cofinanced by <rs type="funder">POSI</rs>. <rs type="person">Bogdan Sacaleanu</rs> was supported by the <rs type="funder">German Federal Ministry of Education and Research (BMBF)</rs> through the projects <rs type="projectName">HyLaP</rs> and <rs type="projectName">COLLATE II</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_y2scDN2">
					<idno type="grant-number">POSI/PLP/43931/2001</idno>
					<orgName type="project" subtype="full">Linguateca</orgName>
				</org>
				<org type="funded-project" xml:id="_gGSQUjX">
					<orgName type="project" subtype="full">HyLaP</orgName>
				</org>
				<org type="funded-project" xml:id="_ued2HJg">
					<orgName type="project" subtype="full">COLLATE II</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="23,74.58,238.29,378.20,9.16" xml:id="b0">
	<monogr>
		<ptr target="http://clefqa.itc.it/guidelines.html" />
		<title level="m" coord="23,88.80,238.29,162.95,9.16">QA@CLEF 2006 Organizing Committee</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,74.58,270.69,449.46,9.16;23,88.80,281.97,435.24,9.16;23,88.80,293.49,435.36,9.16;23,88.80,305.01,111.36,9.16" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="23,230.88,270.69,180.67,9.16">Question answering pilot task at CLEF 2004</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,351.12,281.97,172.92,9.16;23,88.80,293.49,78.45,9.16">Multilingual Information Access for Text, Speech and Images</title>
		<title level="s" coord="23,175.92,293.49,146.90,9.16">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>SpringerVerlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="581" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,74.58,316.53,449.50,9.16;23,88.80,328.05,435.24,9.16;23,88.80,339.57,435.36,9.16;23,88.80,351.09,435.36,9.16;23,88.80,362.61,62.64,9.16" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="23,88.80,328.05,276.14,9.16">Overview of the CLEF 2004 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Erbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,299.76,339.57,224.40,9.16;23,88.80,351.09,27.05,9.16">Multilingual Information Access for Text, Speech and Images</title>
		<title level="s" coord="23,124.80,351.09,148.13,9.16">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>SpringerVerlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page">371391</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,74.58,374.13,449.58,9.16;23,88.80,385.65,435.60,9.16;23,88.80,397.17,288.60,9.16" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="23,221.52,374.13,302.64,9.16;23,88.80,385.65,37.91,9.16">CLEF: Abrindo a porta à participa internacional em avaliação de RI do português</title>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><forename type="middle">&amp;</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,253.92,385.65,270.48,9.16;23,88.80,397.17,143.04,9.16">Avaliação conjunta: um novo paradigma no processamento computacional da língua portuguesa</title>
		<editor>
			<persName><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</editor>
		<meeting><address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<publisher>IST Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="23,74.58,408.45,449.34,9.16;23,88.80,419.97,435.28,9.16;23,88.80,431.49,435.24,9.16;23,88.80,443.01,281.40,9.16" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="23,186.24,408.45,337.68,9.16;23,88.80,419.97,30.70,9.16">The Key to the First CLEF with Portuguese: Topics, Questions and Answers in CHAVE</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,88.80,431.49,255.12,9.16">Multilingual Information Access for Text, Speech and Images</title>
		<title level="s" coord="23,352.32,431.49,146.18,9.16">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>SpringerVerlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page">821832</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,74.58,454.53,449.58,9.16;23,88.80,466.05,435.28,9.16;23,88.80,477.57,54.48,9.16" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="23,158.64,454.53,152.80,9.16">Is question answering a rational task?</title>
		<author>
			<persName coords=""><forename type="first">Spark</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,466.80,454.53,57.36,9.16;23,88.80,466.05,377.65,9.16">Questions and Answers: Theoretical and Applied Perspectives. Second CoLogNETElsNET Symposium</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Moortgat</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="24" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,74.58,489.33,449.58,9.16;23,88.80,500.32,435.22,8.29;23,88.80,510.64,435.36,8.29;23,88.80,520.96,199.32,8.29" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="23,372.72,500.32,151.30,8.29;23,88.80,510.64,99.46,8.29">Overview of the CLEF 2005 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">Alessandro</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lili</forename><surname>Aunimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christelle</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,210.00,510.64,314.16,8.29;23,88.80,520.96,48.40,8.29">Cross Language Evaluation Forum: Working Notes for the CLEF 2005 Workshop (CLEF 2005)</title>
		<meeting><address><addrLine>Vienna, Áustria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2123 September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,79.20,531.33,444.96,9.16;23,88.80,542.85,435.28,9.16;23,88.80,554.37,215.52,9.16" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="23,167.52,531.33,241.10,9.16">Overview of the TREC 2002 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,182.64,542.85,341.44,9.16;23,88.80,554.37,78.79,9.16">Proceedings of the Eleventh Text Retrieval Conference (TREC 2002 NIST Special Publication 500251</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>the Eleventh Text Retrieval Conference (TREC 2002 NIST Special Publication 500251<address><addrLine>Washington DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">123</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
