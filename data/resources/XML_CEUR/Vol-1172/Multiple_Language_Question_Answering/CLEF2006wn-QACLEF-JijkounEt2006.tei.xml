<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,197.67,148.86,207.66,15.15">Overview of WiQA 2006</title>
				<funder ref="#_g8bB4TS #_RzQB2Gf">
					<orgName type="full">NWO</orgName>
				</funder>
				<funder ref="#_jkukVEE">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_xC5RAKu #_tCp6E3M #_hThbkTb #_FwJSSck #_TbAK8Jn #_QNJDVEa #_aSn8nqh #_rZsYnCD #_m6eCq2S #_SS9755b #_XjCbKwz #_YkGDXXd">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.77,182.75,71.54,8.74;1,309.23,182.75,75.99,8.74"><forename type="first">Valentin</forename><forename type="middle">Jijkoun</forename><surname>Maarten De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,197.67,148.86,207.66,15.15">Overview of WiQA 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A89046A3139A8187CA2F5BE02BE3FE91</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Question answering, Questions beyond factoids, Wikipedia</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe WiQA 2006, a pilot task aimed at studying question answering using Wikipedia. Going beyond traditional factoid questions, the task considered at WiQA 2006 was to return-given an source page from Wikipedia-to identify snippets from other Wikipedia pages, possibly in languages different from the language of the source page, that add new and important information to the source page, and that do so without repetition.</p><p>A total of 7 teams took part, submitting 20 runs. Our main findings are twofold: (i) while challenging, the tasks considered at WiQA are do-able as participants achieved impressive scores as measured in terms of yield, mean reciprocal rank, and precision, (ii) on the bilingual task, substantially higher scores were achieved than on the monolingual tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>CLEF 2006 featured a pilot on Question Anwering Using Wikipedia, or WiQA <ref type="bibr" coords="1,434.92,600.64,9.96,8.74" target="#b2">[3]</ref>, for short. The idea to organize a pilot track on QA using Wikipedia builds on several motivations: First, traditionally, people turn to reference works to get answers to their questions. Wikipedia has become one of the largest reference works ever, making it a natural target for question answering systems. Moreover, Wikipedia is a rich mixture of text, link structure, navigational aids, categories,. . . , making it extremely appealing for text mining and link analysis work. And finally, Wikipedia is simply a great resource. It is something we want to work with, and contribute to, both by facilitating access to it, and, as the distinction between readers and authors has become blurred, by creating tools to support the authoring process.</p><p>In this overview we first provide a description of the tasks considered and of the evaluation and assessment procedures (Section 2). After that we describe the runs submitted by the participants (Section 3 and detail the results (Section 4). We end with some preliminary conclusions (Section 5).</p><p>The WiQA 2006 task deals with access to Wikipedia's content, where access is considered both from a point of view and from an author point of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tasks</head><p>As our user model we take the following scenario: a reader or author of a given Wikipedia article (the source page) is interested in collecting information about the topic of the page that is not yet included in the text, but is relevant and important for the topic, so that it can be used to update the content of the source article. Although the source page is in a specific language (the source language), the reader or author would also be interested in finding information in other languages (the target languages) that he explicitely specifies.</p><p>With this user scenario, the task of an automatic system is to locate information snippets in Wikipedia which are:</p><p>• outside the given source page,</p><p>• in one of the specified target languages,</p><p>• substantially new w.r.t. the information contained in the source page, and important for the topic of the source page, in other words, worth including in the content of (the future editions of) the page.</p><p>Participants of the WiQA 2006 pilot could take part in two flavors of the task: a monolingual one (where the snippets to be returned are in the language of the source page) and a multilingual (where the snippets to be returned can be in any of the languages of the Wikipedia corpus used at WiQA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Collections</head><p>The corpus used at WiQA 2006 consists of XML-ified dumps of Wikipedia in three language: Dutch, English, and Spanish. The dumps are based on the XML version of the Wikipedia collections <ref type="bibr" coords="2,127.22,485.43,10.51,8.74" target="#b0">[1]</ref> that include the annotation of the structure of the articles, links between articles, categories, cross-lingual links, etc. For the WiQA 2006 pilot the collections were enriched with annotations of sentences and classification of pages into named entity classes (person, location, organization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Topics</head><p>For each of the three WiQA 2006 languages (Dutch, English, and Spanish) a set of 50 topics correctly tagged as PERSON, LOCATION or ORGANIZATION in the XML data collections was released, together with other topics, announced as optional. These optional topics either did not fall into these three categories, or were not tagged correctly in the XML collections. The optional topics could be ignored by systems without penalty. In fact, the submitted runs provided responses for optional topics as well as for the main topics. When selecting Wikipedia articles as topics, we included articles explicitely marked as stubs, as well as other short and long articles.</p><p>In order to create the topics for the English-Dutch bilingual task, 30 topics were selected from the English monolingual topic set and 30 topics from the Dutch monolingual topic set. The bilingual topics were selected so that the corresponding articles are present in Wikipedias for both languages.</p><p>In addition to the test topics, a set of 80 (English language) development topics was released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Evaluation</head><p>Given a source page, automatic systems return a list of short snippets, defined as sequences of at most two sentences from a Wikipedia page. The ranked list of snippets for the topic were manually assessed using the following binary criteria, largely inspired by the TREC 2003 Novelty task <ref type="bibr" coords="3,496.23,154.32,9.96,8.74" target="#b1">[2]</ref>:</p><p>• support: the snippet does indeed come from the specified target Wikipedia article.</p><p>• importance: the information of the snippet is relevant to the topic of the source Wikipedia article, is in one of the target languages as specified in the topic, and is already present on the page (directly or indirectly) or is interesting and important enough to be included in an updated version of the page.</p><p>• novelty: the information content of the snippet is not subsumed by the information on the source page</p><p>• non-repetition: the information content of the snippet is not subsumed by the target snippets higher in the ranking for the given topic Note that we distinguish between novelty (subsumption by the source page) and non-repetition (subsumption by the higher ranked snippets) in order for the results of the assessment to be re-usable for automatic system evaluation in future: novelty only takes the source page and the snippet into account, while non-repetition is defined on a ranked list of snippets.</p><p>One of the purposes of the WiQA pilot task was to experiment with different measures for evaluating the performance of systems. WiQA 2006 used the following simple principal measure for accessing the performance of the systems:</p><p>• yield : the average (per topic) number of supported, novel, non-repetitive, important target snippets.</p><p>We also considered other simple measures:</p><p>• mean reciprocal rank of the first supported, important, novel, non-repeated snippet, and</p><p>• overall precision: the percentage of supported, novel, non-repetitive, important snippets among all submitted snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Assessment</head><p>To establish the ground truth, an assessment environment was developed by the track organizers. Assessors were given the following guidelines. For each system and each source article P the ordered list of the returned snippets was be manually assessed with respect to importance, novelty and non-repetition following the procedure below:</p><p>1. Each snippet was marked as supported or not. To reduce the workload on the assessors, this aspect was checked automatically. Hence, unsupported snippets were excluded from the subsequent assessment.</p><p>2. Each snippet was marked as important or not, with respect to the topic of the source article. A snippet is important if it contains information that a user of Wikipedia would like to see in P or an author would consider worth to be present in P. Snippets were assessed for importance independently of each other and regardless of whether the important information was already present in P (in particular, presence of some information in P does not necessarily imply its importance).</p><p>3. Each important snippet was marked as novel or not. It was to be considered novel if the important information in the snippet is substantially new with respect to the content of P.</p><p>4. Each important and novel snippet was marked as repeated or non-repeated, with respect to the important snippets higher in the ranked list of snippets. Following this procedure, snippets were assessed along four axes (support, importance, novelty, non-repetition). Assessors were not required to judge novelty and non-repetition of snippets that are considered not important for the topic of the source article. The reason for this was to avoid spending much time on assessing irrelevant information. Assessors provided assessments for the top 20 snippets for each result list returned. Figure <ref type="figure" coords="4,325.46,520.60,4.98,8.74" target="#fig_0">1</ref> contains a screen shot of the assessment interface.</p><p>A total number of 14203 snippets had to be assessed; the number unique snippets assessed is 4959. Of these, 3396 were assessed by at least two assessors.</p><p>The results of the assessments for all submitted runs (anonymized) will be made available to all the participants for further analysis and experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Submission</head><p>For each task (three monolingual and one bilingual), participating teams were allowed to submit up to three runs. For each topic of a run, the top 20 submitted snippets were manually assessed as described above. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submitted Runs</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,90.00,440.95,423.00,8.74;4,90.00,109.26,423.01,316.63"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Assessment interface; first three snippets of a system's response for topic wiqa06-en-39.</figDesc><graphic coords="4,90.00,109.26,423.01,316.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,90.00,705.33,423.01,44.60"><head>Table 1</head><label>1</label><figDesc>lists the runs submitted to WiQA 2006: 19 for the monolingual task (Dutch: 3, English: 12, Spanish: 4) and 1 for the bilingual task (English-Dutch). In Table2we present the aggregate results of the assessment of the runs submitted to WiQA 2006. Columns 3-7 show the following aggregate numbers: total number of snippets (with at most 20 snippets considered per response for</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgments</head><p>We are very grateful to the following people and organizations for helping us with the assessments: <rs type="person">José Luis Martínez Fernández</rs> and <rs type="person">César de Pablo</rs> from the <rs type="affiliation">Daedalus consortium</rs>; <rs type="person">Silke Scheible</rs> and <rs type="person">Bonnie Webber</rs> at the <rs type="affiliation">University of Edinburgh</rs>; <rs type="person">Udo Kruschwitz</rs> and <rs type="person">Richard Sutcliffe</rs> at the <rs type="affiliation">University of Essex</rs>; and <rs type="person">Bouke Huurnink</rs> and <rs type="person">Maarten de Rijke</rs> at the <rs type="affiliation">University of Amsterdam</rs>.</p><p><rs type="person">Valentin Jijkoun</rs> was supported by the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">600.-065.-120</rs> and <rs type="grantNumber">612.000.106</rs>. Maarten de Rijke was supported by <rs type="funder">NWO</rs> under project numbers <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">264-70-050</rs>, <rs type="grantNumber">354-20-005</rs>, <rs type="grantNumber">600.-065.-120</rs>, <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">612.066.302</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">640.001.501</rs>, <rs type="grantNumber">640.002.501</rs>, and and by the <rs type="programName">E.U. IST programme</rs> of the <rs type="programName">6th FP for RTD</rs> under project <rs type="projectName">MultiMATCH</rs> contract <rs type="grantNumber">IST-033104</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jkukVEE">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_xC5RAKu">
					<idno type="grant-number">600.-065.-120</idno>
				</org>
				<org type="funding" xml:id="_g8bB4TS">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_RzQB2Gf">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_tCp6E3M">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_hThbkTb">
					<idno type="grant-number">264-70-050</idno>
				</org>
				<org type="funding" xml:id="_FwJSSck">
					<idno type="grant-number">354-20-005</idno>
				</org>
				<org type="funding" xml:id="_TbAK8Jn">
					<idno type="grant-number">600.-065.-120</idno>
				</org>
				<org type="funding" xml:id="_QNJDVEa">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_aSn8nqh">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_rZsYnCD">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_m6eCq2S">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_SS9755b">
					<idno type="grant-number">640.001.501</idno>
				</org>
				<org type="funding" xml:id="_XjCbKwz">
					<idno type="grant-number">640.002.501</idno>
					<orgName type="program" subtype="full">E.U. IST programme</orgName>
				</org>
				<org type="funded-project" xml:id="_YkGDXXd">
					<idno type="grant-number">IST-033104</idno>
					<orgName type="project" subtype="full">MultiMATCH</orgName>
					<orgName type="program" subtype="full">6th FP for RTD</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The results indicate that the task of detecting important snippets is a hard one: for most submissions, only 50-60% of the found snippets are judged as important. The performance of the systems for detecting novel snippets has a substantially higher range: between 50% and 80% of the found important snippets are judged as novel with respect to the topic article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Table <ref type="table" coords="5,116.54,631.73,4.98,8.74">3</ref> shows the evaluation results for the submitted runs: total yield (for a run, the total number of "perfect" snippets, i.e., supported, important, novel and not repeated), the average yield per topic (only topics with at least one response are considered), the mean reciprocal rank of the first "perfect" snippet and the precision of the systems' responses.</p><p>Clearly, most systems cope well with the pilot task: up to one third of the found snippets are assessed as "perfect" for the English and Spanish monolingual tasks, and up to one half for the Dutch monolingual and the English-Dutch bilingual task. Quite expectedly, the relative ranking of the submitted runs is different for different evaluation measures: as in many complex tasks, the best yield (a recall-oriented measure) does not necessarily lead to the best precision and vice versa. An interesting aspect of the results is that the performance of the systems differs substantially for the four tasks. This can be due to the fact that the submissions for tasks were assessed by different assessors (native speakers of the corresponding languages), as well as due to the differences in the sizes and structures of the Wikipedias in these languages. It is worth pointing out that the highest scores were achieved on the English-Dutch bilingual task; this may suggest that different language versions of Wikipedia do indeed present different material on a given topic.</p><p>Finally, a more detailed analysis of this issue, as well as the analysis of the inter-annotator agreement will be presented by the time of the CLEF workshop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have described the first installment of the WiQA-Question Answering Using Wikipediatask. Set up as an attempt to take question answering beyond the traditional factoid format and to one of the most interesting knowledge sources currently available, WiQA had 8 participants who submitted a total of 20 runs for 4 tasks. The results of the pilot are very encouraging. While challenging, the task turned out to be do-able, and in cases several participants managed to achieve impressive yield, MRR, and precision scores. Surprisingly, the highest scores were achieved on the bilingual task. As to the future of WiQA, as pointed out before we aim to take a close look at our assesssments, perhaps add new assessments, and analyse inter-assessor agreement along various dimensions. The WiQA 2006 pilot has shown that it is possible to set up tractable yet challenging information access tasks involving the multilingual Wikipedia corpus-but this was only a first step. In the future we would like to consider additional information access scenarios, all centered around Wikipedia.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.50,732.08,398.18,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,281.79,732.08,123.52,8.74">The Wikipedia XML Corpus</title>
		<author>
			<persName coords=""><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,414.55,732.08,57.90,8.74">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,112.02,407.50,8.74;8,105.50,123.98,360.33,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,256.55,112.02,185.43,8.74">Overview of the TREC 2003 Novelty track</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,463.34,112.02,49.66,8.74;8,105.50,123.98,241.71,8.74">Proceedings of the Twelfth Text REtrieval Conference (TREC 2003)</title>
		<meeting>the Twelfth Text REtrieval Conference (TREC 2003)</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="38" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,143.90,407.49,9.02;8,105.50,156.57,28.92,8.30" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><surname>Wiqa</surname></persName>
		</author>
		<ptr target="http://ilps.science.uva.nl/WiQA/" />
		<title level="m" coord="8,171.78,143.90,189.51,8.74">Question Answering Using Wikipedia URL</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
