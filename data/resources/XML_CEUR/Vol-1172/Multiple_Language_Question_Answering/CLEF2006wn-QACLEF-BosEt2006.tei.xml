<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.79,148.86,293.43,15.15;1,205.35,170.78,192.31,15.15">Cross-Lingual Question Answering by Answer Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,175.11,204.67,45.44,8.74"><forename type="first">Johan</forename><surname>Bos</surname></persName>
							<email>bos@di.uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Linguistic Computing Laboratory</orgName>
								<orgName type="institution">University of Rome &quot;La Sapienza&quot;</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.82,204.67,67.91,8.74"><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
							<email>malvina.nissim@loa-cnr.it</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratory for Applied Ontology Inst. for Cognitive Science</orgName>
								<orgName type="institution">Technology National Research Council</orgName>
								<address>
									<settlement>Rome</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.79,148.86,293.43,15.15;1,205.35,170.78,192.31,15.15">Cross-Lingual Question Answering by Answer Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">897ED34002AC35D561EE9172F9907874</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.5 [Information Interfaces and Presentation]: H.5.2 User Interfaces</term>
					<term>I.2 [Artificial Intelligence]: I.2.1 Applications and Expert Systems</term>
					<term>I.2.4 Knowledge Representation Formalisms and Methods</term>
					<term>I.2.7 Natural Language Processing Algorithms, Design, Measurement, Performance, Experimentation question answering, machine translation, natural language processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We approach cross-lingual question answering by using a mono-lingual QA system for the source language and by translating resulting answers into the target language. As far as we are aware, this is the first cross-lingual QA system in the history of CLEF that uses this method-all other cross-lingual QA systems known to us use translation of the question or query instead. We demonstrate the feasibility of this approach by using a mono-lingual QA system for English, and translating answers and finding appropriate documents in Italian and Dutch. For factoid and definition questions, we achieve overall accuracy scores ranging from 13% (EN→NL) to 17% (EN→IT) and lenient accuracy figures from 19% (EN→NL) to 25% (EN→IT). The advantage of this strategy to cross-lingual QA is that translation of answers is easier than translating questions-the disadvantage is that answers might be missing from the source corpus and additional effort is required for finding supporting documents of the target language.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated Question Answering (QA) is the task of providing an exact answer (instead of a document) to a question formulated in natural language. Cross-lingual QA is concerned with providing an answer in one language (the target language) to a question posed in a different language (the source language). Most systems tackle the cross-lingual problem by translating the question or query posed in the source language in the target language, and then use a QA system developed for the target language for retrieving an answer. For instance, at the last three QA@CLEF campaigns <ref type="bibr" coords="2,193.35,205.57,23.90,8.74">(2003)</ref><ref type="bibr" coords="2,217.24,205.57,4.78,8.74">(2004)</ref><ref type="bibr" coords="2,222.02,205.57,23.90,8.74">(2005)</ref> there were 28 participants performing in 20 different crosslingual tasks <ref type="foot" coords="2,145.28,215.95,3.97,6.12" target="#foot_0">1</ref> , each of them using off-the-shelf translation software (such as Babelfish, Systran, Reverso, FreeTrans, WorldLingo, Transtool) to translate the question or the keywords of the query of the source language into the target language, followed by processing the translated question using a QA system designed for the target language.</p><p>Little attention has been given to the obvious alternative approach: translating the answer, instead of the question. There are some potential advantages following this route: answers are easier to translate, due to their simpler syntactic structure. In fact, some types of answers (such as date expressions or names of persons) hardly need a translation. In addition, finding a document in the target language supporting the answer (as prescribed in the QA@CLEF exercise) is feasible: using either the translated question or keywords thereof and the translated answer, standard document retrieving tools can be used to find a document. This approach can work provided that the source and target language documents cover the same material, otherwise all bets are off.</p><p>In the context of CLEF, we tested this approach relying on the fact that the documents in the collection are from the same time period. We ran our experiments for two language pairs: EN→NL and EN→IT, using a mono-lingual QA system for English and an off-the-shelf machine translation tool for translating the answers from source into target language. In this report we briefly describe our system, show system output, evaluate the approach, and comment on the feasibility of this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>Our cross-lingual QA system that we used for our experiments at CLEF 2006 is an extension of a mono-lingual QA system for English. It deals with factoid (including list questions) and definition questions, but it uses two different streams of processing for these two types of questions. The first component in the pipeline, Question Analysis, deals with all types of questions. Then, when the question turns out to be of type factoid, Document Analysis, Answer Extraction and Answer Translation will follow. Answers to definition questions are directly searched in the target language corpora (see Figure <ref type="figure" coords="2,220.84,546.93,4.98,8.74" target="#fig_0">1</ref> and Section 2.5). What follows is a more detailed description of each component. Examples of the different data structures of the system are shown in Figure <ref type="figure" coords="2,508.02,558.88,4.98,8.74" target="#fig_1">2</ref> and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Analysis</head><p>The (English) question is tokenised and parsed with a wide-coverage parser based on Combinatory Categorial Grammar (CCG). We use the parser of Clark &amp; Curran <ref type="bibr" coords="2,381.27,628.73,9.96,8.74" target="#b2">[3]</ref>. On the basis of the output of the parser, a CCG-derivation, we build a semantic representation in the form of a Discourse Representation Structure (DRS), closely following Discourse Representation Theory <ref type="bibr" coords="2,463.63,652.64,9.96,8.74" target="#b3">[4]</ref>. This is done using the semantic construction method described in <ref type="bibr" coords="2,348.44,664.60,10.51,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,362.41,664.60,7.01,8.74" target="#b1">2]</ref>. The Question-DRS is the basis for generating four other sources of information required later in the question answering process: an expected answer type; a query for document retrieval; the answer cardinality; and background knowledge for finding appropriate answers. We use 14 main expected answer types which are further divided into subtypes. The main types are definition, description, attribute, numeric, measure, time, location, address, name, language, creation, instance, kind, and part. The answer cardinality denotes a range expressed by an ordered pair of two numbers, the first indicating the mininal number of answers expected, the second the maximal number of answers (or 0 if unspecified). For instance, 3-3 indicates that exactly three answers are expected, 2-0 means at least two answers. Background knowledge is a list of axioms related to the question-it is information gathered from WordNet or other lexical resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Analysis</head><p>In order to maximise the chance of finding an answer in the source language (English), we extended the English CLEF document collection with documents from the Acquaint corpus. All documents were pre-procesed: sentence splitting, tokenisation, and dividing into smaller documents of two sentences each (Taking a sliding window, so each sentence will appear in two mini-documents). These mini-documents are indexed with the Indri information retrieval tools (we used version 2.2, see http://www.lemurproject.org/indri/). The query generated by Question Analysis (see Section 2.1) is used to retrieve the best 1,000 mini-documents, again with the use of Indri.</p><p>Using the same wide-coverage parser as for parsing the question, all retrieved documents are parsed and for each of them a Discourse Representation Structure (DRS) is generated. The parser also performs basic named entity recognition for locations, persons, and organisations. This information is used to assign the right semantic type to discourse referents in the DRS. Date expressions are normalised in the DRS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Extraction</head><p>Given the DRS of the question (the Q-DRS), and a set of DRSs of the retrieved documents (the A-DRSs), we match each A-DRS with the Q-DRS to find a potential answer. This process proceeds as follows: if the A-DRS contains a discourse referent of the expected answer type (see 2.1) matching will commence attempting to identify the semantic structure in the Q-DRS with that of the A-DRS. The result is a score between 0 and 1 indicating the amount of semantic material that could be matched. The background knowledge (such as hyponyms from WordNet) generated by the Question Analysis (see 2.1) is used to assist in the matching. All retrieved answers are re-ranked on the basis of the match-score and frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Answer Translation</head><p>The answer obtained for the source language is now translated into the target language by means of the Babelfish off-the-shelf machine translation tool. However, this was not done for all types of answers: we refrained from translating answers that were person names or titles of creative works. Person names are normally not translated across the languages that we were working with. Names of creative works often are, but the machine translation software that we used did not perform well enough on some examples in our training data, so that we decided to leave these untranslated. In addition, titles of creative works often don't get a literal translation (a case in point is Woody Allen's "Annie Hall", which is translated in Italian as "Io e Annie") so a more sophisticated translation strategy is required.</p><p>Given the answer in the target language, we need to find a supporting document from the target language collection (as prescribed by the QA@TRE exercise). Hence, we also translate the original question, independent of the answer found for it. This we use to construct another query and then retrieve a document from the target language collection that contains the translated answer and as many as possible terms from the translated question. (As with the source language documents, these are indexed and retrieved using Indri.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Definition Question Processing</head><p>We did not put much effort in dealing with definition questions. We basically adopted a simple pattern matching technique directly on the target language documents. Here is how it works.</p><p>Once a question is idenfied as a definition question (see Figure <ref type="figure" coords="5,385.79,154.32,3.88,8.74" target="#fig_0">1</ref>), all non-content words are removed from the question (wh-words, the copula, articles, punctuation, etc.). What is left is the topic of the definition question. For instance, English question: "Who is Radovan Karadzic?" Topic: "Radovan Karadzic".</p><p>Given the topic, we search interesting information about the topic, stated in the target language. We do this using an off-line dump of the Dutch and Italian Wikipedia pages by selecting sentences containing the target. From this we generate an Indri query selecting all (one-sentence) documents containing the topic, and a combination of the interesting terms found in Wikipedia (stop words are removed). This gives us a list of documents of the target language.</p><p>The last step is some simple template matching using regular expressions to extract the relevant clauses of a sentence that could count as an answer to the question. There are only a few templates, but we use different ones for the different sub-types of definition questions (the Question Analysis component distinguishes between three sub-types of definition questions: person, organisation, and thing). Here are the patterns we used: person:</p><p>/(^| )(\,|de|il|la|i|gli|l\') (.+) $topic /i person:</p><p>/$topic \( (\d+) \)/i organisation:</p><p>/$topic \(([^\)]+)\)/i thing:</p><p>/$topic , ([^,]+) , / thing: /$topic \(([^\)]+)\)/i</p><p>As an example, consider the following sentences and answers in bold-face:</p><p>LASTAMPA94-041510 Il leader Radovan Karadzic non era reperibile. LASTAMPA94-042168 Intanto il leader serbo-bosniaco Radovan Karadzic e il comandante in capo delle forze serbe, gen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Performance</head><p>The question analysis performed fairly well. Only 2 of 200 questions of the EN→IT set, and 7 of EN→NL set could not be parsed. For 189 questions of the EN→IT set, and 177 of the EN→NL set the system determined an expected answer type. This already shows that our system had more difficulties with the EN→NL set, which is reflected in the overall scores. Relatively many questions did not have an answer in the English collection, or at least our system failed to find one. For 41 of the 200 EN→NL questions and for 43 of the 200 EN→IT questions no English answer (correct or incorrect) was found. Answer translation introduced only few errors (see Section 3.2), but finding a supporting document proved harder than we had hoped. So several correct answers were associated with wrong documents.</p><p>We submitted four runs-two for the EN→NL task, and two for the EN→IT task. The first runs contained one answer for factoid questions and up to ten for definition and list questions. The second runs contained up to ten answers for each type of question. We used this strategy because it was unclear, at the time of submission, what kind of evaluation would be used. So the number one runs would perform better on an accuracy score, and the number two runs better on scores based on the average mean reciprocal rank (MRR).</p><p>It turned out that both accuracy and MRR were used in the evalation. The results for definition and factoid questions are shown in Table <ref type="table" coords="5,267.74,745.64,3.88,8.74" target="#tab_1">1</ref>, and the figures for the list questions in Table <ref type="table" coords="5,473.03,745.64,3.88,8.74" target="#tab_2">2</ref> [AGZ.941120.0028] I cantieri di Belfast hanno confermato di aver ricevuto la commessa più prestigiosa dopo la costruzione del "Titanic": la messa in mare del "Titanic 2". Una copia perfetta del fantasmagorico transatlantico affondato nell' oceano nel 1912 durante il viaggio inaugurale è stata ordinata da un consorzio giapponese presso la ditta "Harland and Wolff" e sarà completata entro il 1999.  tables illustrate, the scores for accuracy are the same for each run, which means that providing more than one answer for a factoid question was not punished. As expected, we achieved better scores for MRR on the number two runs. It is surprising that we did so well on definition questions, given that we paid very little effort in dealing with them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answer Translation Accuracy</head><p>Recall that depending on the kind of expected answer type, answers were translated into the target language or not. We did not translate names of persons, nor titles of creative works. Whereas this strategy worked out well for person names, it didn't for names of creative works. For instance, for question "0061 What book did Salman Rushdie write?", we found a correct answer "Satanic Verses" in the English documents, but this was not found in the Italian document collection, because the Italian translation is "Versetti Satanici". Babelfish wouldn't have helped us here either, as it translates the title into "Verses Satanic". Overall, answer translation introduced little errors as many answers are easy to translate (Table <ref type="table" coords="9,122.11,123.98,3.88,8.74" target="#tab_3">3</ref>). Locations are usually correcly translated by Babelfish, and so are time expressions. Difficulties sometimes arise for numeric expressions. For instance, for the question "0084 How many times did Jackie Stewart become world champion?" we found the correct English answer "three-time". However, this was wrongly translated in "drie-tijd", and obviously not found in the Dutch corpus. Similarly, many answers for measurements are expressed in the English corpus using the imperial system, whereas the metric system is used in the Dutch and Italian corpora. Finally, some things are just hard to translate. Although we found a reasonably correct answer for "0136 What music does Offspring play?", namely "rock", this was translated in Dutch as "rots". In itself a correct translation, but for the wrong context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>What can we say about the answer-translation strategy to cross-lingual question answering, and how can we improve it? Generally speaking, we believe it is a promising approach, because answers are easier to translate than questions-in fact, many answers don't need to be translated, and some are relatively easy to translate, such as date expressions, locations, and numerals. The remaining translation difficulties, such as translating measure terms from the imperical to the metric system, and titles of creative works, can be dealt with by employing designated translation tools such as pre-compiled lookup-tables. It is also an advantage that the type of the answer is known-this information can serve in obtaining a better translation.</p><p>One problem that came to the surface was the omission of the answer in the source language document collection. There is only one way to deal with this situation, and that is getting a larger pool of documents. One option is to use the web for finding the answer in the source language.</p><p>Another problem that arised was finding a supporting document for the target language. The system can certainly be improved with respect to this point: it currently only takes the translated question as additional information to find a target language document. This is interesting, as the machine translated question need not be grammatically perfect to find a correct document. One way to improve this is to translate the context found for the source language as well, and use it in addition to retrieve a target language document.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,189.12,678.96,224.77,8.74;3,353.29,555.44,86.71,68.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Simplified architecture of the QA system.</figDesc><graphic coords="3,353.29,555.44,86.71,68.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,131.06,709.51,340.89,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: System input and output for a factoid question in the EN→IT task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,129.60,722.24,343.79,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: System input and output for a factoid question in the EN→NL task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,476.90,745.64,36.10,8.74"><head></head><label></label><figDesc>. As the</figDesc><table coords="6,95.98,145.02,405.52,493.23"><row><cell>Input Question:</cell><cell cols="3">[0040] What year did Titanic sink?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>x2 x3</cell></row><row><cell>Q-DRS:</cell><cell>x1</cell><cell>?</cell><cell>named(x3,titanic,nam)</cell></row><row><cell></cell><cell>year(x1)</cell><cell></cell><cell>sink(x2) agent(x2,x3)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>rel(x2,x1)</cell></row><row><cell>Answer Type:</cell><cell>[tim:yea]</cell><cell></cell><cell></cell></row><row><cell>Cardinality:</cell><cell>1-1</cell><cell></cell><cell></cell></row><row><cell>English Query:</cell><cell cols="3">#filreq(Titanic #weight(1 Titanic 3 sink))</cell></row><row><cell cols="2">English Answer: 1912</cell><cell></cell><cell></cell></row><row><cell cols="4">English Context: [APW19981105.0654] Speaking of bad guys: Cartoon News reprints a</cell></row><row><cell></cell><cell cols="3">bunch of political cartoons published after the Titanic sank in 1912.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>x0 x1 x2 x3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>speak(x1) agent(x1,x0)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bad(x2) guy(x2)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>proposition(x3) :(x2,x3)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>x4 x5 x6 x6 x7 x8 x9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>named(x4,cartoon news,org)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bunch(x5) political(x6)</cell></row><row><cell>A-DRS:</cell><cell></cell><cell></cell><cell>publish(x7) patient(x7,x6) proposition(x8) after(x7,x8)</cell></row><row><cell></cell><cell>x3:</cell><cell></cell><cell>x10 x11 x12</cell></row><row><cell></cell><cell>x8:</cell><cell></cell><cell>named(x10,titanic,nam)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>timex(x12)=1912-XX-XX</cell></row><row><cell></cell><cell></cell><cell cols="2">sink(x11) agent(x11,x10) in(x11,x12)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>cartoon(x6) of(x5,x6)</cell></row><row><cell></cell><cell></cell><cell cols="2">reprint(x9) agent(x9,x4) patient(x9,x5)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>of(x1,x2)</cell></row><row><cell>Italian Answer:</cell><cell>1912</cell><cell></cell><cell></cell></row><row><cell>Italian Query:</cell><cell cols="3">#filreq(1912 #combine(1912 anno Titanic affondato))</cell></row><row><cell>Italian Context:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,90.00,179.21,423.00,116.93"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="8,90.00,179.21,423.00,116.93"><row><cell cols="8">: Number of right (R), wrong (W), inexact (X), unsupported (U) answers, accuracy measure</cell></row><row><cell cols="8">of first answers (factoid, definition, overall, and lenient), and mean reciprocal rank score (MRR)</cell></row><row><cell cols="4">for all four submitted CLEF 2006 runs.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Accuracy</cell></row><row><cell>Run</cell><cell>R</cell><cell cols="6">W X U Fact. Acc. Def. Acc. Overall Lenient MRR</cell></row><row><cell cols="4">EN→IT 1 32 141 4 11</cell><cell>15.3%</cell><cell>24.4%</cell><cell>17.0%</cell><cell>25.0%</cell><cell>0.18</cell></row><row><cell cols="4">EN→IT 2 32 141 4 11</cell><cell>15.3%</cell><cell>24.4%</cell><cell>17.0%</cell><cell>25.0%</cell><cell>0.20</cell></row><row><cell cols="3">EN→NL 1 25 150 6</cell><cell>3</cell><cell>11.6%</cell><cell>20.5%</cell><cell>13.4%</cell><cell>18.5%</cell><cell>0.14</cell></row><row><cell cols="3">EN→NL 2 25 149 7</cell><cell>3</cell><cell>11.6%</cell><cell>20.5%</cell><cell>13.4%</cell><cell>19.0%</cell><cell>0.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,90.00,342.70,423.00,94.96"><head>Table 2 :</head><label>2</label><figDesc>Number of right (R), wrong (W), inexact (X), unsupported (U), unassesed (I) answers, P@N score for list questions, for all submitted CLEF 2006 runs.</figDesc><table coords="8,208.97,376.32,185.06,61.34"><row><cell>Run</cell><cell>R W X U</cell><cell cols="2">I P@N</cell></row><row><cell cols="2">EN→IT 1 12 63 1 6</cell><cell>0</cell><cell>0.10</cell></row><row><cell cols="2">EN→IT 2 15 70 1 7</cell><cell>0</cell><cell>0.15</cell></row><row><cell>EN→NL 1</cell><cell cols="2">6 29 0 7 24</cell><cell>0.18</cell></row><row><cell>EN→NL 2</cell><cell cols="2">9 40 0 5 42</cell><cell>0.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,90.00,602.22,423.00,128.43"><head>Table 3 :</head><label>3</label><figDesc>Answer translation accuracy, divided over answer types (EN→NL). Quantified over the first translated answer, disregarding whether the answer was correct or not, but only for questions to which a correct expected answer type was assigned.</figDesc><table coords="8,181.56,647.80,239.88,82.86"><row><cell cols="4">Expacted Answer Type Correct Wrong Accuracy</cell></row><row><cell>location</cell><cell>30</cell><cell>0</cell><cell>100%</cell></row><row><cell>numeric</cell><cell>9</cell><cell>2</cell><cell>82%</cell></row><row><cell>measure</cell><cell>4</cell><cell>2</cell><cell>67%</cell></row><row><cell>instance (names)</cell><cell>14</cell><cell>5</cell><cell>74%</cell></row><row><cell>time</cell><cell>15</cell><cell>0</cell><cell>100%</cell></row><row><cell>other</cell><cell>7</cell><cell>1</cell><cell>88%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,718.61,407.76,6.99;2,90.00,728.08,423.00,6.99;2,90.00,737.54,403.38,6.99"><p>To be more precise, these were BU→EN (3x), BU→FR, DE→EN (4x), DE→FR, EN→DE (3x), EN→ES (2x), EN→FR, EN→NL, EN→PT, ES→EN (2x), ES→FR, FI→EN (2x), FR→EN (8x), IT→EN (3x), IT→ES, IT→FR (2x), NL→EN, NL→FR, PT→FR (2x), and IN→EN (Information gathered from the working notes of CLEF</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2003" xml:id="foot_1" coords="2,90.00,747.00,36.80,6.99"><p><ref type="bibr" coords="2,90.00,747.00,8.47,6.99" target="#b4">[5]</ref>, CLEF</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2004" xml:id="foot_2" coords="2,149.38,747.00,50.91,6.99"><p><ref type="bibr" coords="2,149.38,747.00,8.93,6.99" target="#b6">[7]</ref> and CLEF</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2005" xml:id="foot_3" coords="2,222.88,747.00,14.58,6.99"><p><ref type="bibr" coords="2,222.88,747.00,8.75,6.99" target="#b5">[6]</ref>).</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,105.50,519.16,407.50,8.74;9,105.50,531.12,407.51,8.74;9,105.50,543.07,306.74,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,403.55,519.16,109.45,8.74;9,105.50,531.12,157.13,8.74">Wide-Coverage Semantic Representations from a CCG Parser</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,285.01,531.12,227.99,8.74;9,105.50,543.07,181.69,8.74">Proceedings of the 20th International Conference on Computational Linguistics (COLING &apos;04)</title>
		<meeting>the 20th International Conference on Computational Linguistics (COLING &apos;04)<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,561.52,407.50,8.74;9,105.50,573.47,326.47,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,159.52,561.52,204.08,8.74">Towards wide-coverage semantic interpretation</title>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,386.06,561.52,126.94,8.74;9,105.50,573.47,236.87,8.74">Proceedings of Sixth International Workshop on Computational Semantics IWCS-6</title>
		<meeting>Sixth International Workshop on Computational Semantics IWCS-6</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="42" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,591.92,407.50,8.74;9,105.50,603.88,407.51,8.74;9,105.50,615.83,102.29,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,219.87,591.92,223.45,8.74">Parsing the WSJ using CCG and Log-Linear Models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,463.34,591.92,49.66,8.74;9,105.50,603.88,403.49,8.74">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL &apos;04)</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics (ACL &apos;04)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,634.28,407.50,8.74;9,105.50,646.23,312.71,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,209.59,634.28,303.41,8.74;9,105.50,646.23,192.78,8.74">From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Reyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,664.68,407.50,8.74;9,105.50,676.63,382.02,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,188.53,664.68,319.33,8.74">Results of the CLEF 2003 Cross-Language System Evaluation Campaign</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,105.50,676.63,196.10,8.74">Working Notes for the CLEF 2003 Workshop</title>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08-22">21-22 August 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,695.08,407.50,8.74;9,105.50,707.04,380.09,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,188.53,695.08,319.33,8.74">Results of the CLEF 2005 Cross-Language System Evaluation Campaign</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,105.50,707.04,196.10,8.74">Working Notes for the CLEF 2005 Workshop</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09-23">21-23 September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,725.48,407.49,8.74;9,105.50,737.44,403.48,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,245.19,725.48,267.81,8.74;9,105.50,737.44,41.19,8.74">Results of the CLEF 2004 Cross-Language System Evaluation Campaign</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Borri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,155.40,737.44,196.09,8.74">Working Notes for the CLEF 2004 Workshop</title>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09-17">15-17 September 2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
