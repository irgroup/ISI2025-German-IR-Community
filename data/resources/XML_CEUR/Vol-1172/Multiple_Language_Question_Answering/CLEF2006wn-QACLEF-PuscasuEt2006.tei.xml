<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,92.53,146.21,417.93,18.08;1,134.38,168.13,334.23,18.08">Developing a Question Answering System for the Romanian-English Track at CLEF 2006</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,95.98,203.19,82.82,10.46;1,178.79,202.11,1.95,7.32"><forename type="first">Georgiana</forename><surname>Puşcaşu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Research Group in Computational Linguistics</orgName>
								<orgName type="institution">University of Wolverhampton</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,196.90,203.19,58.43,10.46;1,255.32,202.11,1.83,7.32"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
						</author>
						<author>
							<persName coords="1,265.56,203.19,51.67,10.46;1,317.23,202.11,3.65,7.32;1,327.47,203.19,79.05,10.46"><roleName>Ionut ¸Pistol †</roleName><forename type="first">Diana</forename><surname>Trandabȃt ¸ †</surname></persName>
						</author>
						<author>
							<persName coords="1,412.59,203.19,42.83,10.46;1,455.41,202.11,1.36,7.32"><forename type="first">Dan</forename><surname>Tufiş</surname></persName>
							<email>tufis@racai.ro</email>
							<affiliation key="aff1">
								<orgName type="department">Centre for Artificial Intelligence Romanian Academy</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,466.07,203.19,53.48,10.46;1,519.55,202.11,1.36,7.32"><forename type="first">Alin</forename><surname>Ceauşu</surname></persName>
							<email>alceausu@racai.ro</email>
							<affiliation key="aff1">
								<orgName type="department">Centre for Artificial Intelligence Romanian Academy</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,101.85,217.13,67.17,10.46;1,169.01,216.07,1.36,7.32"><forename type="first">Dan</forename><forename type="middle">S</forename><surname>¸tefȃnescu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Artificial Intelligence Romanian Academy</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,179.67,217.13,40.82,10.46;1,220.50,216.07,1.36,7.32"><forename type="first">Radu</forename><surname>Ion</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Artificial Intelligence Romanian Academy</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.16,217.13,82.35,10.46;1,313.50,216.07,1.83,7.32"><forename type="first">Constantin</forename><surname>Orȃsan</surname></persName>
							<email>c.orasan@wlv.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Research Group in Computational Linguistics</orgName>
								<orgName type="institution">University of Wolverhampton</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.73,217.13,68.91,10.46;1,392.64,216.07,1.83,7.32"><forename type="first">Iustin</forename><surname>Dornescu</surname></persName>
						</author>
						<author>
							<persName coords="1,402.88,217.13,51.23,10.46;1,454.11,216.07,1.83,7.32"><forename type="first">Alex</forename><surname>Moruz</surname></persName>
						</author>
						<author>
							<persName coords="1,464.34,217.13,52.52,10.46;1,516.87,216.07,1.83,7.32"><forename type="first">Dan</forename><surname>Cristea</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Software and Computing Systems</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,92.53,146.21,417.93,18.08;1,134.38,168.13,334.23,18.08">Developing a Question Answering System for the Romanian-English Track at CLEF 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A970068640508D2461EE986D16DFDF23</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>I.2 [Artificial Intelligence]: I.2.7 Natural Language Processing Measurement, Performance, Experimentation Question Answering, Cross-lingual Question Answering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the development of a question answering system for the Romanian-English cross-lingual track organized within the Cross Lingual Evaluation Forum (CLEF) 2006 campaign. The development stages of our cross-lingual Question Answering (QA) system are described incrementally throughout the paper, at the same time pinpointing the problems that occurred and the way they were addressed. Our system adheres to the classical architecture for QA systems, debuting with question processing followed, after term translation, by information retrieval and answer extraction. Besides the common QA difficulties, the track posed some specific problems, such as the lack of a reliable translation engine from Romanian to English, and the need to evaluate each module individually for a better insight into the system's failures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question Answering can be defined as the task which takes a question in natural language and produces one or more ranked answers from a collection of documents. The QA research area has really emerged as a result of the introduction of a monolingual English QA track within the Text Retrieval and Evaluation Conference (TREC). Multilingual, cross-lingual, as well as monolingual QA for languages other than English are addressed at a scientific level by the CLEF evaluation campaigns <ref type="bibr" coords="2,138.75,192.14,9.96,10.46" target="#b6">[7]</ref>.</p><p>This year our team<ref type="foot" coords="2,185.33,203.01,3.97,7.32" target="#foot_0">1</ref> of students, PhD students and researchers, have taken part for the first time in the QA@CLEF competition. As in every other cross-lingual task, the system input consisted of 200 questions in Romanian, and the output was expected to be the exact English answer. As this was the first time the Romanian-English track was introduced at CLEF, most of the effort was directed towards the development of a fully functional cross-lingual QA system having as source language Romanian, and less on fine-tuning the system to maximize the results.</p><p>Following the generic architecture for QA systems <ref type="bibr" coords="2,322.69,275.82,9.96,10.46" target="#b1">[2]</ref>, our system contains a question analyzer, an information retrieval engine and an answer extraction module, as well as a cross-lingual QA specific module which translates the relevant question terms from Romanian into English.</p><p>This paper describes the steps taken in the development of our cross-lingual QA system for the Romanian-English track at CLEF 2006, as well as its evaluation. The remainder of the paper is structured as follows: Section 2 provides a description of the system and its embedded modules, while Section 3 presents the details of the submitted runs. Section 4 comprises an analysis of the results obtained, and finally, in Section 5, conclusions are drawn and future directions of system development are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description 2.1 System Overview</head><p>Question Answering systems normally adhere to the pipeline architecture that consists of three main stages: question analysis, paragraph retrieval and answer extraction <ref type="bibr" coords="2,408.76,458.56,9.96,10.46" target="#b1">[2]</ref>. Specific systems can be seen as instantiations of the general architecture, with particular choices being made concerning representation and processing for each component of the overall model. The first stage is question analysis. The input to this stage is a natural language question and the output is one or more representations of the question to be used in subsequent stages. At this stage most systems identify the semantic type of the entity sought by the question, determine additional constraints on the answer entity, and extract the keywords to be employed at the passage retrieval stage. The next stage, passage retrieval, is typically achieved by employing a conventional IR search engine to select a set of relevant candidate passages from the text collection. At the last stage, answer extraction and ranking, the representation of the question and the representation of the candidate answerbearing passages are compared and a set of candidate answers is produced, ranked according to how likely they constitute the correct answer. Apart from the typical modules included in a QA system, our system also includes a module which translates terms from Romanian into English, in order to perform the cross-lingual transfer. The system architecture and functionality are illustrated in Figure <ref type="figure" coords="2,180.74,625.94,3.87,10.46" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NLP Pre-processing</head><p>The questions are first morpho-syntactically pre-processed using the Romanian POS tagger developed at RACAI <ref type="bibr" coords="2,178.10,684.18,9.96,10.46" target="#b7">[8]</ref>. Afterwards, a pattern-based Named Entity Recognizer is employed to Similar pre-processing operations are carried out on the English document collection, after segmenting it at sentence level. We used the same set of tools with a different language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Question Analysis</head><p>This stage is mainly concerned with the identification of the semantic type of the entity sought by the question (expected answer type). In addition it also identifies the question focus, the question type and the set of keywords relevant for the question. To achieve these goals, our question analyzer performed the following steps:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a) NP-chunking, Named Entity extraction, Temporal Expression identification</head><p>The input at this stage consists of the pre-processed set of questions. On the basis of the morpho-syntactic annotation provided by our Romanian POS-tagger, we have implemented a rule-based shallow noun phrase identifier. The Named Entity recognizer employed at the preprocessing stage provides us with the set of question Named Entities. Temporal expressions (TEs) are also identified using the adaptation to Romanian of the TE identifier and normaliser developed by <ref type="bibr" coords="3,164.22,620.80,9.96,10.46" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b) Question focus identification</head><p>The question focus is the word or word sequence that defines or disambiguates the question, in the sense that it pinpoints what the question is searching for or what it is about. We considered the question focus to be either the noun determined by the question stem (as in What country ) or the head noun of the first question NP if this NP comes before the main verb of the question or if it follows the verb "to be".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c) Distinguishing the expected answer type</head><p>At this stage we identify the category of the entity expected as an answer to the analyzed question. Our system's answer type taxonomy distinguishes the following classes: PERSON, LOCATION, ORGANIZATION, TEMPORAL, NUMERIC, DEFINITION and GENERIC.</p><p>The assignment of a class to an analyzed question is performed using the question stem and the type of the question focus. The question focus type is detected using the WordNet <ref type="bibr" coords="4,502.48,134.45,10.52,10.46" target="#b0">[1]</ref> sub-hierarchies specific to the categories PERSON / LOCATION / ORGANIZATION. We manually attach to each category a set of ILI numbers representing the root nodes of the WordNet sub-trees containing category-specific nouns. Using the ILI numbers, we extract from the Romanian WordNet <ref type="bibr" coords="4,238.63,182.26,10.52,10.46" target="#b8">[9]</ref> lists of nouns for each of the three categories. In the case of ambiguous question stems (e.g. What), we search in the resulted lists for the head of the question focus, and identify as expected answer type the category of the corresponding list (for example, in the case of the question In which city was Vladislav Listyev murdered?, the question focus is city, noun which appears in the LOCATION list, therefore the associated expected answer type is LOCATION).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d) Inferring the question type</head><p>This year, the QA@CLEF competition has distinguished among four types of questions: factoid, definition, list and temporally restricted questions. As temporal restrictions can constrain any type of question, we proceed by first detecting whether the question has the type factoid, definition or list and then test the existence of temporal restrictions. The question type is identified using two simple rules: if the expected answer type is DEFINITION, then obviously the question type is definition; if the question focus is a plural noun, then the question type is list, otherwise factoid. The temporal restrictions are identified using several patterns and the information provided by the TE identifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e) Keyword set generation</head><p>The set of keywords is automatically generated by listing the important question terms in decreasing order of their relevance. Therefore, the set of keywords comprises: the question focus, the identified NEs and TEs, the remaining noun phrases, and all the non-auxiliary verbs present in the question. This set is then passed on to the Term Translation module, in order to obtain English keywords for paragraph retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Term Translation</head><p>In order to achieve term translation we have employed WordNet, a resource available both for English and Romanian. The set of keywords extracted at the question analysis stage served as input for the term translation stage, therefore we were supposed to translate both noun phrases and verbs. The noun phrases are translated in a different manner than the verbs. The NP words are translated individually, by first identifying the Romanian synsets containing the word, by reaching through the ILI numbers the corresponding English synsets, and by forming a set of all possible translations. From this set, using empirical rules based on word frequency, we choose a maximum of three candidates. If the word to be translated does not appear in the Romanian WordNet, as it was quite frequently the case, we search for it in other available dictionaries and preserve the first three translations. If still no translations are found, we consider the word itself as translation. After each individual word is translated, we employ rules that translate the Romanian syntax into English syntax. In this manner we obtain for each NP several translation equivalents.</p><p>In the case of verbs, we extract for each verb the translation equivalents from WordNet as we did for nouns. Attempts to apply the frequency-based methodology used in the case of nouns failed due to the fact that some very general verbs were preferred instead of the correct translation equivalent. To this end, we decided to select the best translation equivalent by considering both the frequency of the verb and of the nouns which appear in the subject and object positions. To achieve this, the verb-noun co-occurrence data described in <ref type="bibr" coords="4,351.46,686.83,10.52,10.46" target="#b4">[5]</ref> has been used to determine which of the verb translation equivalents co-occur more often with the translated nouns, and was selected as the translation. Despite the simplicity of the method, the accuracy of the translation improved dramatically over the row frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Index Creation and Paragraph Retrieval</head><p>As described in section 2.2, the English corpus was initially preprocessed using tokenization, lemmatization, POS-tagging and NE recognition tools. In our runs, we employed an index/search engine based on the Lucene <ref type="bibr" coords="5,213.62,152.84,10.52,10.46" target="#b3">[4]</ref> search engine.</p><p>The document collection was indexed both at document, as well as paragraph level, using the lemmas of the content words and the NE classes (MEASURE, PERSON, LOCATION, etc). For a query like: the search engine searches for a segment (document/paragraph) that contains a measure and words belonging to the set of translation equivalents. The question type is used to approximate the maximum number of hits to be returned. When no paragraphs are returned for a given query, we employ two strategies: we either increase the granularity of the segments from paragraphs to documents, or we reformulate the query using for individual words spelling variations and variable word distance for phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Answer Extraction</head><p>Two answer extraction modules have been developed, one by UAIC and another one by RACAI. Both modules require as input the expected answer type, the question focus, the set of keywords and the retrieved snippets together with their POS, lemma and NE information, and the relevance score returned by Lucene. The extraction process depends on whether the expected answer type is a Named Entity or not. When the answer type is a Named Entity, the answer extraction module identifies within each retrieved sentence Named Entities with the desired answer type. Therefore, in this case, the answer extraction process is greatly dependent on the results of the NE recognition module. When the answer type is not a Named Entity, the extraction process mainly relies on the recognition of the question focus, as in this case the syntactic answer patterns based on the focus are crucial. Both modules use the same pattern-based extractor to address DEFINITION questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a) Answering questions asking about Named Entities</head><p>When the question asks about a Named Entity such as MEASURE, PERSON, LOCATION, ORGANIZATION, DATE, the UAIC answer extractor looks for all the expressions tagged with the desired answer type. If several such expressions exist, we choose the closest to the focus in terms of word distance, if the focus is present in the sentence, otherwise the first one occurring in the analyzed sentence. When there is no named entity of the desired type, we generalize the search using synonyms of the focus extracted from WordNet.</p><p>The RACAI answer extractor computes scores for each snippet based on whether a snippet contains the focus, on the percentage of keywords or keyword synonyms present in the snippet, and on the snippet/document relevance scores provided by the search engine. The entities having the desired answer type are identified and added to the set of candidate answers. For each candidate answer, another score is computed on the basis of the snippet score, the distance to the question focus and to other keywords. The candidate answers having scores above a certain threshold are presented as final answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b) Answering questions looking for GENERIC answers</head><p>When the expected answer type is not a Named Entity (the question has the GENERIC answer type), the UAIC answer extractor locates the answer within the candidate sentence using syntactic patterns. The syntactic patterns for identifying the answer include the focus noun phrase and the answer noun phrase, which can be connected by other elements such as comma, quotation marks, prepositions or even verbs. These syntactic patterns always include the focus of the question. Therefore, the focus has to be determined by the question analysis module in order to enable the system to identify answers consisting of a noun or verb phrase.</p><p>The RACAI answer extractor employs also patterns to select answer candidates. These candidates are then ranked using a score similar to the one employed for answering questions asking about NEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c) Answering DEFINITION questions</head><p>In the case of DEFINITION questions, the candidate paragraphs are matched against a set of patterns. Each possible definition is extracted and added to a set of candidate answers, together with a score revealing the quality of the pattern it matched. The set of noun phrases present in the paragraphs are also investigated to detect those NPs containing the term to be defined surrounded by other words (this operation is motivated by cases like the Atlantis space shuttle, where the correct definition for Atlantis is space shuttle). The selected NPs are added to the set of candidate answers with a low score. Then, the set of candidate answers is ordered according to the score attached to each answer and to the number of other candidate answers it subsumes. The highest ranked candidate answers are presented as final answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Description of Submitted Runs</head><p>We have submitted three different runs, with the following detailed description:</p><p>• UAIC This run was obtained by parsing and analyzing the questions, translating the keywords, retrieving relevant passages and finding the final answers using the UAIC answer extractor.</p><p>• RACAI This run is also obtained by parsing and analyzing the questions, keyword translations, passage retrieval, but the answers were localized using the RACAI answer extractor.</p><p>• DIOGENE Our third run, which unfortunately was not evaluated, was obtained by converting the results of our Question Analysis and Term Translation modules to the format required by the DIOGENE QA system <ref type="bibr" coords="6,154.95,514.00,9.96,10.46" target="#b2">[3]</ref>, and then providing them as input to the DIOGENE IR and Answer Extraction modules.</p><p>Due to the high number of submitted runs having English as target language only the UAIC and RACAI runs were evaluated. In the remainder of this paper, the RACAI run will be referred to as System 1, and the name System 2 will be used for the UAIC run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">General Evaluation Results</head><p>The official evaluation results of Systems 1 and 2 are presented in Figure <ref type="figure" coords="6,401.17,646.92,3.87,10.46" target="#fig_1">2</ref>. Each submitted answer was evaluated as UNKNOWN, CORRECT, UNSUPPORTED, INCORRECT and INEXACT.</p><p>The high number of answers evaluated as UNKNOWN is due to the fact that we provided ten answers for almost all 200 questions, as ten was the maximum number of answers allowed. However, the final evaluation evaluated only the first answer for the majority of the questions (only in the case of list type questions the first three answers were evaluated). As a result of this, the answers ranked 2 to 10 were labelled as UNKNOWN indicating that no attempt was made to check their correctness. From our internal evaluation, we have noticed that the correct answers were found in the first ten answers generated by our systems for 35-40% of the questions. This Most of the UNSUPPORTED and INEXACT answers are caused by the lack of a perfect match between the answer string and the supporting snippet. The majority of these errors can be solved with the use of a knowledge base, that will allow, for example, the identification of Atlantis as a space shuttle (question 101). Also, sometimes our systems provide a more generic, or more specific answer than required, as it is the case for the gold answer Shoemaker contrasted to our answer Carolyn Shoemaker, marked as UNSUPPORTED. These errors could be corrected by improving the answer extractor with more specific rules as to the format of the required answer. Another type of error causing UNSUPPORTED and INEXACT answers are the list type questions. We provided one answer per line, even if the question was of the type list. Most of the correct answers were found in the 10 answers provided, but not on the same line. These errors could be solved with a better classification of the returned answers and by grouping them according to the characteristics of the required list. There are also cases when the answer is marked as UNSUPPORTED, but denotes the same entity as the gold answer. The answer appears in the indicated snippet, but the character string is not identical with the gold answer (for example System 2 identifies the answer United States, but is marked as UNSUPPORTED because the related snippet for this answer contains United States). These snippet changes are introduced at the pre-processing stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detailed Comparison between the two Evaluated Runs</head><p>The two runs/systems chosen for the official evaluation are the two completely designed and implemented by the two Romanian groups involved, UAIC and RACAI. These two runs were chosen from the set of three submitted not because they provide better scores, but because their source code and the knowledge about their original implementation can be easily accessed. As a result of this we expect to be able to significantly improve them in the future.</p><p>The performance of the two systems is compared below for each answer type separately, as the answer extraction methodology differs from one answer type to another. In the following graphs, R 1 designates the normalized percentage of CORRECT answers, U 1 of UNSUPPORTED answers, W 1 of WRONG answers, X 1 of INEXACT answers, and Z 1 of unevaluated answers. Different colors correspond to different answer types (F=factoid, T=temporally restricted, D=definition, L=list). In the case of the PERSON answer type, System 1 has correctly answered a number of factoid and temporally restricted questions, while System 2 only detected correct answers for a small number of temporally restricted questions. There are many cases when the correct answer is retrieved, but not returned as the first answer. This is case of question 13 (where the gold answer Melvyn Percy appears ranked third and fifth), and question 80 the gold answer Anthony Busuttil appears ranked fifth. The improvement of both answer extraction modules and of the NER module employed at pre-processing need to be seriously addressed for a future CLEF participation.</p><p>Figure <ref type="figure" coords="8,196.64,301.78,3.87,10.46">4</ref>: System comparison for the LOCATION answer type System 1 also performs better than System 2 for questions with the LOCATION answer type. Therefore, we need to ensure that our answer extractor preserves the answer format as it appears in the related snippet. The best results have been obtained for the list questions asking about LOCATIONs (30% of the LOCATION list questions were correctly answered by System 1, while the LOCATION list questions were evaluated as UNSUPPORTED in the case of System 2). For a number of questions asking for LOCATIONs, due to the snippet pre-processing stage, answers detected correctly by System 2 are marked as UNSUPPORTED because the indicated snippet contains an " instead of a whitespace. For questions asking about ORGANIZATIONs, System 2 achieves better results than System 1, but still quite low. Both systems fail, mainly because our NE Recognizer does not identify Named Entities of the type ORGANIZATION. Therefore, when our search engine looks for an entity of this type, it is obviously impossible for it to retrieve any snippets containing such entities. Figure <ref type="figure" coords="8,188.82,709.24,3.87,10.46">6</ref>: System comparison for the DATE expected answer type By analyzing Figure <ref type="figure" coords="8,197.33,732.21,3.87,10.46">6</ref>, we deduce that the questions asking about a DATE and being at the same time temporally restricted were the most difficult questions to process. Both systems managed to find correct answers only for the factoid DATE questions, but no correct answer was found for temporally restricted DATE questions. In the case of System 1, most of the temporally restricted DATE questions were considered UNSUPPORTED by the provided snippet. Also, we can see a large number of UNSUPPORTED answers for System 1, possibly due to truncating the extracted paragraphs to comply with the 500-byte limit, and leaving the answer in the truncated part. In any case, the improvement of the answer extractor is required with respect to the identification of the correct extent of the required answer. System 2 answered correctly less factoid DATE questions in comparison with System 1. In the future, we intend to perform a temporal pre-annotation of the corpus in order to facilitate the search for temporal expressions and to be able to provide an answer at the granularity required by the question. For the MEASURE answer type, our answer extractors mainly fail due to errors in the annotation introduced at the pre-processing stage. For a number of questions, we correctly select the snippets (the gold answers are in these snippets), but we give wrong answers due to wrong annotation. For the GENERIC answer type, both systems achieve good results for list questions, but, in the case of factoid questions, most of the returned answers are wrong. Figure <ref type="figure" coords="9,266.79,581.97,4.98,10.46" target="#fig_5">8</ref> shows that System 1 has significantly more correct list and definition answers, unlike System 2.</p><p>As a general overview, we may say that System 1 outperforms System 2 for most answer types. For the next competition, we will try to exploit the beneficial features of each system, at the same time improving each of the developed modules for an increased performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper describes the development stages of our cross-lingual Romanian-English QA system, as well as our participation in the QA@CLEF campaign. We have developed a basic QA system that is able to retrieve answers to Romanian questions in a collection of English documents.</p><p>Adhering to the generic QA system architecture, our system implements the three essential stages (question analysis, information retrieval and answer extraction), as well as a cross-lingual QA specific module which translates the relevant question terms from Romanian into English.</p><p>Our participation in the QA@CLEF competition included three runs. Two runs were obtained by analyzing the questions, translating the keywords, retrieving relevant passages and finding the final answers using two different answer extractors. Our third run, which unfortunately was not evaluated, was provided by the DIOGENE QA system on the basis of the output given by our Question Analysis and Term Translation modules.</p><p>The results are poor for both evaluated runs, but we declare ourselves satisfied with the fact that we have managed to develop a fully functional cross-lingual QA system and that we have learned several important lessons for our future participations.</p><p>A detailed result analysis has revealed a number of major system improvement directions. The term translation module, as a key stage for the performance of any cross-lingual QA system, is our first improvement target. However, the answer extraction module is the one we will dedicate most of our attention in order to increase its accuracy. An improved answer ranking method for the candidate answers will also form part of our priorities.</p><p>The Romanian team's debut in the CLEF competition has enriched us with the experience of developing the first Romanian-English cross-lingual QA system, at the same time setting the scene for subsequent CLEF participations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,195.02,377.73,212.96,10.46;3,131.50,109.48,339.63,254.72"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System Architecture and Functionality</figDesc><graphic coords="3,131.50,109.48,339.63,254.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,181.39,230.87,240.22,10.46;7,103.21,109.70,396.41,107.58"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Evaluation results for the two evaluated runs</figDesc><graphic coords="7,103.21,109.70,396.41,107.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,171.30,739.32,260.41,10.46;7,131.48,643.04,339.73,82.77"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: System comparison for the PERSON answer type</figDesc><graphic coords="7,131.48,643.04,339.73,82.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,152.16,525.96,298.68,10.46;8,131.48,430.16,339.72,82.29"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: System comparison for the ORGANIZATION answer type</figDesc><graphic coords="8,131.48,430.16,339.72,82.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,166.87,336.14,269.27,10.46;9,131.48,240.43,339.72,82.20"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: System comparison for the MEASURE answer type</figDesc><graphic coords="9,131.48,240.43,339.72,82.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,183.14,520.64,236.71,10.46;9,104.94,546.10,408.08,10.46;9,90.00,558.06,423.01,10.46;9,90.01,570.01,422.99,10.46;9,90.00,581.97,423.01,10.46;9,90.00,593.92,175.89,10.46;9,104.94,605.87,408.06,10.46;9,90.00,617.84,423.01,10.46;9,90.01,629.79,334.82,10.46;9,131.48,415.88,339.72,91.25"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: System comparison for GENERIC questions All the questions that didn't fit in the previous categories (PERSON, LOCATION, ORGANI-ZATION, DATE nor MEASURE) are classified as GENERIC.For the GENERIC answer type, both systems achieve good results for list questions, but, in the case of factoid questions, most of the returned answers are wrong. Figure8shows that System 1 has significantly more correct list and definition answers, unlike System 2.As a general overview, we may say that System 1 outperforms System 2 for most answer types. For the next competition, we will try to exploit the beneficial features of each system, at the same time improving each of the developed modules for an increased performance.</figDesc><graphic coords="9,131.48,415.88,339.72,91.25" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,703.98,407.87,8.37;2,90.00,713.45,423.11,8.37;2,90.01,722.92,423.10,8.37;2,90.00,732.38,271.32,8.37"><p>The team that participated in the development of the system included students, PhD students and researchers from the Faculty of Computer Science at the "Alexandru Ioan Cuza" University, Iasi, Romania (furtherly addressed as UAIC), from the Centre for Artificial Intelligence of the Romanian Academy -(RACAI), from the University of Alicante, Spain, and the University of Wolverhampton, United Kingdom.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>The authors would like to thank the following members of the <rs type="institution">UAIC</rs> team: <rs type="person">Cornel Bârnȃ</rs>, <rs type="person">Corina Forȃscu</rs>, <rs type="person">Maria Husarciuc</rs>, <rs type="person">Mȃdȃlina Ionit</rs> ¸ȃ, <rs type="person">Ana Masalagiu</rs>, <rs type="person">Gabriela Mogoş</rs>, <rs type="person">Gabriel Negarȃ</rs>, <rs type="person">Marius Rȃschip</rs>, for their help and support at different stages of system development.</p><p>Special acknowledgements are addressed to <rs type="person">Milen Kouylekov</rs> and <rs type="person">Bernardo Magnini</rs> for their offer to process the output of our question processor and term translation module with the IR and answer extraction modules included in DIOGENE, and for providing us with the DIOGENE run.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,105.50,459.17,407.50,10.46" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,232.44,459.17,176.09,10.46">WordNet: An Eletronic Lexical Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,479.10,407.51,10.46;10,105.50,491.05,407.51,10.46;10,105.50,503.01,22.69,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,275.59,479.10,83.51,10.46">Question answering</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,483.82,479.10,29.19,10.46;10,105.50,491.05,168.93,10.46">Oxford Handbook of Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Mitkov</surname></persName>
		</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="560" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,522.93,407.51,10.46;10,105.50,534.89,382.04,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,338.13,522.93,174.88,10.46;10,105.50,534.89,46.39,10.46">ITC-irst at TREC-2003: the DIOGENE QA system</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kouylekov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,173.40,534.89,282.83,10.46">Proceedings of the Twelvth Text Retrieval Conference (TREC-12)</title>
		<meeting>the Twelvth Text Retrieval Conference (TREC-12)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,554.82,210.81,10.46" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Lucene</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org/java/docs/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,574.74,407.51,10.46;10,105.50,586.70,407.50,10.46;10,105.50,598.65,90.64,10.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,273.22,574.74,239.79,10.46;10,105.50,586.70,35.27,10.46">Feature weighting for cooccurrence-based classification of words</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pekar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krkoska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Staab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,164.52,586.70,348.49,10.46;10,105.50,598.65,59.25,10.46">Proceedings of the 20th International Conference on Computational Linguistics (COLING-04)</title>
		<meeting>the 20th International Conference on Computational Linguistics (COLING-04)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,618.58,407.50,10.46;10,105.50,630.53,244.11,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,162.22,618.58,167.55,10.46">A Framework for Temporal Resolution</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Puscasu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,351.61,618.58,161.39,10.46;10,105.50,630.53,212.76,10.46">Proceedings of the 4th Conference on Language Resources and Evaluation (LREC2004)</title>
		<meeting>the 4th Conference on Language Resources and Evaluation (LREC2004)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,650.46,246.28,10.46" xml:id="b6">
	<monogr>
		<ptr target="http://clef-qa.itc.it/CLEF-2006.html" />
		<title level="m" coord="10,105.50,650.46,45.89,10.46">QA@CLEF</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,670.38,407.50,10.46;10,105.50,682.33,379.24,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,147.91,670.38,265.65,10.46">Tagging with Combined Language Models and Large Tagsets</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tufis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,435.00,670.38,78.00,10.46;10,105.50,682.33,344.24,10.46">Proceedings of the TELRI International Seminar on &quot;Text Corpora and Multilingual Lexicography</title>
		<meeting>the TELRI International Seminar on &quot;Text Corpora and Multilingual Lexicography</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,702.26,407.52,10.46;10,105.50,714.21,407.50,10.46;10,105.50,726.18,288.33,10.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,276.99,702.26,236.03,10.46;10,105.50,714.21,89.53,10.46">BalkaNet: Aims, Methods, Results and Perspectives. A General Overview</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tufis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cristea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Stamou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,300.30,714.21,212.70,10.46;10,105.50,726.18,45.83,10.46">Romanian Journal on Information Science and Technology</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Tufis</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Romanian Academy</publisher>
		</imprint>
	</monogr>
	<note>Special Issue on BalkaNet</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
