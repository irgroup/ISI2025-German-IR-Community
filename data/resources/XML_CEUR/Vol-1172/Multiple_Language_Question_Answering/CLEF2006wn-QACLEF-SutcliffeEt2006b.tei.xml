<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,70.92,74.45,453.66,12.64;1,239.04,91.01,117.30,12.64">Identifying Novel Information using Latent Semantic Analysis in the WiQA Task at CLEF 2006</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,170.28,114.68,90.95,8.96"><forename type="first">Richard</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
						</author>
						<author>
							<persName coords="1,273.24,114.68,72.42,8.96"><forename type="first">Josef</forename><surname>Steinberger#</surname></persName>
						</author>
						<author>
							<persName coords="1,352.32,114.68,63.88,8.96"><forename type="first">Udo</forename><surname>Kruschwitz</surname></persName>
						</author>
						<author>
							<persName coords="1,199.44,126.44,114.26,8.96"><forename type="first">Mijail</forename><surname>Alexandrov-Kabadjov</surname></persName>
						</author>
						<author>
							<persName coords="1,325.44,126.44,66.09,8.96"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
						</author>
						<author>
							<persName coords="2,214.08,88.40,59.86,8.96"><forename type="first">Carolyn</forename><surname>Keene</surname></persName>
						</author>
						<author>
							<persName coords="2,214.08,224.24,74.60,8.96"><forename type="first">Leslie</forename><surname>Mcfarlane</surname></persName>
						</author>
						<author>
							<persName coords="2,302.52,224.24,110.20,8.96"><forename type="first">James</forename><forename type="middle">Duncan</forename><surname>Lawrence</surname></persName>
						</author>
						<author>
							<persName coords="2,426.48,224.24,25.98,8.96;2,214.08,236.00,30.74,8.96"><forename type="first">Nancy</forename><surname>Axelrod</surname></persName>
						</author>
						<author>
							<persName coords="2,256.68,236.00,56.40,8.96"><forename type="first">Priscilla</forename><surname>Doll</surname></persName>
						</author>
						<author>
							<persName coords="2,324.48,236.00,62.55,8.96"><forename type="first">Charles</forename><surname>Strong</surname></persName>
						</author>
						<author>
							<persName coords="2,398.52,236.00,49.63,8.96"><forename type="first">Alma</forename><surname>Sasse</surname></persName>
						</author>
						<author>
							<persName coords="2,214.08,247.76,77.85,8.96"><forename type="first">Wilhelmina</forename><surname>Rankin</surname></persName>
						</author>
						<author>
							<persName coords="2,300.84,247.76,71.57,8.96"><forename type="first">George</forename><surname>Waller</surname><genName>Jr</genName></persName>
						</author>
						<author>
							<persName coords="2,382.92,247.76,65.27,8.96"><forename type="first">Margaret</forename><surname>Scherf</surname></persName>
						</author>
						<author>
							<persName coords="2,233.64,259.52,54.30,8.96"><forename type="first">Susan</forename><surname>Wittig</surname></persName>
						</author>
						<author>
							<persName coords="2,359.28,259.52,54.82,8.96"><forename type="first">John</forename><surname>Keeline</surname></persName>
						</author>
						<author>
							<persName coords="2,214.08,361.28,61.71,8.96"><forename type="first">Harriet</forename><surname>Adams</surname></persName>
						</author>
						<author>
							<persName coords="2,308.28,361.28,81.44,8.96"><forename type="first">Harriet</forename><surname>Stratemeyer</surname></persName>
						</author>
						<author>
							<persName coords="2,297.96,373.04,78.42,8.96"><forename type="first">Franklin</forename><forename type="middle">W</forename><surname>Dixon</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Essex</orgName>
								<address>
									<addrLine>Wivenhoe Park</addrLine>
									<postCode>CO4 3SQ</postCode>
									<settlement>Colchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of West Bohemia</orgName>
								<address>
									<addrLine>Univerzitni 8</addrLine>
									<postCode>306 14</postCode>
									<settlement>Plzen</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,70.92,74.45,453.66,12.64;1,239.04,91.01,117.30,12.64">Identifying Novel Information using Latent Semantic Analysis in the WiQA Task at CLEF 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6321F99DED82BC6994C327140A69A7D8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval; I.2 [Artificial Intelligence]: I.2.7 Natural Language Processing Question Answering</term>
					<term>Latent Semantic Analysis</term>
					<term>Information Filtering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>From the perspective of WiQA, the Wikipedia can be considered as a set of articles each having a unique title. In the WiQA corpus articles are divided into sentences (snippets) each with its own identifier. Given a title, the task is to find snippets which are Important and Novel relative to the article. We indexed the corpus by sentence using Terrier. In our two-stage system, snippets were first retrieved if they contained an exact match with the title. Candidates were then passed to the Latent Semantic Analysis component which judged them Novel if they did not match the text of the article. The test data was varied -some articles were long, some short and indeed some were empty! We prepared a training collection of twenty topics and used this for tuning the system. During evaluation on 65 topics divided into categories Person, Location, Organization and None we submitted two runs. In the first, the ten best snippets were returned and in the second the twenty best. Run 1 was best with Average Yield per Topic 2.46 and Precision 0.37. We also studied performance on six different topic types: Person, Location, Organization and None (all specified in the corpus), Empty (no text) and Long (a lot of text). Precision results in Run 1 for Person and Organization were good (0.46 and 0.44) and were worst for Long (0.24). Compared to other groups, our performance was in the middle of the range except for Precision where our system was equal to the best. We attribute this to our use of exact title matches in the IR stage. We found that judging snippets Novel when preparing training data was fairly easy but that Important was subjective. In future work we will vary the approach used depending on the topic type, exploit co-references in conjunction with exact matches and make use of the elaborate hyperlink structure which is a unique and most interesting aspect of Wikipedia.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This article outlines an experiment in the use of Latent Semantic Analysis (LSA) for selecting information relevant to a topic. It was carried out within the Question Answering using Wikipedia (WiQA) Pilot Task which formed part of the Multiple Language Question Answering Track at the 2006 Cross Language Evaluation Forum (CLEF). We first define the WiQA task for this year. Following this is a brief outline of LSA and its previous application to Natural Language Processing (NLP) tasks. We then describe the development and tuning of our algorithm together with the system which implements it. The runs submitted and results obtained are then outlined. Finally, we draw conclusions for the project and present some directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The WiQA Task</head><p>The Wikipedia <ref type="bibr" coords="2,135.00,516.80,48.18,8.96">(Wiki, 2006</ref>) is a multilingual free-content encyclopaedia which is publicly accessible over the Internet. From the perspective of WiQA it can be viewed as a set of articles each with a unique title. During their preliminary work, the task organisers created an XML compliant corpus from the English Wikipedia articles <ref type="bibr" coords="2,70.56,552.08,126.31,8.96" target="#b4">(Denoyer and Gallinari, 2006)</ref>. The title of each article was assigned automatically to one of four subject categories PERSON, LOCATION, ORGANIZATION and NONE. At the same time the text of each article was split into separate sentences each with its own identifier. The complex hyperlink structure of the original Wikipedia is faithfully preserved in the corpus, although we did not use this in the present project.</p><p>The general aim of WiQA this year was to investigate methods of identifying information on a topic which is present somewhere in the Wikipedia but not included in the article specifically devoted to that topic. For example, there is an article entitled 'Johann Sebastian Bach' in the Wikipedia. The question WiQA sought to answer is this: What information on Bach is there within the Wikipedia other than in this article? The task was formalised by providing participants with a list of articles and requiring their systems to return for each a list of up to twenty sentences (henceforth called snippets) from other articles which they considered relevant to the article and yet not already included in it. There were 65 titles in the test set, divided among the categories PERSON, LOCATION, ORGANIZATION and NONE. Evaluation of each snippet was on the basis of whether it was supported (in the corpus), important to the topic, novel (not in the original article) and non-repeated (not mentioned in previously returned snippets for this topic). Evaluation of systems was mainly in terms of the snippets judged supported and important and novel and non-repeated within the first ten snippets returned by a system for each topic. A detailed description of the task and its associated evaluation measures can be found in the general article on the WiQA task in this volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Latent Semantic Analysis</head><p>Latent Semantic Indexing (LSI) was originally developed as an information retrieval technique <ref type="bibr" coords="3,473.76,131.00,51.09,8.96;3,70.56,142.76,201.19,8.96" target="#b3">(Deerwester, Dumais, Furnas, Landauer and Harshman, 1990)</ref>. A term-by-document matrix of dimensions t*d of the kind commonly used for inverted indexing in Information Retrieval (IR) is transformed by Singular Value Decomposition into a product of three matrices t*r, r*r and r*d. The r*r matrix is diagonal and contains the eponymous 'singular values' in such a way that the top left element is the most important and the bottom right element is the least important. Using the original r*r matrix and multiplying the three matrices together results exactly in the original t*d matrix. However, by using only the first n dimensions (1 &lt;= n &lt;= r) in the r*r matrix and setting the others to zero, it is possible to produce an approximation of the original which nevertheless captures the most important common aspects by giving them a common representation. In the original IR context, this meant that even if a word was not in a particular document it could be detected whether or not another word with similar meaning was present. Thus, LSI could be used to create representations of word senses automatically.</p><p>In abstract terms, LSI can be viewed as a method of identifying 'hidden' commonalities between documents on the basis of the terms they contain. Following on from the original work it was realised that this idea was applicable to a wide range of tasks including information filtering <ref type="bibr" coords="3,340.68,307.40,99.54,8.96" target="#b5">(Foltz and Dumais, 1992</ref>) and cross-language information retrieval <ref type="bibr" coords="3,157.32,319.16,157.04,8.96" target="#b8">(Littman, Dumais and Landauer, 1998)</ref>. Outside IR, the technique is usually referred to as Latent Semantic Analysis. Within NLP, LSA has been applied to a variety of problems such as spelling correction <ref type="bibr" coords="3,115.80,342.68,105.31,8.96" target="#b7">(Jones and Martin, 1997)</ref>, morphology induction <ref type="bibr" coords="3,325.32,342.68,118.39,8.96" target="#b10">(Schone and Jurafsky, 2000)</ref>, text segmentation <ref type="bibr" coords="3,70.56,354.44,193.15,8.96" target="#b2">(Choi, Wiemer-Hastings and Moore, 2001)</ref>, hyponymy extraction <ref type="bibr" coords="3,374.28,354.44,146.24,8.96" target="#b1">(Cederberg and Widdows, 2003)</ref>, summarisation <ref type="bibr" coords="3,131.40,366.20,238.75,8.96" target="#b11">(Steinberger, Kabadjov, Poesio and Sanchez-Graillet, 2005)</ref>, and noun compound disambiguation, prepositional phrase attachment and coordination ambiguity resolution <ref type="bibr" coords="3,369.72,377.96,78.20,8.96" target="#b0">(Buckeridge, 2005)</ref>. It has also been applied to the problem of identifying given/new information <ref type="bibr" coords="3,316.92,389.72,207.90,8.96;3,70.56,401.48,71.48,8.96" target="#b6">(Hempelmann, Dufty, McCarthy, Graesser, Cai and McNamara, 2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Algorithm Development</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Underlying Idea</head><p>We decided on a very simple form of algorithm for our experiments. In the first stage, possibly relevant snippets (i.e. sentences) would be retrieved from the corpus using an IR system. In the second, these would be subjected to the LSA technique in order to estimate their novelty. In previous work, LSA had been applied to summarisation by using it to decide which topics are most important in a document and which sentences are most related to those topics <ref type="bibr" coords="3,98.76,514.64,247.15,8.96" target="#b11">(Steinberger, Kabadjov, Poesio and Sanchez-Graillet, 2005)</ref>. In this project, the idea was reversed by trying to establish that snippets were novel on the basis that they were not 'related' to the original topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Data</head><p>As this was the first year of the task, there was no training data. However, the organisers did supply 80 example topics. Twenty of these were selected. For each, the title was submitted as an exact IR search to a system indexed by sentence on the entire Wikipedia corpus. The 'documents' (i.e. snippets) returned were saved, as was the bare text of the original article. The snippets were then studied by hand and by reference to the original document were judged to be either 'relevant and novel' or not. This data was then used in subsequent tuning.</p><p>An example training topic (Carolyn Keene) can be seen in the figure. Below it are three sample snippets returned by the IR component because they contain the string 'Carolyn Keene'. The first one is clearly Important and Novel because it gives information about a whole series of books written under the same name and not mentioned in the article. The second one is Important because it states that all Nancy Drew books are written under this name. However, it is not Novel because this information is in the original article. Now consider the third example concerning Harriet Adams. The decision here is not quite so easy to make. Adams is mentioned in the article, so the fact that she wrote some of the books is not Novel. However, there is other information which is Novel, for example that she wrote books in the Hardy Boys series and also that she had another pseudonym, Franklin W. Dixon. The question is whether such information is Important. This is very hard to judge. Therefore we concluded that the task was not straightforward even for humans. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">System Architecture and Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pre-processing of the Corpus</head><p>Following our analysis of the training data, it was decided to adopt a similar approach in the final system. This involved retrieving snippets by exact phrase match. To facilitate this process, the corpus was re-formatted replacing sentence and document identifiers within attributes by the equivalent in elements and at the same time removing all hyperlink information which can occur within words and thus affects retrieval. The new version of the corpus was then indexed using Terrier <ref type="bibr" coords="4,243.48,719.72,248.05,8.96" target="#b9">(Ounis, Amati, Plachouras, He, Macdonald and Lioma, 2006;</ref><ref type="bibr" coords="4,494.40,719.72,30.45,8.96;4,70.56,731.48,23.48,8.96">Terrier, 2006)</ref> with each individual snippet (sentence) being considered as a separate document. This meant that any matching of an input query was entirely within a snippet and never across snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Stages of Processing</head><p>An input query consists of the title of a Wikipedia article. An exact search is performed using Terrier, resulting in an ordered list of matching snippets. Those coming from the original article are eliminated. In the actual runs, the number of snippets varies from 0 (where no matching snippets were found at all) to 947. The data is then formatted for LSA processing. The bare text of the original article with no formatting and one sentence per line, including the title which forms the first line, is placed in a text file. A second file is prepared for the topic which contains the snippets, one snippet per line. This file therefore contains between 0 and 947 lines, depending on the topic. LSA processing is then carried out. This assigns to each snippet a probability (between 0 and 1) as to whether it is novel with respect to the topic or not. The n snippets with highest probability are then returned, preserving the underlying order determined by Terrier. n is either 10 or 20 depending on the run. In the final stage, an xml document is created listing for each topic the selected snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Runs and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Runs Submitted</head><p>Two runs were submitted, Run 1 in which the number of accepted snippets was at maximum 10, and Run 2 in which it was at maximum 20. In all other respects the runs were identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Measures</head><p>The table summarises the overall results. In addition to the official figures returned by the organisers (denoted All) we also computed results on various different subsets of the topics. The objective was to see whether the system performed better on some types of topic than on others. Each topic in the corpus had been automatically assigned to one of four categories by the organisers using Named Entity recognition tools. The categories are Person, Location, Organization and None. We decided to make use of these in our analysis. Person denotes topics categorised as describing a person and similarly for Location and Organization. None is assigned to all topics not considered to be one of the first three. To these four were added two further ones of our own invention. Empty denotes an interesting class of topics which consist of a title and no text at all. These are something of an anomaly in the Wikipedia, presumably indicating work in progress. In this case the WiQA task is effectively to create an article from first principles. Finally, Long denotes lengthy articles, rather generally defined by us as 'more than one page on the screen'.</p><p>Each snippet was judged by the evaluators along a number of dimensions. A Supported snippet is one which is indeed in the corpus. If it contains significant information relevant to the topic it is judged Important. This decision is made completely independently of the topic text. It is Novel if it is Important and in addition contains information not already present in the topic. Finally, it is judged Non-Repeated if it is Important and Novel and has not been included in an earlier Important and Novel snippet. Repetition is thus always judged between one snippet and the rest, not relative to the original topic text.</p><p>The main judgements in the evaluation are relative to the top ten snippets returned for a topic. The Yield is the count of Important, Novel and Non-Repeated snippets occurring in the first ten, taken across all topics in the group under consideration (e.g. All). The Average Yield is this figure divided by the number of topics in the group, i.e. it is the Yield per topic. Reciprocal Rank is the inverse of the position of the first Important, Novel and Non-Repeated snippet in the top ten (numbered from 1 up to ten) returned in response to a particular topic. The Mean Reciprocal Rank (MRR) is the average of these values over all the topics in the group. MRR is an attempt to measure 'how high up the list' useful information starts to appear in the output. Finally Precision is the mean precision (number of Important, Novel and Non-Repeated snippets returned in the first ten for a topic, divided by ten) computed over all the topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Run 1 vs. Run 2</head><p>Of the two runs we submitted, Run 1 gave better results than Run 2 in terms of overall Average Yield per Topic, MRR and Precision at top 10 snippets. Having said that, the second run did retrieve far more snippets than the first one (682 as opposed to 435); it also retrieved more Important, Important and Novel, and Important, Novel and Non-Repeated snippets than the first run. However, what counts is how many Important, Novel And Non-Repeated snippets were found in the ten snippets that were ranked highest for each topic (i.e. the Yield). When we look at all 65 topics, then the yield was lower for Run 2 (152 vs. 160). This is not just true for the overall figures, but also for the topics in categories Person, Organization and Long. For Location, None and Empty, yield in the second run was marginally better.</p><p>The difference in performance between the two runs can be accounted for by an anomaly in the architecture of our system. The underlying order of snippets is determined by their degree of match with the IR search query. Because this query is simply the title and nothing else, and since all returned snippets must contain the exact title, the degree of match will be related only to the length of the snippet -short snippets will match more highly than long ones. When this list of snippets is passed to the LSA component, a binary decision is made for each snippet (depending on its score) as to whether it is relevant or not. This depends on a snippet's LSA score but not on its position in the ranking. The difference between the runs lies in the number of snippets LSA is permitted to select.</p><p>In Run 1 this consists of the best ten. In Run 2 when we come to select the best twenty this is a superset of the best ten, but it may well be that lower scoring snippets are now selected which are higher in the underlying Terrier ranking than the higher scoring snippets already chosen for Run 1. In other words, our strategy could result in high scoring snippets in Run 1 being inadvertently pushed down the ranking by low scoring ones in Run 2. This effect could explain why the amount of relevant information in Run 2 is higher but at the same time the Precision is lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Strengths and Weaknesses of the System</head><p>To find out where our approach works best we broke down the total number of topics into groups according to the categories identified earlier, i.e. Person (16 topics), Location (18), Organization ( <ref type="formula" coords="6,430.10,316.64,7.72,8.96">16</ref>), and None (15). A separate analysis was also performed for the two categories we identified, i.e. Empty (6) and Long (15). Instead of analysing individual topics we will concentrate on the aggregated figures that give us average values over all topics of a category.</p><p>We achieved the highest average Yield per Topic (3.25), highest MRR (0.67) as well as highest Precision at top 10 snippets (0.46) for topics of category Person in Run 1. In other words, for queries about persons a third of the top ten retrieved snippets were considered Important, Novel and Non-Repeated. However, the runs did not necessarily retrieve ten snippets for each query; usually we retrieved fewer than that. The Precision indicates that nearly half of all retrieved snippets were high quality matches. These values are better than what we obtained for any of the other categories (including Empty and Long) over both runs. This suggests that our methods work best at identifying Important, Novel and Non-Repeated information about persons.</p><p>We also observe that in Run 1 the Average Yield per Topic, MRR and Precision for topics of type Person or Organization are all better than those for type All. The same is true for Run 2.</p><p>On the other hand, both our runs are much worse on topics of type None. All the considered measures score much lower for topics of this category: Average Yield per Topic (1.53), MRR (0.34) as well as Precision at top 10 snippets (0.25). Interestingly, these lowest values were recorded in Run 1 which overall is better than Run 2. Nevertheless, Precision at top 10 snippets is even lower for Long topics in both runs (0.23 and 0.19, respectively). The last figure is interesting, because this lowest Precision for the long documents in Run 2 corresponds with the lowest Average Yield per Topic (1.6) but the highest average number of snippets per topic (14.0, i.e. 210 snippets divided by 15 topics). No other category retrieved that many responses per query on average. As a comparison, the lowest average number of returned snippets (5.5, i.e. 33 snippets divided by 6 topics) was recorded for Empty topics in Run 1. The conclusion is that the system (perhaps not surprisingly) seems to suggest few snippets for short documents and far more for long documents. More returned snippets do not, however, mean better quality.</p><p>Topics classified as Long or None are therefore a major weakness of our approach, and future work will need to address this. One possibility is that we could use the topic classifications at run time and then apply different methods for different categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>This was our first attempt at this task and at the same time we used a very simple system based around LSA applied only to snippets containing exactly the topic's title. In this context the results are not bad. The system works best for topics of type Person and Organization. Our highest precision overall was 0.46 for Persons in Run</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,101.88,77.65,391.62,537.87"><head></head><label></label><figDesc>Both runs were identical except that in Run 1 the maximum number of snippets returned by the system was 10 while in Run 2 it was 20. The values under the column All are those returned by the organisers. All other columns are analyses on different subsets of the topics. The object is to see if performance of the system varies by topic type. The column Person shows results just for topics which were of type PERSON and similarly for the columns Location, Organization and None. Empty denotes just those topics which contain no text at all (!) while Long is restricted to 'long' topics which contain a lot of text.</figDesc><table coords="4,101.88,77.65,391.62,466.83"><row><cell>Run 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>All</cell><cell>Person</cell><cell>Location</cell><cell>Org</cell><cell>None</cell><cell>Empty</cell><cell>Long</cell></row><row><cell>No. Topics</cell><cell>65</cell><cell>16</cell><cell>18</cell><cell>16</cell><cell>15</cell><cell>6</cell><cell>15</cell></row><row><cell>No. Snippets</cell><cell>435</cell><cell>113</cell><cell>119</cell><cell>112</cell><cell>91</cell><cell>33</cell><cell>123</cell></row><row><cell>Supported</cell><cell>435</cell><cell>113</cell><cell>119</cell><cell>112</cell><cell>91</cell><cell>33</cell><cell>123</cell></row><row><cell>Snippets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Important</cell><cell>226</cell><cell>74</cell><cell>54</cell><cell>61</cell><cell>37</cell><cell>13</cell><cell>49</cell></row><row><cell>Important &amp; Novel</cell><cell>165</cell><cell>54</cell><cell>37</cell><cell>51</cell><cell>23</cell><cell>13</cell><cell>29</cell></row><row><cell>Important &amp; Novel</cell><cell>161</cell><cell>52</cell><cell>37</cell><cell>49</cell><cell>23</cell><cell>12</cell><cell>29</cell></row><row><cell>&amp; Non-Repeated</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yield (Top 10)</cell><cell>160</cell><cell>52</cell><cell>36</cell><cell>49</cell><cell>23</cell><cell>12</cell><cell>29</cell></row><row><cell>Avg. Yield per</cell><cell>2.46</cell><cell>3.25</cell><cell>2.00</cell><cell>3.06</cell><cell>1.53</cell><cell>2.00</cell><cell>1.93</cell></row><row><cell>Topic (Top 10)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean Reciprocal</cell><cell>0.54</cell><cell>0.67</cell><cell>0.47</cell><cell>0.66</cell><cell>0.34</cell><cell>0.56</cell><cell>0.59</cell></row><row><cell>Rank</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision (Top 10)</cell><cell>0.37</cell><cell>0.46</cell><cell>0.30</cell><cell>0.44</cell><cell>0.25</cell><cell>0.36</cell><cell>0.24</cell></row><row><cell>Run 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>All</cell><cell>Person</cell><cell>Location</cell><cell>Org</cell><cell>None</cell><cell>Empty</cell><cell>Long</cell></row><row><cell>No. Topics</cell><cell>65</cell><cell>16</cell><cell>18</cell><cell>16</cell><cell>15</cell><cell>6</cell><cell>15</cell></row><row><cell>No. Snippets</cell><cell>682</cell><cell>155</cell><cell>206</cell><cell>188</cell><cell>133</cell><cell>59</cell><cell>210</cell></row><row><cell>Supported</cell><cell>682</cell><cell>155</cell><cell>206</cell><cell>188</cell><cell>133</cell><cell>59</cell><cell>210</cell></row><row><cell>Snippets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Important</cell><cell>310</cell><cell>87</cell><cell>93</cell><cell>84</cell><cell>46</cell><cell>26</cell><cell>81</cell></row><row><cell>Important &amp; Novel</cell><cell>223</cell><cell>60</cell><cell>60</cell><cell>72</cell><cell>31</cell><cell>26</cell><cell>45</cell></row><row><cell>Important &amp; Novel</cell><cell>194</cell><cell>52</cell><cell>53</cell><cell>61</cell><cell>28</cell><cell>19</cell><cell>36</cell></row><row><cell>&amp; Non-Repeated</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yield (Top 10)</cell><cell>152</cell><cell>44</cell><cell>37</cell><cell>47</cell><cell>24</cell><cell>15</cell><cell>24</cell></row><row><cell>Avg. Yield per</cell><cell>2.34</cell><cell>2.75</cell><cell>2.06</cell><cell>2.94</cell><cell>1.60</cell><cell>2.50</cell><cell>1.60</cell></row><row><cell>Topic (Top 10)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean Reciprocal</cell><cell>0.50</cell><cell>0.58</cell><cell>0.47</cell><cell>0.59</cell><cell>0.36</cell><cell>0.50</cell><cell>0.56</cell></row><row><cell>Rank</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision (Top 10)</cell><cell>0.33</cell><cell>0.39</cell><cell>0.29</cell><cell>0.39</cell><cell>0.26</cell><cell>0.38</cell><cell>0.19</cell></row><row><cell>Summary of results.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,76.32,747.56,207.69,8.96"><p>On Sabbatical from University of Limerick, Ireland.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>is overall Precision for Run 1 where we appear to be equal to the highest overall in the task, with a value of 0.37. This result is probably due to our use of exact matches to titles in the IR stage of the system.</p><p>Concerning the general task, it was very interesting but in some ways it raised more questions than answers. While it is quite easy to judge Novel and Non-Repeated it is not easy to judge Important. This is a subjective matter and can not be decided upon without considering such factors as maximum article length, intended readership, the existence of other articles on related topics, hyperlink structure (e.g. direct links to other articles containing 'missing' information) and editorial policy.</p><p>We prepared our own training data and due to lack of time this only amounted to twenty topics with judged snippets. In future years there will be much more data to carry out tuning and this might well affect results.</p><p>Our policy of insisting on an exact match of a snippet with the title of a topic resulted in the vast majority of cases in the snippet being about the topic. (There are relatively few cases of topic ambiguity although a few were found.) In other words, the precision of the data passed on to the LSA component was high. On the other hand, we must have missed many important snippets. For example we used no co-reference resolution which might well have increased recall while not affecting precision, certainly in cases like substring co-reference within an article (e.g. 'Carolyn Keene ... Mrs. Keene').</p><p>The link structure of the Wikipedia is very complex and has been faithfully captured in the corpus. We did not use this at all. It would be very interesting to investigate snippet association measures based on the 'reachability' of a candidate snippet from the topic article and to compare the information they yield with that provided by LSA. However, the link structure is not all gain: In many cases only a substring of a token constitutes the link. When the markup is analysed it can be very difficult to recover the token accurately -either it is wrongly split into two or a pair of tokens are incorrectly joined. This must affect the performance of the IR and other components though the difference in performance caused may be slight.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,70.56,425.00,454.30,8.96;7,70.56,436.76,364.29,8.96" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,193.92,425.00,330.94,8.96;7,70.56,436.76,207.34,8.96">Latent Semantic Indexing as a Measure of Conceptual Association for the Unsupervised Resolution of Attachment Ambiguities</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Buckeridge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Limerick</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct coords="7,70.56,460.28,454.38,8.96;7,70.56,472.04,454.33,8.96;7,70.56,483.80,182.25,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,232.32,460.28,292.62,8.96;7,70.56,472.04,178.76,8.96">Using LSA and noun coordination information to improve the precision and recall of automatic hyponymy extraction</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cederberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Widdows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,256.08,472.04,268.81,8.96;7,70.56,483.80,138.98,8.96">Proceedings of the Seventh Conference on Computational Natural Language Learning (CoNLL-2003)</title>
		<meeting>the Seventh Conference on Computational Natural Language Learning (CoNLL-2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.56,507.32,454.29,8.96;7,70.56,519.08,145.29,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,325.08,507.32,195.39,8.96">Latent Semantic Analysis for Text Segmentation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">Y Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wiemer-Hastings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,70.56,519.08,92.16,8.96">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Pittsburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.56,542.60,454.33,8.96;7,70.56,554.48,357.45,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,449.28,542.60,75.61,8.96;7,70.56,554.48,68.16,8.96">Indexing by latent semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,145.08,554.48,225.15,8.96">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.56,578.12,373.65,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,216.60,578.12,113.70,8.96">The Wikipedia XML Corpus</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,337.32,578.12,52.37,8.96">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.56,601.64,454.38,8.96;7,70.56,613.52,333.81,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,234.12,601.64,290.82,8.96;7,70.56,613.52,31.58,8.96">Personalized information delivery: An analysis of information filtering methods</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">W</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,109.20,613.52,247.03,8.96">Communications of the Association for Computing Machinery</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="51" to="60" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.56,637.04,454.38,8.96;7,70.56,648.80,454.29,8.96;7,70.56,660.56,454.30,8.96;7,70.56,672.32,153.09,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,501.24,637.04,23.70,8.96;7,70.56,648.80,373.83,8.96">Using LSA to automatically identify givenness and newness of noun phrases in written discourse</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">F</forename><surname>Hempelmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dufty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Graesser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Mcnamara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,213.36,660.56,311.50,8.96">Proceedings of the 27th Annual Conference of the Cognitive Science Society</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Bara</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Barsalou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Bucciarelli</surname></persName>
		</editor>
		<meeting>the 27th Annual Conference of the Cognitive Science Society<address><addrLine>Mahwah, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="941" to="946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.56,695.84,454.35,8.96;7,70.56,707.60,353.01,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,221.16,695.84,247.03,8.96">Contextual spelling correction using Latent Semantic Analysis</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,474.84,695.84,50.07,8.96;7,70.56,707.60,310.66,8.96">Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP &apos;97)</title>
		<meeting>the Fifth Conference on Applied Natural Language Processing (ANLP &apos;97)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="166" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,70.56,73.16,454.38,8.96;8,70.56,84.92,454.29,8.96;8,70.56,96.68,140.13,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,309.72,73.16,215.22,8.96;8,70.56,84.92,102.21,8.96">Automatic cross-language information retrieval using Latent Semantic Indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,280.20,84.92,155.05,8.96">Cross Language Information Retrieval</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Grefenstette</surname></persName>
		</editor>
		<meeting><address><addrLine>Norwell, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="51" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,70.56,120.20,454.30,8.96;8,70.56,131.96,454.30,8.96;8,70.56,143.72,329.85,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,407.04,120.20,117.82,8.96;8,70.56,131.96,187.83,8.96">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,268.20,131.96,256.66,8.96;8,70.56,143.72,138.58,8.96">Proceedings of ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval (OSIR 2006)</title>
		<meeting>ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval (OSIR 2006)<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-08-10">2006. 10th August, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,70.56,167.24,454.29,8.96;8,70.56,179.00,454.30,8.96;8,70.56,190.76,269.61,8.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,218.88,167.24,301.87,8.96">Knowledge-free induction of morphology using Latent Semantic Analysis</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,70.56,179.00,454.30,8.96;8,70.56,190.76,236.71,8.96">Proceedings of the Fourth Conference on Computational Natural Language Learning (CoNLL-2000) and the Second Learning Language in Logic Workshop (LLL-2000)</title>
		<meeting>the Fourth Conference on Computational Natural Language Learning (CoNLL-2000) and the Second Learning Language in Logic Workshop (LLL-2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,70.56,214.28,454.26,8.96;8,70.56,226.04,454.33,8.96;8,70.56,237.80,440.13,8.96;8,70.56,261.32,176.53,8.96;8,70.56,284.84,145.02,8.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,431.28,214.28,93.54,8.96;8,70.56,226.04,179.07,8.96">Improving LSA-based Summarization with Anaphora Resolution</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Kabadjov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Sanchez-Graillet</surname></persName>
		</author>
		<ptr target="http://en.wikipedia.org" />
	</analytic>
	<monogr>
		<title level="m" coord="8,260.64,226.04,264.25,8.96;8,70.56,237.80,269.91,8.96">Proceedings of Human Language Technology Conference / Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference / Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, Canada; Terrier</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">2005. October 2005. 2006. 2006</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
