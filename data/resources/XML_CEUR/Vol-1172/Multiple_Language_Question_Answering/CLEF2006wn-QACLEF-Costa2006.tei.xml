<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,115.92,75.67,374.77,12.19">Esfinge -a modular question answering system for Portuguese</title>
				<funder>
					<orgName type="full">POSI</orgName>
				</funder>
				<funder ref="#_arB5HBN">
					<orgName type="full">Portuguese Fundação para a Ciência e Tecnologia</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,281.76,113.91,43.10,8.74"><surname>Luís Costa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Linguateca at SINTEF ICT</orgName>
								<address>
									<addrLine>Pb ; 124 Blindern</addrLine>
									<postCode>0314</postCode>
									<settlement>Oslo</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,115.92,75.67,374.77,12.19">Esfinge -a modular question answering system for Portuguese</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">14625C33085BCAE3D280862E92436BD8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Question answering, named-entity recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Esfinge is a general domain Portuguese question answering system. It uses the information available in the Web as an additional resource when searching for answers. Other external resources and tools used are a translation tool, a syntactic analyzer, a morphological analyzer, a named entity recognizer and a database of word co-occurrences. In this third participation in CLEF, the main goals were to check whether a database of word co-occurrences could improve the answer scoring algorithm and to get more benefit from the use of a named entity recognizer that was used last year in quite sub-optimal conditions. This year results were slight better than last year's which is somehow surprising due to the fact that this year's question set included more definitions of type Que é X? (What is X?) with which the system had the worst results in previous participations. Another interesting conclusion from this year experiments is that the Web helped more with this year's questions than in previous years. While the best run using the Web achieved an accuracy of 25%, an experiment with the same algorithm but which did not use the Web achieved only 17% in accuracy. Again the main cause for failure this year was in the retrieval of relevant documents (56% of the errors). There is a web interface to a simplified version of Esfinge at http://www.linguateca.pt/Esfinge and in this page is also possible to download the modules used by the system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Esfinge is a question answering system developed for Portuguese inspired on the architecture proposed by Eric Brill <ref type="bibr" coords="1,91.76,633.45,10.65,8.74" target="#b0">[1]</ref>. Brill argues that it is possible to get interesting results, applying simple techniques to large quantities of data, using redundancy to compensate for lack of detailed analysis. The Web in Portuguese can be an interesting resource for such architecture (see <ref type="bibr" coords="1,210.65,656.43,10.51,8.74" target="#b1">[2]</ref>).</p><p>Esfinge participated at CLEF both in 2004 and 2005 editions demonstrating the applicability of this approach to Portuguese. Even though the main goal for the development of the system this year was to refactor and improve the code developed in previous years with a modular approach to make this work useful for other people interested in question answering and to make future developments easier, in this participation at CLEF 2006, we also managed to test the use of a new resource (a database of word co-occurrences) and to get more benefit from the use of a named entity recognizer that was used last year in quite sub-optimal conditions. The modules used in this year's participation are described in the following section. In this paper we also present the results obtained in the official runs this year, an error analysis for these runs, the results obtained by this year's system with the 2004 and 2005 questions and the results obtained in other experiments that were not submitted. Based on the results and error analysis we also discuss some of the evaluation guidelines used in CLEF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Esfinge 2006</head><p>CLEF 2006 brought some new challenges to the participating QA systems. The most relevant were:</p><p>• The answers needed to be supported by text snippets extracted from the document collection (last year only a document ID was required).</p><p>• Some list questions were included (questions requiring a list of items as answers).</p><p>Figure <ref type="figure" coords="2,99.53,255.15,5.01,8.74">1</ref> gives a general overview of the algorithm used in Esfinge:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. Modules used in Esfinge</head><p>If the questions are not in Portuguese, they are translated to Portuguese using the module Lingua::PT::Translate. Esfinge uses the parser PALAVRAS <ref type="bibr" coords="2,236.07,501.75,11.68,8.74" target="#b2">[3]</ref> to estimate the number of answers for a question. It proceeds with the Question Reformulation Module which transforms the question into patterns of plausible answers. These patterns are then searched in the document collection using the Search Document Collection module.</p><p>If the patterns are not found in the document collection the system returns the answer NIL (no answer found) and stops its execution. On the other hand when the system finds the patterns in the document collection, it proceeds by searching the same patterns in the Web. Then, the texts retrieved from the Web and the document collection are analyzed using the named entity recognizer SIEMÊS <ref type="bibr" coords="2,336.72,570.75,11.69,8.74" target="#b3">[4]</ref> (for the questions that imply named entity categories as answers). The found named entities of the relevant categories are then ranked according to their frequency, length and the score of the passage form where they were retrieved. This ranking is in turn adjusted using the database of co-occurrences BACO <ref type="bibr" coords="2,251.45,605.19,11.67,8.74" target="#b4">[5]</ref> and the candidate answers (by ranking order) are analyzed in order to check whether they pass a set of filters and whether there is a document in the collection which supports them.</p><p>When Esfinge does not find the necessary answers using the previous techniques, it uses the n-grams module which counts the word n-grams in the texts retrieved from the Web and the document collection. These n-grams (as the named entities of the relevant categories) are then ranked according to their frequency, length and the score of the passage from where they were retrieved, ranking which is also adjusted using the database of cooccurrences BACO.</p><p>From the moment when Esfinge finds enough answers, it checks only candidate answers that include one of the previously found answers. It will replace one of the original answers if the new one includes the original answer, also passes the filters and has documents in the collection that can support it.</p><p>Each of these modules is described in the following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Translation</head><p>This module was used for the EN-PT run (questions in English, answers in Portuguese). The questions (except the ones of type Where is X) are translated with the module Lingua::PT::Translate freely available at CPAN which provides an easy interface to the Altavista Babelfish translating tool Questions of type Where is X are treated in a different way because the translation tool translates them to Onde está X (is can be translated as está, é or fica in Portuguese). Therefore this module translates these questions to Onde fica X which is a more natural translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Number of Answers Estimation</head><p>The motivation for this module arose from the inclusion of list questions in the question set this year.</p><p>To estimate the number of answers required by a question, Esfinge used the parser PALAVRAS <ref type="bibr" coords="3,478.10,220.41,10.57,8.74" target="#b0">[1]</ref>. The algorithm is very simple; the question is submitted to PALAVRAS:</p><p>• If the noun phrase (NP) is singular Esfinge tries to find exactly one answer (e.g. Quem era o pai de Isabel II?</p><p>/ Who is Elizabeth The Second's father?).</p><p>• If the NP is plural and the parser finds a numeral in the question (e.g. Quais são as três repúblicas bálticas? / Which are the three Baltic states?), the system will return this number of answers (three in this case).</p><p>• If the NP is plural and the parser does not find a numeral in the question (e.g. Quais são as repúblicas bálticas? / Which are the Baltic states?), Esfinge returns five answers (default).</p><p>This strategy is quite naive however, and after analyzing the question set we quickly realize that for some questions with a singular NP, it is quite likely more useful to obtain several answers (e.g. ). This will be studied in more detail in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Question Reformulation</head><p>This module transforms questions into patterns of possible answers. A given question is matched against a list of question patterns which in turn are associated with pairs (Answer pattern/Score).</p><p>For example the question Quem é Stephen Hawking? matches with the patterns (simplified here for illustration purposes):</p><p>Quem ([^\s?]*) ([^?]*)\??/"$2 $1"/10 Quem ([^?]*)\??/$1/1</p><p>Which in turn generate the following (Answer pattern/Score) pairs:</p><p>"Stephen Hawking é"/10 é Stephen Hawking/1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Search the Document Collection</head><p>The document collection was encoded using IMS Corpus Workbench <ref type="bibr" coords="3,354.49,620.79,10.60,8.74" target="#b5">[6]</ref>. Each document was divided in sets of three sentences. The sentence segmentation and tokenization was done using the Perl Module Lingua::PT::PLNbase developed by Linguateca and freely available at CPAN. Esfinge uses the named entity recognizer (NER) SIEMÊS <ref type="bibr" coords="3,321.66,655.23,11.69,8.74" target="#b3">[4]</ref> to refine the patterns obtained by the question reformulation module. The pattern é Stephen Hawking (is Stephen Hawking) is a good example. SIEMÊS tags Stephen Hawking as a &lt;HUM&gt; (Human) and therefore the original pattern is converted to é "Stephen Hawking". There are other cases, however, where Esfinge does not preserve the named entities identified by the NER system. An example are titles (e.g. Presidente do Egipto / President of Egypt), since in many occasions the tokens Presidente and Egipto are likely to appear in different positions in a sentence (or even in different sentences). As an illustration the following text (freely adapted from a text in the Portuguese document collection) can be relevant for answer extraction:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"Yesterday it was Election Day in Egypt. The party of the President Hosny Mubarak is expected to have the best result since 1979."</head><p>The system searches these refined patterns in the document collection. If it does not find any set of three sentences matching one of the patterns, it returns the answer NIL (no answer found) and stops its execution. Otherwise, Esfinge stores the matching passages in memory {P 1 , P 2 … P n }. Stop-words without context are discarded in this search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Search theWeb</head><p>The patterns obtained in the previous modules are then searched in the Web. Esfinge uses the APIs provided by Google and Yahoo search engines for that purpose. Esfinge stores the first 50 snippets (excluding, however, the snippets retrieved from addresses containing words like blog or humor) retrieved by each of the search engines {S 1 , S 2 … S n }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Named Entity Recognition and N-gram Harvesting</head><p>Two techniques are used to extract answers from the relevant passages retrieved from the Web and the document collection: named entity recognition and n-gram harvesting.</p><p>The NER system SIEMÊS <ref type="bibr" coords="4,192.04,288.39,11.67,8.74" target="#b3">[4]</ref> is used for the questions which imply answers of type Place, People, Quantity, Date, Height, Duration, Area, Organization and Distance. Esfinge uses pattern matching to check whether it is possible to infer the type of answer for a given question. For example, questions starting with Onde (Where) imply an answer of type Place, questions starting with Quando (When) imply an answer of type Date, questions starting with Quantos (How Many) imply an answer of type Quantity, etc. For these questions, SIEMÊS tags the relevant text passages in order to count the number of occurrences of NEs belonging to the relevant categories. SIEMÊS was used in 54% of the answered questions in the PT-PT task this year.</p><p>For other questions, however, either the answer is not a named entity (definition questions) or it is very time consuming to create patterns to deal with them (questions of type Qual X / Which X, where X can be potentially anything). For these questions (or when not enough valid answers are obtained within the candidate answers extracted with the NER), Esfinge uses the n-grams module which counts the word n-grams in the relevant text passages.</p><p>The candidate answers (obtained either through NER or n-gram harvesting) are then ranked according to their frequency, length and the score of the passage from where they were retrieved using the formula:</p><p>Candidate answer score = ∑ (F * S * L), through the passages retrieved in the previous modules where:</p><p>F = Candidate answer frequency S = Score of the passage L = Candidate answer length This year we managed to install a local installation of SIEMÊS in our server which enabled us to analyze all the relevant passages with this NER system. In average, 30 different named entities of the relevant types were identified in the passages retrieved from the Web for each question, while 22 different named entities of the relevant types were identified in the passages retrieved from the document collection. Last year, it was not possible to analyze the whole relevant passages for practical reasons, so only the most frequent sequences of 1 to 3 words extracted from these relevant passages were analyzed which meant that the NER system was used in quite suboptimal conditions. Nevertheless, this sub-optimal use of the NER led to some improvement in the results, as described in <ref type="bibr" coords="4,112.60,610.11,10.64,8.74" target="#b6">[7]</ref>. This year we expected still larger improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Database of Co-occurrences</head><p>The score computed in the previous modules is heavily dependent on word frequency, though, and, therefore, very frequent words may appear quite often in the results. The use in a question answering system of an auxiliary corpus to check the frequency of words in order to capture an overrepresentation of candidate answers in a set of relevant passages was successfully tested before, see <ref type="bibr" coords="4,289.86,700.53,10.60,8.74" target="#b7">[8]</ref>. With that in mind, we used the database of word cooccurrences BACO <ref type="bibr" coords="4,152.61,712.05,11.70,8.74" target="#b4">[5]</ref> this year, to take into account the frequency in which n-grams appear in a large corpus. BACO includes tables with the frequencies of word n-grams from length 1 to 4 of the Portuguese web collection WPT-03, a snapshot of the Portuguese Web in 2003 (1.000.000.000 words) <ref type="bibr" coords="4,380.86,735.03,10.61,8.74" target="#b8">[9]</ref>. The scores obtained in the previous module are then adjusted, giving more weight to more rare candidates using the formula:</p><p>Candidate answer adjusted score = Candidate answer score * log (Number of words in BACO / Candidate answer frequency in BACO)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Choice of Longer Answers</head><p>From the moment when Esfinge finds enough answers, it proceeds to check only candidate answers that include one of the previously found answers. For example, for the question O que é a Generalitat? (What is the Generalitat?), the system finds the answer Catalunha first, but afterwards finds the candidate answer governo da Catalunha (Government of Catalonia) that includes the first, also passes the filters and has documents in the collection that can support it. Therefore the answer Esfinge returns is governo da Catalunha.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">Filters</head><p>At this stage Esfinge has an ordered list of candidate answers {A 1 , A 2 … A n }. These candidate answers are then checked using a set of filters:</p><p>• A filter that excludes answers contained in the questions. For example, the answer aspirina (aspirin) is not a good answer to the question Qual o principal componente da aspirina? (What is the main component of the aspirin?).</p><p>• A filter that excludes answers contained in a list of undesired answers (very frequent words that usually can not answer questions). This list includes words like parte (part), antigo (old), pessoas (people), mais (more) and is updated based on experiments performed with the system. At present, it contains 96 entries.</p><p>• The answers obtained through n-gram harvesting are also submitted to a filter that uses the morphological analyzer jspell <ref type="bibr" coords="5,143.03,382.95,16.70,8.74" target="#b9">[10]</ref> to check the PoS of the words contained in the answer. Jspell returns a list of tags for each of the words. Esfinge rejects all answers in which the first and last word are not tagged as one of the following categories: adjectives (adj), common nouns (nc), numbers (card) and proper nouns (np).</p><p>An answer that passes all the filters proceeds to the next module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10">Search for Supporting Document</head><p>In this module, the system tries to find a document in the collection that can support a candidate answer. For that purpose, it looks for three sentence passages including an answer pattern used in the document retrieval, as well as a candidate answer. The search starts with the candidate answers and search patterns with better score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>As in previous years we grouped the questions in a set of categories in order to get a better insight on which type of questions the system gets better results. For example, the category Place includes the questions where the system expects to have a location as answer such as the Onde (Where) questions and the Que X (Which X) questions where X=cidade (city) or X=país (country), in which the NER system is used to identify names of countries, cities or other locations in the retrieved documents. Other interesting categories are People, Quantities and Dates where the NER system is also used to find instances of those categories in the relevant texts. As the answer categories Height, Duration, Area, Organization and Distance are less represented in the question set, they are all grouped in the results presented in this paper.</p><p>Other categories are more pattern-oriented like the categories Que é X (What is X) or Que|Qual X (Which X). Table <ref type="table" coords="5,107.71,676.59,5.01,8.74">1</ref> summarizes the results of the three official runs. Two runs which used the algorithm described in section 2 were sent in the PT-PT task. The only difference is that in Run 1 were considered word n-grams from length 1 to 3 while in Run 2, word n-grams from length 1 to 4 were used. The EN-PT run used word n-grams from length 1 to 3. 1) Which X, 2) What is X, 3) Who is &lt;HUM&gt;, 4) What is X called / Name X</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No. (%) of correct answers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. Results by type of question</head><p>This year there was a large increase in the number of definition questions of type Que é X? and a considerable reduction in the number of questions of type Quem é &lt;HUM&gt;?. Theoretically this evolution would make Esfinge's task more complicated, as it was precisely for the questions of type Que é X? that the system obtained the worst results last year. However, Esfinge managed to have even a slightly better result than last year in Run 1. This was accomplished through considerable improvements in the results with the aforementioned questions of type Que é X?, as well as with the answers of type People and Place.</p><p>In Run 2, Esfinge managed to answer correctly some questions for which it was not possible to have a complete answer in Run 1 (Run 1 used word n-grams up to length 3, so when the n-gram module was used, the maximum answer length was 3 and therefore it was not possible to give an exact answer to questions that required lengthier answers). However, as Run 2 used longer n-grams, 7 of the answers that were returned even though including the correct answer, included also some extra words. As these answers were classified as Inexact, the global results of Run 2 are slightly worse that the ones in Run 1. Not surprisingly, definitions of type Que é X? were the only type of questions with some improvement. If we consider the correct answers plus the answers including the correct answer and some more words then we obtain 51 answers in Run 1 and 53 answers in Run 2.</p><p>Regarding the EN-PT there was a slight improvement in the number of exact answers, even though only small details were adjusted in the automatic translation of some questions.</p><p>Table <ref type="table" coords="6,107.97,565.65,5.01,8.74" target="#tab_2">2</ref> summarizes the main causes for wrong answers. As in last year, most of the errors in the PT-PT task reside in the document retrieval. More than half of the errors occur because the system is not able to find relevant documents to answer the questions.</p><p>The other two main causes for errors are the answer scoring algorithm and the answer supporting module. The category Others includes among other causes: the answer needing more than four words (only word n-grams up to length four are considered), missing patterns to relate questions to answer categories, NER failures and the retrieved documents from the Web not including any answer. Since in more than half of the questions Esfinge is not able to retrieve relevant documents and returns NIL, it is interesting to check the results that are obtained considering only the questions for which the system is able to find relevant documents. These results are summarized in table <ref type="table" coords="7,325.92,244.77,3.76,8.74">3</ref>   <ref type="table" coords="7,159.33,507.57,3.76,8.74">3</ref>. Results for the questions for which the system finds relevant documents</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No. of wrong answers</head><p>Seen from this viewpoint, the results for the answers of type People and Place are even more encouraging. On the other hand, the values in parentheses (total number of questions) point out that the document retrieval for questions of type Que|Qual X? is particularly bad (the system only finds relevant documents for one sixth of these questions). Esfinge has also serious problems with the temporally restricted questions: it only finds relevant documents in the collection for 2 of the 20 temporally restricted questions and it does not answer any of these correctly. One interesting issue is to determine whether the system is really improving its performance for a particular type of questions or whether the questions of a particular type are easier or harder from one year to the other. To get some insight on this issue, we tested this year's system with the questions from 2004 and 2005 as well, see Even though there was a considerable improvement in this years performance with the definitions of type Que é X?, the same improvement is not visible with 2005 questions. This indicates that as more questions of this type were introduced in this year's edition, the global level of difficulty for this type of questions decreased and therefore there was not a real improvement of the system for this type of questions. On the other hand, the improvement in the questions of type People and Place is consistent with the results over the questions from the previous years, which confirms that Esfinge improved its performance with questions of these types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Other experiments</head><p>In addition to the two submitted runs, some more experiments were performed. Table <ref type="table" coords="8,417.66,453.57,5.01,8.74" target="#tab_7">5</ref> gives an overview of the results obtained in these experiments. Run 3 used the same algorithm used in Run 1 except that it did not use the database of word co-occurrences BACO. Run 4 did not use the Web, only the document collection was used to extract the answers. From the results in Run 3, one can conclude that the use of the database of word co-occurrences to adjust the ranking of the candidate answers did not improve the results in a significant way, but it is somehow surprising that the questions correctly answered in the official run that were not answered correctly in Run 3 are precisely questions with answers of type Person and Place, obtained with the NER recognizer. One would expect that the use of BACO would be more useful with answers obtained using n-gram harvesting. The results of Run 4 are quite interesting on the other hand. While the best run using the Web achieved an accuracy of 25%, this experiment with exactly the same algorithm except that it does not use the Web achieved only an accuracy of 17%. This is a bit surprising since in last year experiments the difference was not this large (24% using the Web and 22% with the document collection only as reported in <ref type="bibr" coords="9,391.07,165.45,15.02,8.74" target="#b10">[11]</ref>). Another issue concerns the fact that the document collection is getting older and therefore it should be more difficult to find a clear answer in the Web (which is constantly updated with new information) for some of the questions that one can make based in this collection. Questions like Quem é o presidente de X? (Who is the president of X?) or Quem é &lt;HUM&gt;? are examples of questions where the Web can give some steadily noisier information as the collection gets older. The fact that more definitions of type Que é X? were included this year at the expense of questions of type Quem é &lt;HUM&gt;? can explain this somehow surprising result in Run 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No. (%) of exact answers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation guidelines</head><p>While analyzing Esfinge's results, I stumbled upon some interesting cases which may be worthwhile to discuss. I realized for example that the answers evaluated as incomplete (X) may include quite distinct situations:</p><p>• Literally incomplete answers (ex: Internacional de Atletismo instead of Federação Internacional de Atletismo).</p><p>• Answers that include the right answer and some more words that do not make the answer completely useless (ex: "superficie de Marte", "Suécia, Thomas" , "Eric Cantona, ídolo" where the exact answers are Marte, Suécia and Eric Cantona)</p><p>• Answers that contain more information than what was asked (ex: abril de 1912 / April 1912 where the answer was considered wrong because only the year was asked). In the experiment with the 2005 questions there was another variation of this "error": the system answered escritor peruano (Peruvian writer) to the question Qual a nacionalidade de Mario Vargas Llosa? (What is the nationality of Mario Vargas Llosa?).</p><p>Whereas in my opinion the third type of answers should be evaluated as correct (there is nothing wrong with some additional (potentially useful) information, I think the first and second type of errors should be differentiated. The first type could be labeled as "Partially correct by Shortage" and the second type as "Partially correct by Excess" (this method was proposed in <ref type="bibr" coords="9,240.69,508.65,16.65,8.74" target="#b11">[12]</ref> and used in the NER Evaluation Contest HAREM <ref type="bibr" coords="9,465.55,508.65,16.70,8.74" target="#b12">[13]</ref> for example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Concluding Remarks</head><p>I think that the QA task at CLEF is moving in the right direction. This year's edition had some novelties that I consider very positive. For example, the fact that in this year systems were required to provide document passages to support their answers, whereas in last year the document ID sufficed, makes the task more user-centered and realistic. It is not realistic to assume that a real user would be satisfied with an answer justification that consisted in a (sometimes) quite large document. Another positive innovation was not to provide the type of question to the systems. This year results consisted in the refactoring of the code produced in previous years that now is available to the community in http://www.linguateca.pt/Esfinge/ . Regarding results of the runs themselves, the system managed to maintain the same level of performance even though questions were more difficult (and interesting) this year. It was also interesting to notice that the use of the Web as an auxiliary resource proved to be more crucial than in previous years. This might indicate that more information (and more reliable) is retrieved by Google and Yahoo in Portuguese, or that the questions this year were less time-dependent.</p><p>The document retrieval in the document collection continues to be the main problem affecting Esfinge's performance in CLEF. To improve the results, it is necessary to improve the question reformulation module or/and the document retrieval module. One approach can be to incrementally simplify the queries, first removing the less important words until some possibly relevant documents are retrieved. Query expansion mechanisms (eventually using thesauri and/or ontologies) can provide some more sophisticated improvements in this area. The automatic extraction of patterns can also be an interesting working methodology. Still regarding the question reformulation module, an evaluation of the patterns currently in use can enable an updating of their scores.</p><p>Regarding the answer algorithm, as the results obtained using the database of co-occurrences did not improve the results as much as it could be expected, there might be some space for improvements in the way this resource is being used in Esfinge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,131.76,275.52,343.14,177.18"><head></head><label></label><figDesc></figDesc><graphic coords="2,131.76,275.52,343.14,177.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,119.52,85.65,361.95,116.02"><head>Table 2 . Causes for wrong answers</head><label>2</label><figDesc></figDesc><table coords="7,119.52,85.65,361.95,97.66"><row><cell>Problem</cell><cell>Run 1</cell><cell>Run 2</cell><cell>Run</cell></row><row><cell></cell><cell>PT-PT</cell><cell>PT-PT</cell><cell>EN-PT</cell></row><row><cell>Translation</cell><cell>-</cell><cell>-</cell><cell>106 (62%)</cell></row><row><cell>No documents retrieved in the document collection</cell><cell cols="3">84 (56%) 84 (55%) 25 (15%)</cell></row><row><cell>Answer scoring algorithm</cell><cell cols="3">37 (24%) 30 (19%) 19 (11%)</cell></row><row><cell>Answer support</cell><cell>10 (7%)</cell><cell>11 (7%)</cell><cell>13 (8%)</cell></row><row><cell>Others</cell><cell cols="2">19 (13%) 29 (19%)</cell><cell>8 (5%)</cell></row><row><cell>Total</cell><cell>150</cell><cell>154</cell><cell>171</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,146.22,244.77,311.49,115.66"><head></head><label></label><figDesc>.</figDesc><table coords="7,146.22,268.23,311.49,92.20"><row><cell>Type of question (after semantic analysis)</cell><cell>No. of answered (total) Q. in 2006 PT-PT</cell><cell>No. (%) of correct answers Run 1 PT-PT Run 2 PT-PT</cell><cell>No. of answered (total) Q. in 2006 EN-PT</cell><cell>No. (%) of correct answers Run EN-PT</cell></row><row><cell>Place</cell><cell>13 (26)</cell><cell>6 (46%) 5 (38%)</cell><cell>7 (26)</cell><cell>2 (29%)</cell></row><row><cell>People</cell><cell>12</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,70.92,639.87,32.82,8.74"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table coords="8,165.69,74.01,264.25,218.56"><row><cell></cell><cell>Type of question (after semantic analysis)</cell><cell>No. of Q. in 2005</cell><cell>No. (%) of correct answers Esfinge 2006</cell><cell>No. of Q. in 2004</cell><cell>No. (%) of correct answers Esfinge 2006</cell></row><row><cell></cell><cell>People</cell><cell>47</cell><cell>18 (38%)</cell><cell>43</cell><cell>20 (47%)</cell></row><row><cell></cell><cell>Place</cell><cell>33</cell><cell>17 (52%)</cell><cell>41</cell><cell>21 (51%)</cell></row><row><cell>NER</cell><cell>Quantity Date</cell><cell>16 15</cell><cell>4 (25%) 9 (60%)</cell><cell>18 15</cell><cell>3 (17%) 6 (40%)</cell></row><row><cell></cell><cell>Height, Duration,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Area, Organization,</cell><cell>4</cell><cell>1 (25%)</cell><cell>5</cell><cell>1 (20%)</cell></row><row><cell></cell><cell>Distance</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Que|Qual X</cell><cell>34</cell><cell>9 (26%)</cell><cell>42</cell><cell>8 (19%)</cell></row><row><cell>n-grams</cell><cell>Quem é &lt;HUM&gt; Que é X Como se chama/ Diga X</cell><cell>27 15 9</cell><cell>9 (33%) 2 (13%) 5 (56%)</cell><cell>17 15 3</cell><cell>2 (12%) 5 (33%) 1(33%)</cell></row><row><cell></cell><cell>Total</cell><cell cols="4">200 74 (37%) 199 67 (34%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,205.50,301.95,195.69,8.74"><head>Table 4 . Results with 2004 and 2005 questions</head><label>4</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,160.59,511.47,276.36,235.36"><head>Table 5 . Results of the non-official runs</head><label>5</label><figDesc></figDesc><table coords="8,160.59,511.47,276.36,217.24"><row><cell></cell><cell></cell><cell>No. of</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Type of question</cell><cell>Q. in 2006</cell><cell>Run 3</cell><cell>Run 4</cell><cell>Best in 2006</cell></row><row><cell></cell><cell>People</cell><cell>29</cell><cell>7 (24%)</cell><cell>6 (21%)</cell><cell>9 (31%)</cell></row><row><cell></cell><cell>Place</cell><cell>26</cell><cell cols="2">10 (38%) 10 (38%)</cell><cell>11 (42%)</cell></row><row><cell></cell><cell>Date</cell><cell>20</cell><cell>3 (15%)</cell><cell>3 (15%)</cell><cell>3 (15%)</cell></row><row><cell>NER</cell><cell>Quantity Height, Dura-</cell><cell>7</cell><cell>1 (14%)</cell><cell>1 (14%)</cell><cell>1 (14%)</cell></row><row><cell></cell><cell>tion, Area, Or-ganization,</cell><cell>5</cell><cell>1 (20%)</cell><cell>0</cell><cell>1 (20%)</cell></row><row><cell></cell><cell>Distance</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Que|Qual X</cell><cell>60</cell><cell>11 (18%)</cell><cell>9 (15%)</cell><cell>11 (18%)</cell></row><row><cell>n-grams</cell><cell>Que é X Quem é &lt;HUM&gt; Como se chama/ Diga X</cell><cell>36 9 8</cell><cell>10 (28%) 3 (33%) 1 (13%)</cell><cell>3 (8%) 0 1 (13%)</cell><cell>10 (28%) 3 (33%) 1 (13%)</cell></row><row><cell></cell><cell>Total</cell><cell>200</cell><cell cols="2">47 (24%) 33 (17%)</cell><cell>50 (25%)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p>I thank <rs type="person">Diana Santos</rs> for reviewing previous versions of this paper, <rs type="person">Luís Sarmento</rs> for providing support for the use of both the NER system SIEMÊS and the database of word co-occurrences BACO.</p><p>This work has been produced in the scope of Linguateca, which is financed by the <rs type="funder">Portuguese Fundação para a Ciência e Tecnologia</rs> through grant <rs type="grantNumber">POSI/PLP/43931/2001</rs>, co-financed by <rs type="funder">POSI</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_arB5HBN">
					<idno type="grant-number">POSI/PLP/43931/2001</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,74.29,369.40,450.10,7.85;10,82.26,379.72,210.95,7.85" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,113.94,369.40,243.33,7.85">Processing Natural Language without Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,444.38,369.40,80.02,7.85;10,82.26,379.72,16.22,7.85">CICLing 2003. LNCS 2588</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="360" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,74.30,390.04,450.19,7.85;10,82.26,400.42,164.14,7.85" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,170.98,390.04,160.12,7.85">Characterizing a National Community Web</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,334.09,390.04,186.29,7.85">ACM Transactions on Internet Technology (TOIT)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="508" to="531" />
			<date type="published" when="2005-08">August 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,74.30,410.74,450.14,7.85;10,82.26,421.12,192.81,7.85" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,120.03,410.74,112.92,7.85">The Parsing System &quot;Palavras</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,244.43,410.74,280.01,7.85;10,82.26,421.12,39.37,7.85">Automatic Grammatical Analysis of Portuguese in a Constraint Grammar Framework</title>
		<meeting><address><addrLine>Aarhus</addrLine></address></meeting>
		<imprint>
			<publisher>Aarhus University Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,74.30,432.46,362.98,7.85;10,437.28,429.61,5.04,5.65;10,444.84,432.46,79.51,7.85;10,82.26,442.90,442.15,7.85;10,82.26,453.28,36.79,7.85" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,132.23,432.46,284.78,7.85">SIEMÊS -a named entity recognizer for Portuguese relying on similarity rules</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sarmento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,432.78,432.46,4.50,7.85;10,437.28,429.61,5.04,5.65;10,444.84,432.46,79.51,7.85;10,82.26,442.90,260.44,7.85">7 th Workshop on Computational Processing of Written and Spoken Language (PROPOR&apos;2006)</title>
		<meeting><address><addrLine>Itatiaia, RJ, Brasil</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006-05-17">13-17 May 2006</date>
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,74.30,463.60,450.19,7.85;10,82.26,473.98,36.79,7.85" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,132.16,463.60,190.48,7.85">BACO -A large database of text and co-occurrences</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sarmento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,338.94,463.60,99.48,7.85">Proceedings of LREC 2006</title>
		<meeting>LREC 2006<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">May 22-28, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,74.30,484.30,450.03,7.85;10,82.26,494.62,240.43,7.85" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,290.22,484.30,234.11,7.85;10,82.26,494.62,49.92,7.85">The IMS Corpus Workbench: Corpus Query Processor (CQP): User&apos;s Manual</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">M</forename><surname>Schulze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Koenig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-03-08">March 8, 1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Stuttgart</orgName>
		</respStmt>
	</monogr>
	<note>CQP V2.2</note>
</biblStruct>

<biblStruct coords="10,74.30,505.00,450.26,7.85;10,82.26,516.34,386.80,7.85;10,469.02,513.49,5.04,5.65;10,476.88,516.34,47.49,7.85;10,82.26,526.78,416.76,7.85" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,174.01,505.00,202.66,7.85">Component Evaluation in a Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sarmento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,393.24,516.34,75.82,7.85;10,469.02,513.49,5.04,5.65;10,476.88,516.34,47.49,7.85;10,82.26,526.78,240.20,7.85">Proceedings of the 5 th International Conference on Language Resources and Evaluation (LREC&apos;2006 )</title>
		<editor>
			<persName><forename type="first">Nicoletta</forename><surname>Calzolari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aldo</forename><surname>Gangemi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bente</forename><surname>Maegaard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joseph</forename><surname>Mariani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan</forename><surname>Odjik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Tapias</surname></persName>
		</editor>
		<meeting>the 5 th International Conference on Language Resources and Evaluation (LREC&apos;2006 )<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05-28">22-28 May 2006</date>
			<biblScope unit="page" from="1520" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,74.29,537.16,450.11,7.85;10,82.26,547.48,219.71,7.85" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,278.72,537.16,175.36,7.85">Exploiting Redundancy in Question Answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,473.46,537.16,50.93,7.85;10,82.26,547.48,137.27,7.85">Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="358" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,74.30,557.80,450.28,7.85;10,82.26,568.18,442.16,7.85;10,82.26,578.50,104.54,7.85" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,260.34,557.80,264.25,7.85;10,82.26,568.18,37.97,7.85">WPT 03: a primeira colecção pública proveniente de uma recolha da web portuguesa</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,210.60,568.18,313.82,7.85;10,82.26,578.50,38.86,7.85">Avaliação conjunta: um novo paradigma no processamento computacional da língua portuguesa</title>
		<editor>
			<persName><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</editor>
		<imprint>
			<publisher>IST Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,78.43,588.88,445.98,7.85;10,82.26,599.20,442.19,7.85;10,82.26,609.52,265.33,7.85" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,200.56,588.88,323.85,7.85;10,82.26,599.20,25.60,7.85">Jspell.pm -um módulo de análise morfológica para uso em Processamento de Linguagem Natural</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Simões</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,275.56,599.20,248.89,7.85;10,82.26,609.52,43.30,7.85">Actas do XVII Encontro da Associação Portuguesa de Linguística (APL 2001)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gonçalves</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Correia</surname></persName>
		</editor>
		<meeting><address><addrLine>Lisboa</addrLine></address></meeting>
		<imprint>
			<publisher>APL Lisboa</publisher>
			<date type="published" when="2002">2-4 Outubro 2001. 2002</date>
			<biblScope unit="page" from="485" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,78.43,620.92,57.07,7.85;10,135.48,618.07,5.04,5.65;10,144.30,620.92,380.17,7.85;10,82.26,632.38,398.41,7.85;10,480.60,629.53,5.04,5.65;10,487.98,632.38,36.48,7.85;10,82.26,642.88,442.14,7.85;10,82.26,653.20,265.31,7.85" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,126.56,620.92,8.94,7.85;10,135.48,618.07,5.04,5.65;10,144.30,620.92,224.89,7.85">20 th Century Esfinge (Sphinx) solving the riddles at CLEF 2005</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,476.17,632.38,4.50,7.85;10,480.60,629.53,5.04,5.65;10,487.98,632.38,36.48,7.85;10,82.26,642.88,210.26,7.85">6 th Workshop of the Cross-Language Evaluation Forum (CLEF&apos;2005)</title>
		<title level="s" coord="10,496.96,642.88,27.44,7.85;10,82.26,653.20,98.80,7.85">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Frederic</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><surname>Müeller &amp; Maarten De Rijke</surname></persName>
		</editor>
		<meeting><address><addrLine>Vienna, Áustria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005-09-23">21-23 September 2005</date>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="467" to="476" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="10,78.41,663.52,446.06,7.85;10,82.26,673.90,435.66,7.85" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,168.41,663.52,292.38,7.85">CLEF: Abrindo a porta à participa internacional em avaliação de RI do português</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,103.56,673.90,348.77,7.85">Avaliação conjunta: um novo paradigma no processamento computacional da língua portuguesa</title>
		<editor>
			<persName><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</editor>
		<imprint>
			<publisher>IST Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,78.43,684.22,446.02,7.85;10,82.26,694.54,442.09,7.85;10,82.26,705.94,75.19,7.85;10,157.38,703.09,5.04,5.65;10,165.00,705.94,359.38,7.85;10,82.26,716.38,110.34,7.85" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,264.86,684.22,244.22,7.85">HAREM: An Advanced NER Evaluation Contest for Portuguese</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Seco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vilela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,82.26,705.94,75.19,7.85;10,157.38,703.09,5.04,5.65;10,165.00,705.94,292.26,7.85">Proceedings of the 5 th International Conference on Language Resources and Evaluation (LREC&apos;2006 )</title>
		<editor>
			<persName><forename type="first">Nicoletta</forename><surname>Calzolari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aldo</forename><surname>Gangemi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bente</forename><surname>Maegaard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joseph</forename><surname>Mariani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan</forename><surname>Odjik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Tapias</surname></persName>
		</editor>
		<meeting>the 5 th International Conference on Language Resources and Evaluation (LREC&apos;2006 )<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05-28">22-28 May 2006</date>
			<biblScope unit="page" from="1986" to="1991" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
