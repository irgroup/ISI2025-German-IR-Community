<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.36,98.63,392.62,15.51;1,97.20,120.59,408.81,15.51;1,224.64,142.43,153.96,15.51">MIRACLE at the Spanish WiQA Pilot: Using Named Entities and Cosine Similarity to extend Wikipedia articles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,181.44,175.93,102.11,9.96"><forename type="first">César</forename><surname>De Pablo-Sánchez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.24,175.93,130.66,9.96"><forename type="first">José</forename><surname>Luis Martínez-Fernández</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,264.00,189.85,75.12,9.96"><forename type="first">Paloma</forename><surname>Martínez</surname></persName>
							<email>paloma.martinez@uc3m.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,179.28,231.73,155.50,9.96"><roleName>Data, Decisions</roleName><forename type="first">S</forename><forename type="middle">A</forename><surname>Daedalus</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.38,231.73,61.64,9.96"><forename type="first">S</forename><forename type="middle">A</forename><surname>Language</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.36,98.63,392.62,15.51;1,97.20,120.59,408.81,15.51;1,224.64,142.43,153.96,15.51">MIRACLE at the Spanish WiQA Pilot: Using Named Entities and Cosine Similarity to extend Wikipedia articles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0774C85CEB0BA580D3868E2B873F0178</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Question answering, Questions beyond factoids, Novelty detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The WiQA pilot task explores how to select new and useful information that could be included in Wikipedia articles. Our system explores how the combination of NE and cosine similarity allow to detect new and repeated information. We have submitted two runs for the Spanish subtask wich differ in the way they select candidate sentences using the link structure in the WikipediaXML corpus. Our approach obtains results that provide at least a new snippet per topic in average. The main limitation was found in the candidate selection strategy that results in some topics being not answered or in other cases providing too much noisy candidates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The objective of the WiQA task is to locate important and new information that could be included in a Wikipedia article. The source of the new information are other Wikipedia articles. It is assumed that an editor will judge this information as appropiate or not and she will produce the definitive article.</p><p>The WiQA pilot task has some resemblance with other IR tasks but it is unique in its combination. As passage retrieval and question answering it is expected that results are brief and focused in a topic but there is no explicit kind of information that it is sought. Like in multidocument summarization, the system must select the important information from a set of source documents, but those could contain off-topic information. In contrast, there exist an original document that already contain relevant information and that should be completed. This setting is closer to novelty detection <ref type="bibr" coords="2,205.44,85.21,9.91,9.96" target="#b4">[5]</ref>. Finally, the selected information should account not only relevance, but importance.</p><p>The task uses the WikipediaXML <ref type="bibr" coords="2,251.29,109.21,10.57,9.96" target="#b2">[3]</ref> collection which is an XML version of Wikipedia produced for several languages. It is also a semistructured document collections that marks document format, contain tables of facts, etc. It also features a rich link structure that includes links between articles in the same language subcollection and also links between articles in different languages. For the purpose of the WiQA task, the corpus has also been annotated with additional structures, sentences and some articles have been automatically classified with basic NE classes: Person, Location and Organization.</p><p>We have develop a system for the Spanish subtask that uses the Spanish WikipediaXML subcollection. We have mainly reused components from our previous efforts at CLEF@QA <ref type="bibr" coords="2,475.77,204.85,18.04,9.96" target="#b1">[2]</ref> and have been focused mainly on what we consider the novelty detection part of the challenge. We submitted two runs to the Spanish WiQA task that use similar components but consider different ways to obtain candidate sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System description</head><p>We have divided the system in three modules that process the information sequentally, Candidate selection, Novelty detection and Ranking. The evaluation and retrieval unit for the task is the sentence, and therefore most of the operations proceed at this level. The first module selects candidate sentences that should be included in an article. The second component detects novel sentences, or in other words, filter those sentence whose content is already present in the article or in other sentences. Finally, sentences must be ranked according to their importance to the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Candidate Selection</head><p>The role of candidate selection consist on identifying sentences that are related to the article that we are completing. The article is described by a title, which for an specific language collection is unique as the Wikipedia uses the title as the primary key. When titles have an ambiguous interpretation they are complemented with information that helps to disambiguate. This is the case for topics like Hyderabad that could refer to the indian city or to the pakistani city (Hyderabad (Pakistán)) as both entries exists in Wikipedia.</p><p>Several alternatives exists to retrieve candidates as both content, title and article text, structure and the link structure seem useful for this task. In our case we have experimented only with the link structure between articles, in particular inlinks. We call inlinks those internal links (which use tag collectionlink) that refer to the query article. This approach is very precise and selects sentences that are clearly related to the query because the link is unambiguous. On the other hand, not all mentions are linked to an article, so this approach could leave important information out.</p><p>The collection has been stored using a XML database, Berkeley DBXML <ref type="bibr" coords="2,426.96,556.93,9.91,9.96" target="#b0">[1]</ref>. We use XQuery to retrieve relevant passages that have inlinks to the query article. Several structural indexes have been created to support efficient queries. We have experimented with diferent sizes for the passages, using sentences and paragraphs marked in the XML structure.</p><p>We found some practical problems with the way the collection has been converted and sentence splitting has been done. Tables and list are often recognized as one sentence and therefore lots of irrelevant information is selected. Wikipedia articles contain a lot of tables and list of facts that are not always related to the same topic. Because of that we decided to exclude this kinf od objects from candidate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Novelty Detection</head><p>The second module in our system is in charge of filtering the information that it is already mentioned in the original article and selecting sentences that provide new information. We have implemented novel sentence detection by a combination of two kind of information, cosine similarity between sentences and the ratio of novel an already present Named Entities (NE). A final filters candidate sentences based on three manual defined thresholds for those measures.</p><p>Cosine similarity has been proposed as a simple and effective measure to detect different levels of semantic similarity between sentences <ref type="bibr" coords="3,270.70,151.45,9.91,9.96" target="#b3">[4]</ref>. To calculate cosine similarity we index the original article in a sentence by sentence basis and compare each of the candidate sentences to them.</p><p>Original sentences as marked in the WikipediaXML corpus are processed using our language analysis toolkit composed of DAEDALUS STILUS analyzers <ref type="bibr" coords="3,354.86,187.33,10.57,9.96" target="#b5">[6]</ref> and other custom components for NERC. During development we discovered some problems with the original sentence splitting, so we decided that it could be useful to perform sentence splitting again before indexing. Stopwords are removed and simple terms withouth any other operation have been used for indexing. For indexing we have used Xapian <ref type="bibr" coords="3,221.30,235.21,10.45,9.96" target="#b6">[7]</ref> in-memory databases and their statistics to perform calculations. We obtain a pairwise cosine similariry measure with every article sentence and select the maximum value for the following operations.</p><p>Our second novelty measure is based on the presence and absence of NE in a broad sense, considering quantities and temporal expressions. We believe that the presence of these fact is probably a signal for important information in reference texts like Wikipedia. It usually signal an important relation between the focus of the article and another person, place, etc. We process the original article and produce a list of recognized NE mentions. For every candidate sentence we calculate three related measures, tha number of NE that appear in the article, the number of new NE and the ratio.</p><p>The final step consist on filtering the sentences that are not really related, too similar to the ones in the article or not important enough. The filtering is achieved by combining the previous measures and using three different thresholds. The value for this threshold have been set by manual inspection of the results in a separate and small development set. The first threshold requires a minimun number of common NE to consider the sentence. Another threshold filters sentences with a similarity. Finally, the third threshold filter those sentences in which the ratio of new NE are not significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ranking</head><p>The objective of the ranking module is to present the sentences in a appropiate order to the user. We have used the information that we had available at that point. We have tried different combinations of the two measures, NE ratio of novel entities and cosine similarity. This measures have been chosen using the small development set, and again a more principled way of scoring should be considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Description of the runs</head><p>Our group submitted two runs to the WiQA subtask organized for the Spanish collection. Both runs use the same modules but in a different configuration, with different measures and parameter settings. The first run mira-IS-CN-N uses sentences that link to the article and the ratio of new entities to rank candidates. The second run mira-IP-CN-CN uses paragraphs that link to the query article to obtain initial candidates and combine cosine similarity and the ratio of new NE to rank sentences. The rank measure is proportional to the ratio of NE, and inverse proportional to the similarity. None of the runs controlled non-repetition between candidate sentences as we develop the main system in a couple of weeks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The Spanish subtask consisted in a total of 67 topics that were developed by participant teams. The 50 first topic followed the guidelines for topic creation as close as possible. Only topics tagged as PERSON, LOCATION or ORGANIZATION in the collection were considered as official ones. These topics have different article length distribution, from stubs (very short articles) to longer articles. The other 17 topics are considered additional. Some of them were left over randomly from the official set and others do not strictly follow the guidelines. Table <ref type="table" coords="4,132.48,448.21,4.98,9.96" target="#tab_0">1</ref> summarizes the results obtained by the two runs and presents the runs in the official results and over all the developed topics. There are significant differences between the two runs, especially in the number of snippets returned by each run. As we already expected, run mira-IP-CN-CN, that consider more candidate sentences, returns more results. If we consider that the maximum number of snippets a system could return 670, both runs have low number of results. This is the consequence of our method for candidate selection. This is also the reason for some topics having no results. In contrast, the system is able to provide at least one interesting snippet in average per topic.</p><p>In Table <ref type="table" coords="4,147.48,543.85,3.90,9.96" target="#tab_1">2</ref>, we analyze the performance of the runs regarding the different class of topics from the official set. Topics where classified as Person (P), Location (L) and Organization (O). Significant differences are appreciated in each of the classes for the two runs. For P topics the run based on paragraphs performs much better, while the results are completely different for L topics. We believe that the main reason is that candidate selection in run mira-IP-CN-CN works different for those topics. When persons are mentioned in a paragraph it is probable that the rest of the paragraph also mentioned related facts. In contrast, lots of the references for locations are places where someone was born or died. The rest of the article is probably not related to the location article we would like to extend.</p><p>Our system obtains moderate good results for the task, but a larger yield of snippets is needed for a tool to be useful to complete the proposed task. We have considered a rather precise approach to the way we select candidates initially which is the main reason for the low average yield. We expect to focus on this module to include other sources and forms of selecting candidate sentences that are more recall oriented.</p><p>One of the most interesting things about WiQA is that it seems an interesting task for a multilingual setting. Combining multilingual information would definitely help creating Wikipedia topics, as far as the editor understand several languages. We would like to explore this setting in future editions. For example, we have used some shallow language analysis in the NE detection module, but we believe that the mark-up structure of the corpus could be used for a similar purpose. In this way, the system could be extended to other languages with little effort.</p><p>Finally, more investigation is needed to estimate importance models that reflect the importance of relations between entities described by the articles in real world and not just by textual snippets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,96.00,68.29,444.66,98.16"><head>Table 1 :</head><label>1</label><figDesc>Evaluation results</figDesc><table coords="4,96.00,96.01,444.66,70.44"><row><cell>Run</cell><cell cols="6">#topics #snippets #I #I.N #I.N.N R Av.Y ield M RR</cell><cell>P @10</cell></row><row><cell>mira-IS-CN-N</cell><cell>50</cell><cell>176</cell><cell>96</cell><cell>62</cell><cell>54</cell><cell cols="2">1.080000 0.240833 0.306818</cell></row><row><cell>mira-IS-CN-N</cell><cell>67</cell><cell>251</cell><cell cols="2">127 79</cell><cell>69</cell><cell cols="2">1.029851 0.299129 0.274900</cell></row><row><cell cols="2">mira-IP-CN-CN 50</cell><cell>310</cell><cell cols="2">115 74</cell><cell>52</cell><cell cols="2">1.040000 0.242190 0.167742</cell></row><row><cell cols="2">mira-IP-CN-CN 67</cell><cell>431</cell><cell cols="2">155 95</cell><cell>71</cell><cell cols="2">1.059701 0.285359 0.164733</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,139.08,199.33,325.15,121.44"><head>Table 2 :</head><label>2</label><figDesc>Results by NE category for the official topics</figDesc><table coords="4,139.08,216.97,325.15,103.80"><row><cell>Run</cell><cell cols="3">#category #topics Av.Y ield M RR</cell><cell>P @10</cell></row><row><cell>mira-IS-CN-N</cell><cell>P</cell><cell>17</cell><cell cols="2">0.764706 0.166667 0.302326</cell></row><row><cell cols="2">mira-IP-CN-CN P</cell><cell>17</cell><cell cols="2">1.470588 0.309384 0.223214</cell></row><row><cell>mira-IS-CN-N</cell><cell>L</cell><cell>17</cell><cell cols="2">1.117647 0.247549 0.287879</cell></row><row><cell cols="2">mira-IP-CN-CN L</cell><cell>17</cell><cell cols="2">0.470588 0.150000 0.076923</cell></row><row><cell>mira-IS-CN-N</cell><cell>O</cell><cell>16</cell><cell cols="2">1.375000 0.312500 0.328358</cell></row><row><cell cols="2">mira-IP-CN-CN O</cell><cell>16</cell><cell cols="2">1.187500 0.268750 0.202128</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,105.48,281.41,407.79,9.96;5,105.48,293.41,89.42,9.96" xml:id="b0">
	<monogr>
		<ptr target="http://www.sleepycat.com/products/bdbxml.html" />
		<title level="m" coord="5,105.48,281.41,41.08,9.96">Sleepycat</title>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.48,313.33,407.55,9.96;5,105.48,325.21,300.73,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,229.69,313.33,263.25,9.96">Miracle&apos;s 2005 approach to cross-lingual question answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>De Pablo-Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,105.48,325.21,196.19,9.96">Working Notes for the CLEF 2005 Workshop</title>
		<meeting><address><addrLine>Vienna,Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.48,345.13,398.41,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,281.81,345.13,123.63,9.96">The Wikipedia XML Corpus</title>
		<author>
			<persName coords=""><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,414.60,345.13,57.91,9.96">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.48,365.05,407.66,9.96;5,105.48,377.05,407.50,9.96;5,105.48,389.05,407.41,9.96;5,105.48,400.93,105.37,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,470.43,365.05,42.71,9.96;5,105.48,377.05,167.58,9.96">Similarity measures for tracking information flow</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaniv</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,294.48,377.05,218.50,9.96;5,105.48,389.05,264.42,9.96">CIKM &apos;05: Proceedings of the 14th ACM international conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="517" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.48,420.85,407.44,9.96;5,105.48,432.85,141.01,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,171.86,420.85,180.04,9.96">Overview of the trec 2004 novelty track</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,378.48,420.85,134.44,9.96;5,105.48,432.85,109.99,9.96">The Thirteenth Text Retrieval Conference (TREC 2004)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.48,452.77,257.65,9.96" xml:id="b5">
	<monogr>
		<ptr target="http://www.daedalus.es" />
		<title level="m" coord="5,105.48,452.77,58.08,9.96">Stilus website</title>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.48,472.69,34.10,9.96;5,157.82,472.69,291.37,9.96;5,474.02,472.69,39.00,9.96;5,105.48,484.69,196.85,9.96" xml:id="b6">
	<monogr>
		<ptr target="http://www.xapian.org" />
		<title level="m" coord="5,105.48,472.69,34.10,9.96;5,157.82,472.69,287.47,9.96">Xapian: an open source probabilistic information retrieval library</title>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
