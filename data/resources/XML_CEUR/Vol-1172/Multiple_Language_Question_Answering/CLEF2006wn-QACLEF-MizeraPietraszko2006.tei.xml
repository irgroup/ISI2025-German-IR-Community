<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,75.04,93.71,452.81,14.33;1,225.16,112.07,152.61,14.33">Accuracy of the AID System&apos;s Information Retrieval in Processing Huge Data Collections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,249.52,144.95,103.98,9.02"><forename type="first">Jolanta</forename><surname>Mizera-Pietraszko</surname></persName>
							<email>jolanta.mizera-pietraszko@pwr.wroc.pl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Information Systems</orgName>
								<orgName type="department" key="dep2">Institute of Applied Informatics</orgName>
								<orgName type="institution">Wroclaw University of Technology</orgName>
								<address>
									<addrLine>Wybrzeze Wyspianskiego 27, Building A1, Room 203</addrLine>
									<postCode>50-370</postCode>
									<settlement>Wroclaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,75.04,93.71,452.81,14.33;1,225.16,112.07,152.61,14.33">Accuracy of the AID System&apos;s Information Retrieval in Processing Huge Data Collections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7F2086AB92BFF4AD14464348578F0247</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>-Performance Evaluation H.2.3 [Database Management]: Languages -Query Languages Languages, Measurement, Performance, Experimentation Question answering, Question classes</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The name of the AID system stands for Answer Identifier in terms of opendomain Polish-English question answering system. Both the matching technique and indexing rely on simple question taxonomy. The concept of AID developed from Quest and Aranea which are both simple question answering software. In addition, AID employs direct MT model based on phrase-for-phrase translation having incorporated the LEC multilingual component. The system architecture heuristics include extracting passages, part-of-speech tagging, semantic relations and some parsing rules rather than exploring strategies for query reformulation which were found ineffective. Finally, the system's performance in the run submitted is evaluated on the British and American data collections.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In literature question answering is found as a more complex form of natural language processing. In this respect this is a step beyond information retrieval. In addition to document retrieval such a system is expected to browse the given local collection for precise and complete information in order to form a phrase or a sentence being an unambiguous response to the question posed. AID was developed at Wroclaw University of Technology on the base of Swiss Quest and Aranea produced at the MIT Computer Science and Artificial Intelligence Laboratory in Massachusetts, USA which was tested on TREC data collections. The system works on the Windows and the Linux platforms as well. In order to make the system platform-independent, the tools Knoppix and VPC2004 were used. Despite not residing on the HDD, the newest version of Knoppix embraces drivers for reading files and saving results while booting it from PC with Windows installed.</p><p>The approach presented in this paper is intended to be both simplistic and pragmatic. The backbone of the AID system is classification of the questions both at the translation and the analysis stages. Since this system participates in the CLEF 2006 experiment for the first time, some of its components are planned to be expanded so as to improve the performance on the larger CLEF collections. Having tested a number of information retrieval systems it came clear that producing a system prototype takes considerably less time than searching for and then studying the systems' instructions. Therefore this system has been constructed to serve the purpose of the CLEF experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Architecture of AID</head><p>Figure <ref type="figure" coords="2,100.72,211.55,4.98,9.02">1</ref> presents the system architecture. As seen, the relationship between its components are organized into the whole process due to the CLEF guidelines. Although the system structure may seem a little complex, in fact it reflects the most common components of any question answering system. Modularity represents the phases of the experiment in every detail. Consequently, the words are marked up according to their position in the question. In the next stage each of the words is translated automatically by the inbuilt dictionary. Figure <ref type="figure" coords="2,100.49,678.47,4.98,9.02">2</ref> shows the translation scheme on the base of the test set submitted to AID. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PL EN Ilu</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">English Test Set Processing</head><p>In this section, the process of the query analysis is described. At first, the question taxonomy relies on question classes and subclasses. The test set comprises nine question classes: WHAT, WHICH,WHO, WHERE, WHEN, HOW MANY, HOW MUCH, NAME and LIST. Subclasses on the other hand, constitute the questions that include preposition or a subject associated with the question word. Eventually, both a preposition and a subject.</p><p>The system component Semantic Analysis preprocesses the query by extracting some keywords that determine the type of the document to be indexed and then the text portion relevant to the information required. Query reformulation has been abandoned for a simple reason -in natural interactive verbal communication it is impossible to predict the form of the answer. In this light, any number of suggestive answer forms cannot result with the better system performance what is confirmed by the research reports [e.g. <ref type="bibr" coords="3,401.91,397.91,78.19,9.02" target="#b0">Brill E. et al., 2003]</ref>. AID collects all the documents in which the keywords occur.  <ref type="figure" coords="3,114.88,662.39,4.98,9.02">3</ref> presents, data processing was relatively easy for the system because all the documents have the same format on the contrary to the Web documents. In other words, we can call this type of collection as homogeneous.</p><p>The technique relying on eliminating entities that do not influence the information quality seems essential in case of large collections. This document format consists of many entities that are unnecessary in respect to the information relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Information extraction</head><p>For document selection AID deploys so called shallow technique that ranks the documents according to the syntactic similarity of the snippet to the question. The position of the keywords in the question and the answer determine reliability of the rank criteria. Lin's algorithm <ref type="bibr" coords="4,304.48,188.63,69.68,9.02" target="#b3">[Lin et al., 2003]</ref> used at this stage can be expressed as follows:</p><p>where: A is a set of keywords , N total number of words in the corpus, nXPEHURIRFFXUUHQFHVRIZRUGLQWKHSURSRVHGDQVZHU The system scores the relevant answers aligning the query keywords, the question word and the question notion according to the question taxonomy (classes and subclasses). The first document awards the highest score and the last ranked within the corpus, the lowest one. The most relevant documents seem to be those that retrieve the answers with the greatest number of the words associated both in the query and the snippet supporting the answer. The approach can be illustrated by the following example of definition question type:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What is Atlantis?</head><p>The question word is WHAT, the keyword is ATLANTIS and the notion is NAME.. The system response to this question was: R 0001 utjp061plen 0.981 LA110794-0104 space shuttle the space shuttle Atlantis</p><p>The answer was supported by "the space shuttle Atlantis" so the words between "the" and "Atlantis" form a subject that is why the confidence score is close to 1. Again, instead of grammatical analysis of the whole texts or the documents, AID focuses only on the words associated with the keywords. This methodology constitutes the backbone of the AID system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Experiment Framework</head><p>Despite registering for six out of eight tracks, our group submitted only one run named utjp061plen for Polish-English question answering task. This work has been proceeded by translation of the 200 questions into Polish and earlier the test set for the Ad Hoc track. Both translations have been posted on the CLEF Web site to be available for other participating groups. AID has been tested on two data collections:</p><p>• Los Angeles Times 1994 that contains 113,005 American documents and requires 425 MB • Glasgow Heralds 1995 that contains 56,472 British documents and requires 154 MB of HDD. Additionally, for previously planned Polish-French Ad Hoc track four other data collections have been downloaded:</p><p>• Le Monde 1994 that contains 44,013 French documents and requires 157 MB • Le Monde 1995 that contains 47,646 French documents and requires 156 MB</p><p>• SDA French 1994 with 43,178 documents that require 86MB</p><p>• SDA French 1995 with 42,615 documents that require 88 MB As intended at the beginning these six document collections have been downloaded on computer with Intel Celleron 370 MHz processor and 128 MB RAM. The process took around eight hours. Thus, the experiment was carried out on another computer with Pentium 1,6 GHz processor and 512 MB DDR2 working on two platforms: Windows XP and Linux. For decompression of the databases the gzip program was used whereas Jade DSSSL engine with backend that generates RTF formats empowered printing and displaying SGML documents. Other tools used in this experiment have been mentioned in the previous sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis of the System Performance</head><p>The system performance was evaluated manually on the base of responsiveness and exactness. The following judgments apply to the procedure: right (R), inexact( X when the answer is not complete), unsupported (U when the snippet is incorrect), wrong (W), and not assessed (Z). Table <ref type="table" coords="5,97.48,270.47,4.98,9.02">1</ref> shows the judgments in relation to the question classes.</p><p>Table <ref type="table" coords="5,168.28,567.23,3.76,9.02">1</ref>. Accuracy of AID performance measured over the question classes. This table presents the system accuracy within the class taxonomy. The overall accuracy of the right answers was as high as 86.32%. Such a result is very promising . However the table indicates the impact of the number of some question classes on the overall result. Out of two hundred questions AID produced 164 right answers, 14 wrong ones, 5 inexact ones, 7 unsupported and 0 not assessed. Assuming that the questions are dealt into definition, factoid, temporary restricted and list questions its accuracy equals accordingly to 88%, 80% and 0%. Instead of retrieving answer NIL 25 times, AID responded correctly in this case only 17 times which is 68%. Regarding the list questions, out of 31 correct answers, AID produced 18 right answers (58%), 0 wrong ones, 11 unsupported, 2 inexact and again 0 not assessed. This question type proved the most difficult for AID. As a result, the precision P@N = M / N where M is the number of right responses and N is the number of answers judged per question is 0.65 for the list questions.</p><p>For factoid and definition questions the evaluation measure was based on the MRR (Mean Reciprocal Rank) used also in TREC evaluation. It represents the mean between 0 in case of no correct responses and 1 when the system produces all the right answers at position 1. The overall Confidence Weighted Score for the system performance is CVS 151.602/190 = 0.79790</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The System Strong Sides</head><p>The overall score for the system performance proved excellent. This has been achieved by limitation of fields retrieved at the Document Retrieval stage, direct machine translation model used by the LEC tool and predominantly by methodology applied for the limited information extraction. Furthermore, AID benefits from its modularity and simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The System Limitations</head><p>It is equally important to mention that such a great accuracy lies in environment which is meant as specified conditions. AID has been built and adopted to the CLEF campaign requirements. It was rather a fast forward approach with its limitations being a consequence of the experiment deadlines. Thus, the first drawback is that we can talk about information extraction rather than answer formulation. Then, as yet AID was tested only on homogeneous collections not for instance on the net. The question taxonomy includes only nine question classes. With regard to the list questions, when the passage contains the target answer of two or more sentences, or the keywords are replaced by their synonyms AID fails producing NIL as an answer.</p><p>The list questions are processed correctly but on condition that the snippet contains the words or phrases associated. In any other case the names that are in isolation remain missed by the system. And eventually, the score for overall system accuracy depends on the formula that represents number of definition or factoid questions to the list or temporary restricted questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work is found by me as a great achievement for a number of reasons. The first one is that testing so many question answering systems gave me an idea about the AID methodology and the detailed concept of its architecture. It imposed on me browsing the Internet for the tools, sharing experience with the researchers almost all over the world working in the field, studying unknown yet software instructions in terms of having an intensive course in specific areas of information retrieval. As for the system performance, it gained a very high accuracy score in comparison to other participating groups and Polish was used for the first time in the CLEF campaign. The section above which describes the system limitations indicates how much work is to be done in the future so as to expand the system capacities making it to become a real open-domain question answering Polish-English system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.04,92.08,458.98,169.89"><head></head><label></label><figDesc>Figure 2. Word alignments in the Direct Translation modelSometimes more than one word have the same translation. In the example above both Polish "Ile" and "Ilu" have EHHQ WUDQVODWHG LQWR ³+RZ PDQ\´ ,Q VHQWHQFH WKH FRUUHFW WUDQVODWLRQ RI ³]RVWDáR´ Uequires using the Present Perfect tense "have been". These two examples represent linguistic phenomena between the language pair.</figDesc><table coords="3,72.04,92.08,350.58,100.80"><row><cell></cell><cell></cell><cell></cell><cell>PLHV]NDFyZ</cell><cell>jest</cell><cell>w</cell><cell>Longyearbyean?</cell></row><row><cell>0144</cell><cell>EN</cell><cell>PL</cell><cell cols="2">How many inhabitants are there in Longyearbyean?</cell></row><row><cell>0185</cell><cell>PL</cell><cell>EN</cell><cell cols="2">,OHZRMHQ]RVWDáRUR]HJUDQ\FKSRPLG]\L"</cell></row><row><cell>0185</cell><cell>EN</cell><cell>PL</cell><cell cols="2">How many wars have been fought between … and…?</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>I am grateful to my Adviser, <rs type="person">Professor Aleksander Zgrzywa</rs> and all the colleagues from my university for their invaluable comments. Without them this work would not be completed on time and especially the accuracy of the system performance would not achieve that high score. I would like to thank a student <rs type="person">Jakub Felski</rs> for sharing with me His knowledge and experience in the field, as well as being always on hand. I want to thank <rs type="person">Ms Jozefa Bernardyn</rs> for Her time and presentation of Knoppix. I would like to express my gratitude to <rs type="person">Professor Felisa Verdejo</rs> and the organizers for their kindness and support with my participation in the conference events. I also want to thank <rs type="person">dr. Carol Peters</rs> for arranging the enterprise aimed at integrating our research community. My participation in the Doctoral Consortium and the CLEF campaign gave a final shape to my Ph. D. thesis.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,72.04,131.15,458.98,9.02;7,72.04,142.55,88.29,9.02" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,196.96,131.20,225.45,8.88">An Analysis of the AskMSR Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>One Microsoft Way</publisher>
		</imprint>
		<respStmt>
			<orgName>Microsoft Research</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.04,165.59,459.04,9.02;7,72.04,177.11,282.10,9.02" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,72.04,165.59,306.38,9.02">FEMIa Framework for the Evaluation of Machine Translation in ISLE</title>
		<ptr target="http://www.isi.edu/natural-language/mteval" />
		<imprint/>
		<respStmt>
			<orgName>Information Science Institute, USC Viberti School of Engineering</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.04,200.15,387.93,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,119.92,200.20,195.42,8.88">LogoMedia&apos;s Translate -What does it translate?</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Guzman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,321.76,200.15,79.28,9.02">Localization Ireland</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.04,223.07,459.06,9.02;7,72.04,234.59,307.04,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,137.08,223.12,291.52,8.88">Question Answering from the Web Using Knowledge Mining Techniques</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,435.76,223.07,95.34,9.02;7,72.04,234.59,276.91,9.02">Proceedings of the 12th International Conference of Information and Knowledge Management</title>
		<meeting>the 12th International Conference of Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,74.56,257.63,456.42,9.02;7,72.04,269.03,171.09,9.02" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,223.36,257.68,307.62,8.88;7,72.04,269.03,114.83,9.02">A Fast Forward Approach to Cross-lingual Question Answering for English and German, Working Notes</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Strötgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>CLEF</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
