<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.89,115.90,331.58,12.68;1,270.73,133.83,73.90,12.68">CLEF 2017 Dynamic Search Lab Overview And Evaluation</title>
				<funder ref="#_ntJmY9E">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,216.54,171.50,86.85,8.80"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<email>e.kanoulas@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.54,171.50,63.81,8.80"><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
							<email>leif.azzopardi@strath.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Computer and Information Sciences</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.89,115.90,331.58,12.68;1,270.73,133.83,73.90,12.68">CLEF 2017 Dynamic Search Lab Overview And Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E92F8548FB6C1F0EB248463A67618AE1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we provide an overview of the first edition of the CLEF Dynamic Search Lab. The CLEF Dynamic Search lab ran in the form of a workshop with the goal of approaching one key question: how can we evaluate dynamic search algorithms? Unlike static search algorithms, which essentially consider user request's independently, and which do not adapt the ranking w.r.t the user's sequence of interactions, dynamic search algorithms try to infer the user's intentions from their interactions and then adapt the ranking accordingly. Personalized session search, contextual search, and dialog systems often adopt such algorithms. This lab provides an opportunity for researchers to discuss the challenges faced when trying to measure and evaluate the performance of dynamic search algorithms, given the context of available corpora, simulations methods, and current evaluation metrics. To seed the discussion, a pilot task was run with the goal of producing search agents that could simulate the process of a user, interacting with a search system over the course of a search session. Herein, we describe the overall objectives of the CLEF 2017 Dynamic Search Lab, the resources created for the pilot task, the evaluation methodology adopted, and some preliminary evaluation results of the Pilot task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information Retrieval (IR) research has traditionally focused on serving the best results for a single query -so-called ad-hoc retrieval. However, users typically search iteratively, refining and reformulating their queries during a session. IR systems can still respond to each query in a session independently of the history of user interactions, or alternatively adopt their model of relevance in the context of these interactions. A key challenge in the study of algorithms and models that dynamically adapt their response to a user's query on the basis of prior interactions is the creation of suitable evaluation resources and the definition of suitable evaluation metrics to assess the effectiveness of such IR algorithms. Over the years various initiatives have been proposed which have tried to make progress on this long standing challenge.</p><p>The TREC Interactive track <ref type="bibr" coords="1,278.06,632.15,9.96,8.80" target="#b7">[8]</ref>, which ran between 1994 and 2002, investigated the evaluation of interactive IR systems and resulted in an early standardization of the experimental design. However, it did not lead to a reusable test collection methodology. The High Accuracy Retrieval of Documents (HARD) track <ref type="bibr" coords="2,160.85,130.89,10.51,8.80" target="#b0">[1]</ref> followed the Interactive track, with the primary focus on single-cycle user-system interactions. These interactions were embodied in clarification forms which could be used by retrieval algorithms to elicit feedback from assessors. The track attempted to further standardize the retrieval of interactive algorithms, however it also did not lead to a reusable collection that supports adaptive and dynamic search algorithms. The TREC Session Track <ref type="bibr" coords="2,373.42,190.66,9.96,8.80" target="#b2">[3]</ref>, which ran from 2010 through 2014, made some headway in this direction. The track produced test collections, where included with the topic description was the history of user interactions with a system, that could be used to improve the performance of a given query. While, this mean adaptive and dynamic algorithms could be evaluated for one iteration of the search process, the collection's are not suitable for assessing the quality of retrieval over an entire session. In 2015, the TREC Tasks track <ref type="bibr" coords="2,160.44,274.35,15.49,8.80" target="#b13">[14,</ref><ref type="bibr" coords="2,175.93,274.35,11.62,8.80" target="#b12">13]</ref>, a different direction was taken, where the test collection provides queries for which all possible sub-tasks did to be inferred, and the documents relevant to those sub-tasks identified. Even though the produced test collections could be used in testing whether a system can help the user to perform a task end-to-end, the focus was not on adapting and learning from the user's interactions as in the case of dynamic search algorithms.</p><p>In the related domain of dialogue systems, the advancement of deep learning methods has led to a new generation of data-driven dialog systems. Broadlyspeaking, dialog systems can be categorized along two dimensions, (a) goaldriven vs. non-goal-driven, and (b) open-domain vs. closed domain dialog systems. Goal-driven open-domain dialog systems are in par with dynamic search engines: as they seek to provide assistance, advice and answers to a user over unrestricted and diverse topics, helping them complete their task, by taking into account the conversation history. While, a variety of corpora is available for training such dialog systems <ref type="bibr" coords="2,265.11,476.73,14.61,8.80" target="#b11">[12]</ref>, when it comes to the evaluation, the existing corpora are inappropriate. This is because they only contain a static set of dialogues and any dialog that does not develop in a way similar to the static set cannot be evaluated. Often, the evaluation of goal-driven dialogue systems focuses on goal-related performance criteria, such as goal completion rate, dialogue length, and user satisfaction. Automatically determining whether a task has been solved however is an open problem, while task-completion is not the only quality criterion of interest in the development of dialog systems. Thus, simulated data is often generated by a simulated user <ref type="bibr" coords="2,359.10,572.37,11.25,8.80" target="#b5">[6,</ref><ref type="bibr" coords="2,370.35,572.37,7.50,8.80" target="#b3">4,</ref><ref type="bibr" coords="2,377.85,572.37,11.25,8.80" target="#b10">11]</ref>. Given a sufficiently accurate model of how user's converse, the interaction between the dialog system and the user can be simulated over a large space of possible topics. Using such data, it is then possible to deduce the desired metrics. This suggests that a similar approach could be taken in the context of interactive IR. However, while significant effort has been made to render the simulated data as realistic as possible <ref type="bibr" coords="2,186.66,644.10,10.51,8.80" target="#b6">[7,</ref><ref type="bibr" coords="2,197.18,644.10,7.01,8.80" target="#b8">9]</ref>, generating realistic user simulation models remains an open problem.</p><p>The CLEF Dynamic Tasks Lab attempts to focus attention towards building a bridge between batch TREC-style evaluation methodology and the Interactive Information Retrieval evaluation methodology -so that dynamic search algorithms can be evaluated using re-usable test collections.</p><p>The objectives of the lab is threefold:</p><p>1. to devise a methodology for evaluating dynamic search algorithms by exploring the role of simulation as a means to create re-usable test collections, 2. to develop evaluation metrics that measure the quality during the session, both at different stages of the search process and at the end of the session, 3. to develop algorithms that can provide an optimal response in an interactive retrieval setup.</p><p>The focus of the CLEF 2017 Dynamic Tasks Lab is to provide a forum that can help foster research around the evaluation of dynamic retrieval algorithms. The Lab, in the form of a Workshop, solicits the submission of two types of papers: (a) position papers, and (b) data papers. The goal of position papers was evaluation methodologies for assessing the quality of search algorithms with the user in the loop, under two constraints: any evaluation framework proposed should allow the (statistical) reproduciblility of results, and lead to a reusable benchmark collection. The goal of the data papers was to describe test collections or data sets suitable for guiding the construction of dynamic test collections, tasks and evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pilot Task</head><p>Towards the aforementioned goals of generating simulation data the CLEF 2017 Dynamic Tasks Lab ran a pilot task in the context of developing Task Completion Engines <ref type="bibr" coords="3,191.60,464.78,10.51,8.80" target="#b1">[2]</ref> and Intelligent Search Agents <ref type="bibr" coords="3,334.96,464.78,9.96,8.80" target="#b6">[7]</ref>. Task Completion Engines and Autonomous Search Agents are being developed to help users in acquire information in order to make a decision and complete a search task. At the same time such Intelligent Search Agents, encode a model of a user, and so present the potential to simulate users submitting queries, which can enable the evaluation of dynamic search algorithms. Such engines/agents need to work with a user to ascertain their information needs, then perform their own searches to dynamically identify relevant material, which will be useful in completing a particular task. For example, consider the task of organizing a wedding. There are many different things that need to be arranged and ordered, e.g. a venue, flowers, catering, gift list, dresses, car hire, hotels, etc. Finding relevant sites and resources requires numerous searches and filtering through many documents/sites. A search agent could help to expedite the process by finding the relevant sites to visit, while a task completion engine would provide a structured interface to help complete the process.</p><p>In this year's Dynamic Search Task Track, the task can be interpreted in one of two ways:</p><p>1. to generate a series of queries that a search agent would issue to a search engine, in order to compile a set of links useful for the user. This set might be presented to the user or used for further processing by a task completion engine; or 2. to generate a series of query suggestions that a search engine would recommend to the user, and thus the suggested course of interaction.</p><p>As a starting point, for building a test collection, we first consider how people look for information required to complete various casual leisure and work tasks. The history of queries and interactions are then used as a reference point during the evaluation to determine if agents/dynamic search algorithms/query suggestion algorithms that can generate queries that are like those posed by people. And thus, see how well human like queries can be generated for suggestions)? Thus the focus of the track is on query generation and models of querying, based on task and interaction history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Description and Data Sets</head><p>Starting with an initial query for a given topic, the task is to generate a series of subsequent or related queries. The data used is TREC ClueWeb Part B test collection and the topics used are sourced from the Session Track 2014 <ref type="bibr" coords="4,455.72,377.96,9.96,8.80" target="#b2">[3]</ref>. A fixed search engine was setup, where ClueWeb was indexed using ElasticSearch. The title, url, page contents were indexed, along with the spam rank and page rank of each document. The ElasticSearch API was then provided as the "search engine" that the agent or person is using to undertake each task/topic. From the Session Track 2014 topics, a subset of 26 topics were selected out of the original 50, based on the following criteria: there were four or more queries associated with the topic, where the subsequent interaction on each query lead to identifying at least one TREC relevant document. These were considered, good or useful, queries i.e. they helped identify relevant material. The set of "good" queries were with-held as the relevant set. The TREC topic title, was provided to participants as the initial seed query i.e. the first query in the session.</p><p>Interaction data was then provided to provide simulated interaction with queries issued to the search engine. It was anticipated that the simulated clicks could be used by the algorithms to help infer relevance of the documents. This data could be used could be used as (a) a classifier providing relevance decisions regarding observed items in the result list, or (b) as clicks that the user performed when viewing the results of a query (i.e. given a query, assume that this is what the user clicks on, to help infer the next query). A set of judgments/click was generated based on the probability of Session Track users clicks data, conditioned by relevance (i.e. the probability of a click, if then document was TREC Relevant or TREC Non-Relevant).</p><p>The task, then, was to provide a list of query suggestions/recommendations along with a list of up to 50 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Participants</head><p>Two teams participated in this Pilot Task: (1) the Web Technology and Information Systems, Bauhaus-Universitat Weimar, Germany (Webis), and (2) the Vienna University of Technology (TUW). The Webis team submitted on set of query suggestions, and a list of up to 50 documents. The TUW team focused on the document retrieval, submitting only a ranked list of documents.</p><p>Webis <ref type="bibr" coords="5,182.56,203.25,10.51,8.80" target="#b4">[5]</ref> evaluated query suggestions in form of keyqueries for clicked documents. A query q is a keyquery for a document set D iff q returns the documents from D in its top-k ranks, q has at least l results, and no subquery of q has the previous two properties. Our query suggestion approach derives keyqueries for pairs of documents previously clicked by the user. The Dynamic Search Lab data contains 26 topics, along with one query submitted by some user in a search session and the shown results from the whole session with some indicated as being clicked by the user. Exactly for these clicked documents, they derive keyqueries as query suggestions. Regarding the ranked list for a topic, they used the top-10 results of each of the at most five derived keyqueries returned by the Dynamic Search API and merged them as follows: first the first ranks of the queries, then the second ranks, etc.; duplicate results that already were in the merged list were replaced by the next result from the same keyquery. Note that the Webis team submitted results for 19 out of the 26 topics.</p><p>TUW <ref type="bibr" coords="5,181.83,371.40,15.49,8.80" target="#b9">[10]</ref> propose the creation of a search agent that specifically leverages the structure of Wikipedia articles to understand search tasks. Their assumption is that human editors carefully choose meaningful section titles to cover the various aspects of an article. Their proposed search agent is responsible for two tasks: identifying the key Wikipedia articles related to a complex search task, and selecting section titles from those articles. TUW contributed 5 runs: TUW_0_baseline.run is an ElasticSearch BM25 run that uses the query field for each information need provided; TUW_1 human.run is a manual run where a human judge selected up to 5 section titles from the top Wikipedia articles returned by the Wikipedia API; TUW_2_First_Five.run automatically choosing the first five sections of Wikipedia, TUW_3_Word2VecMean.run used as background text the description of the information need provided and compared it to the text used in each individual section of Wikipedia; it made use of the cosine similarity between the vector representation of each word in the both texts, with text being represented by Word2Vec trained on GoogleNews; TUW_4_W2V_Plus_NB.run extended the TUW_3 run by predicting the relevance of each retrieved document automatically before applying the round-robin algorithm to merge the result of each query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>Given that evaluation is an open problem, the lab was also open to different ways in which to evaluate this task. Some basic measures that were considered are:</p><p>-Query term overlap: how well do the query terms in the suggestions match with the terms used -Query likelihood: how likely are the queries suggested given the model of relevance. -Precision and Recall based measures on the set of documents retrieved.</p><p>-Suggest your own measure: how to measure the usefulness and value of queries is a rather open question. So how can we evaluate how good a set of queries are in relation to completing a particular task?</p><p>During the lab, we will discuss the various challenges of constructing reusable test collections for evaluating dynamic search algorithms and how we can develop appropriate evaluation measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>The intrinsic evaluation the suggested queries is a hard problem. Therefore the preliminary results for the Pilot task focus on the extrinsic evaluation of these queries, by evaluating the ranked list of retrieved document. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>The goal of the CLEF 2017 Dynamic Search lab is to answer one key question: how can we evaluate dynamic search algorithms? Towards this question one position paper was submitted, and will be presented during the CLEF workshop. Further, a Pilot task was put in place, using data from the TREC 2014 Session track, in an attempt to test user simulations. Two teams participated in the task, submitting a total of 6 runs. One team submitted suggested queries, while both teams submitted ranked lists of results. The intrinsic evaluation of queries appeared to be a hard problem, hence participants were evaluated extrinsically, on the basis of their submitted ranked lists. The results are preliminary to draw any conclusions. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,134.77,571.04,345.83,7.93;7,134.77,582.01,154.84,7.92;7,134.77,188.83,345.82,367.47"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Box-plots of average precision values across the topics; the blue dashed line represents the mean average precision.</figDesc><graphic coords="7,134.77,188.83,345.82,367.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.77,313.11,345.83,160.43"><head>Table 1 .</head><label>1</label><figDesc>Table1contains mean values of precision at cut-off 50, recall at cut-off 50, and average precision. Figure1presentes the box plots for average precision across the 26 topics for TUV and 19 topics for Webis. Evaluation of the submitted ranked lists of results by participants.</figDesc><table coords="6,171.08,381.40,270.13,74.07"><row><cell>Run</cell><cell>P@50</cell><cell>R@50</cell><cell>AP</cell></row><row><cell>tuw0_only_query</cell><cell>0.1177</cell><cell>0.0504</cell><cell>0.0181</cell></row><row><cell>tuw1_human</cell><cell>0.1031</cell><cell>0.0472</cell><cell>0.0147</cell></row><row><cell>tuw2_fifo</cell><cell>0.0885</cell><cell>0.0393</cell><cell>0.0099</cell></row><row><cell>tuw3_mean</cell><cell>0.0900</cell><cell>0.0409</cell><cell>0.0133</cell></row><row><cell>tuw4_nb_mean</cell><cell>0.0254</cell><cell>0.0076</cell><cell>0.0020</cell></row><row><cell>webis-result-list</cell><cell>0.1411</cell><cell>0.0502</cell><cell>0.0194</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was partially supported by the <rs type="programName">Google Faculty Research Award program</rs> and the <rs type="grantName">Microsoft Azure for Research Award program</rs> (CRM:<rs type="grantNumber">0518163</rs>). All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors. We would also like to thank <rs type="person">Dr. Guido Zuccon</rs> for setting up the ElasticSearch API.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ntJmY9E">
					<idno type="grant-number">0518163</idno>
					<orgName type="grant-name">Microsoft Azure for Research Award program</orgName>
					<orgName type="program" subtype="full">Google Faculty Research Award program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.95,243.11,337.64,7.92;8,151.52,254.07,142.32,7.92" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,190.35,243.11,285.69,7.92">Hard track overview in trec 2003 high accuracy retrieval from documents</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">DTIC Document</note>
</biblStruct>

<biblStruct coords="8,142.95,264.79,337.64,7.92" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,194.79,264.79,183.18,7.92">Task-completion engines: A vision with a plan</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,398.50,264.79,53.74,7.92">SCST@ECIR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,275.52,337.64,7.92;8,151.52,286.48,329.07,7.92;8,151.52,297.44,329.07,7.92;8,151.52,308.40,329.07,7.92;8,151.52,319.35,329.07,7.92;8,151.52,331.01,197.66,7.47" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,454.62,275.52,25.98,7.92;8,151.52,286.48,261.31,7.92">Evaluating retrieval over sessions: The TREC session track 2011-2014</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<idno type="DOI">10.1145/2911451.2914675</idno>
		<ptr target="http://doi.acm.org/10.1145/2911451.2914675" />
	</analytic>
	<monogr>
		<title level="m" coord="8,384.13,297.44,96.46,7.92;8,151.52,308.40,329.07,7.92;8,151.52,319.35,104.10,7.92">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Perego</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Ruthven</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</editor>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">July 17-21, 2016. 2016</date>
			<biblScope unit="page" from="685" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,341.04,337.63,7.92;8,151.52,352.00,276.04,7.92" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,317.01,341.04,163.58,7.92;8,151.52,352.00,117.66,7.92">User simulation for spoken dialogue systems: learning and evaluation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,290.31,352.00,44.17,7.92">Interspeech</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1065" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,362.72,337.63,7.92;8,151.52,373.68,329.07,7.92;8,151.52,384.64,329.07,7.92;8,151.52,395.60,88.31,7.92" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,351.64,362.72,128.95,7.92;8,151.52,373.68,38.80,7.92">Webis at the clef 2017 dynamic search lab</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alshomary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="8,210.57,373.68,270.02,7.92;8,151.52,384.64,40.16,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="8,358.50,384.64,122.09,7.92;8,151.52,395.60,21.69,7.92">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,406.33,337.64,7.92;8,151.52,417.28,329.07,7.92;8,151.52,428.24,286.82,8.17" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,365.69,406.33,114.90,7.92;8,151.52,417.28,202.87,7.92">Data-driven user simulation for automated evaluation of spoken dialog systems</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csl.2009.03.002</idno>
		<ptr target="http://dx.doi.org/10.1016/j.csl.2009.03.002" />
	</analytic>
	<monogr>
		<title level="j" coord="8,361.81,417.28,92.02,7.92">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="479" to="509" />
			<date type="published" when="2009-10">Oct 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,438.97,337.64,7.92;8,151.52,449.93,329.07,7.92;8,151.52,460.89,329.07,7.92;8,151.52,471.84,25.59,7.92" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,267.79,438.97,212.80,7.92;8,151.52,449.93,110.84,7.92">Agents, simulated users and humans: An analysis of performance and behaviour</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,285.86,449.93,194.73,7.92;8,151.52,460.89,226.03,7.92;8,439.95,460.89,40.64,7.92">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="731" to="740" />
		</imprint>
	</monogr>
	<note>CIKM &apos;16</note>
</biblStruct>

<biblStruct coords="8,142.95,482.57,337.64,7.92;8,151.52,493.53,183.35,7.92" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,190.69,482.57,214.22,7.92">The trec interactive track: an annotated bibliography</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,412.16,482.57,68.43,7.92;8,151.52,493.53,92.75,7.92">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="369" to="381" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,504.25,337.64,7.92;8,151.52,515.21,329.08,7.92;8,151.52,526.17,135.69,7.92" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,204.11,515.21,221.96,7.92">Validating simulated interaction for retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pääkkönen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Keskustalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,432.97,515.21,47.62,7.92;8,151.52,526.17,69.65,7.92">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,536.90,337.98,7.92;8,151.52,547.86,329.07,7.92;8,151.52,558.82,329.08,7.92;8,151.52,569.77,185.32,7.92" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,196.61,536.90,283.98,7.92;8,151.52,547.86,112.14,7.92">Leveraging wikipediaâĂŹs article structure to build search agents: Tuw at clef 2017 dynamic search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="8,285.77,547.86,194.82,7.92;8,151.52,558.82,122.60,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="8,453.98,558.82,26.61,7.92;8,151.52,569.77,118.70,7.92">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,580.50,337.98,7.92;8,151.52,591.46,224.00,7.92" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,250.17,580.50,226.42,7.92">A survey on metrics for the evaluation of user simulations</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,151.52,591.46,138.01,7.92">The knowledge engineering review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="59" to="73" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,602.18,337.98,7.92;8,151.52,613.14,329.07,7.92;8,151.52,624.80,145.89,7.47" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="8,398.26,602.18,82.33,7.92;8,151.52,613.14,199.85,7.92">A survey of available corpora for building data-driven dialogue systems</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<idno>CoRR abs/1512.05742</idno>
		<ptr target="http://arxiv.org/abs/1512.05742" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,634.83,337.97,7.92;8,151.52,645.79,329.07,7.92;8,151.52,656.74,329.07,7.92;9,151.52,119.62,329.08,7.92;9,151.52,130.58,33.27,7.92" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,195.87,645.79,161.99,7.92">Overview of the TREC tasks track 2016</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,177.59,656.74,298.80,7.92">Proceedings of The Twenty-Fifth Text REtrieval Conference, TREC 2016</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting>The Twenty-Fifth Text REtrieval Conference, TREC 2016<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">November 15-18, 2016</date>
			<biblScope unit="page" from="500" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,152.50,337.97,7.92;9,151.52,163.46,329.07,7.92;9,151.52,174.42,329.07,7.92;9,151.52,185.37,312.21,7.92" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,151.52,163.46,161.20,7.92">Overview of the TREC 2015 tasks track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,463.04,163.46,17.55,7.92;9,151.52,174.42,284.69,7.92">Proceedings of The Twenty-Fourth Text REtrieval Conference, TREC 2015</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting>The Twenty-Fourth Text REtrieval Conference, TREC 2015<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">November 17-20, 2015</date>
			<biblScope unit="page" from="500" to="319" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
