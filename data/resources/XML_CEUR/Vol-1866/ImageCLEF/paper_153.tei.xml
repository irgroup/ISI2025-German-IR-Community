<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.08,116.95,329.21,12.62;1,162.20,134.89,290.96,12.62;1,178.22,152.82,258.92,12.62;1,196.62,170.75,222.12,12.62">Generating captions for medical images with a deep learning multi-hypothesis approach: MedGIFT-UPB Participation in the ImageCLEF 2017 Caption Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,171.22,208.42,84.68,8.74"><forename type="first">Liviu-Daniel</forename><forename type="middle">S</forename><surname>¸tefan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<postCode>061071</postCode>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.46,208.42,68.83,8.74"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<postCode>061071</postCode>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,365.22,208.42,68.10,8.74"><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.08,116.95,329.21,12.62;1,162.20,134.89,290.96,12.62;1,178.22,152.82,258.92,12.62;1,196.62,170.75,222.12,12.62">Generating captions for medical images with a deep learning multi-hypothesis approach: MedGIFT-UPB Participation in the ImageCLEF 2017 Caption Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D55CA65F21132E6353BC4ECE878DC102</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>medical images analysis</term>
					<term>biomedical concepts</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this report, we summarize our solution to the ImageCLEF 2017 caption detection task. ImageCLEF's concept detection task provides a testbed for figure caption prediction oriented systems using medical concepts as sentence level descriptions for images, extracted from the Unified Medical Language System (UMLS) dataset. The goal of the task is to efficiently identify the relevant medical concepts from medical images as a predictor of figure captions. For representing the images we used a very deep Convolutional Neural Network, namely ResNet-152 pre-trained on ImageNet and a binary annotation of the concepts. In the concept detection subtask, MedGIFT-UPB group occupied the 3rd place out of 9 groups. the proposed approach obtained the 12th position according to the f1 score (0.89) out of 20 participant runs (runs without external resources). The paper presents the procedure employed, and provides an analysis of the obtained evaluation results. The results were showed the difficulty of the task when not using any other external resources.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF <ref type="foot" coords="1,187.76,541.68,3.97,6.12" target="#foot_0">4</ref> is an image classification and retrieval benchmark that was founded in 2003 and has been run every year since then <ref type="bibr" coords="1,341.69,555.21,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,353.86,555.21,7.01,8.74" target="#b1">2]</ref>. A medical task was added in 2014 and has been held every year since then in varied forms <ref type="bibr" coords="1,415.66,567.17,9.96,8.74" target="#b2">[3]</ref>.</p><p>The aim of the ImageCLEF <ref type="bibr" coords="1,283.04,579.12,10.52,8.74" target="#b3">[4]</ref> concept detection task is to assign to a biomedical image extracted from scholarly articles of the biomedical open access literature (PubMed Central), a set of medical concepts taken from a list of 20.463 possible pre-defined medical concepts. In this task, the participants are given a large-scale noisy dataset of 164,614 training images and 10,000 validation images associated with their corresponding labels, each of which corresponds to a UMLS (Unified Medical Language System) concept. In the test phase, the participants are requested to give to each of the 10,000 test images the labels of all the medical concepts describing the image. The performance evaluation is done at image level by computing the F-Measure for each image. For more details on task setup and full participant results, please refer to <ref type="bibr" coords="2,369.83,167.81,9.96,8.74" target="#b4">[5]</ref>.</p><p>The challenge we addressed in our participation was the semantic gap between captions (text) and the image. Thus, the scientific challenge was to exploit these two types of information in order to automatically describe new medical images without any other resources than the training images provided. Having this in mind, we propose a flexible deep CNN that takes an arbitrary number of hypotheses as the inputs and produce at output the ultimate multi-label predictions. Our approach is based on visual features learned using a very deep Convolutional Neural Network, namely ResNet-152 <ref type="bibr" coords="2,332.55,263.50,9.96,8.74" target="#b5">[6]</ref>. We pretrained the ResNet-152 using the ImageNet model as the initialization of the network with a Sigmoid Cross Entropy Loss Layer in the train phase and Sigmoid layer in the test phase. The labels were binarized using 1 for the relevant concepts and 0 for the other concepts. Finally, we used a threshold over the scores to construct the concept list for the test images.</p><p>The remainder of this paper is organized as follows: In Section 2, we investigate the network from a model-selection and optimization perspective. Section 3 reports the experimental setup. In Section 4 we report the results of our runs with respect to the top scores. Finally in Section 5 we present conclusions and discuss current and future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>To take full advantage of CNNs for multi-label image classification, in this paper we propose a flexible deep CNN structure that takes an arbitrary number of hypotheses (concepts) as input. The proposed method uses the ResNet-152 CNN and the ImageNet model as the initialization of the network. In the training phase we used the Sigmoid Cross Entropy Loss Layer. For training we binarized the labels using 1 for the relevant concepts and 0 for the rest. In the test phase we used a Sigmoid layer which produces a probability distribution over the class labels. Finally the network output gives the final multilabel scores that are thresholded to construct the concept list for the test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network Description</head><p>The ResNet implemented in this paper is based on He et al. <ref type="bibr" coords="2,401.27,585.33,10.52,8.74" target="#b5">[6]</ref> which achieved state-of-the-art results in the ILSVRC <ref type="bibr" coords="2,315.51,597.29,10.52,8.74" target="#b6">[7]</ref> and COCO 2015 <ref type="bibr" coords="2,409.89,597.29,10.52,8.74" target="#b7">[8]</ref> competitions obtaining first places in all main tracks. ResNet-152 is composed of a 152-layer Residual Network having the following architecture (Table <ref type="table" coords="2,392.96,621.20,3.87,8.74" target="#tab_0">1</ref>).</p><p>The central idea of ResNets is to solve the neural network tendency to obtain higher training error as the depth increases due the gradient and training signals vanishing when they are propagated through many layers. This is done by </p><formula xml:id="formula_0" coords="3,220.39,149.31,162.73,64.65">× 1 max-pool 3 × 3(stride 2) - conv1 1 × 1: 3 × 3: 1 × 1 × 3 conv2 1 × 1: 3 × 3: 1 × 1 × 8 conv3 1 × 1: 3 × 3: 1 × 1 × 36 conv5 1 × 1: 3 × 3: 1 × 1 × 3</formula><p>adding skip connections that bypass a few convolutional layers at a time. Each bypass generates a residual block in which the convolution layers predict a residual that is added to the block's input tensor. The last layer of each residual block is applied with a global batch normalization, with the beta and gamma parameters being set to trainable all throught-out. Following most CNN architectures, Rectified Linear units (ReLU) are added after the BN layers.</p><p>We decided to start with ResNet-152 as a baseline due to the capacity of the network to avoid negative outcome while increasing network depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-Label Classification</head><p>Multi-label image classification is a practical problem, due to majority of realworld images being with more than one object of usually different categories. To tackle this problem we use a cross-entropy loss layer that is defined as:</p><formula xml:id="formula_1" coords="3,176.60,426.59,304.00,30.32">L(X, Y ) = - 1 n n i=1 y (i) ln a(x (i) ) + 1 -y (i) ln 1 -a(x (i) )<label>(1)</label></formula><p>where, X = x (1) , . . . , x (n) , is the set of input examples in the training dataset, and Y = y (1) , . . . , y (n) is the corresponding set of labels for these input examples. The a(x) represents the output of the neural network given input x. Each of the y (i) is either 0 or 1, and the output activation a(x) is restricted to the open interval (0, 1) by using a sigmoid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>The aim of the experiments described in this section was to not use external resources for enriching the description of the medical images. Therefore, a single kind of information was used: visual information. As the training dataset is extremely small and the concept relations are relatively complex, as it can be seen in Figure <ref type="figure" coords="3,196.17,621.25,3.87,8.74" target="#fig_0">1</ref>, training very deep CNNs is quite challenging. Deep features have demonstrated that CNN models pre-trained on large-scale single-label datasets with data diversity e.g., ImageNet, can be transferred to extract features for other image datasets without many training samples. Run 1: DET ResNet152 SCEL t 0.06.txt:</p><p>In this run we only used visual features learned using a very deep Convolutional Neural Network. We build a 152-layer ResNet with 50 bottleneck residual blocks. When input and output dimensions did not match, the skip connection uses a learned linear projection for the mismatching dimensions, and an identity transformation for the other dimensions.</p><p>Training In training, each image is resized into 256×256 pixels without cropping. At each iteration, a mini-batch of 4 samples is constructed by sampling 4 training images from the dataset with the gradient being accumulated over 64 batches. The batches were formed by randomly shuffling the database with the random seed parameter set to 45. We pre-trained the ResNet-152 using a starting learning rate of 10 -7 for weights, with a decay of 9 -10 . Optimization is performed using Stochastic Gradient Descent (SGD), with a momentum of 0.9. We executed 40 epoches in total and decreased the learning rate with a polynomial decreasing policy at a power of 3. The network was trained using the Caffe framework <ref type="bibr" coords="4,183.81,602.16,10.51,8.74" target="#b8">[9]</ref> running on an NVIDIA Quadro M4000.</p><p>Testing The output of the last fully-connected layer of the ResNet-152 is fed into a c-way softmax that produces a probability distribution over the class labels. In order to construct the concept list for each image, we defined a threshold. To define an appropriate value for this threshold we performed an exploration by using the validation dataset. In this run we assigned the best threshold found in the exploration performed on the validation set (T = 0.06). This threshold was used for all concepts and yields an F1 score of 0.104 on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In our participation, we submitted one run to the concept detection task. In this section, we describe the run and compare the performance with other automated runs that did not use any external sources.</p><p>In Table <ref type="table" coords="5,189.01,243.86,3.87,8.74" target="#tab_1">2</ref>, it can be seen the performance of the proposed and the top techniques for the caption detection subtask. The table reports scores corresponding to the test set, from the metric proposed by the organizers. Our score is reported by run1, which uses a threshold over the scores of the class labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we present a framework to address the multilabel image classification problem. However, training a multilabel CNN is not applicable on noisy datasets with a limited number of training samples due to the large number of parameters to be learned. We show that a CNN pre-trained on single label image datasets, e.g., ImageNet, can be transferred to tackle the multi-label problem. A research direction is to introduce more aggressive data augmentation techniques designed to improve the network generalization capabilities. Aligned with new techniques to generate medical hypotheses, training a multi-label CNN on a medical dataset would become more feasible. Still, the task at hand is clearly very difficult without use of external resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,212.43,369.32,190.50,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Shows the number of samples per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,215.70,116.91,183.95,40.27"><head>Table 1 .</head><label>1</label><figDesc>ResNet-152 architecture.</figDesc><table coords="3,215.70,135.96,183.95,21.21"><row><cell cols="2">Layer name Residual-Block No. of blocks</cell></row><row><cell>conv1</cell><cell>7 × 7(stride 2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,310.81,345.82,89.04"><head>Table 2 .</head><label>2</label><figDesc>Selected subset of results from ImageCLEFcaption (2017) -concept detection task. Comparison of our submitted run to top scores with no external resources.</figDesc><table coords="5,177.17,342.54,261.02,57.31"><row><cell>Group Name</cell><cell cols="2">Run type Mean F1 Score</cell></row><row><cell>Aegean AI Lab</cell><cell>Automatic</cell><cell>0.158</cell></row><row><cell cols="2">Information Processing Laboratory Automatic</cell><cell>0.143</cell></row><row><cell cols="2">Information Processing Laboratory Automatic</cell><cell>0.141</cell></row><row><cell>MedGIFT-UPB</cell><cell>Automatic</cell><cell>0.089</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="1,144.73,658.44,117.68,7.47"><p>http://www.imageclef.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,138.35,635.88,342.24,7.86;5,146.91,646.84,333.68,7.86;5,146.91,657.79,273.86,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,367.57,635.88,113.02,7.86;5,146.91,646.84,167.81,7.86">ImageCLEF -Experimental Evaluation in Visual Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,374.25,646.84,106.34,7.86;5,146.91,657.79,127.18,7.86">The Springer International Series On Information Retrieval</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,120.67,342.24,7.86;6,146.91,131.63,333.68,7.86;6,146.91,142.59,333.68,7.86;6,146.91,153.55,333.68,7.86;6,146.91,164.51,279.60,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,410.92,142.59,69.67,7.86;6,146.91,153.55,154.71,7.86">General overview of ImageCLEF at the CLEF 2015 labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kazi Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D M</forename><surname>Roldán García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,324.09,153.55,118.72,7.86">Working Notes of CLEF 2015</title>
		<title level="s" coord="6,450.23,153.55,30.36,7.86;6,146.91,164.51,107.98,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,175.46,342.24,7.86;6,146.91,186.42,333.68,7.86;6,146.91,197.38,333.68,7.86;6,146.91,208.32,269.83,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,262.09,186.42,218.50,7.86;6,146.91,197.38,329.28,7.86">Evaluating performance of biomedical image retrieval systems: Overview of the medical image retrieval task at ImageCLEF 2004-2014</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,146.91,208.34,183.45,7.86">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="55" to="61" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,219.30,342.24,7.86;6,146.91,230.26,333.68,7.86;6,146.91,241.22,333.68,7.86;6,146.91,252.18,333.67,7.86;6,146.91,263.14,333.68,7.86;6,146.91,274.09,22.02,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,433.03,241.22,47.56,7.86;6,146.91,252.18,214.52,7.86">Overview of ImageCLEF 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,382.05,252.18,94.32,7.86">CLEF 2017 Proceedings</title>
		<title level="s" coord="6,146.91,263.14,146.04,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,285.05,342.24,7.86;6,146.91,296.01,333.68,7.86;6,146.91,306.97,333.68,7.86;6,146.91,317.93,333.68,7.86;6,146.91,328.89,22.02,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,414.53,285.05,66.06,7.86;6,146.91,296.01,333.68,7.86;6,146.91,306.97,130.56,7.86">Overview of Im-ageCLEFcaption 2017 -the image caption prediction and concept extraction tasks to understand biomedical images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target=".org¡http://ceur-ws.org¿" />
	</analytic>
	<monogr>
		<title level="m" coord="6,299.47,306.97,181.12,7.86;6,146.91,317.93,46.41,7.86">CLEF2017 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Dublin, Ireland, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,339.85,342.25,7.86;6,146.91,350.78,127.81,7.89" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,297.93,339.85,178.74,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,361.77,342.24,7.86;6,146.91,372.73,333.68,7.86;6,146.91,383.68,333.68,7.86;6,146.91,394.62,123.55,7.89" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,415.52,372.73,65.07,7.86;6,146.91,383.68,145.61,7.86">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,305.77,383.68,174.83,7.86;6,146.91,394.64,28.80,7.86">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,405.60,342.24,7.86;6,146.91,416.56,333.67,7.86;6,146.91,427.49,158.84,7.89" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,327.76,416.56,152.83,7.86;6,146.91,427.52,28.22,7.86">Microsoft COCO: common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>CoRR abs/1405.0312</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,438.48,342.24,7.86;6,146.91,449.44,333.68,7.86;6,146.91,460.40,154.44,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="6,235.61,449.44,240.38,7.86">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
