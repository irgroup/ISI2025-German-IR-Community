<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.87,116.95,303.62,12.62;1,156.53,134.89,302.29,12.62;1,183.54,159.50,248.27,7.89">Keyword Generation for Biomedical Image Retrieval with Recurrent Neural Networks FHDO Biomedical Computer Science Group (BCSG)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,207.95,193.66,61.02,8.74"><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
							<email>obioma.pelka@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science of Applied Sciences and Arts Dortmund (FHDO)</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<addrLine>Emil-Figge-Strasse 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">-Essen University School of Medicine</orgName>
								<address>
									<addrLine>Hufelandstrasse 55</addrLine>
									<postCode>45147</postCode>
									<settlement>Duisburg, Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.47,193.66,100.46,8.74"><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
							<email>christoph.friedrich@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science of Applied Sciences and Arts Dortmund (FHDO)</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<addrLine>Emil-Figge-Strasse 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.87,116.95,303.62,12.62;1,156.53,134.89,302.29,12.62;1,183.54,159.50,248.27,7.89">Keyword Generation for Biomedical Image Retrieval with Recurrent Neural Networks FHDO Biomedical Computer Science Group (BCSG)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D2F1B9C4CD8C2538623B569A32D89AB7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>biomedical image retrieval</term>
					<term>keyword generation</term>
					<term>computer vision</term>
					<term>convolutional neural networks</term>
					<term>long short-term memory</term>
					<term>recurrent neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the modeling approaches performed by the FHDO Biomedical Computer Science Group (BCSG) for the caption prediction task at ImageCLEF 2017. The goal of the caption prediction task is to recreate original image captions by detecting the interplay of present visible elements. A large-scale collection of 164,614 biomedical images, represented as imageID -caption pairs, extracted from open access biomedical journal articles (PubMed Central) was distributed for training. The aim of this presented work is the generation of image keywords, which can be substituted as text representation for classifications tasks and image retrieval purposes. Compound figure delimiters were detected and removed as estimated 40% of figures in PubMed Central are compound figures. Text preprocessing such as removal of stopwords, special characters and Porter stemming were applied before training the models. The images are visually represented using a Convolutional Neural Network (CNN) and the Long Short-Term Memory (LSTM) based Recurrent Neural Network (RNN) Show-and-Tell model is adopted for image caption generation. To improve model performance, a second training phase is initiated where parameters are fine-tuned using the pre-trained deep learning networks Inception-v3 and Inception-ResNet-v2. Ten runs representing the different model setups were submitted for evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the modeling methods and experiments performed by the FHDO Biomedical Computer Science Group (BCSG) at the ImageCLEF 2017 <ref type="bibr" coords="2,134.77,119.99,10.52,8.74" target="#b7">[8]</ref> Caption Prediction Task. The caption prediction task, which aims to recreate original image captions by detecting the interplay of present visible elements <ref type="bibr" coords="2,134.77,143.90,9.96,8.74" target="#b4">[5]</ref>, is addressed in this paper. The focus of this presented work is more on the automated generation of keywords for biomedical and medical images and not caption prediction. Several approaches <ref type="bibr" coords="2,307.44,167.81,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="2,319.62,167.81,7.75,8.74" target="#b8">9,</ref><ref type="bibr" coords="2,329.03,167.81,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="2,343.42,167.81,12.73,8.74" target="#b11">12]</ref> have shown that combining visual image representation with text obtains better image classification performance. However, for some image classification tasks, such as ImageCLEF2009 Medical Annotation Task <ref type="bibr" coords="2,250.07,203.68,9.96,8.74" target="#b3">[4]</ref>, corresponding text representations are not available. These keywords can be substituted as text representations and combined with visual representations to obtain multi-modal image representations. These multi-modal image representations can be further adopted for image retrieval purposes.</p><p>The remaining of this paper is organized as follows: Section 2 explains the methodology adopted. The image keyword generation setups, submitted runs and results are displayed and discussed in section 3. Finally, conclusions are drawn in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset</head><p>All figures distributed in the ImageCLEF 2017 Caption Prediction Task originate from biomedical literature published in PubMed Central. The training set contains 164,614 image -caption pairs. An additional validation set of 10,000 biomedical image -caption pairs were distributed for evaluation purposes in the development stage. For the official evaluation, computed using BLEU scores <ref type="bibr" coords="2,462.33,421.59,14.61,8.74" target="#b10">[11]</ref>, a test set of 10,000 biomedical images was distributed. For keyword generation tasks, BLEU score is not suited as an evaluation metric. The order of words and length of captions have significant effects on the calculated scores. Further information is detailed in <ref type="bibr" coords="2,248.03,469.41,9.96,8.74" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Preprocessing</head><p>Focusing on image keyword generation, certain contents in biomedical figure captions are undesirable and were omitted. The preprocessing steps done before model training were:</p><formula xml:id="formula_0" coords="2,134.77,561.65,354.97,8.77">Compound Figure Delimiter: Estimated 40% of biomedical figures in PubMed</formula><p>Central are compound figures <ref type="bibr" coords="2,288.75,573.64,9.96,8.74" target="#b5">[6]</ref>. These captions most likely address the subfigures using delimiters. Such delimiters were detected and removed. An excerpt of delimiters removed is listed in <ref type="bibr" coords="2,332.11,597.55,14.61,8.74" target="#b11">[12]</ref>. English Stopwords: Using the NLTK Stopword corpus, present stopwords in the captions were omitted. This corpus contains 2,400 stopwords for 11 languages <ref type="bibr" coords="2,183.85,633.31,9.96,8.74" target="#b1">[2]</ref>. Special Characters: Special characters such as symbols, punctuations, metrics, etc. were removed.</p><p>Single Digits: Single digits, words which consist of just numbers, were removed. Word Stemming: To reduce complexity, the captions are stemmed using Porter</p><p>Stemming <ref type="bibr" coords="3,199.13,155.66,14.61,8.74" target="#b12">[13]</ref>. For evaluation comparison, not all models were trained using stemmed captions. An overview of model setup is listed in Table <ref type="table" coords="3,452.42,167.62,3.87,8.74" target="#tab_1">1</ref>. The Snowball stemming method <ref type="bibr" coords="3,273.83,179.57,15.50,8.74" target="#b13">[14]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image Keyword Generator</head><p>For keyword generation, a combination of encoding and decoding using Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) <ref type="bibr" coords="3,442.98,322.29,10.52,8.74" target="#b6">[7]</ref> based Recurrent Neural Networks (RNN) <ref type="bibr" coords="3,289.34,334.25,10.52,8.74" target="#b0">[1]</ref> is adopted. This approach, also known as Show-And-Tell model was proposed in <ref type="bibr" coords="3,305.37,346.20,15.50,8.74" target="#b16">[17]</ref> and further improved in <ref type="bibr" coords="3,432.21,346.20,14.61,8.74" target="#b17">[18]</ref>. The CNN is used as an image encoder, to produce rich visual representations of the images, by pre-training it for an image classification task. The LSTM-RNN utilized as caption decoder generates the image keywords, using the CNN last hidden layer as input <ref type="bibr" coords="3,245.79,394.02,14.61,8.74" target="#b16">[17]</ref>. The parameters for the image keyword generation model are: Several models were further trained in the second training phase. In the second phase, parameters of the image submodel and LSTM are fine-tuned using the deep learning networks Inception-v3 <ref type="bibr" coords="3,298.01,645.16,15.50,8.74" target="#b15">[16]</ref> and Inception-ResNet-v2 <ref type="bibr" coords="3,430.30,645.16,14.61,8.74" target="#b14">[15]</ref>. Figure <ref type="figure" coords="3,134.77,657.11,4.98,8.74" target="#fig_0">1</ref> shows the keyword generation model training setup.  For better understanding the difference between predicted keywords Combination and Concatenation is explained as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Model Setup</head><p>Combination = OR: The keyword generator models were not always able to predict the caption of a given image. Some results were &lt;UNK&gt; representing an empty string. In such cases, the predicted keywords of three models are combined. Taking PRED Sub01 for example: when model setup R01 returns an empty string, the final results is substituted with the predicted keywords from model setup R02. In the case where R02 returns an empty string as well, the predicted keywords of R03 is taken as the final caption. When all three models predict &lt;UNK&gt;, the final result is 'unknown'. This process is highlighted in Table <ref type="table" coords="5,243.30,584.13,3.87,8.74">2</ref>. All three models have the same preprocessing steps and vocabulary sizes but differ in the second training phase. Concatenation = AND: The predicted keywords of two models are simply concatenated. Both models were trained using the same preprocessing methods, first and second training phase. The minimum cutoff for word occurrence is different. Multiple keywords are removed. An example using submission run PRED Sub08 is shown in Table <ref type="table" coords="5,311.23,657.11,3.87,8.74">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discussion</head><p>Analyzing Table <ref type="table" coords="8,212.14,418.01,3.87,8.74" target="#tab_2">5</ref>, it can be seen that submitting keywords instead of captions for evaluation on the ImageCLEF 2017 Caption Prediction Task Test Set achieved low BLEU scores. The best score was attained on the test set with submission run PRED Sub09. This is a concatenation of predicted keywords using model setup R03 and R05. Both models parameters were fine-tuned using the deep learning network Inception-v3 and were trained with different vocabulary sizes.</p><p>On the validation set, the best score was obtained with submission run PRED Sub08, which is the concatenation of predicted keywords using models R01 and R04. This run is similar to PRED Sub09 with the exception of parameter fine-tuning with Inception-ResNet-v2. The BLEU scores achieved on the validation set are similar to those of the test set. Captions of biomedical figures mostly consist of multiple sentences and can not be accurately predicted using few keywords, as word order and caption length have effects on the calculated scores.</p><p>The precision score is one of the adequate metrics for image keyword generation. The best precision score was obtained using submission run PRED Sub08. With more extensive text preprocessing steps, higher precision scores can be expected.</p><p>The removal of compound figure delimiters, stop words, single numbers and special characters led to compact and precise captions. However, captions contain several adjectives, pronouns, adverbs etc. which do not necessarily describe the semantic content, characteristics or modality of the images. The reduction of captions to contain just nouns is one preprocessing steps that should be evaluated with the aim of modeling an accurate image keyword generator.</p><p>In comparison to the Show-And-Tell Model, a smaller number of epochs was used to train the image keyword generator. The Show-And-Tell Model has 387 epochs in the first training phase and 1,160 epochs in the fine-tuning phase <ref type="bibr" coords="9,465.09,191.72,15.50,8.74" target="#b16">[17,</ref><ref type="bibr" coords="9,134.77,203.68,11.62,8.74" target="#b17">18]</ref>. Potentially, training the image keyword generator with a larger number of epochs could improve the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>An approach for image keyword generation was presented. Using image -caption pairs of 164,614 biomedical figures, distributed for training at the ImageCLEF Caption Prediction Task, long short-term memory based Recurrent Neural Network models were trained. All compound figure delimiters, stop words, special characters and single numbers were removed from captions before training. For comparison, some models were trained with stemmed captions and different vocabulary sizes. These vocabularies were obtained by using different minimum word occurrence cutoffs. The BLEU and precision scores were applied as evaluation metrics. With the aim of further model accuracy improvement, the reduction of captions to just nouns before training the models should be evaluated. To increase keyword prediction ability, the models should be trained and fine-tuned with a higher number of epochs, as proposed in the Show-And-Tell model. These automatically generated keywords can be substituted as text representation for classification tasks and image retrieval purposes will be researched and evaluated in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,138.97,424.93,290.25,8.74;3,138.97,436.69,122.95,8.74;3,138.97,448.45,117.12,8.74;3,138.97,460.21,215.53,8.74;3,138.97,471.97,154.92,8.74;3,138.97,483.73,156.19,8.74;3,138.97,495.48,154.20,8.74;3,138.97,507.24,209.77,8.74;3,138.97,519.00,139.39,8.74;3,133.99,530.76,136.37,8.74;3,133.99,542.52,146.06,8.74;3,133.99,554.28,184.50,8.74;3,149.71,573.43,330.89,8.74;3,134.77,585.38,345.82,8.74;3,134.77,597.34,345.82,8.74;3,134.77,609.29,37.11,8.74"><head>1 .</head><label>1</label><figDesc>Minibatch size = [1. Trainingphase = 32; 2. Trainingphase = 4] 2. Vocabulary size = 23,000 3. Initial learning rate = 2 4. Model optimizer = stochastic gradient descent 5. Learning rate decay factor = 0.5 6. Number of epochs per decay = 8 7. Inception learning rate = 0.0005 8. Inception model initialization = Inception-v3 9. LSTM embedding size = 512 10. LSTM units number = 512 11. LSTM initializer scale = 0.08 12. LSTM dropout keep probability = 0.7 In the first training phase, the LSTM is trained using a corpus of paired image and captions generated from the biomedical figures in the ImageCLEF 2017 Caption Prediction Task Training Set. No further dataset was used for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,330.03,345.82,7.89;4,134.77,341.01,200.00,7.86;4,141.00,116.83,333.35,198.43"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of Long Short-Term Memory based Recurrent Neural Network Model applied for biomedical image keyword generation.</figDesc><graphic coords="4,141.00,116.83,333.35,198.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,146.44,405.21,327.29,274.49"><head>Table 1 .</head><label>1</label><figDesc>Model training setups applied for image keyword generationSeveral model setups were evaluated and those selected for creating submission runs are listed in Table1. Columns 2 -4 display applied preprocessing methods. The columns 5 and 6 shows the minimum word occurrence cutoff and vocabulary size, as described in subsection 2.2, respectively. The number of epochs for the first and second training phase are listed in columns 7 and 8, respectively. Column 9 shows deep learning networks adopted for parameter fine-tuning.</figDesc><table coords="4,146.44,429.45,327.29,250.26"><row><cell>Run ID</cell><cell>PorterStemming</cell><cell>Special Characters</cell><cell>Stopwords</cell><cell>Compound figure</cell><cell>delimiters</cell><cell>Cutoff Words</cell><cell>Vocabulary Size</cell><cell>1. Trainingphase</cell><cell>Number of Epochs</cell><cell>2. Trainingphase</cell><cell>Number of Epochs</cell><cell>Deep Learning</cell><cell>Network</cell><cell>BLEU Score</cell><cell>Validation Set</cell></row><row><cell>R01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>Vocab01</cell><cell cols="2">25</cell><cell>4</cell><cell></cell><cell cols="2">ResNet-v2 Inception</cell><cell cols="2">0.0686</cell></row><row><cell>R02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>Vocab01</cell><cell cols="2">27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.0670</cell></row><row><cell>R03</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>Vocab01</cell><cell cols="2">25</cell><cell>4</cell><cell></cell><cell cols="4">Inception-v3 0.0674</cell></row><row><cell>R04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell>Vocab02</cell><cell cols="2">25</cell><cell>4</cell><cell></cell><cell cols="2">ResNet-v2 Inception</cell><cell cols="2">0.0336</cell></row><row><cell>R05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell>Vocab02</cell><cell cols="2">25</cell><cell>4</cell><cell></cell><cell cols="4">Inception-v3 0.0323</cell></row><row><cell>R06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell>Vocab02</cell><cell cols="2">27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.0579</cell></row><row><cell>R10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell>Vocab03</cell><cell cols="2">27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.0918</cell></row><row><cell>R11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell>Vocab03</cell><cell cols="2">25</cell><cell>4</cell><cell></cell><cell cols="2">ResNet-v2 Inception</cell><cell cols="2">0.0661</cell></row><row><cell>R12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell>Vocab03</cell><cell cols="2">25</cell><cell>4</cell><cell></cell><cell cols="4">Inception-v3 0.0656</cell></row><row><cell>R16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell>Vocab03</cell><cell cols="2">39</cell><cell>7</cell><cell></cell><cell cols="4">Inception-v3 0.0678</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.77,163.97,345.83,202.81"><head>Table 5 .</head><label>5</label><figDesc>Evaluation metrics obtained on the official test and validation set for all submitted runs. Each set contains 10,000 biomedical figures.</figDesc><table coords="8,146.36,197.14,323.25,169.63"><row><cell></cell><cell>Test</cell><cell>Validation</cell><cell>Validation</cell><cell>Validation</cell></row><row><cell>Run ID</cell><cell>Set</cell><cell>Set</cell><cell>Set</cell><cell>Set</cell></row><row><cell></cell><cell>BLEU Score</cell><cell>BLEU Score</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>PRED BCSG Sub01</cell><cell>0.0624</cell><cell>0.0772</cell><cell>0.1782</cell><cell>0.1245</cell></row><row><cell>PRED BCSG Sub02</cell><cell>0.0411</cell><cell>0.0687</cell><cell>0.2270</cell><cell>0.1582</cell></row><row><cell>PRED BCSG Sub03</cell><cell>0.0527</cell><cell>0.0656</cell><cell>0.1769</cell><cell>0.1253</cell></row><row><cell>PRED BCSG Sub04</cell><cell>0.0537</cell><cell>0.0661</cell><cell>0.1782</cell><cell>0.1245</cell></row><row><cell>PRED BCSG Sub05</cell><cell>0.0200</cell><cell>0.0428</cell><cell>0.1310</cell><cell>0.0828</cell></row><row><cell>PRED BCSG Sub06</cell><cell>0.0365</cell><cell>0.0674</cell><cell>0.2281</cell><cell>0.1582</cell></row><row><cell>PRED BCSG Sub07</cell><cell>0.0375</cell><cell>0.0686</cell><cell>0.2279</cell><cell>0.1612</cell></row><row><cell>PRED BCSG Sub08</cell><cell>0.0675</cell><cell>0.1111</cell><cell>0.2495</cell><cell>0.1888</cell></row><row><cell>PRED BCSG Sub09</cell><cell>0.0749</cell><cell>0.1086</cell><cell>0.2431</cell><cell>0.1861</cell></row><row><cell>PRED BCSG Sub10</cell><cell>0.0326</cell><cell>0.0678</cell><cell>0.2108</cell><cell>0.1418</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>The evaluation metrics achieved with the submitted runs for the ImageCLEF 2017 Caption Prediction Task is listed in Table <ref type="table" coords="7,340.06,157.73,3.87,8.74">5</ref>. For a single biomedical image, the predicted keywords and corresponding BLEU scores are shown in Table <ref type="table" coords="7,472.84,169.68,3.87,8.74">4</ref>.</p><p>The image was hand picked from the ImageCLEF 2017 Caption Prediction Task Validation Set. For better comparison, the ground truth caption is shown below the image. The first and second columns of Table <ref type="table" coords="7,321.61,645.16,4.98,8.74">5</ref> list the mean BLEU <ref type="bibr" coords="7,422.44,645.16,15.50,8.74" target="#b10">[11]</ref> score obtained on the ImageCLEF 2017 Caption Prediction Task Test and Validation Set respectively. Both datasets contain 10,000 biomedical figures. The third column displays the precision score obtained on the validation set.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,493.57,337.64,7.86;9,151.52,504.53,317.87,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,294.47,493.57,186.12,7.86;9,151.52,504.53,71.78,7.86">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,229.71,504.53,153.68,7.86">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,515.45,337.63,7.86;9,151.52,526.41,269.46,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,271.21,515.45,209.38,7.86;9,151.52,526.41,152.28,7.86">Natural language processing with Python: analyzing text with the natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,537.33,337.63,7.86;9,151.52,548.29,329.07,7.86;9,151.52,559.25,329.07,7.86;9,151.52,570.20,176.73,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,402.01,537.33,78.57,7.86;9,151.52,548.29,275.52,7.86">Automated medical image modality recognition by fusion of visual and text information</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Merler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,449.10,548.29,31.49,7.86;9,151.52,559.25,329.07,7.86;9,151.52,570.20,53.20,7.86">Medical Image Computing and Computer-Assisted Intervention-MICCAI Conference Proceedings 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,581.12,337.64,7.86;9,151.52,592.08,329.07,7.86;9,151.52,603.04,329.07,7.86;9,151.52,614.00,62.50,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,384.87,581.12,95.73,7.86;9,151.52,592.08,282.74,7.86">Imageclef 2009 medical image annotation task: PCTs for hierarchical multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dimitrovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kocev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Loskovska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,455.25,592.08,25.34,7.86;9,151.52,603.04,270.07,7.86">Multilingual Information Access Evaluation II. Multimedia Experiments</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,624.92,337.64,7.86;9,151.52,635.88,329.07,7.86;9,151.52,646.84,329.07,7.86;9,151.52,657.79,317.71,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,403.70,624.92,76.89,7.86;9,151.52,635.88,329.07,7.86;9,151.52,646.84,42.85,7.86">Overview of Image-CLEFcaption 2017 -image caption prediction and concept detection for biomedical images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="9,216.68,646.84,131.58,7.86">CLEF 2017 Labs Working Notes</title>
		<title level="s" coord="9,356.02,646.84,124.57,7.86;9,151.52,657.79,43.39,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,120.67,337.64,7.86;10,151.52,131.63,329.07,7.86;10,151.52,142.59,315.94,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,364.70,120.67,115.90,7.86;10,151.52,131.63,122.63,7.86">Overview of the ImageCLEF 2015 medical classification task</title>
		<author>
			<persName coords=""><forename type="first">García</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,293.88,131.63,186.72,7.86;10,151.52,142.59,117.01,7.86">Working Notes of CLEF 2015 -Conference and Labs of the Evaluation forum</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">September 8-11, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,153.55,337.64,7.86;10,151.52,164.51,92.15,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,288.76,153.55,100.73,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,398.69,153.55,81.90,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,175.46,337.63,7.86;10,151.52,186.42,329.07,7.86;10,151.52,197.38,329.07,7.86;10,151.52,208.34,329.07,7.86;10,151.52,219.30,329.07,7.86;10,151.52,230.26,329.07,7.86;10,151.52,241.22,199.89,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,433.34,197.38,47.26,7.86;10,151.52,208.34,215.41,7.86">Overview of ImageCLEF 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,387.32,208.34,93.27,7.86;10,151.52,219.30,329.07,7.86;10,151.52,230.26,105.52,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction 8th International Conference of the CLEF Association, CLEF</title>
		<title level="s" coord="10,285.42,230.26,143.89,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-09-11">2017. September 11-14 2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,252.18,337.64,7.86;10,151.52,263.14,329.07,7.86;10,151.52,274.09,329.07,7.86;10,151.52,285.05,194.64,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,247.64,263.14,232.95,7.86;10,151.52,274.09,324.82,7.86">Evaluating performance of biomedical image retrieval systems -an overview of the medical image retrieval task at imageclef 2004-2013</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,151.52,285.05,125.00,7.86">Comp. Med. Imag. and Graph</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="55" to="61" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,296.01,337.97,7.86;10,151.52,306.97,329.07,7.86;10,151.52,315.66,329.07,10.13;10,151.52,328.89,135.95,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,264.93,296.01,215.66,7.86;10,151.52,306.97,226.38,7.86">Traditional feature engineering and deep learning approaches at medical classification task of imageclef 2016</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,399.96,306.97,80.63,7.86;10,151.52,317.93,217.92,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings, CEUR-WS. org</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">September 5-8 2016. 2016</date>
			<biblScope unit="page" from="304" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,339.85,337.98,7.86;10,151.52,350.81,329.07,7.86;10,151.52,361.77,329.07,7.86;10,151.52,372.73,98.02,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,355.50,339.85,125.09,7.86;10,151.52,350.81,133.70,7.86">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,306.56,350.81,174.03,7.86;10,151.52,361.77,162.47,7.86">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,383.68,337.98,7.86;10,151.52,394.64,329.07,7.86;10,151.52,405.60,329.07,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,262.61,383.68,217.99,7.86;10,151.52,394.64,119.58,7.86">FHDO biomedical computer science group at medical classification task of imageclef</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,310.65,394.64,169.94,7.86;10,151.52,405.60,132.76,7.86">Working Notes of CLEF 2015 -Conference and Labs of the Evaluation forum</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-08">2015. September 8-11, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,416.56,327.98,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,207.88,416.56,130.19,7.86">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,345.00,416.56,34.98,7.86">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,427.52,282.74,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,207.88,427.52,188.81,7.86">Snowball: A language for stemming algorithms</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,438.48,337.97,7.86;10,151.52,449.44,329.07,7.86;10,151.52,460.40,329.07,7.86;10,151.52,471.36,155.91,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,360.37,438.48,120.22,7.86;10,151.52,449.44,201.97,7.86">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,374.25,449.44,106.34,7.86;10,151.52,460.40,187.91,7.86">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">February 4-9, 2017. 2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,482.31,337.97,7.86;10,151.52,493.27,329.07,7.86;10,151.52,504.23,263.00,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,393.24,482.31,87.35,7.86;10,151.52,493.27,147.64,7.86">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,320.22,493.27,160.37,7.86;10,151.52,504.23,169.72,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,515.19,337.98,7.86;10,151.52,526.15,329.07,7.86;10,151.52,537.11,190.99,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,352.45,515.19,128.15,7.86;10,151.52,526.15,69.89,7.86">Show and tell: A neural image caption generator</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,243.98,526.15,236.61,7.86;10,151.52,537.11,97.71,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,548.07,337.97,7.86;10,151.52,559.03,329.07,7.86;10,151.52,569.99,227.94,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,338.40,548.07,142.19,7.86;10,151.52,559.03,194.55,7.86">Show and tell: Lessons learned from the 2015 MSCOCO image captioning challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,354.26,559.03,126.34,7.86;10,151.52,569.99,137.32,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="652" to="663" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
