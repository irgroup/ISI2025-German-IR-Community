<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,165.36,115.96,284.63,12.62">NLM at ImageCLEF 2017 Caption Task</title>
				<funder ref="#_yxgm2An">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">National Library of Medicine</orgName>
					<orgName type="abbreviated">NLM</orgName>
				</funder>
				<funder>
					<orgName type="full">Lister Hill National Center for Biomedical Communications</orgName>
					<orgName type="abbreviated">LHNCBC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,161.70,153.63,78.71,8.74"><forename type="first">Asma</forename><forename type="middle">Ben</forename><surname>Abacha</surname></persName>
							<email>asma.benabacha@nih.gov</email>
						</author>
						<author>
							<persName coords="1,248.78,153.63,105.27,8.74"><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
						</author>
						<author>
							<persName coords="1,361.85,153.63,62.55,8.74"><forename type="first">Soumya</forename><surname>Gayen</surname></persName>
							<email>soumya.gayen@nih.gov</email>
						</author>
						<author>
							<persName coords="1,432.77,153.63,20.88,8.74;1,224.05,165.58,74.47,8.74"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
						</author>
						<author>
							<persName coords="1,326.52,165.58,64.79,8.74"><forename type="first">Sameer</forename><surname>Antani</surname></persName>
							<email>santani@mail.nih.gov</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Lister Hill National Center for Biomedical Communications</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,165.36,115.96,284.63,12.62">NLM at ImageCLEF 2017 Caption Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2050A545FEB741B49774214C4FE47CA0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Concept Detection</term>
					<term>Caption Prediction</term>
					<term>Convolutional Neural Networks</term>
					<term>Multi-label Classification</term>
					<term>Open-i</term>
					<term>MetaMapLite</term>
					<term>UMLS</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the U.S. National Library of Medicine (NLM) in the ImageCLEF 2017 caption task. We proposed different machine learning methods using training subsets that we selected from the provided data as well as retrieval methods using external data. For the concept detection subtask, we used Convolutional Neural Networks (CNNs) and Binary Relevance using decision trees for multi-label classification. We also proposed a retrieval-based approach using Open-i image search engine and MetaMapLite to recognize relevant terms and associated Concept Unique Identifiers (CUIs). For the caption prediction subtask, we used the recognized CUIs and the UMLS to generate the captions. We also applied Open-i to retrieve similar images and their captions. We submitted ten runs for the concept detection subtask and six runs for the caption prediction subtask. CNNs provided good results with regards to the size of the selected subsets and the limited number of CUIs used for training. Using the CUIs recognized by the CNNs, our UMLS-based method for caption prediction obtained good results with 0.2247 mean BLUE score. In both subtasks, the best results were achieved using retrieval-based approaches outperforming all submitted runs by all the participants with 0.1718 mean F1 score in the concept detection subtask and 0.5634 mean BLUE score in the caption prediction subtask.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the participation of the U.S. National Library of Medicine<ref type="foot" coords="1,476.12,565.00,3.97,6.12" target="#foot_0">1</ref> (NLM) in the ImageCLEF 2017 caption task <ref type="bibr" coords="1,328.91,578.53,9.96,8.74" target="#b0">[1]</ref>. ImageCLEF <ref type="bibr" coords="1,400.44,578.53,10.52,8.74" target="#b1">[2]</ref> is an evaluation campaign organized as part of the CLEF<ref type="foot" coords="1,316.61,588.91,3.97,6.12" target="#foot_1">2</ref> initiative labs. In 2017, the caption task consisted of two subtasks including concept detection and caption prediction. A detailed description of the data and the task is presented in Eickhoff et al. <ref type="bibr" coords="1,148.60,626.35,9.96,8.74" target="#b0">[1]</ref>.</p><p>The concept detection subtask consists of identifying the UMLS R (Unified Medical Language System) <ref type="foot" coords="2,254.32,129.37,3.97,6.12" target="#foot_2">3</ref> Concept Unique Identifiers (CUIs). To solve this first challenge of detecting CUIs from a given image from the biomedical literature, we propose several approaches based on multi-label classification and information retrieval. For the multi-label classification, Convolutional Neural Networks (CNNs) and Binary Relevance using Decision Trees (BR-DT) are applied. The information retrieval approach is based on the Open-i Biomedical Image Search Engine<ref type="foot" coords="2,226.26,201.11,3.97,6.12" target="#foot_3">4</ref>  <ref type="bibr" coords="2,234.05,202.68,9.96,8.74" target="#b2">[3]</ref>.</p><p>The caption prediction subtask aims to recreate the original image caption.To predict the captions of the images, we proposed a retrieval-based approach using Open-i and a second approach based on the retrieved CUIs and the UMLS R to find the associated terms and groups.</p><p>The rest of the paper is organized as follows. Section 2 describes the data provided for the two subtasks and our method to select training subsets. Then we present the proposed approaches for concept detection in Section 3 and caption prediction in Section 4. Section 5 provides a description of the submitted runs. Finally Section 6 presents and discusses our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Analysis and Selection</head><p>Training, validation and test datasets were provided containing 164,614, 10,000 and 10,000 biomedical images respectively. The images were extracted from scholarly articles on PubMed Central<ref type="foot" coords="2,297.27,392.93,3.97,6.12" target="#foot_4">5</ref> (PMC).</p><p>For the concept detection subtask, a set of CUIs was provided for each image. For the caption prediction subtask, captions were provided. Figure <ref type="figure" coords="2,432.40,418.79,4.98,8.74" target="#fig_1">1</ref> shows an example from the provided data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Analysis of Concept Detection Data</head><p>We analyzed the task data in order to study the types of methods that could be applied for concept detection subtask and whether it is needed to select training data and remove the less frequent CUIs. Also we studied whether it is relevant to build rule-based methods and construct patterns for the caption prediction subtask based on the recognized CUIs.</p><p>For the concept detection subtask:   Table <ref type="table" coords="3,178.89,444.01,4.98,8.74" target="#tab_1">1</ref> presents the most frequent CUIs in the training data and their UMLS R<ref type="foot" coords="3,171.57,454.39,3.97,6.12" target="#foot_5">6</ref> terms and semantic types. We used these two subsets to train our machine learning (ML) methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Concept Detection Methods</head><p>For the concept detection subtask, each image can be associated with one or multiple CUIs. We approached the problem in two ways, (1) applying multilabel classification methods and (2) using a retrieval-based approach.</p><p>In the multi-label classification approach we consider the CUIs in the training set as the labels to be assigned. Thus each image will be assigned one or multiple labels from the predefined label set. Two methods for multi-label classification were applied: Convolutional Neural Networks (CNNs) and Binary Relevance using Decision Trees (BR-DT). To train our ML models, we utilized the highperformance computational capabilities of the Biowulf Linux cluster at the U.S. National Institutes of Health<ref type="foot" coords="4,260.52,540.73,3.97,6.12" target="#foot_6">7</ref> .</p><p>In the information retrieval approach, we used Open-i to retrieve the most similar images and their associated labels and CUIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-label classification with Convolutional Neural Networks (CNNs)</head><p>Deep learning methods have been widely applied to image analysis. In particular, CNNs achieved excellent results for image classification <ref type="bibr" coords="4,379.18,637.58,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="4,391.36,637.58,7.01,8.74" target="#b5">6]</ref>.</p><p>We applied CNNs for multi-label classification and tested different neural networks such as the GoogleNet network <ref type="bibr" coords="5,317.33,130.95,9.96,8.74" target="#b6">[7]</ref>. GoogLeNet won the classification and object recognition challenges in the 2014 ImageNet LSVRC competition (ILSVRC2014<ref type="foot" coords="5,195.65,153.28,3.97,6.12" target="#foot_7">8</ref> ). In our experiments on the training sets, the GoogleNet network provided better results compared to AlexNet <ref type="bibr" coords="5,333.69,166.81,10.52,8.74" target="#b7">[8]</ref> and LeNet <ref type="bibr" coords="5,396.64,166.81,9.96,8.74" target="#b8">[9]</ref>.</p><p>We ran the CNNs using NVIDIA Deep Learning GPU Training System (DIG-ITS) <ref type="foot" coords="5,154.97,189.67,3.97,6.12" target="#foot_8">9</ref> . DIGITS is a Deep Learning (DL) training system with a web interface that allows designing custom network architectures and evaluating their effectiveness. It also allows the design of new models by providing the details of optimization and network architecture. DIGITS can be used for image classification, segmentation and object detection tasks.</p><p>In our final runs, we used the GoogLeNet network. We applied stochastic gradient descent (SGD) and performed 100 training epochs. We used the two training subsets associated respectively to 92 and 239 CUIs to train the network (see Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-label Classification with Binary Relevance using Decision</head><p>Trees (BR-DT)</p><p>The Meka project <ref type="bibr" coords="5,214.62,354.32,15.50,8.74" target="#b9">[10]</ref> <ref type="foot" coords="5,230.12,352.75,7.94,6.12" target="#foot_9">10</ref> is based on the Weka machine learning library <ref type="bibr" coords="5,443.38,354.32,14.61,8.74" target="#b10">[11]</ref>, and provides an open source implementation of methods for multi-label classification. It contains several algorithms, such as Binary Relevance (BR) or Label Powerset.</p><p>Similar to <ref type="bibr" coords="5,196.13,390.70,15.50,8.74" target="#b11">[12]</ref> we used BR-DT as implemented in Meka (J48). BR methods create an individual model for each label, thus each model is a simply binary problem. We used Decision Trees (DT) as a base classifier because DT are able to capture relations between labels. For the experiments we extract from the images one visual descriptors commonly used for image classification Colour and Edge Directivity Descriptor (CEDD) <ref type="bibr" coords="5,276.83,450.48,14.61,8.74" target="#b12">[13]</ref>. The descriptor was provided as input to Meka.</p><p>Before submitting the runs we carried out some experiments on the training data using also Fuzzy Colour and Texture Histogram (FCTH) <ref type="bibr" coords="5,414.24,486.86,15.50,8.74" target="#b13">[14]</ref> as a visual descriptor. However using CEDD provided better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Retrieval and Annotation Approach with Open-i and MetaMapLite</head><p>The Open-i service of the NLM enables search and retrieval of abstracts and images (including charts, graphs, clinical images) from the open source literature, and biomedical image collections. Open-i provides access to over 3.7 million images from about 1.2 million PubMed Central articles, 7,470 chest x-rays with 3,955 radiology reports, 67,517 images from NLM History of Medicine collection, 2,064 orthopedic illustrations and 8084 medical case images from MedPix<ref type="foot" coords="6,469.38,117.42,7.94,6.12" target="#foot_10">11</ref> . Open-i combines text processing, image analysis and machine learning techniques to retrieve relevant images from an input image-query.</p><p>We submitted each query image to the Open-i search API and selected 10 result images with captions. For each retrieved image, we annotated its caption with MetaMapLite<ref type="foot" coords="6,215.79,177.20,7.94,6.12" target="#foot_11">12</ref> (3.1-SNAPSHOT version) to recognize CUIs. MetaMapLite recognizes named entities using the longest match as well as associated CUIs. It also allows restricting the CUIs with UMLS Semantic Types. We did not use any restriction as CUIs in the provided data have heterogeneous semantic types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Caption Prediction Methods</head><p>To predict image captions, we used two different methods based on UMLS R and Open-i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">UMLS-based Method</head><p>We used the CUIs recognized in the first concept detection subtask to generate the associated UMLS terms and semantic types. We then grouped the recognized UMLS terms using the UMLS groups of their semantic types </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Open-i-based Method</head><p>For each input image, the Open-i biomedical image search engine returns a list of similar images. In our experiments we performed several tests with the caption, mention, Medical Subject Headings (MeSH R ) terms, three outcomes and medical problems from the retrieved images. In our final runs, we used only the captions of the first and second retrieved images.</p><p>The following are two examples of results provided by Open-i:  <ref type="figure" coords="7,262.23,380.53,4.98,8.74">2</ref> and<ref type="figure" coords="7,289.91,380.53,3.87,8.74">3</ref>). 2. 110.1177 2324709614529417-fig1: Open-i provides the following results:</p><p>-Caption: Magnetic resonance imaging after the onset of isolated adrenocorticotropic hormone deficiency. Magnetic resonance imaging showed no space-occupying lesions in the pituitary gland or hypothalamus. -Problem(s): isolated adrenocorticotropic hormone deficiency. -Concept(s): isolated adrenocorticotropic hormone deficiency.</p><p>-Outcomes: (i) Although the neutrOpen-ia and fever immediately improved, he became unable to take any oral medications and was bedridden 1 week after admission. (ii) His serum sodium level abruptly decreased to 122mEq/L on the fifth day of hospitalization. (iii) Hydrocortisone replacement therapy was begun at 20mg/day, resulting in a marked improvement in his anorexia and general fatigue within a few days. -Mention: CT and magnetic resonance imaging showed no space-occupying lesion or atrophic change in his pituitary gland or hypothalamus (Fig- <ref type="figure" coords="7,168.64,573.91,16.99,8.74" target="#fig_1">ure1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Runs</head><p>This section provided a detailed description of the runs submitted to ImageCLEF 2017 caption task. The methods used to implement these runs are described in previous Sections 3 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Concept Detection</head><p>As specified by the task guidelines, a maximum of 50 UMLS concepts per figure is accepted. Therefore, if the limit of 50 CUIs per image is reached, we took only the first 50 CUIs for each image. We submitted the following runs to the Concept Detection subtask: DET 1. DET run 1 Open-i MetaMapLite 1: We used Open-i to find similar images and then extracted CUIs from their captions using MetaMapLite.</p><p>In this first run, we used the caption of the most similar image according to Open-i. The returned CUIs are all the CUIs recognized by MetaMapLite. DET 2. DET run 1 baseline: The same DET 1 run with the exclusion of test images if they are retrieved by Open-i. DET 3. DET run 2 Open-i MetaMapLite 2: The same as DET 1 except that we took only the first CUI recognized by MetaMapLite for each term. DET 4. DET run 3 Open-i MetaMapLite 3: Similar to DET 1 except that we used the captions from the first and second best images retrieved by Open-i. DET 5. DET run 5 Meka CEDD: Multi-label Classification method using MEKA software to applied binary relevance method. CEED is used as a visual descriptor for the images. Subset 1 of 92 CUIs is used for training. DET 6. DET run 6 CNN GoogLeNet 92Cuis: Multi-label classification with a convolutional neural network. We trained the GoogLeNet network using subset 1 of 92 CUIs. DET 7. DET run 7 CNN GoogLeNet 239Cuis: We trained the GoogLeNet network using subset 2 of 239 CUIs. DET 8. DET run 8 comb1 CNN2: Fusion of the runs DET 6 and DET 7 ) DET 9. DET run 9 comb2 CNN2Meka: Fusion of the runs DET 5, DET 6 and DET 7 ) DET 10. DET run 10 comb3 CNN2MekaOpen-i: Fusion of the runs DET 1, DET 5, DET 6 and DET 7 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Caption Prediction</head><p>We submitted the following runs to the Caption Prediction subtask: PRED 1. PRED run 1 Open-iMethod: We used Open-i Biomedical Image Search Engine to find similar images. In this run, we used the caption of the first retrieved image. PRED 2. PRED run 1 baseline: Same as PRED 1, except we excluded the test images if they are retrieved by Open-i. PRED 3. PRED run 2 CNN 92: We used the CUIs recognized by the CNN (CRun DET 6 ) and the UMLS semantic groups to generate the captions.</p><p>PRED 4. PRED run 3 CNN 239: We used the CUIs recognized by the CNN (run DET 7 ) and the UMLS semantic groups to generate the captions. PRED 5. PRED run 4 CNN comb: We used the CUIs recognized by the CNN (run DET 8 ) and the UMLS semantic groups to generate the captions. PRED 6. PRED run 5 comb all: We used the CUIs recognized by the hybrid method (run DET 10 ) and the UMLS R to generate the captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Official Results</head><p>In this section we describe and discuss the results obtained by the submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Concept Detection Results</head><p>Table <ref type="table" coords="9,161.31,304.06,4.98,8.74" target="#tab_3">2</ref> shows our official results in the concept detection subtask and their ranks compared with all the 37 runs submitted by the 9 participating teams. The best overall results were obtained by run DET 1 followed by run DET 3 ; both approaches are based on Open-i retrieval system. To better understand the results, Table <ref type="table" coords="9,213.55,524.61,4.98,8.74" target="#tab_4">3</ref> shows the efficiency of the Open-i system on the test set by presenting how many times the query image itself was retrieved and ranked in the first 10 positions when searching on the full Open-i collection (3.7 million images). We analyze only the first 10 because it is the maximum number of retrieved images that we used in our experiments.</p><p>Open-i was able to find the image in the first top 10 results in 61% of the cases, and extract the relevant information from the image itself.</p><p>For comparison, we performed a second run called DET 2, which is equivalent to run DET 1 but with the exclusion of test images if they are retrieved by Openi. For run DET 2 the mean F1 score decreased to 0.0162, which we consider as baseline result. The best results using Open-i based approaches were obtained when using all the CUIs associated with the first retrieved image. Without using external resources the results were poorer. One of the reasons could be that not all the CUIs in the test set were contained in the training and validation sets. Also, we only considered the most frequent CUIs in the training set. With CNNs, up to 0.0880 mean F1 score was achieved and only 0.0012 when applying BR-DT (BR-DT detected at least one CUI on 2046 images only).</p><p>Table <ref type="table" coords="10,177.93,257.58,4.98,8.74" target="#tab_3">2</ref> also shows the performance of three hybrid methods: run DET 8, run DET 9 and run DET 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Caption Prediction Results</head><p>Table <ref type="table" coords="10,163.25,317.64,4.98,8.74" target="#tab_5">4</ref> shows our official results in the caption prediction subtask and their ranks compared with the 34 runs submitted by the 5 participating teams. The best results were achieved by run PRED 1 using Open-i with 0.5634 mean BLUE score and was ranked first. As baseline, we proposed run PRED 2, similar to run PRED 1 but without including test images if they are retrieved by Open-i. Run PRED 2 obtained 0.2646 mean BLUE score and was the 4th best run out of 34 submitted runs by the participating teams.</p><p>CNN approaches achieved good results with 0.2247 mean BLUE score despite the limited number of CUIs used for training and the simple UMLS-based patterns built for caption generation. Two hybrid methods were also presented: run PRED 5 and run PRED 6. In this subtask, run PRED 6 was ranked second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This paper describes our participation in ImageCLEF 2017 caption task. We proposed and compared different approaches for concept detection and caption prediction. Our retrieval methods using Open-i obtained the best results with 0.1718 mean F1 score in the concept detection subtask and 0.5634 mean BLUE score in the caption prediction subtask. We proposed baseline results by excluding test images if they are found by Open-i. Open-i baseline was ranked 4th with 0.2646 mean BLUE score in the caption prediction subtask.</p><p>We also performed multi-label classification of CUIs with CNNs and BR-DT. Both methods used selected subsets from the training data. CNNs provided acceptable results with regards the limited number of CUIs used for training. CNNs method achieved 0.2247 mean BLUE score in the caption prediction subtask.</p><p>Future improvements can tackle Open-i method as it does not support images with panels. One better way would be to perform panel segmentation before the search. Open-i also has size limitations on images of 2MB. A better approach would be to resize the image if needed before submitting to Open-i API. Also, MetaMapLite provided CUIs that are different from the gold standard even if the labels retrieved by Open-i are correct. Moreover, we only used the fusion to combine the results of our different methods for concept detection (the intersection gave very few CUIs). More sophisticated combination methods could be used to improve the results of the hybrid methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,145.02,119.05,29.57,7.50;3,145.02,281.86,43.32,7.50;3,151.34,299.84,96.89,7.50;3,151.34,310.25,165.68,7.50;3,151.34,320.66,164.13,7.50;3,151.34,331.07,141.03,7.50;3,151.34,341.48,126.30,7.50;3,145.02,359.47,325.31,7.50;3,145.02,369.90,265.95,7.47"><head></head><label></label><figDesc>-C0021485: Injection of therapeutic agent -C0024485: Magnetic Resonance Imaging -C0577559: Mass of body structure -C1533685: Injection procedure Caption: Magnetic resonance imaging. After intravenous injection of adolinium, the mass showed a progressive, heterogeneous, and delayed enhancement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,391.63,345.83,7.89;3,134.77,402.61,166.37,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of an image and the associated CUIs and caption from the training set of the ImageCLEF 2017 caption task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,455.11,349.04,25.48,8.74;6,134.77,361.00,345.82,8.74;6,134.77,372.95,345.82,8.74;6,134.77,384.91,345.82,8.74;6,134.77,396.86,204.32,8.74;6,149.71,420.77,330.88,8.74;6,134.77,432.73,204.98,8.74;6,138.97,450.42,341.62,8.77;6,151.70,462.40,301.79,8.74;6,138.97,473.89,341.62,8.77;6,151.70,485.87,179.22,8.74;6,138.97,497.36,341.63,8.77;6,151.70,509.34,329.01,8.74;6,151.70,521.30,19.37,8.74;6,138.97,532.78,348.09,8.77;6,151.70,544.77,328.89,8.74;6,151.70,556.72,105.00,8.74"><head></head><label></label><figDesc>. The UMLS Semantic Network includes 15 groups: Activities &amp; Behaviors, Anatomy, Chemicals &amp; Drugs, Concepts &amp; Ideas, Devices, Disorders, Genes &amp; Molecular Sequences, Geographic Areas, Living Beings, Objects, Occupations, Organizations, Phenomena, Physiology and Procedures. The following are examples of four captions and their corresponding image IDs, generated using the UMLS-based method: 1. 1471-2342-10-23-4: Procedures: diagnostic computed tomography, imaging pet. Anatomy: armpit. Disorders: metastasis. Physiology: uptake. 2. iej-04-20-g007: Procedures: h&amp;e stain. Chemicals &amp; Drugs: haematoxylin, 11445 red, eosin. Disorders: proliferation. 3. 13014 2015 335 Fig1 HTML: Procedures: brain mri, diffusion weighted imaging, bodies weight. Concepts &amp; Ideas: rows. Chemicals &amp; Drugs: gadolinium. 4. fonc-04-00350-g002: Procedures: antineoplastic chemotherapy regimen. Disorders: abnormally opaque structure, condition response. Anatomy: left lung, anterior thoracic region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,140.99,577.84,339.60,45.01"><head>-</head><label></label><figDesc>Training data includes 164,614 images associated with 20,463 CUIs. 19,145 CUIs have less than 100 images, including 6,251 CUIs with only one image. -Validation data includes 10,000 images associated with 7,070 CUIs. 6,981 CUIs have less than 100 images, including 3,247 CUIs with only one image.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,134.77,495.15,345.83,93.98"><head>Table 1 .</head><label>1</label><figDesc>Most frequent CUIs in the training data of the 2017 concept detection subtask.</figDesc><table coords="3,147.69,524.41,319.97,64.72"><row><cell>CUI</cell><cell>UMLS Term</cell><cell cols="2">UMLS Semantic Type # Associated Images</cell></row><row><cell>C1696103</cell><cell>Image-dosage form</cell><cell>Intellectual Product</cell><cell>17,998</cell></row><row><cell cols="3">C0040405 X-Ray Computed Tomography Diagnostic Procedure</cell><cell>16,217</cell></row><row><cell>C0221198</cell><cell>Lesion</cell><cell>Finding</cell><cell>14,219</cell></row><row><cell>C1306645</cell><cell>Plain x-ray</cell><cell>Diagnostic Procedure</cell><cell>10,926</cell></row><row><cell>C0577559</cell><cell>Mass of body structure</cell><cell>Finding</cell><cell>9,769</cell></row><row><cell>C0027651</cell><cell>Neoplasms</cell><cell>Neoplastic Process</cell><cell>9,570</cell></row><row><cell>C0441633</cell><cell>Scanning</cell><cell>Diagnostic Procedure</cell><cell>9,289</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,134.77,347.04,345.83,116.84"><head>Table 2 .</head><label>2</label><figDesc>Results of our submitted runs to the concept detection subtask and their ranks in comparison with the 37 submitted runs by 9 groups.</figDesc><table coords="9,228.50,377.64,158.23,86.24"><row><cell>Run</cell><cell cols="2">Mean F1 Score Ranking</cell></row><row><cell>DET 1</cell><cell>0.1718</cell><cell>1</cell></row><row><cell>DET 3</cell><cell>0.1648</cell><cell>2</cell></row><row><cell>DET 10</cell><cell>0.1390</cell><cell>10</cell></row><row><cell>DET 4</cell><cell>0.1228</cell><cell>13</cell></row><row><cell>DET 8</cell><cell>0.0880</cell><cell>18</cell></row><row><cell>DET 9</cell><cell>0.0868</cell><cell>20</cell></row><row><cell>DET 6</cell><cell>0.0811</cell><cell>22</cell></row><row><cell>DET 7</cell><cell>0.0695</cell><cell>23</cell></row><row><cell>DET 2 (baseline)</cell><cell>0.0162</cell><cell>34</cell></row><row><cell>DET 5</cell><cell>0.0012</cell><cell>36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,134.77,115.91,345.83,45.51"><head>Table 3 .</head><label>3</label><figDesc>Number of times Open-i retrieves the query image itself and its rank. Images belong to the test set of the ImageCLEF 2017 caption task.</figDesc><table coords="10,204.38,146.92,206.60,14.51"><row><cell>Ranking</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7 8 9 10 Total</cell></row><row><cell cols="8"># matches 3847 756 409 280 230 182 124 99 112 77 6116</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,134.77,360.66,345.83,84.96"><head>Table 4 .</head><label>4</label><figDesc>Results of our submitted runs to the caption prediction subtask and their ranks in comparison with the 34 submitted runs by 5 groups.</figDesc><table coords="10,225.24,391.29,164.88,54.34"><row><cell>Run</cell><cell cols="2">Mean BLUE Score Ranking</cell></row><row><cell>PRED 1</cell><cell>0.5634</cell><cell>1</cell></row><row><cell>PRED 6</cell><cell>0.3317</cell><cell>2</cell></row><row><cell>PRED 2 (baseline)</cell><cell>0.2646</cell><cell>4</cell></row><row><cell>PRED 5</cell><cell>0.2247</cell><cell>11</cell></row><row><cell>PRED 4</cell><cell>0.1384</cell><cell>18</cell></row><row><cell>PRED 3</cell><cell>0.1131</cell><cell>19</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,645.84,98.56,7.86"><p>http://www.nlm.nih.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,144.73,656.80,127.75,7.86"><p>http://clef2017.clef-initiative.eu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.73,634.88,163.01,7.86"><p>https://www.nlm.nih.gov/research/umls</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,144.73,645.84,105.98,7.86"><p>http://Open-i.nlm.nih.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,144.73,656.80,139.51,7.86"><p>http://www.ncbi.nlm.nih.gov/pmc</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,144.73,656.80,146.69,7.86"><p>We used the UMLS 2017AA release.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="4,144.73,656.80,89.85,7.86"><p>http://biowulf.nih.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="5,144.73,634.88,227.40,7.86"><p>http://image-net.org/challenges/LSVRC/2014/eccv2014</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="5,144.73,645.84,150.05,7.86"><p>http://github.com/NVIDIA/DIGITS</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="5,144.73,656.80,112.48,7.86"><p>http://meka.sourceforge.net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="6,144.73,645.84,90.70,7.86"><p>As of September 2016.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11" coords="6,144.73,656.80,204.00,7.86"><p>https://metamap.nlm.nih.gov/MetaMapLite.shtml</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by the <rs type="programName">Intramural Research Program</rs> of the <rs type="funder">National Institutes of Health (NIH)</rs>, <rs type="funder">National Library of Medicine (NLM)</rs>, and <rs type="funder">Lister Hill National Center for Biomedical Communications (LHNCBC)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_yxgm2An">
					<orgName type="program" subtype="full">Intramural Research Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,448.00,337.64,7.86;11,151.52,458.96,329.07,7.86;11,151.52,469.92,307.35,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,431.13,448.00,49.46,7.86;11,151.52,458.96,329.07,7.86;11,151.52,469.92,154.44,7.86">Overview of ImageCLEFcaption 2017 -the image caption prediction and concept extraction tasks to understand biomedical images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,314.30,469.92,82.27,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,480.99,337.63,7.86;11,151.52,491.95,329.07,7.86;11,151.52,502.91,329.07,7.86;11,151.52,513.87,329.07,7.86;11,151.52,524.83,329.07,7.86;11,151.52,535.79,95.77,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,151.52,513.87,277.32,7.86">Overview of ImageCLEF 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">V</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,455.89,513.87,24.70,7.86;11,151.52,524.83,68.40,7.86">CLEF 2017 Proceedings</title>
		<title level="s" coord="11,227.69,524.83,143.35,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,546.86,337.63,7.86;11,151.52,557.82,329.07,7.86;11,151.52,568.75,230.35,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,419.22,546.86,61.36,7.86;11,151.52,557.82,276.11,7.86">Design and development of a multimodal biomedical information retrieval system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,438.81,557.82,41.79,7.86;11,151.52,568.78,146.07,7.86">Journal of Computing Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="168" to="177" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,579.85,337.64,7.86;11,151.52,590.79,225.56,7.89" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="11,341.55,579.85,139.05,7.86;11,151.52,590.81,89.69,7.86">Medical image deep learning with hospital PACS dataset</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Do</surname></persName>
		</author>
		<idno>CoRR abs/1511.06348</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,601.89,337.64,7.86;11,151.52,612.85,329.07,7.86;11,151.52,623.81,147.23,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,179.20,612.85,301.39,7.86;11,151.52,623.81,15.20,7.86">Anatomy-specific classification of medical images using deep convolutional nets</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,188.68,623.81,16.69,7.86">ISBI</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,634.88,337.63,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.77,170.11,7.89" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="11,300.11,645.84,180.48,7.86;11,151.52,656.80,33.97,7.86">Brain tumor segmentation with deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Biard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<idno>CoRR abs/1505.03540</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,119.67,337.64,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,329.07,7.86;12,151.52,152.55,156.79,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,303.76,130.63,128.87,7.86">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,458.46,130.63,22.14,7.86;12,151.52,141.59,290.32,7.86">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">June 7-12, 2015. 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,163.51,337.63,7.86;12,151.52,174.47,329.07,7.86;12,151.52,185.43,271.40,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,328.41,163.51,152.18,7.86;12,151.52,174.47,104.27,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="12,197.61,185.43,210.49,7.86">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,207.34,337.63,7.86;12,151.52,218.28,329.07,7.89;12,151.52,229.26,18.43,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,350.54,207.34,130.05,7.86;12,151.52,218.30,96.21,7.86">Gradient-based learning applied to document recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,255.14,218.30,98.54,7.86">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11">November 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,240.22,337.97,7.86;12,151.52,251.15,329.07,7.89;12,151.52,262.14,13.82,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,365.02,240.22,115.57,7.86;12,151.52,251.18,100.55,7.86">MEKA: A multi-label/multitarget extension to Weka</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,262.92,251.18,157.67,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,273.10,337.98,7.86;12,151.52,284.06,229.25,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,348.99,273.10,131.60,7.86;12,151.52,284.06,116.13,7.86">Data Mining: Practical machine learning tools and techniques</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,295.02,337.98,7.86;12,151.52,305.98,329.07,7.86;12,151.52,316.91,205.50,7.89" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,411.04,295.02,69.55,7.86;12,151.52,305.98,324.74,7.86">A multi-label approach using binary relevance and decision trees applied to functional genomics</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Nozawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Macedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Baranauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,151.52,316.93,137.05,7.86">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="85" to="95" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,327.89,337.98,7.86;12,151.52,338.85,329.07,7.86;12,151.52,349.81,200.27,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,306.75,327.89,173.84,7.86;12,151.52,338.85,239.16,7.86">CEDD: Color and edge directivity descriptor: A compact descriptor for image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="12,414.38,338.85,66.22,7.86;12,151.52,349.81,74.99,7.86">Lecture notes in Computer Sciences</title>
		<imprint>
			<biblScope unit="volume">5008</biblScope>
			<biblScope unit="page" from="312" to="322" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,360.77,337.98,7.86;12,151.52,371.73,329.07,7.86;12,151.52,382.69,329.07,7.86;12,151.52,393.65,32.25,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,306.27,360.77,174.32,7.86;12,151.52,371.73,189.51,7.86">FCTH: Fuzzy color and texture histogram: A low level feature for accurate image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,363.45,371.73,117.14,7.86;12,151.52,382.69,296.14,7.86">Proceedings of the 9th International Workshop on Image Analysis for Multimedia Interactive Service</title>
		<meeting>the 9th International Workshop on Image Analysis for Multimedia Interactive Service</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
