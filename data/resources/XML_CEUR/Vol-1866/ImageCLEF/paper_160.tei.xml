<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.96,65.72,337.28,12.93;1,153.72,83.72,307.77,12.93;1,214.80,101.60,185.66,12.93">A Cross-Modal Concept Detection and Caption Prediction Approach in ImageCLEFcaption Track of ImageCLEF 2017</title>
				<funder ref="#_R5yyrTV">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,180.12,138.85,103.32,9.96"><roleName>Md</roleName><forename type="first">Mahmudur</forename><surname>Rahman</surname></persName>
							<email>md.rahman@morgan.edu</email>
						</author>
						<author>
							<persName coords="1,292.44,138.85,68.64,9.96"><forename type="first">Terrance</forename><surname>Lagree</surname></persName>
						</author>
						<author>
							<persName coords="1,368.76,138.85,66.66,9.96"><forename type="first">Martina</forename><surname>Taylor</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Morgan State University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Morgan State</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.96,65.72,337.28,12.93;1,153.72,83.72,307.77,12.93;1,214.80,101.60,185.66,12.93">A Cross-Modal Concept Detection and Caption Prediction Approach in ImageCLEFcaption Track of ImageCLEF 2017</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">58EEC9445DCEC72FEA43D8863B9A8183</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>in the ImageCLEFcaption under ImageCLEF 2017. The purpose of this research and participation is to be able to predict the caption and detect UMLS concepts of an unknown query (test) image by using Cross Modal Retrieval and Clustering techniques. In our approach, for each image (without any caption or concept information) in the test set, we find the closest matching image in the training set by applying similarity search (e.g., content based image retrieval) in a combined feature space of color, texture, and edge-related visual features. By linking the associated caption and UMLS concepts of the closest matched image, further processing are performed to extract terms (keywords/concepts) to form a text feature vector and finally return the top ranked terms as predicted concepts (caption) from the best matching cluster centroids which are previously generated by applying K-means clustering in a term-document matrix of the training set. In this article we present main objectives of experiments, overview of these approaches, resources employed, and describe our submitted runs and results with conclusions and future directions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This article describes the main objectives of cross modality matching approach based on our first year participation in ImageCLEF 2017 <ref type="bibr" coords="1,385.08,473.89,10.56,9.96" target="#b0">[1]</ref> for the ImageCLE-Fcaption track. This track consists of both Concept Detection and Caption Prediction Tasks <ref type="bibr" coords="1,196.80,497.89,9.99,9.96" target="#b1">[2]</ref>. For the Concept Detection, participating systems are tasked with identifying the presence of relevant UMLS concepts in images appeared in bio-medical journal articles (PubMed Central). For the Caption Prediction, participating systems are tasked with composing coherent captions for the entirety of an image based on the interaction of visual information content and the detected concepts from the first task.</p><p>Besides in clinical settings, bio-medical images are also sources of essential information for research and education in biomedical literature. For example, authors of journal articles frequently use images to elucidate the text and to illustrate important concepts or to highlight special cases as Region of Interests (ROIs) <ref type="bibr" coords="2,168.24,68.29,9.99,9.96" target="#b2">[3]</ref>. Overall, biomedical literature incorporates an approximation of 100 million figures, whereas the biomedical open access literature of PubMed Central of National Library of Medicine (NLM) alone contained almost two million images in 2014 <ref type="bibr" coords="2,202.32,104.17,9.99,9.96" target="#b3">[4]</ref>. The images contained in biomedical articles are seldom selfevident, and much of the information required for their comprehension can be found in the text of the articles in which they appear. Figure captions, article titles, abstracts, and snippets of body text from within the articles all contribute to image understanding. Hence, biomedical images cannot be easily understood when they are removed from their original context. Given the rapid pace of scientific discovery in medical field, it poses significant challenges to transform of massive volumes of image and text data from biomedical articles into useful information and actionable knowledge in the form of effective and efficient search process <ref type="bibr" coords="2,169.44,211.81,9.99,9.96" target="#b2">[3]</ref>.</p><p>There has been a lot of interest in information retrieval, computer vision, and multimedia community recently in developing cross and multi-modal image retrieval techniques with the massive explosion of multimedia content on the web. Multimedia contents, such as web pages, scientific publications, and document images convey information using multiple modalities, including text, layout/style and images. However, an intrinsic problem here is to investigate the semantic correlation amongst the text and image data. Different models have been proposed to learn the dependencies between the visual content of an image set and the associated text captions, then allowing for the automatic creation of semantic indexes for un-annotated images <ref type="bibr" coords="2,320.64,331.57,7.80,9.96" target="#b4">[5]</ref><ref type="bibr" coords="2,328.44,331.57,3.90,9.96" target="#b5">[6]</ref><ref type="bibr" coords="2,328.44,331.57,3.90,9.96" target="#b6">[7]</ref><ref type="bibr" coords="2,332.34,331.57,7.80,9.96" target="#b7">[8]</ref>.</p><p>Motivated by these approaches in general domain, in the following sections, we describe our cross-modal search approach towards the concept detection and caption prediction tasks in ImageCLEFcaption for bio-medical images in journal articles. In the following sections, we describe our content-based visual search approach (Section 2) to link test images to closet match images in the training set, text feature extraction and K-means clustering in a term-document matrix (Section 3) from associated image concepts and captions and detection and prediction of concepts and captions respectively (Section 4) for test images based on using Python 3.5, with OpenCV 2.0, sklearn, scikit-image, NLTK and mahotas packages. Section 5 describes our submitted runs and results that we achieved and finally Section 6 provides our conclusions and future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Content-Based Visual Similarity Search Approach</head><p>We at first performed a content-based visual similarity search for each image in the test set as an example query image to a content-based image retrieval (CBIR) system where images in both training and validation sets are indexed at first by extracting several low-level color, texture, and edge related visual feature. The purpose of this similarity search is to find the closet matching image in the training (validation) set for each query (test) image and using its associated caption (concepts) for further processing (keyword extraction, clustering) for caption prediction and concept detection. So, the CBIR search is the first step of the pipeline of our cross-modal process to link unknown test images with associated captions and UMLS concepts of known images with captions (concepts). To save computational time, each image is resized (100 x 100 pixels) and the following features are extracted: In addition to the color descriptor, we extracted the well-known Local Binary Patterns (LBPs) <ref type="bibr" coords="3,211.08,485.89,10.56,9.96" target="#b8">[9]</ref> as a texture feature and Histogram of Oriented Gradients (HOG) <ref type="bibr" coords="3,170.28,497.89,15.60,9.96" target="#b9">[10]</ref> as an edge-related feature from each image. LBPs also compute a local representation of texture by comparing each pixel with its surrounding neighborhood of pixels. The first step in constructing the LBP texture descriptor is to convert the image to gray-scale. For each pixel in the gray-scale image, we select a neighborhood of size radius (r) surrounding the center pixel. A LBP value is then calculated for this center pixel and stored in the output 2D array with the same width and height as the input image. We initialized our LBP descriptor using a numP oints = 24 to store the number of points and r = 8 for the radius. The process at first generates a 2D array with the same width and height as our input image each of the values inside the array ranges from [0, numP oints + 2], a value for each of the possible numP oints + 1 possible rotation invariant prototypes along with an extra dimension for all patterns that are not uniform, yielding a total of numP oints + 2 unique possible values. Finally, our LBP feature vector as a normalized histogram (numP oints + 2dimensional), which counts the number of times each of the prototypes appears and normalized to the range of [0, 1].</p><p>HOG is known as a keypoint descriptor in literature which expresses the local statistics of the gradient orientations around a keypoint <ref type="bibr" coords="4,413.52,152.05,14.69,9.96" target="#b9">[10]</ref>. The HOG feature can express object appearance due to the reason that the histogram process gives translational invariance the gradient orientations are strong to lighting changes. The HOG feature extraction process consists of three phases. In first phase first order derivatives in x and y directions are computed and the image is divided into m × n tiled regions. Gradient orientations quantized into n bins. Then, for each tiled region 1-D histogram of gradient orientations which is weighted by gradient magnitude is accumulated. Eventually, obtained feature vectors are normalized to provide robustness to illumination changes and HOG feature vectors are collected for all blocks over detection window.</p><p>For similarity matching, each feature is concatenated to form a combined feature vector and Euclidean distance measure is used for k-nearest neighbor image similarity where top matching images are ranked from a low to high scores in the range of [0, 1] and only the top ranked (closest match) image is selected for each query (test) image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Text Feature Extraction and Clustering</head><p>Our next step of the process is the text feature extraction and indexing (creating a document-term matrix for subsequent clustering) of associated image captions (concepts) of training (validation) images and perform clustering to form natural groups of images with similar (related) captions and concepts. For the caption prediction task, each associated caption of training images is converted to lower-case, all punctuation are removed and tokenized into its individual words. Stopwords are removed using NLTK's "english" stopword list and subsequently terms are removed from the vocabulary that occur in fewer than 10 captions, and finally the remaining words are reduced to their stems using NLTK's Snowball stemmer, which finally form the vocabulary list T = {t 1 , t 2 , • • • , t N } of index terms or keywords of the image captions. Finally, a document-term matrix M is created based on T , where each caption is modeled as a vector of keywords (terms) as</p><formula xml:id="formula_0" coords="4,243.24,527.12,237.39,12.79">f D j = [ ŵ1j , • • • , ŵij , • • • ŵNj ] T<label>(1)</label></formula><p>where each ŵij denotes the tf-idf weight of a keyword t i , 1 ≤ i ≤ N in the caption of image I j . This weighting scheme amplifies the influence of terms, which occur often in a document (e.g., tf factor), but relative rarely in the whole collection of documents (e.g., idf factor <ref type="bibr" coords="4,336.79,581.53,15.88,9.96" target="#b10">[11]</ref>. The document-term matrix is converted to a sparse matrix in Python which only records non-zero entries to save memory space as we have significant number of entries that are zero.</p><p>However, with a size of T &gt; 20, 000, our matrix is still too large. Hence, to reduce the dimension further, Latent Semantic Analysis (LSA) is applied and further analysis (clustering) is performed in the LSA projected feature space by keeping explained variance above 90%. In many instances, LSA is found suitable to reduce dimensionality in a spare matrix and discover latent patterns in the data.</p><p>For the concept detection task, the text feature extraction and indexing approaches is more straightforward as we do not to perform any extra preprocessing steps, such as removal of stopwords, tokenization, stemming, LSA etc. In this case, we generate the vocabulary list T based on the presence of all the UMLS concepts in the images of the training set and generate the sparse matrix accordingly for further analysis in the next step of the process.</p><p>Our final goal is is to partition N data points (text feature vectors of training set) into K clusters by applying K-means clustering <ref type="bibr" coords="5,364.20,223.93,14.69,9.96" target="#b11">[12]</ref>. In K-means, each feature vector of image caption (concepts) is assigned to a cluster with the nearest mean where the mean of each cluster is called its "centroid" or "center". Overall, applying K-means yields K separate clusters of the original n data points. Data points inside a particular cluster are considered to be "more similar" to each other than data points that belong to other clusters. For this, we used the scikit-learn implementation of K-means in Python by experimenting with different number of clusters (n clusters) and used parameters (e.g., max iter = 300 and tol = 0.0001) for maximum number of iterations for a single run and relative tolerance with regards to inertia to declare convergence. The K-means algorithm aims to choose centroids that minimise the inertia, or within-cluster sum of squared criterion, which can be recognized as a measure of how internally coherent clusters are. After generating the clusters, the top terms per cluster are ranked and used for later use of caption prediction and concept detection of an unknown query (test) image. It is assumed that caption (concepts) that belong to a given cluster will be more similar in terms than belonging to a separate cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Detecting Concepts and Predicting Captions of Test Images</head><p>After performing all the above processes (e.g., visual feature extraction, CBIR search, text feature extraction, and clustering), our final task is to detect concepts and predict captions of unknown images (without associated captions and UMLS concepts) in the test set. For this step, we at first find the closest matching image in the training (validation) set for each test image by applying the similar visual feature extraction and similarity search process described in Section 2.</p><p>After finding the associated captions (concepts) of closet matching images, we next generate the text feature vectors accordingly and find the closet cluster labels by applying a minimum distance matching function to previously generated "centroid" or "center" in the training set as described in the previous section. Based on the cluster labels, we look-up for the top terms (keywords) and return those as assumed image captions or concepts. Below, all the steps of the above process is described algorithmically:</p><p>Algorithm 1 Concept Detection/Caption Prediction Process 1: Initially, resize and extract color, texture, and edge-related visual features F (local color descriptor, HOG, and LBP) for images in the training set. 2: Extract text feature from captions (concepts) in the training set to generate the vocabulary list T and document-term matrix M 3: Apply K-mean clustering in matrix M to generate different number of clusters (nclusters). 4: for j ∈ S images in the Test Set do 5:</p><p>Resize and extract visual features for test image Ij.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Extract text feature vector from associated caption (concepts) Dj by using vocabulary list T . 7:</p><p>Find the closet cluster label by applying a minimum distance matching function between feature vectors of caption (concept) and cluster centroids. 8:</p><p>Return and print the top (K) terms (keywords) from the best matching centroids. 9: end for 10: Finally, generate the result file (run) for test images with image name and associated caption (concepts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Submitted Runs and Results</head><p>This section provides descriptions of our submitted runs and analysis of the result. We performed, feature (visual and textual) extraction and clustering in both the training set of around 164K images and validation set of 10K images with associated captions and UMLS concepts. We submitted the following four runs for the concept detection task: 1. DET Morgan result concept from CBIR.csv : This is our baseline run for the concept detection task. In this run, each test image is automatically acted as a query image to our CBIR system to find the closet matching image in the training set and use the associated concepts as the concepts for the test image.</p><p>2. DET Morgan result concept from train Kmean top20.csv : In this run, each test image is automatically acted as a query image to our CBIR system to find the closet matching image in the training set and use the associated concepts to form a text feature vector. This vector matches to the closet cluster centroids out of 50 clusters previously generated by K-means in the training set and returns the top (20) terms (concepts) of that particular centroids.</p><p>3. DET Morgan result concept from val Kmean50 top15.csv : In this run, each test image is automatically acted as a query image to our CBIR system to find the closet matching image in the validation set and use the associated concepts to form a text feature vector. This vector matches to the closet cluster centroids out of 50 clusters previously generated by K-means in the training set and returns the top (15) terms (concepts) of that particular centroids.</p><p>4. DET Morgan result concept from train Kmean300 top15.csv : In this run, each test image is automatically acted as a query image to our CBIR system to find the closet matching image in the training set and use the associated concepts to form a text feature vector. This vector matches to the closet cluster centroids out of 300 clusters previously generated by K-means in the training set and returns the top (15) terms (concepts) of that particular centroids.</p><p>For concept detection task, evaluation is conducted in terms of average (mean) F1 scores between system predicted and ground truth concepts in the test set <ref type="bibr" coords="7,169.80,199.81,9.99,9.96" target="#b1">[2]</ref>, which was generated based on the UMLS Full Release 2016AB. Our last run in the Table <ref type="table" coords="7,263.28,370.21,4.98,9.96" target="#tab_0">1</ref> ranked fourth group wise with a mean F1 score, 0.0498 when no external resources were used.</p><p>For the Caption Prediction task, we tried to submit few run based on following the same process described for the Concept Detection task. However, there were some problems in our runs (result files) and we received errors while the files were parsed by the evaluation tool as provided by the CLEF organizer. We are currently trying to fix the problem and will evaluate and analyze our results at a later time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This article describes the cross-modal strategies of the CS Morgan group for the concept detection and caption prediction tasks of the ImageCLEFcaption track. Instead of directly performing image understanding, our cross-modal approach tries to link test images with images in the training set based on visual similarity at first and from there further text processing and clustering are performed to detect concepts and predict captions from groups(clusters) where images with similar concepts/captions reside. Our results indicate that clustering with large number (300) of centroids is better in terms of mean F1 score. However, contentbased approaches to image retrieval are not yet advanced enough to achieve the precision of text-based approaches, and we are currently working towards directly mapping image region to concepts aided by a ground-truth training set of image patches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,179.04,328.60,257.19,8.97;3,188.88,138.28,237.60,176.16"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of dividing our image into 5 different segments</figDesc><graphic coords="3,188.88,138.28,237.60,176.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,136.20,232.00,404.66,102.09"><head>Table 1 .</head><label>1</label><figDesc>Results of the four Submitted Runs for the Concept Detection Task</figDesc><table coords="7,136.20,274.96,30.44,8.97"><row><cell>Run ID</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This research is supported by the <rs type="funder">NSF</rs> <rs type="grantName">HBCU-UP Research Initiation Award</rs> No. <rs type="grantNumber">1601044</rs>. We would like to thank the ImageCLEF [1] organizers for making the database available for the experiments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_R5yyrTV">
					<idno type="grant-number">1601044</idno>
					<orgName type="grant-name">HBCU-UP Research Initiation Award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.88,218.68,337.77,8.97;8,151.56,229.71,329.10,8.97;8,151.56,240.63,328.94,8.97;8,151.56,251.56,277.78,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,442.92,218.68,37.73,8.97;8,151.56,229.71,329.10,8.97;8,151.56,240.63,83.89,8.97">Overview of ImageCLEF 2017: Information extraction from images, Title: Experimental IR Meets Multilinguality</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,241.56,240.63,238.94,8.97;8,151.56,251.56,150.32,8.97">Multimodality, and Interaction 8th International Conference of the CLEF Association, CLEF 2017</title>
		<title level="s" coord="8,309.36,251.56,61.96,8.97">Proc. of LNCS.</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.88,261.99,337.66,8.97;8,151.56,272.91,328.87,8.97;8,151.56,283.96,314.98,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,432.00,261.99,48.54,8.97;8,151.56,272.91,328.87,8.97;8,151.56,283.96,160.45,8.97">Overview of ImageCLEFcaption 2017 -the Image Caption Prediction and Concept Extraction Tasks to Understand Biomedical Images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Garca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,319.56,283.96,112.64,8.97">CLEF working notes. CEUR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.88,294.39,337.76,8.97;8,151.56,305.31,328.99,8.97;8,151.56,316.23,215.08,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,252.12,305.31,228.43,8.97;8,151.56,316.23,24.39,8.97">Literature-based biomedical image classification and retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,182.52,316.23,120.40,8.97">Comput Med Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.87,326.79,337.67,8.97;8,151.56,337.71,328.84,8.97;8,151.56,348.63,328.84,8.97;8,151.56,359.55,25.42,8.97" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,446.04,326.79,34.51,8.97;8,151.56,337.71,324.78,8.97">Combining Text and Visual Features for Biomedical Information Retrieval and Ontologies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-09">September 2010. 2010</date>
		</imprint>
	</monogr>
	<note>Technical Report to the LHNCBC Board of Scientific Counselors</note>
</biblStruct>

<biblStruct coords="8,142.87,370.11,337.76,8.97;8,151.56,381.04,328.96,8.97;8,151.56,391.95,313.06,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,287.11,370.11,193.53,8.97;8,151.56,381.04,167.09,8.97">Image-to-word transformation based on dividing and vector quantizing images with words</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Oka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,339.00,381.04,141.52,8.97;8,151.56,391.95,279.70,8.97">Proc. MISRM99 first international workshop on multimedia intelligent storage and retrieval management</title>
		<meeting>MISRM99 first international workshop on multimedia intelligent storage and retrieval management</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.87,402.39,337.61,8.97;8,151.56,413.31,328.95,8.97;8,151.56,424.35,160.60,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,325.94,402.39,154.54,8.97;8,151.56,413.31,299.12,8.97">Harmonizing hierarchical manifolds for multimedia document semantics understanding and cross-media retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,458.40,413.31,22.11,8.97;8,151.56,424.35,67.60,8.97">IEEE Trans Multimed</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="437" to="446" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.87,434.79,337.67,8.97;8,151.56,445.71,328.98,8.97;8,151.56,456.63,329.14,8.97;8,151.56,467.67,151.84,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,332.36,434.79,148.18,8.97;8,151.56,445.71,164.89,8.97">Automatic image annotation and retrieval using cross-media relevance models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,336.48,445.71,144.06,8.97;8,151.56,456.63,329.14,8.97;8,151.56,467.67,89.12,8.97">Proceeding SIGIR 03 Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval</title>
		<meeting>eeding SIGIR 03 eedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">119126</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.87,478.11,337.88,8.97;8,151.56,489.03,329.08,8.97;8,151.56,499.95,296.68,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,255.72,489.03,206.60,8.97">A newapproach to cross-modalmultimedia retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Costa Pereira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,151.56,499.95,233.05,8.97">Proceedings of the international conference on multimedia</title>
		<meeting>the international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">251260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.87,510.39,337.55,8.97;8,151.56,521.43,328.83,8.97;8,151.56,532.35,279.40,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,319.80,510.39,160.63,8.97;8,151.56,521.43,243.99,8.97">Multiresolution Grayscale and Rotation Invariant Texture Classification with Local Binary Patterns</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Menp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,403.56,521.43,76.83,8.97;8,151.56,532.35,182.58,8.97">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.54,542.79,337.97,8.97;8,151.56,553.71,328.99,8.97;8,151.56,564.75,194.44,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,258.48,542.79,218.10,8.97">Histograms of oriented gradients for human detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,162.84,553.71,317.71,8.97;8,151.56,564.75,129.94,8.97">Proc. of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<meeting>of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">886893</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.54,575.19,338.12,8.97;8,151.56,586.11,40.78,8.97" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="8,264.24,575.19,118.68,8.97">Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">R</forename><surname>Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
	<note>Reading</note>
</biblStruct>

<biblStruct coords="8,142.54,596.55,337.91,8.97" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="8,268.44,596.55,122.52,8.97">Algorithms for Clustering Data</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
