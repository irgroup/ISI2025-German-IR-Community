<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.79,116.95,333.77,12.62;1,215.78,134.89,183.80,12.62">A Textual Filtering of HOG-based Hierarchical Clustering of Lifelog Data</title>
				<funder ref="#_47YRdev #_HKJJh4S">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,238.59,172.56,61.34,8.74"><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
							<email>mdogariu@imag.pub.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Multimedia Lab @ CAMPUS</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.94,172.56,68.83,8.74"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
							<email>bionescu@alpha.imag.pub.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Multimedia Lab @ CAMPUS</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.79,116.95,333.77,12.62;1,215.78,134.89,183.80,12.62">A Textual Filtering of HOG-based Hierarchical Clustering of Lifelog Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4D733E35943F30292ACBBBFA67AA58D6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lifelog</term>
					<term>WordNet</term>
					<term>word similarity</term>
					<term>hierarchical clustering</term>
					<term>concept detector</term>
					<term>Retina</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we address the issue of life logging information retrieval and we introduce an approach that uses the output of a hierarchical clustering of data via assessing word similarities. Word similarity is computed using WordNet and Retina ontologies. We have tested our method during the 2017 ImageCLEF Lifelog challenge, the Summarization subtask. We discuss the performance, limitations and future improvements of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lifelogging is the process of tracking your own activities at every moment of the day. But this is not limited only to actions as it also addresses the places that you visit and the people that you engage into activities with. A formal definition describes lifelogging as "a form of pervasive computing, consisting of a unified digital record of the totality of an individuals experiences, captured multimodally through digital sensors and stored permanently as a personal multimedia archive" <ref type="bibr" coords="1,174.58,465.83,9.96,8.74" target="#b0">[1]</ref>.</p><p>Lifelogging consists of best describing all events within a time frame from ones life. This is usually achieved by wearing a camera which takes automatic pictures at certain time intervals during the entire day, thus capturing a great amount of information from the wearer's point of view. However, this does not offer temporal or spatial information other than what it can be extracted from the details of the captured images. In order to solve this, certain devices can be used to add timestamps and geolocation to the images. In addition, there also exists devices that offer information about numerous other aspects concerning the lifelogger's status and the surrounding environment such as pulse, temperature, pH or acoustic sensors. As it can be seen, only the environment limits the type of data that can be collected and all these details build up leading to an increasingly accurate description of the lifelog.</p><p>Extensive details about lifelogging can also be found in <ref type="bibr" coords="1,407.18,621.25,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="1,419.35,621.25,7.01,8.74" target="#b2">3]</ref>. In these 2 papers, different techniques of acquiring data are presented and they offer important insights on the problems that arise with this new trend. Recent technological advances have aided the growing trend of lifelogging by offering longer battery life for the wearable gadgets, a greater variety of sensors that augment the observable aspects of all events and also higher accuracy for the provided data. However, all these aspects have the drawback of requiring complex processing steps. With the technical problems related to acquisition or storage of information being solved by the state of the art devices there have been obtained large volumes of data which need to be interpreted.</p><p>As it would be a tedious process to manually annotate and interpret all the data from a lifelog, an increasing interest in developing techniques that perform this automatically has emerged. Therefore, Information Retrieval systems fit the requirements for this job, potentially solving the problems posed by the vast quantity of data. A rigorous comparative benchmarking campaign regarding Lifelogging was conducted by the NTCIR initiative <ref type="bibr" coords="2,365.68,251.74,10.52,8.74" target="#b3">[4]</ref> and has now also been adapted by the CLEF initiative in the ImageCLEF Lifelog task <ref type="bibr" coords="2,415.01,263.69,9.96,8.74" target="#b4">[5]</ref>.</p><p>This paper describes our participation to ImageCLEF Lifelog Summarization Task, one of the 4 tasks approached by the ImageCLEF benchmarking campaign <ref type="bibr" coords="2,161.61,299.80,9.96,8.74" target="#b5">[6]</ref>. The summarization task aims at clustering a lifelog database given 10 topics. Each of these topics consists of a short paragraph which briefly describes what is considered relevant and what is not for that specific task. Our work focused on finding a correlation between the textual description of a topic and the confidences provided by the concept detector for each image in the given database. For each image we computed the Wu-Palmer similarity measure <ref type="bibr" coords="2,470.08,359.57,10.52,8.74" target="#b6">[7]</ref> between the most relevant concepts and a selection of words from the topic description. What "relevant concepts" means and how the selection has been done is described in the following Section. Each similarity measure is then weighted by the confidence assigned by a concept detector for the relevant concept, thus obtaining a score for each image. The images from the database are clustered with an off-the-shelf hierarchical clustering implementation. In the end, the similarity score is used to sort and select the best candidates from these clusters. Related works have been described in <ref type="bibr" coords="2,306.22,455.22,10.52,8.74" target="#b3">[4]</ref> with <ref type="bibr" coords="2,344.17,455.22,10.52,8.74" target="#b7">[8]</ref> and <ref type="bibr" coords="2,378.80,455.22,10.52,8.74" target="#b8">[9]</ref> being closest to our approach as they also computed a similar WordNet <ref type="bibr" coords="2,356.39,467.17,15.50,8.74" target="#b9">[10]</ref> based similarity distance in their algorithms.</p><p>The remainder of the paper is organized as follows. In Section 2 we explain our proposed system, in Section 3 we present our experimental results, followed by the conclusions in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The proposed approach</head><p>Due to the fact that the given data contained a large variety of information, our algorithm tries to capture this aspect by approaching several tasks in parallel. It also mixes up textual and visual information in the process of selecting the best candidates for the requirements of the task. In Fig. <ref type="figure" coords="2,391.52,609.29,4.98,8.74" target="#fig_0">1</ref> a general diagram of the involved processes shows that visual and textual descriptions interleave. An important aspect of this algorithm is that it relied solely on the information provided by the organizers and no additional annotations or external data have been used. The algorithm starts by analyzing the output of the concept detector provided by the organizers and selecting for each image the most probable concepts only. Each topic out of the list of 10 is then parsed such that relevant words are kept only. Also, information regarding location, activity and the targeted user are extracted. The image database comes with an .xml file that describes each image in terms of activity, location, timestamp and descriptive information related to the user the image belongs to. The similarity score is computed using WordNet's builtin similarity distance functions and this concludes the fundamental part of the textual part. The images undergo an elimination step imposed by the topic restrictions and thus the number of items of interest is greatly reduced. This shortlist of images is then subject to a clustering step and, finally, the results are pruned with the help of the similarity scores presented before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image concepts</head><p>An important input to our system is represented by the list of concepts along with the respective confidences for each image from the database. This has been obtained by passing each image through the CAFFE CNN-based visual concept detector and storing the results, as described by the organizing team <ref type="bibr" coords="3,444.03,513.38,9.96,8.74" target="#b4">[5]</ref>. This concept detector acts as a soft classifier with 1000 classes and it outputs a given confidence level to each class. It is very debatable if this concept detector is fit for such a task since it covers a large and diverse range of classes, many of which are not related to the objects depicted or seen on a daily basis by the lifeloggers from the database. It is worth mentioning that the vast number of concepts may affect their accuracy and, consequently, the results of our system.</p><p>It is obvious that not all 1000 concepts are relevant for one image as it is difficult even for humans to detect more than a few tens of concepts in a real-life picture as are the ones from the database. Therefore, it was mandatory to filter out the unnecessary concepts and we chose to do so based on the confidence score that was provided by the concept detector. Thus, it became a matter of design to select a certain threshold above which a concept would be qualified as relevant. We chose a statistic approach and set the threshold dependent on the confidence distribution of each individual image.</p><p>Let C (i) = (c 0 , c 1 , ..., c 999 ) be the set of confidences for any image i in the dataset. By imposing a threshold:</p><formula xml:id="formula_0" coords="4,277.44,176.31,203.15,8.74">t = µ + 3 × σ,<label>(1)</label></formula><p>with µ being the mean and σ being the standard deviation over C (i) . The set of relevant concepts becomes:</p><formula xml:id="formula_1" coords="4,224.03,227.41,256.56,14.30">C (i) relevant = {c i | ∀c i ∈ C (i) s.t. c i &gt; t},<label>(2)</label></formula><p>thus giving us some insightful information about the concepts that were detected in the images.</p><p>The first notable aspect is that the number of relevant concepts whose confidence is greater than t differs from one image to another due to the standard deviation. Since all confidences add up to 1 for any image it is clear that the mean will always be 0.001, leaving the standard deviation as the only variable parameter when computing the threshold. As for most distributions, there is a very narrow set, if any, of concepts that have a confidence higher than the given threshold due to its high value. The results can be interpreted as follows: if there are some concepts with a very high confidence score they will raise the standard deviation of the entire set and will eliminate other less confident concepts; if there are no such highly confident concepts then the algorithm will select the most probable few from the set, leading to a larger set of concepts. All in all, this exploits highly confident concepts and it also takes into account the cases where there is no such certainty and where several guesses are bound together to form a less probable description of an image.</p><p>Since each concept is described as one or several words we used WordNet to find the common synset to all the words that describe a concept and use it to describe the concept with one word only. This was also a preliminary step for computing the similarity score described in Section 2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topic analysis</head><p>The topics given by the organizers represent the search query for our information retrieval system. There are a total of 10 topics, each being described by a title, a short summary and a slightly longer detailed description. Most of the times, the information from the title and the summary is also present in the detailed description so we can ignore the words from the title and the summary. Moreover, the detailed descriptions followed a general pattern, stating which user was targeted by that specific query, what action he was involved in and the place where he was at the moment when the picture was taken.</p><p>The topics also impose restrictions on the conditions under which an image is relevant or not with some of them being very strict on this matter. However, all images that are blurry, out of focus or where the hands of the lifelogger covered most of the picture are considered irrelevant.</p><p>As the topic descriptions consisted of a text file, the analysis of the topics falls under the NLP processing, covered by WordNet's embedded tools. In other words, for each of the 10 topics we did as follows: we stripped the general restrictions part as it appeared for each topic and did not help discriminate between them and extracted only the keywords from the text formed by joining the title, summary and description of a topic. Another step was to remove all stopwords, which offer little to no information in a sentence. As WordNet was designed for nouns and verbs only, we kept these parts of speech from the remaining list of words. We are aware that some important information is lost when removing adverbs, adjectives and negations but as NLP is not our main research field we adopted this simplified parsing algorithm.</p><p>In the end, for each topic we obtain some coarse, but fundamental information related to that specific query. As an example, the next topic: T1. In a Meeting 2 Query: Summarize the activities of user u1 in a meeting at work. Description: To be considered relevant, the moment must occur at meeting room and must contain at least two colleagues sitting around a table at the meeting. Meetings that occur outside of the work place are not relevant. Different meetings have to be summarized as different activities. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant.</p><p>is now summarized as the set of words T (1) ={'activities', 'u1', 'meeting', 'work', 'occur', 'room', 'contain', 'colleagues', 'sitting', 'table', 'meetings', 'place'}. There is an obvious shortening of the description, but the meaning of the new context is still understandable to any reader. Automatically extracting the keywords from a sentence is still an open problem on which the NLP community is still working.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Narrowing the list of images</head><p>Metadata pre-processing involved parsing the xml file associated with the image database and transferring the useful information in a more user-friendly format such as a matrix where we stored each important attribute on a different column for each image. Among these attributes we note the user id, image id and path, the activity and the location where the said picture was taken. It is worth mentioning that the location and activity tags are not very specific, thus offering a wide range for extrapolating on them only. We discovered 6 different activities (bus, car, cycling, running, transport, walking) and over 100 semantic locations as they are tagged in the Moves App <ref type="bibr" coords="5,298.96,585.14,14.61,8.74" target="#b10">[11]</ref>, but we also note that images are not mandatory tagged with the activity or location. Some of them do not posses this attribute and we have filled their corresponding attribute value with 0.</p><p>The provided database consisted of images belonging to 3 lifeloggers or users as it is described in the lifelog task overview <ref type="bibr" coords="5,336.61,633.20,10.52,8.74" target="#b4">[5]</ref> and each topic addressed one particular user only. This meant that the other 2 users could be ignored in the decision process. The same logic applies to the activity and location fields, where the topic description would eliminate certain candidate images because of their respective metadata, e.g. if the query asks for images where the user u1 is in a meeting at work it is not possible to select images where the location indicates something else than user u1's workplace (or 0 if the workplace has not been annotated in the Moves App) or where the activity is bus, car, cycling, running or transport. This type of content interpretation allowed us to create a shortlist of images for each distinct topic which we then used in the clustering part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Image clustering</head><p>For the clustering part we implemented a hierarchical clustering algorithm based on the Histogram of Oriented Gradients (HOG) <ref type="bibr" coords="6,346.50,251.07,15.50,8.74" target="#b11">[12]</ref> extracted from each image in the previously described shortlist. For each image we cropped the edges as follows: 100 pixels from the top edge, 128 from both left and right edge and 25 pixels from the bottom edge. We chose these values as the users wear their camera around their neck and, quite often, the camera is slightly tilted upwards, especially when the user is sitting down, thus adding some unnecessary information in the upper part of the image or the user's clothing covers one of the other 3 edges.</p><p>The cropped images underwent a resize step as well, bringing them to a format of 64×128 pixels, so that the HOG extraction process could be performed. We have used a simple builtin HOG extractor from Python's cv2 module and stopped the hierarchical clustering algorithm when 30 clusters were formed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Similarity score</head><p>The process of finding semantic links between any two words is a very complex task for NLP and is still an open problem. However, WordNet offers a large lexical database for English which incorporates not only a glossary, but also a tree-like structure that connects semantic meanings starting from a root node, which is the most abstract concept, and descends to more and more particular concepts. Even so, this tree-like structure has been developed for nouns and verbs only, this being the motive behind the algorithm used for topic description summarization.</p><p>WordNet has its information organized in sets of cognitive synonyms (synsets) interleaved by means of conceptual-semantic and lexical relations <ref type="bibr" coords="6,437.68,537.56,14.61,8.74" target="#b9">[10]</ref>. This means that the number of edges that have to be passed in order to reach from one synset to another gives information about how similar these two synsets are. Having this in mind, a first approach to finding a similarity measure would be to follow the shortest path between two nodes, count the number of edges along the way and then set the similarity measure to be inversely proportional to the previously found number. A maximum similarity score of 1 would be obtained when the two synsets are the same. However, this does not take into account the depth of the node from which the two synsets descend. This node is known as the Least Common Superconcept (LCS) and the distance from the root node to this node gives a measure of abstractness.</p><p>One word similarity measure that takes into account both the path length between two concepts and the depth of their LCS is the Wu-Palmer similarity measure. Given the tree structure from Fig. <ref type="figure" coords="7,325.05,143.90,4.98,8.74" target="#fig_1">2</ref> the Wu-Palmer similarity distance between the concepts C 1 and C 2 is</p><formula xml:id="formula_2" coords="7,231.09,171.82,245.26,23.22">d wup (C 1 , C 2 ) = 2 × N 3 N 1 + N 2 + 2 × N 3 , (<label>3</label></formula><formula xml:id="formula_3" coords="7,476.35,178.56,4.24,8.74">)</formula><p>where C 3 is the LCS, N 3 is the depth of the LCS from the root node and N 1 and N 2 are the depths of C 1 and C2, respectively, from the LCS. This approach gives higher similarity values to the pairs of concepts which have the LCS deeper in the tree structure and the score will always have a value in the range ]0, 1]. The value cannot be 0 because the depth of the LCS can never be 0 (the root of such a taxonomy has a depth of 1), and it will be 1 only when the two synsets coincide.</p><p>Another problem that arises when comparing the similarity of two words is that the previously described method compares the distance between synsets (or concepts) and not words.A word can, and most of the time will, belong to more than one synset as we often encounter words with more than one meaning. A simple way to imagine this is to think of a thesaurus and when we search the meaning of a word we get numerous entries. Each of those entries represents a synset. Therefore, when we want to compare two words from a similarity point of view we actually need to first select the most appropriate synsets for each of them and compare those two synsets. Since it was not feasible to manually find the best matching pairs of synsets corresponding to a pair of words we picked the one that ranked the highest only. Therefore, the Wu-Palmer distance computed between two words w 1 and w 2 becomes</p><formula xml:id="formula_4" coords="7,172.19,626.98,308.39,9.65">d(w 1 , w 2 ) = max(d wup (s i , s j )), ∀i, j s.t. s i ∈ S(w 1 ), s j ∈ S(w 2 )<label>(4)</label></formula><p>where S(w 1 ) and S(w 2 ) are the set of synsets corresponding to w 1 and w 2 , respectively.</p><p>A different method of computing the similarities between two words involves projecting them in a word space model <ref type="bibr" coords="8,307.79,131.95,14.61,8.74" target="#b12">[13]</ref>. This representation is composed of sparse semantic bits and it is also known as Distributional Memory <ref type="bibr" coords="8,431.14,143.90,14.61,8.74" target="#b13">[14]</ref>. Active bits can be interpreted as firing neurons in an analogy to neural networks. For each term we get a semantic fingerprint and in order to measure the similarity between them one can compute the cosine distance between the two semantic fingerprints. This has been implemented in the current work with the use of the Retina API <ref type="bibr" coords="8,187.87,203.68,14.61,8.74" target="#b14">[15]</ref>. However, this did not give results as good as in the case of the previous method, as it is described in the Section 3.</p><p>So far, both similarity computation methods target pairs of words only, but what we needed for this work was to compute similarity scores between a set of concepts corresponding to a certain image and a set of words corresponding to a certain topic description summarization. In order to take into account both the similarity between words and the confidence of every concept we summed up all similarity measures, weighted by the confidence of the respective concept for each concept-word pair. Therefore, for a certain image i and a certain topic j the similarity score is computed as follows:</p><formula xml:id="formula_5" coords="8,236.81,333.84,243.78,20.14">score i,j = k l d(w k , w l ) × c k ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_6" coords="8,164.99,364.25,45.81,12.78">w k ∈ C (i)</formula><p>relevant , w l ∈ T (j) and c k is the confidence associated to w k . Thus, we obtain high scores when there is both a high confidence for the given concept and a strong similarity between it and at least one word from the topic description summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Similarity filtering</head><p>Finally, once we obtained the clusters and the similarity score between each image from the cluster and the topic description we select the best candidates for submitting a run. We remind that the task requested to submit a list of images for each topic representing images that are both relevant for the topic query and diverse. Therefore, we sorted the clusters in descending order based on the mean value of the similarity scores of the images that it contained. From the n-best ranked clusters we then selected the images with the highest similarity score and proposed them as the candidates for our run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results on development data</head><p>Our first approach was to compute the similarity score for each image-topic pair and just perform an ordering based on that score. Theoretically, images that contained objects in close connection, from a semantic point of view, with the topic description should be assigned a higher score. However, this was not exactly the case, as we saw from visual inspection of the highest ranked images, but for a few exceptions. We ran several tests to see how the data is distributed according to the similarity score. Knowing that the confidence scores are not evenly distributed among the images, as described in Section 2.1, we wanted to better understand the impact that low values of the confidence-similarity score products have on the final result. These values can also be considered noise added to the final score value. Therefore, we imposed several thresholds on this product. In other words, eq. 5 now becomes</p><formula xml:id="formula_7" coords="9,195.67,210.67,284.92,20.14">score i,j = k l d(w k , w l ) × c k × 1 [d(w k ,w l )×c k &gt;th] ,<label>(6)</label></formula><p>where th is an imposed threshold. Having obtained these results we plotted for each topic the confidence scores vs. the image's position number in the ordered shortlist only for the images from the ground truth that was provided by the organizers. In Fig. <ref type="figure" coords="9,219.91,272.99,4.98,8.74" target="#fig_2">3</ref> and<ref type="figure" coords="9,249.21,272.99,4.98,8.74" target="#fig_3">4</ref> we can see with different colors where the ground truth images are located in our system's output. Ideally, all the colored points should have been as close as possible to the left edge of the figure, namely they should have the highest confidence scores. We can see that the figures have a shape similar to a cotangent, but we would have desired for it to be steeper and concentrating more points on the left side. We can also observe that by imposing thresholds we get a steeper shape as it was desired. Unfortunately, the concentration of images with a high confidence score (on the left side of the figure) drops. This is valid for both Wu-Palmer and cosine distance between semantic fingerprints. We conclude it is better to not impose any threshold for this type of confidence score computation. An explanation to this is that a significant part of the noisy concepts have already been eliminated when we imposed the threshold from eq. 1. This appears to be enough to reduce the noise in the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on test data</head><p>However, the obtained results were not satisfactory so we needed to bring last minute modifications to our algorithm. We wanted to make use of the similarity score that we previously computed so the extension to the first approach came in the form of the filtering described in Section 2.6.</p><p>In more detail, we ran a hierarchical clustering algorithm on the shortlist of images corresponding to a certain topic and stopped it when it reached a total of 30 clusters. The next step was to sort these clusters according to the mean similarity score computed over the images belonging to each cluster. In order to submit 50 images for each topic we selected the 2 best ranked images from the first 25 clusters. The official results of the run can be seen in Fig. <ref type="figure" coords="10,420.78,572.79,4.98,8.74" target="#fig_4">5</ref> and the last value to be considered in comparing the runs submitted by the participants is F1@10, as described in <ref type="bibr" coords="10,237.99,596.70,9.96,8.74" target="#b4">[5]</ref>.</p><p>It is worth mentioning that the F1-measure at X was computed as the harmonic mean of the Precision at X and Cluster Recall at X. Our algorithm obtained various results depending on the topic, with the best values obtained for topics which contained an enumeration of items which may appear in a relevant picture or whose shortlist (Section 2.3) is very restrictive. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We presented the results of our run to the ImageCLEF 2017 Lifelog Summarization subtask. The submitted run yielded results that were not satisfactory as they were not as good as those of the other participants. However, this work proposes a system which does not need any additional or external data and it heavily relies on the outputs of systems that were not tested under these conditions. We were surprised to see that for certain topics our system performed quite well while for others it was completely unreliable, which lead to an overall weak performance. Good performances have been obtained for topics which contained an enumeration of items that can be present in the image to make it relevant. Such an example is the topic related to shopping where there is an enumeration of the types of stores that make an image relevant for that certain query. Moreover, a good match between these items and the objects with high confidence identified by the concept detector, such as butcher shop, candy store or toyshop, also proved vital for the outcome of our system.</p><p>We identified a large series of tunable parameters which can improve the results such as the threshold from eq. 1, the similarity score computation mechanism, the number of clusters for the hierarchical clustering etc. Another weak point was the lack of correlation between the concepts output by the Caffe Concept Detector and the meaning of the textual descriptions of the topics as it is a daunting task to evaluate similarity between phrases which address completely different subjects. Having a concept detector trained for the specific task of recognizing items that we can encounter on a daily basis would offer more precise information.</p><p>Moreover, making use of the entire metadata linked to the database could have improved the results. On this point we remind that the temporal information has not been used. It is clear that once an image is certain to fit a certain query then it is very likely that at least one of the neighboring images before and after is also a viable candidate for that specific query. Also, special annotations or interpretation of the topic descriptions could have aided the similarity computation system.</p><p>To sum up, we think that this system offers an interesting perspective on how a simple clustering algorithm can benefit from a textual interpretation. Further work can address finding the right set of parameters and better understanding the mixture of visual and textual interpretation of multimedia content.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,203.52,259.16,208.31,7.89;3,140.42,116.83,334.52,127.56"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. General diagram of the proposed algorithm.</figDesc><graphic coords="3,140.42,116.83,334.52,127.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,208.44,385.23,198.47,7.89;7,255.65,242.90,104.05,127.56"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Wu-Palmer similarity distances on a tree.</figDesc><graphic coords="7,255.65,242.90,104.05,127.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,150.19,589.23,314.97,7.89;9,134.77,362.42,345.84,212.04"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Similarity scores obtained with the Wu-Palmer distance for topic T10.</figDesc><graphic coords="9,134.77,362.42,345.84,212.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,160.87,337.45,293.63,7.89;10,134.77,116.83,345.82,205.85"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Similarity scores obtained with the cosine distance for topic T10.</figDesc><graphic coords="10,134.77,116.83,345.82,205.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,231.36,338.23,152.63,7.89;11,134.77,116.83,345.82,206.63"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. F1@X measure official results.</figDesc><graphic coords="11,134.77,116.83,345.82,206.63" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was funded under <rs type="grantName">UEFISCDI research</rs> grant <rs type="grantNumber">PN-III-P2-2.1-PED-2016-1065</rs>, agreement <rs type="grantNumber">30PED/2017</rs>, project <rs type="projectName">Real-time IP Camera-based Intelligent Video Surveillance Security System with DROP Retrieval -SPOTTER</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_47YRdev">
					<idno type="grant-number">PN-III-P2-2.1-PED-2016-1065</idno>
					<orgName type="grant-name">UEFISCDI research</orgName>
				</org>
				<org type="funded-project" xml:id="_HKJJh4S">
					<idno type="grant-number">30PED/2017</idno>
					<orgName type="project" subtype="full">Real-time IP Camera-based Intelligent Video Surveillance Security System with DROP Retrieval -SPOTTER</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,354.38,337.64,7.86;12,151.52,365.34,329.07,7.86;12,151.52,376.30,172.29,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,296.68,354.38,183.91,7.86;12,151.52,365.34,200.41,7.86">Outlines of a World Coming into Existence&apos;: Pervasive Computing and the Ethics of Forgetting</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rob</forename><surname>Kitchin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,359.72,365.34,120.88,7.86;12,151.52,376.30,82.54,7.86">Environment and Planning B: Planning and Design</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="445" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,386.77,337.63,7.86;12,151.52,397.73,297.96,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,382.01,386.77,98.57,7.86;12,151.52,397.73,16.38,7.86">Lifelogging: Personal big data</title>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aiden</forename><forename type="middle">R</forename><surname>Doherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,176.09,397.73,197.89,7.86">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="125" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,408.20,337.63,7.86;12,151.52,419.15,329.07,7.86;12,151.52,430.11,20.99,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,151.52,419.15,244.67,7.86">Information access tasks and evaluation for personal lifelogs</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liadh</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daragh</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,417.23,419.15,57.61,7.86">EVIA@NTCIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,440.58,337.63,7.86;12,151.52,451.54,155.49,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,151.52,451.54,127.57,7.86">Overview of ntcir-12 lifelog task</title>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hideo</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rami</forename><surname>Albatal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,462.01,337.63,7.86;12,151.52,472.97,329.07,7.86;12,151.52,483.93,329.07,7.86;12,151.52,494.88,329.07,7.86;12,151.52,505.84,35.40,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,238.81,472.97,241.78,7.86;12,151.52,483.93,58.97,7.86">Overview of ImageCLEFlifelog 2017: Lifelog Retrieval and Summarization</title>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giulia</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="12,238.96,483.93,241.63,7.86;12,151.52,494.88,31.91,7.86">CLEF 2017 Labs Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,516.31,337.63,7.86;12,151.52,527.27,329.07,7.86;12,151.52,538.23,329.07,7.86;12,151.52,549.19,329.07,7.86;12,151.52,560.15,329.07,7.86;12,151.52,571.11,329.07,7.86;12,151.52,582.06,329.07,7.86;12,151.52,593.02,220.88,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,432.25,549.19,48.34,7.86;12,151.52,560.15,217.22,7.86">Overview of ImageCLEF 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Helbert</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giulia</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bayzidul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josiane</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Immanuel</forename><surname>Schwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,388.22,560.15,92.37,7.86;12,151.52,571.11,329.07,7.86;12,151.52,582.06,72.80,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction 8th International Conference of the CLEF Association</title>
		<title level="s" coord="12,352.59,582.06,128.00,7.86;12,151.52,593.02,15.97,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>CLEF; Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-09-11">2017. September 11-14 2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,603.49,337.64,7.86;12,151.52,614.45,329.07,7.86;12,151.52,625.41,329.07,7.86;12,151.52,636.37,71.90,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,284.73,603.49,144.85,7.86">Verbs semantics and lexical selection</title>
		<author>
			<persName coords=""><forename type="first">Zhibiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,447.41,603.49,33.18,7.86;12,151.52,614.45,329.07,7.86;12,151.52,625.41,33.01,7.86">Proceedings of the 32Nd Annual Meeting on Association for Computational Linguistics, ACL &apos;94</title>
		<meeting>the 32Nd Annual Meeting on Association for Computational Linguistics, ACL &apos;94<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,646.84,337.63,7.86;12,151.52,657.79,296.00,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,326.73,646.84,153.86,7.86;12,151.52,657.79,86.47,7.86">VTIR at the NTCIR-12 2016 Lifelog Semantic Access Task</title>
		<author>
			<persName coords=""><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yufeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weiguo</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,257.90,657.79,100.47,7.86">Proceedings of NTCIR-12</title>
		<meeting>NTCIR-12<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,120.67,337.63,7.86;13,151.52,131.63,329.07,7.86;13,151.52,142.59,50.81,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,371.48,120.67,109.11,7.86;13,151.52,131.63,170.91,7.86">Image Searching by Events with Deep Learning for NTCIR-12 Lifelog</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P.-C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,343.56,131.63,101.31,7.86">Proceedings of NTCIR-12</title>
		<meeting>NTCIR-12<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,153.55,337.97,7.86;13,151.52,164.51,121.36,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,233.89,153.55,165.96,7.86">Wordnet: A lexical database for english</title>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,412.53,153.55,62.06,7.86">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995-11">November 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,175.46,337.98,8.12;13,151.52,186.42,84.33,7.86" xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Protogeo</forename><surname>Oy</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Moves</surname></persName>
		</author>
		<ptr target="https://moves-app.com/;ac-cessed30" />
		<imprint>
			<date type="published" when="2013-05">2013. May-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,197.38,337.97,7.86;13,151.52,208.34,329.07,7.86;13,151.52,219.30,329.07,7.86;13,151.52,230.26,283.76,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,278.23,197.38,202.36,7.86;13,151.52,208.34,14.75,7.86">Histograms of oriented gradients for human detection</title>
		<author>
			<persName coords=""><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,184.87,208.34,295.72,7.86;13,151.52,219.30,177.30,7.86;13,436.31,219.30,40.69,7.86">Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<meeting>the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
	<note>CVPR &apos;05</note>
</biblStruct>

<biblStruct coords="13,142.62,241.22,337.97,7.86;13,151.52,252.18,329.07,7.86;13,151.52,263.14,329.07,7.86;13,151.52,274.09,277.56,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,377.42,252.18,103.18,7.86;13,151.52,263.14,270.92,7.86">Syntagmatic and Paradigmatic Relations Between Words in High-dimensional Vector Spaces</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,320.94,241.22,159.65,7.86;13,151.52,252.18,223.24,7.86">Institutionen för lingvistik. The Word-Space Model: Using Distributional Analysis to Represent</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Stockholms universitet ; Department of Linguistics, Stockholm University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">SICS dissertation series</note>
</biblStruct>

<biblStruct coords="13,142.62,285.05,337.98,7.86;13,151.52,296.01,318.42,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,300.79,285.05,179.81,7.86;13,151.52,296.01,106.19,7.86">Distributional memory: A general framework for corpus-based semantics</title>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,265.93,296.01,71.81,7.86">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="721" />
			<date type="published" when="2010-12">December 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,306.97,337.97,8.12;13,151.52,317.93,44.28,7.86" xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="middle">Io</forename><surname>Cortical</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Retina</surname></persName>
		</author>
		<ptr target="http://www.cortical.io/" />
		<imprint>
			<date type="published" when="2017-05-30">30-May-2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
