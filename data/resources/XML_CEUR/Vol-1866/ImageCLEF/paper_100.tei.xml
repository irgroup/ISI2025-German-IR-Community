<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.90,152.67,301.59,12.64">ISIA at the ImageCLEF 2017 Image Caption Task</title>
				<funder ref="#_javZQyX">
					<orgName type="full">Beijing Municipal Commission of Science and Technology</orgName>
				</funder>
				<funder ref="#_sGcPzu8">
					<orgName type="full">of Eminent Professionals and National Program for Support of Top-notch Young Professionals</orgName>
				</funder>
				<funder ref="#_MjetQFF #_Nd4sNqb">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,157.10,191.70,38.87,8.96"><forename type="first">Sisi</forename><surname>Liang</surname></persName>
							<email>sisi.liang@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<addrLine>No.6 Kexueyuan South Road Zhongguancun, Haidian District</addrLine>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,202.78,191.70,53.96,8.96"><forename type="first">Xiangyang</forename><surname>Li</surname></persName>
							<email>xiangyang.li@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<addrLine>No.6 Kexueyuan South Road Zhongguancun, Haidian District</addrLine>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.05,191.70,56.34,8.96"><forename type="first">Yongqing</forename><surname>Zhu</surname></persName>
							<email>yongqing.zhu@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<addrLine>No.6 Kexueyuan South Road Zhongguancun, Haidian District</addrLine>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.56,191.70,26.70,8.96"><forename type="first">Xue</forename><surname>Li</surname></persName>
							<email>xue.li@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<addrLine>No.6 Kexueyuan South Road Zhongguancun, Haidian District</addrLine>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,376.44,191.70,61.32,8.96"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
							<email>shuqiang.jiang@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<addrLine>No.6 Kexueyuan South Road Zhongguancun, Haidian District</addrLine>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.90,152.67,301.59,12.64">ISIA at the ImageCLEF 2017 Image Caption Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4955F4348C616ED9D123B084ECF7A194</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural network</term>
					<term>Long Short-Term Memory</term>
					<term>Support Vector Machine</term>
					<term>Nearest Neighbor</term>
					<term>Image caption</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the details of our methods for participation in the caption prediction task of ImageCLEF 2017. The dataset we use is all provided by the organizers and doesn't include any external resources. The key components of our framework include a deep model part, an SVM part and a caption retrieval part. In deep model part, we use an end to end architecture with Convolutional neural network (CNN) and a Long Short-Term Memory (LSTM) to encode and decode images and captions. According to the statistics of training dataset, we train different models with different lengths of captions. Then in SVM part, we use Support Vector Machine (SVM) to determine which model to use when generating the description for a test image. In this way, we can combine these models from the previous deep model part. In caption retrieval part, we use the image feature extracted from CNN and apply Nearest Neighbor method to retrieve the most similar image with caption in the training dataset. The final description is the aggregation of the generated sentence and the caption retrieved from the training dataset. The best performance of our 10 submitted runs ranks the 3rd in group which doesn't use external resources.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past few years, there has been a huge interest in the task of automatically generating captions for images. It is interesting to see how a machine can solve this problem which is very easy to a person. Many progress <ref type="bibr" coords="1,362.89,572.75,11.06,8.96" target="#b1">[2,</ref><ref type="bibr" coords="1,377.92,572.75,7.52,8.96" target="#b3">4,</ref><ref type="bibr" coords="1,389.42,572.75,7.52,8.96" target="#b5">6,</ref><ref type="bibr" coords="1,400.79,572.75,12.55,8.96" target="#b10">11,</ref><ref type="bibr" coords="1,417.32,572.75,13.39,8.96" target="#b11">12]</ref> has been achieved after so many years endeavor and research.</p><p>There are three main approaches to generate image caption: one is using templates <ref type="bibr" coords="1,124.70,608.75,10.87,8.96" target="#b1">[2,</ref><ref type="bibr" coords="1,139.54,608.75,7.29,8.96" target="#b5">6]</ref>, these methods rely on detectors and map the output to linguistic structures. Another approach is using language models, like many widely used deep models <ref type="bibr" coords="1,454.60,620.75,15.90,8.96" target="#b10">[11,</ref><ref type="bibr" coords="1,124.70,632.75,11.94,8.96" target="#b11">12]</ref>. This method may yield more expressive captions because it can overcome the limitation of templates. Many deep learning architectures use Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNN) as language models. The third approach is caption retrieval and recombination <ref type="bibr" coords="1,340.03,668.78,10.89,8.96" target="#b1">[2,</ref><ref type="bibr" coords="1,353.59,668.78,7.17,8.96" target="#b3">4]</ref>. Instead of generating new captions, these methods retrieve captions based on training data.</p><p>The purpose of ImageCLEF 2017 caption task <ref type="bibr" coords="2,329.83,150.18,11.01,8.96" target="#b4">[5,</ref><ref type="bibr" coords="2,344.21,150.18,13.30,8.96" target="#b13">14]</ref> is interpreting and summarizing the insights gained from medical images. So the dataset is very different from previous datasets like MSCOCO <ref type="bibr" coords="2,262.71,174.18,11.74,8.96" target="#b6">[7]</ref> or Flick8K <ref type="bibr" coords="2,326.59,174.18,11.72,8.96" target="#b3">[4]</ref> and has its own characteristics. One is that almost half of the captions are more than 20 words and the longest caption reaches 606 words. Therefore it is a tough task to fully use the semantic information of data. Another is that some of images consist of several small images, like CT images from different perspectives, photos before and after treatment. It is hard to detect the internal relation of items in image and to reflect the change of small images.</p><p>Our method can be separated into three parts. The first part is deep model part. This part bases on deep model proposed by Vinyals <ref type="bibr" coords="2,337.59,258.18,15.45,8.96" target="#b10">[11]</ref>. The model is an end to end architecture using Convolutional Neural Network (CNN) for image encoding and Long Short-Term Memory (LSTM) based Recurrent Neural Network for sentence decoding. We divide the training dataset into three parts according to the length of captions and train three different models with different lengths. The second part is SVM part. In this part a three class SVM classifier is trained to determine which model to use in predicting a caption. The third part is caption retrieval part. We use the caption retrieval approach and apply Nearest Neighbor method to retrieve a most similar image. Then the caption of this image will be used as a supplement in the final generated caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>The main structure of our method can be seen in the Fig. <ref type="figure" coords="2,354.91,423.21,3.76,8.96">1</ref>. The structure is composed of three parts: deep model, SVM and caption retrieval part. Each part will be introduced in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Structure of our method</head><p>Preprocessing. In the preprocessing of data, we notice that training dataset is different from the other well-known Image Caption dataset <ref type="bibr" coords="3,342.39,162.18,10.87,8.96" target="#b3">[4,</ref><ref type="bibr" coords="3,355.90,162.18,7.32,8.96" target="#b6">7]</ref>. The training data is medical images and one caption for one image. Some captions in the training data are very long. The statistics of the sentence length in training dataset are shown in the Table <ref type="table" coords="3,462.88,186.18,3.92,8.96" target="#tab_0">1</ref>.</p><p>From Table <ref type="table" coords="3,175.32,198.18,4.98,8.96" target="#tab_0">1</ref> we can see that length of captions under 20 only accounts for 23.73%.</p><p>We implement experiments about the influence of sentence length. The result in Table <ref type="table" coords="3,124.70,222.18,4.98,8.96" target="#tab_3">4</ref> shows that combine different models of different sentence lengths can achieve better result. So we divide the dataset to three parts according to the length of captions.</p><p>The sentence length of each subset are 0-13, 13-30 and 30 over. For each subset of dataset, we train a different deep model based on different lengths. Sentences longer than the max length will be clipped to keep the sentence max length, the max length we use can be seen in the Table <ref type="table" coords="3,259.44,282.21,3.76,8.96" target="#tab_1">2</ref>. The reason we choose length 0-13, 13-30 and 30 over as the subsets is that each subset accounts for around 1/3 of the total sentences. Network (LSTM-RNN) for sentence encoding and decoding. We use a pre-trained VGGNet <ref type="bibr" coords="3,164.06,620.87,11.72,8.96" target="#b8">[9]</ref> for image feature extraction and each image will be transformed into a 4096-dimensional vector. Then we train a LSTM-RNN for encoding and decoding sentences. The LSTM-RNN implementation is based on the NeuralTalk2 1 project. As we divide the training data to three subsets in the preprocessing, we train three differ-ent deep models in training stage. For each deep model, the input training data is a subset of dataset after preprocessing and the max length we set are shown in Table <ref type="table" coords="4,463.20,162.18,3.71,8.96" target="#tab_1">2</ref>.</p><p>Other initial parameters are the same in all the three deep models.</p><p>Training SVM classifier. We try to use SVM classifier combine the three models together to generate captions of images. So in SVM part, we attempt to train a SVM classifier which can predict the three kinds of sentence length 0-13, 13-30 and 30 over. We use all the images from training dataset and extract image features from fc7 layer in VGGNet to train a three class SVM classifier. This SVM classifier will be used to determine which deep model to use in the prediction stage. The accuracy of this SVM classifier in predicting the validation data is shown in Table <ref type="table" coords="4,407.59,264.18,3.77,8.96" target="#tab_2">3</ref>. Caption retrieval: using Nearest Neighbor method. The performance of the model which only use deep model and SVM classifier is not optimal. Therefore, we attempt to use Nearest Neighbor method to retrieve the most similar image in caption retrieval part. If the Euclidean distance between the image CNN feature of predicted image and the image CNN feature of retrieved image is larger than a threshold, we will use the caption of retrieved image as a supplement in the final caption. The performance of model get an optimal when the threshold are 300 in CNN feature and 25 in normalized CNN features after many trials. In table 5, we can see that there is an improvement in the performance after applying the NN method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. An illustration of prediction</head><p>Prediction. As shown in Fig. <ref type="figure" coords="5,252.53,150.18,3.77,8.96">2</ref>, the input is an image and output is the generated sentences in the prediction stage. First, we extract the CNN feature from input image, then use the SVM classifier to determine which model to use. Next we use the trained deep model to generate a caption of this input image. Finally we apply the Nearest Neighbor method to retrieve a closest caption. If the Euclidean distance between image CNN feature of input image and image CNN feature of retrieved image is larger than a threshold, this caption will be used as a supplement of the caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Submitted Runs</head><p>The dataset we use in our method is all from the provided dataset. None of external datasets is used in our experiments. We first divide the ImageCLEF 2017 caption training data to three parts according the sentence length of training data captions.</p><p>Then we use the divided subset to train three different deep models based on the CNN and LSTM model. After that, a three classes SVM classifier is trained, using 4096 dimensions vector extracted from VGGNet fc7 (fully connected) layer as feature and using label 0, 1, 2 represent the three kinds of sentence length. Finally, we use NN (Nearest Neighbor) method to find the most similar image in training dataset. The feature is the same feature used in training the SVM classifier and it is normalize from 0 to 1. The distance function is the Euclidean distance. If the similarity is larger than a threshold, we will use this retrieved image caption as a supplement in final caption. Divide dataset with different sentence length. We conduct experiments to find out that whether the sentence length affects the final performance. Different sentence lengths and their performance in the validation data are shown in Table <ref type="table" coords="5,415.54,435.21,3.81,8.96" target="#tab_3">4</ref>. We use the BLEU <ref type="bibr" coords="5,154.91,447.21,10.70,8.96" target="#b7">[8]</ref>, METEOR <ref type="bibr" coords="5,218.64,447.21,10.73,8.96" target="#b0">[1]</ref>, ROUGE <ref type="bibr" coords="5,275.57,447.21,11.72,8.96" target="#b2">[3]</ref> and CIDEr <ref type="bibr" coords="5,340.93,447.21,16.70,8.96" target="#b9">[10]</ref> scores based on the cococaption code <ref type="bibr" coords="5,179.75,459.21,16.70,8.96" target="#b12">[13]</ref> <ref type="foot" coords="5,196.37,457.02,3.24,5.83" target="#foot_1">2</ref> when measure the performance of different models. All the training dataset are divided into different subsets with different sentence length in the preprocessing. When sentence length are 20, 45 and 60, we only use the deep model with CNN+LSTM to generate captions. SVM_two and SVM_three are models which use both deep model and SVM classifier. The results demonstrate that the length of sentence has a significant impact on performance. Training three different sentence length models and using SVM classifier can result in better performance. Caption retrieval: using Nearest Neighbor method. We notice that although using SVM has some improvement in performance, the deep model cannot achieve an optimal result. So we conduct another experiment to explore whether using Nearest Neighbor method to retrieve image caption can help improve the performance. We use normalized features and non-normalized features for NN method to compare the performance. And the results are shown in Table <ref type="table" coords="6,365.62,229.74,4.62,8.96" target="#tab_4">5</ref>.The results demonstrate that adding the retrieved caption can lead to a better result and the result will be further improved when using normalized features. test_5_svm_nn_dist_3000_nounk_modified_2: use two classes SVM classifier and NN method. In NN method, we didn't use normalized features and threshold is 3000. As mention before, this run also removes UNK.</p><p>test_12_svm_3_nn_dist_25_normal: use three classes SVM classifier and NN method. We use normalized features and threshold is 25 when applied NN method. The difference is that this run didn't remove UNK. We have submitted ten runs in the caption prediction subtask and the performances are shown in the Table <ref type="table" coords="7,221.42,330.21,3.76,8.96" target="#tab_5">6</ref>. The performance of the best run is 0.2600 in Mean BLEU score. Compared to other runs which doesn't including SVM classifier or caption retrieval, the performance has been greatly improved. As shown in the Fig. <ref type="figure" coords="8,219.19,150.18,3.87,8.96" target="#fig_1">3</ref>, the model can generate different lengths of captions according to the picture. The first example shows that the caption retrieved by NN method is similar to the caption generated by deep model and is also relate to the ground truth. Besides, the image of the first example contains images A and B. We are delighted to see the model can learn the pattern and generate a caption include alphabetic number and information in the two images. The third example shows that the distance between input image and the similar image retrieved using Nearest Neighborhood method is far from threshold, so the retrieved caption will not be used in the final caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we describe our method in ImageCLEF 2017 caption prediction subtask. We use statistics of training dataset and divide the training dataset into three parts. Then in the training stage, we train three deep learning models and use CNN and LSTM to generate natural language sentences. A three classes SVM classifier is trained at the same time to determine which deep model to use when predicting image caption. Besides, Nearest Neighbor method is also applied to retrieve a similar image and its caption in the training data as a supplement in final caption. After performing the experiments above, we get the following conclusions. Firstly, the sentence length parameter in training can affect the performance. By training separately models and using SVM classifier, we achieve a better result compared to the model only use CNN+LSTM. Secondly, the similar image can provide useful information in caption.</p><p>After applying the Nearest Neighbor method to retrieve a similar image and caption, the performance of the model can be greatly improved. However, we limit the sentence length and remove the words that only appear one or two times during the training. The removed words in the training dataset cannot be made full use of. This makes some generated captions lack of readability. In addition, the generated language model is too simple to generate a complex and fully descriptive caption. Compared with other participants in this task, the best performance of our 10 submitted runs is 0.2600 in Mean BLEU score, ranks the 3rd in group which doesn't use external resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,136.10,150.18,334.30,8.96;7,124.70,162.18,345.94,8.96;7,124.70,174.18,15.69,8.96;7,136.10,186.18,334.60,8.96;7,124.70,198.18,266.30,8.96;7,136.10,210.18,334.28,8.96;7,124.70,222.08,346.01,9.06;7,124.70,234.18,70.72,8.96;7,136.10,246.18,334.51,8.96;7,124.70,258.18,27.33,8.96;7,136.10,270.18,313.50,8.96;7,136.10,282.21,208.21,8.96;7,136.10,294.21,334.58,8.96;7,124.70,306.21,70.72,8.96"><head></head><label></label><figDesc>test_11_svm_2_nn_dist_25_normal_noUNK: use two classes SVM classifier and NN method. Normalized the image feature and threshold is 25. UNK is remain in this run. test_10_svm_2_nn_dist_25_normal: use two classes SVM classifier and NN method. Use normalized features and threshold is 25. UNK is removed. test_9_svm_three_nn_3000_noUNK: use three classes SVM classifier and NN method. In NN method, we didn't use normalized features and threshold is 3000. UNK is removed. test_6_svm_three_parts: only use three lengths classes SVM classifier and deep model. test_2_svm_two: only use two lengths classes SVM classifier and deep model. test_1_wc5sl70: only use CNN+LSTM deep model. test_8_svm_two_remove_UNK: only use two lengths classes SVM classifier and UNK is removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,124.70,649.79,345.74,8.96;7,124.70,662.47,218.54,8.10;7,131.89,363.40,331.42,276.91"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Some examples of generated caption and ground truth from validation dataset. The blue sentence is the caption using Nearest Neighbor method.</figDesc><graphic coords="7,131.89,363.40,331.42,276.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,135.85,456.50,340.99,189.96"><head></head><label></label><figDesc></figDesc><graphic coords="2,135.85,456.50,340.99,189.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,124.70,506.95,345.90,146.40"><head></head><label></label><figDesc></figDesc><graphic coords="4,124.70,506.95,345.90,146.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,183.50,318.02,228.08,160.76"><head>Table 1 .</head><label>1</label><figDesc>The statistics of the sentence length in training dataset</figDesc><table coords="3,197.57,337.58,200.31,141.20"><row><cell cols="2">Length of sentences Percentage of the total sentences</cell></row><row><cell>0-10</cell><cell>23.73%</cell></row><row><cell>10-20</cell><cell>26.51%</cell></row><row><cell>20-30</cell><cell>15.98%</cell></row><row><cell>30-40</cell><cell>10.16%</cell></row><row><cell>40-50</cell><cell>6.88%</cell></row><row><cell>50-60</cell><cell>4.68%</cell></row><row><cell>60-70</cell><cell>3.31%</cell></row><row><cell>70-80</cell><cell>2.41%</cell></row><row><cell>80-90</cell><cell>1.75%</cell></row><row><cell>90-100</cell><cell>1.28%</cell></row><row><cell>100 over</cell><cell>3.31%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,124.70,495.04,345.90,110.79"><head>Table 2 .</head><label>2</label><figDesc>Max sentence length in models</figDesc><table coords="3,226.73,514.60,142.01,47.34"><row><cell cols="2">Data sentence length Max length used</cell></row><row><cell>1-13</cell><cell>13</cell></row><row><cell>13-30</cell><cell>30</cell></row><row><cell>30-606</cell><cell>100</cell></row></table><note coords="3,124.70,584.87,345.80,8.96;3,124.70,596.87,345.90,8.96"><p>Training deep model. Deep model contain the following parts: Convolutional Neural Network for image encoding, Long-Short Term Memory based Recurrent Neural</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,198.41,288.02,195.22,84.18"><head>Table 3 .</head><label>3</label><figDesc>The accuracy of three class SVM classifier</figDesc><table coords="4,198.41,307.58,195.22,64.62"><row><cell cols="5">Length Precision Recall F1-score Support</cell></row><row><cell>1-13</cell><cell>0.62</cell><cell>0.61</cell><cell>0.62</cell><cell>3343</cell></row><row><cell>13-30</cell><cell>0.44</cell><cell>0.47</cell><cell>0.45</cell><cell>3047</cell></row><row><cell>30-606</cell><cell>0.71</cell><cell>0.68</cell><cell>0.70</cell><cell>3610</cell></row><row><cell>Avg/Total</cell><cell>0.60</cell><cell>0.59</cell><cell>0.60</cell><cell>10000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,126.26,555.04,342.71,112.29"><head>Table 4 .</head><label>4</label><figDesc>Performance of different sentence length models</figDesc><table coords="5,126.26,574.60,342.71,92.73"><row><cell>Sentence length</cell><cell>BLEU -1</cell><cell>BLEU -2</cell><cell>BLEU -3</cell><cell>BLEU -4</cell><cell>METEO R</cell><cell>ROUGE_ L</cell><cell>CI-DEr</cell></row><row><cell>20</cell><cell>0.042</cell><cell>0.018</cell><cell>0.007</cell><cell>0.003</cell><cell>0.028</cell><cell>0.088</cell><cell>0.032</cell></row><row><cell>45</cell><cell>0.047</cell><cell>0.022</cell><cell>0.009</cell><cell>0.004</cell><cell>0.033</cell><cell>0.105</cell><cell>0.034</cell></row><row><cell>60</cell><cell>0.058</cell><cell>0.028</cell><cell>0.013</cell><cell>0.006</cell><cell>0.036</cell><cell>0.110</cell><cell>0.055</cell></row><row><cell>SVM_two(length =20,45)</cell><cell>0.098</cell><cell>0.045</cell><cell>0.020</cell><cell>0.009</cell><cell>0.040</cell><cell>0.107</cell><cell>0.044</cell></row><row><cell>SVM_three(lengt</cell><cell>0.134</cell><cell>0.061</cell><cell>0.026</cell><cell>0.012</cell><cell>0.043</cell><cell>0.113</cell><cell>0.053</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,125.30,277.58,345.31,107.94"><head>Table 5 .</head><label>5</label><figDesc>The performance with and without Nearest Neighbor method</figDesc><table coords="6,125.30,297.14,345.31,88.38"><row><cell>Sentence length</cell><cell>BLEU -1</cell><cell>BLEU -2</cell><cell>BLEU -3</cell><cell>BLEU -4</cell><cell>METE OR</cell><cell>ROUGE _L</cell><cell>CIDEr</cell></row><row><cell>SVM_three</cell><cell>0.134</cell><cell>0.061</cell><cell>0.026</cell><cell>0.012</cell><cell>0.043</cell><cell>0.113</cell><cell>0.053</cell></row><row><cell>Nn+SVM_three(thresh &lt;3000)</cell><cell>0.144</cell><cell>0.068</cell><cell>0.035</cell><cell>0.020</cell><cell>0.058</cell><cell>0.127</cell><cell>0.049</cell></row><row><cell>Nn_normalized+SVM _three (thresh&lt;25)</cell><cell>0.171</cell><cell>0.099</cell><cell>0.065</cell><cell>0.047</cell><cell>0.077</cell><cell>0.144</cell><cell>0.095</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,124.70,403.94,345.77,203.69"><head>Table 6 .</head><label>6</label><figDesc>The performance of submitted runs In NN method, we use normalized features and threshold is 25. Besides, we remove the UNK which is used to represent those word account didn't achieve five times in training dataset.</figDesc><table coords="6,124.70,423.50,345.77,160.13"><row><cell>Submitted runs</cell><cell>Mean BLEU score</cell></row><row><cell>test_13_svm_3_nn_dist_25_normal_noUNK</cell><cell>0.2600</cell></row><row><cell>test_5_svm_nn_dist_3000_nounk_modified_2</cell><cell>0.2507</cell></row><row><cell>test_12_svm_3_nn_dist_25_normal</cell><cell>0.2454</cell></row><row><cell>test_11_svm_2_nn_dist_25_normal_noUNK</cell><cell>0.2386</cell></row><row><cell>test_10_svm_2_nn_dist_25_normal</cell><cell>0.2315</cell></row><row><cell>test_9_svm_three_nn_3000_noUNK</cell><cell>0.2240</cell></row><row><cell>test_6_svm_three_parts</cell><cell>0.2193</cell></row><row><cell>test_2_svm_two</cell><cell>0.1953</cell></row><row><cell>test_1_wc5sl70</cell><cell>0.1912</cell></row><row><cell>test_8_svm_two_remove_UNK</cell><cell>0.1684</cell></row><row><cell cols="2">test_13_svm_3_nn_dist_25_normal_noUNK: use three classes SVM classifier and</cell></row><row><cell>NN method.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,129.98,686.23,142.51,8.10"><p>https://github.com/karpathy/neuraltalk2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,136.10,686.23,133.87,8.10"><p>https://github.com/tylin/coco-caption</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">61532018</rs> and <rs type="grantNumber">61322212</rs>, in part by the <rs type="funder">Beijing Municipal Commission of Science and Technology</rs> under Grant <rs type="grantNumber">D161100001816001</rs>, in part by the <rs type="programName">Lenovo Outstanding Young Scientists Program</rs>, in part by <rs type="programName">National Program for Special Support</rs> <rs type="funder">of Eminent Professionals and National Program for Support of Top-notch Young Professionals</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MjetQFF">
					<idno type="grant-number">61532018</idno>
				</org>
				<org type="funding" xml:id="_Nd4sNqb">
					<idno type="grant-number">61322212</idno>
				</org>
				<org type="funding" xml:id="_javZQyX">
					<idno type="grant-number">D161100001816001</idno>
					<orgName type="program" subtype="full">Lenovo Outstanding Young Scientists Program</orgName>
				</org>
				<org type="funding" xml:id="_sGcPzu8">
					<orgName type="program" subtype="full">National Program for Special Support</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,132.67,176.99,338.00,8.10;9,141.74,188.03,328.94,8.10;9,141.74,199.07,24.09,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,242.66,176.99,228.01,8.10;9,141.74,188.03,70.92,8.10">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,233.31,188.03,184.12,8.10">The Workshop on Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,209.99,338.07,8.10;9,141.74,221.03,329.12,8.10;9,141.74,232.07,169.07,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,175.52,221.03,220.63,8.10">Every picture tells a story: generating sentences from images</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,415.44,221.03,55.42,8.10;9,141.74,232.07,100.74,8.10">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,242.99,337.95,8.10;9,141.74,254.03,173.94,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,178.28,242.99,207.60,8.10">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Flick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,405.10,242.99,65.52,8.10;9,141.74,254.03,123.49,8.10">The Workshop on Text Summarization Branches Out</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,265.07,338.07,8.10;9,141.74,275.99,231.84,8.10" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,293.83,265.07,176.91,8.10;9,141.74,275.99,119.28,8.10">Framing image description as a ranking task: data, models and evaluation metrics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>AI Access Foundation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,286.97,337.86,8.19;9,141.74,298.10,328.84,8.10;9,141.74,309.02,328.48,8.10;9,141.74,320.06,328.88,8.10;9,141.74,331.10,328.84,8.10;9,141.74,342.02,329.00,8.10;9,141.74,353.06,32.36,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,355.63,309.02,114.60,8.10;9,141.74,320.06,128.01,8.10">Overview of ImageCLEF 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">,</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,289.06,320.06,181.56,8.10;9,141.74,331.10,306.17,8.10">Experimental IR Meets Multilinguality, Multimodality, and Interaction 8th International Conference of the CLEF Association, CLEF</title>
		<title level="s" coord="9,141.74,342.02,128.36,8.10">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-09-11">2017. September 11-14 2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,364.10,337.33,8.10;9,141.74,375.02,328.88,8.10;9,141.74,386.06,218.87,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,185.86,375.02,244.34,8.10">Babytalk: understanding and generating simple image descriptions</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,450.22,375.02,20.40,8.10;9,141.74,386.06,132.44,8.10">Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="1601" to="1608" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,397.10,338.01,8.10;9,141.74,408.02,262.08,8.10" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,162.47,408.02,158.59,8.10">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,419.06,337.95,8.10;9,141.74,430.10,328.61,8.10;9,141.74,441.02,71.35,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,330.60,419.06,140.02,8.10;9,141.74,430.10,103.32,8.10">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,265.34,430.10,201.51,8.10">Meeting on Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,452.06,337.83,8.10;9,141.74,463.10,152.73,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,262.75,452.06,207.74,8.10;9,141.74,463.10,54.38,8.10">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,202.02,463.10,66.12,8.10">Computer Science</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.40,474.04,338.22,8.10;9,141.74,485.08,167.27,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,288.40,474.04,182.22,8.10;9,141.74,485.08,13.02,8.10">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,160.30,485.08,66.24,8.10">Computer Science</title>
		<imprint>
			<biblScope unit="page" from="4566" to="4575" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.40,496.12,338.22,8.10;9,141.74,507.04,132.83,8.10" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="9,331.72,496.12,138.90,8.10;9,141.74,507.04,50.23,8.10">Show and tell: A neural image caption generator</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.40,518.08,337.80,8.10;9,141.74,529.12,329.00,8.10;9,141.74,540.04,131.37,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,141.74,529.12,302.39,8.10">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,450.27,529.12,20.47,8.10;9,141.74,540.04,48.85,8.10">Computer Science</title>
		<imprint>
			<biblScope unit="page" from="2048" to="2057" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.40,551.08,337.77,8.10;9,141.74,562.12,285.14,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,435.50,551.08,34.67,8.10;9,141.74,562.12,186.58,8.10">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,334.06,562.12,66.24,8.10">Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.40,572.95,338.10,8.19;9,141.74,584.08,328.88,8.10;9,141.74,595.12,329.00,8.10;9,141.74,606.04,252.01,8.10" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,395.43,573.04,75.07,8.10;9,141.74,584.08,328.88,8.10;9,141.74,595.12,6.47,8.10">Overview of Image-CLEFcaption 2017 -image caption prediction and concept detection for biomed-ical images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">,</forename><surname>Seco De Errera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename></persName>
		</author>
		<ptr target="://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="9,175.19,595.12,295.54,8.10;9,141.74,606.04,42.53,8.10">CLEF 2017 Labs Working Notes. CEUR Workshop Proceedings,CEUR-WS.org&lt;http</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
