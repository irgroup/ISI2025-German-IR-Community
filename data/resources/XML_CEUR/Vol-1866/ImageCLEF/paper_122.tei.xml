<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,125.09,152.87,345.08,12.43;1,242.26,170.87,110.58,12.43">Concept detection on medical images using Deep Residual Learning Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,213.43,209.30,66.01,9.16"><forename type="first">Katsios</forename><surname>Dimitris</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Information and Communication Systems Engineering</orgName>
								<orgName type="institution">University of the Aegean</orgName>
								<address>
									<addrLine>Sa-mos</addrLine>
									<postCode>83200</postCode>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.79,209.30,83.35,9.16"><forename type="first">Kavallieratou</forename><surname>Ergina</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Information and Communication Systems Engineering</orgName>
								<orgName type="institution">University of the Aegean</orgName>
								<address>
									<addrLine>Sa-mos</addrLine>
									<postCode>83200</postCode>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,125.09,152.87,345.08,12.43;1,242.26,170.87,110.58,12.43">Concept detection on medical images using Deep Residual Learning Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E3BF5BC204AF5B090A2F4F416D804904</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image retrieval</term>
					<term>Concept Detection</term>
					<term>Residual Neural Networks</term>
					<term>Medical Images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical images are often used in clinical diagnosis. However, interpreting the insights gained from them is often a time-consuming task even for experts. For this reason, there is a need for methods that can automatically approximate the mapping from medical images to condensed textual descriptions. For identifying the presence of relevant biomedical concepts in medical images for the ImageCLEF 2017 Caption concept detection subtask we propose the use of a pretrained residual deep neural network. Specifically, a 50-layered resNet was used and retrained on the medical images. The proposed method achieved F1 Score 0.1583 on the test data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Concept detection determines whether an image is relevant to a specific concept. A concept of that type ranges from simple objects (e.g. desk, car) to events (people swimming) or scenes (lecture, sky). Concept detection is considered a challenging task especially in the presence of occlusion, background clutter, intra-class variation, pose and lighting changes in images <ref type="bibr" coords="1,237.55,508.67,10.82,9.16" target="#b0">[1]</ref>. Nevertheless, apart from its difficulty it is extremely helpful in tasks like image retrieval where one needs the most relevant images to some concepts from a set of images. While concept detection is not a classification task, it can be solved as one. Semantic concepts can serve as good intermediate semantic metadata for video content indexing and understanding <ref type="bibr" coords="1,256.05,568.67,10.63,9.16" target="#b1">[2]</ref>. Applications of image or video retrieval based on concepts can be met at search engines like google search, social networks like Facebook and content sharing websites like YouTube and Flickr <ref type="bibr" coords="1,345.26,592.70,10.53,9.16" target="#b2">[3]</ref>. A prerequisite for effective image and video search is to analyze and index media content automatically. Establishing a large set of robust concept detectors will yield significant improvements in many challenging applications, such as image/video search and summarization <ref type="bibr" coords="1,417.90,628.70,10.80,9.16" target="#b3">[4]</ref>.</p><p>Concept detection is commonly viewed as a supervised machine learning problem which aims to learn the mapping function between low-level visual features and highlevel semantic concepts based on the annotated training data <ref type="bibr" coords="1,370.23,664.72,10.88,9.16" target="#b4">[5]</ref>. Even if computer vision techniques can solve some of the major problems mentioned above, intra-class variation is one of the most difficult to deal with <ref type="bibr" coords="2,316.83,150.02,10.77,9.16" target="#b5">[6]</ref>. Collecting large scale training data to cover a wide variety of samples might be a promising solution since studies on concept detections <ref type="bibr" coords="2,187.94,174.02,11.81,9.16" target="#b6">[7]</ref> and pedestrian classification <ref type="bibr" coords="2,319.80,174.02,10.68,9.16" target="#b7">[8,</ref><ref type="bibr" coords="2,333.48,174.02,8.40,9.16" target="#b8">9]</ref> indicate that data matters most, since the amount of available data impacts to the accuracy of a classification model more than other parameters.</p><p>Apart from present applications that are based on concept detection, there are many future goals that could be matched by this technique. One goal would be the automatic clinical diagnosis based on patient images. To achieve it one should be able to detect which medical concepts are present at each image and then combine this knowledge to extract a patient status description in natural language. This would be very helpful since diagnosis is a time-consuming task even for highly trained experts.</p><p>One of the most popular approaches in this domain is the Bag of Words (BoW) which transforms local image descriptors into image representations <ref type="bibr" coords="2,398.78,294.04,15.96,9.16" target="#b9">[10,</ref><ref type="bibr" coords="2,417.01,294.04,12.36,9.16" target="#b10">11,</ref><ref type="bibr" coords="2,431.88,294.04,11.82,9.16" target="#b11">12]</ref>. BoW image representation is analogous to the BoW representation for text documents which means that techniques from the second can be applied to semantic concept detection. These models extract local descriptors from images, embed them to a visual vocabulary space and compute statistics based on the occurrences of each visual word in the image while some models use co-occurrences of (visual) words or other higher-order occurrence pooling. Some standard BoW methods are Local Coordinate Coding <ref type="bibr" coords="2,422.56,366.07,15.47,9.16" target="#b12">[13]</ref>, Sparse Coding <ref type="bibr" coords="2,157.23,378.07,15.76,9.16" target="#b13">[14,</ref><ref type="bibr" coords="2,175.74,378.07,11.80,9.16" target="#b14">15]</ref>, Approximate Locality-constrained Linear Coding <ref type="bibr" coords="2,396.28,378.07,15.36,9.16" target="#b15">[16]</ref>, Approximate Locality-constrained Soft Assignment <ref type="bibr" coords="2,280.29,390.07,15.86,9.16" target="#b16">[17,</ref><ref type="bibr" coords="2,299.14,390.07,13.21,9.16" target="#b17">18]</ref> and Soft Assignment and Visual Word Uncertainty <ref type="bibr" coords="2,175.68,402.09,16.03,9.16" target="#b18">[19,</ref><ref type="bibr" coords="2,195.18,402.09,12.60,9.16" target="#b19">20,</ref><ref type="bibr" coords="2,211.49,402.09,11.83,9.16" target="#b20">21]</ref>. There is another group of approaches which use more advanced techniques like Super Vector Coding <ref type="bibr" coords="2,312.05,414.09,15.32,9.16" target="#b21">[22]</ref>, Fisher Vector Encoding <ref type="bibr" coords="2,435.62,414.09,15.86,9.16" target="#b22">[23,</ref><ref type="bibr" coords="2,454.95,414.09,11.99,9.16" target="#b23">24]</ref>, Vector of Locally Aggregated Descriptors <ref type="bibr" coords="2,293.71,426.09,15.40,9.16" target="#b24">[25]</ref>, and Vector of Locally Aggregated Tensors <ref type="bibr" coords="2,143.55,438.09,15.30,9.16" target="#b25">[26]</ref>. One important feature in BoW is the representation choice.</p><p>Some representations are related to text categorization techniques, like stop word removal, word weighting scheme, visual bigram and feature selection, while the others are unique to concept detection in videos or images, like spatial information of the key points or vocabulary size (number of keypoint clusters). Research is mainly focused on finding better keypoint descriptors, keypoint detectors and clustering algorithms <ref type="bibr" coords="2,454.57,498.11,16.36,9.16" target="#b26">[27,</ref><ref type="bibr" coords="2,124.85,510.11,13.44,9.16" target="#b27">28]</ref> or what representation choices (weighting, selection, w.r.t dimension) have better accuracy and efficiency. Some methods <ref type="bibr" coords="2,285.44,522.11,15.76,9.16" target="#b28">[29,</ref><ref type="bibr" coords="2,303.72,522.11,13.20,9.16" target="#b29">30]</ref> use different keypoint sampling methods including sparse detectors as Boosted ColorHarris Laplace as well as keypoint descriptors like SIFT and HueSIFT. Geometric blur features is another way some methods used local features as keypoint descriptor for concept detection <ref type="bibr" coords="2,379.77,558.11,15.31,9.16" target="#b30">[31]</ref>. Dense and sparse representation with grid-based local image patches at the first category and keypoints at the second were compared <ref type="bibr" coords="2,244.30,582.14,16.68,9.16" target="#b31">[32]</ref> based on different sampling strategies of BoW with the results indicating that randomly sampled image patches offer better representation characteristics.</p><p>Apart from BoW new methods have been developed to improve concept detection accuracy like Deep Convolutional Neural Networks <ref type="bibr" coords="2,352.41,630.14,15.31,9.16" target="#b32">[33]</ref>. This has been mainly achieved due to the available pool of features which in recent years has increased rapidly. Deep Convolutional Neural Networks (CNNs) can be combined with other visual descriptors to improve its performance <ref type="bibr" coords="2,284.81,666.16,15.41,9.16" target="#b33">[34]</ref>, which is overall better than previous ap-proaches <ref type="bibr" coords="3,163.46,150.02,15.31,9.16" target="#b34">[35]</ref>. One of the most recently developed architectures of Deep Neural Networks (NNs) is the Deep Residual Learning Network. Deep Residual NN is a network that was developed by researchers from Microsoft Research which received first place in ImageNet Large Scale Visual Recognition Competition (ILSVRC) 2015 image classification. The network that they used had 152 layers, 8 times deeper than a comparable Visual Geometry Group (VGG) network but still having lower complexity. This was the first of many recent applications of Residual Networks. However, this type of networks has not yet been widely implemented at the image concept detection domain. Microsoft Research team developed a variety of Residual Networks, one of which was used in our approach. Specifically, we used the 50-layered network that they developed with some modifications. Due to the nature of the ImageCLEF 2017 Concept Detection subtask, the network must be implemented on multilabeled data, so that for each medical image, more than one label can be assigned. This differs from the usual classification problem where each image belongs to only one class. For this reason, the network output must be not a scalar but a 20,464-long vector, one for each one of the 20,464 potential labels.</p><p>In section 2 the proposed technique is presented in detail, while in section 3 experimental and comparison results are given. Finally, in section 4, our conclusion is drawn and some ideas for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed System</head><p>Training images included a very large variety of situations, content and types, from radiology X-rays and clinical photographs to charts and equipment images. Fig. <ref type="figure" coords="3,451.85,435.21,5.04,9.16">1</ref> includes some images from training and validation datasets where one can observe the wide variety. For this reason, context specific descriptors might be difficult to be defined. A neural network with large enough depth might be more suitable for managing this variety and for that reason it was selected as the main method. Residual neural networks are networks that due to its layer modules and connectivity can go deeper while avoiding the degradation problem. Fig. <ref type="figure" coords="3,307.56,507.23,5.04,9.16">2</ref> shows the building block of such a network. Residual neural networks were used in the past for image classification with very promising results. Also, according to many researchers like <ref type="bibr" coords="3,373.80,531.23,15.74,9.16" target="#b35">[36,</ref><ref type="bibr" coords="3,393.26,531.23,13.20,9.16" target="#b36">37]</ref> the use of pretrained networks even from a quite different topic is better as starting point. Pretrained networks are networks that have been trained on another task and then the layers with their weights can be used as starting point for a new task.</p><p>To train a Deep NN, one can use a framework like Caffe, Torch, Theano, Tensor-Flow etc. which provides utilities in terms of develop, change, tune, train and test networks of different architectures. Each framework supports specific data types not only for the network description but for the training and test data as well. Since knowledge transfer is one of the most useful concepts in machine learning generally and in deep NNs specifically, networks that were not only suitable in terms of architecture but also pretrained on a similar task were searched. Residual Neural Networks have been proved to be able to achieve much better performance in general image classification <ref type="bibr" coords="3,435.77,663.28,15.26,9.16" target="#b37">[38]</ref>. For this reason, it was one of the network types that was tried for this task. Also, an attempt was made to develop and train some networks from scratch. The one with the best performance was ResNet-50 <ref type="bibr" coords="4,296.03,647.18,16.56,9.16" target="#b40">[41]</ref> as described in <ref type="bibr" coords="4,377.82,647.18,15.32,9.16" target="#b37">[38]</ref>. This network was developed by Microsoft Research Asia (MSRA) and trained on ImageNet and COCO datasets (2015) for the corresponding competitions. Both training sets had images and labels of "general content" meaning not targeted to a specific domain, in contrast to the ImageCLEF caption task which included medical-oriented images and labels.</p><p>The three basic components of a network are its architecture, the weights and the parameters. At Caffe framework <ref type="bibr" coords="5,260.50,186.02,15.26,9.16" target="#b41">[42]</ref>, there is a clear separation between these three even at data level. For each layer of the network that was not modified, the initial weights were kept as is, since knowledge transfer was the main purpose of using pretrained networks. The training parameters were tuned for optimal performance and best accuracy of the network. As for the net architecture, some modifications were necessary. The original network was trained on simple labeled images, which means that to each (part of) image corresponded (at most) one label. In contrast, for each medical image of ImageCLEF caption, more than one label could be matched, which means that it is a multilabeled task. Because of this difference, the file format of the inputs of the network had to change. In most cases the network inputs for the training phase consist of the image and the corresponding label. However, in our case the input should be the medical image and a list of zero or more labels. Caffe framework supports this type of inputs based on a specific file format, named HDF5. HDF5 allows us to handle the existence of more than one labels for each training image. The data file transformation took place at batches of 500 images with the standard python library h5py. As result 330 such files were created and used as input files for the network.</p><p>This transformation led to changes at the first (input) layer of the net. Another change that had to be done was at the last two layers, the fully connected penultimate layer and the loss layer. Specifically, the fully connected layer should have 20,464 outputs, one for each potential label. These potential labels are the ones extracted from the training set. These outputs are the inputs of the last layer, the loss function layer. In most of the cases a SoftMax layer is used to give the possibility of each label to match the specific image and the one with the maximum likelihood is selected for the loss computation. However, in our case the loss function should be able to handle more than one labels. For this another change took place at the original network. Summarizing, the changes over the original ResNet-50 <ref type="bibr" coords="5,238.90,486.11,16.80,9.16" target="#b40">[41]</ref> network are:</p><p>ÔÇ∑ the type of the first layer changed to HDF5Data with batch size 2 (see Fig. <ref type="figure" coords="5,436.73,506.27,4.20,9.16">3</ref>) ÔÇ∑ the number of outputs of the last inner product layer changed to 20.464 which is the number of different labels as extracted from the training data (see Fig. <ref type="figure" coords="5,419.21,530.51,4.20,9.16">4</ref>) ÔÇ∑ a new SigmoidCrossEntropyLoss layer was added as the final layer of the network with bottom layers the aforementioned inner product layer and the label layer. This change took place due to the necessity of computing the loss function for multiple outputs each time and not just for the best fitted label (see Fig. <ref type="figure" coords="5,388.46,578.78,3.64,9.16">4</ref>). The final network architecture is shown in Fig. <ref type="figure" coords="6,315.24,281.08,3.78,9.16" target="#fig_2">5</ref>.</p><p>After the network configuration, the training phase took place. In order to optimize the network both in terms of time/resources consumption and results accuracy, proper hyper-parameters had to be set. Some well-known ranges for parameterizing deep nets are momentum ~ 0,9 and weight decay ~ 0,0005 <ref type="bibr" coords="6,326.52,329.11,15.26,9.16" target="#b42">[43]</ref>. Of course, the most important set of parameters has to do with the learning procedure. Specifically, one can select from predefined learning policies like fixed, inv, step, multistep, poly etc. Each learning rate policy decreases the learning rate as the learning process progresses in a different way. For example, step policy returns:</p><formula xml:id="formula_0" coords="6,215.83,397.69,254.98,19.38">ùëôùëü = ùëèùëéùë†ùëí_ùëôùëü ‚Ä¢ ùëîùëéùëöùëöùëé (ùëìùëôùëúùëúùëü( ùëñùë°ùëíùëü ùë†ùë°ùëíùëù )) (1)</formula><p>where lr is the learning rate, base_lr is the starting point for learning rate, gamma and step are hyperparameters and iter is the iteration of the training procedure.</p><p>This means that every step iterations, learning rate will be decreased by a factor gamma. In case of gamma = 0,1 (a common gamma value) this means that learning rate will be divided by 10 every step iterations. In our case the learning policy of the solver was step with gamma 0,1 and base learning rate 10e-6. The base learning rate had to be small enough for the system to be able to compute the loss at each iteration since higher learning rates could not converge. The step size was set to 25,000 iterations so that enough epochs could pass at each step. Another parameter that had to be defined is the iteration size. Iteration size works together with the batch size defined at the inputs layer of the network so that the actual batch of the back propagation is the product of these values. This means that with batch size 2 and iteration size 50, the back propagation with batch Stochastic Gradient Descent will compute the gradient against 100 training examples. The benefit of this combination is that the maximum value for batch size based on the memory limitations of a very deep network (2) was used and the phenomenal batch size was increased to 100 with the iteration size which accelerates the back-propagation procedure. Summarizing, the network training parameters were:</p><p>ÔÇ∑ learning rate: 10e-6 ÔÇ∑ learning policy: step ÔÇ∑ gamma: 0.1 ÔÇ∑ stepsize: 25,000 ÔÇ∑ momentum: 0.9 ÔÇ∑ weight decay: 0.0005 ÔÇ∑ iteration size: 50 One important informal parameter that had to be tuned is the threshold of label acceptance. Each one of the 20,464 outputs of the network, is a real number which determines if the corresponding label is predicted to be relative to the image or not. This real number must be transformed to boolean, to procced with the accuracy computations. Normally a threshold equal to 0, 0.5 or 1 is selected so that negative values denote rejection of the label and positive values acceptance (in case of 0) or values close to 0 for rejection and close to 1 for acceptance (in case of 0.5) etc. In our case, the tuning procedure showed that the best accuracy levels were obtained with a threshold of -2. For this network and these parameters, -2 was almost every time the best choice. However, this value was not the best choice when training either other networks, or the same network (ResNet-50) with other hyperparameters. In these cases, the best suited threshold ranged from -4 to 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental data and results</head><p>In order to train the network, the official training dataset of ImageCLEF caption category <ref type="bibr" coords="8,144.99,500.75,16.79,9.16" target="#b43">[44]</ref> of ImageCLEF 2017 <ref type="bibr" coords="8,248.02,500.75,16.56,9.16" target="#b44">[45]</ref> was used. This dataset contains 164,614 biomedical images extracted from scholarly articles on PubMed Central. Since the given images had no fixed size a data preparation process was necessary. The preparation of data includes the Histogram Equalization for all three channels (RGB) with the standard OpenCV library (cv2.equalizeHist) and image resize to 227 x 227 pixels. Both steps are standard for neural network inputs preparation since the input layer of the proposed network has fixed size (227x227x3) and must be normalized. The same holds for validation and test images as well.</p><p>Many attempts took place in order to find the appropriate values for each hyperparameter. For the tuning of threshold of label acceptance, for example several thresholds were tried until the value -2 was selected. In Fig. <ref type="figure" coords="8,348.14,620.78,5.04,9.16" target="#fig_3">6</ref> the difference in accuracy is shown when changing the threshold value for a specific iteration. As one could expect the curve of the relation between threshold and accuracy is concave with a total maximum at some point. In the case of this network all the experiments showed -2 as optimal value. Another training parameter, that was tuned, is the momentum term. As mentioned before a value of 0.9 was selected, however in different learning phases, other momentum values had better performance. For example, in Fig. <ref type="figure" coords="9,337.08,345.67,5.04,9.16" target="#fig_4">7</ref> one can see the accuracy levels for iterations 250,000 to 350,000 for two different momentum values: 0.7 and 0.99. In Fig. <ref type="figure" coords="9,154.85,541.07,5.04,9.16">8</ref> one can see the training curve of the network. The accuracy percentage pick happens at 32,000 iterations. This is the model (weights) of the proposed network that was selected for the test run. The accuracy level over the validation data was 11.399%, while the F1 score of the same model (DET_ConceptDetectionTesting2017-results.txt) on the test data of ImageCLEF caption was 0.1583.</p><p>It is important to mention the steep increase in accuracy levels that happens at 25,000 iterations, when the learning rate changes from 10e-6 to 10e-7. However, short after this improvement the network performance converged to this level. The same behavior can be observed at other learning rate changes too. For example, in Fig. <ref type="figure" coords="10,143.57,358.15,5.04,9.16" target="#fig_5">9</ref> the same sharp improvement happens when passing from learning rate 10e-6 to 10e-7 for a different iteration size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion -Future work</head><p>The Concept Detection subtask of ImageCLEF caption 2017 required the identification of the presence of relevant biomedical concepts in the medical images. A training dataset included 164,614 biomedical images extracted from scholarly articles on PubMed Central was proposed. The difficulty of this task emerges from three different key points. Firstly, the images variety and diversity was very large as one can see on the image examples (see Fig. <ref type="figure" coords="10,230.74,657.26,4.20,9.16">1</ref>) which limits the use of content specific descriptors. Secondly, the range of labels that could be assigned to each medical image was very large i.e. more than 20,000 labels, when many labeling and classification tasks even today handle a few hundreds of labels. The number of labels of the specific subtask was extremely big for present methods and resources to handle. The third and final key point of difficulty was the fact that the specific subtask was about multilabeled data. This means that each image does not belong to one and only class so that one label has to be assigned to it, but in contrast each image could belong to more than one classes meaning more than one labels could be assigned to it. Deep Residual networks proved able to handle better the diversity and complexity of the medical images than other types of shallower networks. Also, pretrained networks had better performance than training them from scratch while retaining the same architecture and parameters.</p><p>For the future work, we seek further improvements by applying clustering of the images and using different networks for training over each cluster or ensemble methods together with deep neural networks for better performance. Clustering was attempted over the training data, however when 5¬±1 clusters were constructed with different clustering algorithms, more than 90% of the training images were assigned to one cluster, which disused the approach's philosophy. Another promising approach could be the use of different thresholds of acceptance for each label to optimize the detection procedure. This means that instead of having one threshold value for all the labels (e.g. -2), one could use a vector where each value would be the threshold of acceptance for each label. This would mean that the optimal value for each label should be obtained for each network model and then the one with maximum accuracy could be selected.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,127.49,374.90,340.86,8.29;4,186.31,385.94,223.10,8.29;4,124.70,147.40,345.87,213.10"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Medical images sample: main characteristic of training set is the great variety. Training set images of ImageCLEF caption 2017 (Eickhoff et al. 2017)</figDesc><graphic coords="4,124.70,147.40,345.87,213.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,242.50,680.51,110.52,8.29;5,178.40,600.05,238.50,71.85"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. First layer (HDF5Data)</figDesc><graphic coords="5,178.40,600.05,238.50,71.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,124.85,651.93,345.64,8.29;7,124.85,662.99,346.02,8.29;7,124.85,673.79,345.46,8.29;7,124.85,684.83,294.84,8.29;7,130.35,147.90,89.25,482.95"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Proposed Residual Network architecture. Due to the size of the network representation, the figure was split in three parts for clarity, starting from left to right. Red rectangles are the convolution layers, blue rectangles are the elementwise operators and green rectangles are other types of layers like Scale, BatchNorm, ReLU etc. Caffe annotation has been used.</figDesc><graphic coords="7,130.35,147.90,89.25,482.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,199.51,298.31,196.60,8.29"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Threshold of label acceptance (iteration 13500)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,131.57,517.74,332.76,8.29"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Accuracy levels for iterations 250,000 to 350,000 for momentum values 0.7 and 0.99</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,158.47,540.78,278.98,8.29"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Accuracy improvement with learning rate change from 10e-6 to 10e-7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,142.61,157.94,328.59,341.79"><head></head><label></label><figDesc>Training curve of network. Steep increase at 25,000 iterations and peak at 32,000 iterations</figDesc><table coords="9,142.61,157.94,328.59,341.79"><row><cell></cell><cell></cell><cell cols="4">ResNet_50, iteration 13500</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9%</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8%</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7%</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6%</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5%</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4%</cell><cell></cell></row><row><cell>-3,5</cell><cell>-3</cell><cell>-2,5</cell><cell>-2</cell><cell>-1,5</cell><cell>-1</cell><cell>-0,5</cell><cell>0</cell></row><row><cell>Fig. 8.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>11%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>250000</cell><cell></cell><cell>270000</cell><cell>290000</cell><cell>310000</cell><cell>330000</cell><cell cols="2">350000</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0,99</cell><cell>0,7</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,132.83,446.92,337.87,8.29;11,141.89,457.72,280.08,8.29" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,230.62,446.92,240.08,8.29;11,141.89,457.72,74.88,8.29">Representations of keypoint-based semantic concept detection: A comprehensive study</title>
		<author>
			<persName coords=""><forename type="first">Yu</forename><forename type="middle">-</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,226.27,457.72,123.36,8.29">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="53" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.83,468.76,337.56,8.29;11,141.89,479.80,114.41,8.29" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,215.12,468.76,167.55,8.29">Sparse ensemble learning for concept detection</title>
		<author>
			<persName coords=""><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,391.98,468.76,78.41,8.29;11,141.89,479.80,42.16,8.29">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="54" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.83,490.86,337.40,8.29;11,141.89,501.90,328.66,8.29;11,141.89,512.94,244.15,8.29" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,342.68,490.86,127.55,8.29;11,141.89,501.90,219.24,8.29">New trends and ideas in visual concept detection: the MIR flickr retrieval evaluation initiative</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,370.45,501.90,100.10,8.29;11,141.89,512.94,193.46,8.29">Proceedings of the international conference on Multimedia information retrieval</title>
		<meeting>the international conference on Multimedia information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.83,523.74,337.79,8.29;11,141.89,534.78,328.57,8.29;11,141.89,545.82,20.28,8.29" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,210.55,523.74,253.64,8.29">On the sampling of web images for learning visual concept classifiers</title>
		<author>
			<persName coords=""><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Shiai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,141.89,534.78,299.16,8.29">Proceedings of the ACM International Conference on Image and Video Retrieval</title>
		<meeting>the ACM International Conference on Image and Video Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.83,556.86,337.90,8.29;11,141.89,567.90,287.28,8.29" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,201.60,556.86,269.13,8.29;11,141.89,567.90,138.08,8.29">Boosted Near-miss Under-sampling on SVM ensembles for concept detection in large-scale imbalanced datasets</title>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,289.12,567.90,60.93,8.29">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="198" to="206" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.83,578.97,337.87,8.29;11,141.89,589.77,328.67,8.29;11,141.89,600.81,70.22,8.29" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,353.02,578.97,117.68,8.29;11,141.89,589.77,173.75,8.29">Visual concept detection of web images based on group sparse ensemble learning</title>
		<author>
			<persName coords=""><forename type="first">Yongqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyoko</forename><surname>Sudo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yukinobu</forename><surname>Taniguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,324.78,589.77,127.74,8.29">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1409" to="1425" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.83,611.85,337.88,8.29;11,141.89,622.89,328.66,8.29;11,141.89,633.93,244.15,8.29" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,342.68,611.85,128.03,8.29;11,141.89,622.89,219.24,8.29">New trends and ideas in visual concept detection: the MIR flickr retrieval evaluation initiative</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,370.45,622.89,100.10,8.29;11,141.89,633.93,193.46,8.29">Proceedings of the international conference on Multimedia information retrieval</title>
		<meeting>the international conference on Multimedia information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.83,644.97,337.80,8.29;11,141.89,655.77,328.58,8.29;11,141.89,666.83,41.42,8.29" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,307.81,644.97,162.82,8.29;11,141.89,655.77,42.53,8.29">Monocular pedestrian detection: Survey and experiments</title>
		<author>
			<persName coords=""><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dariu</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,194.17,655.77,225.28,8.29">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2179" to="2195" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.83,149.73,337.94,8.29;12,141.89,160.77,321.60,8.29" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,286.31,149.73,177.80,8.29">An experimental study on pedestrian classification</title>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Munder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dariu</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,141.89,160.77,226.43,8.29">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1863" to="1868" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,171.81,338.27,8.29;12,141.89,182.85,328.29,8.29" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,206.72,171.81,192.39,8.29">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,408.25,171.81,59.03,8.29">Computer vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1999">1999. 1999</date>
			<publisher>Ieee</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,193.65,338.57,8.29;12,141.89,204.69,328.73,8.29;12,141.89,215.75,41.42,8.29" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,323.88,193.65,147.14,8.29;12,141.89,204.69,30.29,8.29">A performance evaluation of local descriptors</title>
		<author>
			<persName coords=""><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,182.38,204.69,234.60,8.29">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,226.79,338.03,8.29;12,141.89,237.83,147.05,8.29" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="12,323.92,226.79,146.56,8.29;12,141.89,237.83,77.08,8.29">A comparison of color features for visual concept classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="141" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,248.87,338.10,8.29;12,141.89,259.67,230.02,8.29" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,300.35,248.87,170.20,8.29;12,141.89,259.67,10.57,8.29">Nonlinear learning using local coordinate coding</title>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,161.78,259.67,183.73,8.29">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,270.71,338.08,8.29;12,141.89,281.75,127.29,8.29" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,220.88,270.71,123.89,8.29">Efficient sparse coding algorithms</title>
		<author>
			<persName coords=""><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,354.97,270.71,115.56,8.29;12,141.89,281.75,69.31,8.29">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">801</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,292.79,338.01,8.29;12,141.89,303.86,328.80,8.29;12,141.89,314.90,57.91,8.29" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,225.34,292.79,245.13,8.29;12,141.89,303.86,33.04,8.29">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName coords=""><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,183.34,303.86,148.59,8.29;12,360.11,303.86,41.44,8.29">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note>CVPR 2009</note>
</biblStruct>

<biblStruct coords="12,132.45,325.70,338.28,8.29;12,141.89,336.74,292.63,8.29" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,217.31,325.70,208.81,8.29">Locality-constrained linear coding for image classification</title>
		<author>
			<persName coords=""><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,434.83,325.70,35.91,8.29;12,141.89,336.74,142.58,8.29">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,347.78,338.40,8.29;12,141.89,358.82,270.84,8.29" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,308.25,347.78,132.18,8.29">In defense of soft-assignment coding</title>
		<author>
			<persName coords=""><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,450.53,347.78,20.32,8.29;12,141.89,358.82,72.39,8.29;12,241.17,358.82,121.31,8.29">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV)</note>
</biblStruct>

<biblStruct coords="12,132.45,369.86,338.40,8.29;12,141.89,380.90,328.61,8.29;12,141.89,391.70,161.23,8.29" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,334.84,369.86,136.02,8.29;12,141.89,380.90,240.66,8.29">Comparison of mid-level feature coding approaches and pooling strategies in visual concept detection</title>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,392.82,380.90,77.68,8.29;12,141.89,391.70,75.50,8.29">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="479" to="492" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,402.76,337.96,8.29;12,141.89,413.80,180.43,8.29" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,243.32,402.76,81.32,8.29">Visual word ambiguity</title>
		<author>
			<persName coords=""><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,334.84,402.76,135.57,8.29;12,141.89,413.80,89.86,8.29">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1271" to="1283" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,424.84,337.99,8.29;12,141.89,435.88,328.84,8.29;12,141.89,446.92,101.55,8.29" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,226.29,424.84,244.15,8.29;12,141.89,435.88,76.88,8.29">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,227.92,435.88,148.30,8.29;12,403.96,435.88,41.49,8.29">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
	<note>CVPR 2008</note>
</biblStruct>

<biblStruct coords="12,132.45,457.72,337.72,8.29;12,141.89,468.76,329.16,8.29;12,141.89,479.80,189.51,8.29" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,300.87,457.72,169.30,8.29;12,141.89,468.76,206.95,8.29">Soft assignment of visual words as linear coordinate coding and optimisation of its reconstruction error</title>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,358.09,468.76,88.10,8.29">Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,490.86,338.48,8.29;12,141.89,501.90,174.43,8.29" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,202.77,490.86,261.17,8.29">Image classification using super-vector coding of local image descriptors</title>
		<author>
			<persName coords=""><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,141.89,501.90,91.08,8.29">Computer Vision-ECCV</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page" from="141" to="154" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,512.94,338.44,8.29;12,141.89,523.74,328.36,8.29;12,141.89,534.78,76.61,8.29" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,302.23,512.94,168.66,8.29;12,141.89,523.74,49.99,8.29">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName coords=""><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,201.53,523.74,150.91,8.29;12,381.35,523.74,31.82,8.29">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
	<note>CVPR&apos;07</note>
</biblStruct>

<biblStruct coords="12,132.45,545.82,338.11,8.29;12,141.89,556.86,295.68,8.29" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,358.25,545.82,112.30,8.29;12,141.89,556.86,112.35,8.29">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName coords=""><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><surname>S√°nchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,263.18,556.86,90.81,8.29">Computer Vision-ECCV</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page" from="143" to="156" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,567.90,337.86,8.29;12,141.89,578.97,328.74,8.29" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,220.92,567.90,242.28,8.29">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName coords=""><surname>J√©gou</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Herv√©</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,141.89,578.97,179.82,8.29">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,589.77,338.34,8.29;12,141.89,600.81,328.57,8.29;12,141.89,611.85,101.55,8.29" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="12,367.89,589.77,102.90,8.29;12,141.89,600.81,122.97,8.29">Compact tensor based image representation for similarity search</title>
		<author>
			<persName coords=""><forename type="first">Romain</forename><surname>Negrel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philippe-Henri</forename><surname>Gosselin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,273.95,600.81,86.20,8.29">Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,622.89,337.92,8.29;12,141.89,633.93,181.39,8.29" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="12,287.82,622.89,182.55,8.29;12,141.89,633.93,67.26,8.29">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName coords=""><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,218.58,633.93,13.53,8.29">iccv</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1470</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,644.97,337.91,8.29;12,141.89,655.77,170.43,8.29" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m" coord="12,221.71,644.97,248.66,8.29;12,141.89,655.77,94.21,8.29">Local features and kernels for classification of texture and object categories: An in-depth study</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Diss. INRIA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.45,666.83,338.42,8.29;12,141.89,677.87,101.61,8.29" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="12,224.71,666.83,238.70,8.29">The MediaMill TRECVID 2009 semantic video search engine</title>
		<author>
			<persName coords=""><forename type="first">Cees</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,141.89,677.87,74.83,8.29">TRECVID workshop</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.45,149.73,338.03,8.29;13,141.89,160.77,147.05,8.29" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="13,323.92,149.73,146.56,8.29;13,141.89,160.77,77.08,8.29">A comparison of color features for visual concept classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="141" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.45,171.81,338.51,8.29;13,141.89,182.85,328.45,8.29;13,141.89,193.65,178.26,8.29" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="13,291.58,171.81,133.12,8.29">Geometric blur for template matching</title>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,435.05,171.81,35.91,8.29;13,141.89,182.85,113.73,8.29;13,285.72,182.85,184.62,8.29;13,141.89,193.65,91.31,8.29">Proceedings of the 2001 IEEE Computer Society Conference</title>
		<meeting>the 2001 IEEE Computer Society Conference</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct coords="13,132.45,204.69,338.50,8.29;13,141.89,215.75,230.13,8.29" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="13,307.51,204.69,163.44,8.29;13,141.89,215.75,46.85,8.29">Sampling strategies for bag-of-features image classification</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fr√©d√©ric</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,197.70,215.75,90.98,8.29">Computer Vision-ECCV</title>
		<imprint>
			<biblScope unit="volume">2006</biblScope>
			<biblScope unit="page" from="490" to="503" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.45,226.79,337.98,8.29;13,141.89,237.83,328.58,8.29;13,141.89,248.87,20.28,8.29" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="13,367.63,226.79,102.79,8.29;13,141.89,237.83,128.28,8.29">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,280.71,237.83,185.93,8.29">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.45,259.67,136.00,8.29;13,307.53,259.67,163.16,8.29;13,141.89,270.71,328.43,8.29;13,141.89,281.75,283.28,8.29" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="13,386.81,259.67,83.88,8.29;13,141.89,270.71,272.64,8.29">Ordering of visual descriptors in a classifier cascade towards improved video concept detection</title>
		<author>
			<persName coords=""><forename type="first">Foteini</forename><surname>Markatopoulou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasileios</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,424.65,270.71,45.67,8.29;13,141.89,281.75,132.20,8.29">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.45,292.79,338.17,8.29;13,141.89,303.86,329.08,8.29;13,141.89,314.90,15.96,8.29" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="13,224.15,292.79,246.47,8.29;13,141.89,303.86,32.06,8.29">Higher-order occurrence pooling for bags-of-words: Visual concept detection</title>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,182.88,303.86,223.18,8.29">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="326" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.45,325.70,337.74,8.29;13,141.89,336.74,172.83,8.29" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="13,228.62,325.70,196.28,8.29">How transferable are features in deep neural networks?</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,434.99,325.70,35.20,8.29;13,141.89,336.74,146.43,8.29">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.45,347.78,338.31,8.29;13,141.89,358.82,328.43,8.29;13,141.89,369.86,65.85,8.29" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="13,245.10,347.78,225.67,8.29;13,141.89,358.82,13.50,8.29">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName coords=""><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,165.12,358.82,305.20,8.29;13,141.89,369.86,38.97,8.29">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.45,380.90,338.18,8.29;13,141.89,391.70,231.04,8.29" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="13,216.01,380.90,158.00,8.29">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,383.04,380.90,87.60,8.29;13,141.89,391.70,204.67,8.29">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.45,402.76,338.20,8.29;13,141.89,413.80,328.86,8.29;13,141.89,424.84,20.28,8.29" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="13,256.22,402.76,214.43,8.29;13,141.89,413.80,20.13,8.29">Analyzing classifiers: Fisher vectors and deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,172.10,413.80,294.83,8.29">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.45,435.88,338.48,8.29;13,141.89,446.92,227.42,8.29" xml:id="b39">
	<monogr>
		<title level="m" type="main" coord="13,343.50,435.88,127.43,8.29;13,141.89,446.92,70.32,8.29">ImageNet pre-trained models with batch normalization</title>
		<author>
			<persName coords=""><forename type="first">Marcel</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01452</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,132.45,457.72,338.48,8.29;13,141.89,468.76,109.15,8.29" xml:id="b40">
	<monogr>
		<ptr target="https://github.com/Kaim-ingHe/deep-residual-networks" />
		<title level="m" coord="13,141.89,457.72,230.43,8.29">Deep Residual Learning for Image Recognition GitHub</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.45,479.80,337.90,8.29;13,141.89,490.86,291.91,8.29" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="13,223.22,479.80,220.48,8.29">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,455.08,479.80,15.27,8.29;13,141.89,490.86,240.39,8.29">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.45,501.90,338.28,8.29;13,141.89,512.94,219.02,8.29" xml:id="b42">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03044</idno>
		<title level="m" coord="13,220.30,501.90,250.42,8.29;13,141.89,512.94,61.85,8.29">Incremental network quantization: Towards lossless cnns with lowprecision weights</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,132.45,523.74,337.98,8.29;13,141.89,534.78,329.15,8.29;13,141.89,545.82,303.06,8.29" xml:id="b43">
	<analytic>
		<title level="a" type="main" coord="13,141.89,534.78,329.15,8.29;13,141.89,545.82,144.04,8.29">Overview of ImageCLEFcaption 2017 -Image Caption Prediction and Concept Extraction Tasks to Understand Biomedical Images</title>
		<author>
			<persName coords=""><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Immanuel</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Garc√≠a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,292.42,545.82,97.81,8.29">CLEF Labs Working Notes</title>
		<imprint>
			<publisher>CEUR</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.45,556.86,338.52,8.29;13,141.89,567.90,328.59,8.29;13,141.89,578.97,328.67,8.29;13,141.89,589.77,328.78,8.29;13,141.89,600.81,223.65,8.29" xml:id="b44">
	<analytic>
		<title level="a" type="main" coord="13,274.99,589.77,195.68,8.29;13,141.89,600.81,43.59,8.29">Overview of ImageCLEF 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Helbert</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giulia</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bayzidul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josiane</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Immanuel</forename><surname>Schwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,191.74,600.81,86.71,8.29">CLEF 2017 Proceedings</title>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
