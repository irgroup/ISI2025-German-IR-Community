<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.33,115.96,310.68,12.62;1,199.37,133.89,216.62,12.62">ImageCLEF 2017: ImageCLEF Tuberculosis Task -the SGEast Submission</title>
				<funder>
					<orgName type="full">ST Electronics-SUTD Cyber Security Laboratory</orgName>
				</funder>
				<funder>
					<orgName type="full">ISTD-SUTD</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.89,171.85,48.29,8.74"><forename type="first">Jiamei</forename><surname>Sun</surname></persName>
							<email>jiameisun@mymail.sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">ISTD Pillar</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<addrLine>8 Somapah Road</addrLine>
									<postCode>487372</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.74,171.85,58.53,8.74"><forename type="first">Penny</forename><surname>Chong</surname></persName>
							<email>pennychong@mymail.sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">ISTD Pillar</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<addrLine>8 Somapah Road</addrLine>
									<postCode>487372</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ST Electronics-SUTD Cyber Security Laboratory</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<addrLine>8 Somapah Road</addrLine>
									<postCode>487372</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.16,171.85,39.30,8.74"><forename type="first">Yi</forename><surname>Xiang</surname></persName>
						</author>
						<author>
							<persName coords="1,329.78,171.85,52.11,8.74"><forename type="first">Marcus</forename><surname>Tan</surname></persName>
							<email>marcustan@mymail.sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">ISTD Pillar</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<addrLine>8 Somapah Road</addrLine>
									<postCode>487372</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ST Electronics-SUTD Cyber Security Laboratory</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<addrLine>8 Somapah Road</addrLine>
									<postCode>487372</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,418.16,171.85,44.31,8.74;1,287.66,183.80,29.22,8.74"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
							<email>alexanderbinder@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">ISTD Pillar</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<addrLine>8 Somapah Road</addrLine>
									<postCode>487372</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ST Electronics-SUTD Cyber Security Laboratory</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<addrLine>8 Somapah Road</addrLine>
									<postCode>487372</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.33,115.96,310.68,12.62;1,199.37,133.89,216.62,12.62">ImageCLEF 2017: ImageCLEF Tuberculosis Task -the SGEast Submission</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">263311DC0233C563D09A573624B20112</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Convolutional Neural Network</term>
					<term>Residual Networks</term>
					<term>Recurrent Neural Networks</term>
					<term>Long-Short Term Memory</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our methodologies in an attempt to improve the diagnosis accuracy of drug resistant tuberculosis and also of identifying the type of tuberculosis present in the patient, as per the requirements of the ImageCLEF tuberculosis task of ImageCLEF 2017. Firstly, we employed the concept of Convolutional Neural Networks (CNN), which can be used to identify useful features in the Computerized Tomography (CT) scans that were provided in the competition and perform the classification based on them. Secondly, Recurrent Neural Networks (RNN) were used on top of CNNs by utilizing CNNs as a feature extractor and the RNNs as a classifier. In order for our model to produce acceptable results, proper preprocessing was performed prior to providing the input data to the models for training, such as image slicing and data augmentation. Our methods were able to reach rank 4 and 5 for the subtask involving drug-resistant tuberculosis and rank 1 and 2 for the other subtask of identifying the tuberculosis type, according to the evaluation performed by ImageCLEF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tuberculosis (TB) caused by Mycobacterium tuberculosis bacteria is a persistent and deadly threat that endangers the lives of people, even with today's advanced medical technology. Standard medications are known to be ineffective in multidrug-resistant (MDR) tuberculosis. Early appropriate identification of the presence of drug resistance (MDR) and accurate diagnosis of TB types can reduce the potential detrimental effects on patients. Determining whether a strain of TB shows signs of MDR is cost-and equipment-intensive, which unfortunately still cannot be afforded by all the needy patients. At the same time, mobile internet and cloud technologies are becoming wide-spread even in economically underdeveloped and remote regions on this planet. This nourishes hopes that the great successes of deep learning can be employed to help in such circumstances. Using image processing techniques on CT scan images could aid medical doctors in providing hints for a more accurate diagnosis, which is the motivation behind the ImageCLEF 2017 challenge, more specifically focusing on TB. We refer the reader to papers published by Dicente et al. <ref type="bibr" coords="2,332.75,178.77,10.52,8.74" target="#b3">[4]</ref> and Ionescu et al. <ref type="bibr" coords="2,429.12,178.77,10.52,8.74" target="#b7">[8]</ref> for more details of this competition task.</p><p>We decided to participate in this task due to its importance and also due to its challenging nature when compared to many standard datasets used, e.g. in deep learning tutorials: As tuberculosis does not always affect the whole lung volume, one can expect that many of the areas in the lung do not contain discriminative evidence. When seen on volume-or slice-level it implies that the signal to noise ratio is very challenging. The signal to noise ratio makes this challenge unique. This observation in combination with the relatively small sample size of the tasks poses a problem even for transfer learning approaches.</p><p>In this work, we have decided to tackle this problem from two perspectives. Firstly, by viewing each patient's 3-dimensional CT scan and slicing them up into 2-dimensional images to be used as a training data for a CNN model. Secondly, by viewing each 3-dimensional CT scan of patients as sequences of 2-dimensional images and using those sequences to train a RNN model. For the CNN, we adopted He et al. ResNet-50 model <ref type="bibr" coords="2,298.90,358.46,10.52,8.74" target="#b6">[7]</ref> and performed transfer learning with the TB images. For the RNN model, we used several stacked Long-Short Term Memory (LSTM) layers, inspired by Goh et al. <ref type="bibr" coords="2,347.62,382.37,9.96,8.74" target="#b4">[5]</ref>, to train on image features that were generated by the default ResNet-50 model.</p><p>The main contributions of this work are:</p><p>1. Aiding medical doctors in the diagnosis of drug-resistant TB and TB type identification through image processing techniques. 2. Introduce work towards inexpensive and quick methods for early detection of the MDR status and TB types in patients.</p><p>The remainder of the paper will be organized as such. Section 2 will introduce two methods used to the task: transfer learning using CNN and sequence learning using RNN. Experimental results and discussion will be in Section 3 and some possible future works will be discussed in Section 4. Finally, we will conclude our work for ImageCLEF tuberculosis in Section 5. Lastly, the authors would like to point out that our models used for this ImageCLEF tuberculosis task are uploaded to GitHub for sharing <ref type="foot" coords="2,274.48,553.76,3.97,6.12" target="#foot_0">1</ref> and we encourage readers to further improve on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we will introduce two solutions to the TB task in detail. The two sub-tasks of ImageCLEF tuberculosis task are considered as typical image classification task, where deep learning gives state-of-the-art results. Thus in this TB task, we have adopted both CNN and LSTM approaches that are wellknown for their performances in images and sequence classification tasks. We first transform the CT scans into image slices followed by preprocessing as described in (Section 2.1). Then every preprocessed slice of a patient is used to fine tune a CNN. Our early experiments showed that ResNet-50 outperformed GoogLenet and Caffe-reference in terms of accuracy (Section 2.2). Our observations also showed that not all the slices of CT scans contain significant information or are relevant to tuberculosis. Hence, LSTM is implemented using the features of ResNet-50 (Section 2.3) to overcome this problem. Figure <ref type="figure" coords="3,391.46,226.59,4.98,8.74" target="#fig_0">1</ref> shows an overview of our methodology. Both methods are employed on the two sub-tasks. ResNet-50 will output the scores (probability) of different classes. For LSTM method, we extracted the feature of every image slice in original training set from pre-trained ResNet-50 and formed a feature sequence for every patient. The feature sequences are used to train LSTM, which can also output the scores of MDR and different TB classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Preparation</head><p>For data preparation, we emulated what doctors usually do when interpreting CT scans. The slices were extracted along the top-down dimension so that they were of size 512*512. All the pixel values of one image slice were scaled linearly to the range of [0,255] to form a greyscale image. Also, the slices inherit the label of the CT scan.</p><p>More often than not, the more training data we feed into the neural network, the better the performance. In order to gather more training data, every image slice was augmented a little by enhancing the contrast and brightness, blurring, as well as rotating. Contrast and brightness were enhanced by Python ImageEnhance module. When the parameter is set as 1, the output is the original image, thus, we choose 1.5 as the enhancing parameter, in order not to introduce much degradation. We also use 3*3 mode filter to blur the image, where the window size is relatively small. Considering that the lungs positions may vary a little from patient-to-patient, we had also left rotated image slices by 5 degrees to account for the slight differences between patients. Visually, the slices still looked the same after the enhancements. However, for the neural network, the augmented and original slices were considered to be different. By augmenting the training data, we hoped to prevent overfitting of the trained model. For simplicity, we call this dataset as the original training set.</p><p>We generated another training set called masked training set. Since only areas within the lungs contain relevant information, we can use the mask files provided by ImageCLEF to extract the lung area. The mask files are also 3D data with a value of 0, 1 and 2. 0 represents the non-relevant area; 1 and 2 represent the left and the right side of the lung respectively <ref type="bibr" coords="4,317.38,298.32,9.96,8.74" target="#b2">[3]</ref>. Hence, we can get a mask slice for every image slice with the same method in Section 2.1. The objective of using mask files is to only train the lung area in images. During CNN training, the input images are always normalized by subtracting the mean value calculated on the training set from each pixel <ref type="bibr" coords="4,269.50,346.14,16.11,8.74" target="#b9">[10]</ref> to get better backpropagation. Thus, we can set the pixels with 0-mask as the mean of image slices in the original training set, and retain the value of pixels in lung area to get the masked training set.</p><p>During training, we also used the mean of the original training set, so that the non-relevant area would all be 0 after subtracting the mean, thereby highlighting the lung area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transfer learning</head><p>Transfer learning is proven to be useful in image and video processing tasks. Besides the advantage of the model is fast and easy to train, this approach always gives satisfiable results especially when training dataset is relatively small <ref type="bibr" coords="4,134.77,485.71,9.96,8.74" target="#b5">[6]</ref>. In transfer learning, we must first choose a CNN model. According to our experiments on different CNN models including GoogLenet, Caffe-reference, and ResNet-50, we found ResNet-50 to be most stable and has the best performance. Thus, ResNet-50 was used in the following experiment. Both original training set and masked training set were used in transfer learning.</p><p>Training stage After preparing the data, we obtained two training sets, the original set and the masked set. Since both training sets are comprised of 2D images, the image slices of all patients were shuffled and trained as individual images. This would undoubtedly introduce noise in our training data. Hence, in order to address this problem, we used maxpooling layer instead of average pooling so that the pooling layer will extract the feature of relevant slices and reduce the influence of noisy training data. In addition, we also worked around this problem during test stage, which will be further discussed in the next paragraph. When fine tuning the CNN, layers after the last pooling layer are fine tuned more as compared to the other layers in the neural network because the early layers contain generic features such as edge or blobs that are relevant to many image processing tasks. This approach can also avoid overfitting due to the small training dataset.</p><p>Testing stage At test time, slices of one patient are fed into the neural network one at a time and each slice will have one output. We average the outputs of slices of one patient as the final output. As mentioned, training on individual slices will introduce noise to the training data, since some slices do not contain TB nor contain the lungs at all. Nevertheless, our approach of averaging the scores at test time can reduce the impact of noisy training data for a more reliable output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sequence Learning</head><p>RNNs are especially useful in solving problems that contain some sort of sequential or time-series data. This is because they are able to learn the temporal patterns within them. Variants of RNNs have already been used many times before, whether in the context of text generation <ref type="bibr" coords="5,350.89,348.74,15.50,8.74" target="#b13">[14]</ref> or in the area of anomaly detection in Cyber-Physical Systems <ref type="bibr" coords="5,301.04,360.70,9.96,8.74" target="#b4">[5]</ref>. In this subsection, we will provide a brief introduction to RNNs and its variants, before moving on to discuss our implementation in this image recognition context and the reason for doing so.</p><p>Vanilla RNN vs LSTM Vanilla RNNs are very simple cells with just one activation function within it. These cells will be arranged in a sequence with a length that is dependent on a user-defined number. An illustration of a vanilla RNN cell is shown in Figure <ref type="figure" coords="5,261.37,452.56,3.87,8.74" target="#fig_1">2</ref>. </p><formula xml:id="formula_0" coords="6,243.11,135.93,233.24,24.60">h t = H(W h h t-1 + W x x t + b h ) y t = W y h t + b y (<label>1</label></formula><formula xml:id="formula_1" coords="6,476.35,143.51,4.24,8.74">)</formula><p>where H is the activation function in the RNN cell, W are the weights, h t are the hidden vectors at time step t, x t are the inputs at time step t, y t are the outputs at time step t and b are the biases. Hence, it can be said that the output of a RNN cell will become the input of the next RNN cell. The weights in the network are updated at every training iteration using this concept called backpropagation through time. At each iteration, the gradients are calculated and the weights will be fine tuned based on these calculated gradients, so as to minimize the difference between the predicted and the actual result.</p><p>Traditional RNNs are less widely used due to its inherent exploding and vanishing gradients problem, which is found in gradient-based learning and backpropagation, thereby affecting the learning quality of the model negatively when attempting to learn long temporal sequences. A paper published by Pascanu et al. illustrates this point clearly <ref type="bibr" coords="6,271.97,324.45,14.61,8.74" target="#b11">[12]</ref>.</p><p>LSTMs are variants of the vanilla RNNs and they have a more complex cell architecture. As such, they take longer time for backpropagation through time. They are also arranged in sequence, like the case for vanilla RNNs with a sequence length that is user-defined. Furthermore, there are different variations of LSTM models, one of which being peephole LSTM. An example of a peephole LSTM cell is shown in Figure <ref type="figure" coords="6,267.18,398.70,3.87,8.74" target="#fig_2">3</ref>. </p><formula xml:id="formula_2" coords="7,211.80,129.49,268.79,69.43">i t = σ(W xi x t + W hi h t-1 + W ci c t-1 + b i ) f t = σ(W xf x t + W hf h t-1 + W cf c t-1 + b f ) c t = f t c t-1 + i t tanh(W xc x t + W hc h t-1 + b c ) o t = σ(W xo x t + W ho h t-1 + W co c t + b o ) h t = o t tanh(c t ) (2)</formula><p>where σ is a sigmoid activation function, tanh is an activation function, W are the weights, h t and h t-1 are the hidden vectors, c t and c t-1 are the cell states of a LSTM cell, b are the biases, o t are the output gates, i t are the input gates and f t are the forget gates. This variant is called the peephole LSTM because of its peephole connections from the cell state c t-1 to f t and i t , and from c t to o t . LSTM solves the gradients issue that was mentioned earlier that was present in the case of vanilla RNNs, as LSTMs have the ability to learn what to remember and what to forget through the forget gate, f t . Hence, LSTM is more popular and more widely used, and we chose that to be implemented in our context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN-LSTM methodology</head><p>As our raw data was given as a CT scan of patients, we were able to extract out the image slices of the lungs. It is clear that these extracted images can form a sequence of images from the start to the end of the lungs. Also, as the TB-affected areas would not appear in the entire lung, some images will be seen as exempted from TB traces. Hence, we explored the idea of using a sequence of images, that potentially show different parts of the TB area, mixed together with images without TB to make the classification more accurate and robust. Thus, we used the CNN to generate the image features and then followed by LSTM for classification, which we will refer to as CNN-LSTM. The CNN-LSTM architecture can be thought of as a two-step process. Firstly, images will be passed into the CNN model to generate image feature vectors. These feature vectors describe the images that were passed into the CNN in a numerical form. After that, these feature vectors will be passed into the LSTM model for classification, since it is not possible to pass an image to a recurrent layer, to the best of our knowledge. This idea was illustrated in Figure <ref type="figure" coords="7,446.02,503.03,3.87,8.74" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment and submitted runs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental data and Libraries used</head><p>Experimental data Our experimental data was provided by ImageCLEF <ref type="bibr" coords="7,460.19,584.39,15.30,8.74" target="#b3">[4]</ref>. We were given 230 CT scans for MDR task and 500 for the TB-Type task. To evaluate the performance of our models, we split 20% of the data as the local test set and used the rest of the data, which we call local training set, to implement transfer learning and sequence learning. On the local training set, we employed 5-fold cross validation to guarantee that our model was robust. As for the final submitted runs, we trained on the total training set.</p><p>Adhering to our methodology as described earlier, CT scans were transformed into image slices. Thus, there was a total of 27992 slices for MDR task and 68935 slices for the TB-Type task. After data augmentation, we obtained four times more image slices to train and all the slices inherit the label of their corresponding patient. However, tuberculosis only exists in some of the slices in CT scan. Slices that do not contain tuberculosis but inherit the label of MDR or tuberculosis patient will impact the accuracy of our model. In other words, the signal to noise ratio of our training set is very low This is the main reason why the accuracy is low. Experimental results will be shown in the following sections.</p><p>Libraries used For the training of the ResNet-50, we used the Caffe library <ref type="bibr" coords="8,470.08,245.50,10.52,8.74" target="#b8">[9]</ref> by Jia et al. On the other hand, we used Keras <ref type="bibr" coords="8,337.07,257.46,10.52,8.74" target="#b1">[2]</ref> with Theano <ref type="bibr" coords="8,407.83,257.46,15.50,8.74" target="#b14">[15]</ref> as a backend for the training of the LSTM neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment of Transfer learning</head><p>Transfer learning was implemented based on pre-trained ResNet-50 and we trained on the original training set and masked training set as described in Section 2.1. We also trained different models using AVE and MAX for pooling layers. AVE pooling uses the average score as the output of sub-sampling, while MAX pooling uses the maximal score <ref type="bibr" coords="8,291.65,369.00,15.97,8.74" target="#b12">[13]</ref>. All the slices were resized to 256*256 in order to increase the batch size and accelerate training. In addition, the crop size was set as 192 to increase the batch size further, and at the same time, we do not lose much information of the lung. At test time, only the center crop was used. Training parameters were modified based on the original training parameters of ResNet-50, and we mainly lowered the learning rate to do transfer learning. We employed stochastic gradient descent (SGD) as the optimizer. The base learning rate was set to 0.0001 and decremented every 150000 iterations using the step function. The weight decay was set to 0.0001, gamma to 0.1 and momentum to 0.9. To compare different models, we plotted the results of the local experiment in Figure <ref type="figure" coords="8,177.81,488.55,3.87,8.74" target="#fig_4">4</ref>, where original and masked represent the two different training sets while AVE and MAX represent the pooling policies chosen.</p><p>By comparing the black and red lines in Figure <ref type="figure" coords="8,361.49,512.66,9.22,8.74" target="#fig_4">4a</ref>) and Figure <ref type="figure" coords="8,430.09,512.66,8.58,8.74" target="#fig_4">4b</ref>), we can see that, on average, models trained on original training set obtained higher accuracy and are also more stable as compared to models trained on the masked training set. This suggests that the usage of masks may not be appropriate for both tasks. The manual inspection also showed that some masks appeared to disrupt the features of TB by introducing additional noises in the CT scans. By comparing green and red lines, we also observed that the usage of maxpooling as compared to average pooling results in a lower accuracy in MDR task but had similar or higher accuracy in the TB-Type task. Generally, we believe maxpooling performs better than average pooling in capturing features of relevant slices with tuberculosis. However, our local testing on MDR task suggests the opposite, which may be due to our relatively small test data in which the result may not be a true indicator of our model performance.</p><p>Among our submitted runs, MDR resnet partial and TBT resnet partial, were generated by ResNet-50 model trained on the local training set with average pooling. As mentioned, the local training set is the remaining data set after the extraction of 20% of original slices for local testing. MDR resnet full and TBT resnet full were generated by ResNet-50 model trained on the entire training set which includes all the original and augmented slices with maxpooling. For testing, we had used the official data set provided by ImageCLEF.</p><p>The results of our final submitted runs and their corresponding ranks are shown in Table <ref type="table" coords="9,203.44,215.31,3.87,8.74" target="#tab_0">1</ref>. We have included the performances of our models in terms of area under curve (AUC) and accuracy (ACC) for MDR task, and Kappa coefficient and ACC for TB-Type task. Comparing the results of submitted runs we can see that, maxpooling models indeed performed better than average pooling and this proves our earlier belief that maxpooling can, to some extent, capture features of relevant image slices of a patient.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiment of Sequence Learning</head><p>In order to use LSTM, some data preprocessing had to be done so that we were able to pass our data into the LSTM model for training. Firstly, we used the pre-trained ResNet-50 model as a feature extractor. We passed the image slices into the pre-trained ResNet-50 model and extracted the output of the last fully connected layer, which is a vector of size 2048. So each individual slice gets a vector representation. Next, we grouped feature vectors that belong to the same patient into the same sequence. As the number of image slices per patient can vary from 50 to 400, we chose an arbitrary value of 150 feature vectors to form one sequence per patient. If there were not enough slices to form the 150 feature vectors, some slices will be repeated. However, if there were more than enough slices, we will sub-sample the image slices to construct the sequence length of 150. After which, our data would be in 3-dimension in the format of (patient, sequence, feature vector), where each patient has only one sequence. We then further lowered the sequence length to 75 so that we had a more zoomed-in representation by increasing the number of sequences per patient. This was so that the areas with TB will become more significant. Lastly, we passed these The LSTM portion of the CNN-LSTM consists of 3 stacked LSTM layers. By having stacked LSTM layers, complex temporal sequences could be learned. Figure <ref type="figure" coords="10,165.60,620.25,4.98,8.74">5</ref> shows how the general architecture of our neural network making use of LSTM looks like, with the output dimensionality of the LSTM layers reducing after each layer. In the model that we defined, we also have some Dropout layers that attempt to prevent the model from over-fitting. The prediction as shown in Figure <ref type="figure" coords="11,165.94,118.99,4.98,8.74">5</ref> would either be the probability of detecting MDR-TB or 1 of the 5 TB types. Fig. <ref type="figure" coords="11,238.29,230.70,3.87,8.74">5</ref>: Illustration of LSTM architecture Figures <ref type="figure" coords="11,185.79,271.34,4.98,8.74">6</ref> and<ref type="figure" coords="11,214.88,271.34,4.98,8.74">7</ref> summarize the accuracy we obtained, when we performed local testing on two different RNN variations, vanilla RNN and LSTM. The difference between the two architectures was just the type of RNN being used and all the other factors remained constant. For the MDR task, as shown in Figure <ref type="figure" coords="11,166.57,319.16,3.87,8.74">6</ref>, we can see that the accuracy for using vanilla RNN and LSTM were very similar but LSTM still outperformed the vanilla RNN by a slight margin. For the TB-Type task, as shown in Figure <ref type="figure" coords="11,320.89,343.07,3.87,8.74">7</ref>, the differences between the vanilla RNN and LSTM were much greater than that of the MDR task, with the LSTM clearly outperforming the vanilla RNN. As such, we chose to use LSTM as our RNN layers.</p><p>For our submitted runs, we trained our model with the augmented and original data that we had in preparation for the prediction of labels for the test data given by ImageCLEF, that was provided at a later date. We also set aside some labeled data for model verification purposes, so that we can ensure that our prediction accuracy based on the labeled data is acceptable, before moving on to the prediction of unlabelled data. This splitting of data is similar to that described in Section 3.2, where 80% of the data was used for training purposes and the remaining 20% being used for validation. Table <ref type="table" coords="11,374.86,476.76,4.98,8.74" target="#tab_1">2</ref> summarizes the results that our team achieved based on the evaluation performed by the ImageCLEF committee, with the evaluation method as described in 3.2.  As mentioned in Section 2.2, TB does not always affect the whole area of the lungs, particularly in early stages. Also, the image slices at the two extreme ends often do not contain any part of the patient's lungs. Thus only a certain percentage of the slices contain information relevant to the discrimination between various TB types which made the classification problems in this challenge very challenging due to the low signal to noise ratio. Although we tried maxpooling and averaging slices' output of patients to address this problem, we also suggest another method which is a potentially better method to decrease the noise. The proposed way is to manually extract, at least for a subset of patients, the relevant slices of different types of TB, and label the image slices that are not relevant as belonging to a sixth class. All patient data that would not have been preprocessed in that way, would be fully assigned to one of the five existing classes. Using this approach, even by an incomplete annotation of the patients, one could increase the signal to noise ratio. We did not do this at all for the submitted runs, as we lacked the time and the only expertise in our group present was regarding histopathological stains. The emphasis here is that even a partial annotation of a subset of patients could improve prediction capability. In the TB-Type task, we can train the data with six classes: 5 types of TB and nondiscriminative/background slices.</p><p>At the testing phase, image slices of a particular patient are fed into the network, and the network will not only give the type of TB but also output the slices that contain TB and those that don't contain TB as well. We can also use methods such as layer-wise relevance propagation <ref type="bibr" coords="13,390.69,238.70,13.41,8.74" target="#b0">[1]</ref> and deep Taylor decomposition <ref type="bibr" coords="13,194.28,250.66,18.31,8.74" target="#b10">[11]</ref> to further analyse the predictions of trained models. Both methods can output heatmaps that show pixels' relevance <ref type="bibr" coords="13,382.15,262.61,12.53,8.74" target="#b0">[1]</ref> to the prediction of a model. And in our tuberculosis task, heatmaps can highlight the TB part in an image slice. This may also help doctors as a second fall back check that may help to identify overlooked areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our work uses techniques of deep learning to complete the ImageCLEF 2017 TB task. By converting CT scans to image slices, we were able to implement CNN transfer learning and LSTM sequence learning on MDR task and TB-Type task. Both methods gave relatively good results in the competition. Hence, we can conclude that transfer learning CNN can learn to discriminate different TB types, and the feature sequence extracted from CNN can also give a good representation of CT scans. However, due to the highly noisy nature of the training data, our performance of using neural network for training was affected negatively. If we were able to label the slices more accurately, the two methods in this paper would give better results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,406.68,345.82,8.74;3,134.77,418.64,345.83,8.74;3,134.77,430.59,345.82,8.74;3,134.77,442.55,345.82,8.74;3,134.77,454.50,345.82,8.74;3,134.77,466.46,345.82,8.74;3,134.77,478.41,345.83,8.74;3,134.77,490.37,345.82,8.74;3,134.77,502.32,146.17,8.74;3,139.42,269.16,336.53,126.00"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Method description. Preprocessing transforms the CT scans into image slices along the top-down dimension. Data augmentation increases training data. Each slice inherits the label of the patient. For transfer learning, we tried both the original training set and masked training set to fine tune ResNet-50 (Section 2.1). ResNet-50 will output the scores (probability) of different classes. For LSTM method, we extracted the feature of every image slice in original training set from pre-trained ResNet-50 and formed a feature sequence for every patient. The feature sequences are used to train LSTM, which can also output the scores of MDR and different TB classes.</figDesc><graphic coords="3,139.42,269.16,336.53,126.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,221.16,618.96,173.05,8.74;5,205.35,484.85,204.66,122.58"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Illustration of a vanilla RNN cell</figDesc><graphic coords="5,205.35,484.85,204.66,122.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,213.54,614.79,188.26,8.74;6,143.69,435.17,327.99,168.09"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Illustration of a peephole LSTM cell</figDesc><graphic coords="6,143.69,435.17,327.99,168.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,166.55,513.81,282.24,8.74;10,178.08,314.54,259.20,172.80"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Local testing result of selected transfer learned ResNet-50</figDesc><graphic coords="10,178.08,314.54,259.20,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="12,187.19,280.36,240.99,8.74;12,155.80,115.84,303.75,153.00"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>Fig. 6: Local testing of sequence learning for MDR task</figDesc><graphic coords="12,155.80,115.84,303.75,153.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,161.20,319.39,292.96,8.74"><head>Table 1 :</head><label>1</label><figDesc>Results of submitted run using transfer learned ResNet-50</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,134.77,547.97,345.83,86.50"><head>Table 2 :</head><label>2</label><figDesc>Results of highest scoring submitted run using CNN-LSTM for MDR and TBT tasks</figDesc><table coords="11,246.67,576.10,118.96,58.37"><row><cell>MDR</cell><cell>TB-Type</cell></row><row><cell cols="2">AUC 0.5620 Kappa 0.2374</cell></row><row><cell cols="2">ACC 0.5493 ACC 0.3900</cell></row><row><cell>Rank 4</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,656.80,260.92,7.86"><p>https://github.com/maizesix92/ImageCLEF2017 TB SGEast.git</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>This work was generously supported by the <rs type="funder">ST Electronics-SUTD Cyber Security Laboratory</rs>. The authors express gratefulness for the <rs type="funder">ISTD-SUTD</rs> start-up funding.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.96,601.69,337.63,7.86;13,151.52,612.65,329.07,7.86;13,151.52,623.61,231.52,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,184.71,612.65,295.88,7.86;13,151.52,623.61,86.19,7.86">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">D</forename><surname>Suarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,246.02,623.61,41.75,7.86">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,634.73,288.76,8.12" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,645.84,337.63,7.86;13,151.52,656.80,329.07,7.86;14,151.52,119.67,329.07,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,272.18,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,463.57,645.84,17.02,7.86;13,151.52,656.80,265.61,7.86">Efficient and fully automatic segmentation of the lungs in ct volumes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">Dicente</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A</forename><surname>Jiménez Del Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,462.94,119.67,17.66,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,119.44,7.86">Proceedings of the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI, CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Jiménez Del Toro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Foncubierta-Rodríguez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<meeting>the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI, CEUR Workshop Proceedings</meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,152.55,337.63,7.86;14,151.52,163.51,329.07,7.86;14,151.52,174.47,329.07,7.86;14,151.52,185.43,271.49,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,442.93,152.55,37.65,7.86;14,151.52,163.51,325.30,7.86">Overview of ImageCLEFtuberculosis 2017 -predicting tuberculosis type and drug resistances</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">Dicente</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kalinovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="14,163.71,174.47,261.00,7.86">CLEF 2017 Labs Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,196.39,337.63,7.86;14,151.52,207.34,329.07,7.86;14,151.52,218.30,305.11,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,317.45,196.39,163.14,7.86;14,151.52,207.34,144.95,7.86">Anomaly detection in cyber physical systems using recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Adepu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,335.66,207.34,144.93,7.86;14,151.52,218.30,195.46,7.86">IEEE 18th International Symposium on High Assurance Systems Engineering (HASE)</title>
		<imprint>
			<date type="published" when="2017-01">2017. Jan 2017</date>
			<biblScope unit="page" from="140" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,229.26,337.63,7.86;14,151.52,240.22,329.07,7.86;14,151.52,251.18,256.61,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,365.95,229.26,114.64,7.86;14,151.52,240.22,324.98,7.86">Guest editorial deep learning in medical imaging: Overview and future promise of an exciting new technique</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,151.52,251.18,157.55,7.86">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1153" to="1159" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,262.14,337.64,7.86;14,151.52,273.10,158.15,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="14,302.08,262.14,174.58,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.96,284.06,337.64,7.86;14,151.52,295.02,329.07,7.86;14,151.52,305.98,329.07,7.86;14,151.52,316.93,329.07,7.86;14,151.52,327.89,329.07,7.86;14,151.52,338.85,329.07,7.86;14,151.52,349.81,233.12,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,442.94,305.98,37.65,7.86;14,151.52,316.93,236.42,7.86">Overview of ImageCLEF 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,413.10,316.93,67.49,7.86;14,151.52,327.89,329.07,7.86;14,151.52,338.85,97.20,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction 8th International Conference of the CLEF Association</title>
		<title level="s" coord="14,373.22,338.85,107.37,7.86;14,151.52,349.81,28.17,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>CLEF; Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-09-11">2017. September 11-14 2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,360.77,337.64,7.86;14,151.52,371.73,329.07,7.86;14,151.52,382.69,153.46,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="14,239.37,371.73,236.62,7.86">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.61,393.65,337.98,7.86;14,151.52,404.61,329.07,7.86;14,151.52,415.56,117.45,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,346.35,393.65,134.24,7.86;14,151.52,404.61,119.00,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,290.93,404.61,189.67,7.86;14,151.52,415.56,16.91,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,426.52,337.98,7.86;14,151.52,437.48,329.07,7.86;14,151.52,448.44,100.04,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,437.20,426.52,43.39,7.86;14,151.52,437.48,260.07,7.86">Explaining nonlinear classification decisions with deep taylor decomposition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,421.08,437.48,59.52,7.86;14,151.52,448.44,22.56,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="211" to="222" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,459.40,337.98,7.86;14,151.52,470.36,195.67,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,324.70,459.40,155.89,7.86;14,151.52,470.36,62.15,7.86">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,222.01,470.36,24.16,7.86">ICML</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,481.32,337.98,7.86;14,151.52,492.28,329.07,7.86;14,151.52,503.24,103.86,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,309.17,481.32,171.42,7.86;14,151.52,492.28,176.57,7.86">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,338.31,492.28,142.28,7.86">Artificial Neural Networks-ICANN</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,514.19,337.97,7.86;14,151.52,525.15,329.07,7.86;14,151.52,536.11,143.25,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,328.73,514.19,151.85,7.86;14,151.52,525.15,33.97,7.86">Generating text with recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,202.79,525.15,277.80,7.86;14,151.52,536.11,42.25,7.86">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,547.07,337.98,7.86;14,151.52,558.03,291.67,7.86" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="14,307.78,547.07,172.81,7.86;14,151.52,558.03,111.93,7.86">A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName coords=""><forename type="first">Theano</forename><surname>Development</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Team</forename><surname>Theano</surname></persName>
		</author>
		<idno>arXiv e-prints, abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
