<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,178.28,115.96,258.79,12.62;1,177.85,133.89,259.66,12.62">Overview of ImageCLEFlifelog 2017: Lifelog Retrieval and Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.29,171.64,103.51,8.74"><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
							<email>duc-tien.dang-nguyen@dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Insight Centre for Data Analytics</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.34,171.64,46.86,8.74"><forename type="first">Luca</forename><surname>Piras</surname></persName>
							<email>luca.piras@diee.unica.it</email>
							<affiliation key="aff1">
								<orgName type="department">DIEE</orgName>
								<orgName type="institution">University of Cagliari</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.75,171.64,67.69,8.74"><forename type="first">Michael</forename><surname>Riegler</surname></persName>
							<email>michael@simula.no</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Simula Research Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.00,171.64,55.84,8.74"><forename type="first">Giulia</forename><surname>Boato</surname></persName>
							<email>boato@disi.unitn.it</email>
							<affiliation key="aff3">
								<orgName type="department">DISI</orgName>
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,233.62,183.60,51.61,8.74"><forename type="first">Liting</forename><surname>Zhou</surname></persName>
							<email>zhou.liting2@mail.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Insight Centre for Data Analytics</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.16,183.60,62.11,8.74"><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
							<email>cathal.gurrin@dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Insight Centre for Data Analytics</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,178.28,115.96,258.79,12.62;1,177.85,133.89,259.66,12.62">Overview of ImageCLEFlifelog 2017: Lifelog Retrieval and Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">651EACF82513C2C62856AF9E9A5D59F7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the increasing number of successful related workshops and panels, lifelogging has rarely been the subject of a rigorous comparative benchmarking exercise. Following the success of the new lifelog evaluation task at NTCIR-12, 1 the first ImageCLEF 2017 LifeLog task aims to bring the attention of lifelogging to a wide audience and to promote research into some of the key challenges of the coming years. The ImageCLEF 2017 LifeLog task aims to be a comparative evaluation framework for information access and retrieval systems operating over personal lifelog data. Two subtasks were available to participants; all tasks use a single mixed modality data source from three lifeloggers for a period of about one month each. The data contains a large collection of wearable camera images, an XML description of the semantic locations, as well as the physical activities of the lifeloggers. Additional visual concept information was also provided by exploiting the Caffe CNN-based visual concept detector. For the two sub-tasks, 51 topics were chosen based on the real interests of the lifeloggers. In this first year three groups participated in the task, submitting 19 runs across all subtasks, and all participants also provided working notes papers. In general, the groups performance is very good across the tasks, and there are interesting insights into these very relevant challenges.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The availability of a large variety of personal devices, such as smartphones, video cameras as well as wearable devices that allow capturing pictures, videos and audio clips of every moment of our life is creating vast archives of personal data where the totality of an individual's experiences, captured multi-modally through digital sensors are stored permanently as a personal multimedia archive. These unified digital records, commonly referred to as lifelogs, have gathered increasing attention in recent years within the research community. This happened due to the need for, and challenge of, building systems that can automatically analyse these huge amounts of data in order to categorize, summarize and query them to retrieve information that the user may need. For example, lifeloggers may want to recall some events that they do not remember clearly or to know some insights of their activities at work to improve the performance. Figure <ref type="figure" coords="2,475.61,190.72,4.98,8.74" target="#fig_0">1</ref> shows an example of what a lifelogger wants to retrieve. The ImageCLEF 2017 LifeLog task is inspired by the general image annotation and retrieval tasks that have been part of ImageCLEF since 2003. In the early years the focus was on retrieving relevant images from a web collection given (multilingual) queries, from 2006 onwards annotation tasks were also held, initially aimed at object detection, but more recently also covering semantic concepts <ref type="bibr" coords="2,160.55,476.68,11.88,8.74" target="#b8">[9,</ref><ref type="bibr" coords="2,172.42,476.68,11.88,8.74" target="#b9">10,</ref><ref type="bibr" coords="2,184.30,476.68,11.88,8.74" target="#b11">12,</ref><ref type="bibr" coords="2,196.18,476.68,11.88,8.74" target="#b10">11]</ref>. In the last two editions <ref type="bibr" coords="2,321.39,476.68,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="2,331.91,476.68,7.01,8.74" target="#b4">5]</ref>, the image annotation task was expanded to concept localization and also natural language sentential description of images. As there is an increased interest in recent years in research combining text and vision, this year task, changing a little the focus of the retrieval object, aim at further stimulating and encouraging multi-modal research that uses text and visual data, and natural language processing for image retrieval and summarization.</p><p>This paper presents the overview of the first edition of the ImageCLEF 2017 LifeLog task, one of the four benchmark campaigns organized by ImageCLEF <ref type="bibr" coords="2,470.08,575.81,10.52,8.74" target="#b5">[6]</ref> in 2017 under the CLEF initiative <ref type="foot" coords="2,287.03,586.19,3.97,6.12" target="#foot_1">2</ref> . Section 2 describes the task in detail, including the participation rules and the provided data and resources. Section 3 presents and discusses the results of the submissions received for the task. Finally, Section 4 concludes the paper with final remarks and future outlooks.</p><p>2 Overview of the Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation and Objectives</head><p>Based on the successful of NTCIR-12 lifelog task, we present here new tasks which aim to advance the state-of-the-art research in lifelogging as an application of information retrieval. By proposing these tasks at ImageCLEF, we intent to enlarge the association, by linking lifelogging researchers to the image retrieval community. We also hope that novel approaches based on multi-modal retrieval will be able to provide new insights from the personal lifelogs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenge Description</head><p>The ImageCLEF 2017 LifeLog task<ref type="foot" coords="3,294.01,271.27,3.97,6.12" target="#foot_2">3</ref> aims to be a comparative evaluation of information access and retrieval systems operating over personal lifelog data. The task consisted of two sub-tasks, both allow participation independently. These sub-tasks are:</p><p>-Lifelog Retrieval Task (LRT); -Lifelog Summarization Task (LST).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lifelog retrieval task</head><p>The participants had to analyse the lifelog data and for several specific queries, return the correct answers. For example: Shopping for Wine: Find the moment(s) when I was shopping for wine in the supermarket or The Metro: Find the moment(s) when I was riding a metro. The ground truth for this sub-task was created by extending the queries from the NTCIR-12 dataset, which already provides a sufficient ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lifelog summarization task</head><p>In this sub-task the participants had to analyse all the images and summarize them according to specific requirements. For instance: Public Transport: Summarize the use of public transport by a user. Taking any form of public transport is considered relevant, such as bus, taxi, train, airplane and boat. The summary should contain all different day-times, means of transport and locations, etc.</p><p>Particular attention had to be paid to the diversification of the selected images with respect to the target scenario. The ground truth for this sub-task was created utilizing crowdsourcing and manual annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset</head><p>The Lifelog dataset<ref type="foot" coords="3,218.79,600.46,3.97,6.12" target="#foot_3">4</ref> consists of data from three lifeloggers for a period of about one month each. The data contains 88, 124 wearable camera images (approximately two images per minute), an XML description of 130 associated semantic  <ref type="table" coords="4,472.84,239.70,3.87,8.74" target="#tab_0">1</ref>.</p><p>In order to reduce the barriers-to-participation, the output of the Caffe CNNbased visual concept detector <ref type="bibr" coords="4,264.01,263.61,10.52,8.74" target="#b6">[7]</ref> was included in the test collection as additional metadata. This classifier provided labels and probabilities for 1,000 objects in every image. The accuracy of the Caffe visual concept detector is variable and is representative of the current generation of off-the-shelf visual analytics tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Topic and Ground-truth</head><p>Aside from the data, the test collection included a set of topics (queries) that were representative of the real-world information needs of lifeloggers. There were 36 ad-hoc search topics, 16 for the development set and 20 for the test set, representing the challenge of retrieval for the LRT task (see Tables <ref type="table" coords="4,441.88,385.16,4.98,8.74" target="#tab_1">2</ref> and<ref type="table" coords="4,471.74,385.16,4.43,8.74">3</ref>) and 15 search topics, 5 for the development set and 10 for the test set, for the Summarization sub-task (see Tables <ref type="table" coords="4,293.62,409.07,4.98,8.74" target="#tab_2">4</ref> and<ref type="table" coords="4,320.63,409.07,3.87,8.74">5</ref>). For full descriptions of the topics, please see the Appendix A.</p><p>The ground-truth of retrieval topics were created by the task organizers, with the verification of the lifeloggers. For summarization topics, task organizers manually classified the images into the clusters which are provided by the lifeloggers. All the results are then verified by the lifeloggers once more time before publishing. T2. On the Bus or Train Query: Find the moment(s) when user u1 was taking a bus or a train in his home country.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T3. Having a Drink</head><p>Query: Find the moment(s) when user u1 was having a drink in a bar with someone.</p><p>T4. Riding a Red (and Blue) Train Query: Find the moment(s) when user u1 was riding a red (and blue) coloured train.</p><p>T5. The Rugby Match Query: Find the moment(s) when user u1 was watching rugby football on a television when not at home.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T6. Costa Coffee</head><p>Query: Find the moment(s) when user u1 was in Costa Coffee. T7. Antiques Store Query: Find the moment(s) when user u1 was browsing in an antiques store.</p><p>T8. ATM Query: Find the moment(s) when user u1 was using an ATM machine.</p><p>T9. Shopping for Fish Query: Find the moment(s) when user u1 was shopping for fish in the supermarket. T10. Cycling home Query: Find the moment(s) when user u2 was cycling home from work. T11. Shopping Query: Find the moment(s) in which user u2 was grocery shopping in the supermarket.</p><p>T12. In a Meeting Query: Find the moment(s) in which user u2 was in a meeting at work with 2 or more people.</p><p>T13. Checking the Menu Query: Find the moment(s) when user u2 was standing outside a restaurant checking the menu. T14. Watching TV Query: Find the moment(s) when user u3 was watching TV. T15. Writing Query: Find the moment(s) when user u3 was writing on a paper using a pen or pencil.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T16. Drinking in a Pub</head><p>Query: Find the moment(s) when user u3 was drinking in a pub with friends or alone. Table <ref type="table" coords="5,198.21,603.97,3.87,8.74">3</ref>: Search topics for the test set in Lifelog Retrieval Task T1. Using laptop out of office Query: Find the moment(s) in which user u1 was using his laptop outside the working places. T2. On stage Query: Find the moment(s) in which user u1 was giving a talk as a presenter/speaker. T3. Shopping in the electronic market. Query: Find the moment(s) in which user u1 was shopping in the electronic market.</p><p>T4. Jogging in the park Query: Find the moments(s) in which user u1 was jogging in a park. T5. In a Meeting 2 Query: Find the moment(s) that user u1 was in a meeting at work. T6. Watching TV 2 Query: Summarize the moment(s) when user u1 was watching TV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T7. Pizza and Friends</head><p>Query: Find the moment(s) in which user u1 was eating pizza in the restaurant with friends. T8. Working in the air. Query: Find the moment(s) in which user u1 was using computer in the airplane. T9. Playing Guitar Query: Find the moment(s) in which user u2 was playing guitar. T10. Exercise in the gym Query: Find the moment(s) in which user u2 was doing exercise in the gym. T11. Eating fruits Query: Find the moment(s) in which user u2 was having fruits. T12. Brushing or washing face Query: Find the moment(s) in which user u2 was brushing or washing face. T13. Eating 2 Query: Find the moment(s) when user u2 was eating or drinking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T14. At McDonald</head><p>Query: Find the moment(s) in which user u2 was at McDonald for eating or just for relaxing. T15. Viewing a statue Query: Find the moment(s) in which user u2 was viewing a statue. T16. ATM Query: Find the moment(s) when user u2 was using an ATM machine. T17. Have party with friends at friends home. Query: Find the moment(s) in which user u3 was attending a party with many friends at a friends home. T18. Shopping in the butchers shop. Query: Find the moment(s) in which user u3 was consuming in the butchers shop.</p><p>T19. Buying a ticket via ticket machine. Query: Find the moment(s) in which user u3 was buying a ticket via ticket machine. T20. Shopping 2 Query: Find the moment(s) in which user u3 doing shopping. T4. In a Meeting Query: Summarize the activities of user u2 in a meeting at work. T5. Watching TV Query: Summarize the moment(s) when user u3 was watching TV. Table <ref type="table" coords="7,193.61,411.90,3.87,8.74">5</ref>: Search topics for the test set in Lifelog Summarization Task T1. In a Meeting 2 Query: Summarize the activities of user u1 in a meeting at work. T2. Watching TV 2 Query: Summarize the moment(s) when user u1 was watching TV.</p><p>T3. Using laptop outside the office Query: Summarise the moment(s) in which user u1 was using his laptop outside the working places.</p><p>T4. Working at home Query: Find the moment(s) in which user u1 was working at home. T5. Eating 2 Query: Summarize the moment(s) when user u2 was eating or drinking. T6. Social Drinking 2 Query: Summarize the the social drinking habits of user u2. T7: Sightseeing Query: Summarize the moments when the user u2 seeing street, people, landscape, etc. when he was traveling to other cities or countries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T8. Transporting</head><p>Query: Summarize the moments when user u2 using public transportation. T9. Preparing meals Query: Find the moment(s) in which user u3 was preparing meals at home. T10. Shopping 2 Query: Summarize the moment(s) in which user u3 was doing shopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Performance Measures</head><p>For the Lifelog Rerieval Task evaluation metrics based on NDCG (Normalized Discounted Cumulative Gain) at different depths were used, i.e., N DCG@N , where N varies based on the type of the topics, for the recall oriented topics N was larger (&gt; 20), and for the precision oriented topics N was smaller N (5, 10 or 20).</p><p>In the Lifelog Summarization Task classic metrics were deployed:</p><p>-Cluster Recall at X(CR@X) a metric that assesses how many different clusters from the ground truth are represented among the top X results; -Precision at X(P @X) measures the number of relevant photos among the top X results; -F1-measure at X(F 1@X) the harmonic mean of the previous two.</p><p>Various cut off points were considered, e.g., X = 5, 10, 20, 30, 40, 50. Official ranking metrics this year was the F1-measure@10 or images, which gives equal importance to diversity (via CR@10) and relevance (via P @10).</p><p>Participants were also encouraged to undertake the sub-tasks in an interactive or automatic manner. For interactive submissions, a maximum of five minutes of search time was allowed per topic. In particular, the organizers would like to emphasize methods that allowed interaction with real users (via Relevance Feedback (RF), for example), i.e., beside of the best performance, the way of interaction (like number of iterations using RF), or innovation level of the method (for example, new way to interact with real users) has been evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Participation</head><p>This year, being the fist edition of this challenging task, the participation was not so high but, taking into account the number of teams that downloaded the dataset (11 registered teams signed the copyright form), there are grounds for this number to increase considerably over coming iterations of the task. In total the three groups that took part in the task and submitted overall 19 runs. All three participating groups submitted a working paper describing their system, thus for these there were specific details available:</p><p>-I2R: <ref type="bibr" coords="9,170.20,118.99,10.52,8.74" target="#b7">[8]</ref> The team from Institute for Infocomm Research, A*STAR, Agency for Science Technology and Research (A*STAR), Singapore, represented by Ana Garcia del Molino, Bappaditya Mandal, Jie Lin, Joo Hwee Lim, Vigneshwaran Subbaraju and Vijay Chandrasekhar. -UPB: <ref type="bibr" coords="9,175.92,166.69,10.52,8.74" target="#b2">[3]</ref> The team from University Politehnica of Bucharest, Romania, represented by Mihai Dogariu and Bogdan Ionescu. -Organizers: <ref type="bibr" coords="9,205.40,190.48,15.50,8.74" target="#b12">[13]</ref> The team from Insight Centre for Data Analytics (Dublin City University), University of Cagliari, Simula Research Laboratory, University of Trento, was represented by Liting Zhou, Luca Piras, Michael Rieger, Giulia Boato, Duc-Tien Dang-Nguyen, and Cathal Gurrin.</p><p>Table <ref type="table" coords="9,161.12,247.64,4.98,8.74" target="#tab_3">6</ref> provide the main key details for the submitted runs of each group describing their system for each subtask. This table serves as a summary of the systems, and are also quite illustrative for quick comparisons. For a more in-depth look at the systems of each team, please refer to the corresponding papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results for Subtask 1: Retrieval</head><p>Unfortunately only the Organizers team submitted runs for the Retrieval Subtask <ref type="bibr" coords="9,155.61,344.28,14.61,8.74" target="#b12">[13]</ref>. They proposed an approach composed by several step. First of all they grouped similar moments together based on time and concepts and, applying this chronological-based segmentation, they turned the problem of images retrieval into image segments retrieval. Then, starting from a topic query, they transformed it into small inquiries, where each of them is asking for a single piece of information of concepts, location, activity, and time. The moments that matched all of those requirements are returned as the retrieval results. In the end, in order to remove non-relevant images, a filtering step is applied on the retrieved images, by removing blurred and images that covered mainly by huge object or by the arms of the user. On the Retrieval Subtask the Organizers team submitted 3 runs summarized in Table <ref type="table" coords="9,172.63,475.79,4.98,8.74">7</ref> The first run (baseline) exploited only time and the concepts information. Every single image has been considered as the basic unit and the retrieval just returns all images that contains the concepts extracted from the topics. They submitted this run as reference with the purpose that any other approach should obtain better performance than this. In the second run (Segmentation), the Organizers team introduced also the segmentation so as basic unit of retrieval has been used the segment, not image. In the last run (Fine-Tuning), the "translation" of the query into small inquiries has been applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results for Subtask 2: Summarization</head><p>For Subtask 2, participants were asked to analyse all the lifelog images and summarize them according to specific requirements (see the topics in Tables <ref type="table" coords="9,475.61,620.25,4.98,8.74" target="#tab_2">4</ref> and<ref type="table" coords="9,153.99,632.21,3.87,8.74">5</ref>). All the three teams, I2R <ref type="bibr" coords="9,277.90,632.21,9.96,8.74" target="#b7">[8]</ref>, UPB <ref type="bibr" coords="9,318.85,632.21,10.52,8.74" target="#b2">[3]</ref> and Organizers* <ref type="bibr" coords="9,406.54,632.21,14.61,8.74" target="#b12">[13]</ref>, participated in this subtask. Table <ref type="table" coords="9,232.61,644.16,4.98,8.74" target="#tab_5">8</ref> shows the F1-measure@10, for all submitted runs by participants. Baseline Baseline method, fully automatic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmentation</head><p>Apply segmentation and automatic retrieval based on concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finetunning</head><p>Apply segmentation and fine-tunning. Using all information.</p><p>Lifelog Summarization Subtask.</p><p>I2R <ref type="bibr" coords="10,154.02,274.09,9.72,7.86" target="#b7">[8]</ref> Run and interactive. UPB <ref type="bibr" coords="10,159.01,433.40,9.73,7.86" target="#b2">[3]</ref> Run 1 Textual filtering and word similarity using WordNet and Retina.</p><p>Organizers <ref type="bibr" coords="10,136.23,472.15,14.34,7.86" target="#b12">[13]</ref> Baseline Baseline method, fully automatic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmentation</head><p>Apply segmentation and automatic retrieval and diversification based on concepts. Filtering Apply segmentation, filtering, and automatic diversification. Using all information. Finetunning Apply segmentation, fine-tunning, filtering, and automatic diversification. Using all information. RF Relevance feedback. Using all information.</p><p>Table <ref type="table" coords="10,249.81,543.60,4.13,7.89">7</ref>. Lifelog Retrieval Subtask results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieval Subtask. Team</head><p>Description NDCG</p><p>Organizers <ref type="bibr" coords="10,272.15,597.27,14.34,7.86" target="#b12">[13]</ref> Baseline 0.09 Segmentation 0.14 Fine-Tuning 0.39 I2R achieved the best F1@10 measure (excluding the organizers' runs) of 0.497 by building a multi-step approach. As first step they filtered out unin- formative images, i.e., the ones with very homogeneous colors and with a high blurriness. Then the system ranked the remaining images and clustered the top ranked images into a series of events using either k-means or a hierarchical tree. As final step they selected, in an iterative manner, as many images per cluster as to fill a size budget. They submitted two different sets of runs: automatic (Run 1-3,6-9) and interactive (Run 4, 5, and 10). In the first ones in order to select the key-frames, all frames in each cluster are ranked according to distance to the cluster center (for k-means clustering) or relevance score (for hierarchical trees), then, the selection is sorted according to each frames relevance score. In the interactive process, they give the user the opportunity of removing, replacing and adding frames refining the automatically generated summary. They obtained the best result in the Run 2 where used visual and metadata information and automatic frame selection. It is worth to note that, on the contrary, the organizers team considerably improved the results of theie automatic approach with the Fine Tuning introducing the human-in-the-loop, i.e., thanks to relevance feedback.</p><p>UPB team proposed an approach that combines textual and visual information in the process of selecting the best candidates for the tasks requirements. The run that they submitted relied solely on the information provided by the organizers and no additional annotations or external data, nor feedback from the users had been employed. Additionally, a multi-stage approach has been used. The algorithm starts by analyzing the concept detectors output provided by the organizers and selecting for each image only the most probable concepts. From the list of the topics, each of them has been then parsed such that only relevant words have been kept and information regarding location, activity and the targeted user are extracted as well. The images that did not fit the topic requirements have been removed and this shortlist of images is then subject to a clustering step. Finally, the results are pruned with the help of a similarity scores computed using WordNets builtin similarity distance functions.</p><p>The Organizers team submitted 5 runs for the Summarization Subtask applying the same strategy as in the retrieval subtask, in which the first three runs were to test the automatic approach with the increasing level of the 'criteria' as proposed in <ref type="bibr" coords="12,198.96,214.69,9.96,8.74" target="#b0">[1]</ref>, while the last two runs are used to test the fine tuning and the relevance feedback approaches <ref type="bibr" coords="12,267.60,226.64,9.96,8.74" target="#b1">[2]</ref>. For the relevance feedback approach, they ran a simulation by exploiting the ground-truth annotated data. The results confirm what is highlighted in Section 3.2; applying segmentation improved both retrieval and summarization performance. From Table <ref type="table" coords="12,329.76,262.51,4.98,8.74" target="#tab_5">8</ref> it is quite clear that applying finetunning significantly improved performance but what is worth to note is the big gaps in results between the automatic approach with the fine-tunning and the fine-tunning with the human-in-the-loop (relevance feedback) approaches. This shows that a better natural language processing is needed as well as machine learning studies in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Limitations of the challenge</head><p>The major limitation that we learned from the task is about the difficulty of the topics. Many topics require huge effort on natural language processing to make the system understand the topics, which limit major of the teams, which are mainly from the image retrieval community. We also learned that the scope of the subtasks should be better defined since the summarization subtask already covers the retrieval task. As the result, most of the teams only interested in doing the second subtask.</p><p>As the ultimate goal is to provide insights from lifelogs, the current two subtasks only provide basic information, which is far away meaningful information. Thus, a subtask that better focuses on the quantified self, i.e., knowledge mined from self-tracking data, should be considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussions and Conclusions</head><p>A large gap between signed-up teams and submitted runs from the teams was observed. This can have two reasons, (i) due to the amount of data that has to be processed some teams might not be able to do so. (ii) the task seemed to be very complex requiring participants not just only to process single types of data but different ones such as audio and video, etc. For future iterations of the task it will be important to support teams by providing pre-extracted features or maybe access to hardware for the computation. Nevertheless, the submitted runs show that multimodal analysis is not used often. A closer contact with the teams during the whole task could help to find out individual bottle necks of the teams that prevents them from using other modalities and support them to overcome these bottlenecks. All in all the task was quite successful for the first year and tacking into account that lifelogging is a rather new and not common field. The task helped to raise more awareness for lifelogging in general but also to point at the potential research questions such as the previous mentioned multimodal analysis, system aspects for efficiency, etc. For a possible next iteration of the task the dataset should be enchanced with more data and pre-extracted visual and multi-modal features. Furthermore, a platform should be established that can help the organizers to communicate and support the participants during their participation period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Topics List 2017</head><p>The following tables present the descriptions of 51 search topics for the Image-CLEF 2017 LifeLog Retrieval and Summarization Tasks. Table <ref type="table" coords="15,192.27,177.12,3.87,8.74">9</ref>: Description of search topics for the development set in LRT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T1. Presenting/Lecturing</head><p>Query: Find the moment(s) when user u1 was lecturing a group of people in a classroom environment. Description: A lecture can be in any classroom environment and must contain more than one person in the audience who are sitting down. The moments from entry to exit of the classroom are relevant. A classroom environment has desks and chairs with students. Discussion or lecture encounters in which the audience are standing up, or outside of a classroom environment are not considered relevant. T2. On the Bus or Train Query: Find the moment(s) when user u1 was taking a bus or a train in his home country. Description: The user normally drives a car. On some occasions he takes public transport and leaves the car at home. Moments in which the user is on a train or a bus are relevant only when he is in his home country. Moments in which the user is on public transport in other countries are not relevant. Moments in taxis are also considered non-relevant. T3. Having a Drink Query: Find the moment(s) when user u1 was having a drink in a bar with someone. Description: Any moment in which the user is clearly seen having a beer or other drink in a bar venue is considered relevant. Having a drink in any other location (e.g. a cafe), or without another person present is not considered relevant. The type of drink is not relevant once it is presumed alcoholic in nature and not tea/coffee. T4. Riding a Red (and Blue) Train Query: Find the moment(s) when user u1 was riding a red (and blue) coloured train. Description: In order to be considered relevant, the moment must contain an external view of the red (and blue) train followed by a period of time spent riding the train. Moments that just show a red train in the field of view are not considered relevant if the user does not ride the train. T5. The Rugby Match Query: Find the moment(s) when user u1 was watching rugby football on a television when not at home. Description: Moments that show rugby football on a television when the user is not at home are considered relevant. To be considered relevant the moment(s) must show the entirety or part of the TV screen and be of sufficient duration to indicate the act of observation. It does not matter which teams are playing. Any point from the start to the end of this sports event is consider relevant. T6. Costa Coffee Query: Find the moment(s) when user u1 was in Costa Coffee. Description: Moments that show the user consuming coffee/food in a Costa Coffee outlet are considered relevant. Any other consumption of food / drink is not considered relevant. Costa Coffee is clearly identified by the red coloured logo on the cups or the logo in the environment. The moments from entry to exit of the Costa Coffee outlet are relevant. T7. Antiques Store Query: Find the moment(s) when user u1 was browsing in an antiques store. Description: Moments which show the user browsing for antiques in antiques stores are relevant. If the user exits an antique store and enters another shortly afterwards, then this would be considered two moments. The antiques stores can be identified by the presence of a large number of old objects of art/furniture/decoration arranged on/in display units. T8. ATM Query: Find the moment(s) when user u1 was using an ATM machine. Description: The ATM Machine can be from any bank and in any location. To be relevant, the user must be directly in front of the machine with no people between the user and machine. Moments that show an ATM machine without showing the user directly in front of the machine are not considered relevant. T9. Shopping for Fish Query: Find the moment(s) when user u1 was shopping for fish in the supermarket. Description: To be considered relevant the moment must show the user inside the supermarket on a shopping activity. The user must be clearly shopping and interacting with objects, including fish, in the supermarket. If the user is in a supermarket and does not buy fish, the shopping event is not considered to be relevant. T10. Cycling home Query: Find the moment(s) when user u2 was cycling home from work. Description: The relevant moments must show the user cycling a bicycle from his/her point of view. Cycling home from work is relevant. Cycling to work or cycling to/from other destinations are not considered to be relevant. T11. Shopping Query: Find the moment(s) in which user u2 was grocery shopping in the supermarket.</p><p>Description: To be relevant, the user must clearly be inside a supermarket and shopping. Passing by or otherwise seeing a supermarket are not considered relevant if the user does not enter the supermarket to go shopping. T12. In a Meeting Query: Find the moment(s) in which user u2 was in a meeting at work with 2 or more people. Description: To be considered relevant, the moment must occur at meeting room and must contain at least two colleagues sitting around a table at the meeting. Meetings that occur outside of my place of work are not relevant. T13. Checking the Menu Query: Find the moment(s) when user u2 was standing outside a restaurant checking the menu. Description: To be considered relevant, the user must be checking the menu of a restaurant while outside the restaurant. Reading the menu inside the restaurant is not considered relevant. Other views of restaurants are not considered relevant if the user is not reading the menu outside. T14. Watching TV Query: Find the moment(s) when user u3 was watching TV. Description: To be relevant, TV set must be on and entirely or partially visible during the moments. The user must be watching TV for a period of time not less than 5 minutes. Moments which show the user was watching TV while having meals are considered relevant. Moments in which the user is using desktop computer or laptop to watch TV shows are not considered relevant. T15. Writing Query: Find the moment(s) when user u3 was writing on a paper using a pen or pencil. Description: To be considered relevant the user must be writing some information on a paper using a pen or a pencil. The writing behaviour must be visible. It does not matter what type of pen is being used or the type of paper. It does not matter what the user is writing. T16. Drinking in a Pub Query: Find the moment(s) when user u3 was drinking in a pub with friends or alone. Description: Relevant moments show the user drinking in a pub. Drinking at home or in any place other than a pub are not considered to be relevant. The user may be with a friend, or alone. T1. Using laptop out of office Query: Find the moment(s) in which user u1 was using his laptop outside the working places. Description: To be consider to relevant, the user should use his laptop, for work or for entertainment out of his working place. T2. On stage Query: Find the moment(s) in which user u1 was giving a talk as a presenter/speaker. Description: The user may be sitting or standing on a stage, facing many audience. A microphone should appear occasionally at the front of the user. T3. Shopping in the electronic market. Query: Find the moment(s) in which user u1 was shopping in the electronic market. Description: Find the moment(s) that user u1 was at the electronic market. Spending time at normal supermarket is not considered relevant. T4. Jogging in the park Query: Find the moments(s) in which user u1 was jogging in a park. Description: Find the moment(s) that user u1 was was jogging in a park. Walking or jogging in other places are not considered relevant. T5. In a Meeting 2 Query: Find the moment(s) that user u1 was in a meeting at work. Description: To be considered relevant, the moment must occur at meeting room and must contain at least two colleagues sitting around a table at the meeting. Meetings that occur outside of the work place are not relevant. T6. Watching TV 2 Query: Summarize the moment(s) when user u1 was watching TV. Description: To be relevant, TV set must be on and entirely or partially visible during the moments. Moments which show the user was watching TV while having meals are considered relevant. Moments in which the user is using desktop computer or laptop to watch TV are not considered relevant. T7. Pizza and Friends Query: Find the moment(s) in which user u1 was eating pizza in the restaurant with friends. Description: The location must be a restaurant. The user should be eating the pizza together with his friend(s) (the friends can eat other food). T8. Working in the air. Query: Find the moment(s) in which user u1 was using computer in the airplane. Description: To be relevant, the user must be using computer in an airplane. Using computer for entertainment is not considered relevant. T9. Playing Guitar Query: Find the moment(s) in which user u2 was playing guitar. Description: To be considered relevant, the moment must clearly show the user is playing his guitar. T10. Exercise in the gym Query: Find the moment(s) in which user u2 was doing exercise in the gym. Description: To be considered relevant, the moment must clearly show the user is doing exercise in the gym. Chatting or not doing exercise are not considered relevant. T11. Eating fruits Query: Find the moment(s) in which user u2 was having fruits. Description: To be considered relevant, the moment must clearly show the user is eating some fruit, no matter where and when he was. T12. Brushing or washing face Query: Find the moment(s) in which user u2 was brushing or washing face. Description: To be considered relevant, the moment must clearly show the user is brushing or washing face T13. Eating 2 Query: Find the moment(s) when user was eating or drinking. Description: To be relevant, the images must show entirely or partially visible food/drink. T14. At McDonald Query: Find the moment(s) in which user u2 was at McDonald for eating or just for relaxing. Description: To be considered relevant, the moment must clearly show the user is in McDonald. T15. Viewing a statue Query: Find the moment(s) in which user u2 was viewing a statue. Description: To be considered relevant, the moment must clearly show a statue, at any possible location while the user was standing or walking. T16. ATM Query: Find the moment(s) when user u2 was using an ATM machine. Description: The ATM Machine can be from any bank and in any location. To be relevant, the user must be directly in front of the machine with no people between the user and machine. Moments that show an ATM machine without showing the user directly in front of the machine are not considered relevant. T17. Have party with friends at friends home. Query: Find the moment(s) in which user u3 was attending a party with many friends at a friends home. Description: To be relevant, the user should be at a party his friend's home, whether indoor or outdoor. Some food and drink should be visualized. T18. Shopping in the butchers shop. Query: Find the moment(s) in which user u3 was consuming in the butchers shop. Description: To be relevant, the user must be at the butcher's shop, no matter what the user bought. Buying meet in the supermarket is not relevant. T19. Buying a ticket via ticket machine.</p><p>Query: Find the moment(s) in which user u3 was buying a ticket via ticket machine. Description: The ticket may include movie ticket, food ticket, any transport ticket. Using automatic ticket machine must be relevant, no matter what kinds of ticket and whether the user bought any ticket. Using ATM , Vending machine are not relevant. T20. Shopping 2 Query: Find the moment(s) in which user u3 doing shopping. Description: To be relevant, the user must clearly be inside a supermarket or shopping stores (includes book store, convenience store, pharmacy, etc). Passing by or otherwise seeing a supermarket are not considered relevant if the user does not enter the shop to go shopping. T1. Eating Query: Summarize the moment(s) when user u1 was eating or drinking. Description: User u1 wants to know insight of his eating/drinking habits. He would like to have a summary of what, when, where, and whom together he was eating or drinking. To be relevant, the images must show entirely or partially visible food/drink. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant, even if they are recorded while the user was eating. T2. Social Drinking Query: Summarize the the social drinking habits of user u1. Description: Drinking in a bar, away from home would be considered relevant. Moments drinking alcohol at home would not be considered social drinking. Drinking alone does not classify as social drinking. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant. T3. Shopping Query: Summarize the moment(s) in which user u1 doing shopping. Description: To be relevant, the user must clearly be inside a supermarket or shopping stores (includes book store, convenient store, pharmacy, etc). Passing by or otherwise seeing a supermarket are not considered relevant if the user does not enter the shop to go shopping. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant T4. In a Meeting Query: Summarize the activities of user u2 in a meeting at work. Description: This is an extension of topic 12 from the retrieval subtask. To be considered relevant, the moment must occur at meeting room and must contain at least two colleagues sitting around a table at the meeting. Meetings that occur outside of the work place are not relevant. Different meetings have to be summarized as different activities. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant. T5. Watching TV Query: Summarize the moment(s) when user u3 was watching TV. Description: This is an extension of topic 14 from the retrieval subtask. To be relevant, TV set must be on and entirely or partially visible during the moments. Moments which show the user was watching TV while having meals are considered relevant. Moments in which the user is using desktop computer or laptop to watch TV shows are not considered relevant. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant. T1. In a Meeting 2 Query: Summarize the activities of user u1 in a meeting at work. Description: To be considered relevant, the moment must occur at meeting room and must contain at least two colleagues sitting around a table at the meeting. Meetings that occur outside of the work place are not relevant. Different meetings have to be summarized as different activities. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant. T2. Watching TV 2 Query: Summarize the moment(s) when user u1 was watching TV. Description: To be relevant, TV set must be on and entirely or partially visible during the moments. Moments which show the user was watching TV while having meals are considered relevant. Moments in which the user is using desktop computer or laptop to watch TV shows are not considered relevant. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant. T3. Using laptop outside the office Query: Summarise the moment(s) in which user u1 was using his laptop outside the working places. Description: To be consider to relevant, the user should use his laptop, for working or for entertainment out of his working place. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant. T4. Working at home Query: Find the moment(s) in which user u1 was working at home. Description: To be consider to relevant, the user should be using computer for work, reviewing an article or taking some notes at home. Using computer for entertainment is not relevant. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant. T5. Eating 2 Query: Summarize the moment(s) when user u2 was eating or drinking. Description: User u2 wants to know insight of his eating/drinking habits. He would like to have a summary of what, when, where, and whom together he was eating or drinking. be relevant, the images must show entirely or partially visible food/drink. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant, even if they are recorded while the user was eating. T6. Social Drinking 2 Query: Summarize the the social drinking habits of user u2. Description: Drinking in a bar, away from home would be considered relevant. Moments drinking alcohol at home would not be considered as social drinking. Drinking alone does not classify as social drinking. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant. T7: Sightseeing Query: Summarize the moments when the user u2 seeing street, people, landscape, etc. when he was traveling to other cities or countries. Description: Photos taken inside public transport are not relevant. Sightseeing in his hometown is not relevant. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant. T8. Transporting Query: Summarize the moments when user u2 using public transportation. Description: Photos taken inside a car or a taxi are not relevant. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant. T9. Preparing meals Query: Find the moment(s) in which user u3 was preparing meals at home. Description: To be considered relevant, the moment must clearly show the user is preparing meals in the kitchen. Eating is not relevant. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant. T10. Shopping 2 Query: Summarize the moment(s) in which user u3 was doing shopping. Description: To be relevant, the user must clearly be inside a supermarket or shopping stores (includes book store, convenience store, pharmacy, etc). Passing by or otherwise seeing a supermarket is not considered relevant if the user does not enter the shop to go shopping. Blurred or out of focus images are not relevant. Images that are covered (mostly by the lifelogger's arm) are not relevant.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,145.98,376.17,323.39,7.89;2,150.91,304.55,76.08,56.84"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example lifelogging information need: 'Show me all eating moments'.</figDesc><graphic coords="2,150.91,304.55,76.08,56.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,115.91,345.82,132.53"><head>Table 1 .</head><label>1</label><figDesc>Statistics of Lifelog Dataset. Starbucks cafe, McDonalds restaurant, home, work) and the four physical activities: walking, cycling, running and transport of the lifeloggers at a granularity of one minute. A summary of the data collection is shown in Table</figDesc><table coords="4,134.77,136.68,307.36,87.85"><row><cell>Number of Lifeloggers</cell><cell>3</cell></row><row><cell>Size of the Collection (Images)</cell><cell>88,124 images</cell></row><row><cell cols="2">Size of the Collection (Locations) 130 locations</cell></row><row><cell>Number of LRT Topics</cell><cell>36 (16 for devset, 20 for testset)</cell></row><row><cell>Number of LST Topics</cell><cell>15 (5 for devset, 10 for testset)</cell></row><row><cell>locations (e.g.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,136.56,503.06,342.24,65.72"><head>Table 2 :</head><label>2</label><figDesc>Search topics for the development set in Lifelog Re-</figDesc><table coords="4,136.56,515.02,342.24,53.77"><row><cell>trieval Task</cell></row><row><cell>T1. Presenting/Lecturing</cell></row><row><cell>Query: Find the moment(s) when user u1 was lecturing a group of people in a</cell></row><row><cell>classroom environment.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,151.17,218.22,313.02,108.76"><head>Table 4 :</head><label>4</label><figDesc>Search topics for the development set in Lifelog Summa-</figDesc><table coords="7,151.17,230.18,313.02,96.81"><row><cell>rization Task</cell></row><row><cell>T1. Eating</cell></row><row><cell>Query: Summarize the moment(s) when user u1 was eating or drinking.</cell></row><row><cell>T2. Social Drinking</cell></row><row><cell>Query: Summarize the the social drinking habits of user u1.</cell></row><row><cell>T3. Shopping</cell></row><row><cell>Query: Summarize the moment(s) in which user u1 doing shopping.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,136.23,115.91,289.75,67.02"><head>Table 6 .</head><label>6</label><figDesc>Submitted runs for ImageCLEFlifelog 2017 task.</figDesc><table coords="10,136.23,136.29,231.46,46.64"><row><cell></cell><cell></cell><cell>Lifelog Retrieval Subtask.</cell></row><row><cell>Team</cell><cell>Run</cell><cell>Description</cell></row><row><cell>Organizers</cell><cell></cell><cell></cell></row><row><cell>[13]</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,181.61,115.91,252.14,226.72"><head>Table 8 .</head><label>8</label><figDesc>Lifelog Summarization Subtask results. Results from the organizers team are just for reference.</figDesc><table coords="11,181.61,136.29,225.86,206.35"><row><cell></cell><cell>Summarization Subtask.</cell><cell></cell></row><row><cell>Team</cell><cell>Best Run</cell><cell>F1@10</cell></row><row><cell></cell><cell>Run 1</cell><cell>0.394</cell></row><row><cell></cell><cell>Run 2</cell><cell>0.497</cell></row><row><cell></cell><cell>Run 3</cell><cell>0.311</cell></row><row><cell></cell><cell>Run 4</cell><cell>0.440</cell></row><row><cell>I2R [8]</cell><cell>Run 5 Run 6</cell><cell>0.456 0.397</cell></row><row><cell></cell><cell>Run 7</cell><cell>0.487</cell></row><row><cell></cell><cell>Run 8</cell><cell>0.341</cell></row><row><cell></cell><cell>Run 9</cell><cell>0.356</cell></row><row><cell></cell><cell>Run 10</cell><cell>0.461</cell></row><row><cell>UPB [3]</cell><cell>Run 1</cell><cell>0.132</cell></row><row><cell></cell><cell>Baseline</cell><cell>0.103</cell></row><row><cell></cell><cell>Segmentation</cell><cell>0.172</cell></row><row><cell>Organizers* [13]</cell><cell>Filtering</cell><cell>0.194</cell></row><row><cell></cell><cell>Fine Tuning</cell><cell>0.319</cell></row><row><cell></cell><cell>Relevance Feedback</cell><cell>0.772</cell></row><row><cell>*Note:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="17,173.35,599.99,268.66,8.74"><head>Table 10 :</head><label>10</label><figDesc>Description of search topics for the test set in LRT.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="20,163.68,299.12,288.00,20.69"><head>Table 11 :</head><label>11</label><figDesc>Description of search topics for the development set in LST.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="21,173.83,346.54,267.69,8.74"><head>Table 12 :</head><label>12</label><figDesc>Description of search topics for the test set in LST.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.44,217.03,7.47"><p>http://ntcir-lifelog.computing.dcu.ie/NTCIR12/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,657.44,137.01,7.47"><p>http://www.clef-initiative.eu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,645.84,260.50,8.12"><p>Challenge website at http://www.imageclef.org/2017/lifelog</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,144.73,656.80,305.12,8.12"><p>Dataset available at http://imageclef-lifelog.computing.dcu.ie/2017/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.96,277.93,337.64,7.86;13,151.52,288.89,329.07,7.86;13,151.52,299.85,302.88,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,456.92,277.93,23.67,7.86;13,151.52,288.89,260.59,7.86">A hybrid approach for retrieving diverse social images of landmarks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">G</forename><surname>De Natale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,435.86,288.89,44.73,7.86;13,151.52,299.85,236.53,7.86">2015 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,311.59,337.63,7.86;13,151.52,322.55,329.07,7.86;13,151.52,333.51,329.07,7.86;13,151.52,344.47,107.57,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,455.25,311.59,25.34,7.86;13,151.52,322.55,329.07,7.86;13,151.52,333.51,25.50,7.86">Multimodal retrieval with diversification and relevance feedback for tourist attraction images</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">G</forename><surname>De Natale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,184.14,333.51,183.20,7.86">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Communications, and Applications</publisher>
		</imprint>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct coords="13,142.96,356.21,337.64,7.86;13,151.52,367.17,174.61,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="13,255.75,356.21,224.85,7.86;13,151.52,367.17,75.78,7.86">A Textual Filtering of HOG-based Hierarchical Clustering of Lifelog Data</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,378.91,337.63,7.86;13,151.52,389.87,329.07,7.86;13,151.52,400.83,329.07,7.86;13,151.52,411.79,329.07,7.86;13,151.52,422.75,25.60,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,239.87,389.87,240.73,7.86;13,151.52,400.83,162.53,7.86">Overview of the imageclef 2015 scalable image annotation, localization and sentence generation task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,334.33,400.83,146.26,7.86;13,151.52,411.79,162.05,7.86">Working Notes of CLEF 2015 -Conference and Labs of the Evaluation forum</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">September 8-11, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,434.49,337.64,7.86;13,151.52,445.45,329.07,7.86;13,151.52,456.41,329.07,7.86;13,151.52,465.10,272.66,10.13" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,287.50,445.45,193.09,7.86;13,151.52,456.41,117.61,7.86">Overview of the ImageCLEF 2016 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,293.35,456.41,187.25,7.86;13,151.52,467.37,110.74,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>vora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">September 5-8 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,479.11,337.63,7.86;13,151.52,490.07,329.07,7.86;13,151.52,501.03,329.07,7.86;13,151.52,511.98,329.07,7.86;13,151.52,522.94,329.07,7.86;13,151.52,533.90,329.07,7.86;13,151.52,544.86,199.89,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,433.34,501.03,47.26,7.86;13,151.52,511.98,215.41,7.86">Overview of ImageCLEF 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,387.32,511.98,93.27,7.86;13,151.52,522.94,329.07,7.86;13,151.52,533.90,105.52,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction 8th International Conference of the CLEF Association, CLEF</title>
		<title level="s" coord="13,285.42,533.90,143.89,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-09-11">2017. September 11-14 2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,556.60,337.63,7.86;13,151.52,567.56,329.07,7.86;13,151.52,578.52,329.07,7.86;13,151.52,589.48,329.07,8.12;13,151.52,601.08,94.15,7.47" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,237.79,567.56,238.19,7.86">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1145/2647868.2654889</idno>
		<ptr target="http://doi.acm.org/10.1145/2647868.2654889" />
	</analytic>
	<monogr>
		<title level="m" coord="13,166.40,578.52,293.07,7.86;13,189.91,589.48,31.23,7.86">Proceedings of the 22Nd ACM International Conference on Multimedia</title>
		<meeting>the 22Nd ACM International Conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
	<note>MM &apos;14</note>
</biblStruct>

<biblStruct coords="13,142.96,612.18,337.63,7.86;13,151.52,623.14,329.07,7.86;13,151.52,634.10,159.78,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="13,151.52,623.14,329.07,7.86;13,151.52,634.10,60.95,7.86">VC-I2R@ImageCLEF2017: Ensemble of Deep Learned Features for Lifelog Video Summarization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G D</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Subbaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,645.84,337.63,7.86;13,151.52,656.80,310.27,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,259.08,645.84,221.51,7.86;13,151.52,656.80,101.62,7.86">Overview of the ImageCLEF 2012 Flickr Photo Annotation and Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,274.60,656.80,103.78,7.86">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,119.67,337.98,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,329.07,7.86;14,151.52,152.55,46.58,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,257.71,119.67,222.89,7.86;14,151.52,130.63,65.14,7.86">Overview of the ImageCLEF 2012 Scalable Web Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,435.38,130.63,45.21,7.86;14,151.52,141.59,223.39,7.86">CLEF 2012 Evaluation Labs and Workshop, Online Working Notes</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">September 17-20 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,163.51,337.98,7.86;14,151.52,174.47,329.07,7.86;14,151.52,185.43,329.07,7.86;14,151.52,196.39,22.02,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,262.58,163.51,218.02,7.86;14,151.52,174.47,96.12,7.86">Overview of the ImageCLEF 2014 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,271.70,174.47,107.11,7.86">CLEF2014 Working Notes</title>
		<title level="s" coord="14,387.41,174.47,93.18,7.86;14,151.52,185.43,31.91,7.86">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS.org</publisher>
			<date type="published" when="2014">September 15-18 2014</date>
			<biblScope unit="volume">1180</biblScope>
			<biblScope unit="page" from="308" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,207.34,337.97,7.86;14,151.52,218.30,329.07,7.86;14,151.52,229.26,301.98,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,315.57,207.34,165.02,7.86;14,151.52,218.30,167.36,7.86">Overview of the ImageCLEF 2013 Scalable Concept Image Annotation Subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,342.84,218.30,137.75,7.86;14,151.52,229.26,133.89,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,240.22,337.98,7.86;14,151.52,251.18,329.07,7.86;14,151.52,262.14,177.71,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="14,457.53,240.22,23.06,7.86;14,151.52,251.18,329.07,7.86;14,151.52,262.14,78.86,7.86">Organizer Team at ImageCLEFlifelog 2017: Baseline Approaches for Lifelog Retrieval and Summarization</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
