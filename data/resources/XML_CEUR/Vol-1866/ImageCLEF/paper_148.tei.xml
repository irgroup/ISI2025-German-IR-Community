<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.77,115.96,345.82,12.62;1,263.43,133.89,88.50,12.62">IRIT &amp; MISA at Image CLEF 2017 -Multi label classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,144.70,171.70,82.75,8.74"><forename type="first">Nomena</forename><forename type="middle">Ny</forename><surname>Hoavy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MISA</orgName>
								<orgName type="institution">Univ. Antananarivo</orgName>
								<address>
									<country key="MG">Madagascar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,238.00,171.70,63.02,8.74"><forename type="first">Josiane</forename><surname>Mothe</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">IRIT</orgName>
								<orgName type="laboratory" key="lab2">UMR5505</orgName>
								<orgName type="institution">CNRS &amp; Univ. Toulouse</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.57,171.70,154.62,8.74"><forename type="first">Mamitiana</forename><forename type="middle">Ignace</forename><surname>Randrianarivony</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MISA</orgName>
								<orgName type="institution">Univ. Antananarivo</orgName>
								<address>
									<country key="MG">Madagascar</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.77,115.96,345.82,12.62;1,263.43,133.89,88.50,12.62">IRIT &amp; MISA at Image CLEF 2017 -Multi label classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BD7C08329F0F52F67B4BF6F36D4FA79F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information systems</term>
					<term>Information retrieval</term>
					<term>multi-label classification</term>
					<term>convolutional neural network</term>
					<term>transfer learning</term>
					<term>fine-tuning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe the participation of the Mami team at ImageCLEF 2017 for the Image Caption task. We participated to the concept detection subtask which aims at assigning a set of concept labels to a medical image. We used transfer learning method with VGG19 model for feature extraction to solve this task, and apply those features as input of a new neural network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we describe the participation of the Mami team to ImageCLEF 2017 for the Image Caption task. This team results from a collaboration between IRIT SIG team from the Université de Toulouse (France) and MISA from the Universit d'Antananarivo (Madagascar).</p><p>The Image Caption task consists in two subtasks: concept detection and caption prediction. The main goal of the concept detection task is to identify the existence of relevant biomedical concepts in the medical images delivered by the organizers (see <ref type="bibr" coords="1,236.65,476.21,10.52,8.74" target="#b6">[7]</ref> for a general overview and <ref type="bibr" coords="1,374.36,476.21,10.52,8.74" target="#b3">[4]</ref> for the Caption task overview as well as its web page <ref type="bibr" coords="1,277.75,488.16,14.76,8.74" target="#b9">[10]</ref>).</p><p>This problem can be see as a classification problem where each class corresponds to a concept label. As each image may contain multiple concepts, we tackled this task as a multi-label classification problem. The multi-label classification consists to associate a given instance x i ∈ X to a set of labels Y i = y i1 , .., y ili ∈ Y . Where x i is the instance of the i-st image and Y i the set of concepts x i belongs to. l i is the number of concepts in Y i .</p><p>We used deep learning method to solve this problem. Deep convolutional neural networks are considered among the best classifiers for single-label image classification <ref type="bibr" coords="1,192.99,596.05,15.50,8.74" target="#b11">[12,</ref><ref type="bibr" coords="1,208.48,596.05,11.62,8.74" target="#b13">14]</ref>.</p><p>In this work, we adapt a convolutional neural network with transfers learning to multi-label classification task.</p><p>In the next section we present in Section 2 a brief analysis of the dataset used in the concept detection subtask. We then present in Section 3 the method we developed. Section 4 presents the results while Section 4 concludes the paper.</p><p>The dataset we used was delivered by the image CLEF Lab and contains 3 subsets: the training set is composed of 164,614 images and 20,463 labels; the validation set consists of 10,000 images and 7,070 labels from which 309 are not in the training set nor in the test set. The test set contains 10,000 images. We found that 3.9% of the training images and 3.79% of the validation images have no labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method we developed</head><p>As said previously, we used a convolutional neural network. For the transfer learning <ref type="bibr" coords="2,172.69,456.14,14.61,8.74" target="#b15">[16]</ref>, we used the Oxford VGG19 model from Simonyan et al. <ref type="bibr" coords="2,439.87,456.14,14.61,8.74" target="#b13">[14]</ref>. This model performed well at ImageNet Large Scale Visual Recognition Challenge (ILVRC 2012 <ref type="bibr" coords="2,195.65,480.05,15.50,8.74" target="#b10">[11]</ref>) for classifying images over 1,000 classes <ref type="bibr" coords="2,393.08,480.05,14.61,8.74" target="#b13">[14]</ref>.</p><p>As illustrated in Figure <ref type="figure" coords="2,258.04,492.08,3.87,8.74" target="#fig_0">1</ref>, the Oxford VGG19 model consists in 19 layers. To adapt the pre-trained model<ref type="foot" coords="2,273.49,502.46,3.97,6.12" target="#foot_0">3</ref> to the imageCLEF Concept detection task, we froze the parameters of the 18 first layers and add 2 new layers above it. Our learning consists to train the 2 new layers to map the images to the corresponding concepts. The goal of the training is to assign a positive score to concepts the given image belongs to. The method consists in 2 steps that are described in the following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Processing</head><p>The first step of the process we developed is to forward each image to the VGG19 model and extract the output of the second to last layer of the VGG19 CNN (fully connected layer 7(fc7)). This output, named feature vector, is a 4,096 dimensional vector. This feature vector summarizes the representation of the forwarded image. This representation helps to gain time and space on disk, as suggested by Sharif et al. <ref type="bibr" coords="3,249.93,461.71,14.61,8.74" target="#b12">[13]</ref>. Moreover, Bengio et al. suggested that learning from such a feature vector gives a great potential in various vision recognition tasks <ref type="bibr" coords="3,160.06,485.62,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="3,170.57,485.62,7.01,8.74" target="#b2">3]</ref>.</p><p>Then, the feature vector is sent to the first new layer we add for multi-label classification ; this process results in scores associated to each concept or label. To train our model for assigning positive scores to the relevant concepts, we adopted the sigmoid cross entropy as loss function.</p><p>Let us define: pn = σ(x n ) ∈ [0, 1], σ the sigmoid function, x n the predicted score for the concept n, p n = 1 if the image belongs to the concept n and 0 otherwise. Formally, the sigmoid cross entropy loss <ref type="bibr" coords="3,312.10,610.96,12.09,8.77" target="#b0">[1]</ref> E is :</p><formula xml:id="formula_0" coords="3,207.01,638.06,273.58,30.20">E = -1 n N n=1 [p n log pn + (1 -p n ) log(1 -pn )].<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>We used the test/validation method to train our model performance and its capacity to generalize on new data. For that, we kept the data sets as split by the organizers. We trained our model using the training set. The validation set serves then as a new data set to validate the model performance and to tune the hyper parameters of the model. We trained our model with a stochastic gradient descent method using 0,0001 as learning rate (which corresponds to 1/10 of the pre-trained model initial learning rate) and a batch size of 128. We preserved the momentum default value: 0.9 and set the weight decay 5.10 -4 . After 150 epochs, training stopped and the resulting model was stored although it converges to E=42.547 . The delivered model is applied to predict the concepts in the validation and test data sets; the results are presented in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>The task organizers suggested F1-score <ref type="bibr" coords="4,305.09,335.53,10.52,8.74" target="#b3">[4]</ref> to evaluate the results. F1-score is the average of F1-scores. It represents the weighted average of precision and recall.</p><p>We also added 2 other measures to check the performance of our model on the validation dataset : exact matching ratio measure, Hamming loss. The results are detailed in Table <ref type="table" coords="4,264.39,383.94,3.87,8.74" target="#tab_3">3</ref>. As we predicted a vector, the exact matching ratio corresponds to the percentage of correctly predicted vector elements. This measure does not take into account partially correct predictions. The formula is:</p><formula xml:id="formula_1" coords="4,251.37,429.71,224.98,30.32">M R = 1 N N i=1 I(y i = x i ). (<label>2</label></formula><formula xml:id="formula_2" coords="4,476.35,440.12,4.24,8.74">)</formula><p>where N is the number of samples, y i the ground truth vector, and x i is the prediction vector. The Hamming loss (HamLoss) computes the percentage of labels that are misclassified in a multi-label classification, i.e., relevant labels that are not predicted or irrelevant labels that are predicted <ref type="bibr" coords="4,400.42,507.78,9.96,8.74" target="#b7">[8]</ref>. The formula is given by:</p><formula xml:id="formula_3" coords="4,236.70,541.60,239.64,30.32">HamLoss = 1 N N i=1 xor(x i , y i ) L . (<label>3</label></formula><formula xml:id="formula_4" coords="4,476.35,552.01,4.24,8.74">)</formula><p>where N is the number of samples, L the number of labels, y i the ground truth, and x i is the prediction. Table <ref type="table" coords="4,177.17,608.30,4.98,8.74" target="#tab_0">2</ref> presents the results for our model when evaluated on the validation data set with the different measures as mentioned before. The same configuration of the model was used for our unique run named DET CORRECTED mami resulat.txt that we submitted to ImageCLEF and that was evaluated by the organizers on the test set (see Table <ref type="table" coords="4,237.60,656.12,4.98,8.74" target="#tab_3">3</ref> for the results). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The model we developed to participate to ImageCLEF 2017 uses a transfer learning based on VGG19 and that we adapt for multi-label classification. Using this model, we obtain a low F1-score (0.0462) on the test set. Our result is far from the other teams. We expect to improve this score by using more appropriate machine that will make it possible to make a better training. We would like to make a finer tuning and deeper network. Alternatively, we could experiment with another classifier like SVM to classify the feature vectors that we extracted. Also, one of the main issue we faced was to handle the medical compound images (like formula, graphs, ) that is more complex to process. We envisage to tackle this problem using Region based CNN (R-CNN) <ref type="bibr" coords="5,345.97,405.47,23.30,8.74">[5] [9]</ref>. On other hand, previous works <ref type="bibr" coords="5,162.12,417.42,10.52,8.74" target="#b5">[6]</ref> [15] can be used to extract relevant regions that can be used for a MIML (Multi-Instance Multi-Label) learning <ref type="bibr" coords="5,303.43,429.38,14.61,8.74" target="#b16">[17]</ref>. Unfortunately, our current facilities in terms of computing did not allow us to do more while the competition was still open.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,342.46,345.82,7.89;3,134.77,353.44,345.83,7.86;3,134.77,364.40,345.83,7.86;3,134.77,375.36,345.82,7.86;3,134.77,386.32,199.02,7.86;3,186.64,115.83,242.08,211.85"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Transfer learning with VGG19 CNN. The top 18 layers (conv3-64 to FC7-4096) are frozen and the second to last and last are replaced with a multilabel layer with 20,483 nodes (number of concepts or labels). More specifically, conv3-64 is transformed into FC7-4096 and FC-1000 into FC-20483. This deep neural network model is then trained using sigmoid cross entropy loss function.</figDesc><graphic coords="3,186.64,115.83,242.08,211.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,149.71,191.16,296.91,118.48"><head>Table 2</head><label>2</label><figDesc>presents some characteristics of the training set.</figDesc><table coords="2,168.74,224.66,277.88,84.97"><row><cell></cell><cell>Number of labels per image</cell><cell></cell><cell>Number of images per label</cell></row><row><cell>mean</cell><cell>5.58</cell><cell>mean</cell><cell>44.95</cell></row><row><cell>std</cell><cell>4.47</cell><cell>std</cell><cell>320.06</cell></row><row><cell>min</cell><cell>0.00</cell><cell>min</cell><cell>1.00</cell></row><row><cell>25%</cell><cell>3.00</cell><cell>25%</cell><cell>1.00</cell></row><row><cell>50%</cell><cell>4.00</cell><cell>50%</cell><cell>3.00</cell></row><row><cell>75%</cell><cell>7.00</cell><cell>75%</cell><cell>13.00</cell></row><row><cell>max</cell><cell>75.00</cell><cell>max</cell><cell>17,998.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,134.77,322.62,345.83,18.85"><head>Table 1 .</head><label>1</label><figDesc>Number of labels per image and number of images per label in the training set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,212.15,117.78,191.05,102.59"><head>Table 2 .</head><label>2</label><figDesc>Evaluation on the validation dataset</figDesc><table coords="5,212.15,117.78,191.05,102.59"><row><cell cols="2">Measure Names</cell><cell>Measure Values</cell></row><row><cell cols="2">F1-score</cell><cell>0.047</cell></row><row><cell cols="2">Exact matching ratio</cell><cell>0.041</cell></row><row><cell cols="2">Hamming loss</cell><cell>0.0002</cell></row><row><cell>F1-score</cell><cell cols="2">Run name</cell></row><row><cell>0.046</cell><cell cols="2">DET CORRECTED mami resulat.txt</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,182.15,233.35,251.07,7.89"><head>Table 3 .</head><label>3</label><figDesc>F1-score evaluated by the organizers on the test set.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,645.84,335.87,8.12;2,144.73,657.44,122.39,7.47"><p>The VGG19 model trained on ILSRVC-2014 http://www.robots.ox.ac.uk/ %7Evgg/research/very_deep/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,142.96,505.30,337.64,7.86;5,151.52,516.26,180.55,7.86" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00775</idno>
		<title level="m" coord="5,243.22,505.30,237.38,7.86;5,151.52,516.26,14.75,7.86">A simple squared-error reformulation for ordinal classification</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,142.96,526.83,337.64,7.86;5,151.52,537.79,329.07,7.86;5,151.52,548.75,70.14,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,307.06,526.83,173.54,7.86;5,151.52,537.79,47.38,7.86">Representation learning: A review and new perspectives</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,205.03,537.79,249.84,7.86">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,559.32,337.63,7.86;5,151.52,570.28,206.92,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,273.20,559.32,207.39,7.86;5,151.52,570.28,61.97,7.86">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,220.43,570.28,44.32,7.86">Ann Arbor</title>
		<imprint>
			<biblScope unit="volume">1001</biblScope>
			<biblScope unit="issue">48109</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,580.86,337.64,7.86;5,151.52,591.81,329.07,7.86;5,151.52,602.77,177.39,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,403.70,580.86,76.89,7.86;5,151.52,591.81,329.07,7.86;5,151.52,602.77,25.50,7.86">Overview of Image-CLEFcaption 2017 -image caption prediction and concept detection for biomedical images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,184.34,602.77,82.28,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,613.35,337.64,7.86;5,151.52,624.31,329.07,7.86;5,151.52,635.27,302.72,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,354.18,613.35,126.41,7.86;5,151.52,624.31,205.73,7.86">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,379.58,624.31,101.01,7.86;5,151.52,635.27,218.93,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,645.84,337.63,7.86;5,151.52,656.80,154.44,7.86" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.6962</idno>
		<title level="m" coord="5,309.23,645.84,171.36,7.86">How good are detection proposals, really?</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,142.96,119.67,337.63,7.86;6,151.52,130.63,329.07,7.86;6,151.52,141.59,329.07,7.86;6,151.52,152.55,329.07,7.86;6,151.52,163.51,329.07,7.86;6,151.52,174.47,95.77,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,433.34,141.59,47.26,7.86;6,151.52,152.55,220.84,7.86">Overview of ImageCLEF 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,394.93,152.55,85.66,7.86;6,151.52,163.51,14.79,7.86">CLEF 2017 Proceedings</title>
		<title level="s" coord="6,174.27,163.51,146.18,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">September 11-14 2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,185.43,337.64,7.86;6,151.52,196.39,329.07,7.86;6,151.52,207.34,100.39,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,267.40,185.43,213.19,7.86;6,151.52,196.39,141.10,7.86">Efficient multilabel classification algorithms for largescale problems in the legal domain</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">L</forename><surname>Mencía</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fürnkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,315.89,196.39,143.98,7.86">Semantic Processing of Legal Texts</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="192" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,218.30,337.64,7.86;6,151.52,229.26,329.07,7.86;6,151.52,240.22,104.35,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,301.55,218.30,179.05,7.86;6,151.52,229.26,137.84,7.86">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,310.29,229.26,170.29,7.86;6,151.52,240.22,29.48,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,251.18,337.98,8.12;6,151.52,262.14,110.44,7.86" xml:id="b9">
	<monogr>
		<ptr target="http://imageclef.org/2017/caption/" />
		<title level="m" coord="6,179.25,251.18,76.49,7.86">ImageCLEFcaption</title>
		<imprint>
			<date type="published" when="2017-05">2017. 17-May-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,273.10,337.97,7.86;6,151.52,284.06,329.07,7.86;6,151.52,295.02,329.07,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,345.21,284.06,135.39,7.86;6,151.52,295.02,61.52,7.86">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,219.68,295.02,166.76,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,305.98,337.98,7.86;6,151.52,316.93,329.07,7.86;6,151.52,327.89,219.45,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,305.55,305.98,175.05,7.86;6,151.52,316.93,108.39,7.86">Deep-carving: Discovering visual attributes by carving deep neural nets</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,279.95,316.93,200.64,7.86;6,151.52,327.89,126.18,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3403" to="3412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,338.85,337.98,7.86;6,151.52,349.81,329.07,7.86;6,151.52,360.77,329.07,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,397.08,338.85,83.51,7.86;6,151.52,349.81,175.67,7.86">Cnn features off-theshelf: an astounding baseline for recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,348.14,349.81,132.45,7.86;6,151.52,360.77,246.05,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,371.73,337.97,7.86;6,151.52,382.69,231.27,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="6,278.92,371.73,201.67,7.86;6,151.52,382.69,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,142.62,393.65,337.97,7.86;6,151.52,404.61,329.07,7.86;6,151.52,415.56,25.60,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="6,417.88,393.65,62.71,7.86;6,151.52,404.61,87.13,7.86">Selective search for object recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,246.61,404.61,165.50,7.86">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,426.52,337.97,7.86;6,151.52,437.48,329.07,7.86;6,151.52,448.44,70.14,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="6,347.95,426.52,132.64,7.86;6,151.52,437.48,89.37,7.86">How transferable are features in deep neural networks?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,257.72,437.48,202.96,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,459.40,337.98,7.86;6,151.52,470.36,188.69,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="6,339.71,459.40,137.01,7.86">Multi-instance multi-label learning</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,151.52,470.36,84.25,7.86">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2291" to="2320" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
