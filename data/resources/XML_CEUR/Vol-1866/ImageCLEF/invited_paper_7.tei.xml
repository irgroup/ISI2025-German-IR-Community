<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,168.85,115.96,277.65,12.62;1,168.59,133.89,278.17,12.62;1,192.19,151.82,230.98,12.62">Overview of ImageCLEFcaption 2017 -Image Caption Prediction and Concept Detection for Biomedical Images</title>
				<funder>
					<orgName type="full">Lister Hill National Center for Biomedical Communications</orgName>
					<orgName type="abbreviated">LHNCBC</orgName>
				</funder>
				<funder>
					<orgName type="full">National Library of Medicine</orgName>
					<orgName type="abbreviated">NLM</orgName>
				</funder>
				<funder ref="#_5vvM9Pq">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,164.35,189.55,72.45,8.74"><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
							<email>c.eickhoff@acm.org</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.36,189.55,78.88,8.74"><forename type="first">Immanuel</forename><surname>Schwall</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.79,189.55,106.98,8.74"><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Lister Hill National Center for Biomedical Communications</orgName>
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,281.07,201.50,68.10,8.74"><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,168.85,115.96,277.65,12.62;1,168.59,133.89,278.17,12.62;1,192.19,151.82,230.98,12.62">Overview of ImageCLEFcaption 2017 -Image Caption Prediction and Concept Detection for Biomedical Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EC8AF8F32A4FE4856C64BE9B055BC19C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF 2017</term>
					<term>Caption Prediction</term>
					<term>Image Understanding</term>
					<term>Computer Vision</term>
					<term>Radiology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an overview of the ImageCLEF 2017 caption tasks on the analysis of images from the biomedical literature. Two subtasks were proposed to the participants: a concept detection task and caption prediction task, both using only images as input. The two subtasks tackle the problem of providing image interpretation by extracting concepts and predicting a caption based on the visual information of an image alone. A dataset of 184,000 figure-caption pairs from the biomedical open access literature (PubMed Central) are provided as a testbed with the majority of them as trainign data and then 10,000 as validation and 10,000 as test data. Across two tasks, 11 participating groups submitted 71 runs. While the domain remains challenging and the data highly heterogeneous, we can note some surprisingly good results of the difficult task with a quality that could be beneficial for health applications by better exploiting the visual content of biomedical figures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Interpreting and summarizing the insights gained from medical images such as radiography or biopsy samples is a time-consuming task that involves highly trained experts and often represents a bottleneck in clinical diagnosis pipelines. As a consequence, there is a considerable need for automatic methods that can approximate the mapping from visual information to condensed textual descriptions. ImageCLEF<ref type="foot" coords="1,214.91,599.32,3.97,6.12" target="#foot_0">4</ref> is an evaluation campaign that has being organized as part of the CLEF initiative labs since 2003 <ref type="bibr" coords="1,298.50,612.85,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,310.68,612.85,7.01,8.74" target="#b1">2]</ref>. The campaign offers several research tasks that welcome participation from teams around the world and change from year to year <ref type="bibr" coords="1,193.11,636.76,9.96,8.74" target="#b2">[3]</ref>. In 2017, the caption task of ImageCLEF 2017 addresses the problem of image understanding as a cross-modality matching scenario in which visual content and textual descriptors need to be aligned and concise textual interpretations of medical images are generated. A similar task was proposed in 2016 but without any submission <ref type="bibr" coords="2,300.42,154.86,9.96,8.74" target="#b3">[4]</ref>, as it is a very challenging task. The task is based on a large-scale collection of figures from open access biomedical journal articles from PubMed Central (PMC) <ref type="foot" coords="2,335.18,177.20,3.97,6.12" target="#foot_1">5</ref> . Each image is accompanied by its original caption and a set of extracted UMLS R (Unified Medical Language System R ) <ref type="foot" coords="2,178.41,201.11,3.97,6.12" target="#foot_2">6</ref> Concept Unique Identifiers (CUIs), constituting a natural testbed for this image captioning task. A subset of PMC concentrating on clinical images and limiting the number of compound figures is used.</p><p>This paper gives an overview of the caption task at ImageCLEF 2017. Section 2 introduces the two subtasks and Section 3 the data set and ground truth. A description of the evaluation methodology is provided in Section 4. Subsequently, the participant submissions are analysed in Section 5 and Section 6 briefly discusses their respective strengths and weaknesses as well as their implications for academic research and medical practice. Finally, we conclude with an outlook to the possible future of the evaluation campaign in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks</head><p>This first edition of the biomedical image captioning task at ImageCLEF comprises two sub tasks: (1) Concept Detection and (2) Caption Prediction. Figure <ref type="figure" coords="2,475.61,393.58,4.98,8.74" target="#fig_1">1</ref> shows an example image of a tomographic angiography reconstruction along with its relevant concepts as well as the reference caption.</p><p>Concept Detection As a first step towards automatic image caption understanding, participating systems are tasked with identifying the presence of relevant biomedical concepts in medical images. Based on the visual image content, this subtask provides the building blocks for the image understanding step by identifying the individual components from which full captions can be composed.</p><p>Caption Prediction On the basis of the concept vocabulary detected in the first subtask as well as the visual information of their interaction in the image, participating systems are tasked with composing coherent natural language captions for the entirety of an image. In this step, rather than the mere coverage of visual concepts, detecting the interplay of visible elements is crucial for recreating the original image caption.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Collection</head><p>The experimental corpus is derived from scholarly biomedical articles on PMC from which we extract figures and their corresponding captions. In total, the collection is comprised of 184,614 image-caption pairs. This overall set is further split into disjunct training (164,614 pairs), validation (10,000 pairs) and test (10,000 pairs) sets. For the concept detection sub task, we used the QuickUMLS library <ref type="bibr" coords="3,167.20,577.51,10.52,8.74" target="#b4">[5]</ref> to identify all UMLS concepts mentioned in the caption text.</p><p>The subset of PMC was created using an automated method to classify all 3 million images of PMC from early 2016 into image types <ref type="bibr" coords="3,383.07,608.30,10.52,8.74" target="#b5">[6]</ref> fully automatically. We keep clinical image types and remove compound figures. As PMC contains many compound figures and as the method was fully automatic we have approximately 10-20% of the images that are either compound or non-clinical, which creates noise in the data set and makes the task even more challenging.</p><p>The evaluation of both sub tasks is conducted separately. For the concept detection task, we measure the balanced precision and recall trade-off in terms of F 1 scores. To this end, we use Python's scikit-learn (v0.17.1-2) library. We compute micro F 1 per image and average across all test images. A total of 393 reference captions in the test set do not contain any UMLS concepts. The respective images are excluded from the evaluation.</p><p>Caption prediction performance is assessed on the basis of BLEU scores [7] using the Python NLTK (v3.2.2) default implementation. Candidate captions are lower cased, stripped of all punctuation and English stop words. Finally, to increase coverage, we apply Snowball stemming. BLEU scores are computed per reference image, treating each entire caption as a sentence, even though it may contain multiple natural sentences. We report average BLEU scores across all 10,000 test images.</p><p>The source code of both evaluation scripts is available on the task Web page<ref type="foot" coords="4,473.36,298.16,3.97,6.12" target="#foot_3">7</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We received a total of 71 submissions by 11 individual teams. Table <ref type="table" coords="4,436.92,357.55,4.98,8.74" target="#tab_0">1</ref> gives an overview of all participants and their runs. There was a limit of at most 10 runs per team and sub task and the submissions are roughly evenly split between tasks. The call for contributions did not initially make any assumptions about the kinds of strategies and external data that participants would rely on. As a consequence, in this first edition of the task, we see a broad range of performance scores as well as methods being applied. Evaluation of the results showed that some teams employed methods that were at least partially trained on external resources including PMC articles. Since such approaches cannot be guaranteed to have respected our division into training, validation and test folds and might subsequently leak test examples into the training process, we separately list runs relying exclusively on the official collection as well as those making use of external information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Concept Detection</head><p>The concept detection task received 37 runs from 9 participating groups. Table <ref type="table" coords="4,475.61,552.85,4.98,8.74" target="#tab_1">2</ref> lists the performance of all official (no external information used) runs. The global overview of all runs, including those using external information, can be found in Table <ref type="table" coords="4,201.73,588.71,3.87,8.74" target="#tab_2">3</ref>.</p><p>The vast majority of runs was purely automatic (A) in nature with only few submissions relying on some form of manual intervention (M). There was no noticeable advantage of relying on manual interventions as all manual runs lie well in the center of the performance score range. While most teams rely on some form of convolutional neural networks to represent visual information (NLM <ref type="bibr" coords="5,272.84,422.20,14.61,8.74" target="#b17">[18]</ref>, PRNA <ref type="bibr" coords="5,324.99,422.20,9.96,8.74" target="#b8">[9]</ref>, BMET <ref type="bibr" coords="5,373.26,422.20,14.61,8.74" target="#b11">[12]</ref>, AAI <ref type="bibr" coords="5,414.89,422.20,9.96,8.74" target="#b7">[8]</ref>, MAMI <ref type="bibr" coords="5,462.33,422.20,14.61,8.74" target="#b14">[15]</ref>, MUPB <ref type="bibr" coords="5,169.67,434.16,14.76,8.74" target="#b15">[16]</ref>), some chose more traditional bag-of-visual-words representations (IPL <ref type="bibr" coords="5,159.57,446.11,14.61,8.74" target="#b12">[13]</ref>, MSU <ref type="bibr" coords="5,208.62,446.11,15.50,8.74" target="#b16">[17]</ref>) or even relied on mixtures of both representation types (UAPT <ref type="bibr" coords="5,171.56,458.07,14.76,8.74" target="#b10">[11]</ref>). While, on average, CNN-based models seem to deliver more robust results, some of the most competitive submissions are purely based on traditional features. The use of very deep residual networks (AAI <ref type="bibr" coords="5,352.11,497.32,9.96,8.74" target="#b7">[8]</ref>, MAMI <ref type="bibr" coords="5,402.24,497.32,14.61,8.74" target="#b14">[15]</ref>, MUPB <ref type="bibr" coords="5,458.45,497.32,14.76,8.74" target="#b15">[16]</ref>), on average, did not introduce significant improvements over shallower CNN versions. On top of the basic image representation approaches, we see a broad range of affiliate techniques used for recognizing bio-medical concepts. PRNA <ref type="bibr" coords="5,449.49,533.18,10.52,8.74" target="#b8">[9]</ref> successfully rely on attention models for image understanding which seems to introduce a considerable relative advantage over other model variants. The use of convolutional de-noising auto-encoders (UAPT <ref type="bibr" coords="5,343.01,569.05,15.50,8.74" target="#b10">[11]</ref>) for unsupervised representation learning did not seem to lead to considerable improvements.</p><p>Several groups included retrieval-based methods that would identify highly visually related images in the official training set (IPL <ref type="bibr" coords="5,383.98,608.30,14.61,8.74" target="#b12">[13]</ref>, MSU <ref type="bibr" coords="5,433.10,608.30,15.50,8.74" target="#b16">[17]</ref>) or an external collection of images (NLM <ref type="bibr" coords="5,294.91,620.25,14.76,8.74" target="#b17">[18]</ref>). The captions of such related images are then scanned for bio-medical concepts to be assigned to the candidate image. This approach generally resulted in very good results, among them several of the best-performing submissions for the task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Caption Prediction</head><p>The harder caption prediction task received 34 runs from 5 participating groups. Table <ref type="table" coords="7,162.17,150.93,4.98,8.74" target="#tab_3">4</ref> lists the performance of all official (no external information used) runs.</p><p>The global overview of all runs, including those using external information, can be found in Table <ref type="table" coords="7,215.30,174.84,3.87,8.74" target="#tab_4">5</ref>. For this task, no manual runs were submitted. Most submitted runs are based on the teams' respective contributions to the concept detection task expanded by language modeling capabilities. Often this takes the form of recurrent neural networks (ISIA <ref type="bibr" coords="7,358.90,210.71,14.61,8.74" target="#b13">[14]</ref>, BCSG <ref type="bibr" coords="7,412.38,210.71,14.61,8.74" target="#b9">[10]</ref>, PRNA <ref type="bibr" coords="7,467.31,210.71,9.96,8.74" target="#b8">[9]</ref>, BMET <ref type="bibr" coords="7,168.25,222.67,14.76,8.74" target="#b11">[12]</ref>), making the CNN + LSTM combination a frequently-used setup.</p><p>As for the first sub task, the use of retrieval-based methods to identify highly visually related images and using their captions as a starting point for candidate caption generation (ISIA <ref type="bibr" coords="7,243.43,258.54,15.50,8.74" target="#b13">[14]</ref> MSU <ref type="bibr" coords="7,285.78,258.54,14.61,8.74" target="#b16">[17]</ref>, NLM <ref type="bibr" coords="7,331.60,258.54,15.50,8.74" target="#b17">[18]</ref>) resulted in highly competitive performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>There are several observations that should be taken into account when analyzing the results presented in the previous section. Most notably, as a consequence of the data source (scholarly biomedical journal articles), the collection contains a considerable amount of noise in the form of compound figures with potentially highly heterogeneous content. In future editions of this task, we will consider using a less diverse source of images such as radiology/pathology in order to reduce the amount of variation in the data.</p><p>Secondly, the UMLS concept extraction employed here is a probabilistic process that introduces its own errors. As a consequence, there are several training captions that do not contain any UMLS concepts, making such examples difficult to use for concept detection purposes. In the future, we will rely on more rigorous filtering to ensure good concept coverage across training, validation and test data.</p><p>Finally, there should have been a clearer specification of what external material, if any, is permissible for use. The teams employed a wide number of third-party material ranging from general academic collections such as Ima-geNet, mainly in the form of pre-trained networks, to corpora of scholarly articles. While the former do not represent a major problem, the latter could, conceivably contain the exact image and caption pairs of our test set, the use of which would create a strong advantage and a non-realistic setting for really novel data. The experimental overview shows some evidence of this happening when methods using PubMed Central images in the training step vastly outperform all competitors on both tasks. For this reason, we made the conservative decision to separate between official runs using no external information at all and those that used third-party material. In the future, we will more carefully specify which kind of external material is safe to use. It does make sense to allow for external data to be used but it needs to be made clear that no test data are included. This paper presents an overview of the ImageCLEF 2017 biomedical image captioning task. We consider the sub tasks of concept detection and full caption prediction. The participating groups investigated the use of a wide range of image understanding techniques. Especially neural network methods are highly popular and delivered convincing performance on these hard problems. The individually relatively low scores motivate further homogenization of tasks and collection in future editions of the challenge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,292.90,119.05,29.57,7.50;3,266.02,266.71,83.31,7.50;3,150.13,286.59,91.06,7.50;3,150.13,297.00,91.98,7.50;3,150.13,307.41,70.81,7.50;3,150.13,317.82,139.22,7.50;3,150.13,328.23,137.85,7.50;3,150.13,338.64,187.48,7.50;3,150.13,349.05,234.21,7.50;3,264.72,368.93,85.92,7.50;3,143.81,379.37,327.74,7.47;3,158.43,389.78,298.49,7.47"><head></head><label></label><figDesc>Aneurysm -C0002978: angiogram -C0027530: Neck -C0087111: Therapeutic procedure -C0524425: inside the blood vessel -C0524865: Reconstructive Surgical Procedures -C3887704: treatment -ActInformationManagementReason Caption prediction: Preoperative computed tomographic angiography reconstruction showing hostile neck anatomy amenable to treatment with endovascular aneurysm sealing (EVAS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,149.61,411.98,316.13,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of an image and the information provided in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,172.88,115.91,272.03,258.52"><head>Table 1 .</head><label>1</label><figDesc>Participating Groups.</figDesc><table coords="5,172.88,138.83,272.03,235.60"><row><cell>Team</cell><cell>Institution</cell><cell cols="2"># Runs T1 # Runs T2</cell></row><row><cell>AAI [8]</cell><cell>AI Lab, University of the Aegean, Mytilene,</cell><cell>1</cell><cell>0</cell></row><row><cell></cell><cell>Greece</cell><cell></cell><cell></cell></row><row><cell cols="2">PRNA [9] Artificial Intelligence Lab, Philips Research North</cell><cell>3</cell><cell>4</cell></row><row><cell></cell><cell>America, Cambridge, MA, USA</cell><cell></cell><cell></cell></row><row><cell cols="2">BCSG [10] Biomedical Computer Science Group, University</cell><cell>0</cell><cell>10</cell></row><row><cell></cell><cell>of Applied Sciences and Arts Dortmund, Germany</cell><cell></cell><cell></cell></row><row><cell cols="2">UAPT [11] Institute of Electronics and Informatics Engineer-</cell><cell>3</cell><cell>0</cell></row><row><cell></cell><cell>ing, University of Aveiro, Portugal</cell><cell></cell><cell></cell></row><row><cell cols="2">BMET [12] School of Information Technologies, University of</cell><cell>3</cell><cell>4</cell></row><row><cell></cell><cell>Sydney, Australia</cell><cell></cell><cell></cell></row><row><cell>IPL [13]</cell><cell>Information Processing Laboratory, Athens Uni-</cell><cell>10</cell><cell>0</cell></row><row><cell></cell><cell>versity of Economics and Business, Athens,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Greece</cell><cell></cell><cell></cell></row><row><cell cols="2">ISIA [14] Key Laboratory of Intelligent Information Pro-</cell><cell>0</cell><cell>10</cell></row><row><cell></cell><cell>cessing, Chinese Academy of Sciences, Beijing,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>China</cell><cell></cell><cell></cell></row><row><cell cols="2">MAMI [15] CNRS, University of Toulouse, France, &amp; Univer-</cell><cell>2</cell><cell>0</cell></row><row><cell></cell><cell>sity of Antananarivo, Madagascar</cell><cell></cell><cell></cell></row><row><cell cols="2">MUPB [16] University Politehnica of Bucharest, Romania,</cell><cell>1</cell><cell>0</cell></row><row><cell></cell><cell>University of Applied Sciences Western Switzer-</cell><cell></cell><cell></cell></row><row><cell></cell><cell>land, Sierre &amp; University of Geneva, Switzerland</cell><cell></cell><cell></cell></row><row><cell cols="2">MSU [17] Computer Science Department, Morgan State</cell><cell>4</cell><cell>0</cell></row><row><cell></cell><cell>University, Baltimore, MD, USA</cell><cell></cell><cell></cell></row><row><cell cols="2">NLM [18] Lister Hill National Center for Biomedical</cell><cell>10</cell><cell>6</cell></row><row><cell></cell><cell>Communications, National Library of Medicine,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Bethesda, MD, USA</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,123.78,345.83,320.04"><head>Table 2 .</head><label>2</label><figDesc>Concept detection performance in terms of F1 scores without the use of external resources.</figDesc><table coords="6,236.43,156.98,142.51,286.84"><row><cell cols="2">Team Run</cell><cell>Type F1</cell></row><row><cell>IPL</cell><cell cols="2">1494006128917 A 0.1436</cell></row><row><cell>IPL</cell><cell cols="2">1494006074473 A 0.1418</cell></row><row><cell>IPL</cell><cell cols="2">1494009510297 A 0.1417</cell></row><row><cell>IPL</cell><cell cols="2">1494006054264 A 0.1415</cell></row><row><cell>IPL</cell><cell cols="2">1494009412127 A 0.1414</cell></row><row><cell>IPL</cell><cell cols="2">1494009455073 A 0.1394</cell></row><row><cell>IPL</cell><cell cols="2">1494006225031 A 0.1365</cell></row><row><cell>IPL</cell><cell cols="2">1494006181689 A 0.1364</cell></row><row><cell>IPL</cell><cell cols="2">1494006414840 A 0.1212</cell></row><row><cell>IPL</cell><cell cols="2">1494006360623 A 0.1208</cell></row><row><cell cols="3">BMET 1493791786709 A 0.0958</cell></row><row><cell cols="3">BMET 1493791318971 A 0.0880</cell></row><row><cell cols="3">NLM 1494013963830 A 0.0880</cell></row><row><cell cols="3">NLM 1494014008563 A 0.0868</cell></row><row><cell cols="3">BMET 1493698613574 A 0.0838</cell></row><row><cell cols="3">NLM 1494013621939 A 0.0811</cell></row><row><cell cols="3">NLM 1494013664037 A 0.0695</cell></row><row><cell cols="3">MSU 1494060724020 M 0.0498</cell></row><row><cell cols="3">UAPT 1493841144834 M 0.0488</cell></row><row><cell cols="3">UAPT 1493995613907 M 0.0463</cell></row><row><cell cols="3">MSU 1494049613114 M 0.0461</cell></row><row><cell cols="3">MSU 1494048615677 M 0.0434</cell></row><row><cell cols="3">UAPT 1493976564810 M 0.0414</cell></row><row><cell cols="3">MSU 1494048330426 A 0.0273</cell></row><row><cell cols="3">NLM 1494012725738 A 0.0012</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,471.67,345.82,179.32"><head>Table 3 .</head><label>3</label><figDesc>Concept detection performance of runs using external resources; the exact type of third-party material is indicated.</figDesc><table coords="6,185.56,506.62,244.24,144.37"><row><cell cols="2">Team Run</cell><cell>Type</cell><cell>Resources</cell><cell>F1</cell></row><row><cell cols="5">NLM 1494012568180 A Open-i indexed PubMed 0.1718</cell></row><row><cell cols="5">NLM 1494012586539 A Open-i indexed PubMed 0.1648</cell></row><row><cell>AAI</cell><cell cols="4">1491857120689 A ImageNet &amp; MS COCO 0.1583</cell></row><row><cell cols="5">NLM 1494014122269 A Open-i indexed PubMed 0.1390</cell></row><row><cell cols="5">NLM 1494012605475 A Open-i indexed PubMed 0.1228</cell></row><row><cell cols="3">PRNA 1493823116836 A</cell><cell>ImageNet</cell><cell>0.1208</cell></row><row><cell cols="3">MAMI 1496127572481 M</cell><cell>ImageNet</cell><cell>0.0462</cell></row><row><cell cols="3">PRNA 1493823633136 A</cell><cell>ImageNet</cell><cell>0.0234</cell></row><row><cell cols="3">PRNA 1493823760708 A</cell><cell>ImageNet</cell><cell>0.0215</cell></row><row><cell cols="5">NLM 1495446212270 A Open-i indexed PubMed 0.0162</cell></row><row><cell cols="3">MUPB 1493803509469 A</cell><cell>ImageNet</cell><cell>0.0028</cell></row><row><cell cols="3">MAMI 1493631868847 M</cell><cell>ImageNet</cell><cell>0.0000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,134.77,302.02,345.82,232.37"><head>Table 4 .</head><label>4</label><figDesc>Caption prediction performance in terms of BLEU scores without the use of external resources.</figDesc><table coords="7,248.19,335.22,118.98,199.17"><row><cell>Team Run</cell><cell>BLEU</cell></row><row><cell cols="2">ISIA 1493921574200 0.2600</cell></row><row><cell cols="2">ISIA 1493666388885 0.2507</cell></row><row><cell cols="2">ISIA 1493922473076 0.2454</cell></row><row><cell cols="2">ISIA 1494002110282 0.2386</cell></row><row><cell cols="2">ISIA 1493922527122 0.2315</cell></row><row><cell cols="2">NLM 1494038340934 0.2247</cell></row><row><cell cols="2">ISIA 1493831729114 0.2240</cell></row><row><cell cols="2">ISIA 1493745561070 0.2193</cell></row><row><cell cols="2">ISIA 1493715950351 0.1953</cell></row><row><cell cols="2">ISIA 1493528631975 0.1912</cell></row><row><cell cols="2">ISIA 1493831517474 0.1684</cell></row><row><cell cols="2">NLM 1494038056289 0.1384</cell></row><row><cell cols="2">NLM 1494037493960 0.1131</cell></row><row><cell cols="2">BMET 1493702564824 0.0982</cell></row><row><cell cols="2">BMET 1493698682901 0.0851</cell></row><row><cell cols="2">BMET 1494020619666 0.0826</cell></row><row><cell cols="2">BMET 1493701062845 0.0656</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,134.77,115.91,345.83,234.11"><head>Table 5 .</head><label>5</label><figDesc>Caption prediction performance of runs using external resources; the exact type of third-party material is indicated.</figDesc><table coords="8,197.96,150.86,219.44,199.17"><row><cell>Team Run</cell><cell>Resources</cell><cell>BLEU</cell></row><row><cell cols="3">NLM 1494014231230 Open-i indexed PubMed 0.5634</cell></row><row><cell cols="3">NLM 1494081858362 Open-i indexed PubMed 0.3317</cell></row><row><cell>PRNA 1493825734124</cell><cell>ImageNet</cell><cell>0.3211</cell></row><row><cell cols="3">NLM 1495446212270 Open-i indexed PubMed 0.2646</cell></row><row><cell>PRNA 1493824027725</cell><cell>ImageNet</cell><cell>0.2638</cell></row><row><cell>PRNA 1493825504037</cell><cell>ImageNet</cell><cell>0.1801</cell></row><row><cell>PRNA 1493824818237</cell><cell>ImageNet</cell><cell>0.1107</cell></row><row><cell>BCSG 1493885614229</cell><cell>ImageNet</cell><cell>0.0749</cell></row><row><cell>BCSG 1493885575289</cell><cell>ImageNet</cell><cell>0.0675</cell></row><row><cell>BCSG 1493885210021</cell><cell>ImageNet</cell><cell>0.0624</cell></row><row><cell>BCSG 1493885397459</cell><cell>ImageNet</cell><cell>0.0537</cell></row><row><cell>BCSG 1493885352146</cell><cell>ImageNet</cell><cell>0.0527</cell></row><row><cell>BCSG 1493885286358</cell><cell>ImageNet</cell><cell>0.0411</cell></row><row><cell>BCSG 1493885541193</cell><cell>ImageNet</cell><cell>0.0375</cell></row><row><cell>BCSG 1493885499624</cell><cell>ImageNet</cell><cell>0.0365</cell></row><row><cell>BCSG 1493885708424</cell><cell>ImageNet</cell><cell>0.0326</cell></row><row><cell>BCSG 1493885450000</cell><cell>ImageNet</cell><cell>0.0200</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="1,144.73,656.80,86.04,7.86"><p>http://imageclef.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="2,144.73,623.92,335.86,7.86;2,144.73,634.88,335.86,7.86;2,144.73,645.84,221.86,7.86"><p>PubMed Central (PMC) is a free fulltext archive of biomedical and life sciences journal literature at the U.S. National Institute of Healths National Library of Medicine (NIH/NLM) (see http://www.ncbi.nlm.nih.gov/pmc/).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="2,137.50,655.03,3.65,5.24;2,144.73,656.80,163.01,7.86"><p><ref type="bibr" coords="2,137.50,655.03,3.65,5.24" target="#b5">6</ref> https://www.nlm.nih.gov/research/umls</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="4,144.73,656.80,138.77,7.86"><p>http://imageclef.org/2017/caption</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported in part by the <rs type="programName">Intramural Research Program</rs> of the <rs type="funder">National Institutes of Health (NIH)</rs>, <rs type="funder">National Library of Medicine (NLM)</rs>, and <rs type="funder">Lister Hill National Center for Biomedical Communications (LHNCBC)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5vvM9Pq">
					<orgName type="program" subtype="full">Intramural Research Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,349.92,337.64,7.86;9,151.52,360.88,329.07,7.86;9,151.52,371.84,329.07,7.86;9,151.52,382.77,269.83,7.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,264.01,360.88,216.58,7.86;9,151.52,371.84,324.67,7.86">Evaluating performance of biomedical image retrieval systems: Overview of the medical image retrieval task at ImageCLEF 2004-2014</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,151.52,382.80,183.45,7.86">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="55" to="61" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,393.76,337.63,7.86;9,151.52,404.72,329.07,7.86;9,151.52,415.68,329.07,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,388.76,393.76,91.83,7.86;9,151.52,404.72,207.52,7.86">ImageCLEF -Experimental Evaluation in Visual Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,426.66,404.72,53.93,7.86;9,151.52,415.68,182.45,7.86">The Springer International Series On Information Retrieval</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,426.64,337.64,7.86;9,151.52,437.60,329.07,7.86;9,151.52,448.56,329.07,7.86;9,151.52,459.52,329.07,7.86;9,151.52,470.47,313.03,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,449.27,448.56,31.32,7.86;9,151.52,459.52,188.21,7.86">General overview of ImageCLEF at the CLEF 2015 labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kazi Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D M</forename><surname>Roldán García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,360.28,459.52,116.11,7.86">Working Notes of CLEF 2015</title>
		<title level="s" coord="9,151.52,470.47,141.41,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,481.44,337.63,7.86;9,151.52,492.40,329.07,7.86;9,151.52,503.36,329.07,7.86;9,151.52,514.31,329.07,7.86;9,151.52,525.27,329.07,7.86;9,151.52,536.23,71.19,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,449.27,503.36,31.32,7.86;9,151.52,514.31,200.11,7.86">General overview of ImageCLEF at the CLEF 2016 labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Toselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,378.28,514.31,98.09,7.86">CLEF 2016 Proceedings</title>
		<title level="s" coord="9,222.81,525.27,142.19,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Evora. Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-09">September 2016</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,547.20,337.64,7.86;9,151.52,558.15,228.99,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,263.54,547.20,217.05,7.86;9,151.52,558.15,73.14,7.86">Quickumls: a fast, unsupervised approach for medical concept extraction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,246.77,558.15,100.36,7.86">MedIR Workshop, SIGIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,569.12,337.64,7.86;9,151.52,580.08,329.07,7.86;9,151.52,591.04,122.35,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,437.12,569.12,43.47,7.86;9,151.52,580.08,310.29,7.86">Creating a classification of image types in the medical literature for visual categorization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,151.52,591.04,89.30,7.86">SPIE Medical Imaging</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,602.00,337.63,7.86;9,151.52,612.96,329.07,7.86;9,151.52,623.92,329.07,7.86;9,151.52,634.88,77.87,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,351.45,602.00,129.14,7.86;9,151.52,612.96,133.38,7.86">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,307.18,612.96,173.41,7.86;9,151.52,623.92,329.07,7.86;9,151.52,634.88,13.87,7.86">Proceedings of the 40th annual meeting on association for computational linguistics, Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics, Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,645.84,337.63,7.86;9,151.52,656.80,133.99,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,280.97,645.84,199.62,7.86;9,151.52,656.80,99.99,7.86">Concept detection on medical images using deep residual learning network</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Katsios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,119.67,337.64,7.86;10,151.52,130.63,329.07,7.86;10,151.52,141.59,187.76,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,332.40,130.63,148.19,7.86;10,151.52,141.59,154.26,7.86">PRNA at ImageCLEF 2017 caption prediction and concept detection tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sreenivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Swisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,152.55,337.98,7.86;10,151.52,163.51,135.83,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,259.48,152.55,221.11,7.86;10,151.52,163.51,101.90,7.86">Keyword generation for biomedical image retrieval with recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,174.47,337.97,7.86;10,151.52,185.43,329.07,7.86;10,151.52,196.39,173.75,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pinho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Figueira Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Ferreira Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Costa</surname></persName>
		</author>
		<title level="m" coord="10,419.08,174.47,61.51,7.86;10,151.52,185.43,329.07,7.86;10,151.52,196.39,120.00,7.86">Towards representation learning for biomedical concept detection in medical images: UA.PT bioinformatics in ImageCLEF</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,207.34,337.98,7.86;10,151.52,218.30,132.91,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,298.82,207.34,181.77,7.86;10,151.52,218.30,99.33,7.86">Neural captioning for the ImageCLEF 2017 medical image challenges</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lyndon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,229.26,337.98,7.86;10,151.52,240.22,144.57,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,282.92,229.26,193.82,7.86">IPL at ImageCLEF 2017 concept detection task</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Valavanis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Stathopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,240.22,82.27,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,251.18,337.98,7.86;10,151.52,262.14,48.94,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,321.33,251.18,159.26,7.86;10,151.52,262.14,15.40,7.86">ISIA at ImageCLEF 2017 image caption task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,273.10,337.97,7.86;10,151.52,284.06,157.27,7.86" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,357.55,273.10,123.04,7.86;10,151.52,284.06,124.03,7.86">IRIT &amp; MISA at ImageCLEF 2017 -multi label classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ny Hoavy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Randrianarivony</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,295.02,337.98,7.86;10,151.52,305.98,329.07,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,302.19,295.02,178.41,7.86;10,151.52,305.98,296.01,7.86">Generating captions for medical images with a deep learning multi-hypothesis approach: ImageCLEF 2017 caption task</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,316.93,337.98,7.86;10,151.52,327.89,312.61,7.86" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,301.04,316.93,179.55,7.86;10,151.52,327.89,258.87,7.86">A cross-modal concept detection and caption prediction approach in ImageCLEFcaption track of ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lagree</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,338.85,337.97,7.86;10,151.52,349.81,329.07,7.86;10,151.52,360.77,25.60,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,199.23,349.81,157.35,7.86">NLM at ImageCLEF 2017 caption task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,364.60,349.81,82.34,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
