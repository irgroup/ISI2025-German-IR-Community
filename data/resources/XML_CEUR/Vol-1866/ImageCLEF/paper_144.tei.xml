<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.84,115.96,311.68,12.62;1,291.37,133.89,32.61,12.62">IPL at ImageCLEF 2017 Concept Detection Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,205.53,171.56,82.72,8.74"><forename type="first">Leonidas</forename><surname>Valavanis</surname></persName>
							<email>valavanisleonidas@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Information Processing Laboratory</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76 Patission Str</addrLine>
									<postCode>10434</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.94,171.56,98.88,8.74"><forename type="first">Spyridon</forename><surname>Stathopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Information Processing Laboratory</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76 Patission Str</addrLine>
									<postCode>10434</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.84,115.96,311.68,12.62;1,291.37,133.89,32.61,12.62">IPL at ImageCLEF 2017 Concept Detection Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">090918339DC6AB24E9D4FDF7EA94310E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>probabilistic k-nearest neighbors</term>
					<term>image annotation</term>
					<term>concept detection</term>
					<term>quad-tree bag of colors</term>
					<term>bag of visual words</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present the methods and techniques performed by the IPL Group for the concept detection task of ImageCLEF 2017. A probabilistic k-nearest neighbor approach was used for automatically detecting multiple concepts in medical images. The visual representation of images was based on the well known, bag of visual words and bag-of-colors models. Detection performance was further enhanced by applying late fusion on the results obtained using different image representations. Our best results were ranked 2nd compared with runs under the same conditions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic Image annotation is an important and challenging task within the field of computer vision with applications in several domains. In the medical domain it plays an important role in supporting image search, browsing and organization for clinical diagnosis and treatment. Image retrieval based on semantic information has many advantages and is more robust than using only low-level visual features. In the case of absence of semantic information a typical method to bridge the gap between low level visual features and high level semantics is through the automatic image annotation. This is achieved by applying machine learning techniques to learn a mapping of visual features to textual words. The learned model is then used to assign semantic concepts to new unseen images.</p><p>The ImageCLEFcaption 2017 task, <ref type="bibr" coords="1,304.19,567.56,9.96,8.74" target="#b2">[3]</ref>, part of ImageCLEF 2017 <ref type="bibr" coords="1,431.06,567.56,9.96,8.74" target="#b4">[5]</ref>, consists of 2 subtasks: concept detection and caption prediction. Our group participated in the concept detection subtask. For this task, participating groups were asked to develop systems to identify the presence of relevant bio-medical concepts in medical images.</p><p>Details of this task can be found in the overview paper <ref type="bibr" coords="1,390.88,627.33,10.52,8.74" target="#b2">[3]</ref> and the web page of the contest 1 . Our approach to concept detection is based on a Probabilistic k-nearest neighbor (PKNN) merging two well known models for image representation, that of the Bag of Visual Words (BoVW), <ref type="bibr" coords="2,350.30,130.95,10.52,8.74" target="#b5">[6]</ref> and an improved version of bag of colors (QBoC), <ref type="bibr" coords="2,233.68,142.90,14.61,8.74" target="#b9">[10]</ref>. When combined with late fusion, results are further improved, ranking 2nd in best performing runs compared to algorithms that don't rely on external data sources.</p><p>The following sections, present the image representation methods and our algorithm for concept detection. Finally we report on our results and conclude with possible venues for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Visual representation of images</head><p>Three different visual representation models were used in our experiments:</p><p>1. Localized compact features 2. Bag of Visual Words (BoVW) 3. Bag of Colors (BoC)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Localized compact features</head><p>Compact visual descriptors have been used extensively in the past years to efficiently represent images in a dataset. For this task, two kinds of visual features were extracted:</p><p>1. Color and Edge Directivity Descriptor (CEDD) <ref type="bibr" coords="2,352.06,395.37,15.07,8.74" target="#b0">[1]</ref>. 2. Fuzzy Color and Texture Histogram (FCTH) <ref type="bibr" coords="2,341.96,407.30,14.74,8.74" target="#b1">[2]</ref>.</p><p>However, in their original form, these descriptors are extracted globally from an image. In order to include a degree of spatial information, features are extracted over a spatial 4x4 grid. The image is first resized into 256 x 256 pixels and then is split into a 4x4 grid of non-overlaping image blocks. The visual features are then extracted for each block and their corresponding vectors are concatenated to form a single feature vector. The final vector size for the CEDD and FCTH is 4 × 4 × 144 = 2, 303 and 4 × 4 × 192 = 3, 702 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dense SIFT features and BoVW</head><p>Inspired from text retrieval, the Bag-of-visual Words (BoVW) approach has shown promising results in the field of image retrieval and classification. Here the BoVW model was implemented using the DenseSIFT visual descriptor.</p><p>The Dense SIFT algorithm <ref type="bibr" coords="2,271.88,584.39,9.96,8.74" target="#b6">[7]</ref>, is a variant of the SIFT algorithm, which is equivalent to extracting SIFT <ref type="bibr" coords="2,268.11,596.34,10.51,8.74" target="#b7">[8]</ref> from a dense grid of locations at a fixed scale and orientation. The SIFT feature is invariant with respect to many common image deformations, including position, scale, illumination, rotation, and affine transformation. The number of features extracted from local interest points using the Dense SIFT descriptor may vary, depending on the image. In order to have a fixed number of feature dimensions, a visual codebook is created by clustering the extracted local interest points of a number of sample images, using the k-means clustering algorithm. After experiments, the number of clusters used is 4, 096. Each cluster (visual word), represents a different local pattern which shares similar interest points. The histogram of an image, is created by performing a vector quantization which assigns each key-point to its closest cluster (visual word).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Quad-Tree Bag-of-Colors Model (QBoC)</head><p>The QBoC representation was successfully used for representing images in previous works <ref type="bibr" coords="3,180.52,246.64,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="3,192.70,246.64,11.62,8.74" target="#b9">10]</ref>. With the BoC model <ref type="bibr" coords="3,305.79,246.64,15.50,8.74" target="#b11">[12]</ref> a color vocabulary is learned from a sub-set of the image collection. This vocabulary is used to extract the color histograms for each image. The BoC model was used for classification of biomedical images in <ref type="bibr" coords="3,179.68,282.51,10.52,8.74" target="#b3">[4]</ref> and it was shown that it is combined successfully with the BoW-SIFT model in a late fusion manner. Similarly to the BoW model the main drawback with the BoC is the lack of spatial information. Quad-Tree decomposition sub-divides an image into regions of homogeneous colors. Each time the image is split into four equal size squares and the process continues until we reach a sub-region of size 1 × 1 pixel (see Figure <ref type="figure" coords="3,349.90,342.28,8.58,8.74" target="#fig_0">1b</ref>). In both models the Term Frequency-Inverse Document Frequency (TF-IDF) weights of visual words were calculated and the image vectors were normalized with the L 1 norm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Probabilistic k-NN concept detection (PKNN)</head><p>In this section we briefly present our baseline algorithm for automatic concept detection in medical images. The algorithm is divided into two main phases, namely, the visual retrieval step and the annotation step.</p><p>In the visual retrieval phase, for a given test image, a sample of the k most visually similar images from the training dataset is retrieved. Several experiments on the validation set helped to determine the optimum value for k (k=100).</p><p>In the annotation step, the concepts associated to the k retrieved images form the candidate concepts for a test image. The final assigned concepts are determined by a probability score based on the occurrence of concepts in the selected sample. For every distinct concept, w, present in the retrieved training subset, we calculate ConceptScore(w) as:</p><formula xml:id="formula_0" coords="4,225.84,222.28,250.51,30.32">ConceptScore(w) = k j=1 P (j) • P (w|j) (<label>1</label></formula><formula xml:id="formula_1" coords="4,476.34,232.69,4.24,8.74">)</formula><p>where j is an image from the top-k results and P (w|j) is calculated by:</p><formula xml:id="formula_2" coords="4,260.32,283.92,220.27,23.22">P (w|j) = count(w, j) |W j |<label>(2)</label></formula><p>where count(w, j) is the number of times concept w is found in image j, and |W j | is the total number of concepts in image j. P (j) is considered uniform for all images and thus it is ignored.</p><p>The top 6 concepts with the highest ConceptScore are selected for the test image. This number was determined by calculating the average number of concepts per image in the training set (i.e. 5.58).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Concept scoring with Random walks with Restart (RWR)</head><p>As an alternative concept scoring method, a Random Walk with Restart (RWR) algorithm <ref type="bibr" coords="4,180.70,432.52,15.50,8.74" target="#b10">[11]</ref> was tested. First, from the set of the top k retrieved images an adjacency matrix A of size [c × c] is constructed, where c is the number of distinct concepts in the retrieved images. Matrix A defines the graph whose nodes correspond to concepts and edges connect concepts if they are assigned in the same image of the train set. Next, the RWR algorithm is applied to matrix A resulting in a vector r of size [c × 1] representing the most probable concepts for the test image. Similarly, the top 6 concepts with the highest r(w) value are selected as the concepts for the test image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Late fusion</head><p>In order to improve results, late fusion was applied to the ranked lists of concepts for each test image. In late fusion, the ranked concept score lists from different visual descriptors are combined. A new score is calculated based on the combSUM function:</p><formula xml:id="formula_3" coords="4,221.08,619.87,255.27,30.55">combSU M (w) = D d=1 ConceptScore d (w) (<label>3</label></formula><formula xml:id="formula_4" coords="4,476.35,630.29,4.24,8.74">)</formula><p>where D is the number of descriptors to combine.</p><p>To determine our system's optimal parameters and best visual features we experimented with the validation set provided by the organizers. Trying different visual descriptors and concept detection algorithms, helped us conclude on the submitted runs. Table <ref type="table" coords="5,236.72,177.41,4.98,8.74" target="#tab_0">1</ref> presents some of the top results obtained from these experiments on the validation set. The "PKNN" prefix in run id corresponds to experiments using the probabilistic concept detection algorithm and "RWR" the random walks with restarts algorithm. The "LFS" prefix corresponds to runs using the late fusion method described in Section 3.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks</head><p>In this report, we presented the image concept detection methods used by the IPL Group for the medical concept detection subtask at ImageCLEF 2017. For our runs, we used a simple Probabilistic k-Nearest Neighbor approach. Experiments show that using late fusion on BoVW and QBoC performs best and that the image representation plays an important role in performance. Furthermore, the Random walks with Restarts algorithm seemed to perform slightly less, however, a more systematic research is currently underway for this method. Our best run was ranked 2nd in the top performing runs compared to algorithms that don't rely on any external data sources. This results are encouraging and lead to further research on improving the concept detection algorithm with additional textual meta-data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,546.76,345.83,7.89;3,143.98,399.23,155.63,121.65"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Representation of image 1743-7075-7-33-5 (a) original image; (b) QBoC image.</figDesc><graphic coords="3,143.98,399.23,155.63,121.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,225.23,345.83,332.63"><head>Table 1 .</head><label>1</label><figDesc>Table 2 presents the runs submitted to clef and their corresponding results. The same prefixes are also used to describe the individual runs. Results on the validation set</figDesc><table coords="5,202.16,279.85,210.91,278.01"><row><cell>Run ID</cell><cell cols="2">Accuracy</cell></row><row><cell>PKNN CEDD</cell><cell></cell><cell>0.113</cell></row><row><cell>PKNN CEDD 4x4</cell><cell></cell><cell>0.114</cell></row><row><cell>PKNN FCTH</cell><cell></cell><cell>0.112</cell></row><row><cell>PKNN FCTH 4x4</cell><cell></cell><cell>0.120</cell></row><row><cell>PKNN GBOC</cell><cell></cell><cell>0.123</cell></row><row><cell>PKNN DSIFT</cell><cell></cell><cell>0.137</cell></row><row><cell>RWR GBOC</cell><cell></cell><cell>0.132</cell></row><row><cell>RWR DSIFT</cell><cell></cell><cell>0.140</cell></row><row><cell>LFS PKNN (FCTH 4x4 DSIFT GBOC)</cell><cell></cell><cell>0.147</cell></row><row><cell>LFS PKNN (CEDD 4x4 DSIFT GBOC)</cell><cell></cell><cell>0.145</cell></row><row><cell>Run ID</cell><cell></cell><cell>Accuracy</cell></row><row><cell>LFS PKNN DSIFT GBOC</cell><cell></cell><cell>0.1436</cell></row><row><cell>LFS PKNN CEDD4x4 DSIFT GBOC</cell><cell></cell><cell>0.1418</cell></row><row><cell>LFS RWR DSIFT GBOC</cell><cell></cell><cell>0.1417</cell></row><row><cell>LFS PKNN FCTH4x4 DSIFT GBOC</cell><cell></cell><cell>0.1415</cell></row><row><cell>LFS RWR CEDD4x4 DSIFT GBOC</cell><cell></cell><cell>0.1414</cell></row><row><cell>DET LFS RWR FCTH4x4 DSIFT GBOC</cell><cell></cell><cell>0.1394</cell></row><row><cell>RWR DSift Top100 L2 SqrtNorm L1Norm</cell><cell></cell><cell>0.1365</cell></row><row><cell cols="2">PKNN DSift Top100 L2 SqrtNorm L1Norm</cell><cell>0.1364</cell></row><row><cell cols="2">RWR GBOC Top100 L2 SqrtNorm L1Norm</cell><cell>0.1212</cell></row><row><cell cols="3">PKNN GBOC Top100 L2 SqrtNorm L1Norm 0.1208</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,193.00,569.74,229.36,7.89"><head>Table 2 .</head><label>2</label><figDesc>Results of the submitted runs with the test set</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.44,174.17,7.47"><p>http://www.imageclef.org/2017/caption</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,266.19,337.63,7.86;6,151.52,277.15,329.07,7.86;6,151.52,288.11,25.60,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,303.51,266.19,177.08,7.86;6,151.52,277.15,227.20,7.86">Cedd: Color and edge directivity descriptor: A compact descriptor for image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,402.13,277.15,24.57,7.86">ICVS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="312" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,298.64,337.64,7.86;6,151.52,309.59,318.05,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,303.68,298.64,176.92,7.86;6,151.52,309.59,178.32,7.86">Fcth: Fuzzy color and texture histogram -a low level feature for accurate image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,350.55,309.59,39.16,7.86">WIAMIS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,320.12,337.64,7.86;6,151.52,331.08,329.07,7.86;6,151.52,342.04,329.07,7.86;6,151.52,352.99,317.71,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,403.70,320.12,76.89,7.86;6,151.52,331.08,329.07,7.86;6,151.52,342.04,42.85,7.86">Overview of Image-CLEFcaption 2017 -image caption prediction and concept detection for biomedical images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="6,216.68,342.04,131.58,7.86">CLEF 2017 Labs Working Notes</title>
		<title level="s" coord="6,356.02,342.04,124.57,7.86;6,151.52,352.99,43.39,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,363.52,337.64,7.86;6,151.52,374.48,329.07,7.86;6,151.52,385.44,191.21,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,366.03,363.52,114.57,7.86;6,151.52,374.48,121.00,7.86">Bag-of-colors for biomedical document image classification</title>
		<author>
			<persName coords=""><forename type="first">García</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Markonis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,295.32,374.48,185.28,7.86;6,151.52,385.44,67.49,7.86">Medical Content-Based Retrieval for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="110" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,395.96,337.63,7.86;6,151.52,406.92,329.07,7.86;6,151.52,417.88,329.07,7.86;6,151.52,428.84,329.07,7.86;6,151.52,439.80,329.07,7.86;6,151.52,450.75,329.07,7.86;6,151.52,461.71,199.89,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,433.34,417.88,47.26,7.86;6,151.52,428.84,215.41,7.86">Overview of ImageCLEF 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,387.32,428.84,93.27,7.86;6,151.52,439.80,329.07,7.86;6,151.52,450.75,105.52,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction 8th International Conference of the CLEF Association, CLEF</title>
		<title level="s" coord="6,285.42,450.75,143.89,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-09-11">2017. September 11-14 2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,472.24,337.64,7.86;6,151.52,483.20,170.82,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,235.56,472.24,245.04,7.86;6,151.52,483.20,26.05,7.86">A bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,198.44,483.20,26.62,7.86">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="524" to="531" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,493.72,337.64,7.86;6,151.52,504.68,329.07,7.86;6,151.52,515.64,215.94,8.12" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,280.11,493.72,200.48,7.86;6,151.52,504.68,60.96,7.86">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2010.147</idno>
		<ptr target="http://dx.doi.org/10.1109/TPAMI.2010.147" />
	</analytic>
	<monogr>
		<title level="j" coord="6,220.73,504.68,169.78,7.86">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="978" to="994" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,526.16,337.63,7.86;6,151.52,537.12,329.07,7.86;6,151.52,548.08,329.07,7.86;6,151.52,559.68,221.25,7.47" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,201.91,526.16,210.54,7.86">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=850924.851523" />
	</analytic>
	<monogr>
		<title level="m" coord="6,432.52,526.16,48.07,7.86;6,151.52,537.12,223.43,7.86;6,200.03,548.08,38.77,7.86">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
	<note>ICCV &apos;99</note>
</biblStruct>

<biblStruct coords="6,142.96,569.56,337.64,7.86;6,151.52,580.52,329.07,7.86;6,152.35,589.21,328.23,10.38;6,151.52,603.08,99.35,7.47" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,350.82,569.56,125.92,7.86">IPL at CLEF 2016 medical task</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Valavanis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kalamboukis</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1609/16090413.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="6,165.55,580.52,310.64,7.86">Working Notes of CLEF 2016 -Conference and Labs of the Evaluation forum</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09-08">5-8 September, 2016. 2016</date>
			<biblScope unit="page" from="413" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,612.96,337.98,7.86;6,151.52,623.92,329.07,7.86;6,151.52,634.88,329.07,7.86;6,151.52,645.84,329.07,8.12;6,151.52,657.44,96.14,7.47" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,356.80,612.96,123.79,7.86;6,151.52,623.92,187.15,7.86">Fusion of bag-of-words models for image classification in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Valavanis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kalamboukis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-56608-5_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-56608-5_11" />
	</analytic>
	<monogr>
		<title level="m" coord="6,361.92,623.92,118.67,7.86;6,151.52,634.88,260.02,7.86">Advances in Information Retrieval -39th European Conference on IR Research, ECIR 2017</title>
		<meeting><address><addrLine>Aberdeen, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">April 8-13, 2017. 2017</date>
			<biblScope unit="page" from="134" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,119.67,337.97,7.86;7,151.52,130.63,329.07,7.86;7,151.52,141.59,329.07,7.86;7,151.52,153.20,197.71,7.47" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,337.35,119.67,143.23,7.86;7,151.52,130.63,106.81,7.86">Image annotation refinement using random walk with restarts</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/1180639.1180774</idno>
		<ptr target="http://doi.acm.org/10.1145/1180639.1180774" />
	</analytic>
	<monogr>
		<title level="m" coord="7,279.98,130.63,200.62,7.86;7,151.52,141.59,88.18,7.86;7,300.87,141.59,30.67,7.86">Proceedings of the 14th ACM International Conference on Multimedia</title>
		<meeting>the 14th ACM International Conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="647" to="650" />
		</imprint>
	</monogr>
	<note>MM &apos;06</note>
</biblStruct>

<biblStruct coords="7,142.62,163.51,337.97,7.86;7,151.52,174.47,329.07,7.86;7,151.52,185.43,329.07,8.12;7,151.52,197.03,145.93,7.47" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,300.04,163.51,162.03,7.86">Bag-of-colors for improved image search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wengert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="DOI">10.1145/2072298.2072034</idno>
		<ptr target="http://doi.acm.org/10.1145/2072298.2072034" />
	</analytic>
	<monogr>
		<title level="m" coord="7,151.52,174.47,278.45,7.86">Proceedings of the 19th International Conference on Multimedia 2011</title>
		<meeting>the 19th International Conference on Multimedia 2011<address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12-01">November 28 -December 1, 2011. 2011</date>
			<biblScope unit="page" from="1437" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
