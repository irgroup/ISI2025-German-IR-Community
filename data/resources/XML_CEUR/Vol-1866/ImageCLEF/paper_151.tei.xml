<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.97,115.96,307.43,12.62;1,214.44,133.89,186.48,12.62">3D-CNN in Drug Resistance Detection and Tuberculosis Classification</title>
				<funder ref="#_WEvWMKP">
					<orgName type="full">FCT Fundação para a Ciência e a Tecnologia</orgName>
				</funder>
				<funder ref="#_ubB59nc">
					<orgName type="full">FCT</orgName>
				</funder>
				<funder ref="#_rQANmyn">
					<orgName type="full">ERDF -European Regional Development Fund</orgName>
				</funder>
				<funder>
					<orgName type="full">National Funds</orgName>
				</funder>
				<funder ref="#_8MtTBK4">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.88,171.69,59.25,8.74"><forename type="first">João</forename><surname>Figueira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DETI -Institute of Electronics and Informatics Engineering</orgName>
								<orgName type="institution">Aveiro University of Aveiro</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.29,171.69,79.27,8.74"><forename type="first">Jorge</forename><forename type="middle">Miguel</forename><surname>Silva</surname></persName>
							<email>joaofsilva@ua.pt</email>
							<affiliation key="aff0">
								<orgName type="department">DETI -Institute of Electronics and Informatics Engineering</orgName>
								<orgName type="institution">Aveiro University of Aveiro</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.81,171.69,64.21,8.74"><forename type="first">Eduardo</forename><surname>Pinho</surname></persName>
							<email>eduardopinho@ua.pt</email>
							<affiliation key="aff0">
								<orgName type="department">DETI -Institute of Electronics and Informatics Engineering</orgName>
								<orgName type="institution">Aveiro University of Aveiro</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,413.43,171.69,56.05,8.74"><forename type="first">Carlos</forename><surname>Costa</surname></persName>
							<email>carlos.costa@ua.pt</email>
							<affiliation key="aff0">
								<orgName type="department">DETI -Institute of Electronics and Informatics Engineering</orgName>
								<orgName type="institution">Aveiro University of Aveiro</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.97,115.96,307.43,12.62;1,214.44,133.89,186.48,12.62">3D-CNN in Drug Resistance Detection and Tuberculosis Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">80D328CBB65C7DA230CEB915DD23856B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D-CNN</term>
					<term>Neural Networks</term>
					<term>Deep Learning</term>
					<term>Medical Imaging</term>
					<term>CT</term>
					<term>Tuberculosis</term>
					<term>ImageCLEF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object classification is a very demanding field in computer vision, especially when dealing with medical imaging datasets, which are often small and have unbalanced distributions. Deep learning (DL) methods have proven to be effective in dealing with such problems and have established themselves as the state-of-the-art. ImageCLEFtuberculosis is a challenge that encompasses the classification problem on medical images, and is divided into two subtasks: Drug Resistance Detection and Tuberculosis classification. For both subtasks, provided images were pre-processed to segment the lungs from the CT volumes. Afterwards, pre-processed CT volumes were fed in batches to a 3D convolutional neural network. Test results for the Drug Resistance detection task scored an accuracy of 46.5% and AUC of 0.46, while in the Tuberculosis classification task an accuracy of 24% and Cohen's Kappa value of 0.022 were obtained. Using data augmentation and weight normalization, the overfitting problem could be reduced, and submitted models' performance improved.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEFtuberculosis task <ref type="bibr" coords="1,286.95,512.66,10.52,8.74" target="#b0">[1]</ref> from ImageCLEF 2017 <ref type="bibr" coords="1,406.73,512.66,10.52,8.74" target="#b1">[2]</ref> is a challenge centered on medical imaging, that has the motivation of improving tuberculosis treatment and reducing its impact on patients through the development of systems capable of extracting the tuberculosis type and drug resistances from image data alone. Usually, working with medical imaging datasets encompasses distinct challenges such as the limited access to data, its reduced size and the unbalanced distributions. Deep learning (DL) methods have been increasingly explored in the field of image analysis, with neural networks leading to major breakthroughs in renown challenges, such as the MNIST Digit Image Classification Problem and the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) <ref type="bibr" coords="1,182.34,632.21,9.96,8.74" target="#b2">[3]</ref>, where they are considered the state-of-the-art <ref type="bibr" coords="1,397.74,632.21,9.96,8.74" target="#b3">[4]</ref>. These networks present great interest since they automatically learn high-level representations from the data, and can be used to reduce the data dimensionality <ref type="bibr" coords="1,424.96,656.12,9.96,8.74" target="#b4">[5]</ref>.</p><p>In recent years, deep learning has started to make a significant appearance in the field of medical imaging with promising results <ref type="bibr" coords="2,356.07,130.95,9.96,8.74" target="#b5">[6]</ref>. Following this trend, this article assesses the viability of this technology to solve ImageCLEF challenges through the development of a 3D Convolutional Neural Network (CNN) model.</p><p>The ImageCLEFtuberculosis task is divided into two separate and independent subtasks: drug resistance detection and tuberculosis classification. The goal of the first task was to assess the probability of a tuberculosis (TB) patient having a resistant form of tuberculosis based on the analysis of a chest CT scan, whereas the second one focused on classifying the TB type from five possible types of TB.</p><p>This article describes the proposed solution and runs submitted by the Bioinformatics team for both subtasks. The developed methodology is presented in Section 2, results are presented and discussed in Section 3, and finally Section 4 draws some conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>To address the MDR detection and TB type subtasks from ImageCLEFtuberculosis, we propose a two-stage pipeline: Data pre-processing and a DL model (Figure <ref type="figure" coords="2,170.33,347.38,3.87,8.74">1</ref>). The pre-processing stage was applied to both subtasks whereas the DL model was fine-tuned for each subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-Processing CT Volumes</head><p>Deep Learning Output Fig. <ref type="figure" coords="2,273.87,447.49,4.13,7.89">1</ref>. Pipeline Overview.</p><p>Pre-processing stage used the Computed Tomography (CT) images, segmented the lungs, and resized data to be ready to feed the DL model. On the other hand, the DL model used batches of pre-processed data and classified it. Each of the stages is explained in more detail in the next subsections.</p><p>An important aspect of proposed approach is related with the fact that a CT volume is composed by several images and the observation that a single slice might provide poor classification results. So, we decided to feed the DL models with volumes composed of stacks of CT slices, option that conducted us to use of a 3D-CNN model instead of a conventional CNN model. This option brought also implications concerning the shape of the models input tensors, which were solved in the data pre-processing step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Pre-processing</head><p>Pre-processing stage has the responsibility of preparing data for posterior processes, namely feeding the DL model. For the drug resistance detection subtask, a train dataset with 230 CT volumes and a test dataset with 214 CT volumes were provided. In this subtask data had two possible classes. Regarding the tuberculosis classification subtask, a train dataset with 500 CT volumes and a test dataset with 300 CT volumes were provided, with data having five possible classes. CT volumes had a variable number of slices, with slice size being 512x512 pixels <ref type="bibr" coords="3,162.77,178.77,9.96,8.74" target="#b0">[1]</ref>.</p><p>In the training datasets, CT volumes had the lungs segmented using masks created with a developed algorithm. To create the masks, the following method was used: a thresholded was applied to the images where intensities below -300 Hounsfield units were set as background, the pixel values were normalized to have an intensity range from 0 to 255, and resulting images were passed through a binary thresholding process with a threshold value of 20. Using scikit-image<ref type="foot" coords="3,473.36,249.08,3.97,6.12" target="#foot_0">1</ref> , small holes and small objects were removed, using methods with the same name and parameterized with minimum size of 100 and connectivity of 4. Next, the two methods were reapplied but with a minimum size of 1000. The result is the desired masks.</p><p>Obtained masks were highly similar compared to those provided to the participants <ref type="bibr" coords="3,175.35,322.54,9.96,8.74" target="#b6">[7]</ref>. Dice's coefficient, which is scaled from 0 to 1 with 1 corresponding to image equality, was computed to assess the similarity between created masks and the original provided masks, with a global average value of 0.9755 being obtained. Regarding the test dataset, provided masks were used to ensure that test data to feed the 3D-CNN was not tampered.</p><p>After this, the resulting masked volumes were reshaped to comply with the NHWC channel ordering (number of samples x height x width x channels) used in CNNs. In our case, the number of samples corresponds to the number of CT slices. Next, each CT slice was resized to dimensions of 256x256 pixels.</p><p>The resulting volumes were resized, regarding the number of slices, so that all volumes had the same number of slices. This was achieved by padding the top and bottom of each volume, resulting in a final volume with fixed size (real data in the center, and padding in the extremities). Finally, data was normalized to have zero mean.</p><p>For each subtask, processed datasets were saved in HDF5 files resulting in two HDF5 files per task with the train and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Learning</head><p>As expressed, we opted by a 3D-CNN model for the DL model stage. The model was implemented with TensorFlow <ref type="bibr" coords="3,286.19,565.64,10.52,8.74" target="#b7">[8]</ref> version 1.0.0 with support for GPU, which massively increases the speed and efficiency of training and developing models such as neural networks. Moreover, TensorBoard was used during the development of the 3D-CNN model for debugging and optimization purposes. Some additional functions needed for the models' development were imported from TFLearn<ref type="foot" coords="3,173.54,623.84,3.97,6.12" target="#foot_1">2</ref> (v0.3), a DL library that provides a higher-level API to TensorFlow.</p><p>The 3D-CNN model training ran on an Ubuntu server machine equipped with an NVIDIA Tesla K80 GPU accelerator.</p><p>Regarding the DL model itself, Figure <ref type="figure" coords="4,324.81,142.90,4.98,8.74">2</ref> presents an overview of the built model with the respective composition of each layer. The decision to use a 3D-CNN model with seven convolutional layers and two fully connected layers was empirical. Literature supports that deeper models can be more powerful than shallow ones, as the former can learn how to represent high-level abstractions, presenting particular interest for the fields of vision, language and other AI-level tasks <ref type="bibr" coords="4,467.30,345.28,9.96,8.74" target="#b8">[9]</ref>. However, it is also known that deeper models are more difficult to train due to problems such as the vanishing gradient problem, where initial layers learn at slower speeds than final layers. Naturally, the deeper the network, the more prone it is to the vanishing gradient problem <ref type="bibr" coords="4,310.28,393.10,14.61,8.74" target="#b9">[10]</ref>. Moreover, deeper models demand bigger compute power, which is a very significant overhead. Thus, bearing in mind the associated implications of creating a deep neural network, and the existing limited compute power, it was decided to build a network with a small number of layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Layer</head><p>As it is possible to observe in Figure <ref type="figure" coords="4,307.99,452.88,3.87,8.74">2</ref>, the network's first six layers share the same structure (but not the hyperparameters). In these six layers, the incoming tensor is passed through a sequence of 3D convolution, 3D max pooling, batch normalization and non-linear activation function.</p><p>Batch normalization is very important as it addresses a phenomenon called internal covariate shift, which slows down the training of neural networks <ref type="bibr" coords="4,462.32,512.66,14.61,8.74" target="#b10">[11]</ref>. Concerning the activation function, since the sigmoid activation function can cause problems when training deep neural networks <ref type="bibr" coords="4,360.04,536.57,14.60,8.74" target="#b11">[12]</ref>, a variation of the rectified linear unit (ReLU) -the leaky ReLU -which can lead to better performances was used in this neural network <ref type="bibr" coords="4,276.13,560.48,14.60,8.74" target="#b12">[13]</ref>.</p><p>Overfitting is other serious concern in neural networks, specially when dealing with medical imaging datasets which frequently consist of reduced amounts of data, with unbalanced distributions. For that reason, dropout <ref type="bibr" coords="4,405.24,596.34,14.61,8.74" target="#b13">[14]</ref>, a regularizer used to reduce overfitting in neural networks, was used in our model. However, it was only applied to the fully-connected part of the network as convolutional layers have considerable inbuilt resistance to overfitting <ref type="bibr" coords="4,383.71,632.21,14.61,8.74" target="#b14">[15]</ref>. Also, L2 regularization was used in each convolutional layer to reduce model overfitting, and the last Fully-Connected layer has a softmax activation function.</p><p>The described 3D-CNN was used for both subtasks, though with different hyperparameters due to the fine-tuning procedure performed for each subtask. All Leaky ReLUs were used with the leaking coefficient α = 0.1 and Dropout with a drop probability of 0.5 for both subtasks. Table <ref type="table" coords="5,370.04,154.86,4.98,8.74" target="#tab_1">1</ref> summarizes the remaining hyperparameters for the models' layers. It should be noted that the hyperparameters were defined with compute power constraints in mind. All weights were initialized as described in <ref type="bibr" coords="5,271.64,190.72,14.61,8.74" target="#b15">[16]</ref>. Concerning data handling, the training dataset was split into 80/20 parcels, for training and validation splits respectively. Data distribution had moderately balanced classes for the MDR detection subtask, whereas for the TB type subtask a less balanced dataset was provided. For each subtask, data was split taking into account class distributions, in order to ensure the same class distribution in training and validation splits. Even though the network was prepared to work with K-fold cross validation, due to time constraints and the inherent nature of the training process of a neural network, the network was validated offline using a single combination of the 80/20 split. Furthermore, the model was fed with mini-batches of data containing complete CT scans, where each sample is one of the CT volumes being forwarded through the net. Since this type of network is demanding in terms of memory and computational cost, and aiming to enable the use of bigger batch sizes, each pre-processed volume was cropped into a fixed smaller number of slices, corresponding to the size of the smallest volume in the original dataset. As expressed, this cropping method extracts data from the center of each CT volume.</p><p>In order to prevent the network from learning a given data sequence/order, data splits were shuffled in each epoch, prior to being fed to the model. Finally, a group of four metrics was used to assess model performance, consisting of: cross entropy, accuracy, precision, and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submitted Runs and Results</head><p>A single run was submitted for each subtask of the ImageCLEFtuberculosis task, with the results and respective neural network configurations being discussed in this section.</p><p>In both subtasks the neural network models were trained using an Adam optimizer <ref type="bibr" coords="6,178.36,350.11,15.50,8.74" target="#b16">[17]</ref> for stochastic optimization, with the following settings being used: α = LearningRate, β 1 = 0.9, β 2 = 0.999 and = 10 -8 . Table <ref type="table" coords="6,421.01,362.07,4.98,8.74" target="#tab_2">2</ref> summarizes some of the hyperparameters used in order to train the neural network for each subtask. All hyperparameters were defined empirically. As previously mentioned, the graphics card used to accelerate the training of the neural network was an NVIDIA Tesla K80, which possesses two separate GPUs. Due to the use of the graphics card for other tasks, the MDR detection model was trained using a single GPU whereas the TB type model was trained using both GPUs. Therefore, and as shown in Table <ref type="table" coords="6,404.72,595.68,3.87,8.74" target="#tab_2">2</ref>, it was possible to significantly increase the batch size for the TB type network.</p><p>Learning rate was reduced by a fraction of 5 percent of its value after 10 and 15 epochs for subtask 1 and subtask 2, respectively. Validation was performed in intervals of 3 epochs for the MDR detection's model, and in intervals of 2 epochs for the TB type's model. The best results obtained for each subtask during the validation phase are shown in Table <ref type="table" coords="7,204.22,240.64,3.87,8.74" target="#tab_3">3</ref>. In MDR detection, which is a two class problem, the trained model favors the retrieval of the most frequent class but struggles to detect the less frequent and more relevant class, leading to a substantially lower recall comparatively to obtained accuracy and precision.</p><p>In the TB type task, a multi-class problem (five classes), it is possible to see that the tuned model attained a lower accuracy, while keeping slightly similar precision and improved recall. The impact of having a higher number of classes, combined with a less balanced dataset for this task had a repercussion on the validation accuracy which was significantly lower than in the MDR detection task. Such accuracy value demonstrates that the neural network had difficulties in identifying the classes in data, which explains why some classes had no occurrence registered in the validation dataset.</p><p>Regarding the testing phase, in the MDR detection subtask contestants had to submit the probability of each patient having MDR, whereas for the TB type task submissions had to contain the expected TB type for each TB patient. Model performance was assessed with different metrics for each subtask: in MDR detection, performance was measured with Accuracy and Area Under the Curve (AUC) obtained from the ROC-curves produced with the submitted probabilities; in TB type classification Accuracy and Cohen's Kappa were the selected metrics. Table <ref type="table" coords="7,239.06,469.29,4.98,8.74" target="#tab_4">4</ref> presents test results both for the submitted run and for the best run in each subtask.</p><p>The list of test results for the two subtasks comprised in ImageCLEF's tuberculosis task <ref type="bibr" coords="7,202.91,505.91,10.52,8.74" target="#b0">[1]</ref> clearly demonstrates the high difficulty associated with this challenge's proposition. On the one hand, submitted runs performed worse in each subtask than the remaining entries. On the other hand, for the MDR de- For the TB type subtask, test results were in general worse comparatively to results of the MDR detection subtask. In this subtask, the top ranking entry had an accuracy of 40% compared to our model's 24.3%, and a Cohen's Kappa of 0.24 compared to the marginal value of 0.02 obtained by our model.</p><p>Aside from the comparison with other models' performance, it is noticeable that for subtask 1 our model had lower accuracy in the test phase (46.5%) than in the validation phase (55%), whereas for subtask 2 the opposite occurred with test accuracy (24%) being higher than validation accuracy (17.4%). For the first part, it is very likely that the model suffered overfitting to the training dataset (a common issue when dealing with medical imaging datasets), and testing performance suffered a significant impact from that. For the second part, there exists the possibility of having a test dataset less balanced, regarding class distribution, than the training dataset. By having a class distribution more skewed towards the more frequent classes, our model can attain higher accuracy scores than during the validation process.</p><p>In spite of our models' poor performance in general, the final ImageCLEFtuberculosis result list <ref type="bibr" coords="8,236.99,361.33,10.52,8.74" target="#b0">[1]</ref> shows that there are other entries with comparable performance. Fine-tuning a model is a slow, thorough process that should be methodical. In our approach, the search for the best hyperparameters was empirical and not extensive enough due to limitations in terms of available time. There is much confidence that there exists a big margin for progress and improvement in our work, provided there is more time to better train the models, and correctly fine-tune them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>The ImageCLEFtuberculosis task is a challenge that encompasses the classification problem on medical images. This task was divided into two subtasks: Drug Resistance Detection and TB classification. In the first subtask the objective was to assess the probability of a TB patient having a resistant form of tuberculosis, whereas on the second one the goal was to classify the TB type from a pool of five possible types.</p><p>In this paper we presented two separate runs that were submitted for the two subtasks. In both subtasks, provided images were pre-processed for this challenge. Although the test results of our submitted runs for both subtasks were low (46.5% accuracy, AUC of 0.46 and 24% accuracy, Cohen's Kappa of 0.022, respectively), the majority of the submitted runs behaved in a similar way, since the differences in terms of accuracy between the best submitted run and our own were of 5% and 15% for the MDR detection and TB type subtask, respectively. As a side note, it is interesting to notice in the list of submissions that various entries used DL approaches to tackle this challenge, which shows that DL is an area that holds great promise.</p><p>Since overfitting was an effective reality during the development of the neural network models, in the future we hope to evaluate the impact of techniques such as data augmentation and weight normalization on our models' results. Furthermore, running the model with K-fold cross-validation and performing an ensemble of the resulting networks could further improve our results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,201.49,208.43,275.16,87.30"><head></head><label></label><figDesc>Diagram of the neural network model used.</figDesc><table coords="4,201.49,208.43,275.16,87.30"><row><cell>6x</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D Convolution</cell><cell>3D Convolution</cell><cell></cell><cell></cell></row><row><cell>3D Max Pooling</cell><cell>3D Global Avg Pool</cell><cell>Fully -Connected</cell><cell></cell></row><row><cell>Batch Normalization</cell><cell>Leaky ReLU</cell><cell>Leaky ReLU</cell><cell>Fully -Connected</cell></row><row><cell>Leaky ReLU</cell><cell>Dropout</cell><cell>Dropout</cell><cell></cell></row><row><cell></cell><cell>Layer</cell><cell>Layer</cell><cell>Layer</cell><cell>Result</cell></row><row><cell>Fig. 2.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,230.91,345.83,294.54"><head>Table 1 .</head><label>1</label><figDesc>List of layer hyperparameters for the MDR detection and TB type models. The following parameters are displayed: number of units/filters, kernel size, stride and L2 weight decay.</figDesc><table coords="5,142.23,273.20,327.84,252.25"><row><cell>Layer</cell><cell cols="3">MDR Detection Units Kernel Stride</cell><cell>L2</cell><cell cols="3">TB Type Units Kernel Stride</cell><cell>L2</cell></row><row><cell>Conv1</cell><cell>35</cell><cell>11</cell><cell>2</cell><cell>0.01</cell><cell>20</cell><cell>7</cell><cell>3</cell><cell>0.001</cell></row><row><cell>Max1</cell><cell></cell><cell>5</cell><cell>5</cell><cell></cell><cell></cell><cell>5</cell><cell>2</cell><cell></cell></row><row><cell>Conv2</cell><cell>60</cell><cell>7</cell><cell>7</cell><cell>0.001</cell><cell>15</cell><cell>11</cell><cell>3</cell><cell>0.001</cell></row><row><cell>Max2</cell><cell></cell><cell>5</cell><cell>5</cell><cell></cell><cell></cell><cell>5</cell><cell>2</cell><cell></cell></row><row><cell>Conv3</cell><cell>60</cell><cell>5</cell><cell>3</cell><cell>0.002</cell><cell>15</cell><cell>9</cell><cell>3</cell><cell>0.001</cell></row><row><cell>Max3</cell><cell></cell><cell>3</cell><cell>2</cell><cell></cell><cell></cell><cell>5</cell><cell>2</cell><cell></cell></row><row><cell>Conv4</cell><cell>60</cell><cell>5</cell><cell>3</cell><cell>0.002</cell><cell>15</cell><cell>7</cell><cell>7</cell><cell>0.001</cell></row><row><cell>Max4</cell><cell></cell><cell>3</cell><cell>2</cell><cell></cell><cell></cell><cell>5</cell><cell>2</cell><cell></cell></row><row><cell>Conv5</cell><cell>92</cell><cell>5</cell><cell>3</cell><cell>0.002</cell><cell>32</cell><cell>5</cell><cell>3</cell><cell>0.001</cell></row><row><cell>Max5</cell><cell></cell><cell>2</cell><cell>2</cell><cell></cell><cell></cell><cell>2</cell><cell>2</cell><cell></cell></row><row><cell>Conv6</cell><cell>92</cell><cell>3</cell><cell>2</cell><cell>0.003</cell><cell>64</cell><cell>3</cell><cell>2</cell><cell>0.001</cell></row><row><cell>Max6</cell><cell></cell><cell>2</cell><cell>2</cell><cell></cell><cell></cell><cell>2</cell><cell>2</cell><cell></cell></row><row><cell>Conv7</cell><cell>128</cell><cell>1</cell><cell>1</cell><cell></cell><cell>128</cell><cell>1</cell><cell>1</cell><cell></cell></row><row><cell>FC1</cell><cell>128</cell><cell></cell><cell></cell><cell></cell><cell>128</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FC2</cell><cell>Num Classes</cell><cell></cell><cell></cell><cell></cell><cell>Num Classes</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,147.20,418.80,320.96,98.95"><head>Table 2 .</head><label>2</label><figDesc>Hyperparameters used in the neural networks for the submitted runs.</figDesc><table coords="6,182.88,439.17,246.52,78.57"><row><cell>Hyperparameter</cell><cell>MDR detection (Subtask 1)</cell><cell>TB type (Subtask 2)</cell></row><row><cell>Batch Size</cell><cell>18</cell><cell>30</cell></row><row><cell>Learning Rate (LR)</cell><cell>7 x 10 -5</cell><cell>9 x 10 -6</cell></row><row><cell>LR Decay</cell><cell>5%</cell><cell>5%</cell></row><row><cell>Decay Interval</cell><cell>10 epochs</cell><cell>15 epochs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,157.19,115.91,300.98,82.53"><head>Table 3 .</head><label>3</label><figDesc>Performance metrics used in the validation of submitted models.</figDesc><table coords="7,214.35,134.54,183.58,63.90"><row><cell>Metric</cell><cell>MDR detection (Subtask 1)</cell><cell>TB type (Subtask 2)</cell></row><row><cell>Accuracy</cell><cell>0.5501</cell><cell>0.1744</cell></row><row><cell>Precision</cell><cell>0.5470</cell><cell>0.5223</cell></row><row><cell>Recall</cell><cell>0.3440</cell><cell>0.4413</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,134.77,562.82,345.83,97.20"><head>Table 4 .</head><label>4</label><figDesc>Test results obtained for the submitted models, and best submissions for each subtask.</figDesc><table coords="7,150.03,592.41,312.21,67.62"><row><cell>Metrics</cell><cell cols="2">MDR Detection Our Run Best Run</cell><cell cols="2">TB Type Our Run Best Run</cell></row><row><cell>Test Accuracy</cell><cell>0.4648</cell><cell>0.5164</cell><cell>0.2400</cell><cell>0.4033</cell></row><row><cell>AUC</cell><cell>0.4596</cell><cell>0.5825</cell><cell></cell><cell></cell></row><row><cell>Cohen's Kappa</cell><cell></cell><cell></cell><cell>0.0222</cell><cell>0.2438</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,645.84,92.23,7.86"><p>http://scikit-image.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,656.80,111.93,7.86"><p>TFLearn: http://tflearn.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is financed by the <rs type="funder">ERDF -European Regional Development Fund</rs> through the <rs type="programName">Operational Programme for Competitiveness and Internationalisation -COMPETE 2020 Programme</rs>, and by <rs type="funder">National Funds</rs> through the <rs type="funder">FCT Fundação para a Ciência e a Tecnologia</rs>. <rs type="person">João Figueira Silva</rs> is funded by the research grant of <rs type="grantNumber">PTDC/EEI-ESS/6815/2014</rs> project and <rs type="person">Jorge Miguel Silva</rs> is funded by the research grant of <rs type="grantNumber">CMUP-ERI/ICT/0028/2014-SCREEN-DR</rs> project. <rs type="person">Eduardo Pinho</rs> also was funded by the <rs type="funder">FCT</rs> under the grant <rs type="grantNumber">PD/BD/ 105806/2014</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rQANmyn">
					<orgName type="program" subtype="full">Operational Programme for Competitiveness and Internationalisation -COMPETE 2020 Programme</orgName>
				</org>
				<org type="funding" xml:id="_WEvWMKP">
					<idno type="grant-number">PTDC/EEI-ESS/6815/2014</idno>
				</org>
				<org type="funding" xml:id="_ubB59nc">
					<idno type="grant-number">CMUP-ERI/ICT/0028/2014-SCREEN-DR</idno>
				</org>
				<org type="funding" xml:id="_8MtTBK4">
					<idno type="grant-number">PD/BD/ 105806/2014</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,391.98,337.64,7.86;9,151.52,402.94,329.07,7.86;9,151.52,413.90,176.36,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,442.93,391.98,37.66,7.86;9,151.52,402.94,329.07,7.86;9,151.52,413.90,23.74,7.86">Overview of ImageCLEFtuberculosis 2017 -Predicting Tuberculosis Type and Drug Resistances</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kalinovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,183.31,413.90,82.27,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,425.16,337.63,7.86;9,151.52,436.12,329.07,7.86;9,151.52,447.08,329.07,7.86;9,151.52,458.04,329.07,7.86;9,151.52,469.00,329.07,7.86;9,151.52,479.95,329.07,7.86;9,151.52,490.91,237.28,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,442.94,447.08,37.65,7.86;9,151.52,458.04,234.37,7.86">Overview of ImageCLEF 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,412.25,458.04,68.34,7.86;9,151.52,469.00,329.07,7.86;9,151.52,479.95,96.53,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction 8th International Conference of the CLEF Association</title>
		<title level="s" coord="9,372.32,479.95,108.27,7.86;9,151.52,490.91,26.69,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>CLEF; Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-09-11">2017. September 11-14 2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,502.17,337.63,7.86;9,151.52,513.13,329.07,7.86;9,151.52,524.09,329.07,7.86;9,151.52,535.02,123.55,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,415.87,513.13,64.72,7.86;9,151.52,524.09,144.22,7.86">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,307.61,524.09,172.98,7.86;9,151.52,535.05,28.80,7.86">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,546.31,337.63,7.86;9,151.52,557.27,130.29,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,318.70,546.31,161.89,7.86;9,151.52,557.27,68.71,7.86">Representation Learning: A Review and New Perspectives</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,568.53,337.63,7.86;9,151.52,579.46,211.84,7.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,303.92,568.53,176.67,7.86;9,151.52,579.48,65.53,7.86">Reducing the Dimensionality of Data with Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,225.59,579.48,29.19,7.86">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,590.74,337.63,7.86;9,151.52,601.70,329.07,7.86;9,151.52,612.64,158.01,7.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,176.74,601.70,151.00,7.86">Deep Learning for Health Informatics</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Deligianni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Andreu-Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,336.44,601.70,144.15,7.86;9,151.52,612.66,66.06,7.86">Biomedical and Health Informatics, IEEE Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="21" />
			<date type="published" when="2017-01">jan 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,623.92,337.63,7.86;9,151.52,634.88,329.07,7.86;9,151.52,645.84,329.07,7.86;9,151.52,656.80,152.77,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,407.63,623.92,72.95,7.86;9,151.52,634.88,209.18,7.86">Efficient and fully automatic segmentation of the lungs in CT volumes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A</forename><surname>Del Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,384.04,634.88,96.55,7.86;9,151.52,645.84,329.07,7.86;9,151.52,656.80,46.41,7.86">Proceedings of the VIS-CERAL Anatomy Grand Challenge at the 2015 IEEE ISBI. CEUR Workshop Proceedings</title>
		<meeting>the VIS-CERAL Anatomy Grand Challenge at the 2015 IEEE ISBI. CEUR Workshop Proceedings</meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,119.67,337.64,7.86;10,151.52,130.63,329.07,7.86;10,151.52,141.59,329.07,7.86;10,151.52,152.55,329.07,7.86;10,151.52,163.51,329.07,7.86;10,151.52,174.47,329.07,7.86;10,151.52,185.43,329.07,7.86;10,151.52,196.39,25.60,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,151.52,185.43,324.86,7.86">TensorFlow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,207.34,337.64,7.86;10,151.52,218.28,197.91,7.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,204.11,207.34,192.27,7.86">Yoshua: Learning Deep Architectures for AI</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,411.18,207.34,69.42,7.86;10,151.52,218.30,122.84,7.86">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,229.26,337.98,7.86;10,151.52,240.22,329.07,7.86;10,151.52,251.18,208.35,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,382.76,229.26,97.83,7.86;10,151.52,240.22,248.00,7.86">Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,421.91,240.22,58.68,7.86;10,151.52,251.18,124.69,7.86">Field Guide to Dynamical Recurrent Networks</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,262.14,337.98,7.86;10,151.52,273.07,283.36,7.89" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,242.30,262.14,238.30,7.86;10,151.52,273.10,147.83,7.86">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CoRR abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,284.06,337.98,7.86;10,151.52,295.02,329.07,7.86;10,151.52,305.98,329.07,7.86;10,151.52,316.93,45.10,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,249.01,284.06,231.58,7.86;10,151.52,295.02,61.33,7.86">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,233.12,295.02,247.47,7.86;10,151.52,305.98,142.17,7.86">Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10)</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics (AISTATS10)</meeting>
		<imprint>
			<publisher>Society for Artificial Intelligence and Statistics</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,327.89,337.97,7.86;10,151.52,338.85,329.07,7.86;10,151.52,349.81,181.36,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,303.32,327.89,177.27,7.86;10,151.52,338.85,87.40,7.86">Rectifier Nonlinearities Improve Neural Network Acoustic Models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,262.27,338.85,218.33,7.86;10,151.52,349.81,100.47,7.86">ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,360.77,337.97,7.86;10,151.52,371.73,329.07,7.86;10,151.52,382.66,208.58,7.89" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,151.52,371.73,288.37,7.86">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,450.24,371.73,30.35,7.86;10,151.52,382.69,121.69,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,393.65,246.63,7.86" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,213.25,393.65,147.32,7.86">Neural Networks and Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Nielsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,404.61,337.98,7.86;10,151.52,415.56,329.07,7.86;10,151.52,426.52,231.00,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,293.24,404.61,187.35,7.86;10,151.52,415.56,174.50,7.86">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,346.88,415.56,133.70,7.86;10,151.52,426.52,153.80,7.86">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,437.48,337.98,7.86;10,151.52,448.41,94.60,7.89" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,253.77,437.48,188.39,7.86">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
