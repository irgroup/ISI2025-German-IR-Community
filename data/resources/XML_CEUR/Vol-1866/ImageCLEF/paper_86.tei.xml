<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.00,115.96,321.36,12.62;1,184.85,133.89,245.65,12.62;1,254.40,151.82,106.57,12.62">VC-I2R@ImageCLEF2017: Ensemble of Deep Learned Features for Lifelog Video Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.51,189.49,99.72,8.74;1,242.24,187.92,2.39,6.12"><forename type="first">Ana</forename><surname>Garcia Del Molino</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Visual Computing Department</orgName>
								<orgName type="institution">Institute for Infocomm Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">STAR</orgName>
								<address>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">NTU</orgName>
								<address>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">STAR</orgName>
								<address>
									<settlement>Singapore</settlement>
									<region>{stugdma</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.58,189.49,86.21,8.74"><forename type="first">Bappaditya</forename><surname>Mandal</surname></persName>
							<email>bmandal@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Visual Computing Department</orgName>
								<orgName type="institution">Institute for Infocomm Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,362.34,189.49,30.17,8.74"><forename type="first">Jie</forename><surname>Lin</surname></persName>
							<email>lin-j@i2r.a-star.edu.sg</email>
						</author>
						<author>
							<persName coords="1,403.06,189.49,62.55,8.74"><forename type="first">Joo</forename><forename type="middle">Hwee</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Visual Computing Department</orgName>
								<orgName type="institution">Institute for Infocomm Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,189.77,201.45,109.15,8.74"><forename type="first">Vigneshwaran</forename><surname>Subbaraju</surname></persName>
							<email>subbaraju@sbic.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Visual Computing Department</orgName>
								<orgName type="institution">Institute for Infocomm Research</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Singapore Bioimaging Consortium</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.08,201.45,92.27,8.74"><forename type="first">Vijay</forename><surname>Chandrasekhar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Visual Computing Department</orgName>
								<orgName type="institution">Institute for Infocomm Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.00,115.96,321.36,12.62;1,184.85,133.89,245.65,12.62;1,254.40,151.82,106.57,12.62">VC-I2R@ImageCLEF2017: Ensemble of Deep Learned Features for Lifelog Video Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9B837AD7FCE72B07EED7729058028BF6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our approach for the ImageCLEFlifelog summarization task. A total of ten runs were submitted, which used only visual features, only metadata information, or both. In the first step, a set of relevant frames are drawn from the whole lifelog. Such frames must be of good visual quality, and match the given task semantically. For the automatic runs, this subset of images is clustered into events, and the key-frames are selected from the clusters iteratively. In the interactive runs, the user can select which frames to keep or discard in each interaction, and the clustering is adapted accordingly. We observe that the more relevant features to be used depend on the context and the nature of the input lifelog.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rising availability of affordable wearable recording devices in the market (e.g. SenseCam or Narrative Clip), as well as the presence of countless mobile apps for fitness and lifestyle tracking, one may resort to personal Life-logging solutions to create memory collections or monitor their own life. However, little support is available for the browsing of such digital memories, and as a result, our phones and computers can get filled with personal information we may never revisit or analyze.</p><p>To solve this problem, ImageCLEF LifeLog Task <ref type="bibr" coords="1,374.03,572.43,10.52,8.74" target="#b4">[6,</ref><ref type="bibr" coords="1,386.20,572.43,12.73,8.74" target="#b11">13]</ref> aims to bring attention of researchers from diverse fields to study, evaluate and propose new methodologies to address the challenging problems in lifelog video summarization tasks. This rigorous comparative benchmarking would help the various research groups to evaluate their existing methodologies among each others on a common platform and also spur new thinking for solving the long standing key problems. In the following section we discuss these key challenges and related work in the literature. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Key-Frame Selection</head><p>• Top n k images per cluster, where n decreases in each iteration according to the existing user selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering for Diversity</head><p>• K-means: images sorted by distance to cluster center. • Hierarchic tree:</p><p>images sorted by relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Key-Frame Selection</head><p>• Top n k images per cluster.</p><p>Fig. <ref type="figure" coords="2,153.45,292.05,3.87,8.74">1</ref>: Pipeline of our runs. The input image stream is first analyzed to remove bad quality images. Then, the remaining frames are assessed in terms of relevance to the task. The top images are selected to be clustered into different events.</p><p>Finally, at least one key-frame is selected from each event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Summarization of egocentric videos has become a problem of much interest. In a recent comprehensive survey on the summarization of high temporal resolution videos <ref type="bibr" coords="2,165.74,402.53,14.61,8.74" target="#b17">[19]</ref>, we note the importance of context dependency to generate better summaries. For low resolution videos, two recent surveys review the methods for better summarizing Lifelogs for memory augmentation <ref type="bibr" coords="2,376.54,426.44,15.50,8.74" target="#b9">[11]</ref> and storytelling <ref type="bibr" coords="2,467.31,426.44,9.96,8.74" target="#b2">[4]</ref>. The authors of <ref type="bibr" coords="2,203.08,438.40,15.50,8.74" target="#b9">[11]</ref> observe that our memory recall accuracy is directly related to how different that episode is from the rest of memories.</p><p>In <ref type="bibr" coords="2,162.90,462.31,10.52,8.74" target="#b2">[4]</ref> four steps in the process are identified: informative pictures filtering (removal of blurred or dark images and those with useless content <ref type="bibr" coords="2,423.46,474.27,14.76,8.74" target="#b22">[24]</ref>), episode segmentation (using low level features k -means, eigenvalues or graphs, and timedependent methods <ref type="bibr" coords="2,222.61,498.18,10.52,8.74" target="#b3">[5,</ref><ref type="bibr" coords="2,234.79,498.18,7.20,8.74" target="#b7">9]</ref>), summarization (based on representativeness, or relying in the presence of important people/objects <ref type="bibr" coords="2,344.18,510.13,15.50,8.74" target="#b14">[16]</ref>) and retrieval (by means of encoded context, people, objects or activities <ref type="bibr" coords="2,334.15,522.09,15.50,8.74" target="#b8">[10,</ref><ref type="bibr" coords="2,351.31,522.09,11.62,8.74" target="#b13">15]</ref>).</p><p>All the aforementioned surveys conclude that richer semantic-level features are needed to encode the different episodes. Moreover, the key-frames included in a summary should be diverse, informative, and good memory triggers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Methodologies</head><p>We compare a series of summarization methods that (i) filter out uninformative images; (ii) rank the remaining images according to how well they match the given query; (iii) cluster the top ranked images into a series of events; and (iv) select, in an iterative manner, as many images per cluster as to fill the length budget. Fig. <ref type="figure" coords="2,191.08,656.12,4.98,8.74">1</ref> shows the flow of our proposed approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pre-processing Techniques</head><p>The incoming frame stream is pre-processed to evaluate the quality and informativeness of the images. All frames below a certain quality threshold are then discarded. The quality rate is obtained by combining the following scores:</p><p>Blurriness assessment We use two different methodologies: Modified Laplacian: This method applies a non-linear filter operation on an image and filters out the prominent edges from the input image using a Gaussian kernel along the x-and y-directions. The edge score is taken as the mean value of all absolute values.</p><p>Variance Of Laplacian: The input image is convolved with the Laplacian operator (3 × 3 kernel). Then, the variances (i.e. standard deviation squared) of the response are computed. All values over a certain threshold are averaged to obtain a blurriness score.</p><p>Color diversity Images with very homogeneous colors are deem to be uninformative, since this usually means there are few different objects in the image. We compute the histograms of the quantized RGB values, and find the color diversity score based on the frequency of the predominant color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task Relevance Retrieval</head><p>Since the summaries are query-driven, we first need to evaluate the relevance of each image to the given task. For each task, we define a set of objects and places to be found or avoided in the target images (as listed in Table <ref type="table" coords="3,409.47,632.21,3.87,8.74" target="#tab_0">1</ref>). Additionally, from the location and activity information available in the metadata, we define the relevant locations and activities, and the ones to avoid strictly. For each image, a relevance score is given by the presence of such key objects, places, locations and activities, and the number of people in tasks In a meeting and Social drinking. To fuse all these aspects, each one is given a different weight, as described in section 3.1. The N images with higher relevance score are drawn and used in the following steps.</p><p>Image Features Used: Image understanding is crucial for lifelog data analysis, of which, what objects are present in the images and where are the images taken is capable of linking lifelog images to certain topics/events. Here, our objective is to estimate "what" and "where" of the lifelog images, using deep convolutional neural networks (DCNN) <ref type="bibr" coords="5,247.92,246.03,14.61,8.74" target="#b12">[14]</ref>.</p><p>Objects and Places: We use DCNN respectively trained on ImageNet1K <ref type="bibr" coords="5,450.84,271.44,10.52,8.74" target="#b6">[8]</ref> and Places365 <ref type="bibr" coords="5,180.46,283.40,10.52,8.74" target="#b1">[3]</ref> to identify objects and places depicted in the Lifelog images. The ImageNet1K training set contains 1000 object categories from WordNet and 1.2 million images, the Places365 train set has 365 place categories and around 1.8 million images. A separate ResNet152 <ref type="bibr" coords="5,299.42,319.27,15.50,8.74" target="#b10">[12]</ref> is pre-trained on each dataset (termed as ResNet152-ImageNet1K and ResNet152-Places365). During test, by passing a lifelog image through ResNet152-ImageNet1K (ResNet152-Places365), a 1000-D (365-D) probability vector is extracted from the last layer (after Softmax). As in <ref type="bibr" coords="5,146.33,367.09,14.60,8.74" target="#b18">[20]</ref>, data augmentation is performed to generate scaled and rotated versions for each lifelog image. The maximum activation value (instead of the average) is chosen for each class. These probability vectors serve as object and place features for the retrieval stage.</p><p>Besides image-level object recognition, we also perform object detection to locate objects in lifelog images. A Faster R-CNN <ref type="bibr" coords="5,334.21,427.16,15.50,8.74" target="#b20">[22]</ref> is pre-trained on MSCOCO <ref type="bibr" coords="5,470.75,427.16,15.50,8.74" target="#b15">[17]</ref> training dataset, containing bounding box annotations for more than 200K images over 80 object categories. Most of the categories are common in lifelog images (e.g. laptop and tv). Given a lifelog image as test input to Faster R-CNN, we compute maximum probability for each category over the top 20 detections, empirically. The maximum probabilities serve as detection features for the subsequent relevance stage.</p><p>Human Detections &amp; Counting: Detecting and counting the number of persons in an image may provide vital information which may be useful to determine the relevance of the image for a particular query. The most popular method for detecting people in an image is by using the histogram-of-gradients (HOG) approach. We tried this approach, however, due to the lack of sufficient and representative number of training samples from this database and possible involvement of huge manual efforts, good training cannot be achieved.</p><p>Several commercial entities also provide cloud-based APIs that perform the task of detecting and counting the humans in an image. Many of these entities use proprietary deep learning based approaches to perform the task of human detection. We selected the person detection API provided by Sighthound, Inc.</p><p>[2], as it provided convenient features such as detecting and counting people, as well as providing the coordinates of the bounding box of the detected people. The performance of the Sighthound API on several computer vision tasks on benchmark datasets has been studied and a superior performance has been reported <ref type="bibr" coords="6,166.57,154.86,10.52,8.74" target="#b5">[7,</ref><ref type="bibr" coords="6,178.74,154.86,11.62,8.74" target="#b16">18]</ref>. The pre-trained model used by this API requires that the person in the image should occupy at least 96 × 40 pixels for upper/full body detection and at least 72 × 64 pixels for head and shoulders. In general we found the API to be more accurate on images with good lighting conditions and when the head and shoulders of the person are clearly visible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Event Clustering</head><p>The N images with best relevance score are then described with the concatenation of all the available features. Each feature (deep learned features, locations, activities, day and time) is given a weight which can be 0, 1, the feature frequency score tf (for the deep learned features), or the inverse of its maximum (for the metadata).</p><p>The images are then clustered into events using either k -means or a hierarchical tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Features Used:</head><p>Objects and Places: A part from the object and place features described in section 2.2, we also test describing the images with low-level deep descriptors. Using the widely used VGG16 architecture <ref type="bibr" coords="6,321.43,402.51,15.50,8.74" target="#b21">[23]</ref> pre-trained on ImageNet1K data set, we extract 512 feature maps from the last pooling layer (i.e. pool5) for each augmented image, followed by nested invariance pooling over all feature maps <ref type="bibr" coords="6,162.13,438.38,14.61,8.74" target="#b19">[21]</ref>. This results in a 512-D global descriptor representing each lifelog image. Post-processing techniques (e.g. PCA whitening) can be applied to further enhance the discriminative power of the pooled descriptors.</p><p>Locations and Activities: Each location in the user's lifelog is given a unique id. Same process is done for the activity tag.</p><p>Day and Time: From the image TimeStamp, we extract the day of the month and the hour it was taken. Alternatively, we quantize the hour into morning, afternoon, evening or night.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Key-Frame Selection</head><p>To select the key-frames, all frames in each cluster c i = {f i k } are ranked according to distance to the cluster center (for k -means clustering) or relevance score (for hierarchical trees), so that c i = [f k ] i , and the summary is initialized empty, S = []. The following process is repeated iteratively until reaching the desired summary length X: The first available image in each cluster is selected to be part of the final summary s = {f k | ∀ i , k = 0}, and discarded from the bag of available frames. Then, the selection is sorted according to each frame's relevance score, so that the most relevant are first in the generated summary. The sorted sequence is added at the end of the summary, S = S s. Note that to force that each event will be represented if selecting a summary shorter than X, the sequence to be sorted is the newly drawn s, and never S. If, at the end of the drawing process, the cadence of S is greater than X, the last elements of S are discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>For this task, we submit two different sets of runs: automatic and interactive. In the interactive, we give the user the opportunity of removing, replacing and adding frames from the automatically generated summary. The bag of frames from where the user can replace images is the same set of relevant images as in the automatic approach. In this section, we will first present the parameters used to find the set of relevant images, and then explain in more detail the two types of submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning the Best Parameters</head><p>We use the development set <ref type="bibr" coords="7,257.26,376.50,10.52,8.74" target="#b4">[6]</ref> to train the relevance weights and select the best features to fuse into the image descriptor. For the five test tasks that do not match any of the development ones, we have split the test set into two parts, and used the smaller of them as training data. All parameters are learned for each run and task separately.</p><p>Quality Filtering For most tasks, the quality threshold is defined by the 35 or 50 percentile, depending on whether the run uses only image features, or also metadata (note that since the quality assessment is visual, the threshold is set to zero when only using metadata). For task 6 (Social Drinking), where images are usually taken in dark places and are thus of bad quality, the threshold is set to be of 10 percentile.</p><p>Computing the Relevance Score The relevance score is a fusion of three modules: the visual score, obtained from the DCNN activations; the location and activity relevance from tags; and the locations and activities to remove.</p><p>The visual relevance score for each frame is the dot product between its descriptor and the reference query descriptor. For this purpose, the frame descriptor is defined as the 1365D vector of activations. To define the reference query descriptor, relevant object classes are found by using the WordNet [1] structure on two or three key concepts, and places are selected manually (Table 1). The reference query descriptor is initialized to zeros. Then, all classes present among the wanted objects are given a constant value w objy , the objects @5 @10 @20 @30 @40 @50 0.1 0. (a) Average for all tasks @5 @10 @20 @30 @40 @50 0 0.2 0.4 0.6 0.8 1 Task 1 @5 @10 @20 @30 @40 @50 0 0.2 0.4 0.6 0.8 1 Task 2 @5 @10 @20 @30 @40 @50 0 0.2 0.4 0.6 0.8 1 Task 3 @5 @10 @20 @30 @40 @50 0 0.2 0.4 0.6 0.8 1 Task 4 @5 @10 @20 @30 @40 @50 0 0.2 0.4 0.6 0.8 1 Task 5 @5 @10 @20 @30 @40 @50 0 0.2 0.4 0.6 0.8 1 Task 6 @5 @10 @20 @30 @40 @50 0 0.2 0.4 0.6 0.8 1 Task 7 @5 @10 @20 @30 @40 @50 0 0.2 0.4 0.6 0.8 1 Task 8 @5 @10 @20 @30 @40 @50 0 0.2 0.4 0.6 0.8 1 Task 9 @5 @10 @20 @30 @40 @50 0    to avoid are set to w objn , and the same is done for places. Additionally, we perform object detection as described in section 2.2, and a weight w coco is applied over the score of the relevant items. Following, a value w loc is added to the relevance score of all these images with a relevant location label, and the score for frames matching the relevant activity is increased w act .</p><p>Finally, all the frames with location or activity label to avoid are given a relevance score of 0, and thus removed from the pool of frames.</p><p>Additionally, for tasks 1 (In a meeting) and 6 (Social Drinking), where the presence of other people is a task-relevance indicator, we count people as described in section 2.2, and increase the relevance score of those images with enough people by w ppl . We observe that, given that relevant images in task 1 have many occlusions, and that images in task 6 have poor lightning conditions, the performance of the people detector for such tasks is not good enough.</p><p>The final relevance value is smoothed using a triangular window of size win, which ranges between 1 (0 extra frames) and 11 (5 frames to each side). The optimal values of w coco , w objy , w objn , w ply , w pln , w loc , w act , w ppl and win are found heuristically by analyzing the retrieval performance in the training data, as shown in Fig. <ref type="figure" coords="9,208.47,334.19,3.87,8.74">3</ref>. The objective is best recall at X = 400, to have the greatest number of events brought forward for the next step in the summarization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Automatic Runs</head><p>We submitted seven different automatic runs. Such runs are compiled in Table <ref type="table" coords="9,472.84,394.34,3.87,8.74" target="#tab_3">2</ref>, and are defined by the range of features used: only image (with and without object detection), only metadata and mixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering the Lifelog Images into Events</head><p>The weights for each feature in the image descriptor for each task are defined by the best combination in the test set. The images are then clustered into M events using k -means, or hierarchical trees when only using metadata. The number of events M is set to be equal or smaller than the summary length budget. When using k -means, the frames with relevance below the 50 percentile for each cluster are discarded. The selection and ranking of keyframes from the clusters is described in section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Interactive Runs</head><p>We submitted three different interactive runs, as compiled in Table <ref type="table" coords="9,436.37,567.26,3.87,8.74" target="#tab_3">2</ref>. The interaction time per task is of 3 on average, ranging between 1 30 and 4 40 .</p><p>Clustering the Lifelog Images into Events For the interactive runs, kmeans is chosen for clustering the relevant images into M events, where M is greater than the summary length budget. In each iteration, the user can select which images to preserve for the final summary, which frames to remove from the bag of relevant images, and whether all other images in the cluster should Fig. <ref type="figure" coords="10,153.45,324.55,3.87,8.74">5</ref>: GUI for Interactive Summarization. Frames are shown with information of their location and timestamp, and those already selected for the final summary are marked "In Summary". The user can select the frames to be included (green Y ), to be removed (red N ), or to remove all frames in the same cluster (*N ). be removed (Fig. <ref type="figure" coords="10,215.79,394.80,3.87,8.74">5</ref>). Two methodologies for updating the summary are proposed: First, re-clustering the remaining frames. Second, using the same initial clustering for all iterations.</p><p>In the first approach, the frames are clustered into a 20% more clusters than additional keyframes needed (note that this number changes at each iteration). For the second approach, two configurations are tested: clustering into either a 20% or 100% more frames than the length of the final summary. Once clustered, the frames closest to each cluster center are chosen as candidates to be added to the summary. The most relevant ones (as many as needed to fill the summary budget length) are then included in the proposed summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>By looking at the results in Fig. <ref type="figure" coords="10,274.67,548.52,3.87,8.74" target="#fig_3">4</ref>, we can observe that the use of mixed features generally improves the performance. Using only metadata (run 3) is, in average, worse than using only visual features (runs 1 and 8). We noted that the location metadata was not very precise in terms of starting and finishing times. It is interesting to see how interactive approaches (runs 4, 5 and 10) do not necessarily perform better than automatic ones for low values of X. This may be due to the subjectivity of such kind of retrieval task. One may think that, being the images handpicked (and sorted in drawing order), the precision score should be much higher than the automatic runs, and close to 1 for low values of X (being those the first selected keyframes). However, this only happens for some tasks.</p><p>Analyzing each task independently, we observe that visual features are not very precise for tasks 1 (In a meeting) and 4 (Working at home). Surprisingly, using only visual features yield the best results for task 6 (Social drinking) for low values of X, specially if also using object detection, which performs best for larger X. An outstanding retrieval performance (precision greater than 80% for all X) is achieved with a mix of visual and metadata features for tasks 7 (Sightseeing), 8 (Transporting) and 10 (Shopping), possibly due to the quality of the metadata (although the performance of using only visual features is competitive with the mixed approaches). The best results for tasks 2 (Watching TV ), 4 (Working at home), 5 (Eating), 9 (Preparing meals) and 10 (Shopping) are obtained with the interactive approaches, since recall can be improved when manually selecting images from different clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we have presented a generic framework for the summarization of lifelogs given a target task. It can be used both in an automatic or interactive way, with the user providing feedback on the retrieved frames. The proposed approaches require that the user selects the relevant locations for each task. In order to ease this forced manual input, the metadata obtained with lifelog apps could contain additional info of, e.g. , the nature of each location.</p><p>We have observed that different tasks require different summarization methodologies (e.g. different weights), which may not be completely consistent when changing the lifelog input. Trained on the development set, we have obtained a 0.497 best averaged F1 score @X = 10 (the averaged F1 for the best run for each task is 0.563), meaning there is still a lot of room for improvement. We encourage other researchers to participate in future such competitions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,213.05,345.82,8.74;3,134.77,225.01,345.83,8.74;3,134.77,236.96,345.83,8.74;3,134.77,248.92,345.83,8.74;3,134.77,260.87,345.83,8.74;3,134.77,272.83,345.83,8.74;3,134.77,284.78,301.56,8.74"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Flow of summary generation in the proposed methodology. Each scatter plot represents the frames in the lifelog for task 10. (a) Input data. Colors correspond to temporal evolution. (b) Remaining images after quality preprocessing (bad quality images filtered out in light gray) and relevance assessment (irrelevant images filtered out in darker gray). (c) Final summary. The keyframes are represented with different colors to map the clusters, and larger size to represent higher positions in the final summary ranking. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,405.63,356.21,4.34,2.89;8,405.63,347.32,4.34,2.89;8,405.63,338.42,4.34,2.89;8,405.63,329.52,4.34,2.89;8,408.23,320.63,1.74,2.89;8,429.09,317.01,11.10,2.89;8,232.65,377.08,150.06,7.86"><head></head><label></label><figDesc>Results for each task individually</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,134.77,398.02,345.83,8.74;8,134.77,409.97,167.73,8.74"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Results for all the submitted runs: F1 score plotted in solid line, and precision doted. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="10,134.77,115.83,345.82,197.19"><head></head><label></label><figDesc></figDesc><graphic coords="10,134.77,115.83,345.82,197.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,120.57,345.83,539.74"><head>Table 1 :</head><label>1</label><figDesc>Fig.3: Weight learning from the development set. Description of the parameters: q = quality threshold; w =[w coco , w objy , w objn , w ply , w pln , w loc , w act , w ppl ]; win = size of the smoothing window. (Best viewed in color.) Semantic queries for the retrieval task: Concepts to search in WordNet to find all related (and to avoid) ImageNet classes, manual selection of relevant (and to avoid) places classes, and objects to detect.</figDesc><table coords="4,137.07,120.57,342.71,496.88"><row><cell></cell><cell></cell><cell>task 1 (X=400)</cell><cell></cell><cell></cell><cell></cell><cell>task 2 (X=400)</cell><cell></cell><cell></cell><cell></cell><cell>task 5 (X=400)</cell><cell></cell><cell></cell><cell></cell><cell>task 6 (X=400)</cell><cell></cell><cell></cell><cell cols="2">task 10 (X=400)</cell><cell></cell></row><row><cell>1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell cols="2">0.8</cell><cell></cell><cell></cell><cell cols="2">0.8</cell><cell></cell><cell></cell><cell cols="2">0.8</cell><cell></cell><cell></cell><cell cols="2">0.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell cols="2">0.6</cell><cell></cell><cell></cell><cell cols="2">0.6</cell><cell></cell><cell></cell><cell cols="2">0.6</cell><cell></cell><cell></cell><cell cols="2">0.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell cols="2">0.4</cell><cell></cell><cell></cell><cell cols="2">0.4</cell><cell></cell><cell></cell><cell cols="2">0.4</cell><cell></cell><cell></cell><cell cols="2">0.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell cols="2">0.2</cell><cell></cell><cell></cell><cell cols="2">0.2</cell><cell></cell><cell></cell><cell cols="2">0.2</cell><cell></cell><cell></cell><cell cols="2">0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">q=50; w=2.7.0.0.0.6.0.1; win=1</cell><cell></cell><cell cols="3">q=0; w=6.4.-4.5.0.1.4.1; win=1</cell><cell></cell><cell cols="3">q=35; w=4.8.-3.2.0.0.0.1; win=1</cell><cell></cell><cell cols="3">q=0; w=2.0.0.8.0.1.1.1; win=3</cell><cell></cell><cell cols="3">q=35; w=0.0.0.3.0.3.1.1; win=3</cell></row><row><cell></cell><cell cols="3">q=50; w=2.7.0.0.0.6.0.10; win=1</cell><cell></cell><cell cols="3">q=0; w=6.4.-4.5.0.1.6.1; win=1</cell><cell></cell><cell cols="3">q=35; w=4.8.-3.2.0.0.1.1; win=1</cell><cell></cell><cell cols="3">q=10; w=2.0.0.8.0.1.1.1; win=3</cell><cell></cell><cell cols="3">q=35; w=0.2.0.3.0.3.1.1; win=9</cell></row><row><cell></cell><cell cols="3">q=50; w=2.7.0.0.0.6.0.2; win=1</cell><cell></cell><cell cols="3">q=0; w=6.4.-4.5.0.2.4.1; win=1</cell><cell></cell><cell cols="3">q=35; w=4.8.-3.2.0.0.2.1; win=1</cell><cell></cell><cell cols="3">q=0; w=2.0.0.8.0.1.1.2; win=3</cell><cell></cell><cell cols="3">q=35; w=0.2.-2.3.0.3.1.1; win=5</cell></row><row><cell></cell><cell cols="3">q=50; w=2.7.0.0.0.8.0.1; win=1</cell><cell></cell><cell cols="3">q=0; w=6.4.-4.5.0.2.6.1; win=1</cell><cell></cell><cell cols="3">q=35; w=4.8.-5.2.0.0.0.1; win=1</cell><cell></cell><cell cols="3">q=10; w=2.0.0.8.0.1.1.2; win=3</cell><cell></cell><cell cols="3">q=35; w=0.0.-1.3.0.3.1.1; win=3</cell></row><row><cell></cell><cell cols="3">q=50; w=2.7.0.0.0.8.0.10; win=1</cell><cell></cell><cell cols="3">q=0; w=6.4.-4.5.-2.1.4.1; win=1</cell><cell></cell><cell cols="3">q=35; w=4.8.-5.2.0.0.1.1; win=1</cell><cell></cell><cell cols="3">q=0; w=2.0.0.8.0.1.2.1; win=3</cell><cell></cell><cell cols="3">q=35; w=0.2.0.3.0.3.1.1; win=5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Objects</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Places</cell><cell></cell><cell></cell><cell></cell><cell cols="2">MSCOCO</cell></row><row><cell cols="2">Task</cell><cell cols="2">Relevant</cell><cell></cell><cell></cell><cell cols="2">Avoid</cell><cell></cell><cell></cell><cell cols="3">Relevant</cell><cell></cell><cell></cell><cell>Avoid</cell><cell></cell><cell></cell><cell cols="2">Relevant</cell></row><row><cell>1</cell><cell></cell><cell cols="2">computer</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell cols="3">computer</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell cols="2">laptop</cell></row><row><cell></cell><cell></cell><cell cols="3">group meeting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">group meeting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">keyboard</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>etc.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell></cell><cell cols="2">television</cell><cell></cell><cell></cell><cell cols="3">computer</cell><cell></cell><cell cols="3">living room</cell><cell></cell><cell cols="4">conference room</cell><cell></cell><cell>tv</cell></row><row><cell></cell><cell></cell><cell cols="2">food</cell><cell></cell><cell></cell><cell cols="4">group meeting</cell><cell cols="4">television room</cell><cell cols="4">lecture room</cell><cell cols="2">remote</cell></row><row><cell></cell><cell></cell><cell cols="2">glass</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>etc.</cell><cell></cell><cell></cell><cell></cell><cell>etc.</cell><cell></cell><cell></cell><cell cols="2">etc.</cell></row><row><cell>3</cell><cell></cell><cell cols="2">computer</cell><cell></cell><cell></cell><cell></cell><cell>office</cell><cell></cell><cell></cell><cell cols="3">coffee shop</cell><cell></cell><cell cols="4">conference room</cell><cell cols="2">laptop</cell></row><row><cell></cell><cell></cell><cell cols="3">group meeting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">living room</cell><cell></cell><cell></cell><cell>office</cell><cell></cell><cell></cell><cell cols="2">keyboard</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>etc.</cell><cell></cell><cell></cell><cell></cell><cell>etc.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell></cell><cell cols="2">computer</cell><cell></cell><cell></cell><cell></cell><cell>office</cell><cell></cell><cell></cell><cell cols="3">living room</cell><cell></cell><cell cols="4">conference room</cell><cell cols="2">laptop</cell></row><row><cell></cell><cell></cell><cell cols="2">pencil</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">hotel room</cell><cell></cell><cell></cell><cell>office</cell><cell></cell><cell></cell><cell cols="2">book</cell></row><row><cell></cell><cell></cell><cell cols="2">notebook</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>etc.</cell><cell></cell><cell></cell><cell></cell><cell>etc.</cell><cell></cell><cell></cell><cell cols="2">etc.</cell></row><row><cell>5</cell><cell></cell><cell cols="2">food</cell><cell></cell><cell></cell><cell cols="2">drum</cell><cell></cell><cell></cell><cell cols="3">food court</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell cols="2">fork</cell></row><row><cell></cell><cell></cell><cell cols="2">glass</cell><cell></cell><cell></cell><cell cols="3">white goods</cell><cell></cell><cell cols="3">restaurant</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">sandwich</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">menu'</cell><cell></cell><cell></cell><cell></cell><cell>etc.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">etc.</cell></row><row><cell>6</cell><cell></cell><cell cols="2">drink</cell><cell></cell><cell></cell><cell cols="3">computer</cell><cell></cell><cell></cell><cell>bar</cell><cell></cell><cell></cell><cell></cell><cell>home</cell><cell></cell><cell></cell><cell cols="2">bottle</cell></row><row><cell></cell><cell></cell><cell cols="2">glass</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pub</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">wine glass</cell></row><row><cell></cell><cell></cell><cell cols="2">beverage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>etc</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell cols="4">public transport</cell><cell cols="2">temple</cell><cell></cell><cell></cell><cell cols="4">residential neigh.</cell><cell></cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">palace</cell><cell></cell><cell></cell><cell></cell><cell>bus</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>etc.</cell><cell></cell><cell></cell><cell></cell><cell>etc.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">8 public transport</cell><cell></cell><cell>cab</cell><cell></cell><cell></cell><cell cols="3">bus interior</cell><cell></cell><cell cols="3">car interior</cell><cell></cell><cell cols="2">bus</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">car seat</cell><cell></cell><cell></cell><cell cols="4">subway station</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">train</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>taxi</cell><cell></cell><cell></cell><cell></cell><cell>etc.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell></cell><cell cols="2">food</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell cols="2">pantry</cell><cell></cell><cell></cell><cell cols="3">living room</cell><cell></cell><cell cols="2">oven</cell></row><row><cell></cell><cell></cell><cell cols="4">cooking utensil</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">kitchen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">refrigerator</cell></row><row><cell></cell><cell></cell><cell cols="3">white goods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>etc.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">etc.</cell></row><row><cell>10</cell><cell></cell><cell cols="2">shopping</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell cols="3">supermarket</cell><cell></cell><cell cols="3">shopfront</cell><cell></cell><cell></cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">shop</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>store</cell><cell></cell><cell></cell><cell cols="4">shopping mall</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>etc.</cell><cell></cell><cell></cell><cell></cell><cell>etc.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,193.61,621.10,228.14,8.74"><head>Table 2 :</head><label>2</label><figDesc>Description of the different submitted runs.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,494.98,337.64,8.12;11,151.52,506.59,98.85,7.47" xml:id="b0">
	<monogr>
		<ptr target="https://www.sighthound.com/docs/cloud/detection/" />
		<title level="m" coord="11,151.53,494.98,98.08,7.86">Sighthound detection api</title>
		<imprint>
			<publisher>Sighthound, Inc</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,516.39,337.64,7.86;11,151.52,527.35,174.93,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="11,316.03,516.39,164.56,7.86;11,151.52,527.35,55.69,7.86">Places: An image database for deep scene understanding</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L A T</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02055</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,537.79,337.63,7.86;11,151.52,548.75,267.52,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,310.33,537.79,170.26,7.86;11,151.52,548.75,48.34,7.86">Toward storytelling from visual lifelogging: An overview</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bolanos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dimiccoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,207.10,548.75,130.54,7.86">IEEE Trans. HumanMach. Syst</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,559.20,337.63,7.86;11,151.52,570.15,329.07,7.86;11,151.52,581.11,107.89,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,432.07,559.20,48.52,7.86;11,151.52,570.15,253.56,7.86">Visual summary of egocentric photostreams by representative keyframes</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bolanos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Talavera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Giró-I Nieto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,429.28,570.15,51.30,7.86;11,151.52,581.11,41.81,7.86">IEEE ICME Workshops</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,591.56,337.64,7.86;11,151.52,602.52,329.07,7.86;11,151.52,613.48,329.07,7.86;11,151.52,624.44,160.43,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,151.52,602.52,308.83,7.86">Overview of ImageCLEFlifelog 2017: Lifelog Retrieval and Summarization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,613.48,323.88,7.86">CLEF 2017 Labs Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,634.88,337.64,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,97.80,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="11,374.27,634.88,106.33,7.86;11,151.52,645.84,257.63,7.86">Dager: Deep age, gender and emotion recognition using convolutional neural network</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">G</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Z</forename><surname>Masood</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04280</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.96,119.67,337.64,7.86;12,151.52,130.63,214.48,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,399.39,119.67,81.21,7.86;12,151.52,130.63,137.62,7.86">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,310.72,130.63,26.62,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,141.59,337.64,7.86;12,151.52,152.55,329.07,7.86;12,151.52,163.51,204.25,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,446.04,141.59,34.55,7.86;12,151.52,152.55,274.42,7.86">Combining image descriptors to effectively retrieve events from visual lifelogs</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Conaire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blighe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,445.37,152.55,35.22,7.86;12,151.52,163.51,100.92,7.86">Proceedings of the 1st ACM MIR</title>
		<meeting>the 1st ACM MIR</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,174.47,337.98,7.86;12,151.52,185.43,307.98,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,302.93,174.47,177.66,7.86;12,151.52,185.43,103.42,7.86">Towards memory supporting personal information management tools</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Elsweiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ruthven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,261.65,185.43,107.24,7.86">J. Assoc. Inf. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="924" to="946" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,196.39,337.97,7.86;12,151.52,207.34,320.72,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,316.96,196.39,163.63,7.86;12,151.52,207.34,146.37,7.86">Remembering through lifelogging: A survey of human memory augmentation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Langheinrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,305.40,207.34,97.22,7.86">Pervasive Mob Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="14" to="26" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,218.30,337.98,7.86;12,151.52,229.26,69.37,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,299.05,218.30,177.61,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,165.60,229.26,26.61,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,240.22,337.97,7.86;12,151.52,251.18,329.07,7.86;12,151.52,262.14,329.07,7.86;12,151.52,273.10,329.07,7.86;12,151.52,284.06,329.07,7.86;12,151.52,295.02,329.07,7.86;12,151.52,305.98,199.89,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,433.34,262.14,47.26,7.86;12,151.52,273.10,215.41,7.86">Overview of ImageCLEF 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,387.32,273.10,93.27,7.86;12,151.52,284.06,329.07,7.86;12,151.52,295.02,105.52,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction 8th International Conference of the CLEF Association, CLEF</title>
		<title level="s" coord="12,285.42,295.02,143.89,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-09-11">2017. September 11-14 2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,316.93,337.98,7.86;12,151.52,327.89,176.85,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,328.09,316.93,152.50,7.86;12,151.52,327.89,105.15,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,278.07,327.89,21.63,7.86">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,338.85,337.98,7.86;12,151.52,349.81,307.27,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,240.72,338.85,239.87,7.86;12,151.52,349.81,60.62,7.86">Providing good memory cues for people with episodic memory impairment</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,233.73,349.81,112.42,7.86">ACM SIGACCESS ASSETS</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,360.77,337.98,7.86;12,151.52,371.73,248.40,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,297.58,360.77,183.01,7.86;12,151.52,371.73,125.46,7.86">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,298.56,371.73,26.62,7.86">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,382.69,337.98,7.86;12,151.52,393.65,329.07,7.86;12,151.52,404.61,104.57,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,328.62,393.65,151.96,7.86;12,151.52,404.61,28.22,7.86">Microsoft COCO: common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,200.93,404.61,26.48,7.86">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,415.56,337.98,7.86;12,151.52,426.52,329.07,7.86;12,151.52,437.48,69.14,7.86" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="12,365.78,415.56,114.81,7.86;12,151.52,426.52,260.60,7.86">License plate detection and recognition using deeply learned convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Z</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">G</forename><surname>Ortiz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07330</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,448.44,337.98,7.86;12,151.52,459.40,329.07,7.86;12,151.52,470.36,25.60,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,366.46,448.44,114.13,7.86;12,151.52,459.40,130.11,7.86">Summarization of egocentric videos: A comprehensive survey</title>
		<author>
			<persName coords=""><forename type="first">Garcia</forename><surname>Del Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,290.10,459.40,134.87,7.86">IEEE Trans. HumanMach. Syst</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="76" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,481.32,337.97,7.86;12,151.52,492.28,329.07,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,327.82,481.32,152.77,7.86;12,151.52,492.28,149.75,7.86">Describing lifelogs with convolutional neural networks: A comparative study</title>
		<author>
			<persName coords=""><forename type="first">Garcia</forename><surname>Del Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,321.43,492.28,58.41,7.86">LTA Workshop</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="39" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,503.24,337.98,7.86;12,151.52,514.19,329.07,7.86;12,151.52,525.15,25.60,7.86" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="12,453.15,503.24,27.45,7.86;12,151.52,514.19,257.75,7.86">Nested invariance pooling and rbm hashing for image instance retrieval</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Morère</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Veillard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ACM ICMR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,536.11,337.97,7.86;12,151.52,547.07,222.24,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,308.79,536.11,171.80,7.86;12,151.52,547.07,150.54,7.86">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,323.46,547.07,21.62,7.86">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,558.03,337.97,7.86;12,151.52,568.99,142.10,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,278.92,558.03,201.67,7.86;12,151.52,568.99,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,242.42,568.99,22.52,7.86">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,579.95,337.98,7.86;12,151.52,590.91,171.67,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,258.61,579.95,221.98,7.86;12,151.52,590.91,44.49,7.86">Detecting snap points in egocentric video with a web photo prior</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,216.84,590.91,26.49,7.86">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="282" to="298" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
