<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,125.90,150.75,343.40,12.64;1,150.62,174.99,294.09,12.64">Application of Deep Learning Neural Network for Classification of TB Lung CT Images Based on Patches</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,242.93,215.22,58.44,8.96"><forename type="first">Xiaohong</forename><surname>Gao</surname></persName>
							<email>x.gao@mdx.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Middlesex University</orgName>
								<address>
									<postCode>NW4 4BT</postCode>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.27,215.22,34.12,8.96"><forename type="first">Yu</forename><surname>Qian</surname></persName>
							<email>yu.qian@cortexica.com</email>
							<affiliation key="aff1">
								<orgName type="department">Cortexcia Vision Systems</orgName>
								<address>
									<postCode>SE1 8RT</postCode>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,125.90,150.75,343.40,12.64;1,150.62,174.99,294.09,12.64">Application of Deep Learning Neural Network for Classification of TB Lung CT Images Based on Patches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">89CF7D325F6E583E7C960CBCDF7C6D1B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>patch-based</term>
					<term>classification</term>
					<term>Tuberculosis disease</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, convolutional neural network (CNN) is applied to classify the five types of Tuberculosis (TB) lung CT images. In doing so, each image has been segmented into rectangular patches with side width and high varying between 20 and 55 pixels, which are later normalised into 30x30 pixels. While classifying TB types, six instead of five categories are distinguished. Group 6 houses those patches/segments that are common to most of the other types, or background. In this way, while each 3D dataset only has less than 10% distinguishable volumes that are applied to perform the training, the rest remains part of the learning cycle by participating to the classification, leading to an automated process to differentiation of five types of TB. When tested against 300 datasets, the Kappa value is 0.2187, ranking 5 among 23 submissions. However, the accuracy value of ACC is 0.4067, the highest in this competition of classification of TB types.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Convolutional Neural Network (CNN 1.A Subsection Sample</head><p>Deep learning models refer to a class of computing machines that can learn a hierarchy of features by building high-level attributes from low-level ones <ref type="bibr" coords="1,440.55,595.91,10.87,8.96" target="#b0">[1,</ref><ref type="bibr" coords="1,455.63,595.91,8.35,8.96" target="#b1">2]</ref> , thereby automating the process of feature construction. One of these models is the well-known convolutional neural network (CNN) <ref type="bibr" coords="1,325.96,622.34,10.66,8.96" target="#b1">[2]</ref>. Consisted of a set of algorithms in machine learning, CNN comprises several (deep) layers of processing involving learnable operators (both linear and non-linear), and hence has the ability to learn and build high-level information from low-level features in an automatic fashion <ref type="bibr" coords="1,456.20,662.06,10.79,8.96" target="#b2">[3]</ref>.</p><p>Stemming from biological vision processes, a CNN applies a feed-forward artificial neural network to simulate variations of multilayer perceptrons whereby the individual neurons are tiled in such a way that they respond to overlapping regions in the visual field <ref type="bibr" coords="2,187.30,176.22,10.66,8.96" target="#b3">[4]</ref>. As a direct result, these networks are widely applied to image and video recognition. Specifically, CNNs have demonstrated as an effective class of models for understanding image content, proffering state of the art results on image recognition, segmentation, detection and retrieval. For example, when trained with appropriate regularization, CNNs can achieve superior performance on visual object recognition tasks without relying on any hand-crafted features, e.g. SIFT, SURF. In addition, CNNs have been shown to be relatively insensitive to certain variations on the inputs <ref type="bibr" coords="2,167.63,268.77,10.66,8.96" target="#b4">[5]</ref>. Significantly, recent advances of computer hardware technology (e.g., Graphics Processing Unit (GPU)) have propitiated the implementation of CNNs in representing images.</p><p>Theoretically, CNN can be expressed in the following formulas. For example, for a set of training data , where image is in three-dimension (inclusive of RGB channel as the 3 rd dimension) and the indicator vector of affiliated class of the feature maps of an image, namely, , will be learnt based on CNN by solving Eq. ( <ref type="formula" coords="2,250.01,366.33,3.55,8.96" target="#formula_0">1</ref>).</p><p>(</p><formula xml:id="formula_0" coords="2,448.33,379.53,7.80,8.96">)<label>1</label></formula><p>Where refers to a suitable loss function (e.g. the hinge or log loss) and the selected classifier.</p><p>To obtain these feature maps computationally, in a 2D CNN, convolution is conducted at convolutional layers to extract features from local neighbourhood on the feature maps acquired in the previous layer. Then an additive bias is applied and the result is passed through a sigmoid function as formulated in Eq. ( <ref type="formula" coords="2,387.67,493.67,3.90,8.96" target="#formula_1">2</ref>) mathematically in order to obtain a newly calculated feature value at position on the feature map in the layer.</p><p>(</p><formula xml:id="formula_1" coords="2,441.61,563.15,7.80,8.96">)<label>2</label></formula><p>where the notations of those parameters in Eq. ( <ref type="formula" coords="2,316.40,629.78,3.90,8.96" target="#formula_1">2</ref>) are explained in Table <ref type="table" coords="2,418.22,629.78,3.76,8.96">1</ref>.</p><p>Table <ref type="table" coords="2,242.44,655.99,3.41,8.10">1</ref>. Notations of parameters in Eq. ( <ref type="formula" coords="2,367.31,655.99,3.17,8.10">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Notation Parameter</head><p>hyperbolic tangent function index over the set of feature maps in the layer bias for the feature map in Eq. ( <ref type="formula" coords="3,354.55,190.74,3.55,8.96" target="#formula_0">1</ref>). value at the position (p, q) of the kernel connected to the kth feature map 2D position of a kernel</p><p>As a result, CNN architecture can be constructed by stacking multiple layers of convolution and subsampling in an alternating fashion. The parameters of CNN, such as the bias and the kernel weight are trained using unsupervised approaches <ref type="bibr" coords="3,124.70,315.21,10.88,8.96" target="#b5">[6,</ref><ref type="bibr" coords="3,138.11,315.21,7.20,8.96" target="#b6">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Datasets</head><p>This work is to response to the challenge task of automatic detection of Tuberculosis (TB) types using datasets that is organised by ImageCLEF as put forward in <ref type="bibr" coords="3,444.92,410.01,10.87,8.96" target="#b7">[8,</ref><ref type="bibr" coords="3,459.40,410.01,7.17,8.96" target="#b8">9]</ref>, part of CLEF conference to take place in Dublin <ref type="bibr" coords="3,320.08,427.31,15.90,8.96" target="#b9">[10,</ref><ref type="bibr" coords="3,338.50,427.31,11.92,8.96" target="#b10">11]</ref>.</p><p>As explained in <ref type="bibr" coords="3,206.01,444.47,10.66,8.96" target="#b7">[8]</ref>, data are collected for the evaluation of ImageCLEF tuberculosis competition with 500 datasets of 3D CT lung images (512x512xdepth) for training and further 300 for testing. Among 500 datasets there are 140, 120, 100, 80 and 60 respectively for five TB types of for Infiltrative (type 1), Focal (type 2), Tuberculoma (type 3), Miliary (type 4) and Fibro-cavernous (type 5).</p><p>For the training data, each 3D dataset firstly undergoes pre-processing stage, whereby those artefacts are removed, i.e., slices that contain little visual content will be excluded. In this way, each dataset includes slices between 100 and 170. Then upon each slice, patches of varying sizes are created based on the lung boundary is created from its mask file <ref type="bibr" coords="3,229.80,599.78,15.31,8.96" target="#b11">[12]</ref>. Figure <ref type="figure" coords="3,280.23,599.78,4.98,8.96" target="#fig_0">1</ref> illustrates the pre-processing by depicting the montage of original dataset (top-left), segmented (top-right) and patches segmented from one slice (bottom) that contains at least 80% of lung contents, checked by its mask. to be categorised, class 6 is created to include those patches that appear to be common among the first 5 classes, which again contains about 1000 patches, which is illustrated in Table <ref type="table" coords="5,175.44,253.17,3.76,8.96">2</ref>. Figure <ref type="figure" coords="5,215.95,253.17,4.98,8.96" target="#fig_1">2</ref> exemplifies the five types of patches that are applied in the training, which are all normalised into 30x30 pixels. As discussed above, type 6 remains an extra class to contain those common features shared between types 1 to 5 with patches randomly selected from type 1 to 3, and 5 without type 4, this is because the outstanding features in type 4 with miliary TB spread nearly every slice. Since the patches in each class are randomly generated by computers and selected manually, it is more likely that many patches belong to both of the five classes and class 6, which should not cause too much concerns as the final decision making is based upon the probabilities obtained for the first 5 classes. In addition, the patches belong to each subject will stay together when dividing between training and test data.</p><p>Table <ref type="table" coords="5,203.77,442.48,3.48,9.00">2</ref>. The lists the number of patches in the training process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Architecture of Deep Learning Convolution Neural Network (CNN) with SVM</head><p>The CNN network that has been designed for this task is built upon matConvNet package written using Matlab software<ref type="foot" coords="6,286.13,321.78,3.24,5.83" target="#foot_0">1</ref> . Seven layers of CNN have been designed</p><p>with input data of 30x30 pixels. The filter sizes for each layer are of <ref type="bibr" coords="6,400.44,340.17,10.82,8.96" target="#b5">(6,</ref><ref type="bibr" coords="6,411.26,340.17,7.21,8.96" target="#b5">6)</ref>, <ref type="bibr" coords="6,424.62,340.17,10.82,8.96" target="#b3">(4,</ref><ref type="bibr" coords="6,435.44,340.17,7.21,8.96" target="#b3">4)</ref>, (</p><p>) respectively. At layer 6, instead of scoring features into one of the six classes using Softmax approach that applies cross-entropy loss interpreting the scores as (unnormalised) log probabilities for each class, this study applies support vector machines (SVM) that adapts hinge loss to encourage the correct class to have a score higher by a margin than the other class scores. In this way, each class has a distinguishing boundary. For example, if a feature belongs to one of the two classes with probabilities of 0.51 and 0.49 respectively, the classification is not quite convincing. However, a score with a higher margin, e.g., 0.65, is more acceptable.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Within the existing of 500 datasets that have known ground truth, with reference of patch-wise, the classification results are given in Table <ref type="table" coords="7,346.95,417.57,4.98,8.96" target="#tab_0">3</ref> in percentage with an overall classification rate of 96%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future directions</head><p>While the overall classification appears to be reasonable, with 86.49% accuracy rate, the result for Type 1 only sustains 50%. Since the testing datasets only include 8 subject samples in this type, the remaining work is to evaluate the results from real testing datasets when the ground truth is obtained. In order to obtain higher accuracy, it is recommended that medical knowledge should be embedded. Additionally, 3D segments should be also included to further enhance the characteristics that 3D datasets entail.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,132.50,411.02,330.08,8.10;4,241.61,421.34,111.88,8.10;4,138.86,442.19,331.14,8.96;4,124.70,459.47,345.47,8.96;4,124.70,476.75,345.67,8.96;4,124.70,493.91,345.45,8.96;4,124.70,511.19,345.49,8.96;4,124.70,528.47,345.71,8.96;4,124.70,545.75,162.90,8.96;4,138.86,562.68,331.69,9.31;4,124.70,580.08,345.79,9.21;4,124.70,597.50,345.44,8.96;4,124.70,614.78,345.38,8.96;4,124.70,632.06,345.42,8.96;4,124.70,649.22,345.53,8.96;4,124.70,666.50,345.56,8.96;4,124.70,683.78,345.36,8.96;4,124.70,324.00,345.60,78.50"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Data pre-processing stage. Top-left: original datasets; top-right: Segmented; bottom: patches from segmented slices. At present, two sizes of patches are fixed, including 30×30 and 50×50 pixels in an attempt to cover both small features of nodules and big characteristics of cavity, which are overlapping each other. As a result, each slice entails around 150 patches, leading to more than 1500 patches for each dataset being generated with 100 slices.Since more than 90% of patches are common among all five TB types, for each TB type, around 1000 to 2000 patches are selected, which demonstrate distinguishable visual features and are elaborated below.For classes 4 (Miliary) and 5 (Cavity), the visual features are apparent with widespread dissemination of small spots (i.e. Mycobacterium tuberculosis) and dominating holes (cavities) respectively. However, for classes 1 to 3, visual features are not easily distinguishable. Therefore, the selection starts from background patches. In another words, those patches with very similar visual appearances (background)are pulled together to form Class 6 for the training, leaving remaining ones assuming to be representative. It is very likely that those background patches contain information from their individual classes, which does not appear to pose huge problems</figDesc><graphic coords="4,124.70,324.00,345.60,78.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,127.94,200.39,338.98,9.00;6,242.93,216.83,109.24,9.00"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The examples of patches in each class, which are applied for training. From top to bottom, Types 1, 2, 3, 4, 5, and 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,124.70,495.47,345.68,8.96;6,124.70,512.63,345.50,8.96;6,124.70,529.91,345.45,8.96;6,124.70,547.19,345.56,8.96;6,124.70,564.47,28.25,8.96"><head>Figure 3</head><label>3</label><figDesc>Figure 3 illustrates the CNN architecture that is applied in this study. When those patches are stitched together for each individual subject, class 6 is ignored, i.e., only scores of first five classes are taken into consideration. In this way, computerised selection of patches is made possible to ensure that any kind of patch belongs to a group .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,212.57,329.42,170.15,8.10;7,124.70,147.25,346.05,174.25"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The CNN network applied in this study.</figDesc><graphic coords="7,124.70,147.25,346.05,174.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,133.70,469.24,271.67,146.58"><head>Table 3 .</head><label>3</label><figDesc>Confusion matrix for Patch-wise accuracy.During the competition, 300 testing data were supplied at 2 and are ranked based on Kappa value 2 . This work was ranked 5 out of 23 submissions with 0.2187 Kappa value whereas ACC value was the highest (0.4067), implying that the proposed patchbased deep learning network has achieved averaged best accuracy results in the competition.</figDesc><table coords="7,133.70,502.91,271.67,112.91"><row><cell>Class</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell>1</cell><cell>0.87</cell><cell>0.09</cell><cell>0.03</cell><cell>0</cell><cell>0.01</cell><cell>0</cell></row><row><cell>2</cell><cell>0.02</cell><cell>0.96</cell><cell>0</cell><cell>0</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>3</cell><cell>0</cell><cell>0.03</cell><cell>0.97</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>4</cell><cell>0.04</cell><cell>0</cell><cell>0</cell><cell>0.96</cell><cell>0</cell><cell>0</cell></row><row><cell>5</cell><cell>0.03</cell><cell>0</cell><cell>0</cell><cell>0.01</cell><cell>0.96</cell><cell>0</cell></row><row><cell>6</cell><cell>0</cell><cell>0.04</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.96</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,129.98,667.63,244.09,8.10"><p>MatConvNet: http://www.vlfeat.org/matconvnet/. Retrieved in May</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2017" xml:id="foot_1" coords=""><p></p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,146.11,456.64,324.27,8.10;8,160.70,472.24,310.01,8.10;8,160.70,487.72,309.92,8.10;8,160.70,503.20,250.78,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,423.85,456.64,46.53,8.10;8,160.70,472.24,306.32,8.10">Overview of ImageCLEFtuberculosis 2017, Predicting Tuberculosis Type and Drug Resistances</title>
		<author>
			<persName coords=""><forename type="first">Dicente</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kalinovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename></persName>
		</author>
		<ptr target="http://ceur-ws.org" />
	</analytic>
	<monogr>
		<title level="m" coord="8,172.82,487.72,297.80,8.10;8,160.70,503.20,9.25,8.10">CLEF 2017 Labs Working Notes of CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,146.11,518.80,324.35,8.10;8,160.70,534.28,309.67,8.10;8,160.70,549.76,309.54,8.10;8,160.70,565.36,104.27,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,439.17,518.80,31.29,8.10;8,160.70,534.28,226.25,8.10">Efficient and fully automatic segmentation of the lungs in CT volumes</title>
		<author>
			<persName coords=""><forename type="first">Dicente</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiménez-Del-Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,183.84,549.76,286.40,8.10;8,160.70,565.36,61.60,8.10">Proceedings of the VISCERAL Challenge at ISBI. No. 1390 in CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</editor>
		<meeting>the VISCERAL Challenge at ISBI. No. 1390 in CEUR Workshop Proceedings</meeting>
		<imprint>
			<date type="published" when="2015-04">Apr 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,146.11,580.84,324.32,8.10;8,160.70,596.32,309.97,8.10;8,160.70,611.95,20.35,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,216.28,580.84,254.15,8.10;8,160.70,596.32,210.85,8.10">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,380.83,596.32,35.27,8.10">Biol. Cyb</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,146.11,627.43,324.46,8.10;8,160.70,643.03,271.04,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,343.33,627.43,127.24,8.10;8,160.70,643.03,77.48,8.10">Gradient-based learning applied to document recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,244.01,643.03,87.20,8.10">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.11,149.39,324.19,8.10;9,160.70,164.99,309.80,8.10;9,160.70,180.47,147.32,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,290.98,149.39,179.32,8.10;9,160.70,164.99,134.84,8.10">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,302.69,164.99,167.81,8.10;9,160.70,180.47,72.17,8.10">Processings of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>essings of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.11,198.47,294.05,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,278.20,198.47,52.63,8.10">Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,337.04,198.47,23.21,8.10">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.11,216.47,324.27,8.10;9,160.70,232.07,309.93,8.10;9,160.70,247.55,68.36,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,342.55,216.47,127.83,8.10;9,160.70,232.07,219.65,8.10">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,388.75,232.07,81.88,8.10">Processings of CVPR</title>
		<meeting>essings of CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.11,265.58,324.25,8.10;9,160.70,281.18,309.92,8.10;9,160.70,296.66,70.01,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,278.21,265.58,192.14,8.10;9,160.70,281.18,39.71,8.10">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName coords=""><forename type="first">Ji</forename><forename type="middle">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,207.89,281.18,246.61,8.10">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.11,314.66,324.58,8.10;9,160.70,330.26,309.60,8.10;9,160.70,345.74,309.47,8.10;9,160.70,361.22,310.03,8.10;9,160.70,376.82,309.50,8.10;9,160.70,392.30,310.00,8.10;9,160.70,407.78,20.35,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,407.09,345.74,63.07,8.10;9,160.70,361.22,188.07,8.10">Overview of Im-ageCLEF 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><forename type="middle">I</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,365.35,361.22,105.38,8.10;9,160.70,376.82,309.50,8.10;9,160.70,392.30,91.46,8.10">Experimental IR Meets Multilinguality, Multimodality, and Interaction 8th International Conference of the CLEF Association, CLEF 2017</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-11">2017. September 11-14, 2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.28,423.38,319.97,8.10;9,160.70,438.88,309.77,8.10;9,160.70,454.48,309.65,8.10;9,160.70,469.96,245.06,8.10" xml:id="b9">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="9,238.59,438.88,231.88,8.10;9,160.70,454.48,309.65,8.10;9,160.70,469.96,40.50,8.10">Proceedings of Experimental IR Meets Multilinguality, Multimodality, and Interaction 8th International Conference of the CLEF Association, CLEF 2017</title>
		<title level="s" coord="9,207.65,469.96,22.91,8.10">LNCS</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lawless</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Kelly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>Experimental IR Meets Multilinguality, Multimodality, and Interaction 8th International Conference of the CLEF Association, CLEF 2017<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.28,485.44,320.38,8.10;9,160.70,501.23,146.72,8.96" xml:id="b10">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="9,373.48,485.44,97.18,8.10;9,160.70,501.88,19.37,8.10">CLEF 2017 Labs Working Notes</title>
		<title level="s" coord="9,186.26,501.23,94.37,8.96">CEUR-WS Proceedings</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.28,518.20,319.99,8.10;9,160.70,533.80,309.97,8.10;9,160.70,549.28,20.35,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,242.81,518.20,221.99,8.10">MatConvNet Convolutional Neural Networks for MATLAB</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,160.70,533.80,258.70,8.10">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
