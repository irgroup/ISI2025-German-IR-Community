<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.75,115.96,311.86,12.62;1,215.33,133.89,184.69,12.62">Neural Captioning for the ImageCLEF 2017 Medical Image Challenges</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,202.68,171.79,59.91,8.74"><forename type="first">David</forename><surname>Lyndon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technologies</orgName>
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.04,171.79,59.02,8.74"><forename type="first">Ashnil</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technologies</orgName>
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.29,171.79,54.39,8.74"><forename type="first">Jinman</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technologies</orgName>
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.75,115.96,311.86,12.62;1,215.33,133.89,184.69,12.62">Neural Captioning for the ImageCLEF 2017 Medical Image Challenges</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7B938B1AF11566E1F7844FA915ADFF4E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Image Captioning</term>
					<term>LSTM</term>
					<term>CNN</term>
					<term>RNN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Manual image annotation is a major bottleneck in the processing of medical images and the accuracy of these reports varies depending on the clinician's expertise. Automating some or all of the processes would have enormous impact in terms of efficiency, cost and accuracy. Previous approaches to automatically generating captions from images have relied on hand-crafted pipelines of feature extraction and techniques such as templating and nearest neighbour sentence retrieval to assemble likely sentences. Recent deep learning-based approaches to general image captioning use fully differentiable models to learn how to generate captions directly from images. In this paper, we address the challenge of end-to-end medical image captioning by pairing an imageencoding convolutional neural network (CNN) with a language-generating recurrent neural network (RNN). Our method is an adaptation of the NICv2 model that has shown state-of-the-art results in general image captioning. Using only data provided in the training dataset, we were able to attain a BLEU score of 0.0982 on the ImageCLEF 2017 Caption Prediction Challenge and an average F1 score of 0.0958 on the Concept Detection Challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generating a textual summary of the insights gleaned from a medical image is a routine, yet nonetheless time-consuming task requiring much human effort on the part of highly trained clinicians. Prior efforts to automate this task relied on hand-crafted pipelines, employing manually designed feature extraction and techniques such as templating and sentence retrieval to assemble likely sentences <ref type="bibr" coords="1,134.77,572.20,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="1,146.85,572.20,12.73,8.74" target="#b12">13,</ref><ref type="bibr" coords="1,161.15,572.20,11.62,8.74" target="#b13">14]</ref>. Recent deep learning-based approaches to general image captioning, however, use fully differentiable models to learn how to generate captions directly from images. In general the advantages of such fully learnable models is that any part of the model can adapt in a manner most useful for the problem at hand, whereas a hand-designed system is constrained by the assumptions made during feature extraction, concept detection, and sentence generation.</p><p>In this paper we describe the submission of the University of Sydney's Biomedical Engineering &amp; Technology (BMET) group to the caption prediction and concept detection task of the ImageCLEF 2017 caption challenge <ref type="bibr" coords="2,433.51,118.99,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="2,446.07,118.99,7.01,8.74" target="#b6">7]</ref>. This submission employs a fully differentiable model, pairing an image-encoding CNN with a language-generating RNN to generate captions for images from a range of modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Image captioning, whereby the contents of an image are automatically described in natural language, is challenging task in machine learning, requiring methods from both image and natural language processing. Many early approaches to this problem involved complex systems comprising of visual feature extractors and rule based methods for sentence generation. Li et al. <ref type="bibr" coords="2,387.44,261.08,15.50,8.74" target="#b10">[11]</ref> utilise image feature similarity measures to locate likely n-grams from a large corpus of image and text, then use a simple sentence template and local search to generate a caption. Yao et al. <ref type="bibr" coords="2,217.56,296.94,15.50,8.74" target="#b21">[22]</ref> extract image features such as SIFT and edges to match images to a concept database, then apply a graph-based ontology to these concepts to produce readable sentences. Ordonez et al. <ref type="bibr" coords="2,357.04,320.85,15.50,8.74" target="#b11">[12]</ref> use image features and a ranking-based approach to locate likely sentences in an extremely large database of images and text. Such methods require a great deal of hand-crafted optimisation and produce systems which are brittle and limited to specialised domains.</p><p>Recently, deep learning-based encoder-decoder frameworks for machine translation <ref type="bibr" coords="2,163.41,380.89,15.50,8.74" target="#b15">[16]</ref> have been adapted and applied to problem of image captioning. By replacing the language-encoding Long Short-Term Memory (LSTM) <ref type="bibr" coords="2,443.27,392.84,10.52,8.74" target="#b5">[6]</ref> RNN with an image-encoding CNN, the model is able to learn to generate captions directly from images. The entire model is completely differentiable so errors are propagated to the different components proportional to their contribution to the error, allowing them to adapt appropriately. While there were several precursors that replaced various components of existing image to caption frameworks with trainable RNNs or CNNs, Vinyals et al. <ref type="bibr" coords="2,334.89,464.57,15.50,8.74" target="#b18">[19]</ref> proposed the first end-to-end neural network based approach to captioning with their "Show and Tell" (also called Neural Image Captioning (NIC)) model. An updated method, NICv2 <ref type="bibr" coords="2,462.32,488.49,14.61,8.74" target="#b19">[20]</ref>, won the Microsoft Common Objects in Contex (MSCOCO) challenge in 2015. Qualitative analysis has shown that neural captioning methods are preferred in comparison with conventional nearest-neighbour sentence lookup approaches <ref type="bibr" coords="2,467.30,524.35,9.96,8.74" target="#b2">[3]</ref>.</p><p>There has been limited work in adapting such methods to the medical domain, despite the large volume of image and text data found in PACS. Schlegl et al. <ref type="bibr" coords="2,159.15,560.48,15.50,8.74" target="#b12">[13]</ref> present the first such work that leveraged text reports to improve classification accuracy of CNN applied to Optical Coherence Tomography (OCT) images. Mahmood et al. <ref type="bibr" coords="2,244.85,584.39,15.50,8.74" target="#b16">[17]</ref> present a method that uses hand-coded topic extraction, hand-coded image features and a SVM-based correlation system. Shin et al. <ref type="bibr" coords="2,160.69,608.30,15.50,8.74" target="#b13">[14]</ref> document efforts to mine an extremely large database of images and text extracted from the PACS of the National Institutes of Health Clinical Center (approximately 216 thousand images) using latent Dirichlet allocation (LDA) to extract topics from the raw text and then correlate these topics to image features. Kisilev et al. <ref type="bibr" coords="2,220.70,656.12,10.52,8.74" target="#b8">[9]</ref> proposes an SVM-based approach to highlight regions of interest (ROIs) and generate template-based captions for the Digital Database for Screening Mammography (DDSM). This is extended using a multi-task loss CNN in a later work <ref type="bibr" coords="3,227.81,142.90,9.96,8.74" target="#b7">[8]</ref>.</p><p>To the best of our knowledge, only one published work exists for applying neural image captioning to a medical dataset <ref type="bibr" coords="3,345.27,168.94,14.61,8.74" target="#b14">[15]</ref>. In this work the authors employ an architecture similar to Vinyals et al. <ref type="bibr" coords="3,354.90,180.90,15.50,8.74" target="#b18">[19]</ref> to generate an array of keywords for a radiological dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Unless otherwise specified, the same method was applied for both the caption prediction and concept detection tasks. The set of concepts assigned to an image in the concept detection task is considered to be a caption where each concept label is a word in the sentence. In both cases only the supplied training dataset was used to train the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>In order to simplify the task, each image in the training set was preprocessed in accordance with the task's evaluation preprocessing specifications. This involved converting the caption to lower case, removing all punctuation (some captions contained multiple sentences, however, after this step each caption became a single sentence), removing stopwords using the NLTK <ref type="bibr" coords="3,357.55,436.67,10.52,8.74" target="#b0">[1]</ref> English stopword list and finally applying stemming using NLTK's Snowball stemmer. No preprocessing was applied to the 'sentences' for the concept detection task. After this preprocessing the count of each unqiue word in the training corpus was taken. Words that appeared less than 4 times were discarded and this resulted in a dictionary of 25237 distinct words. For the RNN framework described below, two reserved words indicating the start and end of sentences are added to the dictionary and used to prepend and append each sentence.</p><p>The images are first resized to 324x324px, and a 299x299px crop is then selected. During training this is a random crop, but during evaluation a central crop is used. We apply image augmentation during training to regularise the model <ref type="bibr" coords="3,163.58,570.30,14.61,8.74" target="#b9">[10]</ref>. This augmentation consists of distorting the image, first by randomly flipping it horizontally then randomly adjusting the brightness, saturation, hue and contrast. The random cropping and distortion are performed each time an image is passed into the model and means that it is extremely rare that exactly the same image is seen twice.</p><p>A validation set was provided by the organisers of the task and it entirely reserved for validation. No part of it was used for training, and specifically we did use it to build the dictionary of unique words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>Our method extends Vinyals et. al's NICv2 model <ref type="bibr" coords="4,353.29,140.56,15.50,8.74" target="#b19">[20]</ref> 1 . The NICv2 model consists of two different types of neural networks paired together to form an imageto-language, encoder-decoder pair. A CNN, specifically the InceptionV3 <ref type="bibr" coords="4,449.75,164.48,15.50,8.74" target="#b17">[18]</ref> architecture, is used as an image encoder. InceptionV3 is one of the most accurate architecture for general image classification according to the ImageNet <ref type="bibr" coords="4,438.78,188.39,10.52,8.74" target="#b1">[2]</ref> benchmark, but is significantly more computationally efficient than alternatives such as Residual Networks <ref type="bibr" coords="4,231.98,212.30,9.96,8.74" target="#b4">[5]</ref>. We utilised a RNN based on LSTM units as the language decoder as per the original paper, however, we doubled the number of units from 512 to 1024 as this showed improved results in our experiments.</p><p>An image is first preprocessed as described above and then fed to the input of the CNN. The logits of the CNN are passed into a single layer fully-connected neural network which functions as an image embedding layer. This image embedding then the becomes to initial state of the LSTM network. As per <ref type="bibr" coords="4,448.11,284.36,15.50,8.74" target="#b19">[20]</ref> the embedding was passed only at the initial state and is not used subsequently. At each state subsequent to the initial state, then LSTM's output is passed to a word embedding layer and then to a softmax layer. At each time step the output of the softmax is the probability of each word in the dictionary. For two of the caption prediction experiments (PRED2 &amp; PRED4) we modified the baseline language model to use a 3-layer LSTM model with a single dropout layer on the output. Increasing the number of LSTM layers improves the ability of the language model to represent complex sentences and long term dependencies. Industrial neural machine translation models have been demonstrated to use decoder layers with up to 8 layers <ref type="bibr" coords="4,285.23,403.91,14.61,8.74" target="#b20">[21]</ref>.</p><p>In all our models we used 1024 units for both the image and word embedding layers. The CNN is initialised using weights from a model trained on the ImageNet dataset, while the weights for the LSTM are initialised from a random normal distribution with values between -0.08 and 0.08 as per <ref type="bibr" coords="4,427.76,452.06,14.61,8.74" target="#b15">[16]</ref>. For the final caption prediction experiment (PRED4) we attempted to domain transfer the updated CNN weights from the DET2 caption detection model. This was attempted to avoid corruption of CNNs during end-to-end training (discussed in Sect. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>The loss optimised during training is the summed cross entropy of the output of the softmax compared to the one-hot encoding of the next word in the ground truth sentence. This loss was minimised with standard Stochastic Gradient Descent (SGD) using an initial learning rate of 1.0 and a decay procedure that reduced the learning rate by half every 8 epochs (there were 164541 examples in the training set and a batch size of 16, so each epoch contains 10284 minibatches). Gradients were clipped to 5.0 for all experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference</head><p>As suggested by Vinyals et al. <ref type="bibr" coords="5,273.10,403.93,15.50,8.74" target="#b18">[19]</ref> we use Beam Search to generate sentences at inference time. This avoids the non-trivial issue that greedily selecting the most probable word at each time-step may result in a sentence which is itself of low probability. Ideally we would search the entire space for the most probable sentence, however, this would have an exponential computational cost associated with it as a forward pass through the entire model must be made for each node of the search tree. Therefore some search procedure is required in order to find the most probable sentence given limited computational resources. Our best results were achieved with a beam size of 3 and maximum caption length of 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Post Processing</head><p>The sentence output of the concept detection task was converted to an ordered set of concept labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Details of Submitted Runs</head><p>Tables 1 &amp; 2 detail the specifics of the 3 runs submitted to the concept detection challenge and the 4 runs submitted the caption prediction challenge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Tables <ref type="table" coords="6,166.13,452.66,4.98,8.74" target="#tab_3">3</ref> &amp;<ref type="table" coords="6,185.59,452.66,4.98,8.74" target="#tab_4">4</ref> detail the training, validation and test results of the various runs. Please note that due to the high cost of inference, the training scores are estimated based on a random sample of 10000 images from the training set. We attempted a two-phase training procedure as suggested by Vinyals et al. <ref type="bibr" coords="6,148.12,500.70,15.50,8.74" target="#b19">[20]</ref> for DET2 and some unsubmitted experiments. In the first phase we froze the CNN weights and trained only the LSTM and embedding layers. Then, once the language model had begun to converge we trained the entire model end-toend with a very small learning rate (1e -5). This is suggested by the Vinyals et al. as necessary as otherwise the CNN model will become corrupted and never recover. However, we found that despite training the LSTM for a very long time in the first phase and using a very small learning rate in the second phase we would very quickly corrupt the CNN as evidenced by a sharp increase in dead ReLUs and a large decrease in BLEU score. We found that BLEU scores would eventually return to those achieved in the first phase of training, however, the dead ReLUs did not revive. We believe that the underlying issue is that the degree of domain transfer required to go from general images to medical images is vastly greater than that required to go from one collection of general images to another (i.e. ImageNet to MSCOCO). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Qualitative Analysis</head><p>Table <ref type="table" coords="7,162.77,383.83,4.98,8.74">5</ref> shows some generated and predicted captions from the validation set. For the first image, the model has learned to generate the exact same caption as the ground truth, with the exception of the second word which is unknown. This exact caption, with only the second word different, appears alongside 21 images in the training set. The unknown word, 'latiterga', does not appear in the training set. In the second example, the model has correctly identified the modality, orientation and anatomy from the image and has generated a very similar sentence even though no such sentence exists in the training set. It has not, however, determined that this is a preoperative image. The third example demonstrates a very poor result. The model has not correctly determined that there are two subfigures, although it has correctly estimated the magnification of the left subfigure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Based on the small variance between our training and validation scores we do not believe that the models were overfitting, however, the large variance between validation and test scores indicates that there was a large disparity between the training and validation data and the data in test set. Based on the non-overfitting analysis we could potentially train a much larger vision model for a longer time and improve the overall performance.</p><p>Additionally the fact that we could not successfully train the vision model without corrupting the network was a major limiting factor in our experiments.</p><p>Future work will investigate the potential of larger language models and devising a training regime that allows true end-to-end training for medical images.  <ref type="table" coords="8,171.26,504.76,4.13,7.89">5</ref>. Sample of predicted and actual captions with associated BLEU metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,327.77,345.82,7.89;5,134.77,338.75,135.71,7.86;5,161.34,234.53,141.72,74.40"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic of the Neural Image Captioning architecture with a validation image and the actual generated caption.</figDesc><graphic coords="5,161.34,234.53,141.72,74.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,236.22,180.80,26.88,7.86;8,286.20,164.31,172.93,7.86;8,277.96,175.27,189.40,7.86;8,290.00,186.23,165.33,7.86;8,313.20,197.19,118.93,7.86;8,236.22,224.64,38.55,7.86;8,310.80,208.15,123.74,7.86;8,285.28,219.11,174.76,7.86;8,285.51,230.07,174.31,7.86;8,295.12,241.03,155.08,7.86;8,236.22,262.99,25.16,7.86;8,308.22,251.96,128.89,7.89;8,310.18,262.92,124.97,7.89;8,339.88,273.88,65.55,7.89;8,236.22,296.27,26.88,7.86;8,301.56,296.22,142.21,7.86;8,236.22,329.14,38.55,7.86;8,318.35,329.10,108.63,7.86;8,236.22,362.02,25.16,7.86;8,315.13,350.99,115.06,7.89;8,319.39,361.95,106.55,7.89;8,346.80,372.90,51.74,7.89;8,236.22,411.74,26.88,7.86;8,301.31,384.29,142.71,7.86;8,287.52,395.25,170.29,7.86;8,304.24,406.21,136.85,7.86;8,291.82,417.16,161.68,7.86;8,294.28,428.12,156.77,7.86;8,316.68,439.08,111.97,7.86;8,236.22,455.57,38.55,7.86;8,308.06,450.04,129.21,7.86;8,301.44,461.00,142.44,7.86;8,236.22,482.97,25.16,7.86;8,277.96,471.93,128.89,7.89;8,277.96,482.89,106.54,7.89;8,277.96,493.85,51.74,7.89;8,142.36,504.76,25.36,7.89"><head></head><label></label><figDesc>Actual orthomorpha latiterga sp n holotyp b right gonopod mesal later view respect cf distal part right gonopod mesal later subor subcaud view respect scale bar 02 mm Predicted orthomorpha unk sp n holotyp b right gonopod mesal later view respect cf distal part right gonopod mesal later subor subcaud view respect scale bar 02 mm Scores BLEU: 0.9304 BLEU1: 0.9629 BLEU2: 0.92 BLEU3: 0.9231 BLEU4: 0.9167 Actual preoper later radiograph right knee Predicted later radiograph right knee Scores BLEU: 0.7788 BLEU1: 1.0 BLEU2: 1.0 BLEU3: 1.0 BLEU4: 1.0 Actual discret epithelioid granuloma crohn diseas discret epithelioid granuloma associ epitheli injuri stain 100x b discret epithelioid granuloma musculari mucosa stain 200x histiocyt epithelioid contain abund eosinophil cytoplasm Predicted histolog examin resect specimen hematoxylin eosin stain magnif 100 Scores BLEU: 0.0781 BLEU1: 0.1111 BLEU2: 0.0 BLEU3: 0.0 BLEU4: 0.0 Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,136.56,117.78,343.35,262.74"><head>Table 1 .</head><label>1</label><figDesc>Details of Concept Detection submitted runs.</figDesc><table coords="6,136.56,117.78,343.35,262.74"><row><cell></cell><cell>Concept Detection</cell></row><row><cell>Run</cell><cell>Description</cell></row><row><cell>DET1</cell><cell>CNN weights frozen. LSTM and embedding layers trained for 958069</cell></row><row><cell></cell><cell>minibatches (approx. 90 epochs)</cell></row><row><cell>DET2</cell><cell>DET1 trained end-to-end for an additional 2658763 minibatches (ap-</cell></row><row><cell></cell><cell>prox. 350 epochs total). Model suffered from destructive gradient issue</cell></row><row><cell></cell><cell>discussed below.</cell></row><row><cell>DET3</cell><cell>Naive merge of DET1 &amp; DET2, using a set union of each model's</cell></row><row><cell></cell><cell>predicted labels</cell></row><row><cell></cell><cell>Caption Prediction</cell></row><row><cell>Run</cell><cell>Description</cell></row><row><cell>PRED1</cell><cell>CNN weights frozen. LSTM and embedding layers trained for 1499176</cell></row><row><cell></cell><cell>minibatches (approx. 145 epochs)</cell></row><row><cell>PRED2</cell><cell>CNN weights frozen. 3-layer LSTM with a single dropout layer. Trained</cell></row><row><cell></cell><cell>for 998981 minibatches approx. 97 epochs)</cell></row><row><cell>PRED3</cell><cell>Naive Merge of PRED1 and PRED2 based on most non-known words</cell></row><row><cell></cell><cell>in sentence</cell></row><row><cell>PRED4</cell><cell>CNN Domain transfer from fine tuned detection task model DET3. 3-</cell></row><row><cell></cell><cell>layer LSTM with a single dropout layer. CNN weights then frozen and</cell></row><row><cell></cell><cell>LSTM and embedding layers trained for 437805 minibatches (approx.</cell></row><row><cell></cell><cell>42 epochs)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,194.93,383.54,225.50,7.89"><head>Table 2 .</head><label>2</label><figDesc>Details of Caption Prediction submitted runs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,134.77,117.78,345.83,187.13"><head>Table 3 .</head><label>3</label><figDesc>Details of Concept Detection results. *Training score estimated on a random sample of 10k training images.</figDesc><table coords="7,146.99,117.78,261.94,187.13"><row><cell></cell><cell></cell><cell>Concept Detection</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Average F1 Score</cell><cell></cell></row><row><cell>Run</cell><cell cols="2">Train (estimated*) Validation</cell><cell>Test</cell></row><row><cell>DET1</cell><cell>0.1055</cell><cell>0.1088</cell><cell>0.0838</cell></row><row><cell>DET2</cell><cell>0.1119</cell><cell>0.1117</cell><cell>0.0880</cell></row><row><cell>DET3</cell><cell>0.0860</cell><cell>0.0952</cell><cell>0.0958</cell></row><row><cell></cell><cell></cell><cell>Caption Prediction</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Average BLEU Score</cell><cell></cell></row><row><cell>Run</cell><cell cols="2">Train (estimated*) Validation</cell><cell>Test</cell></row><row><cell>PRED1</cell><cell>0.1367</cell><cell>0.1315</cell><cell>0.0656</cell></row><row><cell>PRED2</cell><cell>0.1590</cell><cell>0.1533</cell><cell>0.0851</cell></row><row><cell>PRED3</cell><cell>0.1383</cell><cell>0.1734</cell><cell>0.0982</cell></row><row><cell>PRED4</cell><cell>0.1556</cell><cell>0.1489</cell><cell>0.0826</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,134.77,308.43,345.83,18.85"><head>Table 4 .</head><label>4</label><figDesc>Details of Caption Prediction results. *Training score estimated on a random sample of 10k training images.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>The authors are grateful to the <rs type="institution">NVIDIA Corporation</rs> for their donation of the Titan X GPU used in this research.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,139.31,337.64,7.86;9,151.52,150.26,329.07,7.86;9,151.52,161.22,296.23,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,192.79,139.31,155.55,7.86">NLTK: The natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,373.64,139.31,106.95,7.86;9,151.52,150.26,194.44,7.86;9,402.36,150.26,74.65,7.86">Proceedings of the COL-ING/ACL on Interactive Presentation Sessions</title>
		<meeting>the COL-ING/ACL on Interactive Presentation Sessions<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="69" to="72" />
		</imprint>
	</monogr>
	<note>COLING-ACL &apos;06</note>
</biblStruct>

<biblStruct coords="9,142.96,171.53,337.64,7.86;9,151.52,182.48,329.07,7.86;9,151.52,193.44,181.90,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,385.85,171.53,94.75,7.86;9,151.52,182.48,110.63,7.86">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,306.27,182.48,174.33,7.86;9,151.52,193.44,79.80,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06">2009. Jun 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,203.75,337.63,7.86;9,151.52,214.70,249.27,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,441.28,203.75,39.31,7.86;9,151.52,214.70,199.87,7.86">Exploring nearest neighbor approaches for image captioning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,225.01,337.64,7.86;9,151.52,235.97,329.07,7.86;9,151.52,246.92,329.07,7.86;9,151.52,257.88,317.71,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,403.70,225.01,76.89,7.86;9,151.52,235.97,329.07,7.86;9,151.52,246.92,42.85,7.86">Overview of Image-CLEFcaption 2017 -image caption prediction and concept detection for biomedical images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="9,216.68,246.92,131.58,7.86">CLEF 2017 Labs Working Notes</title>
		<title level="s" coord="9,356.02,246.92,124.57,7.86;9,151.52,257.88,43.39,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,268.19,337.63,7.86;9,151.52,279.14,43.91,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,300.63,268.19,179.96,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,289.45,337.64,7.86;9,151.52,300.41,89.33,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,285.25,289.45,98.97,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,392.55,289.45,65.66,7.86">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">Nov 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,310.71,337.63,7.86;9,151.52,321.67,329.07,7.86;9,151.52,332.63,329.07,7.86;9,151.52,343.59,329.07,7.86;9,151.52,354.54,329.07,7.86;9,151.52,365.50,329.07,7.86;9,151.52,376.46,129.72,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,433.34,332.63,47.26,7.86;9,151.52,343.59,215.41,7.86">Overview of ImageCLEF 2017: Information extraction from images</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,387.32,343.59,93.27,7.86;9,151.52,354.54,329.07,7.86;9,151.52,365.50,105.52,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction 8th International Conference of the CLEF Association, CLEF</title>
		<title level="s" coord="9,285.42,365.50,143.89,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,386.76,337.64,7.86;9,151.52,397.72,329.07,7.86;9,151.52,408.68,189.01,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,348.65,386.76,131.95,7.86;9,151.52,397.72,79.27,7.86">Medical image description using multi-task-loss CNN</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kisilev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sason</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Barkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hashoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,254.51,397.72,226.09,7.86;9,151.52,408.68,18.39,7.86">Deep Learning and Data Labeling for Medical Applications</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-10">Oct 2016</date>
			<biblScope unit="page" from="121" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,418.98,337.64,7.86;9,151.52,429.94,329.07,7.86;9,151.52,440.90,257.79,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,443.98,418.98,36.62,7.86;9,151.52,429.94,264.97,7.86">Semantic description of medical image findings: structured learning approach</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kisilev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Walach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hashoul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Barkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ophir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,436.62,429.94,43.97,7.86;9,151.52,440.90,165.73,7.86">Procedings of the British Machine Vision Conference</title>
		<meeting>edings of the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,462.16,337.97,7.86;9,151.52,473.12,329.07,7.86;9,151.52,484.08,235.96,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,372.06,462.16,108.53,7.86;9,151.52,473.12,254.80,7.86">An ensemble of Fine-Tuned convolutional neural networks for medical image classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lyndon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fulham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,414.12,473.12,66.47,7.86;9,151.52,484.08,137.03,7.86">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="40" />
			<date type="published" when="2017-01">Jan 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,494.38,337.97,7.86;9,151.52,505.34,329.07,7.86;9,151.52,516.30,329.07,7.86;9,151.52,527.26,246.98,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,366.42,494.38,114.17,7.86;9,151.52,505.34,135.83,7.86">Composing simple image descriptions using web-scale n-grams</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,308.09,505.34,172.51,7.86;9,151.52,516.30,172.47,7.86;9,384.57,516.30,43.35,7.86">Proceedings of the Fifteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="220" to="228" />
		</imprint>
	</monogr>
	<note>CoNLL &apos;11</note>
</biblStruct>

<biblStruct coords="9,142.62,537.56,337.98,7.86;9,151.52,548.52,329.07,7.86;9,151.52,559.48,329.07,7.86;9,151.52,570.44,74.75,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,307.58,537.56,173.01,7.86;9,151.52,548.52,90.11,7.86">Im2Text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,267.29,559.48,213.30,7.86">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,580.74,337.97,7.86;9,151.52,591.70,329.07,7.86;9,151.52,602.66,329.07,7.86;9,151.52,613.62,22.02,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,448.19,580.74,32.40,7.86;9,151.52,591.70,324.82,7.86">Predicting semantic descriptions from medical images with convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">D</forename><surname>Vogl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,165.24,602.66,170.81,7.86">Information Processing in Medical Imaging</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-06">Jun 2015</date>
			<biblScope unit="page" from="437" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,623.92,337.98,7.86;9,151.52,634.88,329.07,7.86;9,151.52,645.84,329.07,7.86;9,151.52,656.80,70.14,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,436.28,623.92,44.31,7.86;9,151.52,634.88,269.76,7.86">Interleaved text/image deep mining on a very large-scale radiology database</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,445.37,634.88,35.22,7.86;9,151.52,645.84,308.23,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1090" to="1099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,119.67,337.97,7.86;10,151.52,130.63,329.07,7.86;10,151.52,141.59,329.07,7.86;10,151.52,152.55,173.07,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,151.52,130.63,329.07,7.86;10,151.52,141.59,58.24,7.86">Learning to read chest x-rays: recurrent neural cascade model for automated image annotation</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,231.05,141.59,249.55,7.86;10,151.52,152.55,79.80,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2497" to="2506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,163.51,337.98,7.86;10,151.52,174.47,329.07,7.86;10,151.52,185.43,329.07,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,294.62,163.51,185.97,7.86;10,151.52,174.47,21.39,7.86">Sequence to sequence learning with neural networks</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="10,198.19,185.43,206.84,7.86">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,207.34,337.98,7.86;10,151.52,218.30,329.07,7.86;10,151.52,229.26,329.07,7.86;10,151.52,240.22,43.51,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,345.31,207.34,135.28,7.86;10,151.52,218.30,202.97,7.86">Learning the correlation between images and disease labels using ambiguous learning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Syeda-Mahmood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Compas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,374.55,218.30,106.04,7.86;10,151.52,229.26,208.09,7.86">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-10">Oct 2015</date>
			<biblScope unit="page" from="185" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,251.18,337.97,7.86;10,151.52,262.14,195.34,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="10,393.24,251.18,87.35,7.86;10,151.52,262.14,148.36,7.86">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,273.10,337.98,7.86;10,151.52,284.06,118.57,7.86" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="10,352.45,273.10,128.15,7.86;10,151.52,284.06,70.70,7.86">Show and tell: A neural image caption generator</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-11">Nov 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,295.02,337.97,7.86;10,151.52,305.98,329.07,7.86;10,151.52,316.93,67.71,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,338.40,295.02,142.19,7.86;10,151.52,305.98,186.95,7.86">Show and tell: Lessons learned from the 2015 MSCOCO image captioning challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,345.13,305.98,135.46,7.86;10,151.52,316.93,23.55,7.86">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2016-07">Jul 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,327.89,337.97,7.86;10,151.52,338.85,329.07,7.86;10,151.52,349.81,329.07,7.86;10,151.52,360.77,329.07,7.86;10,151.52,371.73,329.07,7.86;10,151.52,382.69,280.80,7.86" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="10,300.04,371.73,180.55,7.86;10,151.52,382.69,234.72,7.86">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">,</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rudnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-09">Sep 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,393.65,337.98,7.86;10,151.52,404.61,215.88,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,372.15,393.65,108.45,7.86;10,151.52,404.61,42.78,7.86">I2t: Image parsing to text description</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,201.25,404.61,46.60,7.86">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1485" to="1508" />
			<date type="published" when="2010-08">Aug 2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
