<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.55,115.90,308.26,12.90;1,223.43,135.75,168.50,10.75">N-GrAM: New Groningen Author-profiling Model Notebook for PAN at CLEF 2017</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,208.68,172.15,55.30,8.64"><forename type="first">Angelo</forename><surname>Basile</surname></persName>
							<email>a.basile@student.rug.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<settlement>Groningen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.39,172.15,54.26,8.64"><forename type="first">Gareth</forename><surname>Dwyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<settlement>Groningen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,332.08,172.15,69.77,8.64"><forename type="first">Maria</forename><surname>Medvedeva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<settlement>Groningen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,202.06,184.10,51.94,8.64"><forename type="first">Josine</forename><surname>Rawee</surname></persName>
							<email>j.n.rawee@student.rug.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<settlement>Groningen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,261.39,184.10,63.67,8.64"><forename type="first">Hessel</forename><surname>Haagsma</surname></persName>
							<email>hessel.haagsma@rug.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<settlement>Groningen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.37,184.10,63.93,8.64"><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
							<email>m.nissim@rug.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<settlement>Groningen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.55,115.90,308.26,12.90;1,223.43,135.75,168.50,10.75">N-GrAM: New Groningen Author-profiling Model Notebook for PAN at CLEF 2017</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7F7AC33D20BA8C678ECF38A04480AF86</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our participation in the PAN 2017 shared task on Author Profiling, identifying authors' gender and language variety for English, Spanish, Arabic and Portuguese. We describe both the final, submitted system, and a series of negative results. Our aim was to create a single model for both gender and language, and for all language varieties. Our best-performing system (on cross-validated results) is a linear support vector machine (SVM) with word unigrams and character 3-to 5-grams as features. A set of additional features, including POS tags, additional datasets, geographic entities, and Twitter handles, hurt, rather than improve, performance. Results from cross-validation indicated high performance overall and results on the test set confirmed them, at 0.86 averaged accuracy, with performance on sub-tasks ranging from 0.68 to 0.98.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rise of social media, more and more people acquire some kind of on-line presence or persona, mostly made up of images and text. This means that these people can be considered authors, and thus that we can profile them as such. Profiling authors, that is, inferring personal characteristics from text, can reveal many things, such as their age, gender, personality traits, location, even though writers might not consciously choose to put indicators of those characteristics in the text. The uses for this are obvious, for cases like targeted advertising and other use cases, such as security, but it is also interesting from a linguistic standpoint.</p><p>In the shared task on author profiling <ref type="bibr" coords="1,305.90,536.89,15.27,8.64" target="#b9">[10]</ref>, organised within the PAN framework <ref type="bibr" coords="1,134.77,548.84,10.58,8.64" target="#b7">[8]</ref>, the aim is to infer Twitter users' gender and language variety from their tweets in four different languages: English, Spanish, Arabic, and Portuguese. Gender consists of a binary classification (male/female), whereas language variety differs per language, from 2 varieties for Portuguese (Brazilian and Portugal) to 7 varieties for Spanish (Argentina, Chile, Colombia, Mexico, Peru, Spain, Venezuela). The challenge is thus to classify users along two very different axes, and in four highly different languages -forcing participants to either build models that can capture these traits very generally (languageindependent) or tailor-make models for each language or subtask.</p><p>Even when looking at the two tasks separately, it looks like the very same features could be reliable clues for classification. Indeed, for both profiling authors on Twitter as well as for discriminating between similar languages, word and character n-grams have proved to be the strongest predictors of gender as well as language varieties. For language varieties discrimination, the systems that performed best at the DSL shared tasks in 2016 (on test set B, i.e. social media) used word/character n-grams, independently of the algorithm <ref type="bibr" coords="2,190.39,167.13,10.58,8.64" target="#b5">[6]</ref>. The crucial contribution of these features was also observed by <ref type="bibr" coords="2,459.01,167.13,10.79,8.64" target="#b6">[7,</ref><ref type="bibr" coords="2,469.80,167.13,7.19,8.64" target="#b1">2]</ref>, who participated in the 2017 DSL shared task with the two best performing systems. For author profiling, it has been shown that tf-idf weighted n-gram features, both in terms of characters and words, are very successful in capturing especially gender distinctions <ref type="bibr" coords="2,171.84,214.95,15.27,8.64" target="#b10">[11]</ref>. If different aspects such as language variety and gender of a speaker on Twitter might be captured by the same features, can we build a single model that will characterise both aspects at once?</p><p>In the context of the PAN 2017 competition on user profiling we therefore experimented with enriching a basic character and word n-gram model by including a variety of features that we believed should work. We also tried to view the task jointly and model the two problems as one single label, but single modelling worked best.</p><p>In this paper we report how our final submitted system works, and provide some general data analysis, but we also devote substantial space to describing what we tried (under which motivations), as we believe this is very informative towards future developments of author profiling systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Final System</head><p>After an extensive grid-search we submitted as our final run, a simple SVM system (using the scikit-learn LinearSVM implementation) that uses character 3-to 5-grams and word 1-to 2-grams with tf-idf weighting with sublinear term frequency scaling, where instead of the standard term frequency the following is used:</p><formula xml:id="formula_0" coords="2,284.02,453.85,47.32,8.74">1 + log(tf )</formula><p>We ran the grid search over both tasks and all languages on a 64-core machine with 1 TB RAM (see Table <ref type="table" coords="2,202.35,489.47,4.98,8.64" target="#tab_1">2</ref> for the list of values over which the grid search was performed). The full search took about a day to complete. In particular, using min_df=2 (i.e. excluding all terms that are used by only one author) seems to have a strong positive effect and greatly reduces the feature size as there are many words that appear only once. The different optimal parameters for different languages provided only a slight performance boost for each language. We decided that this increase was too small to be significant, so we decided to use a single parameter set for all languages and both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Analysis</head><p>The training dataset provided consist of 11400 sets of tweets, each set representing a single author. The target labels are evenly distributed across variety and gender. The labels for the gender classification task are 'male' and 'female'. Table <ref type="table" coords="2,409.56,644.48,4.98,8.64" target="#tab_2">3</ref> shows the labels for the language variation task and also shows the data distribution across languages. We produced two visualisations, one per label (i.e. variety and gender), in order to gain some insights that could help the feature engineering process. For the variety label we trained a decision tree classifier using word unigrams: although the performance is poor (accuracy score of 0.63) this setup has the benefit of being easy to interpret: Figure <ref type="figure" coords="3,134.77,403.92,4.98,8.64" target="#fig_0">1</ref> shows which features are used for the first splits of the tree.</p><p>We also created a visualisation of the English dataset using the tool described in <ref type="bibr" coords="3,466.48,415.96,10.58,8.64" target="#b4">[5]</ref>, and comparing the most frequent words used by males to those used by females. The visualisation shown in Figure <ref type="figure" coords="3,255.74,439.87,4.98,8.64" target="#fig_1">2</ref> indicates several interesting things about the gendered use of language. The words used often by males and very seldom by females are often sport-related, and include words such as "league", and "chelsea". There are several emojis that are used frequently by females and infrequently by males, e.g. " ", " ", as well as words like "kitten", "mom", "sister" and "chocolate". In the top right of the visualisation we see words like "trump" and "sleep", which indicates that these words are used very frequently, but equally so by both genders. This also shows that distinguishing words include both time-specific ones, like "gilmore" and "imacelebrityau", and general words from everyday life, which are less likely to be subject to time-specific trends, like "player", and "chocolate".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Alternative Features and Methods: An Analysis of Negative Results</head><p>This section is meant to highlight all of the potential contributions to the systems which turned out to be detrimental to performance, when compared to the simpler system that we have described in Section 2. We divide our attempts according to the different ways we attempted to enhance performance: manipulating the data itself (adding more, and  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Supplementary Data and Features</head><p>Adding Previous PAN Data We extended the training dataset by adding data and gender labels from the PAN 16 Author Profiling shared task <ref type="bibr" coords="4,363.42,506.86,15.27,8.64" target="#b10">[11]</ref>. However, the additional data consistently resulted in lower cross-validation scores than when using only the training data provided with the PAN 17 task. One possible explanation for this is that our unigram model captures aspects that are tied specifically to the PAN 17 dataset, because it contains topics that may not be present in datasets that were collected in a different time period. To confirm this, we attempted to train on English data from PAN 17 and predict gender labels for the English data from PAN 16, as well as vice versa.</p><p>Training on the PAN 16 data resulted in an accuracy score of 0.754 for the PAN 17 task, and training on PAN 17 gave an accuracy score of 0.70 for PAN 16, both scores significantly lower than cross-validated results on data from a single year.</p><p>Using the Twitter 14k dataset We attempted to classify the English tweets by Gender using only the data collected by <ref type="bibr" coords="4,268.42,656.44,10.58,8.64" target="#b0">[1]</ref>. This dataset consists of aggregated word counts by gender for about 14,000 Twitter users and 9 million Tweets. We used this data to calculate whether each word in our dataset was a 'male' word (used more by males), or a 'female' word, and classified users as male or female based on a majority count of the words they used. Using this method we achieved 71.2 percent accuracy for the English gender data, showing that this simple method can provide a baseline to the gender task.</p><p>Tokenization We experimented with different tokenization techniques for different languages, but our average results did not improve, so we decided to use the default scikit-learn tokenizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS tags</head><p>We tried adding POS-tags to the English tweets using the spaCy<ref type="foot" coords="5,445.54,257.99,3.49,6.05" target="#foot_0">1</ref> tagger: compared to the model using unigrams only the performances dropped slightly for gender and a bit more for variety: It is not clear whether the missed increase in performance is due to the fact that the data are not normal (i.e. the tokenizer is not Twitter specific) or to the fact that POS tags confuse the classifier. Considering the results we decided not to include a POS-tagger in the final system.</p><p>Emojis ( ) In April 2015, SwiftKey did an extensive report<ref type="foot" coords="5,346.94,495.71,3.49,6.05" target="#foot_1">2</ref> on emoji use by country. They discovered that emoji use varies across languages and across language varieties. For example, they found that Australians use double the average amount of alcohol-themed emoji and use more junk food and holiday emoji than anywhere else in the world.</p><p>We tried to leverage these findings but the results were disappointing. We used a list of emojis<ref type="foot" coords="5,188.56,555.48,3.49,6.05" target="#foot_2">3</ref> as a vocabulary for the td/idf vectorizer. Encouraged by the results of the SwiftKey report, we tried first to use emojis as the only vocabulary and although the results are above the baseline and also quite high considering the type of features, they were still below the simple unigram model. Adding emojis as extra features to the unigram model also did not provide any improvement.</p><p>Since emojis are used across languages we built a single model for the four languages. We trained the model for the gender label on English, Portuguese and Arabic and tested it on Spanish: the system scored 0.67 in accuracy.</p><p>Excluding Specific Word Patterns We looked at accuracy scores for the English gender and variety data more closely. We tried different representations of the tweet texts, to see what kind of words were most predictive of variety and gender. Specifically, we look at using only words that start with an uppercase letter, only words that start with a lowercase letter, only Twitter handles (words that start with an "@") and all the text excluding the handles.</p><p>It is interesting that the accuracies are so high although we are using only a basic unigram model, without looking at the character n-grams that we include in our final model. Representing each text only by the Twitter handles used in that text results in 0.77 accuracy for variety, probably because users tend to interact with other users who are in the same geographic area. However, excluding handles from the texts barely decreases performance for the variety task, showing that while the handles can be discriminative, they are not necessary for this task. It is also interesting to note that for this dataset, looking only at words beginning with an uppercase character results in nearly the same score for the Gender task as we get when using all of the available text, while using only lowercase words decreases performance. The opposite is true for the variety task, where using lowercase-only words results in as good performance as using all the text, but using only uppercase words decreases accuracy by over 10 percent.</p><p>Table <ref type="table" coords="6,158.50,410.95,3.36,8.06">5</ref>. Results (accuracy) on the English data for Gender and Variety when excluding certain words. We preprocessed the text to exclude the specified word-patterns and then vectorized the resulting text with tf-idf. Classification was done using an SVM with a linear kernel over five-fold cross-validation. Place Names and Twitter Handles We tried using the counts of geographical names related to the language varieties were as a feature. We also treated this list of locations as vocabulary for our model. Both these approaches did not improve our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gender Variety</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All</head><p>We then tried enriching the data to improve the Unigram model. For each of the language varieties, we obtained 100 geographical location names, representing the cities with the most inhabitants. When this location was mentioned in the tweet, the language variety the location was part of was added to the tweet.</p><p>We attempted to use Twitter handles in a similar manner. The 100 most-followed Twitter users per language variety were found and the language variety was added to the text when one of its popular Twitter users was mentioned.</p><p>Unfortunately, this method did not improve our model. We suspect that the information is being captured by the n-gram model, which could explain why this did not improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GronUP Combos</head><p>We have tried the partial setup of last year's winning system, GronUP <ref type="bibr" coords="7,134.77,221.14,10.58,8.64" target="#b2">[3]</ref>, with the distinction that we had to classify language variety instead of age groups. We have excluded the features that are language-dependent (i.e. pos-tagging and misspelling/typos), and experimented with various feature combinations of the rest while keeping word and character n-grams the same. We achieved average accuracy from 0.810 to 0.830, which is clearly lower than our simple final model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Modelling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Prediction</head><p>We tried to build a single model that predicts at the same time both the language variety and the gender of each user: as expected (since the task is harder) the performance goes down when compared to a model trained independently on each label. However, as highlighted in Table 6, the results are still surprisingly high. To train the system we simply merged the two labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different Approaches</head><p>We experimented with Facebook's FastText system, which is an out-of-the-box supervised learning classifier <ref type="bibr" coords="7,209.19,444.84,10.58,8.64" target="#b3">[4]</ref>. We used only the data for the English gender task, trying both tweet-level and author-level classification. We pre-processed all text with the NLTK Tweet Tokenizer and used the classification-example script provided with the FastText code base. Training on 3,000 authors and testing on 600 authors gave an accuracy score of 0.64. Changing the FastText parameters such as number of epochs, word n-grams, and learning rate showed no improvement. We achieved an accuracy on 0.79 when we attempted to classify on a per-tweet basis (300,000 tweets for training and 85,071 for test), but this is an easier task as some authors are split over the training and test sets. There are various ways to summarise per-tweet predictions into author-predictions, but we did not experiment further as it seemed that the SVM system worked better for the amount of data we have.</p><p>In the final system we used the SVM classifier because it outperformed all the others that we tried. Table <ref type="table" coords="7,213.93,588.34,4.98,8.64" target="#tab_6">7</ref> highlights the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results on Test Data</head><p>For the final evaluation we submitted our system, N-GrAM, as described in Section 2. Overall, N-GrAM came first in the shared task, with a score of 0.8253 for gender 0.9184 for variety, a joint score of 0.8361 and an average score of 0.8599 (final rankings were taken from this average score <ref type="bibr" coords="8,254.97,264.04,14.94,8.64" target="#b9">[10]</ref>). For the global scores, all languages are combined. We present finer-grained scores showing the breakdown per language in Table <ref type="table" coords="8,456.81,276.00,3.74,8.64" target="#tab_7">8</ref>. We compare our gender and variety accuracies against the LDR-baseline <ref type="bibr" coords="8,411.61,287.95,10.58,8.64" target="#b8">[9]</ref>, a low dimensionality representation especially tailored to language variety identification, provided by the organisers. The final column, + 2nd shows the difference between N-GrAM and that achieved by the second-highest ranked system (excluding the baseline).</p><p>Results are broken down per language, and are summarised as both joint and average scores. The joint score is is the percentage of texts for which both gender and variety were predicted correctly at the same time. The average is calculated as the mean over all languages.</p><p>N-GrAM ranked first in all cases except for the language variety task. In this case, the baseline was the top-ranked system, and ours was second by a small margin. Our system significantly out-performed the baseline on the joint task, as the baseline scored significantly lower for the gender task than for the variety task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We conclude that, for the current author profiling task, a seemingly simple system using word and character n-grams and an SVM classifier proves very hard to beat. Indeed, N-GrAM turned out to be the best-performing out of the 22 systems submitted in this shared task. Using additional training data, 'smart' features, and hand-crafted resources hurts rather than helps performance. A possible lesson to take from this would be that manually crafting features serves only to hinder a machine learning algorithm's ability to find patterns in a dataset, and perhaps it is better to focus one's efforts on parameter optimisation instead of feature engineering. However, we believe that this is too strong a conclusion to draw from this limited study, since several factors specific to this setting need to be taken into account. For one, a support vector machine clearly outperforms other classifiers, but this does not mean that this is an inherently more powerful. Rather, we expect that an SVM is the best choice for the given amount of training data, but with more training data, a neural network-based approach would achieve better results.</p><p>Regarding the frustrating lack of benefit from more advanced features than n-grams, a possible explanation comes from a closer inspection of the data. Both the decision tree model (see Figure <ref type="figure" coords="9,227.12,274.73,4.15,8.64" target="#fig_0">1</ref>) and the data visualisation (see Figure <ref type="figure" coords="9,389.31,274.73,4.15,8.64" target="#fig_1">2</ref>) give us an insight in the most discriminating features in the dataset. In the case of language variety, we see that place names can be informative features, and could therefore be used as a proxy for geographical location, which in turn serves as a proxy for language variety. Adding place names explicitly to our model did not yield performance improvements, which we take to indicate that this information is already captured by n-gram features. Whether and how geographical information in the text can be useful in identifying language variety, is a matter for future research.</p><p>In the case of gender, many useful features are ones that are highly specific to the Twitter platform (#iconnecthearts), time (cruz), and topics (pbsnewshour) in this dataset, which we suspect would not carry over well to other datasets, but provide high accuracy in this case. Conversely, features designed to capture gender in a more general sense do not yield any benefit over the more specific features, although they would likely be useful for a robust, cross-dataset system. These hypotheses could be assessed in the future by testing author profiling systems in a cross-platform, cross-time setting. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,252.13,245.77,111.09,8.12;4,134.77,115.83,358.20,115.20"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Decision Tree output</figDesc><graphic coords="4,134.77,115.83,358.20,115.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="11,490.99,516.92,8.06,25.24;11,490.99,508.70,8.06,6.72;11,491.34,481.56,7.77,24.90;11,491.34,465.36,7.77,13.95;11,491.34,455.65,7.77,7.47;11,491.34,433.49,7.77,19.92;11,491.34,392.89,7.77,38.36;11,491.34,374.21,7.77,16.44;11,491.34,363.00,7.77,8.97;11,491.34,343.33,7.77,17.43;11,491.34,328.14,7.77,12.95;11,491.34,301.50,7.77,24.40;11,491.34,271.86,7.77,27.40;11,491.34,236.60,7.77,33.02;11,111.76,67.92,364.50,595.00"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.Scatter plot of terms commonly used by male and female English speakers.</figDesc><graphic coords="11,111.76,67.92,364.50,595.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,203.68,115.83,207.99,81.11"><head>Table 1 .</head><label>1</label><figDesc>Results (accuracy) for the 5-fold cross-validation</figDesc><table coords="3,245.01,139.98,125.33,56.96"><row><cell cols="2">Language Variety Gender Both</cell></row><row><cell>Arabic</cell><cell>0.831 0.800 0.683</cell></row><row><cell>English</cell><cell>0.898 0.823 0.742</cell></row><row><cell>Spanish</cell><cell>0.962 0.832 0.803</cell></row><row><cell cols="2">Portuguese 0.981 0.845 0.828</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,157.42,222.88,300.53,103.03"><head>Table 2 .</head><label>2</label><figDesc>A list of values over which we performed the grid search.</figDesc><table coords="3,157.42,247.04,300.53,78.88"><row><cell>Name</cell><cell>Values</cell><cell>Description</cell></row><row><cell cols="2">lowercase True, False</cell><cell>Lowercase all words</cell></row><row><cell>max_df</cell><cell>0.01, None</cell><cell>Exclude terms that appear in more than n documents</cell></row><row><cell>min_df</cell><cell>1, 2, 3</cell><cell>Exclude terms that appear in fewer than n documents</cell></row><row><cell>use_idf</cell><cell>True, False</cell><cell>Use Inverse Document Frequency weighting</cell></row><row><cell cols="2">sublinear_tf True, False</cell><cell>Replace term frequency (tf) with 1 + log(tf )</cell></row><row><cell>C</cell><cell cols="2">0.1, 0.5, 1, 1.5, 5 Penalty parameter for the SVM</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,134.77,265.90,345.82,187.57"><head>Table 3 .</head><label>3</label><figDesc>Variety</figDesc><table /><note coords="4,303.29,266.25,69.22,7.77;4,187.15,290.05,70.69,8.06;4,187.15,306.36,27.40,7.77;4,229.78,306.36,198.43,7.77;4,229.78,317.32,48.07,7.77;4,187.15,328.28,241.06,7.77;4,229.78,339.24,36.84,7.77;4,187.15,350.20,99.37,7.77;4,187.15,361.15,24.40,7.77;4,229.78,361.15,108.13,7.77;4,134.77,408.96,345.82,8.64;4,134.77,420.92,345.82,8.64;4,134.77,432.87,345.82,8.64;4,134.77,444.83,37.91,8.64"><p>labels per language Language Variety English Australia, Canada, Great Britain, Ireland, New Zealand, United States Spanish Argentina, Chile, Colombia, Mexico, Peru, Spain, Venezuela Portuguese Brazil, Portugal Arabic Egypt, Gulf, Levant, Maghreb changing preprocessing), using a large variety of features, and changing strategies in modelling the problem by using different algorithms and paradigms. All reported results are on the PAN 2017 training data using five-fold cross-validation, unless otherwise specified.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,134.77,314.10,345.83,65.55"><head>Table 4 .</head><label>4</label><figDesc>Results (accuracy) on the English data for Gender and Variety with and without part of speech tags.</figDesc><table coords="5,227.14,349.21,161.07,30.44"><row><cell></cell><cell>Gender Variety</cell></row><row><cell>Unigrams</cell><cell>0.826 0.895</cell></row><row><cell cols="2">Unigrams + Part-of-Speech 0.818 0.853</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,384.21,320.79,96.38,97.97"><head>Table 6 .</head><label>6</label><figDesc>Results (accuracy) for the joint prediction of Gender and Variety.</figDesc><table coords="7,393.83,356.91,74.66,61.85"><row><cell></cell><cell>Joint</cell></row><row><cell>Arabic</cell><cell>0.630</cell></row><row><cell>English</cell><cell>0.645</cell></row><row><cell>Spanish</cell><cell>0.686</cell></row><row><cell cols="2">Portuguese 0.792</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,134.77,115.83,345.83,106.62"><head>Table 7 .</head><label>7</label><figDesc>Performances per classifier: DT: Decision Tree; MLP: Multi-Layer Perceptron, NB: Naive Bayes.</figDesc><table coords="8,197.10,150.94,221.16,71.50"><row><cell></cell><cell></cell><cell>Gender</cell><cell></cell><cell>Variety</cell></row><row><cell></cell><cell>DT</cell><cell>MLP NB</cell><cell>DT</cell><cell>MLP NB</cell></row><row><cell>Arabic</cell><cell cols="2">0.619 0.619 0.699</cell><cell cols="2">0.685 0.813 0.729</cell></row><row><cell>English</cell><cell cols="2">0.635 0.798 0.745</cell><cell cols="2">0.689 0.845 0.696</cell></row><row><cell>Spanish</cell><cell cols="2">0.608 0.783 0.677</cell><cell cols="2">0.782 0.944 0.829</cell></row><row><cell cols="3">Portuguese 0.714 0.813 0.676</cell><cell cols="2">0.955 0.986 0.983</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,152.05,450.58,311.26,99.23"><head>Table 8 .</head><label>8</label><figDesc>Results (accuracy) on the test set for variety, gender and their joint prediction.</figDesc><table coords="8,157.96,474.74,299.44,75.07"><row><cell>Task</cell><cell>System</cell><cell cols="4">Arabic English Portuguese Spanish Average + 2nd</cell></row><row><cell cols="3">Variety N-GrAM 0.8313 0.8988</cell><cell>0.9813</cell><cell>0.9621</cell><cell>0.9184 0.0013</cell></row><row><cell></cell><cell>LDR</cell><cell>0.8250 0.8996</cell><cell>0.9875</cell><cell>0.9625</cell><cell>0.9187</cell></row><row><cell cols="3">Gender N-GrAM 0.8006 0.8233</cell><cell>0.8450</cell><cell>0.8321</cell><cell>0.8253 0.0029</cell></row><row><cell></cell><cell>LDR</cell><cell>0.7044 0.7220</cell><cell>0.7863</cell><cell>0.7171</cell><cell>0.7325</cell></row><row><cell>Joint</cell><cell cols="2">N-GrAM 0.6831 0.7429</cell><cell>0.8288</cell><cell>0.8036</cell><cell>0.7646 0.0101</cell></row><row><cell></cell><cell>LDR</cell><cell>0.5888 0.6357</cell><cell>0.7763</cell><cell>0.6943</cell><cell>0.6738</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,144.73,623.83,56.33,7.77"><p>https://spacy.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,144.73,634.98,304.74,7.77;5,144.73,645.94,61.26,7.77"><p>https://blog.swiftkey.com/americans-love-skulls-brazilians-love-cats-swiftkey-emojimeanings-report/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,139.00,655.22,2.99,5.18;5,144.73,657.08,202.75,7.77"><p>3 http://www.unicode.org/emoji/charts/full-emoji-list.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.61,494.43,335.55,7.77;9,150.95,505.39,205.72,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,314.66,494.43,163.50,7.77;9,150.95,505.39,20.13,7.77">Gender identity and lexical variation in social media</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schnoebelen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,177.35,505.39,95.64,7.77">Journal of Sociolinguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="160" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,516.00,333.04,7.77;9,150.95,526.96,307.81,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,196.62,516.00,279.03,7.77;9,150.95,526.96,128.34,7.77">Improving the character ngram model for the DSL task with BM25 weighting and less frequently used feature sets</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bestgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,296.87,526.96,135.74,7.77">Proceedings of the VarDial Workshop</title>
		<meeting>the VarDial Workshop</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,537.57,326.06,7.77;9,150.95,548.53,328.78,7.77;9,150.95,559.49,243.19,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,246.84,548.53,124.65,7.77">GronUP: Groningen User Profiling</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Busger Op Vollenbroek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Carlotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kreutz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Medvedeva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Haagsma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nissim</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,389.30,548.53,85.60,7.77">Working Notes of CLEF</title>
		<title level="s" coord="9,200.27,559.49,133.12,7.77">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="846" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,570.11,296.37,7.77;9,150.95,581.06,195.79,7.77" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<title level="m" coord="9,333.00,570.11,105.97,7.77;9,150.95,581.06,45.79,7.77">Bag of tricks for efficient text classification</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.61,591.68,311.32,7.77;9,150.95,602.64,323.52,7.77;9,150.95,613.60,315.45,7.77;9,150.95,624.55,53.04,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,199.43,591.68,239.24,7.77">Scattertext: a browser-based tool for visualizing how corpora differ</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,150.95,602.64,323.52,7.77;9,150.95,613.60,269.61,7.77">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations. Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations. Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,635.17,332.74,7.77;9,150.95,646.13,310.29,7.77;9,150.95,657.09,299.61,7.77;10,150.95,119.96,316.06,7.77;10,150.95,130.92,112.54,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,421.55,635.17,53.80,7.77;9,150.95,646.13,310.29,7.77;9,150.95,657.09,38.99,7.77">Discriminating between similar languages and Arabic dialect identification: A report on the third DSL shared task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zampieri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ljubešić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,207.73,657.09,242.84,7.77;10,150.95,119.96,117.32,7.77;10,310.07,119.96,152.73,7.77">Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial3)</title>
		<meeting>the Third Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial3)<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12">December 2016</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct coords="10,142.61,141.88,327.26,7.77;10,150.95,152.84,295.00,7.77;10,150.95,163.80,329.36,7.77;10,150.95,174.76,228.14,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,291.33,141.88,178.55,7.77;10,150.95,152.84,278.99,7.77">When sparse traditional models outperform dense neural networks: the curious case of discriminating between similar languages</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Medvedeva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kroon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,150.95,163.80,325.85,7.77">Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects</title>
		<meeting>the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="156" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,185.71,334.54,7.77;10,150.95,196.67,315.97,7.77;10,150.95,207.63,324.88,7.77;10,150.95,218.59,317.88,7.77;10,150.95,229.55,310.94,7.77;10,150.95,240.51,23.90,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,432.45,185.71,44.71,7.77;10,150.95,196.67,262.32,7.77">Overview of PAN&apos;17: Author Identification, Author Profiling, and Author Obfuscation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tschuggnall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,150.95,218.59,317.88,7.77;10,150.95,229.55,162.70,7.77">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 8th International Conference of the CLEF Initiative (CLEF 17)</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lawless</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Kelly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,251.47,308.17,7.77;10,150.95,262.43,257.82,7.77" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10754</idno>
		<title level="m" coord="10,307.56,251.47,143.22,7.77;10,150.95,262.43,107.79,7.77">A low dimensionality representation for language variety identification</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.24,273.39,331.22,7.77;10,150.95,284.34,329.64,7.77;10,150.95,295.30,309.81,7.77;10,150.95,306.26,255.14,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,312.90,273.39,160.55,7.77;10,150.95,284.34,234.76,7.77">Overview of the 5th Author Profiling Task at PAN 2017: Gender and Language Variety Identification in Twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="10,272.31,295.30,188.45,7.77;10,150.95,306.26,15.74,7.77">Working Notes Papers of the CLEF 2017 Evaluation Labs</title>
		<title level="s" coord="10,172.87,306.26,172.47,7.77">CEUR Workshop Proceedings, CLEF and CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,317.22,316.04,7.77;10,150.95,328.18,318.78,7.77;10,150.95,339.14,70.98,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,386.67,317.22,71.61,7.77;10,150.95,328.18,210.35,7.77">Overview of the 4th author profiling task at PAN 2016: Cross-genre evaluations</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">D M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,379.30,328.18,85.60,7.77">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="750" to="784" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
