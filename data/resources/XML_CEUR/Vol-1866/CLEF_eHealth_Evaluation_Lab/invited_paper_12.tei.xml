<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.22,115.90,330.92,12.68;1,205.46,133.83,204.45,12.68">CLEF 2017 Technologically Assisted Reviews in Empirical Medicine Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,161.77,171.50,86.85,8.80"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<email>e.kanoulas@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.17,171.50,30.44,8.80"><forename type="first">Dan</forename><surname>Li</surname></persName>
							<email>d.li@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.16,171.50,63.81,8.80"><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
							<email>leif.azzopardi@strath.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Computer and Information Sciences</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,393.89,171.50,55.22,8.80"><forename type="first">Rene</forename><surname>Spijker</surname></persName>
							<email>spijker-2@umcutrecht.nl</email>
							<affiliation key="aff2">
								<orgName type="department">Julius Center for Health Sciences and Primary Care</orgName>
								<orgName type="institution" key="instit1">Cochrane Netherlands</orgName>
								<orgName type="institution" key="instit2">UMC Utrecht</orgName>
								<address>
									<country>Netherlands, R</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.22,115.90,330.92,12.68;1,205.46,133.83,204.45,12.68">CLEF 2017 Technologically Assisted Reviews in Empirical Medicine Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ED76C1908DD2B204591B213573D25DD3</idno>
					<idno type="DOI">10.1002/14651858.CD010438.pub2)</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Evaluation</term>
					<term>Information Retrieval</term>
					<term>Systematic Reviews</term>
					<term>TAR</term>
					<term>Text Classification</term>
					<term>Active Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Systematic reviews are a widely used method to provide an overview over the current scientific consensus, by bringing together multiple studies in a reliable, transparent way. The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying all relevant studies in an unbiased way both complex and time consuming to the extent that jeopardizes the validity of their findings and the ability to inform policy and practice in a timely manner. The CLEF 2017 e-Health Lab Task 2 focuses on the efficient and effective ranking of studies during the abstract and title screening phase of conducting Diagnostic Test Accuracy systematic reviews. We constructed a benchmark collection of fifty such reviews and the corresponding relevant and irrelevant articles found by the original Boolean query. Fourteen teams participated in the task, submitting 68 automatic and semi-automatic runs, using information retrieval and machine learning algorithms over a variety of text representations, in a batch and iterative manner. This paper reports both the methodology used to construct the benchmark collection, and the results of the evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="792.0" lry="612.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="792.0" lry="612.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="792.0" lry="612.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="792.0" lry="612.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Evidence-based medicine has become an important pillar in health care and policy making. In order to practice evidence-based medicine, it is important to have a clear overview over the current scientific consensus. These overviews are provided in systematic review articles, that summarize all available evidence regarding a certain topic (e.g., a treatment or diagnostic test). In order to write a systematic review, researchers have to conduct a search that will retrieve all the studies that are relevant. The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying relevant studies in an unbiased way both complex and time consuming to the extent that jeopardizes the validity of their findings and the ability to inform policy and practice in a timely manner. Hence, the need for automation in this process becomes of utmost importance. Finding all relevant studies in a corpus is a difficult task, known in the Information Retrieval (IR) domain as the total recall problem.</p><p>To this date, retrieval of evidence to inform systematic reviews is being conducted in multiple stages:</p><p>1. Boolean Search: At the first stage information specialists build a broad</p><p>Boolean query expressing what constitutes relevant information. The query is then submitted to a medical database containing titles, abstracts, and indexing terms of a controlled vocabulary of medical studies. The result is a set, A, of potentially interesting studies. 2. Title and Abstract Screening: At a second stage experts are screening the titles and abstracts of the returned set and decide which one of those hold potential value for their systematic review, a set D. If screening an abstract has a cost C a , screening all |A| abstracts has a cost of C a * |A|. 3. Study Screening: At a third stage experts are downloading the full text of the potentially relevant abstracts, D, identified in the previous phase and examine the content to decide whether indeed these studies are relevant or not. Examining a document has typically a larger cost of C d &gt; C a . The result of the second screening is a set of references to be included in the systematic review.</p><p>Unfortunately, the precision of the Boolean searches is typically low, hence reviewers often need to look manually through many thousands of irrelevant titles and abstracts in order to identify a small number of relevant ones. Furthermore, the recall of the searches is often assumed to be 100%, which may not be the case.</p><p>To overcome some of the limitations of the Boolean search, researchers have been testing the effectiveness of machine learning and information retrieval methods. O'Mara-Eves et al. <ref type="bibr" coords="2,236.76,477.66,14.86,8.80" target="#b14">[15]</ref> provide a systematic review of the use of text mining techniques for study identification in systematic reviews.</p><p>The goal of this lab is to bring together academic, commercial, and government researchers that will conduct experiments and share results on automatic methods to retrieve relevant studies with high precision and high recall, and release a reusable test collection that can be used as a reference for comparing different retrieval and mining approaches in the field of medical systematic reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Benchmark Collection</head><p>To construct the benchmark collection, the organizers of the task considered 58 systematic reviews on Diagnostic Test Accuracy conducted by the Cochrane researchers. These reviews are publicly available through the Cochrane Library <ref type="foot" coords="2,476.12,636.64,3.97,6.16" target="#foot_0">4</ref>and can be identified by setting the topic filter in the library to "Diagnostic" and "Diagnostic Test Accuracy" and the stage fitler to "Review". At the date of the publication of this article 79 such studies are available, however the last 22 were performed after the organizers put the collection together. The 58 systematic reviews considered can be found in the Appendix of this articles at Table <ref type="table" coords="3,457.67,166.75,3.87,8.80" target="#tab_7">6</ref>.</p><p>Participants were provided with two data sets: (a) a development set, and (b) a test set. The development set consists of 20 topics for Diagnostic Test Accuracy (DTA) systematic reviews, while the test set consists of 30 topics. For both sets, one topic file and two files of relevance judgments at abstract and document level respectively are constructed (qrel's).</p><p>The topic file is generated through the following procedure. For each systematic review, we reviewed the search strategy from the corresponding study in Cochrane Library. A search strategy, among others, consists of the exact Boolean query developed and submitted to a medical database, at the time the review was conducted, and typically can be found in the Appendix of the study. Rene Spijker, a co-author of this work and a Cochrane information specialist examined the grammatical correctness of the search query and specified the date range which dictated the valid dates for the articles to be included in this systematic review. The date range was necessary because a study published after the systematic review should not be included even though it might be relevant, since that would require manually examining its content to quantify its relevance. Important note: A number of medical databases, and search interfaces to these databases is available for search, and for each one information specialists construct a different variation of their query that better fits the data and meta-data of the database. For this task, we only considered the Boolean query constructed for the MEDLINE database, using the Wolters Kluwer Ovid interface. Then we submitted the constructed Boolean query to the OVID system <ref type="foot" coords="3,403.42,428.21,3.97,6.16" target="#foot_1">5</ref> and collected all the returned PubMed document identification numbers (PMID's) which satisfied the date range constraint. This step was automated by a Python script we put together and through an interface available to the University of Amsterdam<ref type="foot" coords="3,473.36,464.08,3.97,6.16" target="#foot_2">6</ref> . Out of the 58 reviews 8 were discarded since the provided Boolean query was not in the right format, which made it difficult if not impossible to reconstruct the set of PMID's, hence the 50 topics in the development and test set.</p><p>The topic file is in a text format and contains four sections, Topic, Title, Query, and PMID's, where Topic is the topic ID, a substring of DOI of the document (e.g. CD010438 for 10.1002/14651858.CD010438.pub2), and PMID's are the document IDs returned by the Boolean query. The PIDs can be used to access the corresponding document through the National Center for Biotechnology Information (NCBI) <ref type="foot" coords="3,222.64,571.68,3.97,6.16" target="#foot_3">7</ref> . An example of a topic file can be viewed below. For the construction of the qrel files, we considered the reference section of the 50 systematic reviews. The references are split into three categories: Included, Exclude, and Additional. Included are the studies that are relevant to the systematic review. Excluded are the studies that in the abstract and title screening stage were considered relevant, but at the article screening phase were considered irrelevant to the study and hence excluded from it. Additional are additional references that do not impact the outcome of the study, and hence irrelevant to it. The included references were the relevant studies at the document-level qrels, while both the included and excluded references were considered relevant at the abstract-level qrels. The format of the qrels followed the standard TREC format:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Iteration Document Relevance</head><p>where Topic is the topic ID of the systematic review, Iteration in our case is a dummy field always zero and not used, Document is the PMID, and Relevancy is a binary code of 0 for not relevant and 1 for relevant studies. The order of documents in the qrel files is not indicative of relevance. Studies that were returned by the Boolean query but were not relevant based on the above process, were considered irrelevant. Those are studies that were excluded at the abstract and title screening phase. All other documents in MEDLINE were also assumed to be irrelevant, given that they were not judged by the human assessor. Important Note: Note that, as mentioned earlier, the references of a systematic review were produced after a number of Boolean queries were submitted to a number of medical databases, and their titles and abstracts were screened. The PMID's provided however were only those that came out of the MEDLINE query. Therefore, there was a number of abstract-level relevant studies (the gray area in the Venn diagram below) that were not part of the result set of the Boolean query provided to the participants. For the development set, the qrel file contained those additional PMIDs, for those participants that would decide to search the entire MEDLINE database, and not only consider the studies provided to them in the Topic files. To the best of our knowledge, no one submitted such a system, hence to avoid any bias we excluded those relevant studies from the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MEDLINE Boolean Query Relevant Studies</head><p>Table <ref type="table" coords="5,176.88,440.43,4.98,8.80" target="#tab_1">1</ref> shows the distribution of the relevant documents at abstract or document level for all the topics in the development set and the test set. The total number of unique PMID is 149,405 for the development set and 117,562 for the test set. Their percentages of relevant documents at abstract level are quite close, which is 1.88% for the development set and 1.58% for the test set. This is not true at document level, however, where the relevant documents in the test set is almost twice as large as in the development set, even though there are 0.52% and 0.33% of relevant studies, respectively. In <ref type="bibr" coords="5,388.55,524.12,14.61,8.80" target="#b16">[17]</ref>, a test collection was developed based on a random selection of 93 Cochrane systematic reviews (not just DTAs), and reported a slightly higher rate of relevance ( 14 1159 = 1.2%). However, compared with the TREC campaign, the rate of relevant documents is 5.45%, 2.78% for the Adhoc track of TREC 8 and the Web track of TREC 2002. Overall, the number of relevant documents is not very high in this lab, making locating them quite a difficult task.</p><p>Important Note: As one can observe in Table <ref type="table" coords="5,355.09,608.24,3.87,8.80" target="#tab_1">1</ref>, there are topics for which the output of the Boolean query is rather narrow, with as few as 64 studies to be reviewed for topic CD008760. Cochrane is conducting systematic reviews on a regular basis, in an attempt to update each review every two-three years. Some of the reviews considered for the construction of the benchmark collection, such as the CD008760 review, are updates to previous reviews. These updates, only specify a query for a time range that starts after the last review on the topic was conducted. Hence, the 64 studies, are the output of the Boolean query for this short time range, hence its small number. If the Boolean query were to run against the entire MEDLINE database, the number of studies would be in the range of tens of thousands, as is the case for some other reviews considered, e.g. CD008782.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>The CLEF 2017 e-Health Lab <ref type="bibr" coords="7,269.30,249.06,9.96,8.80" target="#b7">[8]</ref>, task 2, focused on retrieving studies for conducting Diagnostic Test Accuracy (DTA) systematic reviews. Retrieval in this area is generally considered very difficult, where sensitive searches result in large quantities of references to be screened manually, and a breakthrough in this field would likely be applicable to other areas as well. The task has a focus on the second stage of the process, i.e. given the results of a Boolean search how to make abstract and title screening more effective and efficient. Currently a typical number needed to read (NNR), the number of studies to screen to identify 1 eligible study, for DTA systematic reviews is approximately 80 when applied to potential abstracts that need further full text assessment. With an average of 7000 results to be screened, which would take approximately 120 hours to screen (1 minute per abstract <ref type="bibr" coords="7,266.47,380.57,14.76,8.80" target="#b17">[18]</ref>), a huge benefit can be made in reducing the workload in this process.</p><p>Given the results of the Boolean search from stage 1 as the starting point, participants were asked to rank the set of the provided abstracts. The task had two goals: (i) to produce an efficient ordering of the documents, such that all the relevant abstracts are retrieved above the irrelevant ones, and (ii) to identify the relevant subset of abstracts to be shown to a user, that is a stopping point in the ranked list of abstract, where a researcher could confidently stop screening abstracts and titles. Therefore, we solicited two types of submissions: (i) ranking submission: automatic or manual methods that rank all abstracts, with the goal of retrieving relevant abstracts as early in the ranking as possible, and (ii) thresholding submission: thresholding can be performed in a batch, or iterative manner as well.</p><p>We also considered two evaluation frameworks, (a) a simple evaluation, and (b) a cost-effective evaluation. The assumption behind the simple evaluation framework is the following: The user of your system is the researcher that performs the abstract and title screening of the retrieved articles. Every time an abstract is returned (i.e. ranked) there is an incurred cost/effort of CA, while the abstract is either irrelevant (in which case no further action will be taken) or relevant (and hence passed to the next stage of document screening) to the topic under review. The assumption behind the cost-effective evaluation is the following: The user that performs the screening is not the end-user. The user can interchangeably perform abstract and title screening, or document screening, and decide what PMIDs to pass to the end-user. Every time an abstract is returned the user can either (a) read the abstract (with an incurred cost of CA) and decide whether to pass this PMID to the end-user, or (b) read the full document (with an incurred cost of CA+CD) and decide whether to pass this PMID to the end-user, or (c) directly pass the PMID to the end user (with an incurred cost of 0), or (d) directly discard the PMID and not pass it to the end user (with an incurred cost of 0). For every PMID passed to the end-user there is a cost of attached to it: CA if the abstract passed on is not relevant, and CA + CD if the abstract passed on is relevant (that is, we assume that the end-user completes a two-round abstract and document screening, as usual, but only for the PMIDs the algorithm+feedback user decided to be relevant). Although a small number of teams participated in the cost-effective sub-task, the lab focused on the simple evaluation sub-task, and this is what is described in the remaining of this report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Evaluation within the context of using technology to assist in the reviewing process is very much dependent on how the user(s) interact with the system -and the goal of the technology assistance. For example, is the goal of the assistance to automate the screening process -where the system assess all the abstracts and returns a subset of the initial set to be screened by the end-user (i.e. screened in batch mode). Or, it could be used to identify all the relevant documents as soon as possible, in an iterative manner -where the system asks for feedback from the end-user to help improve the ranking. Of course, then the an open problem is decide when to stop requesting feedback, and when to stop assessing abstracts. In which case a subset of abstracts is identified, which consist of abstracts have been screened during the feedback cycles and the remainder that are screened but are not used for feedback (i.e. in batch mode). There are, of course, many other possible variations. For the purposes of this initial track/task, we consider the problem as a ranking task -that is to rank the set of documents associated with the topic in decreasing order of relevance. We consider a document relevant if the abstract passed the abstract screening phase (regardless of whether it was included or excluded from the study).</p><p>For this task we employ a number of standard measures, typically used in IR ranking evaluations, along with other measures from related tracks and some new measures we have developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Standard Measures</head><p>• Average Precision (AP)</p><p>• Normalized cumulative gain @ 0% to 100% of documents shown; for the simple case that judgments are binary, normalized cumulative gain @ % is simply Recall @ % of shown documents <ref type="bibr" coords="8,347.73,617.18,19.09,8.80" target="#b9">[10]</ref> • Number of Relevant Found (nr) • Recall r = nr/R, where R the total number of relevant documents • Number of documents returned/shown (n) -Related Measures (from <ref type="bibr" coords="9,259.35,118.93,10.96,8.80" target="#b5">[6,</ref><ref type="bibr" coords="9,270.31,118.93,7.30,8.80" target="#b4">5]</ref> • LOSS-R loss r = (1 -r) 2</p><p>• LOSS-E loss e = (n/(R + 100) * 100/N ) 2 , where N is the size of the collection • Reliability = loss r + loss e <ref type="bibr" coords="9,286.85,166.29,10.51,8.80" target="#b5">[6]</ref> • Work Saved over Sampling at r, W SS@Recall = (T N +F N )/N (1-r) <ref type="bibr" coords="9,466.77,178.19,4.61,8.74">[</ref> To calculate the cost based measured, we considered three possible interactions to support a range of different ways to screen the items and to utilize feedback when ranking. We consider the follow possibilities:</p><p>1. suppose we have an ranking algorithm, which uses no feedback from the user, simply ranks the list of abstracts. The list is then presented to the end-user, who evaluates them in a batch. In this case, no feedback is requested, and abstracted are marked, NF. 2. suppose we have a ranking algorithm which uses feedback (i.e. abstract(s) are presented to the user, feedback on their relevance is obtained, which is then used by the algorithm, thus simulating online feedback from the user). In this case, for each document where feedback from the users is requested, abstracts are marked AF, but if no feedback is requested it is marked NF. Abstracts marked NF, are then presented to the end-user to evaluate in a final batch. 3. for either above option, the algorithm may decided that an abstract is not relevant, and thus it does not need to be shown to a user, and so are marked NS.</p><p>To calculate the total cost (TC), we calculated:</p><formula xml:id="formula_0" coords="9,233.35,509.53,247.24,9.71">T C = #N F.C a + #AF.(C a + C f )<label>(1)</label></formula><p>where C a is the cost of assessing the abstract, C f is the cost of asking for feedback #N F is the number of NF items, #AF is the number of AF items. We also created two additional cost measures which included a penalty for missing relevant abstracts (a) with a uniform penalty and (b) a weighted penalty. The uniform penalty was calculated as follows:</p><formula xml:id="formula_1" coords="9,220.72,602.61,259.88,9.71">T CU = T C + (R -r/R) * (N -n) * C p (2)</formula><p>where C p is the cost of the penalty of missing a relevant abstract, N is the total number of documents in the set for the topic. The assumption behind this penalty is that the end-user would need to continue examining abstracts before they would from the remaining (R -r) relevant items, and encounters them at a uniform rate in the remaining N -n abstracts which were not shown. So if half the relevant items were missing, then the penalty component would be</p><formula xml:id="formula_2" coords="10,134.77,142.90,60.27,9.65">(N -n)C p /2.</formula><p>If no relevant items were missing the penalty component would be zero.</p><p>The weighted penalty was calculated as follows:</p><formula xml:id="formula_3" coords="10,220.30,187.47,260.29,31.18">T CW = T C + (R-r) i=1 (1/2 i )(N -n) * CP<label>(3)</label></formula><p>where the assumption is that the end user would been to examine half of the remaining documents to find the next relevant abstract, per missing relevant abstract. So if all relevant items were missing, then the summation would tend to one, and the penalty component tends to (N -n) * C p , while if only one relevant item is missing then, the penalty component is</p><formula xml:id="formula_4" coords="10,379.87,276.62,68.86,9.65">(N -n) * C p /2.</formula><p>To compute these measures we set C a = 1,C f = 2 and C p = 2, to represent the relative costs of the different actions. Note that these are not based on any empirical data and used as a way to regulate penalize feedback and no shows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participants</head><p>Fourteen groups from eleven countries submitted a total of 68 runs for this task:  evaluation framework, while 16 on the cost-effective one. Out of the 52 submitted runs for the simple sub-task, 35 ranked all the PMIDs that were returned by the Boolean query, while 17 tested different stopping criteria over the ranking. Participants employed both supervised and unsupervised methods, for ranking articles. A large number of runs were trained over the provided development set, and their generalization was tested against the test topics. 26 runs used the development set in some fashion, while 26 made no explicit use of it; it may be the case that participants tried different models and algorithms over the development set, and selected to submit the best performing ones, hence there may be a flavor of model selection, however we did not consider this as use of the development set. Participants represented the textual data in a variety of ways, including document-topic features, bag-of-words, topic model distributions, embeddings, metadata. In the remainder of section, by article we mean the abstract and the title of an article. We are not aware of any participant that worked on the full text of these articles.</p><p>In particular, AMC took a batch supervised approach, training a Random Forest over a topic model representation of the articles. A 75-topic model was fitted over all articles in the collection, and the Topic-to-Document matrix was used to extract features <ref type="bibr" coords="12,241.17,338.74,9.96,8.80" target="#b1">[2]</ref>.</p><p>AUTH took a learning-to-rank approach, using both batch and active learning. Their model, HybridRankSVM, consists of two parts: an inter-topic model which utilizes XGBoost and is trained over the entire development corpus and an intra-topic model, an iteratively-built SVM, trained over relevance feedback provided partially in the test topics. For the inter-topic model a total of 24 topicdocument (or solely topic) features were computed over the title, abstract and mesh terms of the articles and the query. For the intra-topic model a TF-IDF vectorization of the articles was used <ref type="bibr" coords="12,298.77,438.99,9.96,8.80" target="#b2">[3]</ref>.</p><p>CNRS trained a logistic regression model on n-gram features from the titles and abstracts and structured data from the Medline citations. One of their models was trained using stochastic gradient descent on the majority of the features, and one on the principal components of a subset of the features. Class imbalance was handled by reweighting and undersampling, while two approaches for relevance feedback were investigated <ref type="bibr" coords="12,296.46,515.33,14.61,8.80" target="#b12">[13]</ref>.</p><p>ECNU took a learning-to-rank approach, using BM25, PL2, and BB2 as features. The trained model was also combined with a vector space model <ref type="bibr" coords="12,460.16,543.85,9.96,8.80" target="#b3">[4]</ref>.</p><p>ETH used a LAMBDA-Mart model trained on features, such as BM25, Fuzzy search, Vector content representation, publishing data. This model was used to experiment with different stopping criteria. One of the approaches taken was to use minimal relevance feedback to estimate the distribution of positive samples by score. This was done by sampling from the articles, preferring articles with higher score. A Gaussian distribution was fitted on the positive samples and the resulting biased distribution was corrected. The correction worked by first adapting the mean and then iteratively finding the standard deviation matching the sampled data the best. For more details the reader can refer to <ref type="bibr" coords="12,430.17,656.06,9.96,8.80" target="#b8">[9]</ref>. NCSU adopted a continuous active learning framework for this task. An SVM classifier was trained on the relevance feedback labels and undersampling of the negatively labeled articles removing those furthest from the SVM decision hyperplane was employed. Different runs made use of different weights on the labels depending on whether the abstract or the full text was considered relevant <ref type="bibr" coords="13,172.42,178.71,14.61,8.80" target="#b19">[20]</ref>.</p><p>NTU examined the role of convolutional neural networks for classifying medical articles for systematic reviews <ref type="bibr" coords="13,286.97,203.46,14.61,8.80" target="#b11">[12]</ref>.</p><p>Padua used a two-dimensional probabilistic version of BM25 to rank articles. The parameters were tuned using the development set. Further, the top abstract returned by BM25 was provided to two non-experts who generated one additional query each. The tree queries were then used to re-rank articles. Different approaches for relevance feedback and thresholding were investigated <ref type="bibr" coords="13,439.41,264.07,14.61,8.80" target="#b13">[14]</ref>.</p><p>QUT trained a learning-to-rank model using domain specific features. As domain specific features, PICO annotations (Population, Intervention, Control, Outcome) were used; these were extracted automatically from articles and manually from the Boolean queries <ref type="bibr" coords="13,271.97,312.73,14.61,8.80" target="#b15">[16]</ref>.</p><p>Sheffield automatically parsed the Boolean queries to extract both the terms and MeSH heading,s and used TF-IDF cosine similarity to calculate the similarity score between document title and abstracts <ref type="bibr" coords="13,342.22,349.43,9.96,8.80" target="#b0">[1]</ref>.</p><p>UOS explored two methods: (i) topic models, where they used Latent Dirichlet Allocation to identify topics within the set of retrieved articles, and then ranking articles by the topic most likely to be relevant to the query, and (ii) relevance feedback, where they used Rocchio's algorithm to update the query model for subsequent rounds of interaction. A third approach combined the topic model and relevance feedback approaches to quickly identify the relevant articles. For the thresholding task, they applied a score threshold over BM25 <ref type="bibr" coords="13,418.27,433.96,14.61,8.80" target="#b10">[11]</ref>.</p><p>UCL took a supervised approach and trained a deep model architecture to identify studies pertaining to a given review topic <ref type="bibr" coords="13,354.89,458.71,14.61,8.80" target="#b18">[19]</ref>.</p><p>Waterloo applied the Baseline Model Implementation (BMI) from the TREC Total Recall Track (2015-2016). They further applied their "knee-method" stopping criterion to BMI to determine how many abstracts should be examined for each topic <ref type="bibr" coords="13,182.08,507.36,9.96,8.80" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Tables 7, 8, 9, 10 provide the results of a selection of the evaluation measures for all participating runs, both against the abstract and the document level relevance judgments, for the simple evaluation scenario. Figure <ref type="figure" coords="13,410.59,595.44,4.98,8.80" target="#fig_0">1</ref> shows the corresponding box plots for Average Precision, with the Mean Average Precision against the abstract and document level judgments respectively denoted with a blue rectangle over the box plot.</p><p>In the following subsections we present results separately for ranking and thresholding runs, so that comparisons can be more meaningful. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ranking Abstracts</head><p>Table <ref type="table" coords="15,161.65,141.34,4.98,8.80" target="#tab_4">3</ref> presents a number of evaluation measures for those runs that ranked the entire set of articles provided by the original Boolean queries; no thresholding has been applied. Some runs, as it may appear from Tables <ref type="table" coords="15,407.93,165.25,7.75,8.80" target="#tab_8">7,</ref><ref type="table" coords="15,420.01,165.25,3.87,8.80" target="#tab_9">8</ref>, 9, 10, even though they applied no stopping criterion, still missed a number of documents. There may be multiple reasons for that, e.g. missing some topic, or not being able to download the abstract text, since participants were provided by PIDs only. The number of documents for which feedback was requested appears in the second column of the table, while the remaining of the columns report different measures of performance.</p><p>Figure <ref type="figure" coords="15,181.14,249.43,4.98,8.80" target="#fig_2">2</ref> shows the recall-effort curves for the participating runs, that is the recall value at different percentage of documents shown to the user. The straight pink line with the triangular markers on x=y is the results of the Boolean query randomly shuffled, and it serves as a naive baseline, provided by the UOS team. The brown curve with the triangular markers is the BM25 retrieval function, also provided by the UOS team as a baseline; it ranks abstracts by BM25 over the Boolean query terms, with the default BM25 parameters setting.   the effectiveness of the ranking algorithms, as expected, however it may come with additional cost in terms of assessing the relevance of abstract (based on the screening setup considered). Fig. <ref type="figure" coords="17,154.39,414.07,4.13,7.93" target="#fig_3">3</ref>. Box-plots of Mean Average Precision for runs that do not make use of relevance feedback and those that do make use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Drawing a Threshold</head><p>Table <ref type="table" coords="17,162.85,488.69,4.98,8.80">4</ref> presents a number of evaluation measures for those runs that applied a threshold criterion. The total number of shown to the user abstracts can be found in the second column of the table, the number of documents for which feedback was requested in the third, while the remaining of the columns report different measures of performance. The cost measures account both for the cost of presenting a document to the user and for the additional cost of requesting feedback for a document, while they also account for the cost one would need to pay to reach 100% recall, under certain assumptions. Reliability considers the cost of not finding all relevant documents but makes no discrimination between the documents returned to the user and those for which feedback is requested. Average precision is well defined under the stopping criterion but hard to be used for comparing runs that use different thresholds. An easy to understand measure is the achieved recall at the rank of the threshold.</p><p>Figure <ref type="figure" coords="17,182.11,644.10,4.98,8.80" target="#fig_5">5</ref> presents recall at the point of the threshold as a function of the number of documents presented to the user; that is at different stopping criteria,  <ref type="table" coords="18,163.34,358.73,4.13,7.93">4</ref>. Evaluation results for submitted runs using different threshold criteria; measures are computed using abstract-level relevance judgments.</p><p>but also with different ranking and thresholding algorithms. As expected the more documents presented to the user (the lower the threshold criterion) the higher the achieved recall. Nevertheless, there are still algorithms that dominate others. The figure present the Pareto frontier. Figure <ref type="figure" coords="18,377.11,455.46,4.98,8.80" target="#fig_5">5</ref> presents recall at the point of the threshold as a function of the feedback documents requested. As it can be viewed, although feedback documents, are in principle helpful towards achieving a high recall, there are algorithms that used no relevance feedback and still achieved high recall at a threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Topic Difficulty</head><p>Table <ref type="table" coords="18,163.36,584.33,4.98,8.80" target="#tab_6">5</ref> provides statistics on the topics used in the test set, along with the average Average Precision (AAP) for each topic, a measure that can be seen as a proxy of the difficult of each topic. The Pearson correlation coefficient between AAP and the percentage of relevant documents, the total number of documents, and the total number of relevant documents is -0.4868 (p-value = 0.006), 0.1295 (p-value = 0.495), and 0.8994 (p-value = 0). Figures <ref type="figure" coords="18,358.24,644.10,4.98,8.80" target="#fig_6">6</ref> and<ref type="figure" coords="18,383.93,644.10,4.98,8.80" target="#fig_7">7</ref> visually demonstrate this correlation.     The CLEF 2017 e-Health Lab Task 2 constructed a benchmark collection of 50 Diagnostic Test Accuracy systematic reviews to study the effectiveness and efficiency of information retrieval and machine learning algorithms in prioritizing the studies to be screened at the abstract and title screening stage, and providing a stopping criterion over the ranked list. The results demonstrate that automatic methods can be trusted for finding most, if not all, relevant studies in a fraction of the time manual screening can do the same. Given that across different runs many parameters change simultaneously it is not easy to come to certain conclusions about the relative performance of automatic methods.</p><p>Regarding the benchmark collection itself, there is a number of limitations to be considered: (a) Pivoting on the results of the the OVID MEDLINE Boolean query limits our ability to identify all relevant studies, i.e. relevant studies that are outputted by Boolean queries over different databases, and relevant studies that are actually not found by these Boolean queries. The former can be overcome by considering all the different queries submitted; for the latter extra manual judgments would be required. (b) Pivoting on abstract and title only we miss the opportunity to study the effect of automatic methods when applied to the full text of the studies, that would present an opportunity to completely overcome the multi-stage process of systematic reviews. However, most of the full text articles are protected under copyright laws that do not give all participants access to those. (c) The evaluation setup of ranking does not allows us to consider the cost of the process, since given a ranking a researcher would have to still go over all studies ranked. A more realistic setup, e.g. a double-screening setup, could be considered. (d) In the construction of relevant judgments we considered the included and excluded references of the systematic reviews under study, which prevented us to study the noise and disagreement between reviewers. (e) In our effort to allow iterative algorithms, e.g. active learning algorithms, to be submitted, we handed the test sets' relevant judgments directly to the participants, which is rather unusual for this type of evaluation exercises. An alternative would be the setup used by the TREC Total Recall, where participants submitted their running algorithms to the organizers. (f) When it comes to evaluation measures there is a large variety of those, all of which take a different often useful view point on the effectiveness of algorithm, but which makes it difficult to decide upon a single golden measure to rank participants' runs. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="10,138.97,386.81,246.98,8.80;10,138.97,398.66,243.29,8.80;10,138.97,410.52,341.62,8.80;10,151.70,422.47,146.61,8.80;10,138.97,434.33,216.53,8.80;10,138.97,446.18,303.84,8.80;10,138.97,458.03,339.34,8.80;10,138.97,469.88,256.39,8.80;10,138.97,481.74,242.20,8.80;10,138.97,493.59,164.71,8.80;10,133.99,505.44,239.96,8.80;10,133.99,517.30,346.59,8.80;10,151.70,529.25,52.42,8.80;10,133.99,541.10,206.53,8.80;10,133.99,552.96,304.98,8.80;10,133.99,564.81,237.77,8.80;10,149.71,584.33,330.88,8.80;10,134.77,596.28,345.83,8.80;10,134.77,608.24,345.83,8.80;10,134.77,620.19,345.82,8.80;10,134.77,632.15,345.82,8.80;10,134.77,644.10,345.83,8.80;10,134.77,656.06,345.83,8.80"><head>1 .</head><label>1</label><figDesc>Amsterdam Medical Center, The Netherlands (AMC) 2. Aristotle University of Thessaloniki, Greece (AUTH) 3. Centre Nationnal de la Recherche Scientifique, France &amp; Amsterdam Medical Center, The Netherlands (CNRS) 4. East China Normal University, China (ECNU) 5. Eidgenoessische Technische Hochschule Zurich, Switzerland (ETH) 6. International Institute of Information Technology, Hyderabad, India (IIIT) 7. North Carolina State University, United States (NCSU) 8. Nanyang Technological University, Singapore (NTU) 9. University of Padua, Italy (Padua) 10. University of Sheffield, United Kingdom (Sheffield) 11. University College London, United Kingdom &amp; Northeastern University, USA (UCL) 12. University of Waterloo, Canada (Waterloo) 13. Queensland University of Technology &amp; CSIRO, Australia (QUT) 14. University of Strathclyde, United Kingdom (UOS) Table 2 categorizes the participating runs along five dimensions: (a) automatic vs manual runs; (b) use of the development set; (c) use of supervised and semi-supervised learning algorithms, (d) use of relevance feedback; and (e) thresholding the ranked list of articles. The categorization has been performed by the lab coordinators -not by the participants -based on the submitted participants description of their algorithms. Hence, there is always a chance of mis-classifying some run. Out of the 68 runs submitted, 52 focused on the simple</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="14,134.77,699.47,345.82,7.93;14,134.77,710.44,69.41,7.92;14,169.35,400.79,276.65,283.95"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Average precision using the abstract (top) and document (bottom) level relevance judgments.</figDesc><graphic coords="14,169.35,400.79,276.65,283.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="15,189.43,597.40,236.50,7.93;15,177.99,353.63,259.36,229.03"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Recall at different percentage of shown documents.</figDesc><graphic coords="15,177.99,353.63,259.36,229.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="15,149.71,632.15,330.88,8.80;15,134.77,644.10,345.83,8.80;15,134.77,656.06,345.83,8.80"><head>Figure 3</head><label>3</label><figDesc>Figure3presents the box-plots of Mean Average Precision values for runs that do not make use of relevance feedback (left) and runs that make use of relevance feedback (right) respectively. On average relevance feedback boosts</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="19,134.77,360.72,345.83,7.93;19,134.77,371.68,46.14,7.92;19,177.99,120.71,259.37,225.27"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Recall at the threshold rank as a function of the number of documents shown to the user.</figDesc><graphic coords="19,177.99,120.71,259.37,225.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="19,134.77,639.16,345.83,7.93;19,134.77,650.13,87.66,7.92;19,177.99,395.59,259.36,228.84"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Recall at the threshold rank as a function of the number of documents for which feedback is requested.</figDesc><graphic coords="19,177.99,395.59,259.36,228.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="21,134.77,361.72,345.83,7.93;21,134.77,372.69,45.61,7.92;21,177.99,122.27,259.37,224.72"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Average Average Precision (AAP) as a function of the percentage of relevant documents.</figDesc><graphic coords="21,177.99,122.27,259.37,224.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="21,134.77,639.34,345.83,7.93;21,134.77,650.31,26.41,7.92;21,177.99,399.70,259.38,224.91"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Average Average Precision (AAP) as a function of the total number of documents.</figDesc><graphic coords="21,177.99,399.70,259.38,224.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="17,177.99,173.75,259.36,225.59"><head></head><label></label><figDesc></figDesc><graphic coords="17,177.99,173.75,259.36,225.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,153.34,120.91,308.67,617.19"><head>Table 1 .</head><label>1</label><figDesc>Statistics of development and test set.</figDesc><table coords="6,153.34,120.91,308.67,604.20"><row><cell cols="7">file name Topic # total PMIDs # abs rel # doc rel % abs rel % doc rel</cell></row><row><cell></cell><cell></cell><cell cols="2">Development Set</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>CD010438</cell><cell>3250</cell><cell>39</cell><cell>3</cell><cell>1.20</cell><cell>0.09</cell></row><row><cell></cell><cell>CD007427</cell><cell>1521</cell><cell>123</cell><cell>17</cell><cell>8.09</cell><cell>1.12</cell></row><row><cell></cell><cell>CD009593</cell><cell>14922</cell><cell>78</cell><cell>24</cell><cell>0.52</cell><cell>0.16</cell></row><row><cell></cell><cell>CD011549</cell><cell>12705</cell><cell>2</cell><cell>1</cell><cell>0.02</cell><cell>0.01</cell></row><row><cell></cell><cell>CD011134</cell><cell>1953</cell><cell>215</cell><cell>49</cell><cell>11.01</cell><cell>2.51</cell></row><row><cell></cell><cell>CD008686</cell><cell>3966</cell><cell>7</cell><cell>5</cell><cell>0.18</cell><cell>0.13</cell></row><row><cell></cell><cell>CD011975</cell><cell>8201</cell><cell>619</cell><cell>60</cell><cell>7.55</cell><cell>0.73</cell></row><row><cell></cell><cell>CD009323</cell><cell>3881</cell><cell>122</cell><cell>9</cell><cell>3.14</cell><cell>0.23</cell></row><row><cell></cell><cell>CD009020</cell><cell>1584</cell><cell>162</cell><cell>12</cell><cell>10.23</cell><cell>0.76</cell></row><row><cell></cell><cell>CD011548</cell><cell>12708</cell><cell>113</cell><cell>5</cell><cell>0.89</cell><cell>0.04</cell></row><row><cell>4</cell><cell>CD011984</cell><cell>8192</cell><cell>454</cell><cell>28</cell><cell>5.54</cell><cell>0.34</cell></row><row><cell></cell><cell>CD010409</cell><cell>43363</cell><cell>76</cell><cell>41</cell><cell>0.18</cell><cell>0.09</cell></row><row><cell></cell><cell>CD008054</cell><cell>3217</cell><cell>274</cell><cell>41</cell><cell>8.52</cell><cell>1.27</cell></row><row><cell></cell><cell>CD010771</cell><cell>322</cell><cell>48</cell><cell>1</cell><cell>14.91</cell><cell>0.31</cell></row><row><cell></cell><cell>CD009591</cell><cell>7991</cell><cell>144</cell><cell>41</cell><cell>1.80</cell><cell>0.51</cell></row><row><cell></cell><cell>CD008691</cell><cell>1316</cell><cell>73</cell><cell>20</cell><cell>5.55</cell><cell>1.52</cell></row><row><cell></cell><cell>CD010632</cell><cell>1504</cell><cell>32</cell><cell>14</cell><cell>2.13</cell><cell>0.93</cell></row><row><cell></cell><cell>CD007394</cell><cell>2545</cell><cell>95</cell><cell>47</cell><cell>3.73</cell><cell>1.85</cell></row><row><cell>6</cell><cell>CD008643</cell><cell>15083</cell><cell>11</cell><cell>4</cell><cell>0.07</cell><cell>0.03</cell></row><row><cell>9</cell><cell>CD009944</cell><cell>1181</cell><cell>117</cell><cell>64</cell><cell>9.91</cell><cell>5.42</cell></row><row><cell>total</cell><cell></cell><cell>149405</cell><cell>2804</cell><cell>486</cell><cell>1.88</cell><cell>0.33</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Test Set</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CD007431</cell><cell>2074</cell><cell>24</cell><cell>15</cell><cell>1.16</cell><cell>0.72</cell></row><row><cell></cell><cell>CD008803</cell><cell>5220</cell><cell>99</cell><cell>99</cell><cell>1.90</cell><cell>1.90</cell></row><row><cell></cell><cell>CD008782</cell><cell>10507</cell><cell>45</cell><cell>34</cell><cell>0.43</cell><cell>0.32</cell></row><row><cell></cell><cell>CD009647</cell><cell>2785</cell><cell>56</cell><cell>17</cell><cell>2.01</cell><cell>0.61</cell></row><row><cell></cell><cell>CD009135</cell><cell>791</cell><cell>77</cell><cell>19</cell><cell>9.73</cell><cell>2.40</cell></row><row><cell></cell><cell>CD008760</cell><cell>64</cell><cell>12</cell><cell>9</cell><cell>18.75</cell><cell>14.06</cell></row><row><cell>2</cell><cell>CD010775</cell><cell>241</cell><cell>11</cell><cell>4</cell><cell>4.56</cell><cell>1.66</cell></row><row><cell></cell><cell>CD009519</cell><cell>5971</cell><cell>104</cell><cell>46</cell><cell>1.74</cell><cell>0.77</cell></row><row><cell></cell><cell>CD009372</cell><cell>2248</cell><cell>25</cell><cell>10</cell><cell>1.11</cell><cell>0.44</cell></row><row><cell></cell><cell>CD010276</cell><cell>5495</cell><cell>54</cell><cell>24</cell><cell>0.98</cell><cell>0.44</cell></row><row><cell></cell><cell>CD009551</cell><cell>1911</cell><cell>46</cell><cell>16</cell><cell>2.41</cell><cell>0.84</cell></row><row><cell></cell><cell>CD012019</cell><cell>10317</cell><cell>3</cell><cell>1</cell><cell>0.03</cell><cell>0.01</cell></row><row><cell></cell><cell>CD008081</cell><cell>970</cell><cell>26</cell><cell>10</cell><cell>2.68</cell><cell>1.03</cell></row><row><cell></cell><cell>CD009185</cell><cell>1615</cell><cell>92</cell><cell>23</cell><cell>5.70</cell><cell>1.42</cell></row><row><cell></cell><cell>CD010339</cell><cell>12807</cell><cell>114</cell><cell>9</cell><cell>0.89</cell><cell>0.07</cell></row><row><cell></cell><cell>CD010653</cell><cell>8002</cell><cell>45</cell><cell>0</cell><cell>0.56</cell><cell>0.00</cell></row><row><cell></cell><cell>CD010542</cell><cell>348</cell><cell>20</cell><cell>8</cell><cell>5.75</cell><cell>2.30</cell></row><row><cell></cell><cell>CD010896</cell><cell>169</cell><cell>6</cell><cell>3</cell><cell>3.55</cell><cell>1.78</cell></row><row><cell></cell><cell>CD010023</cell><cell>981</cell><cell>52</cell><cell>14</cell><cell>5.30</cell><cell>1.43</cell></row><row><cell></cell><cell>CD010772</cell><cell>316</cell><cell>47</cell><cell>11</cell><cell>14.87</cell><cell>3.48</cell></row><row><cell></cell><cell>CD011145</cell><cell>10872</cell><cell>202</cell><cell>48</cell><cell>1.86</cell><cell>0.44</cell></row><row><cell></cell><cell>CD010705</cell><cell>114</cell><cell>23</cell><cell>18</cell><cell>20.18</cell><cell>15.79</cell></row><row><cell></cell><cell>CD010633</cell><cell>1573</cell><cell>4</cell><cell>3</cell><cell>0.25</cell><cell>0.19</cell></row><row><cell></cell><cell>CD010173</cell><cell>5495</cell><cell>23</cell><cell>10</cell><cell>0.42</cell><cell>0.18</cell></row><row><cell>5</cell><cell>CD009786</cell><cell>2065</cell><cell>10</cell><cell>6</cell><cell>0.48</cell><cell>0.29</cell></row><row><cell></cell><cell>CD010386</cell><cell>626</cell><cell>2</cell><cell>1</cell><cell>0.32</cell><cell>0.16</cell></row><row><cell></cell><cell>CD010783</cell><cell>10905</cell><cell>30</cell><cell>11</cell><cell>0.28</cell><cell>0.10</cell></row><row><cell></cell><cell>CD010860</cell><cell>94</cell><cell>7</cell><cell>4</cell><cell>7.45</cell><cell>4.26</cell></row><row><cell>7</cell><cell>CD009579</cell><cell>6455</cell><cell>138</cell><cell>79</cell><cell>2.14</cell><cell>1.22</cell></row><row><cell>8</cell><cell>CD009925</cell><cell>6531</cell><cell>460</cell><cell>55</cell><cell>7.04</cell><cell>0.84</cell></row><row><cell>total</cell><cell></cell><cell>117562</cell><cell>1857</cell><cell>607</cell><cell>1.58</cell><cell>0.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,134.77,128.25,357.52,516.92"><head>Table 2 .</head><label>2</label><figDesc>Categorization of participant's runs in the simple evaluation framework along five dimensions.</figDesc><table coords="11,136.16,128.25,356.13,485.13"><row><cell>Team</cell><cell>Run</cell><cell cols="5">Auto Develop-Supervised Feedback Threshold</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ment</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AMC</cell><cell>amc.run.res</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>AUTH</cell><cell>simple.run1/run2/run3/run4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell></row><row><cell cols="2">BASELINE BM25</cell><cell></cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell></row><row><cell cols="2">BASELINE random.pubmed</cell><cell></cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell></row><row><cell>CNRS</cell><cell>cnrs.abrupt.all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell></row><row><cell>CNRS</cell><cell>cnrs.gradual.all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell></row><row><cell>CNRS</cell><cell>cnrs.noaf.all</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>CNRS</cell><cell>cnrs.noaffull.all</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>ECNU</cell><cell>run1</cell><cell></cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell></row><row><cell>ECNU</cell><cell>run2</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell></row><row><cell>ECNU</cell><cell>run3</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell></row><row><cell>ETH</cell><cell>m1</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell></row><row><cell>ETH</cell><cell>m2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ETH</cell><cell>m4</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell></row><row><cell>IIIT</cell><cell>run1/run2/run3/run4</cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell></row><row><cell>NCSU</cell><cell>simple</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NCSU</cell><cell>abs</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NTU</cell><cell>run1/run2/run3</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>Padua</cell><cell>iafa_m10k150f0m10</cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>Padua</cell><cell>iafap_m10p2f0m10</cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>Padua</cell><cell>iafap_m10p5f0m10</cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>Padua</cell><cell>iafas_m10k50f0m10</cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>QUT</cell><cell>ca_bool_ltr</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>QUT</cell><cell>ca_pico_ltr</cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>QUT</cell><cell>rf_bool_ltr</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>QUT</cell><cell>rf_pico_ltr</cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>QUT</cell><cell>bool_es</cell><cell></cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell></row><row><cell>QUT</cell><cell>pico_es</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell></row><row><cell>Sheffield</cell><cell>run1/run2/run3/run4</cell><cell></cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell></row><row><cell>UCL</cell><cell>abstract</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>UCL</cell><cell>fulltext</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>UOS</cell><cell>sis.AL30Q_BM25</cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell></row><row><cell>UOS</cell><cell>sis.TMBEST_BM25</cell><cell></cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell></row><row><cell>UOS</cell><cell>sis.TMAL30Q_BM25</cell><cell></cell><cell>x</cell><cell>x</cell><cell></cell><cell>x</cell></row><row><cell>UOS</cell><cell>sis.bm25_t1.5</cell><cell></cell><cell>x</cell><cell>x</cell><cell>x</cell><cell></cell></row><row><cell>UOS</cell><cell>sis.bm25_t1</cell><cell></cell><cell>x</cell><cell>x</cell><cell>x</cell><cell></cell></row><row><cell>UOS</cell><cell>sis.bm25_t2.5</cell><cell></cell><cell>x</cell><cell>x</cell><cell>x</cell><cell></cell></row><row><cell>UOS</cell><cell>sis.bm25_t2</cell><cell></cell><cell>x</cell><cell>x</cell><cell>x</cell><cell></cell></row><row><cell cols="2">Waterloo A-rank-normal.txt</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell></row><row><cell cols="2">Waterloo A-thresh-normal.txt</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Waterloo B-rank-normal.txt</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell></row><row><cell cols="2">Waterloo B-thresh-normal.txt</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="16,134.77,178.49,345.83,416.10"><head>Table 3 .</head><label>3</label><figDesc>Evaluation results for submitted runs ranking the entire set of articles provided by the Boolean query.</figDesc><table coords="16,157.02,178.49,301.32,391.88"><row><cell>Run</cell><cell cols="3">Feedback Last wss@100 wss@95 Area AP</cell></row><row><cell></cell><cell></cell><cell>Rank</cell><cell>Under</cell></row><row><cell></cell><cell></cell><cell>Rel</cell><cell>Recall</cell></row><row><cell>amc.run</cell><cell>0</cell><cell>2913 0.249</cell><cell>0.333 0.761 0.129</cell></row><row><cell>auth.simple.run1</cell><cell cols="2">41337 2143 0.519</cell><cell>0.693 0.928 0.297</cell></row><row><cell>auth.simple.run2</cell><cell cols="2">41377 2124 0.521</cell><cell>0.697 0.920 0.293</cell></row><row><cell>auth.simple.run3</cell><cell cols="2">23337 2183 0.511</cell><cell>0.678 0.924 0.285</cell></row><row><cell>auth.simple.run4</cell><cell cols="2">41537 2119 0.519</cell><cell>0.690 0.920 0.293</cell></row><row><cell>BASELINE.BM25</cell><cell>0</cell><cell>2851 0.285</cell><cell>0.400 0.809 0.174</cell></row><row><cell>BASELINE.pubmed.random</cell><cell>0</cell><cell>3722 0.040</cell><cell>0.034 0.484 0.045</cell></row><row><cell>cnrs.abrupt.all</cell><cell cols="2">19980 3414 0.173</cell><cell>0.243 0.735 0.143</cell></row><row><cell>cnrs.gradual.all</cell><cell cols="2">23683 3406 0.195</cell><cell>0.288 0.708 0.146</cell></row><row><cell>cnrs.noaf.all</cell><cell>0</cell><cell>2993 0.261</cell><cell>0.362 0.780 0.145</cell></row><row><cell>cnrs.noaffull.all</cell><cell>0</cell><cell>2250 0.412</cell><cell>0.497 0.839 0.179</cell></row><row><cell>ecnu.run1</cell><cell>0</cell><cell>3633 0.099</cell><cell>0.121 0.627 0.091</cell></row><row><cell>ntu.run1</cell><cell>0</cell><cell>3403 0.089</cell><cell>0.108 0.612 0.078</cell></row><row><cell>ntu.run2</cell><cell>0</cell><cell>3204 0.117</cell><cell>0.131 0.595 0.060</cell></row><row><cell>ntu.run3</cell><cell>0</cell><cell>3570 0.091</cell><cell>0.075 0.538 0.052</cell></row><row><cell>padua.iafa_m10k150f0m10</cell><cell>2350</cell><cell>2269 0.415</cell><cell>0.508 0.896 0.280</cell></row><row><cell>padua.iafap_m10p2f0m10</cell><cell>2367</cell><cell>2395 0.366</cell><cell>0.476 0.875 0.253</cell></row><row><cell>padua.iafap_m10p5f0m10</cell><cell>5893</cell><cell>2260 0.398</cell><cell>0.496 0.885 0.269</cell></row><row><cell>padua.iafas_m10k50f0m10</cell><cell>4320</cell><cell>2304 0.410</cell><cell>0.517 0.892 0.266</cell></row><row><cell>qut.ca_bool_ltr</cell><cell>0</cell><cell>3142 0.201</cell><cell>0.288 0.733 0.114</cell></row><row><cell>qut.ca_pico_ltr</cell><cell>0</cell><cell>3344 0.212</cell><cell>0.294 0.751 0.153</cell></row><row><cell>qut.rf_bool_ltr</cell><cell>0</cell><cell>3099 0.194</cell><cell>0.267 0.705 0.106</cell></row><row><cell>qut.rf_pico_ltr</cell><cell>0</cell><cell>3155 0.235</cell><cell>0.293 0.727 0.121</cell></row><row><cell>sheffield.run1</cell><cell>0</cell><cell>2678 0.310</cell><cell>0.422 0.818 0.170</cell></row><row><cell>sheffield.run2</cell><cell>0</cell><cell>2441 0.385</cell><cell>0.493 0.845 0.218</cell></row><row><cell>sheffield.run3</cell><cell>0</cell><cell>2404 0.384</cell><cell>0.473 0.841 0.199</cell></row><row><cell>sheffield.run4</cell><cell>0</cell><cell>2382 0.395</cell><cell>0.488 0.847 0.218</cell></row><row><cell>ucl.run_abstract</cell><cell>0</cell><cell>3801 0.072</cell><cell>0.064 0.507 0.060</cell></row><row><cell>ucl.run_fulltext</cell><cell>0</cell><cell>3755 0.077</cell><cell>0.076 0.522 0.053</cell></row><row><cell>uos.sis.TMAL30Q_BM25</cell><cell cols="2">35432 2305 0.398</cell><cell>0.530 0.837 0.162</cell></row><row><cell>uos.sis.TMBEST_BM25</cell><cell>0</cell><cell>3124 0.274</cell><cell>0.324 0.727 0.124</cell></row><row><cell>waterloo.A-rank-normal</cell><cell cols="2">117558 1464 0.601</cell><cell>0.700 0.927 0.279</cell></row><row><cell>waterloo.B-rank-normal</cell><cell cols="2">117558 1469 0.611</cell><cell>0.701 0.933 0.318</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="20,134.77,205.62,345.83,361.84"><head>Table 5 .</head><label>5</label><figDesc>Average Average Precision (AAP) per topic as a measure of topic difficulty, along with statistics about relevant documents.</figDesc><table coords="20,188.72,205.62,237.92,337.08"><row><cell>Topic</cell><cell cols="4">Average AP % of Relevant Documents Relevant</cell></row><row><cell>CD010173</cell><cell>0.035</cell><cell>0.42</cell><cell>5495</cell><cell></cell></row><row><cell>CD010783</cell><cell>0.036</cell><cell>0.28</cell><cell>10905</cell><cell></cell></row><row><cell>CD010386</cell><cell>0.040</cell><cell>0.32</cell><cell>626</cell><cell>2</cell></row><row><cell>CD012019</cell><cell>0.042</cell><cell>0.03</cell><cell>10317</cell><cell>3</cell></row><row><cell>CD010339</cell><cell>0.051</cell><cell>0.89</cell><cell>12807</cell><cell>114</cell></row><row><cell>CD008081</cell><cell>0.076</cell><cell>2.68</cell><cell>970</cell><cell></cell></row><row><cell>CD007431</cell><cell>0.077</cell><cell>1.16</cell><cell>2074</cell><cell></cell></row><row><cell>CD009786</cell><cell>0.078</cell><cell>0.48</cell><cell>2065</cell><cell></cell></row><row><cell>CD010653</cell><cell>0.079</cell><cell>0.56</cell><cell>8002</cell><cell></cell></row><row><cell>CD010276</cell><cell>0.094</cell><cell>0.98</cell><cell>5495</cell><cell></cell></row><row><cell>CD008782</cell><cell>0.096</cell><cell>0.43</cell><cell>10507</cell><cell></cell></row><row><cell>CD009647</cell><cell>0.096</cell><cell>2.01</cell><cell>2785</cell><cell></cell></row><row><cell>CD009372</cell><cell>0.102</cell><cell>1.11</cell><cell>2248</cell><cell></cell></row><row><cell>CD011145</cell><cell>0.107</cell><cell>1.86</cell><cell>10872</cell><cell>202</cell></row><row><cell>CD010896</cell><cell>0.119</cell><cell>3.55</cell><cell>169</cell><cell>6</cell></row><row><cell>CD008803</cell><cell>0.132</cell><cell>1.90</cell><cell>5220</cell><cell></cell></row><row><cell>CD010633</cell><cell>0.146</cell><cell>0.25</cell><cell>1573</cell><cell>4</cell></row><row><cell>CD010542</cell><cell>0.149</cell><cell>5.75</cell><cell>348</cell><cell></cell></row><row><cell>CD009551</cell><cell>0.156</cell><cell>2.41</cell><cell>1911</cell><cell></cell></row><row><cell>CD009519</cell><cell>0.158</cell><cell>1.74</cell><cell>5971</cell><cell>104</cell></row><row><cell>CD009185</cell><cell>0.254</cell><cell>5.70</cell><cell>1615</cell><cell></cell></row><row><cell>CD010775</cell><cell>0.266</cell><cell>4.56</cell><cell>241</cell><cell></cell></row><row><cell>CD009925</cell><cell>0.269</cell><cell>7.04</cell><cell>6531</cell><cell>460</cell></row><row><cell>CD010023</cell><cell>0.290</cell><cell>5.30</cell><cell>981</cell><cell></cell></row><row><cell>CD010860</cell><cell>0.310</cell><cell>7.45</cell><cell>94</cell><cell>7</cell></row><row><cell>CD009579</cell><cell>0.317</cell><cell>2.14</cell><cell>6455</cell><cell>138</cell></row><row><cell>CD009135</cell><cell>0.351</cell><cell>9.73</cell><cell>791</cell><cell></cell></row><row><cell>CD010772</cell><cell>0.395</cell><cell>14.87</cell><cell>316</cell><cell></cell></row><row><cell>CD008760</cell><cell>0.423</cell><cell>18.75</cell><cell>64</cell><cell></cell></row><row><cell>CD010705</cell><cell>0.524</cell><cell>20.18</cell><cell>114</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="25,134.77,538.37,345.82,18.89"><head>Table 6 .</head><label>6</label><figDesc>The DOI's of the studies considered for the construction of the benchmark collection</figDesc><table coords="26,147.29,136.66,510.67,359.00"><row><cell>Run</cell><cell cols="10">Docs Feedback Rel Last wss@100 wss@95 Cost w/ Cost w/ Area AP Recall@ Reliability</cell></row><row><cell></cell><cell>Shown</cell><cell></cell><cell>Docs Rank</cell><cell></cell><cell></cell><cell cols="3">Uniform Weighted Under</cell><cell>Thresh</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Found Rel</cell><cell></cell><cell></cell><cell cols="3">Penalty Penalty Recall</cell><cell></cell></row><row><cell>amc.run</cell><cell>117548</cell><cell>0</cell><cell>1857 2913</cell><cell>0.25</cell><cell>0.33</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.76 0.13 1.00</cell><cell>0.54</cell></row><row><cell>auth.simple.run1</cell><cell cols="3">117561 41337 1857 2143</cell><cell>0.52</cell><cell>0.69</cell><cell>6674</cell><cell>6674</cell><cell cols="2">0.93 0.30 1.00</cell><cell>0.54</cell></row><row><cell>auth.simple.run2</cell><cell cols="3">117561 41377 1857 2124</cell><cell>0.52</cell><cell>0.70</cell><cell>6677</cell><cell>6677</cell><cell cols="2">0.92 0.29 1.00</cell><cell>0.54</cell></row><row><cell>auth.simple.run3</cell><cell cols="3">117561 23337 1857 2183</cell><cell>0.51</cell><cell>0.68</cell><cell>5474</cell><cell>5474</cell><cell cols="2">0.92 0.28 1.00</cell><cell>0.54</cell></row><row><cell>auth.simple.run4</cell><cell cols="3">117561 41537 1857 2119</cell><cell>0.52</cell><cell>0.69</cell><cell>6687</cell><cell>6687</cell><cell cols="2">0.92 0.29 1.00</cell><cell>0.54</cell></row><row><cell>BASELINE.BM25</cell><cell>117550</cell><cell>0</cell><cell>1857 2851</cell><cell>0.28</cell><cell>0.40</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.81 0.17 1.00</cell><cell>0.54</cell></row><row><cell cols="2">BASELINE.pubmed.random 117562</cell><cell>0</cell><cell>1857 3722</cell><cell>0.04</cell><cell>0.03</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.48 0.04 1.00</cell><cell>0.54</cell></row><row><cell>cnrs.abrupt.all</cell><cell cols="3">117557 19980 1857 3414</cell><cell>0.17</cell><cell>0.24</cell><cell>5250</cell><cell>5250</cell><cell cols="2">0.73 0.14 1.00</cell><cell>0.54</cell></row><row><cell>cnrs.gradual.all</cell><cell cols="3">117557 23683 1857 3406</cell><cell>0.20</cell><cell>0.29</cell><cell>5497</cell><cell>5497</cell><cell cols="2">0.71 0.15 1.00</cell><cell>0.54</cell></row><row><cell>cnrs.noaf.all</cell><cell>117557</cell><cell>0</cell><cell>1857 2993</cell><cell>0.26</cell><cell>0.36</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.78 0.14 1.00</cell><cell>0.54</cell></row><row><cell>cnrs.noaffull.all</cell><cell>117557</cell><cell>0</cell><cell>1857 2250</cell><cell>0.41</cell><cell>0.50</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.84 0.18 1.00</cell><cell>0.54</cell></row><row><cell>ecnu.run1</cell><cell>117561</cell><cell>0</cell><cell>1857 3633</cell><cell>0.10</cell><cell>0.12</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.63 0.09 1.00</cell><cell>0.54</cell></row><row><cell>ecnu.run2</cell><cell>30000</cell><cell>0</cell><cell>1191 699</cell><cell>0.07</cell><cell>0.16</cell><cell>4003</cell><cell>6641</cell><cell cols="2">0.64 0.16 0.71</cell><cell>0.44</cell></row><row><cell>ecnu.run3</cell><cell>30000</cell><cell>0</cell><cell>1197 725</cell><cell>0.08</cell><cell>0.17</cell><cell>4016</cell><cell>6717</cell><cell cols="2">0.65 0.17 0.72</cell><cell>0.44</cell></row><row><cell>eth.m1</cell><cell>51640</cell><cell>0</cell><cell>1686 1372</cell><cell>0.24</cell><cell>0.28</cell><cell>2306</cell><cell>4740</cell><cell cols="2">0.81 0.22 0.93</cell><cell>0.20</cell></row><row><cell>eth.m2</cell><cell>51604</cell><cell>5063</cell><cell>1702 1435</cell><cell>0.14</cell><cell>0.24</cell><cell>2676</cell><cell>4720</cell><cell cols="2">0.80 0.21 0.90</cell><cell>0.14</cell></row><row><cell>eth.m4</cell><cell>27046</cell><cell>0</cell><cell>1406 785</cell><cell>0.12</cell><cell>0.16</cell><cell>2527</cell><cell>5590</cell><cell cols="2">0.74 0.21 0.82</cell><cell>0.14</cell></row><row><cell>iiit.run1</cell><cell cols="3">15354 15354 1006 548</cell><cell>0.11</cell><cell>0.14</cell><cell>3550</cell><cell>6685</cell><cell cols="2">0.68 0.16 0.74</cell><cell>0.15</cell></row><row><cell>iiit.run2</cell><cell cols="3">15354 15354 1006 548</cell><cell>0.11</cell><cell>0.14</cell><cell>3550</cell><cell>6685</cell><cell cols="2">0.68 0.16 0.74</cell><cell>0.15</cell></row><row><cell>iiit.run3</cell><cell cols="3">15354 15354 1006 548</cell><cell>0.11</cell><cell>0.14</cell><cell>3550</cell><cell>6685</cell><cell cols="2">0.68 0.16 0.74</cell><cell>0.15</cell></row><row><cell>iiit.run4</cell><cell cols="3">15354 15354 1006 548</cell><cell>0.11</cell><cell>0.14</cell><cell>3550</cell><cell>6685</cell><cell cols="2">0.68 0.16 0.74</cell><cell>0.15</cell></row><row><cell>ncsu.abs</cell><cell cols="3">12942 12942 1073 378</cell><cell>0.12</cell><cell>0.16</cell><cell>4409</cell><cell>7695</cell><cell cols="2">0.61 0.11 0.71</cell><cell>0.33</cell></row><row><cell>ncsu.simple</cell><cell cols="3">27950 27950 1611 928</cell><cell>0.14</cell><cell>0.27</cell><cell>4145</cell><cell>6964</cell><cell cols="2">0.68 0.11 0.83</cell><cell>0.18</cell></row><row><cell>ntu.run1</cell><cell>111170</cell><cell>0</cell><cell>1795 3403</cell><cell>0.09</cell><cell>0.11</cell><cell>3936</cell><cell>4130</cell><cell cols="2">0.61 0.08 0.98</cell><cell>0.55</cell></row><row><cell>ntu.run2</cell><cell>111170</cell><cell>0</cell><cell>1795 3204</cell><cell>0.12</cell><cell>0.13</cell><cell>3936</cell><cell>4130</cell><cell cols="2">0.59 0.06 0.98</cell><cell>0.55</cell></row><row><cell>ntu.run3</cell><cell>111196</cell><cell>0</cell><cell>1795 3570</cell><cell>0.09</cell><cell>0.07</cell><cell>3937</cell><cell>4130</cell><cell cols="2">0.54 0.05 0.98</cell><cell>0.55</cell></row><row><cell cols="3">padua.iafa_m10k150f0m10 117557 2350</cell><cell>1857 2269</cell><cell>0.41</cell><cell>0.51</cell><cell>4075</cell><cell>4075</cell><cell cols="2">0.90 0.28 1.00</cell><cell>0.54</cell></row><row><cell cols="3">padua.iafap_m10p2f0m10 117557 2367</cell><cell>1857 2395</cell><cell>0.37</cell><cell>0.48</cell><cell>4076</cell><cell>4076</cell><cell cols="2">0.88 0.25 1.00</cell><cell>0.54</cell></row><row><cell cols="3">padua.iafap_m10p5f0m10 117557 5893</cell><cell>1857 2260</cell><cell>0.40</cell><cell>0.50</cell><cell>4311</cell><cell>4311</cell><cell cols="2">0.89 0.27 1.00</cell><cell>0.54</cell></row><row><cell cols="3">padua.iafas_m10k50f0m10 117557 4320</cell><cell>1857 2304</cell><cell>0.41</cell><cell>0.52</cell><cell>4206</cell><cell>4206</cell><cell cols="2">0.89 0.27 1.00</cell><cell>0.54</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="26,184.63,498.60,435.97,7.93"><head>Table 7 .</head><label>7</label><figDesc>PART I: Evaluation results for submitted runs computed using abstract-level relevance judgments</figDesc><table coords="27,152.98,162.54,499.28,271.33"><row><cell>Run</cell><cell cols="10">Docs Feedback Rel Last wss@100 wss@95 Cost w/ Cost w/ Area AP Recall@ Reliability</cell></row><row><cell></cell><cell>Shown</cell><cell></cell><cell>Docs Rank</cell><cell></cell><cell></cell><cell cols="3">Uniform Weighted Under</cell><cell>Thresh</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Found Rel</cell><cell></cell><cell></cell><cell cols="3">Penalty Penalty Recall</cell><cell></cell></row><row><cell>qut.ca_bool_ltr</cell><cell>117557</cell><cell>0</cell><cell>1857 3142</cell><cell>0.20</cell><cell>0.29</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.73 0.11 1.00</cell><cell>0.54</cell></row><row><cell>qut.ca_pico_ltr</cell><cell>117557</cell><cell>0</cell><cell>1857 3344</cell><cell>0.21</cell><cell>0.29</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.75 0.15 1.00</cell><cell>0.54</cell></row><row><cell>qut.rf_bool_ltr</cell><cell>117557</cell><cell>0</cell><cell>1857 3099</cell><cell>0.19</cell><cell>0.27</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.70 0.11 1.00</cell><cell>0.54</cell></row><row><cell>qut.fr_pico_ltr</cell><cell>117557</cell><cell>0</cell><cell>1857 3155</cell><cell>0.23</cell><cell>0.29</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.73 0.12 1.00</cell><cell>0.54</cell></row><row><cell>qut.bool_es_test</cell><cell>69951</cell><cell>0</cell><cell>1475 1972</cell><cell>0.10</cell><cell>0.11</cell><cell>3480</cell><cell>4976</cell><cell cols="2">0.64 0.13 0.76</cell><cell>0.36</cell></row><row><cell>qut.pico_es_test</cell><cell>63018</cell><cell>0</cell><cell>1414 1873</cell><cell>0.11</cell><cell>0.13</cell><cell>3527</cell><cell>5168</cell><cell cols="2">0.62 0.12 0.74</cell><cell>0.34</cell></row><row><cell>sheffield.run1</cell><cell>117562</cell><cell>0</cell><cell>1857 2678</cell><cell>0.31</cell><cell>0.42</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.82 0.17 1.00</cell><cell>0.54</cell></row><row><cell>sheffield.run2</cell><cell>117562</cell><cell>0</cell><cell>1857 2441</cell><cell>0.39</cell><cell>0.49</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.84 0.22 1.00</cell><cell>0.54</cell></row><row><cell>sheffield.run3</cell><cell>117562</cell><cell>0</cell><cell>1857 2404</cell><cell>0.38</cell><cell>0.47</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.84 0.20 1.00</cell><cell>0.54</cell></row><row><cell>sheffield.run4</cell><cell>117562</cell><cell>0</cell><cell>1857 2382</cell><cell>0.40</cell><cell>0.49</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.85 0.22 1.00</cell><cell>0.54</cell></row><row><cell>ucl.run_abstract</cell><cell>117562</cell><cell>0</cell><cell>1857 3727</cell><cell>0.04</cell><cell>0.03</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.48 0.04 1.00</cell><cell>0.54</cell></row><row><cell>ucl.run_fulltext</cell><cell>117562</cell><cell>0</cell><cell>1857 3727</cell><cell>0.04</cell><cell>0.03</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.48 0.04 1.00</cell><cell>0.54</cell></row><row><cell>uos.bm25_threshold1</cell><cell>103051</cell><cell>0</cell><cell>1828 2503</cell><cell>0.28</cell><cell>0.40</cell><cell>3454</cell><cell>3786</cell><cell cols="2">0.81 0.17 0.99</cell><cell>0.45</cell></row><row><cell>uos.bm25_threshold2.5</cell><cell>76104</cell><cell>0</cell><cell>1758 1877</cell><cell>0.22</cell><cell>0.35</cell><cell>2905</cell><cell>3902</cell><cell cols="2">0.79 0.17 0.94</cell><cell>0.27</cell></row><row><cell>uos.bm25_threshold2</cell><cell>84740</cell><cell>0</cell><cell>1784 2068</cell><cell>0.23</cell><cell>0.37</cell><cell>3117</cell><cell>3748</cell><cell cols="2">0.80 0.17 0.95</cell><cell>0.33</cell></row><row><cell>uos.sis.AL30Q_BM25</cell><cell>94967</cell><cell>0</cell><cell>1809 2333</cell><cell>0.27</cell><cell>0.39</cell><cell>3280</cell><cell>3865</cell><cell cols="2">0.80 0.17 0.97</cell><cell>0.38</cell></row><row><cell cols="4">uos.sis.TMAL30Q_BM25 117551 35432 1857 2305</cell><cell>0.40</cell><cell>0.53</cell><cell>6280</cell><cell>6280</cell><cell cols="2">0.84 0.16 1.00</cell><cell>0.54</cell></row><row><cell cols="2">uos.sis.TMBEST_BM25 117557</cell><cell>0</cell><cell>1857 3124</cell><cell>0.27</cell><cell>0.32</cell><cell>3918</cell><cell>3918</cell><cell cols="2">0.73 0.12 1.00</cell><cell>0.54</cell></row><row><cell cols="4">waterloo.A-rank-normal 117558 117558 1857 1464</cell><cell>0.60</cell><cell>0.70</cell><cell>11755</cell><cell>11755</cell><cell cols="2">0.93 0.28 1.00</cell><cell>0.54</cell></row><row><cell cols="4">waterloo.A-thresh-normal 87767 87767 1842 1161</cell><cell>0.56</cell><cell>0.70</cell><cell>8809</cell><cell>9543</cell><cell cols="2">0.93 0.28 1.00</cell><cell>0.50</cell></row><row><cell cols="4">waterloo.B-rank-normal 117558 117558 1857 1469</cell><cell>0.61</cell><cell>0.70</cell><cell>11755</cell><cell>11755</cell><cell cols="2">0.93 0.32 1.00</cell><cell>0.54</cell></row><row><cell cols="4">waterloo.B-thresh-normal 60936 60936 1548 914</cell><cell>0.54</cell><cell>0.66</cell><cell>6470</cell><cell>7150</cell><cell cols="2">0.91 0.31 0.97</cell><cell>0.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="27,181.56,436.81,442.12,7.93"><head>Table 8 .</head><label>8</label><figDesc>PART II: Evaluation results for submitted runs computed using abstract-level relevance judgments.</figDesc><table coords="28,145.75,136.66,510.67,359.00"><row><cell>Run</cell><cell cols="10">Docs Feedback Rel Last wss@100 wss@95 Cost w/ Cost w/ Area AP Recall@ Reliability</cell></row><row><cell></cell><cell>Shown</cell><cell></cell><cell>Docs Rank</cell><cell></cell><cell></cell><cell cols="3">Uniform Weighted Under</cell><cell>Thresh</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Found Rel</cell><cell></cell><cell></cell><cell cols="3">Penalty Penalty Recall</cell><cell></cell></row><row><cell>amc.run</cell><cell>109547</cell><cell>0</cell><cell>607 1742</cell><cell>0.51</cell><cell>0.51</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.84 0.10 1.00</cell><cell>0.74</cell></row><row><cell>auth.simple.run1</cell><cell cols="2">109559 39337</cell><cell>607 853</cell><cell>0.80</cell><cell>0.82</cell><cell>6490</cell><cell>6490</cell><cell cols="2">0.95 0.23 1.00</cell><cell>0.74</cell></row><row><cell>auth.simple.run2</cell><cell cols="2">109559 39377</cell><cell>607 857</cell><cell>0.79</cell><cell>0.81</cell><cell>6493</cell><cell>6493</cell><cell cols="2">0.94 0.21 1.00</cell><cell>0.74</cell></row><row><cell>auth.simple.run3</cell><cell cols="2">109559 22337</cell><cell>607 839</cell><cell>0.80</cell><cell>0.82</cell><cell>5318</cell><cell>5318</cell><cell cols="2">0.95 0.22 1.00</cell><cell>0.74</cell></row><row><cell>auth.simple.run4</cell><cell cols="2">109559 39537</cell><cell>607 858</cell><cell>0.79</cell><cell>0.81</cell><cell>6504</cell><cell>6504</cell><cell cols="2">0.94 0.21 1.00</cell><cell>0.74</cell></row><row><cell>BASELINE.BM25</cell><cell>109549</cell><cell>0</cell><cell>607 1664</cell><cell>0.54</cell><cell>0.57</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.85 0.14 1.00</cell><cell>0.74</cell></row><row><cell cols="2">BASELINE.pubmed.random 109560</cell><cell>0</cell><cell>607 3316</cell><cell>0.09</cell><cell>0.07</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.48 0.02 1.00</cell><cell>0.74</cell></row><row><cell>cnrs.abrupt.all</cell><cell cols="2">109555 19980</cell><cell>607 2619</cell><cell>0.35</cell><cell>0.39</cell><cell>5155</cell><cell>5155</cell><cell cols="2">0.80 0.11 1.00</cell><cell>0.74</cell></row><row><cell>cnrs.gradual.all</cell><cell cols="2">109555 22684</cell><cell>607 2384</cell><cell>0.41</cell><cell>0.46</cell><cell>5342</cell><cell>5342</cell><cell cols="2">0.77 0.11 1.00</cell><cell>0.74</cell></row><row><cell>cnrs.noaf.all</cell><cell>109555</cell><cell>0</cell><cell>607 2263</cell><cell>0.42</cell><cell>0.50</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.82 0.10 1.00</cell><cell>0.74</cell></row><row><cell>cnrs.noaffull.all</cell><cell>109555</cell><cell>0</cell><cell>607 1678</cell><cell>0.59</cell><cell>0.64</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.89 0.13 1.00</cell><cell>0.74</cell></row><row><cell>ecnu.run1</cell><cell>109559</cell><cell>0</cell><cell>607 2905</cell><cell>0.26</cell><cell>0.27</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.66 0.06 1.00</cell><cell>0.74</cell></row><row><cell>ecnu.run2</cell><cell>29000</cell><cell>0</cell><cell>426 515</cell><cell>0.29</cell><cell>0.30</cell><cell>3069</cell><cell>5286</cell><cell cols="2">0.72 0.12 0.79</cell><cell>0.49</cell></row><row><cell>ecnu.run3</cell><cell>29000</cell><cell>0</cell><cell>426 486</cell><cell>0.30</cell><cell>0.31</cell><cell>3069</cell><cell>5286</cell><cell cols="2">0.73 0.12 0.79</cell><cell>0.49</cell></row><row><cell>eth.m1</cell><cell>47500</cell><cell>0</cell><cell>561 1000</cell><cell>0.44</cell><cell>0.58</cell><cell>1876</cell><cell>2170</cell><cell cols="2">0.86 0.16 0.97</cell><cell>0.24</cell></row><row><cell>eth.m2</cell><cell>46538</cell><cell>4662</cell><cell>553 1050</cell><cell>0.42</cell><cell>0.55</cell><cell>2196</cell><cell>2489</cell><cell cols="2">0.84 0.15 0.93</cell><cell>0.18</cell></row><row><cell>eth.m4</cell><cell>25381</cell><cell>0</cell><cell>476 596</cell><cell>0.31</cell><cell>0.36</cell><cell>1850</cell><cell>3739</cell><cell cols="2">0.80 0.15 0.87</cell><cell>0.15</cell></row><row><cell>iiit.run1</cell><cell cols="2">15234 15234</cell><cell>406 501</cell><cell>0.15</cell><cell>0.19</cell><cell>3583</cell><cell>5094</cell><cell cols="2">0.70 0.12 0.77</cell><cell>0.18</cell></row><row><cell>iiit.run2</cell><cell cols="2">15234 15234</cell><cell>406 501</cell><cell>0.15</cell><cell>0.19</cell><cell>3583</cell><cell>5094</cell><cell cols="2">0.70 0.12 0.77</cell><cell>0.18</cell></row><row><cell>iiit.run3</cell><cell cols="2">15234 15234</cell><cell>406 501</cell><cell>0.15</cell><cell>0.19</cell><cell>3583</cell><cell>5094</cell><cell cols="2">0.70 0.12 0.77</cell><cell>0.18</cell></row><row><cell>iiit.run4</cell><cell cols="2">15234 15234</cell><cell>406 501</cell><cell>0.15</cell><cell>0.19</cell><cell>3583</cell><cell>5094</cell><cell cols="2">0.70 0.12 0.77</cell><cell>0.18</cell></row><row><cell>ncsu.abs</cell><cell cols="2">12682 12682</cell><cell>480 356</cell><cell>0.35</cell><cell>0.38</cell><cell>3354</cell><cell>5978</cell><cell cols="2">0.69 0.07 0.81</cell><cell>0.31</cell></row><row><cell>ncsu.simple</cell><cell cols="2">27950 27950</cell><cell>607 960</cell><cell>0.66</cell><cell>0.66</cell><cell>2891</cell><cell>2891</cell><cell cols="2">0.80 0.06 1.00</cell><cell>0.13</cell></row><row><cell>ntu.run1</cell><cell>103170</cell><cell>0</cell><cell>606 2954</cell><cell>0.20</cell><cell>0.22</cell><cell>3606</cell><cell>3557</cell><cell cols="2">0.65 0.05 1.00</cell><cell>0.72</cell></row><row><cell>ntu.run2</cell><cell>103170</cell><cell>0</cell><cell>606 2779</cell><cell>0.20</cell><cell>0.20</cell><cell>3606</cell><cell>3557</cell><cell cols="2">0.62 0.04 1.00</cell><cell>0.72</cell></row><row><cell>ntu.run3</cell><cell>103194</cell><cell>0</cell><cell>606 3050</cell><cell>0.15</cell><cell>0.14</cell><cell>3607</cell><cell>3558</cell><cell cols="2">0.55 0.02 1.00</cell><cell>0.72</cell></row><row><cell cols="3">padua.iafa_m10k150f0m10 109555 2286</cell><cell>607 1055</cell><cell>0.71</cell><cell>0.71</cell><cell>3935</cell><cell>3935</cell><cell cols="2">0.93 0.22 1.00</cell><cell>0.74</cell></row><row><cell cols="3">padua.iafap_m10p2f0m10 109555 2206</cell><cell>607 1007</cell><cell>0.66</cell><cell>0.69</cell><cell>3929</cell><cell>3929</cell><cell cols="2">0.92 0.20 1.00</cell><cell>0.74</cell></row><row><cell cols="3">padua.iafap_m10p5f0m10 109555 5492</cell><cell>607 838</cell><cell>0.71</cell><cell>0.70</cell><cell>4156</cell><cell>4156</cell><cell cols="2">0.93 0.21 1.00</cell><cell>0.74</cell></row><row><cell cols="3">padua.iafas_m10k50f0m10 109555 4170</cell><cell>607 990</cell><cell>0.71</cell><cell>0.72</cell><cell>4065</cell><cell>4065</cell><cell cols="2">0.93 0.19 1.00</cell><cell>0.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="28,180.06,499.32,445.12,7.93"><head>Table 9 .</head><label>9</label><figDesc>PART I: Evaluation results for submitted runs computed using document-level relevance judgments.</figDesc><table coords="29,151.45,162.18,499.28,271.33"><row><cell>Run</cell><cell cols="10">Docs Feedback Rel Last wss@100 wss@95 Cost w/ Cost w/ Area AP Recall@ Reliability</cell></row><row><cell></cell><cell>Shown</cell><cell></cell><cell>Docs Rank</cell><cell></cell><cell></cell><cell cols="3">Uniform Weighted Under</cell><cell>Thresh</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Found Rel</cell><cell></cell><cell></cell><cell cols="3">Penalty Penalty Recall</cell><cell></cell></row><row><cell>qut.ca_bool_ltr</cell><cell>109555</cell><cell>0</cell><cell>607 2582</cell><cell>0.33</cell><cell>0.36</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.75 0.08 1.00</cell><cell>0.74</cell></row><row><cell>qut.ca_pico_ltr</cell><cell>109555</cell><cell>0</cell><cell>607 2638</cell><cell>0.36</cell><cell>0.40</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.78 0.11 1.00</cell><cell>0.74</cell></row><row><cell>qut.rf_bool_ltr</cell><cell>109555</cell><cell>0</cell><cell>607 2477</cell><cell>0.33</cell><cell>0.35</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.72 0.06 1.00</cell><cell>0.74</cell></row><row><cell>qut.rf_pico_ltr</cell><cell>109555</cell><cell>0</cell><cell>607 2610</cell><cell>0.35</cell><cell>0.38</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.76 0.09 1.00</cell><cell>0.74</cell></row><row><cell>qut.bool_es</cell><cell>65389</cell><cell>0</cell><cell>465 1595</cell><cell>0.22</cell><cell>0.25</cell><cell>3217</cell><cell>4041</cell><cell cols="2">0.68 0.10 0.81</cell><cell>0.43</cell></row><row><cell>qut.pico_es</cell><cell>58456</cell><cell>0</cell><cell>451 1424</cell><cell>0.21</cell><cell>0.23</cell><cell>3251</cell><cell>4519</cell><cell cols="2">0.67 0.09 0.78</cell><cell>0.40</cell></row><row><cell>sheffield.run1</cell><cell>109560</cell><cell>0</cell><cell>607 1801</cell><cell>0.52</cell><cell>0.54</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.84 0.12 1.00</cell><cell>0.74</cell></row><row><cell>sheffield.run2</cell><cell>109560</cell><cell>0</cell><cell>607 1928</cell><cell>0.53</cell><cell>0.58</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.87 0.18 1.00</cell><cell>0.74</cell></row><row><cell>sheffield.run3</cell><cell>109560</cell><cell>0</cell><cell>607 1902</cell><cell>0.52</cell><cell>0.59</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.87 0.15 1.00</cell><cell>0.74</cell></row><row><cell>sheffield.run4</cell><cell>109560</cell><cell>0</cell><cell>607 1846</cell><cell>0.54</cell><cell>0.59</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.87 0.18 1.00</cell><cell>0.74</cell></row><row><cell>ucl.run_abstract</cell><cell>109560</cell><cell>0</cell><cell>607 3472</cell><cell>0.13</cell><cell>0.12</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.51 0.04 1.00</cell><cell>0.74</cell></row><row><cell>ucl.run_fulltext</cell><cell>109560</cell><cell>0</cell><cell>607 3505</cell><cell>0.14</cell><cell>0.12</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.51 0.04 1.00</cell><cell>0.74</cell></row><row><cell>uos.bm25_threshold1</cell><cell>95721</cell><cell>0</cell><cell>601 1548</cell><cell>0.53</cell><cell>0.57</cell><cell>3311</cell><cell>3406</cell><cell cols="2">0.85 0.14 0.99</cell><cell>0.59</cell></row><row><cell>uos.bm25_threshold2.5</cell><cell>73548</cell><cell>0</cell><cell>591 1483</cell><cell>0.52</cell><cell>0.56</cell><cell>2580</cell><cell>2926</cell><cell cols="2">0.85 0.13 0.98</cell><cell>0.36</cell></row><row><cell>uos.bm25_threshold2</cell><cell>80976</cell><cell>0</cell><cell>597 1506</cell><cell>0.53</cell><cell>0.57</cell><cell>2820</cell><cell>3037</cell><cell cols="2">0.85 0.14 0.99</cell><cell>0.45</cell></row><row><cell>uos.sis.AL30Q_BM25</cell><cell cols="2">109549 33300</cell><cell>607 906</cell><cell>0.68</cell><cell>0.69</cell><cell>6074</cell><cell>6074</cell><cell cols="2">0.90 0.16 1.00</cell><cell>0.74</cell></row><row><cell cols="3">uos.sis.TMAL30Q_BM25 109550 33002</cell><cell>607 1228</cell><cell>0.65</cell><cell>0.67</cell><cell>6053</cell><cell>6053</cell><cell cols="2">0.87 0.11 1.00</cell><cell>0.74</cell></row><row><cell cols="2">uos.sis.TMBEST_BM25 109555</cell><cell>0</cell><cell>607 1980</cell><cell>0.50</cell><cell>0.49</cell><cell>3777</cell><cell>3777</cell><cell cols="2">0.76 0.09 1.00</cell><cell>0.74</cell></row><row><cell cols="4">waterloo.A-rank-normal 109556 109556 607 461</cell><cell>0.82</cell><cell>0.81</cell><cell>11333</cell><cell>11333</cell><cell cols="2">0.95 0.19 1.00</cell><cell>0.74</cell></row><row><cell cols="3">waterloo.A-thresh-normal 79765 79765</cell><cell>607 461</cell><cell>0.82</cell><cell>0.81</cell><cell>8251</cell><cell>8251</cell><cell cols="2">0.95 0.19 1.00</cell><cell>0.66</cell></row><row><cell cols="4">waterloo.B-rank-normal 109556 109556 607 469</cell><cell>0.83</cell><cell>0.82</cell><cell>11333</cell><cell>11333</cell><cell cols="2">0.95 0.23 1.00</cell><cell>0.74</cell></row><row><cell cols="3">waterloo.B-thresh-normal 52934 52934</cell><cell>575 375</cell><cell>0.78</cell><cell>0.77</cell><cell>5765</cell><cell>6559</cell><cell cols="2">0.94 0.23 0.98</cell><cell>0.53</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="29,176.90,437.17,451.45,7.93"><head>Table 10 .</head><label>10</label><figDesc>PART II: Evaluation results for submitted runs computed using document-level relevance judgments</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,656.74,136.47,7.92"><p>http://www.cochranelibrary.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="3,144.73,634.83,221.76,7.92"><p>http://demo.ovid.com/demo/ovidsptools/launcher.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="3,144.73,645.79,183.32,7.92"><p>https://github.com/dli1/tar_data_collection</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="3,144.73,656.74,202.40,7.92"><p>https://www.ncbi.nlm.nih.gov/books/NBK25497/</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>10.1002/14651858.CD010438.pub2/full 10.1002/14651858.CD009551.pub3/full 10.1002/14651858.CD010775.pub2/full 10.1002/14651858.CD012019/full 10.1002/14651858.CD009175.pub2/full 10.1002/14651858.CD008686.pub2/full 10.1002/14651858.CD011984/full 10.1002/14651858.CD009020.pub2/full 10.1002/14651858.CD009786.pub2/full 10.1002/14651858.CD011548/full 10.1002/14651858.CD008643.pub2/full 10.1002/14651858.CD010896.pub2/full 10.1002/14651858.CD009579.pub2/full 10.1002/14651858.CD010023.pub2/full 10.1002/14651858.CD009925/full 10.1002/14651858.CD010772.pub2/full 10.1002/14651858.CD009944.pub2/full 10.1002/14651858.CD011145.pub2/full 10.1002/14651858.CD007431.pub2/full 10.1002/14651858.CD010409.pub2/full 10.1002/14651858.CD007427.pub2/full 10.1002/14651858.CD008054.pub2/full 10.1002/14651858.CD008803.pub2/full 10.1002/14651858.CD010771.pub2/full 10.1002/14651858.CD008122.pub2/full 10.1002/14651858.CD009694.pub2/full 10.1002/14651858.CD009593.pub3/full 10.1002/14651858.CD010705.pub2/full 10.1002/14651858.CD008782.pub4/full 10.1002/14651858.CD010633.pub2/full 10.1002/14651858.CD009647.pub2/full 10.1002/14651858.CD010173.pub2/full 10.1002/14651858.CD009135.pub2/full 10.1002/14651858.CD009591.pub2/full 10.1002/14651858.CD008760.pub2/full 10.1002/14651858.CD010386.pub2/full 10.1002/14651858.CD011549/full 10.1002/14651858.CD011021.pub2/full 10.1002/14651858.CD009263.pub2/full 10.1002/14651858.CD008691.pub2/full 10.1002/14651858.CD009519.pub2/full 10.1002/14651858.CD010632.pub2/full 10.1002/14651858.CD009372.pub2/full 10.1002/14651858.CD007394.pub2/full 10.1002/14651858.CD011134.pub2/full 10.1002/14651858.CD010783.pub2/full 10.1002/14651858.CD010079.pub2/full 10.1002/14651858.CD010860.pub2/full 10.1002/14651858.CD010276.pub2/full 10.1002/14651858.CD007424.pub2/full 10.1002/14651858.CD008081.pub3/full 10.1002/14651858.CD011431/full 10.1002/14651858.CD009185.pub2/full 10.1002/14651858.CD010339.pub2/full 10.1002/14651858.CD011975/full 10.1002/14651858.CD010653.pub2/full 10.1002/14651858.CD009323.pub2/full 10.1002/14651858.CD010542.pub2/full</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="22,142.95,591.12,337.64,7.92;22,151.52,602.08,329.08,7.92;22,151.52,613.04,329.08,7.92;22,151.52,624.00,329.07,7.92;22,151.52,634.96,88.31,7.92" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="22,269.61,591.12,210.99,7.92;22,151.52,602.08,329.08,7.92;22,151.52,613.04,3.58,7.92">Ranking abstracts to identify relevant evidence for systematic reviews: The university of sheffield&apos;s approach to clef ehealth 2017 task 2</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alharbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stevenson</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="22,178.70,613.04,301.90,7.92;22,151.52,624.00,21.98,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="22,353.96,624.00,126.63,7.92;22,151.52,634.96,21.69,7.92">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.95,645.79,337.64,7.92;22,151.52,656.74,329.08,7.92;23,151.52,119.62,329.07,7.92;23,151.52,130.58,239.06,7.92" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="22,223.94,645.79,256.66,7.92;22,151.52,656.74,189.19,7.92">Predicting publication inclusion for diagnostic accuracy test reviews using random forests and topic modelling</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Altena</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="22,361.64,656.74,118.95,7.92;23,151.52,119.62,186.09,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="23,175.57,130.58,148.39,7.92">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.95,141.55,337.64,7.92;23,151.52,152.51,329.07,7.92;23,151.52,163.47,329.07,7.92;23,151.52,174.43,311.78,7.92" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="23,415.50,141.55,65.09,7.92;23,151.52,152.51,243.01,7.92">Hybridranksvm: A cost-effective hybrid ltr approach for document ranking</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Anagnostou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lagopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="23,419.02,152.51,61.57,7.92;23,151.52,163.47,255.92,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="23,248.29,174.43,148.39,7.92">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.95,185.40,337.64,7.92;23,151.52,196.36,329.07,7.92;23,151.52,207.31,329.07,7.92;23,151.52,218.27,329.08,7.92" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="23,425.64,185.40,54.96,7.92;23,151.52,196.36,274.51,7.92">Ecnu at 2017 ehealth task 2: Technologically assisted reviews in empirical medicine</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="23,446.53,196.36,34.06,7.92;23,151.52,207.31,274.32,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="23,268.45,218.27,146.23,7.92">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.95,229.24,337.64,7.92;23,151.52,240.20,329.07,7.92;23,151.52,251.16,261.75,7.92" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="23,371.94,229.24,108.66,7.92;23,151.52,240.20,265.71,7.92">Reducing workload in systematic review preparation using automated citation classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">Y</forename><surname>Yen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,423.86,240.20,56.73,7.92;23,151.52,251.16,171.15,7.92">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="206" to="219" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.95,262.13,337.63,7.92;23,151.52,273.09,329.07,7.92;23,151.52,284.05,329.08,7.92;23,151.52,295.01,329.07,8.17;23,151.52,306.67,32.95,7.47" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="23,286.48,262.13,194.11,7.92;23,151.52,273.09,59.53,7.92">Engineering quality and reliability in technologyassisted review</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<idno type="DOI">10.1145/2911451.2911510</idno>
		<ptr target="http://doi.acm.org/10.1145/2911451.2911510" />
	</analytic>
	<monogr>
		<title level="m" coord="23,234.67,273.09,245.92,7.92;23,151.52,284.05,248.81,7.92;23,454.81,284.05,25.78,7.92;23,151.52,295.01,10.75,7.92">Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;16</note>
</biblStruct>

<biblStruct coords="23,142.95,316.94,337.64,7.92;23,151.52,327.90,329.07,7.92;23,151.52,338.86,329.07,7.92;23,151.52,349.82,266.20,7.92" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="23,307.94,316.94,172.64,7.92;23,151.52,327.90,210.58,7.92">Technology-assisted review in empirical medicine: Waterloo participation in clef ehealth 2017</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="23,383.01,327.90,97.58,7.92;23,151.52,338.86,211.35,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="23,202.70,349.82,148.38,7.92">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.95,360.79,337.63,7.92;23,151.52,371.74,329.07,7.92;23,151.52,382.70,329.07,7.92;23,151.52,393.66,221.10,7.92" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="23,284.61,371.74,177.49,7.92">CLEF 2017 eHealth evaluation lab overview</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Spijker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,151.52,382.70,254.63,7.92">CLEF 2017 -8th Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="23,413.87,382.70,66.72,7.92;23,151.52,393.66,100.19,7.92">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-09">September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.95,404.63,337.63,7.92;23,151.52,415.59,329.07,7.92;23,151.52,426.55,329.08,7.92;23,151.52,437.51,142.00,7.92" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="23,269.89,404.63,210.70,7.92;23,151.52,415.59,75.38,7.92">Relevance-based stopping for recall-centric medical document retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hollmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="23,249.27,415.59,231.32,7.92;23,151.52,426.55,85.06,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="23,410.28,426.55,70.31,7.92;23,151.52,437.51,75.38,7.92">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.61,448.48,337.98,7.92;23,151.52,459.44,329.07,8.17;23,151.52,471.10,61.18,7.47" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="23,272.48,448.48,204.06,7.92">Cumulated gain-based evaluation of ir techniques</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
		<idno type="DOI">10.1145/582415.582418</idno>
		<ptr target="http://doi.acm.org/10.1145/582415.582418" />
	</analytic>
	<monogr>
		<title level="j" coord="23,151.52,459.44,89.16,7.92">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002-10">Oct 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.61,481.37,337.98,7.92;23,151.52,492.33,329.07,7.92;23,151.52,503.29,329.07,7.92;23,151.52,514.24,58.62,7.92" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="23,332.93,481.37,129.24,7.92">Sis at clef 2017 ehealth tar task</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kalphov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Georgiadis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="23,151.52,492.33,324.68,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="23,320.94,503.29,153.72,7.92">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.61,525.22,337.98,7.92;23,151.52,536.17,329.08,7.92;23,151.52,547.13,329.07,7.92;23,151.52,558.09,266.20,7.92" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="23,196.38,525.22,284.21,7.92;23,151.52,536.17,211.82,7.92">Medical document classification for systematic reviews using convolutional neural networks: Sysreview at clef ehealth 2017</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="23,383.76,536.17,96.84,7.92;23,151.52,547.13,211.35,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="23,202.70,558.09,148.38,7.92">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.61,569.06,337.98,7.92;23,151.52,580.02,329.08,7.92;23,151.52,590.98,329.07,7.92;23,151.52,601.94,215.01,7.92" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="23,314.15,569.06,166.44,7.92;23,151.52,580.02,158.02,7.92">Limsi@clef ehealth 2017 task 2: Logistic regression for automatic article ranking</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Leeflang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neveol</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="23,331.16,580.02,149.43,7.92;23,151.52,590.98,163.06,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="23,151.52,601.94,148.39,7.92">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.61,612.91,337.98,7.92;23,151.52,623.87,329.07,7.92;23,151.52,634.83,329.07,7.92;23,151.52,645.79,329.08,7.92;23,151.52,656.74,142.00,7.92" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="23,396.51,612.91,84.08,7.92;23,151.52,623.87,329.07,7.92;23,151.52,634.83,83.76,7.92">An interactive twodimensional approach to query aspects rewriting in systematic reviews. ims unipd at clef ehealth task 2</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M D</forename><surname>Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Beghini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Henrot</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="23,256.15,634.83,224.44,7.92;23,151.52,645.79,85.06,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="23,410.28,645.79,70.31,7.92;23,151.52,656.74,75.38,7.92">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,119.62,337.98,7.92;24,151.52,130.58,329.07,7.92;24,151.52,141.54,187.80,7.92" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="24,439.02,119.62,41.57,7.92;24,151.52,130.58,329.07,7.92;24,151.52,141.54,42.64,7.92">Using text mining for study identification in systematic reviews: a systematic review of current approaches</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>O'mara-Eves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mcnaught</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,201.50,141.54,76.39,7.92">Systematic reviews</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,152.50,337.98,7.92;24,151.52,163.46,329.07,7.92;24,151.52,174.42,329.07,7.92;24,151.52,185.37,311.78,7.92" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="24,349.68,152.50,130.92,7.92;24,151.52,163.46,249.98,7.92">Qut ielab at clef 2017 technology assisted reviews track: Initial experiments with learning to rank</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Scells</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Deacon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="24,421.35,163.46,59.23,7.92;24,151.52,174.42,255.92,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="24,248.29,185.37,148.39,7.92">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,196.33,337.98,7.92;24,151.52,207.29,329.08,7.92;24,151.52,218.25,329.08,7.92;24,151.52,229.21,263.99,7.92" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="24,455.51,196.33,25.08,7.92;24,151.52,207.29,325.07,7.92">A test collection for evaluating retrieval of studies for inclusion in systematic reviews</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Scells</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Deacon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,220.04,218.25,260.55,7.92;24,151.52,229.21,206.62,7.92">Proceedings of the 40th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 40th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,240.17,337.98,7.92;24,151.52,251.13,329.07,7.92;24,151.52,262.09,153.23,7.92" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="24,335.99,240.17,144.60,7.92;24,151.52,251.13,305.29,7.92">Use of cost-effectiveness analysis to compare the efficiency of study identification methods in systematic reviews</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Shemilt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,463.91,251.13,16.69,7.92;24,151.52,262.09,65.96,7.92">Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">140</biblScope>
			<date type="published" when="2016-08">Aug 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,273.05,337.98,7.92;24,151.52,284.00,329.07,7.92;24,151.52,294.96,329.07,7.92;24,151.52,305.92,215.01,7.92" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="24,351.03,273.05,129.56,7.92;24,151.52,284.00,148.01,7.92">Identifying diagnostic test accuracy publications using a deep model</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wallace</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="24,321.29,284.00,159.31,7.92;24,151.52,294.96,158.28,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="24,151.52,305.92,148.39,7.92">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.61,316.88,337.98,7.92;24,151.52,327.84,329.07,7.92;24,151.52,338.80,329.07,7.92;24,151.52,349.76,142.00,7.92" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="24,235.98,316.88,244.60,7.92;24,151.52,327.84,96.87,7.92">Technologically assisted reviews in empirical medicine: Data balancing or reweighting</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Menzies</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="24,269.12,327.84,211.47,7.92;24,151.52,338.80,92.67,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="24,411.37,338.80,69.23,7.92;24,151.52,349.76,75.38,7.92">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
