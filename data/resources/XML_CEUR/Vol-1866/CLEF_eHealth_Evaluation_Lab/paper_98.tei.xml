<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.83,116.95,323.69,12.62;1,151.07,134.89,313.22,12.62;1,225.27,152.82,164.82,12.62">QUT ielab at CLEF eHealth 2017 Technology Assisted Reviews Track: Initial Experiments with Learning To Rank</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,154.44,190.49,64.09,8.74"><forename type="first">Harrisen</forename><surname>Scells</surname></persName>
							<email>harrisen.scells@hdr.qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.09,190.49,60.96,8.74"><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
							<email>g.zuccon@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.60,190.49,72.92,8.74"><forename type="first">Anthony</forename><surname>Deacon</surname></persName>
							<email>aj.deacon@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,384.08,190.49,72.37,8.74"><forename type="first">Bevan</forename><surname>Koopman</surname></persName>
							<email>bevan.koopman@csiro.au</email>
							<affiliation key="aff1">
								<orgName type="department">Australian E-Health Research Centre</orgName>
								<orgName type="institution">CSIRO</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.83,116.95,323.69,12.62;1,151.07,134.89,313.22,12.62;1,225.27,152.82,164.82,12.62">QUT ielab at CLEF eHealth 2017 Technology Assisted Reviews Track: Initial Experiments with Learning To Rank</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5AF83C280EE970A13F609F926D370C61</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our participation to the CLEF eHealth 2017 Technology Assisted Reviews track (TAR). This track aims to evaluate and advance search technologies aimed at supporting the creation of biomedical systematic reviews. In this context, the track explores the task of screening prioritisation: the ranking of studies to be screened for inclusion in a systematic review. Our solution addresses this challenge by developing ranking strategies based on learning to rank techniques and exploiting features derived by the use of the PICO framework. PICO (Population, Intervention, Control or comparison and Outcome) is a technique used in evidence based practice to frame and answer clinical questions and is used extensively in the compilation of systematic reviews. Our experiments show that the use of the PICO-based feature within learning to rank provides improvements over the use of baseline features alone.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A systematic review is a type of literature review that appraises and synthesises the work of primary research studies to answer one or more research questions. Most authors follow the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method for conducting and reporting these reviews. This includes the definition of a formal search strategy to retrieve studies which are to be considered for inclusion in the review.</p><p>Given a research question and a set of inclusion/exclusion criteria, researchers undertaking a systematic review define a search strategy (the query) to be issued to one or more search engines that index published literature (e.g. PubMed). In medical and biomedical research, search strategies are commonly expressed as (large) boolean queries. After the search strategy has been executed, the title, and then abstract, of studies retrieved by it are reviewed in a process known as screening. Where the study appears relevant the full-text is then retrieved for more detailed examination.</p><p>The compilation of systematic reviews can take significant time and resources, hampering their effectiveness. Tsafnat et al. report that it can take several years to complete and publish a systematic review <ref type="bibr" coords="2,332.21,119.99,9.96,8.74" target="#b3">[4]</ref>. When systematic reviews take such significant time to complete, they can become out-of-date even at time of publishing. While the compilation of a systematic review involves several steps, one of the most time-consuming is screening. Thus, the development of IR methods that decrease the number of documents to be screened, would have a major impact on the time and resources required to undertake systematic reviews. Similarly, the ordering of studies to be screened according to the likelihood of satisfying the inclusion criteria of the systematic reviews (screening prioritisation) would allow relevant studies to be identified early on in the screening process, thus providing a feedback loop to improve the development of search strategies. Screening prioritisation is typically done as a two-stage process. An initial set of studies are retrieved using a boolean retrieval process; these are then ranked according to some relevance measure.</p><p>The challenge of compiling systematic reviews can be fertile ground for information retrieval (IR) research, as this can provide techniques to improve current screening and screening prioritisation processes. The CLEF eHealth 2017 Technology Assisted Reviews track (TAR) <ref type="bibr" coords="2,302.52,311.53,10.96,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,313.48,311.53,7.31,8.74" target="#b1">2]</ref> joins our recent work <ref type="bibr" coords="2,419.79,311.53,10.52,8.74" target="#b2">[3]</ref> in devising evaluation resources for evaluation of information retrieval techniques that attempt to automate and improve processes involved in the creation of systematic reviews. The TAR track considers two tasks: <ref type="bibr" coords="2,333.69,347.40,12.73,8.74" target="#b0">(1)</ref> to produce an the efficient ordering of studies retrieved by a boolean search strategy, such that all of the relevant abstracts are retrieved as early as possible, and (2) to identify a subset of the ranked studies which contains all or as many of the relevant abstracts for the least effort (i.e. total number of abstracts to be assessed). In our submissions, we tackle the first task, and use learning to rank to produce a re-ranking of the initial set of studies retrieved for screening by the systematic review's boolean search strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach for TAR</head><p>We trained a learning to rank model using domain specific features to provide an efficient ordering of studies retrieved by a systematic review. Specifically, we aim to observe what effect PICO features have with respect to learning to rank algorithms. PICO (Population, Intervention, Control or comparison and Outcome) is a technique used in evidence based practice to frame and answer clinical questions and is used extensively in the compilation of systematic reviews. We investigated several learning to rank algorithms and observed the effect queries annotated with PICO elements had on the reordering of results compared to the original Boolean queries.</p><p>We trained two learning to rank models using both the original queries provided by the task organiser, and another modified set of queries which contains annotations from the PICO framework. In total, we used seven features to train our learning to rank model. Table <ref type="table" coords="2,283.21,633.20,4.98,8.74" target="#tab_0">1</ref> summarises the features used. The first four features (IDFSum, IDFStd, IDFMax, and IDFAvg) calculate the inverse document frequency (idf) for each of the terms in the document that also appear in the query. IDFSum sum of all idf scores, IDFStd is the standard deviation of the idf scores, IDFMax is the maximum idf score and IDFAvg is the mean idf score. The other three features (PopulationCount, InterventionCount, and OutcomeCount) are the number of terms in the document and in the query that also appear in the respective PICO annotation. PICO annotations for documents were automatically extracted using RobotReviewer <ref type="bibr" coords="3,396.91,300.68,9.96,8.74" target="#b4">[5]</ref>. This automatic process only annotates the Population, Intervention, and Outcome for studies (the Control element is not annotated). PICO annotations for queries were manually collected by one of the team members, who is a clinician (AD). Afterwards, search strategies (both the original boolean query, and the new boolean query with PICO annotations) were manually transformed into Elasticsearch queries. The result is two Elasticsearch queries per topic -one which is representative of the original query made by the systematic review authors, and another annotated with PICO elements.</p><p>Initial testing on a recent collection we developed <ref type="bibr" coords="3,372.20,409.01,10.52,8.74" target="#b2">[3]</ref> allowed us to select a number of candidate learning to rank algorithms that may be effective in the screening prioritisation of systematic reviews.</p><p>We then empirically evaluated the selected five learning to rank algorithms listed in Table <ref type="table" coords="3,201.99,457.56,4.98,8.74" target="#tab_1">2</ref> and found that Coordinate Ascent provided us with the best MAP score on validation data for CLEF eHealth 2017 over the other models. Each time we trained a model, we used the default values for that model<ref type="foot" coords="3,476.12,479.89,3.97,6.12" target="#foot_0">1</ref> and we set aside the same 30% of queries for validation. Table <ref type="table" coords="3,421.30,493.42,4.98,8.74" target="#tab_1">2</ref> summarises the NDCG@10 and average precision (AP) scores for both the original Boolean queries and the annotated PICO queries. We found that Coordinate Ascent was the best algorithm for learning to rank these types of studies. Additionally, we found that Random Forests and MART methods both had similar levels of NCG@10 and AP.</p><p>Additionally, we also used Elasticsearch (version 5.3) to produce a re-ranking. We did this by issuing the Boolean and PICO query to Elasticsearch and limited the results to only the PubMed identifiers contained in the topic file for each query. We then let Elasticsearch rank these documents using BM25 with the default settings<ref type="foot" coords="3,201.57,612.13,3.97,6.12" target="#foot_1">2</ref> . We considered the Elasticsearch runs as our baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Analysis</head><p>We found that a learning to rank approach to re-ranking studies for systematic reviews shows promising results. Table <ref type="table" coords="4,335.83,319.06,4.98,8.74" target="#tab_1">2</ref> illustrates our submitted runs compared to the baseline Elasticsearch ranking and additional runs performed post-submission. The models trained using the search strategies annotated using PICO achieved slightly better results than the provided Boolean search strategies. None of our models were able to score higher than the baseline in NCG@10, however the Coordinate Ascent model trained using PICO annotations outperformed the baseline in AP. Additionally, we report AP, NCG@10, WSS@100 and the position of the last relevant document (last rel) in Figure <ref type="figure" coords="4,320.32,415.39,3.87,8.74" target="#fig_0">1</ref>. These visualisations show that the Coordinate Ascent model provides the most effective ranking of documents (in terms of AP and WSS@100) and scores the highest amongst the learning to rank models for recall based measurements (NCG@10). Figure <ref type="figure" coords="4,418.98,451.26,9.41,8.74" target="#fig_0">1c</ref> shows that learning to rank models trained on the Boolean search strategies positioned the last relevant document in the re-ranked list the highest; and that the baseline Elasticsearch runs do not do this as well.</p><p>Figure <ref type="figure" coords="4,182.22,499.78,4.98,8.74" target="#fig_1">2</ref> examines the effect PICO had on re-ranking. The effect appears negligible on the baseline, however, we notice an increase in precision when PICO annotations are used as training data for learning to rank models. This suggests that the use of PICO provides a trade off between precision and recall. Our results illustrate this clearly when precision-based measures are compared against recall-based measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Future Work</head><p>We plan to further increase the precision of our experiments by tuning the hyper parameters of the best performing learning to rank models. Our learning to rank models were trained using only a small number of features. We will investigate the effects of other features that are commonly used for learning to rank, and explore more domain specific features in addition to PICO.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,144.44,671.46,326.46,8.74"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Comparison of the effects each algorithm had on different measures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,134.77,515.96,345.82,8.74;6,134.77,527.92,65.69,8.74"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Precision-recall curves for the Elasticsearch baselines and the Coordinate Ascent models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,164.37,121.96,286.62,103.94"><head>Table 1 :</head><label>1</label><figDesc>Features used as training for our learning to rank model.</figDesc><table coords="3,265.19,121.96,84.99,89.58"><row><cell>Id Feature</cell></row><row><cell>1 IDFSum</cell></row><row><cell>2 IDFStd</cell></row><row><cell>3 IDFMax</cell></row><row><cell>4 IDFAvg</cell></row><row><cell>5 PopulationCount</cell></row><row><cell>6 InterventionCount</cell></row><row><cell>7 OutcomeCount</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,134.77,121.96,345.83,135.84"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of each learning to rank model using the features listed in Table 1 on the test data compared to the Elasticsearch baseline. Algorithms marked with * indicate our submitted runs.</figDesc><table coords="4,136.16,121.96,343.04,94.58"><row><cell></cell><cell>NCG@10</cell><cell></cell><cell>AP</cell><cell></cell></row><row><cell></cell><cell>Boolean</cell><cell>PICO</cell><cell>Boolean</cell><cell>PICO</cell></row><row><cell>Elasticsearch</cell><cell>0.397</cell><cell>0.409</cell><cell>0.104</cell><cell>0.102</cell></row><row><cell>MART</cell><cell>0.237</cell><cell>0.327</cell><cell>0.066</cell><cell>0.086</cell></row><row><cell>AdaRank</cell><cell>0.0875</cell><cell>0.2197</cell><cell>0.0255</cell><cell>0.0619</cell></row><row><cell>Coordinate Ascent*</cell><cell>0.305</cell><cell>0.378</cell><cell>0.076</cell><cell>0.114</cell></row><row><cell>LambdaMART</cell><cell>0.259</cell><cell>0.377</cell><cell>0.068</cell><cell>0.097</cell></row><row><cell>Random Forests*</cell><cell>0.247</cell><cell>0.275</cell><cell>0.061</cell><cell>0.088</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,635.88,335.86,8.12;3,144.73,647.48,254.20,7.47"><p>The default values for each model can be found at the following URL https:// sourceforge.net/p/lemur/wiki/RankLib%20How%20to%20use/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,657.79,73.82,7.86"><p>k1 = 1.2, b = 0.75.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,138.35,143.58,342.25,7.86;5,146.91,154.54,333.68,7.86;5,146.91,165.50,333.68,7.86;5,146.91,176.46,132.01,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,258.72,154.54,175.46,7.86">CLEF 2017 ehealth evaluation lab overview</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Spijker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,455.89,154.54,24.70,7.86;5,146.91,165.50,219.28,7.86">CLEF 2017 -8th Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="5,373.06,165.50,107.54,7.86;5,146.91,176.46,56.67,7.86">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,187.42,342.24,7.86;5,146.91,198.38,333.68,7.86;5,146.91,209.34,333.68,7.86;5,146.91,220.30,215.06,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,339.20,187.42,141.39,7.86;5,146.91,198.38,157.95,7.86">CLEF 2017 technologically assisted reviews in empirical medicine overview</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Spijker</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="5,327.68,198.38,152.92,7.86;5,146.91,209.34,165.58,7.86">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="5,146.91,220.30,148.42,7.86">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,231.26,342.24,7.86;5,146.91,242.21,333.68,7.86;5,146.91,253.17,130.95,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,455.17,231.26,25.42,7.86;5,146.91,242.21,315.41,7.86">A test collection for evaluating retrieval of studies for inclusion in systematic reviews</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Scells</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Deacon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Geva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,146.91,253.17,102.27,7.86">Proceedings of SIGIR &apos;17</title>
		<meeting>SIGIR &apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,264.13,342.24,7.86;5,146.91,275.09,307.92,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,463.90,264.13,16.69,7.86;5,146.91,275.09,158.46,7.86">Systematic review automation technologies</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsafnat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Glasziou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Choong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Galgani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Coiera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,312.36,275.09,76.42,7.86">Systematic reviews</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">74</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,286.05,342.24,7.86;5,146.91,297.01,333.67,7.86;5,146.91,307.97,197.21,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,411.30,286.05,69.30,7.86;5,146.91,297.01,286.41,7.86">Extracting PICO sentences from clinical trial reports using supervised distant supervision</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kuiper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">J</forename><surname>Marshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,440.07,297.01,40.52,7.86;5,146.91,307.97,111.20,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">132</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
