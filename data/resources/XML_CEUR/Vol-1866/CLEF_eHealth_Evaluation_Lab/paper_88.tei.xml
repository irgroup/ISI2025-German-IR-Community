<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.10,115.96,331.14,12.62;1,142.24,133.89,330.88,12.62;1,153.75,151.82,307.86,12.62">A Study of Convolutional Neural Networks for Clinical Document Classification in Systematic Reviews: SysReview at CLEF eHealth 2017</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,261.83,189.49,91.70,8.74"><forename type="first">Grace</forename><forename type="middle">Eunkyung</forename><surname>Lee</surname></persName>
							<email>leee0020@e.ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.10,115.96,331.14,12.62;1,142.24,133.89,330.88,12.62;1,153.75,151.82,307.86,12.62">A Study of Convolutional Neural Networks for Clinical Document Classification in Systematic Reviews: SysReview at CLEF eHealth 2017</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">144225AD0604C6E42671F483410B304B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>document classification</term>
					<term>systematic review</term>
					<term>convolutional neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Identifying eligible documents for systematic reviews is one of the most time-consuming steps in writing the reviews. From retrieving numerous clinical documents to manually checking the documents with detailed criteria requires a tremendous amount of time and skilled workforce. In this paper, to increase the efficiency of the process we examine the role of convolutional neural networks for classifying medical documents for systematic reviews. The analysis is carried out in the context of the CLEF 2017 eHealth Task 2 as a participant. The evaluation demonstrates that the suggested methods show slightly better performance for full document screening than abstract screening.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recognizing relevant documents out of thousands of documents is one of the most time-consuming yet important steps in writing systematic reviews. Systematic reviews analyze and appraise all pertinent literature that meets a set of pre-defined eligibility criteria. Before analyzing selected literature for a review, systematic review authors need to filter related documents by manually investigating numerous documents for their eligibility. Since missing out relevant documents is critical, researchers initially collect thousands of documents from several databases which might be eligible for a review. The collected documents are thoroughly examined for eligibility through two steps of abstract and full document screenings.</p><p>There have been several studies to automatic the laborious screening process. However, imbalanced data and different levels of complexity for eligibility criteria make automating the process a challenging task. Specifically, among 50 Cochrane systematic reviews more than 5,000 documents are initially collected on average, and only around 20 documents are turned out to be eligible for the review as indicated in Table <ref type="table" coords="1,233.59,632.21,3.87,8.74" target="#tab_0">1</ref>. Furthermore, systematic reviews have a broad range of topics from education for health professionals to heart disease and blood circulation. The review topic and its scope lead to manifold eligibility criteria <ref type="bibr" coords="1,467.30,656.12,9.96,8.74" target="#b1">[2]</ref>.</p><p>The approaches toward solving the issues have been proposed for the past years. Regarding to the imbalanced data, negative undersampling and weighting schemes are used and, especially, active learning showed promising performance to settle the limited number of positive data <ref type="bibr" coords="2,327.34,154.86,9.96,8.74" target="#b5">[6]</ref>. Moreover, the majority of existing work for improving screening process applied feature selections and conventional machine learning algorithms such as SVM to train classifiers. In addition, in <ref type="bibr" coords="2,147.32,190.72,9.96,8.74" target="#b4">[5]</ref>, systematic reviews from two different domains are evaluated and show different characteristics, but the number of reviews is limited and diversity of review topics can be further expanded.</p><p>In this paper, we examine the efficacy of convolutional neural networks(CNN) for medical document classification in systematic reviews. The analysis of the approach is carried out in the context of CLEF 2017 eHealth Task 2: Technologically Assisted Reviews in Empirical Medicine <ref type="bibr" coords="2,349.55,263.62,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,363.13,263.62,7.01,8.74" target="#b2">3]</ref>. The contribution of this approach is studying a modern machine learning algorithm, CNN, on the task of identifying eligible clinical documents, despite the challenges of imbalanced data distribution. To resolve the imbalance of data with the small number of positive cases, we train the model on sentence-level context, rather than document-level, with undersampling. We also concatenate context of systematic reviews criteria with sentences of the collected documents. This provides a hint of anchor information of to which systematic review each sentence is related. We evaluate various combinations of contexts from reviews and documents with CNN.</p><p>The remainder of the paper is laid out as follows. In Section 2 we present data description and Section 3 provides detailed description of our approach on the task and variations of models. In Section 5 we evaluate our models and analyze the results. Finally, we conclude the paper by summarizing the major results in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data description</head><p>In this work, we use the CLEF 2017 eHealth Task 2 dataset <ref type="bibr" coords="2,410.20,487.59,9.96,8.74" target="#b2">[3]</ref>. The dataset consists of 50 diagnostic test accuracy (DTA) Cochrane systematic reviews. The reviews include a title, boolean queries, and PMIDs retrieved from the queries. Besides, PMIDs are indicated for eligibility results after two-stage screening: abstract screening, and full document screening.</p><p>Table <ref type="table" coords="2,177.16,548.52,4.98,8.74" target="#tab_0">1</ref> demonstrates statistics of medical documents collected from 50 systematic reviews and the number of documents as a result of examining title and abstract, and full document screening, respectively. From the Table <ref type="table" coords="2,457.57,572.43,3.87,8.74" target="#tab_0">1</ref>, we can see that the initial collection contains numerous medical research papers. In contrast, the number of positive documents after abstract screening is a small fraction of the entire collection. Even further, after full document screening, the final number of documents to be included in the reviews is dramatically reduced from the initial collection of documents. Hence, the collection of documents retrieved via boolean queries are noisy and contain many irrelevant documents for reviews. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Different from common approaches to document categorization or sentiment analysis, several inherent characteristics of systematic reviews make the current task unique and challenging. One characteristic of the task is scarcity of positive data. The number of final positive documents is not enough to train a model for a review since it is often less than 50. In spite of adopting techniques of reducing the imbalance, the absolute number of positive documents is still not sufficient.</p><p>In order to overcome data sparsity, we combine all documents in training dataset and utilize sentences as a training unit to build one general classifier for DTA systematic reviews.</p><p>Training a general classifier leads to face another challenge. In this task, each document can be classified either positive or negative, depending on eligibility criteria, and the eligibility criteria vary over systematic reviews. For instance, a medical document is positive in review A and negative in review B because of different criteria, even though the document is retrieved in both the reviews. As a result, one document labeled positive and negative becomes training inputs for one classifier. Thus, a document or sentence itself is not able to be a stand-alone input as training data.</p><p>To resolve the challenge, we provide eligibility criteria with each sentence by concatenating a title of reviews and sentences from clinical documents. Since titles of reviews contain imperative elements of reviews in a brief format, we believe that titles of reviews would provide a snippet of eligibility criteria of reviews. The detailed description of concatenating eligibility criteria and sentences are demonstrated in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CNN model</head><p>In this section, we explain a simple CNN with one layer of convolution on top of word vectors. The model is a slight variant of the CNN architecture proposed in <ref type="bibr" coords="3,146.62,620.44,9.96,8.74" target="#b3">[4]</ref>. Let x i ∈ R k be the k-dimensional word embedding vector corresponding to the i-th word in the sentence. A sentence s is represented as</p><formula xml:id="formula_0" coords="3,257.52,656.12,223.07,9.65">s = x 1 ⊕ x 2 ⊕ • • • ⊕ x n ,<label>(1)</label></formula><p>where ⊕ is the concatenation operator and n is the maximum length of sentences. Sentences are padded to the maximum length if necessary. Likewise, review context is represented as equation 1. A convolution layer involves a filter w ∈ R hk , which is applied with a window size, h, to grasp information from surrounding words. A new feature, c i capturing the context with a window of h words, is generated by</p><formula xml:id="formula_1" coords="4,211.55,201.07,264.80,9.65">c i = f (w • x i:i+h-1 + b) , 1 ≤ i ≤ (n -h + 1) (<label>2</label></formula><formula xml:id="formula_2" coords="4,476.35,201.07,4.24,8.74">)</formula><p>where b is a bias term and function f is a hyperbolic tangent. As a result, a sentence is represented by multiple feature vectors</p><formula xml:id="formula_3" coords="4,258.69,257.62,221.90,9.65">c = [c 1 , c 2 , . . . , c n-h+1 ]<label>(3)</label></formula><p>with c ∈ R n-h+1 . Next, we max-pool the result of the convolutional layer into a long feature vector, c = max {c}, which is to merge the results into the most representative feature vector. We then incorporated the common regularization method, dropout, to prevent feature vectors from co-adapting and force them to learn useful features in a independent manner. For more details on dropout we refer the reader to <ref type="bibr" coords="4,238.99,339.69,9.96,8.74" target="#b3">[4]</ref>. After regularization we classify the result using a softmax layer. Finally, predicted results of sentences are combined for document classification, which is our ultimate task, and derive the document classification as follows</p><formula xml:id="formula_4" coords="4,267.20,383.54,213.39,26.80">D = 1 |D| s∈D p (s) ,<label>(4)</label></formula><p>where |D| is the total number of sentences in a document D and p (s) is the prediction probability of sentence s derived from the CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>In this section, we discuss our experimental setup to evaluate the effectiveness of the proposed approach for modeling a medical document classifier. In particular, Section 4.1 describes the preprocessing and normalization according to characteristics of biomedical text, while Section 4.2 presents the hyperparameters for the CNN model. Section 4.3 discusses the variants of our approaches used in our experiments.</p><p>Undersampling is a common way to deal with imbalanced data. We used negative undersampling when training classifiers because of the limited number of eligible documents compared to irrelevant documents.</p><p>Given ids for PubMed documents collected for 50 systematic reviews, we used a title and abstract of clinical documents from PubMed. Even though the goal of a task is to improve both of a title and abstract screening and full document screening, we solely exploit only a title and abstract of the documents as input data. We believe they contain the most important content of documents like a summary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Normalization</head><p>Prior to classification, sentences from documents undergo normalization in which a script using regular expressions simplifies complex numerical and mathematical notation into a canonical form. All integers, real numbers, and percentage are mapped to INT, FLOAT and PERCENT, respectively. Acronyms are appeared with parenthesis when they are mentioned for the first time, so the parenthesis are eliminated and the acronyms are considered as single words. Lastly, measurements such as dosages, 100g/d, are normalized by MEASUREMENT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyperparameters and Training</head><p>After normalization word tokens are represented by pre-trained word embedding.</p><p>In order to reflect characteristics of biomedical text, we leverage the pre-trained Word2Vec vectors with PubMed and PubMed Central dumps<ref type="foot" coords="5,405.85,373.10,3.97,6.12" target="#foot_0">1</ref> . Since the word embeddings are trained on the entire available biomedical literature, we believe that it can effectively capture semantics for the biomedical domain. The vector representations has the dimensionality of 200. If words are not present in the set of pre-trained words, they are initialized with all zeros. We use rectified linear units and filter size (h) are set to 3, 4, 5, and 6. The dropout rate is set to 0.5, mini batch size is 50, and L2 norm in regularization is not used for the purpose of simplicity. For training, 20 systematic reviews are employed and the training is conducted through stochastic gradient descent over shuffled mini-batches. The rest 30 reviews are allocated for testing which is identical with the set up of CLEF eHealth 2017 Task 2. The statistics of training data for relevant and irrelevant sentences is presented in Table <ref type="table" coords="5,411.31,506.18,3.87,8.74" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Variations of Model</head><p>In this work, we try three variants of data concatenation between eligibility criteria and documents to be evaluate. Eligibility criteria have various elements for reviews and they are often described in a document so-called protocols. Rather than accessing long and descriptive protocols about criteria, we consider a title of systematic review as criteria, since a review title represents vital elements of documents in a brief format. By providing a hint of eligibility criteria, each sentence is differentiated from which criteria it is evaluated on. The variations of concatenation of criteria and sentence information are as follows.</p><p>-Cri-Titlesent: A model where a title of systematic review, a title of medical document are concatenated to a sentence of abstract of the medical document as prefix and utilized as input data. -Cri-Sent: Same as the model above but a title of systematic review is used concatenated except a title of document. -Cri-Title: A model where a title of systematic review and a title of clinical documents are combined. Sentences from abstract are not used in this model. Thus, compared to the previous models, it is built with less input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>In this section, we present and discuss the results obtained by our models on the test data of the task. Table <ref type="table" coords="6,177.78,273.05,4.98,8.74" target="#tab_2">3</ref> shows results of the three models on different evaluation measurements. A wss@N indicates Work Saved over Sampling @ Recall and measures how much a model reduces workload of reviewers <ref type="bibr" coords="6,336.94,296.96,9.96,8.74" target="#b5">[6]</ref>. Measuring reduced workload has been one of the common evaluation approaches for the task of automating screening process in systematic reviews. A norm area represents area under the cumulative recall curve normalized by the optimal area. More details on evaluation measures used in the task, we refer to <ref type="bibr" coords="6,350.93,344.78,9.96,8.74" target="#b2">[3]</ref>.</p><p>Since CNN architecture requires massive amount of training data to achieve reasonable performance, the suggested models show poor performances. This indicates that the models meed more consistent labeled data even though the number of training data has been increased in this models. Compared to the two models, Cri-Titlesent and Cri-Sent, the model Cri-Title displays lower performance because it utilizes the fewer number of data for training.</p><p>From the results of wss@100 and wss@95 presented in Table <ref type="table" coords="6,413.76,428.47,3.87,8.74" target="#tab_2">3</ref>, the proposed models have slightly better evaluation results on full document screening than abstract screening. The relevance results of abstract screening include not only relevant cases but also cases which cannot be judged because of the lack of information in the currently given data. Hence, the results from abstract screening might be less consistent.</p><p>Besides, further investigations on the limited performances revealed that the model fails to make right predictions when there is no abstract text. Some relevant documents do not have abstract text in PubMed, only their titles. Therefore, low-ranked relevant studies deteriorate the overall performances.</p><p>We believe that performances of the models have room for improvement. Handling the process of collecting abstract text of relevant studies from various biomedical literature databases as well as PubMed, the increased training data, and fine tuning on CNN architecture will lead to enhanced results. We leave this part as future work for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we have presented simple CNN models for improving the laborious task of identifying eligible documents for systematic reviews. The suggested models are designed for any DTA systematic reviews even though every systematic review is accompanied with different complexities of eligibility criteria. The models take advantage of concatenated context from criteria and clinical documents. The evaluation results show that while the performance of the proposed approaches has room for improvement, they have higher performance in full document screening than abstract screening. This work is a step towards applying deep neural networks to improve the screening process despite the scarcity of labeled documents and the data imbalance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,160.40,127.36,294.56,87.55"><head>Table 1 :</head><label>1</label><figDesc>Statistics of clinical documents from 50 systematic reviews</figDesc><table coords="3,180.43,139.70,254.49,75.21"><row><cell></cell><cell>Total # of documents</cell><cell># of documents after title and abstract screening</cell><cell># of documents after full document screening</cell></row><row><cell>Min</cell><cell>64</cell><cell>2</cell><cell>0</cell></row><row><cell>Max</cell><cell>43411</cell><cell>619</cell><cell>99</cell></row><row><cell cols="2">Average 5389.26</cell><cell>93.22</cell><cell>21.86</cell></row><row><cell>SD</cell><cell>7040.28</cell><cell>123.88</cell><cell>22.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,127.36,345.82,54.47"><head>Table 2 :</head><label>2</label><figDesc>Statistics of training data. The number of relevant sentences are the total number of sentences of relevant documents from 20 systematic reviews</figDesc><table coords="5,217.75,151.65,179.86,30.18"><row><cell></cell><cell># of relevant sentences</cell><cell># of irrelevant sentences</cell><cell>Total</cell></row><row><cell>Training</cell><cell>4435</cell><cell>4437</cell><cell>8872</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,146.09,127.36,322.58,155.24"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results of the three models 3936.481 3606.368 3936.481 3606.368 3937.347 3607.195 total cost weighted 4130.067 3557.586 4130.067 3557.586 4130.933 3558.414</figDesc><table coords="7,146.09,149.66,320.40,132.95"><row><cell></cell><cell cols="2">Cri-Titlesent</cell><cell cols="2">Cri-Sent</cell><cell cols="2">Cri-Title</cell></row><row><cell></cell><cell cols="6">abstract full doc abstract full doc abstract full doc</cell></row><row><cell>wss@100</cell><cell>0.089</cell><cell>0.204</cell><cell>0.117</cell><cell>0.204</cell><cell>0.091</cell><cell>0.148</cell></row><row><cell>wss@95</cell><cell>0.108</cell><cell>0.217</cell><cell>0.131</cell><cell>0.197</cell><cell>0.075</cell><cell>0.141</cell></row><row><cell>norm area</cell><cell>0.612</cell><cell>0.647</cell><cell>0.595</cell><cell>0.618</cell><cell>0.538</cell><cell>0.545</cell></row><row><cell>total cost uniform average precision</cell><cell>0.078</cell><cell>0.05</cell><cell>0.06</cell><cell>0.039</cell><cell>0.052</cell><cell>0.024</cell></row><row><cell>reliability</cell><cell>0.548</cell><cell>0.717</cell><cell>0.548</cell><cell>0.717</cell><cell>0.549</cell><cell>0.718</cell></row><row><cell>loss r</cell><cell>0.01</cell><cell>0</cell><cell>0.01</cell><cell>0</cell><cell>0.01</cell><cell>0</cell></row><row><cell>loss e</cell><cell>0.538</cell><cell>0.717</cell><cell>0.538</cell><cell>0.717</cell><cell>0.539</cell><cell>0.717</cell></row><row><cell>recall</cell><cell>0.982</cell><cell>0.996</cell><cell>0.982</cell><cell>0.996</cell><cell>0.982</cell><cell>0.996</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,144.73,656.80,88.85,7.86"><p>http://bio.nlplab.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,155.24,454.57,325.35,8.74;7,155.24,466.53,325.35,8.74;7,155.24,478.48,228.34,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,270.81,454.57,200.09,8.74">CLEF 2017 eHealth Evaluation Lab Overview</title>
		<author>
			<persName coords=""><forename type="first">Lorraine</forename><surname>Goeuriot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,169.84,466.53,271.16,8.74">CLEF 2017 -8th Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="7,449.26,466.53,31.33,8.74;7,155.24,478.48,149.00,8.74">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,155.24,490.44,325.35,8.74;7,155.24,502.39,245.58,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,316.38,490.44,164.21,8.74;7,155.24,502.39,93.36,8.74">Cochrane handbook for systematic reviews of interventions</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">T</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sally</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Green</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,155.24,514.35,325.35,8.74;7,155.24,526.30,325.35,8.74;7,155.24,538.26,325.35,8.74;7,155.24,550.21,22.69,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,275.13,514.35,205.46,8.74;7,155.24,526.30,128.22,8.74">CLEF 2017 Technologically Assisted Reviews in Empirical Medicine Overview</title>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,313.26,526.30,167.33,8.74;7,155.24,538.26,181.70,8.74">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="7,345.66,538.26,130.38,8.74">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,155.24,562.17,325.35,8.74;7,155.24,574.12,172.39,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,210.75,562.17,246.70,8.74">Convolutional neural networks for sentence classification</title>
		<author>
			<persName coords=""><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,155.24,586.08,330.24,8.74;7,155.24,598.03,325.34,8.74;7,155.24,609.99,17.71,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,244.72,586.08,240.77,8.74;7,155.24,598.03,66.45,8.74">Reducing systematic review workload through certaintybased screening</title>
		<author>
			<persName coords=""><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,248.31,598.03,145.20,8.74">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="242" to="253" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,155.24,619.43,325.34,11.26;7,155.24,633.90,325.35,8.74;7,155.24,645.85,81.82,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,271.69,621.94,208.89,8.74;7,155.24,633.90,251.85,8.74">Using text mining for study identification in systematic reviews: a systematic review of current approaches</title>
		<author>
			<persName coords=""><forename type="first">Alison</forename><surname>Ómara-Eves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,434.00,633.90,46.59,8.74;7,155.24,645.85,31.19,8.74">Systematic reviews</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
