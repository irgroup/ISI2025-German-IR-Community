<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.69,115.96,297.98,12.62;1,185.77,133.89,243.81,12.62">Ranking and Feedback-based Stopping for Recall-centric Document Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,225.79,171.56,68.63,8.74"><forename type="first">Noah</forename><surname>Hollmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<postCode>8092</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.12,171.56,72.44,8.74"><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<postCode>8092</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.69,115.96,297.98,12.62;1,185.77,133.89,243.81,12.62">Ranking and Feedback-based Stopping for Recall-centric Document Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">438098563DD4BC2BD9FFC184B302DB5B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cutoff problem</term>
					<term>Stopping criteria</term>
					<term>Total Recall</term>
					<term>Medical Information Retrieval</term>
					<term>Relevance-Feedback</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Systematic reviews require researchers to identify the entire body of relevant literature. Algorithms that filter the list for manual scanning with nearly perfect recall can significantly decrease the workload. This paper presents a novel stopping criterion that estimates the score-distribution of relevant articles from relevance feedback of random articles (S-D Minimal Sampling). Using 20 training and 30 test topics, we achieve a mean recall of 93.3%, filtering out 59.1% of the articles. This approach achieves higher F2-Scores at significantly reduced manual reviewing work loads. The method is especially suited for scenarios with sufficiently many relevant articles (&gt;5) that can be sampled and employed for relevance feedback.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Systematic reviews give a comprehensive overview of all published evidence on a given topic. It has been estimated that every year, more than 4000 systematic reviews are conducted and published with each review requiring at least 6-12 months of preparation time <ref type="bibr" coords="1,280.46,464.84,9.96,8.74" target="#b3">[4]</ref>. In order to write a systematic review in a first step all related articles have to be collected. Often a huge initial number of articles is retrieved and subsequently filtered by manually scanning each document's abstract. This practice creates a considerable workload for researchers. With medical libraries expanding rapidly it is crucial to find methods that can algorithmically reduce the number of articles that need to be reviewed by domain experts, while not missing any relevant ones. This task is known as the Total Recall Problem in the Information Retrieval community.</p><p>Systematic reviews of diagnostic test accuracy (DTA) compare the effectiveness of index tests for a target condition. Filtering relevant studies for DTA reviews has been identified to be exceptionally challenging due to an increased class-imbalance, a broader-than usual target class definition, and a lack of metadata quality e.g., missing abstracts <ref type="bibr" coords="1,294.86,608.30,14.61,8.74" target="#b11">[12]</ref>. However, significant advances in this domain are expected to be applicable to other areas as well. Due to unreliable performance, the Cochrane Organization, a leading authority in systematic DTA reviews does not, currently, recommend to use any search filters in the review process <ref type="bibr" coords="1,169.49,656.12,14.61,8.74" target="#b12">[13]</ref>.</p><p>This paper describes ETH Zurich's participation in "Task 2: Technologically Assisted Reviews in Empirical Medicine" at the CLEF eHealth Evaluation lab 2017 <ref type="bibr" coords="2,157.91,142.90,9.96,8.74" target="#b6">[7]</ref>. The aim of this task is to find reliable filtering methods for Diagnostic Test Accuracy (DTA) reviews. In a first step a human expert collects a list of PubMed-articles by Boolean search for each topic. The aim of the task is to filter this initial list of articles with total recall. The filtered list can then be reviewed by experts at a lower expenditure of time and resources.</p><p>We propose a learning to rank pipeline that extracts information for each article from PubMed, creates numerical features from this information, ranks each article and finally determines a cut-off point on the ranked list based on a novel score distribution approach.</p><p>The remainder of this document is structured as follows. Section 2 describes our learning-to-rank system alongside a statistical stopping criterion for manual result list inspection. Section 3 empirically compares the proposed method with a wide range of state-of-the-art baselines. Section 4 discusses a number of qualitative observations and Section 5 concludes with an outlook on future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In order to judge the relevance of an article for a given query, we propose a learning-to-rank pipeline that extracts information for each candidate article and represents them as dense numerical feature vectors. These vectors can be used to train ranking systems that create ordered lists of articles. In a final step the algorithm decides where to cut off the ranked list for manual inspection.</p><p>For each article, we extract the title, abstract, MeSH headings, a list of publication types and the publication language via the NCBI EUtilities<ref type="foot" coords="2,442.14,439.34,3.97,6.12" target="#foot_0">1</ref> . MeSH headings are a list of tags from a comprehensive controlled vocabulary for indexing journal articles in the life sciences. While a title is available for all papers, the abstract is missing in many cases (12.2% on the training set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Extraction</head><p>From this data we extract a number of features and group them into two categories: dynamic and static features. While static features only depend on the article, dynamic features depend on the article in relation to each query. Static features will capture the aptness of an article to be included in any systematic review, while dynamic features express its relevance for the query at hand. Static features include a similarity score between each article's 128-dimensional Doc2Vec embedding <ref type="bibr" coords="2,228.15,600.72,15.50,8.74" target="#b10">[11]</ref> and an average embedding of relevant documents, a number of statistical features denoting how likely publication type and language are relevant, the publication year and the number of words in abstract, title and MeSH headings. Dynamic features include a tf-idf similarity measure of query text and query title to document title, abstract and MeSH headings. The tf-idf score is calculated using Lucene and is optimized using a stop word filter. Also included in the dynamic features is the cosine similarity of the document embeddings between the query and various document fields. For this purpose also the query and query title are mapped to a vector using Doc2Vec. The method includes a total of 51 features the respective effectiveness of which will be discussed in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Static relevancy using document embedding</head><p>In previous search filters the occurrence of words that frequently appear in relevant documents for arbitrary DTA reviews was used to filter the articles <ref type="bibr" coords="3,333.01,245.42,14.61,8.74" target="#b13">[14]</ref>. We propose a deep learning approach that models the similarity to frequent words by creating a document embedding of a common relevant document. We create the vectors</p><formula xml:id="formula_0" coords="3,250.87,290.30,112.43,68.32">Doc2V ec + q = a∈A + q d2v(a) |A + q | Doc2V ec - q = a∈A - q d2v(a) |A - q |</formula><p>where d2v denotes the Doc2Vec The similarity of title, abstract and MeSH headings to vectors created by both methods become ranking features. We call the first approach the classic method, the second one the difference method. This score is calculated for title, abstract and MeSH headings, respectively.</p><p>MetaMap MetaMap<ref type="foot" coords="4,229.78,240.45,3.97,6.12" target="#foot_1">2</ref> is a tool that maps biomedical text to the UMLS Metathesaurus using symbolic, natural-language processing (NLP) and computationallinguistic techniques. It is a state-of-the-art library that has shown to be highly effective <ref type="bibr" coords="4,173.37,277.89,9.96,8.74" target="#b2">[3]</ref>. Mapping text to UMLS Context Unique Identifiers (CUIs) will reduce ambiguity of medical documents and also tag the CUIs with a semantic group. We measure the tf-idf score of the CUIs identified in query title and query to the CUIs identified in the document content fields using a BM25 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical features</head><p>The publication type probability P (T ) is the probability that a given publication type is included in the list of publication types of a relevant article. The publication type score S(T ) is calculated such that S(T ) = P (T ) * N (T ), where N (T ) is the number of times that publication type was relevant.</p><p>Treating missing values After creating features for each article and its topic combination, we process the feature file to account for missing data. For missing abstracts, we set all abstract related features to the average on the training set and do the same for MeSH headings. We tried setting these features to an unused value such as -100, which produced worse results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ranking Models</head><p>On the basis of the previously described features, we use Ranklib<ref type="foot" coords="4,428.56,548.91,3.97,6.12" target="#foot_2">3</ref> to train a number of rankers and validate their performance. After evaluating a broad range of models including coordinate ascent, MART <ref type="bibr" coords="4,360.28,574.40,9.96,8.74" target="#b7">[8]</ref>, AdaRank <ref type="bibr" coords="4,419.78,574.40,14.61,8.74" target="#b15">[16]</ref>, Random-Forests <ref type="bibr" coords="4,168.58,586.35,10.52,8.74" target="#b4">[5]</ref> and LambdaMART <ref type="bibr" coords="4,270.31,586.35,14.61,8.74" target="#b14">[15]</ref>, we decided for a straightforward coordinate ascent model that yielded the most reliable results on the given data. Unless stated otherwise, all further experiments in this paper are based on this ranking model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Stopping Criteria</head><p>While generating a model to score the relevancy of articles is essential to finding a set of relevant articles, it is equally important to find the right point to stop retrieving more documents. In order to threshold the ranking, we propose a naive baseline technique that cuts off at a fixed rank and an extended baseline that will decide based solely on previous distributions of relevant articles according to their retrieval model score.</p><p>In order to find a suitable cutoff method we need to define a metric to optimize for. In a systematic review we emphasize recall over precision and reliability of optimality. Additionally, we would like to use as little manual relevance feedback as possible for our ranking. The optimal cost C opt for some optimal rank r trading off between recall, precision and relevance feedback can be found according to:</p><formula xml:id="formula_1" coords="5,226.50,281.64,162.36,17.12">C opt = min ∀r∈(1,|A|) f (r, |A + &lt;r |, |A + &gt;r |, |R|)</formula><p>for any weighting formula f , a list of articles A, a list of positive articles that are retrieved A + &lt;r , a list of positive articles that are not retrieved A + &lt;r and a set of documents the method got relevance feedback for R. A weighting formula that weights precision with α and recall with β looks the following:</p><formula xml:id="formula_2" coords="5,182.48,363.09,250.40,26.78">f (r, |A + &lt;r |, |A + &gt;r |, |R|) = α * (1 - |A + &lt;r | r ) + β * (1 - |A + &lt;r | |A + &gt;r | )</formula><p>Static cutoff models We use two very simple baseline models that cut off the list at a fixed rank r * or a fixed score s * for every topic. The parameters r * and s * can be fit on the training topics. We use an improved static method that normalizes the scores for each ranking linearly, which yields better results than the original version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BMI Method</head><p>We take as another baseline method the default TREC rule for stopping to read a ranked list using relevance feedback for each document. We stop when the number of documents reviewed exceeds 2R + 1000, where R + is the number of relevant documents retrieved so far.</p><p>Knee Method This method locates a so-called "knee" or negative inflection point in the gain curve of relevant documents. The gain curve indicates for each rank x how many relevant documents were found up to that rank. The method stops when the slope following the knee is less than 1 α of the slope before the knee and the index is higher than some β.</p><p>Classic S-D Method Score distributions (S-Ds) of relevant and irrelevant documents have been studied since the early days of IR. By modelling the scoredistribution of relevant documents P + for a topic we can estimate the number of relevant documents |A + &lt;r | that are retrieved until rank r using</p><formula xml:id="formula_3" coords="6,246.87,138.47,121.61,26.29">|A + &lt;r | = r 1 P + (score(x)) dx</formula><p>where score(x) denotes the score at rank x. We can approximate the best cutoff according to some metric that uses |A + &lt;r * | for all r * and r. Using the distribution we can estimate a cost function for all possible cutoff points and select the best position. We follow Arampatzis et al. <ref type="bibr" coords="6,326.41,211.68,10.52,8.74" target="#b1">[2]</ref> in modelling the distribution of relevant documents by a Gaussian. In this "classic" S-D Method, we learn the distribution P learn + ∼ N (µ, σ) by fitting a Gaussian to the score-distribution of relevant articles in the training set and make the simplifying assumption that the same distribution will also hold for previously unseen test topics.</p><p>Feedback-Based S-D Method In practice, however, the true distribution may vary strongly for each topic. Instead of using a distribution P learn + that was trained beforehand, we can sample some documents X ⊆ A randomly with a sampling distribution P sample (a) for a ∈ A that assigns a probability of being sampled for each article. We approximate the Gaussian distribution P f eedback + by fitting it on the score distribution of relevant sampled articles X + = {rf (x) = 1|x =∈ X} where rf (x) = 1 if x is relevant and 0 otherwise.</p><p>In the simplest case we give each document the same probability 1 |A| to be sampled and fit a Gaussian distribution P f eedback + on X + . However, notice that we do not use the irrelevant articles at all. Sampling irrelevant documents will create additional work and may, in some cost functions, be penalized for the use of relevance feedback as well (e.g. CostU niRF ). It is therefore desirable to ask for feedback on as few irrelevant articles as possible. However, since the classes are very imbalanced, if we sample each document with uniform probability we will get much more irrelevant documents than relevant ones. We correct for this observation by boosting the sampling-probability of articles with a high score which will increase the probability to get feedback on a relevant article. We use a sampling distribution P sample (s) that assigns a sampling probability according to the score s = score(x) of an article x now. During sampling a random score s with probability P sample (s) is drawn and the document with the closest score is sampled.</p><p>If we try to fit a Gaussian distribution to the data X + after the optimized sampling step we notice that the mean of the resulting distribution is biased towards higher scores. A document with score s will be in X with the sampling probability P sample (s) instead of the true probability of a score in the entire data set P overall (s). Thus, a document with score s will be in the sampled data X with P sample (s)/P overall (s) times the true probability. A distribution that is fit on the data X will therefore show approximately the same bias. In order to remove this bias from the sampled data X we discretize the scores into N chunks Chunk = [1, .., N ] associating a score score c (n) = s 0 + n nsn from the lowest score s 0 to the highest score s n to the pieces. For each chunk we calculate the sampling bias</p><formula xml:id="formula_4" coords="7,220.14,127.46,174.76,23.23">B(n) = P overall (score c (n)) P sample (score c (n) , n ∈ Chunk</formula><p>, where P overall is obtained for each score by fitting a Gaussian to the score distribution of all articles A. The bias B(n) indicates how much more often a data point in the chunk n was sampled relative to how often it would be sampled if every document had the same probability. Now we obtain an unbiased P+ ∼ N ( mu, σ) by fitting a Gaussian to the n data points</p><formula xml:id="formula_5" coords="7,134.77,203.59,345.83,24.90">P sample (chunk(n)) B(n) ∀n ∈ Chunk.</formula><p>The standard deviation σ was found by using the data points of the n chunks instead of the raw sampled data X. We find a better standard deviation σ * by iteratively testing the fit of P * + ∼ N ( mu, σ * ) on the sampled data X and selecting the best σ * . We test the method with two sampling distributions: the uniform distribution and a triangular distribution with P triangular (s) = γ * (s+δ) with γ such that P triangular (s) is a probability distribution and the offset δ such that P (s) &gt; 0 for all scores. Some topics will have very few relevant documents. In the most extreme case just a single one, making it impossible to make a robust estimate. A very low number of relevant documents from sampling will result in a high variance in the score distribution of the relevant samples which will decreases the similarity of our predicted score distribution to the true one. For a very low number of relevant sampled articles |A + sampled | &lt; α the method will perform worse than some other method M for some parameter α (e.g. 4). Fitting a Gaussian on the relevant articles requires at least two relevant articles in the sampling phase. Thus, in both cases the method should use the alternative technique M * to handle this run. The method M * can be any of the methods described before. We introduce a parameter β that defines the sampling rate β = </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data set</head><p>The experiments are evaluated on the data set provided by the CLEF 2017 Task 2 eHealth Challenge <ref type="bibr" coords="7,228.43,548.52,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="7,245.59,548.52,7.01,8.74" target="#b6">7]</ref>. The models are trained on 20 topics with a total of 125k articles (∼2.5% relevant) and then evaluated on 30 topics with overall about 120k articles. Each topic consists of a query and a title, while each article contains a PubMed-ID and its relevancy. A relevant document corresponds to a document that was selected by a human exert to be possibly included in the systematic review based only on the abstract and title. In a next step these documents are filtered again according to the content of their full documents filtering out about two thirds again. We do not predict the final inclusion in a systematic review but rather the relevancy of a document according to its abstract.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>Average Precision &amp; Mean Average Precision (MAP) MAP measures the quality of a ranking including the last documents in the ranking. It is apt to evaluate recall-centric systems in which the ordering of the last documents still matters. Notice that mean average precision is greatly influenced by the number of relevant documents contained in the ranking. A random ranking with 50% relevant documents will achieve an MAP-score of 0.5. A perfectly ordered ranking with 5% relevant documents will only achieve an MAP-score of ∼ 0.2. Average Precision will be most useful to compare different models for the same data and hardly give an absolute performance measure.</p><p>Work Saved over Sampling (WSS) WSS is an intuitive measure that indicates how much less work w(r) has to be done to achieve a recall r on the set of articles A if an optimal cutoff is applied to the ranking, compared to an unordered list. Since in an unordered list we need to examine a fraction of r documents to achieve recall r we get: </p><formula xml:id="formula_6" coords="8,263.07,333.45,78.07,15.48">W SS(r) = r - w<label>(</label></formula><formula xml:id="formula_7" coords="8,214.53,461.73,186.29,24.53">C = α(r -|R|) + β(|A| -r)( |A + &gt;r | |A + | ) + γ|R|</formula><p>The weights proposed by CLEF are α = 1, β = 2, γ = 2, we denote the measure as CostU niRF . In a more realistic setting we would not get punished for using relevance feedback (γ = α), which we will simply call CostU ni. Note that the official evaluation script assigns γ = 3, likely because a document using relevance feedback has cost α for being shown plus cost γ for the feedback. We use γ = 2 as indicated on the official evaluation measure description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Table <ref type="table" coords="8,161.73,608.30,4.98,8.74" target="#tab_1">1</ref> reports WSS@95, precision, recall and last relevant document scores obtained for each of the 30 test topics. Unless stated otherwise we use the minimal feedback based S-D method as a stopping criterion. The mean WSS@95 score is 0.544 at precision 0.091 and recall 0.933. Precision and recall compare favorably to the search filters that are reviewed in the Cochrane handbook <ref type="bibr" coords="8,420.55,656.12,14.61,8.74" target="#b12">[13]</ref>, albeit on different datasets. Howard et al. <ref type="bibr" coords="9,283.16,118.99,10.52,8.74" target="#b8">[9]</ref> achieve a WSS@95 of 0.488 on 15 topics using PubMed articles and Cohen <ref type="bibr" coords="9,286.21,130.95,10.52,8.74" target="#b5">[6]</ref> reports WSS@95 score 0.408. These comparisons suggest a reliable performance, but do not replace a true side-by-side comparison. We achieve a recall of 100% on 11 topics, while the lowest recall is at 65.2% followed 70.0% and 86.0%. Here recall can be traded off for precision by raising the penalty of missing a relevant document in the cost function that is optimized by the cutoff method.</p><p>Features We assess the descriptive power of features from three different content fields: Abstracts, Title and MeSH headings (see Table <ref type="table" coords="9,400.33,266.21,4.43,8.74" target="#tab_2">2</ref>) by training our model only using features that are derived from one content field at a time, excluding all other sources of evidence. We find MeSH headings to only insignificantly improve the performance of our ranking. Abstracts are the best source for ranking the documents even though they might not be present in some cases. We also compare dynamic features, that show the similarity of query and document with static features, that reflect the general aptness of a document for a systematic DTA review in Table <ref type="table" coords="9,248.09,349.89,3.87,8.74" target="#tab_2">2</ref>. Using only dynamic features yields a MAP score of 0.239, while using only static features yields a MAP score of 0.148. Adding both feature groups together results in a MAP of 0.2866.</p><p>We further asses the effectiveness of six different feature groups: Gensim Similarities, tf-idf, Metamap, Gensim Relevancies, language and publication types in Table <ref type="table" coords="9,173.63,421.82,3.87,8.74" target="#tab_3">3</ref>. For these features we estimate the importance of a feature group by observing the effect of systematically removing one group at a time and comparing the results to the original results obtained using all features. We find features that are not text related such as language and publication type will only weakly affect the ranking. Also tf-idf achieves the highest gain among the three methods to estimate similarity of a query to a document, even though it does not natively consider synonyms and word combinations.</p><p>Stopping Criteria Table <ref type="table" coords="9,254.94,524.42,4.98,8.74" target="#tab_4">4</ref> examines the effectiveness of eight cutoff methods according to the evaluation measures above. We find the sampling S-D methods to be more effective than all other methods on F2-Score, CostU niRf and CostU ni, while CostU niRf was optimized by it. Minimal S-D sampling proves 11.2% more effective on the average F2-Score than the best non-S-D sampling method on average. The average cost CostU ni is 17.1% lower than on the best non S-D sampling method. Using CostU niRf and thus penalizing relevance feedback the method is 9.2% stronger than the second best contestant. The cost of Minimal S-D sampling is 4.3% lower than using uniform S-D sampling.</p><p>The parameters of our stopping criteria were optimized on the 20 available training topics. This yielded an optimal average normalized score of 1.45 and an optimal cutoff rank of 1410. For the knee method we found a slope after the knee parameter of α = 9 and the minimum cutoff rank β = 500 to be optimal. For the relevance-based S-D methods we found the sampling α = 5% and the minimum number of feedback β = 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fit to normal distribution</head><p>The S-D based stopping methods assume the score distribution of relevant documents P + to be Gaussian distributed. Figure <ref type="figure" coords="12,151.80,436.22,4.98,8.74" target="#fig_2">1</ref> visualizes the fit of a standard normal distribution to the summed up and normalized score distributions of each topic. Aside from the intuitively appealing fit, we conduct a Kolmogorov-Smirnov test of normality <ref type="bibr" coords="12,406.21,460.13,9.96,8.74" target="#b0">[1]</ref>. We find that normality is retained at p ≤ 5% significance level for all topics. Figure <ref type="figure" coords="13,167.33,142.90,4.98,8.74" target="#fig_3">2</ref> compares the distribution before and after being corrected on some examples. We observe the goodness-of-fit to be much higher and very close to the actual distribution after the correction. By increasing the amount of relevance feedback we can increase the goodness-of-fit of our distribution to that of the best normal distribution. Correlation of Feedback Size and Cutoff Quality in S-D Sampling Stopping Criterion Figure <ref type="figure" coords="13,268.17,450.06,4.98,8.74">3</ref> shows the correlation of CostU nif orm of our algorithm against the number of relevant articles sampled in each run. We evaluate the same ten topics multiple times, each time sampling until we reach N relevant documents. As expected, when the number of sampled relevant documents increases the cost of the run decreases. Increasing the sample size will decrease the variance and thus improve the predictive quality of the method. However, if we continue increasing this number the cost of sampling will grow as it will be harder to find more relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Variance between topics and the need for relevance feedback</head><p>Static features that do not depend on the topic query but only on the article can be observed to perform much worse in predicting the relevance of an article. Dynamic features however rely heavily on the initial query which is created by human experts using the system and will thus be written in a very different way for each topic. Also, researchers have a focus on different properties of a document. Figure <ref type="figure" coords="14,214.08,354.99,4.98,8.74">5</ref> shows relevant articles from three topics. They are arranged on the plane such that we can see the sum of dynamic features on one axis and the sum of static features on the other. As predicted by the above hypothesis, we see that the preference for inclusion for these two feature groups is very different. In one topic A three articles are included that have low static features, while in topic B there are only articles with high static features. Also probably due to a query that was created very differently, we observe low dynamic features in topic B, but much higher ones in topic A.</p><p>An effective system will need to make use of relevance feedback for the articles in order to be able to capture the great differences between each topic and rank the documents more accurately. Human experts will also judge differently which articles need a full content scan and which can be refused immediately. This is reflected in the wide score range of relevant documents that we observe on the data in absolute numbers (2 to 460) as well as relative numbers (0.029% to 20%). In addition, the quality of a ranking will vary depending on medical focus and the initial query. Figure <ref type="figure" coords="14,260.73,548.52,4.98,8.74">6</ref> shows the scores of relevant documents in the 30 test topics. As expected each topic shows a unique score distribution with very different means. We conclude that the cutoff point will be hard to predict using the scores or ranking position without using any relevance feedback. Systems using relevance feedback rely on sampling some relevant documents which is hard due to the high class imbalance in the data of systematic DTA reviews <ref type="bibr" coords="14,434.26,608.30,14.61,8.74" target="#b11">[12]</ref>. When sampling randomly one might need to include up to 3000 irrelevant documents to sample a single relevant document. Therefore, a system using relevance feedback needs a high initial ranking quality and should use feedback mostly at the top of this ranking. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Missing Metadata</head><p>Petersen et al. <ref type="bibr" coords="15,198.87,341.75,15.50,8.74" target="#b11">[12]</ref> hypothesize that missing metadata is one of the main reasons that systematic DTA reviews are difficult to support with an IR system. In our experiments abstracts have shown to be by far the most effective field in predicting the relevance of a document, however they are missing in 12.3% of all documents in the test data. We suppose that abstracts that are missing in our records were available to the researchers judging their relevance. Thus the probability that a document is relevant should not depend on the presence or absence of abstracts. Figure <ref type="figure" coords="15,277.87,425.44,4.98,8.74" target="#fig_6">7</ref> shows that our model fulfills this property. Documents that lie on the x-axis do not have an abstract text in our data, but their frequency at the end of the ranking is similar to that of documents with an abstract text. Also, the length of the abstract text does not seem to influence the relevancy in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a recall-centric learning-to-rank scheme accompanied by a statistical cutoff criterion that identifies the optimal point for stopping human inspection of results by estimating the score-distribution of relevant documents in a biased sampling process. Our experiments show that this approach is able to reduce the size of the ranked list by more than half while retaining a recall close to 95% without using relevance feedback in the ranking step. This level of performance significantly exceeds the results obtained by traditional cutoff methods.</p><p>In the future, we will move beyond the currently employed simple Gaussian score distributions in favor of more accurate approximations of the true distribution of relevance. Additionally, we plan to further evaluate this method in other recall-driven domains such as e-discovery. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,429.40,433.20,37.67,6.79;7,442.88,441.75,10.72,6.12;7,472.57,436.27,8.03,8.74;7,134.77,449.92,298.67,9.65;7,433.47,448.35,4.08,6.12;7,442.54,449.92,38.05,8.74;7,134.77,461.88,42.90,8.74;7,177.70,460.30,4.08,6.12;7,185.61,461.88,83.66,8.74"><head></head><label></label><figDesc>|A sampled | |A| of sampled articles and use the fixed score method if |A sampled | &lt; α * for some optimal α * on the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,341.13,333.45,9.96,8.74;8,334.63,347.02,13.01,8.74;8,134.77,375.67,345.83,8.77;8,134.77,387.66,345.83,8.74;8,134.77,399.61,288.54,8.74;8,423.30,398.04,6.12,6.12;8,433.88,399.61,46.72,8.74;8,134.77,411.57,345.83,8.74;8,134.77,423.52,345.83,8.74;8,134.77,439.10,153.87,8.74;8,293.15,433.75,13.48,7.97;8,301.51,435.60,11.78,7.68;8,295.05,443.96,16.34,6.73;8,317.80,439.10,101.98,8.74"><head></head><label></label><figDesc>r) |A| CLEF 2017 Task 2 Uniform Cost CLEF eHealth Challenge Task 2 2017 provides α, β, and γ-parametrized cost based metric that measures the performance of a ranking of documents |A| with relevant documents A + that is cut off at rank r. The cost depends on the amount of relevance feedback used |R| weighted by γ, the effort r -|R| to review the documents weighted by α and the share of relevant documents missed |A + &gt;r | |A + | weighted by β(|A| -r).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="12,160.32,655.03,294.72,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Shape of positive score distribution compared to normal gaussian</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="13,134.77,374.07,345.82,7.89;13,134.77,385.06,238.94,7.86"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example distribution correction CD009185. The biased distribution is skewed towards higher scores compared to the correct distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="14,152.06,257.14,138.33,7.89;14,152.06,268.12,138.33,7.86;14,152.06,279.08,138.33,7.86;14,152.06,290.04,26.16,7.86"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig.3. Mean cost on the test topics when we sample until we found N relevant topics, while N is on the x-axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="15,152.06,250.44,138.33,7.89;15,152.06,261.42,138.33,7.86;15,152.06,272.38,138.33,7.86;15,152.06,283.34,138.33,7.86;15,152.06,294.30,82.99,7.86"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig.5. The distribution of relevant articles on dynamic and static features for two selected topics. The dots correspond to CD010276, the crosses to CD011145</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="16,134.77,270.57,345.83,7.89;16,134.77,281.55,345.83,7.86;16,134.77,292.51,300.61,7.86"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. Each point represents a relevant document. The x-axis shows the position of the document in the ranking and the y-axis the number of words in the abstract. The documents that lie on the x-axis are missing the abstract text in our data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,134.77,119.77,367.33,536.04"><head>Table 1 .</head><label>1</label><figDesc>Overview of per topic performance. N refers to the number of documents in the topic and Included refers to the number of articles judged relevant after abstract screening.</figDesc><table coords="10,137.75,168.54,364.34,487.26"><row><cell>Topic</cell><cell>N</cell><cell>Included</cell><cell cols="5">WSS@100 WSS@95 Last Rel. Cutoff Recall Precision</cell></row><row><cell cols="3">CD010633 1575 4 (0.25%)</cell><cell>91.4%</cell><cell>96.4%</cell><cell>55</cell><cell>565</cell><cell>100.0% 0.7%</cell></row><row><cell cols="4">CD012019 10319 3 (0.029%) 71.9%</cell><cell>76.9%</cell><cell>2379</cell><cell cols="2">3250 100.0% 0.1%</cell></row><row><cell cols="4">CD010339 12809 114 (0.89%) 49.3%</cell><cell>27.1%</cell><cell>9331</cell><cell cols="2">2844 86.0% 3.4%</cell></row><row><cell cols="4">CD009786 2067 10 (0.48%) 42.7%</cell><cell>47.7%</cell><cell>1080</cell><cell>180</cell><cell>70.0% 3.9%</cell></row><row><cell cols="3">CD009185 1617 92 (5.7%)</cell><cell>55.4%</cell><cell>47.9%</cell><cell>841</cell><cell>702</cell><cell>98.9% 13.0%</cell></row><row><cell cols="4">CD010276 5497 54 (0.98%) 63.6%</cell><cell>60.3%</cell><cell>2182</cell><cell cols="2">1410 94.4% 3.6%</cell></row><row><cell cols="4">CD011145 10874 202 (1.9%) 59.9%</cell><cell>20.6%</cell><cell>8633</cell><cell cols="2">2598 87.6% 6.8%</cell></row><row><cell cols="2">CD010772 318</cell><cell>47 (15%)</cell><cell>18.4%</cell><cell>15.5%</cell><cell>266</cell><cell>96</cell><cell>85.1% 41.7%</cell></row><row><cell cols="4">CD010653 8004 45 (0.56%) 60.6%</cell><cell>17.8%</cell><cell>6574</cell><cell cols="2">3291 95.6% 1.3%</cell></row><row><cell cols="2">CD010775 243</cell><cell>11 (4.5%)</cell><cell>69.3%</cell><cell>74.3%</cell><cell>61</cell><cell>105</cell><cell>100.0% 10.5%</cell></row><row><cell cols="2">CD010896 171</cell><cell>6 (3.5%)</cell><cell>50.6%</cell><cell>55.6%</cell><cell>74</cell><cell>99</cell><cell>100.0% 6.1%</cell></row><row><cell cols="3">CD008803 5222 99 (1.9%)</cell><cell>62.3%</cell><cell>48.4%</cell><cell>2691</cell><cell cols="2">1948 98.0% 5.0%</cell></row><row><cell cols="4">CD009519 5973 104 (1.7%) 75.8%</cell><cell>61.6%</cell><cell>2291</cell><cell cols="2">1348 98.1% 7.6%</cell></row><row><cell cols="3">CD007431 2076 24 (1.2%)</cell><cell>30.1%</cell><cell>21.0%</cell><cell>1638</cell><cell cols="2">1594 95.8% 1.4%</cell></row><row><cell cols="4">CD009579 6457 138 (2.1%) 56.7%</cell><cell>28.9%</cell><cell>4590</cell><cell cols="2">2215 93.5% 5.8%</cell></row><row><cell cols="2">CD009135 793</cell><cell>77 (9.7%)</cell><cell>20.9%</cell><cell>12.8%</cell><cell>689</cell><cell>252</cell><cell>87.0% 26.6%</cell></row><row><cell cols="2">CD010705 116</cell><cell>23 (20%)</cell><cell>12.5%</cell><cell>0.9%</cell><cell>112</cell><cell>49</cell><cell>65.2% 30.6%</cell></row><row><cell cols="4">CD008782 10509 45 (0.43%) 79.3%</cell><cell>83.6%</cell><cell>1724</cell><cell cols="2">3205 100.0% 1.4%</cell></row><row><cell cols="2">CD008760 66</cell><cell>12 (18%)</cell><cell>43.4%</cell><cell>48.4%</cell><cell>32</cell><cell>43</cell><cell>100.0% 27.9%</cell></row><row><cell cols="3">CD009551 1913 46 (2.4%)</cell><cell>79.0%</cell><cell>81.1%</cell><cell>361</cell><cell>221</cell><cell>84.8% 17.6%</cell></row><row><cell cols="3">CD009372 2250 25 (1.1%)</cell><cell>72.5%</cell><cell>73.8%</cell><cell>589</cell><cell>425</cell><cell>88.0% 5.2%</cell></row><row><cell cols="2">CD010023 983</cell><cell>52 (5.3%)</cell><cell>43.6%</cell><cell>43.8%</cell><cell>550</cell><cell>586</cell><cell>100.0% 8.9%</cell></row><row><cell cols="2">CD010386 627</cell><cell>2 (0.32%)</cell><cell>77.6%</cell><cell>82.6%</cell><cell>108</cell><cell>329</cell><cell>100.0% 0.6%</cell></row><row><cell cols="4">CD010783 10907 30 (0.28%) 75.3%</cell><cell>76.5%</cell><cell>2557</cell><cell cols="2">3430 100.0% 0.9%</cell></row><row><cell cols="2">CD010860 96</cell><cell>7 (7.3%)</cell><cell>50.3%</cell><cell>55.3%</cell><cell>41</cell><cell>42</cell><cell>100.0% 16.7%</cell></row><row><cell cols="2">CD010542 350</cell><cell>20 (5.7%)</cell><cell>4.8%</cell><cell>5.7%</cell><cell>327</cell><cell>280</cell><cell>90.0% 6.4%</cell></row><row><cell cols="2">CD008081 972</cell><cell>26 (2.7%)</cell><cell>41.6%</cell><cell>34.1%</cell><cell>638</cell><cell>532</cell><cell>96.2% 4.7%</cell></row><row><cell cols="4">CD010173 5497 23 (0.42%) 76.1%</cell><cell>78.9%</cell><cell>1156</cell><cell cols="2">1654 100.0% 1.4%</cell></row><row><cell cols="3">CD009925 6533 460 (7%)</cell><cell>52.0%</cell><cell>13.6%</cell><cell>5642</cell><cell cols="2">2027 86.7% 19.7%</cell></row><row><cell cols="3">CD009647 2787 56 (2%)</cell><cell>45.8%</cell><cell>23.6%</cell><cell>2128</cell><cell cols="2">1674 98.2% 3.3%</cell></row><row><cell>Average</cell><cell></cell><cell></cell><cell>47.2%</cell><cell>54.4%</cell><cell></cell><cell></cell><cell>93.3% 9.4 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,155.44,150.56,304.47,187.38"><head>Table 2 .</head><label>2</label><figDesc>Feature Effectiveness measured as MAP using only these features</figDesc><table coords="11,164.60,187.38,286.16,150.56"><row><cell>Feature name</cell><cell>Description</cell><cell>MAP</cell></row><row><cell>Abstract Features Title Features MeSH Heading Features</cell><cell>tf-idf, Gensim Similarity, Gensim Relevancy, Number of words</cell><cell>0.2612 0.2053 0.1386</cell></row><row><cell>Dynamic Features</cell><cell>tf-idf, Gensim Similarity</cell><cell>0.2319</cell></row><row><cell>Static Features</cell><cell>Gensim Relevancy, Number of words,</cell><cell>0.148</cell></row><row><cell></cell><cell>Language, Publication type</cell><cell></cell></row><row><cell>Overall</cell><cell></cell><cell>0.2866</cell></row><row><cell>Random</cell><cell></cell><cell>0.0479</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,155.44,428.50,304.47,186.55"><head>Table 3 .</head><label>3</label><figDesc>Feature Effectiveness measured as MAP using only these features</figDesc><table coords="11,208.81,465.32,197.73,149.73"><row><cell>Feature name</cell><cell>MAP MAP Gain</cell></row><row><cell>Gensim Similarity</cell><cell>0.1337 +0.0056</cell></row><row><cell>TD/IDF</cell><cell>0.2254 +0.0402</cell></row><row><cell>Metamap</cell><cell>0.2035 +0.0044</cell></row><row><cell cols="2">Gensim Relevancy Method 1 0.1439 +0.033</cell></row><row><cell cols="2">Gensim Relevancy Method 2 0.0724 -0.055</cell></row><row><cell>Gensim Relevancy Both</cell><cell>0.1444 +0.026</cell></row><row><cell>Language</cell><cell>0.0497 +0.0015</cell></row><row><cell>Publication Type</cell><cell>0.0986 +0.003</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,149.89,115.91,315.57,186.15"><head>Table 4 .</head><label>4</label><figDesc>Comparison of Stopping Criteria</figDesc><table coords="12,149.89,152.73,315.57,149.33"><row><cell>Model</cell><cell cols="3">Recall Precision F2-Score ∅CostU niRf ∅CostU ni</cell></row><row><cell>Fixed Rank</cell><cell>93.5% 6.2%</cell><cell>0.11067 2041</cell><cell>2041</cell></row><row><cell>Fixed Score</cell><cell>94.5% 7.7%</cell><cell>0.13041 1903</cell><cell>1903</cell></row><row><cell>Knee Method</cell><cell>89.0% 7.8%</cell><cell>0.13581 3179</cell><cell>2386</cell></row><row><cell>BMI Method</cell><cell>91.3% 6.8%</cell><cell>0.12064 2896</cell><cell>2106</cell></row><row><cell>S-D Classic</cell><cell>98.9% 5.7%</cell><cell>0.10011 2722</cell><cell>2722</cell></row><row><cell>S-D List Sampling</cell><cell>93.7% 8.5%</cell><cell>0.14137 1847</cell><cell>2095</cell></row><row><cell cols="2">S-D Uniform Sampling 94.8% 7.9%</cell><cell>0.13708 1830</cell><cell>1579</cell></row><row><cell cols="2">S-D Minimal Sampling 94.2% 9.2%</cell><cell>0.14978 1754</cell><cell>1529</cell></row><row><cell>Optimal</cell><cell>94.1% 11.0%</cell><cell>0.18045 1263</cell><cell>1263</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,656.80,202.46,7.86"><p>https://www.ncbi.nlm.nih.gov/books/NBK25500/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,645.84,87.04,7.86"><p>metamap.nlm.nih.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.73,656.80,195.23,7.86"><p>https://sourceforge.net/p/lemur/wiki/RankLib/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="16,142.96,344.40,337.63,7.86;16,151.52,355.36,329.07,7.86;16,151.52,366.32,264.24,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,361.52,344.40,119.07,7.86;16,151.52,355.36,204.67,7.86">Biometrika tables for statisticians vol. i, 3. edition. university press, cambridge</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ahrens</surname></persName>
		</author>
		<idno type="DOI">10.1002/bimj.19680100309</idno>
		<ptr target="https://doi.org/10.1002/bimj.19680100309" />
	</analytic>
	<monogr>
		<title level="j" coord="16,384.16,355.36,96.44,7.86">Biometrische Zeitschrift</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Pearson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Hartley</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="226" />
			<date type="published" when="1966">1966. 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,376.67,337.64,7.86;16,151.52,387.63,329.07,7.86;16,151.52,398.59,329.07,7.86;16,151.52,409.55,324.19,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,327.84,376.67,152.75,7.86;16,151.52,387.63,234.76,7.86">Where to stop reading a ranked list?: Threshold optimization using truncated score distributions</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,406.70,387.63,73.89,7.86;16,151.52,398.59,329.07,7.86;16,151.52,409.55,77.18,7.86;16,289.42,409.55,39.61,7.86">Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;09</note>
</biblStruct>

<biblStruct coords="16,142.96,419.91,337.63,7.86;16,151.52,430.87,239.79,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,215.94,419.91,264.65,7.86;16,151.52,430.87,88.17,7.86">Effective mapping of biomedical text to the umls metathesaurus: the metamap program</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,247.30,430.87,73.36,7.86">Proc AMIA Symp</title>
		<meeting>AMIA Symp</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,441.23,337.64,7.86;16,151.52,452.18,329.07,7.86;16,151.52,463.14,187.48,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,315.56,441.23,165.03,7.86;16,151.52,452.18,161.11,7.86">Seventy-five trials and eleven systematic reviews a day: How will we ever keep up?</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bastian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Glasziou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Chalmers</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pmed.1000326</idno>
		<ptr target="https://doi.org/10.1371/journal.pmed.1000326" />
	</analytic>
	<monogr>
		<title level="j" coord="16,314.92,452.18,60.16,7.86">PLoS Medicine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1000326</biblScope>
			<date type="published" when="2010-09">sep 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,473.50,45.32,7.86;16,211.95,473.50,10.88,7.86;16,246.50,473.50,33.80,7.86;16,303.96,473.50,35.48,7.86;16,363.11,473.50,23.54,7.86;16,410.33,473.50,18.43,7.86;16,452.43,473.50,28.16,7.86;16,151.52,484.46,166.72,7.86" xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1023/a:1010933404324</idno>
		<ptr target="https://doi.org/10.1023/a:1010933404324" />
	</analytic>
	<monogr>
		<title level="j" coord="16,246.50,473.50,33.80,7.86;16,303.96,473.50,35.48,7.86">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,494.82,337.64,7.86;16,151.52,505.78,329.07,7.86;16,151.52,516.74,329.07,7.86;16,151.52,527.69,173.12,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,211.26,494.82,269.33,7.86;16,151.52,505.78,299.14,7.86">Performance of support-vector-machine-based classification on 15 systematic review topics evaluated with the wss@95 measure: Table 1</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1136/jamia.2010.008177</idno>
		<ptr target="https://doi.org/10.1136/jamia.2010.008177" />
	</analytic>
	<monogr>
		<title level="j" coord="16,459.45,505.78,21.14,7.86;16,151.52,516.74,228.39,7.86">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">104</biblScope>
			<date type="published" when="2011-01">jan 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,538.05,337.63,7.86;16,151.52,549.01,329.07,7.86;16,151.52,559.97,274.25,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,303.98,538.05,176.61,7.86;16,151.52,549.01,112.51,7.86">Clef 2017 technologically assisted reviews in empirical medicine overview</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Spijker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,286.24,549.01,194.36,7.86;16,151.52,559.97,117.01,7.86">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation forum</title>
		<title level="s" coord="16,276.01,559.97,121.09,7.86">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,570.33,337.64,7.86;16,151.52,581.29,166.55,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,220.51,570.33,255.64,7.86">Greedy function approximation: A gradient boosting machine</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,151.52,581.29,78.48,7.86">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,591.65,337.63,7.86;16,151.52,602.60,329.07,7.86;16,151.52,613.56,329.07,7.86;16,151.52,624.52,242.54,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,151.52,613.56,242.49,7.86">Swift-review: a text-mining workbench for systematic review</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mav</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Holmgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E</forename><surname>Pelch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Rooney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Thayer</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13643-016-0263-z</idno>
		<ptr target="https://doi.org/10.1186/s13643-016-0263-z" />
	</analytic>
	<monogr>
		<title level="j" coord="16,401.07,613.56,79.53,7.86">Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,634.88,337.97,7.86;16,151.52,645.84,329.07,7.86;16,151.52,656.80,324.00,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,443.01,634.88,37.57,7.86;16,151.52,645.84,125.16,7.86">Clef 2017 ehealth evaluation lab overview</title>
		<author>
			<persName coords=""><forename type="first">Lorraine</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liadh</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">S A N A R E K R S J P</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,297.99,645.84,182.61,7.86;16,151.52,656.80,70.48,7.86">CLEF 2017 -8th Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="16,229.87,656.80,170.31,7.86">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.62,119.67,337.98,7.86;17,151.52,130.63,329.07,7.86;17,151.52,141.59,329.07,7.86;17,151.52,152.55,273.05,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,387.82,119.67,92.78,7.86;17,151.52,130.63,213.13,7.86">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="17,385.44,130.63,95.16,7.86;17,151.52,141.59,105.87,7.86">Neural and Information Processing System (NIPS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.62,163.51,337.98,7.86;17,151.52,174.47,329.07,7.86;17,151.52,185.43,329.07,7.86;17,151.52,196.39,163.92,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="17,348.60,163.51,132.00,7.86;17,151.52,174.47,329.07,7.86;17,151.52,185.43,118.33,7.86">Increased workload for systematic review literature searches of diagnostic tests compared with treatments: Challenges and opportunities</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="DOI">10.2196/medinform.3037</idno>
		<ptr target="https://doi.org/10.2196/medinform.3037" />
	</analytic>
	<monogr>
		<title level="j" coord="17,278.00,185.43,108.70,7.86">JMIR Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.62,207.34,337.97,7.86;17,151.52,218.30,311.63,7.86" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Hcw</forename><surname>De Vet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eisinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">I A B P D</forename></persName>
		</author>
		<title level="m" coord="17,324.29,207.34,137.05,7.86;17,151.52,218.30,282.96,7.86">Cochrane handbook for systematic reviews of diagnostic test accuracy</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Chapter 7: Searching for studies</note>
</biblStruct>

<biblStruct coords="17,142.62,229.26,337.98,7.86;17,151.52,240.22,329.07,7.86;17,151.52,251.18,329.07,7.86;17,151.52,262.14,226.61,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="17,312.58,229.26,168.01,7.86;17,151.52,240.22,329.07,7.86;17,151.52,251.18,78.42,7.86">Clinical evidence diagnosis: developing a sensitive search strategy to retrieve diagnostic studies on deep vein thrombosis: a pragmatic approach</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Greenley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Beaven</surname></persName>
		</author>
		<idno type="DOI">10.1046/j.1365-2532.2003.00427.x</idno>
		<ptr target="https://doi.org/10.1046/j.1365-2532.2003.00427.x" />
	</analytic>
	<monogr>
		<title level="j" coord="17,237.42,251.18,160.08,7.86">Health Information &amp; Libraries Journal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="150" to="159" />
			<date type="published" when="2003-08">aug 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.62,273.10,337.98,7.86;17,151.52,284.06,329.07,7.86;17,151.52,295.02,172.40,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="17,359.32,273.10,121.27,7.86;17,151.52,284.06,107.96,7.86">Adapting boosting for information retrieval measures</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10791-009-9112-1</idno>
		<ptr target="https://doi.org/10.1007/s10791-009-9112-1" />
	</analytic>
	<monogr>
		<title level="j" coord="17,269.83,284.06,89.89,7.86">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="254" to="270" />
			<date type="published" when="2009-09">sep 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.62,305.98,337.97,7.86;17,151.52,316.93,329.07,7.86;17,151.52,327.89,329.07,7.86;17,151.52,338.85,279.57,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="17,212.89,305.98,228.58,7.86">Adarank: A boosting algorithm for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/1277741.1277809</idno>
		<ptr target="http://doi.acm.org/10.1145/1277741.1277809" />
	</analytic>
	<monogr>
		<title level="m" coord="17,463.03,305.98,17.56,7.86;17,151.52,316.93,329.07,7.86;17,151.52,327.89,172.66,7.86;17,387.08,327.89,40.34,7.86">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="391" to="398" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;07</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
