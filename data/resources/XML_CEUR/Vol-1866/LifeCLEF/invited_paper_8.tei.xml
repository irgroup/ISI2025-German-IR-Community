<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,167.73,115.96,279.89,12.62">LifeCLEF Bird Identification Task 2017</title>
				<funder ref="#_xp3shkq">
					<orgName type="full">French CNRS</orgName>
				</funder>
				<funder>
					<orgName type="full">Xeno-Canto foundation</orgName>
				</funder>
				<funder ref="#_pZBACBW">
					<orgName type="full">EADM GDR CNRS MADICS</orgName>
				</funder>
				<funder>
					<orgName type="full">BRILAAM STIC-AmSud</orgName>
				</funder>
				<funder>
					<orgName type="full">Floris&apos;Tic</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.19,153.81,56.55,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<email>herve.goeau@cirad.fr</email>
							<affiliation key="aff0">
								<orgName type="department">IRD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,209.29,153.81,56.56,8.74"><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
							<email>glotin@univ-tln.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">LSIS UMR 7296</orgName>
								<orgName type="institution" key="instit1">Aix Marseille Univ</orgName>
								<orgName type="institution" key="instit2">Universit de Toulon</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">ENSAM</orgName>
								<orgName type="institution" key="instit5">IUF</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,276.40,153.81,90.66,8.74"><forename type="first">Willem-Pier</forename><surname>Vellinga</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Xeno-canto Foundation</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.62,153.81,68.94,8.74"><forename type="first">Robert</forename><surname>Planqué</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Xeno-canto Foundation</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.24,165.76,48.07,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.fr</email>
							<affiliation key="aff3">
								<orgName type="institution">Inria ZENITH team</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,167.73,115.96,279.89,12.62">LifeCLEF Bird Identification Task 2017</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">99AAFCBDC726194A9F9B77B08EF395A4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>bird</term>
					<term>song</term>
					<term>call</term>
					<term>species</term>
					<term>retrieval</term>
					<term>audio</term>
					<term>collection</term>
					<term>identification</term>
					<term>fine-grained classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
					<term>bioacoustics</term>
					<term>ecological monitoring 6 Scaled Acoustic Biodiversity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The LifeCLEF challenge BirdCLEF offers a large-scale proving ground for system-oriented evaluation of bird species identification based on audio recordings of their sounds. One of its strengths is that it uses data collected through Xeno-canto, the worldwide community of bird sound recordists. This ensures that BirdCLEF is close to the conditions of real-world application, in particular with regard to the number of species in the training set (1500). The main novelty of the 2017 edition of BirdCLEF was the inclusion of soundscape recordings containing time-coded bird species annotations in addition to the usual Xeno-canto recordings that focus on a single foreground species. This paper reports an overview of the systems developed by the five participating research groups, the methodology of the evaluation of their performance, and an analysis and discussion of the results obtained.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate knowledge of the identity, the geographic distribution and the evolution of bird species is essential for a sustainable development of humanity as well as for biodiversity conservation. The general public as well as professionals like park rangers, ecological consultants and of course the ornithologists themselves are potential users of an automated bird identifying system, typically in the context of wider initiatives related to ecological surveillance or biodiversity conservation. The LifeCLEF bird challenge BirdCLEF proposes to evaluate the state-of-the-art of audio-based bird identification systems at a very large scale. Before BirdCLEF started in 2014, three previous initiatives on the evaluation of acoustic bird species identification took place, including two from the SABIOD 6  group <ref type="bibr" coords="2,163.34,118.99,10.79,8.74" target="#b5">[6,</ref><ref type="bibr" coords="2,174.13,118.99,7.20,8.74" target="#b4">5,</ref><ref type="bibr" coords="2,181.32,118.99,7.20,8.74" target="#b0">1]</ref>. In collaboration with the organizers of these previous challenges, the <ref type="bibr" coords="2,152.02,130.95,67.55,8.74">BirdCLEF 2014</ref><ref type="bibr" coords="2,227.52,130.95,19.93,8.74">BirdCLEF , 2015</ref> and 2016 challenges went one step further by (i) significantly increasing the species number by an order of magnitude, (ii) working on real-world social data built from thousands of recordists, and (iii) moving to a more usage-driven and system-oriented benchmark by allowing the use of meta-data and defining information retrieval oriented metrics. Overall, these tasks were much more difficult than previous benchmarks because of the higher confusion risk between the classes, the higher background noise and the higher diversity in the acquisition conditions (different recording devices, contexts diversity, etc.). They therefore produced substantially lower scores and offered a better progression margin towards building real-world generalist identification tools. The main novelty of the 2017 edition of the challenge with respect to the previous years was the inclusion of soundscape recordings containing time-coded bird species annotations. Usually xeno-canto recordings focus on a single foreground species and result from using mono-directional recording devices. Soundscapes, on the other hand, are generally based on omnidirectional recording devices that monitor a specific environment continuously over a long period. This new kind of recording reflects (possibly crowdsourced) passive acoustic monitoring scenarios that could soon augment the number of collected sound recordings by several orders of magnitude. In this paper, we report the methodology of the performance evaluation as well as an analysis and a discussion of the results achieved by the 5 participating groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>As the soundscapes appeared to be very challenging in 2015 and 2016 (with an accuracy below 15%), new soundscape recordings containing time-coded bird species annotations were integrated in the test set (so as to better understand what makes state-of-the-art methods fail on such contents). This new data was specifically created for BirdCLEF thanks to the work of three people: Paula Caycedo Rosales (ornithologist from the Biodiversa Foundation of Colombia and Instituto Alexander von Humboldt, Xeno-canto recordist), Herv Glotin (bioaccoustician, co-author of this paper) and Lucio Pando (field guide and ornithologist). In total, about 6,5 hours of audio recordings were collected and annotated in the form of time-coded segments with associated species name. This dataset is composed of two main subsets:</p><p>Peru soundscapes, about 2 hours (1:57:08) 32 annotated segments: recorded in the summer of 2016 with the support of Amazon Explorama Lodges within the BRILAAM STIC-AmSud and SABIOD.org project. These recordings have been realized in the jungle canopy at 35 meters high (the highest point of the area), and at the level of the Amazon river, in the Peruvian basin. The recordings are sampled at 96 kHz, 24 bits PCM, stereo, dual -12 dB, using multiple systems: TASCAM DR, SONY PMC10, Zoom H1.</p><p>Colombia soundscapes, about 4,5 hours (4:25:55), 1990 annotated segments: These documents were annotated by Paula Caycedo Rosales, ornithologist from the Biodiversa Foundation of Colombia and an active Xeno-Canto member.</p><p>In addition to these newly introduced records, the test set still contained the 925 soundscapes and 8,596 single species recordings of BirdCLEF 2016 (collected by the members of Xeno-Canto<ref type="foot" coords="3,300.06,213.06,3.97,6.12" target="#foot_0">7</ref> network, see <ref type="bibr" coords="3,364.66,214.64,10.52,8.74" target="#b6">[7]</ref> for more details).</p><p>As for the training data, we consistently enriched the training set of the 2016 edition of the task, in particular to cover the species of the newly introduced time-coded soundscapes. Therefore, we extended the covered geographical area to the union of Brazil, Colombia, Venezuela, Guyana, Suriname, French Guiana, Bolivia, Ecuador and Peru, and collected all Xeno-Canto records in these countries. We then kept only the 1500 species having the most recordings so as to get sufficient training samples per species (48,843 recordings in total). The training set has a massive class imbalance with a minimum of four recordings for Laniocera rufescens and a maximum of 160 recordings for Henicorhina leucophrys. Recordings are associated to various metadata such as the type of sound (call, song, alarm, flight, etc.), the date, the location, textual comments of the authors, multilingual common names and collaborative quality ratings. All audio records are associated with various meta-data including the species name of the most active singing bird, the species of the other birds audible in the background, the type of sound (call, song, alarm, flight, etc.), the date and location of the observations (from which rich statistics on species distribution may be derived), some textual comments by the authors, multilingual common names and collaborative quality ratings. All of them were produced collaboratively by the Xeno-canto community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>Participants were asked to run their system so as to identify all the actively vocalising birds species in each test recording (or in each test segment of 5 seconds for the soundscapes). Up to 4 run files per participant could be submitted to allow evaluating different systems or system configurations (a run file is a formatted text file containing the species predictions for all test items). Each species had to be associated with a normalized score in the range [0, 1] reflecting the likelihood that this species is singing in the test sample. For each submitted run, participants had to signal if the run was performed fully automatically or with human assistance, and if they used a method based only on audio analysis only or with the use of the metadata.</p><p>Participants were asked to run their system so as to identify all the actively vocalising birds species in each test recording (or in each test segment of 5 seconds for the soundscapes). The submission run files had to contain as many lines as the total number of identifications, with a maximum of 100 identifications per recording or per test segment). Each prediction had to be composed of a species name belonging to the training set and a normalized score in the range [0, 1] reflecting the likelihood that this species is singing in the segment. The used evaluation metric used was the Mean Average Precision.</p><p>The evaluation metric used to compare the systems is the mean Average Precision (mAP) averaged across all queries, considering each audio file in the test set as a query and computed as:</p><formula xml:id="formula_0" coords="4,254.31,232.00,106.74,26.77">mAP = Q q=1 AveP (q) Q ,</formula><p>where Q is the number of test audio files and AveP (q) for a given test file q is computed as</p><formula xml:id="formula_1" coords="4,213.72,287.99,187.91,25.41">AveP (q) = n k=1 (P (k) × rel(k)) number of relevant documents .</formula><p>Here k is the rank in the sequence of returned species, n is the total number of returned species, P (k) is the precision at cut-off k in the list and rel(k) is an indicator function equaling 1 if the item at rank k is a relevant species (i.e. one of the species in the ground truth).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participants and methods</head><p>78 research groups registered for the BirdCLEF 2017 challenge. Five of them finally submitted run files and four of them submitted working notes describing their system. Details of the used methods and evaluated systems are synthesized below (by alphabetical order) and further developed in the working notes of the participants [9,4,10,3]:</p><p>Cynapse, Austria, 4 runs [3]: This system is based on a multi-modal deep neural network taking audio samples and metadata as input. The audio is fed into a convolutional neural network using four convolutional layers. The additionally provided metadata is processed using fully connected layers. The flattened convolutional layers and the fully connected layer of the metadata were joined and put into a large dense layer. For the sound pre-processing and data augmentation, they used a similar pipeline as the best system of BirdCLEF 2016 described in <ref type="bibr" coords="4,146.10,560.48,9.96,8.74" target="#b1">[2]</ref>. The two runs Cynapse Run 2 and 3 mainly differ in the FFT window size used for constructing the time-frequency representation passed as input to the CNN (respectively 512 and 256). Cynapse Run 4 is an average of Cynapse Run 2 and 3.</p><p>DYNI UTLN, France, 2 runs [10]: This system is based on an adaptation of the image classification model Inception V4 <ref type="bibr" coords="4,325.69,632.21,15.50,8.74" target="#b10">[11]</ref> extended with a time-frequency attention mechanism. The main steps of the processing pipeline are (i) the construction of a multi-scaled time-frequency representation passed as a RGB image to the Inception model, (ii) data augmentation: random hue, contrast, brightness, saturation, random crop in time and frequency domain and (iii) the training phase relying on transfer learning from the initial weights of the Inception V4 model (learned in the visual domain using the ImageNet dataset).</p><p>FHDO BCSG, Germany, 4 runs <ref type="bibr" coords="5,305.29,178.74,11.46,8.77" target="#b3">[4]</ref>: Like the DYNI UTLN team, these participants based his system on an adaptation of an image classification model, i.e Inception V3 <ref type="bibr" coords="5,206.74,202.68,14.61,8.74" target="#b11">[12]</ref>. Audio records were encoded through spectrograms and further processed by applying bandpass filtering, noise filtering, and silent region removal. For data augmentation purposes, they intended to use time shifting, time stretching, pitch shifting, and pitch stretching. Unfortunately, the data augmentation was not properly executed and the learned models suffered from overfitting problems. The first three runs differ in term of preprocessing, while the Run 3 is an average of the runs: Run 2 manipulates binary pictures and Run 4 uses grayscale pictures. Run 1 exploited the 3 RGB channels: the original grayscale picture in one channel, its blurred and sharpened versions for the two other channels.</p><p>TUCMI, Germany, 4 runs [9]: This system is also based on convolutional neural networks (CNN) but using more classical architectures than the Inception model used by DYNI UTLN. The main steps of the processing pipeline are (i) the construction of magnitude spectrograms with a resolution of 512x256 pixels, which represent five-second chunks of audio signal, (ii) data augmentation (vertical roll, Gaussian noise, Batch Augmentation) and (iii) the training phase relying on either a classical categorical loss with a softmax activation (TUCMI Run 1), or on a set of binary cross entropy losses with sigmoid activations as an attempt to better handle the multi-labeling scenario of the soundscapes (TUCMI Run 2). TUCMI Run 3 is an ensemble of 7 CNN models including the ones of Run 1 and Run 2. TUCMI Run 4 was an attempt to use geo-coordinates and time as a way to reduce the list of species to be recognized in the soundscapes recordings. Therefore, the occurrences of the eBird initiative were used complementary to the data provided within BirdCLEF. More precisely, only the 100 species having the most occurrences in the Loreto/Peru area for the months of June, July and August were kept in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Figure <ref type="figure" coords="5,166.09,596.34,4.98,8.74" target="#fig_0">1</ref> reports the performance measured for the 18 submitted runs. For each run (i.e. each evaluated system), we report the Mean Average Precision for the three categories of queries: traditional mono-directional recordings (the same as the one used in 2016), non time-coded soundscape recordings (the same as the one used in 2016) and the newly introduced time-coded soundscape recordings.</p><p>To measure the progress over last year, we also plot on the graph the perfor-mance of last year's best system <ref type="bibr" coords="6,278.69,118.99,9.96,8.74" target="#b1">[2]</ref>. It is remarked that all submitted runs were based on Convolutional Neural Networks (CNN) confirming the supremacy of this approach over previous methods (in particular the ones based on hand-crafted features which were performing the best until 2015). The best MAP of 0.71 (for the single species recordings) was achieved by the best system configuration of DYNI UTLN (Run 1). That rather similar to the MAP of 0.68 achieved last year by <ref type="bibr" coords="6,381.99,488.75,10.52,8.74" target="#b1">[2]</ref> but with 50% more species in the training set. Regarding the newly introduced time-coded soundscapes, the best system was also the one of DYNI UTLN (Run 1) whereas it did not introduce any specific features towards solving the multi-labeling issue. The main conclusions we can draw from the results are the following:</p><p>The network architecture plays a crucial role: Inception V4 that was known to be the state of the art in computer vision <ref type="bibr" coords="6,376.53,572.43,15.49,8.74" target="#b10">[11]</ref> also performed the best within the BirdCLEF 2017 challenge that is much different (time-frequency representations instead of images, a very imbalanced training set, mono-and multi-labeling scenarios, etc.). This shows that its architecture is intrinsically well-suited for a variety of machine-learning tasks across different domains. It also reveals a convergence of the methods to be used for machine learning tasks in the audio and the visual domain.</p><p>Ensembles of networks improve the performance consistently: This can be seen through Cynapse Run 4 and TUCMI Run 3 that outperform the other respective runs of these participants. The problem of such ensembles of networks is that their practical use in real-world applications is limited. They actually require a much higher GPU consumption so that their use in data intensive contexts is limited by cost issues. A promising solution towards this issue could be to rely on knowledge distilling <ref type="bibr" coords="7,329.44,190.72,9.96,8.74" target="#b7">[8]</ref>. Knowledge distilling consists in transferring the generalization ability of a cumbersome model to a small model by using the class probabilities produced by the cumbersome model as soft targets for training the small model. Alternatively, more efficient architectures and learning procedures should be devised.</p><p>The use of a multi-label loss function provides some improvements for soundscapes: The class-wise binary cross-entropy losses used in TUCMI Run 2 did allow a consistent performance gain on the time-coded soundscapes compared to the classical softmax loss in TUCMI Run 1. This was not enough to compensate the gain due to the Inception v4 architecture in the DYNI UTLN runs but we could expect a similar improvement with that architecture. Nevertheless, the multi-label loss function degrades the performance in the case of the traditional mono-directional recordings. Thus, it should be used only for records with many vocalizing birds such as the soundscapes.</p><p>The size of the FFT window used to construct the spectrograms plays an important role: This aspect was one of the main factor evaluated by the Cynapse team through their different runs. In particular, Cynapse Run 2 used a FFT window size of 512 whereas Cynapse Run 3 used a FFT window size of 256. As shown on Figure <ref type="figure" coords="7,244.80,437.23,3.87,8.74" target="#fig_0">1</ref>, the smaller FFT window size enables a considerable gain on the time-coded soundscapes probably because it reduces the overlap of different bird species within each chunk. On the other side, it degrades the performance on the traditional mono-directional recordings. On that content, a larger FFT window size helps recognizing the main foreground species.</p><p>Location-based and time-based species filtering is promising: TUCMI Run 4, that restricted the training set to the 100 most likely species according to the probability of eBird's occurrences did perform consistently worse than the other runs of TUCMI (N.B: only the time-coded soundscapes have to be considered here, i.e. the green bar in Figure <ref type="figure" coords="7,316.84,548.52,3.87,8.74" target="#fig_0">1</ref>). This reveals an unfitting selection of bird species. The period June-August in the Loreto/Peru area they used for selecting the most likely species is actually fitting only the Peruvian subset of the soundscapes not the Colombian one that is much larger. To better evaluate the benefit of the filtering strategy of TUCMI team, Table <ref type="table" coords="7,401.62,596.34,4.98,8.74" target="#tab_0">1</ref> provides the results of their submitted runs detailed by country. It shows that on the Peru's subset, the location-based and time-based filtering (Run 4) is very effective. On the other side, it degraded the performance on the Colombia subset because of the unfitting selection. Overall, we believe such location-based and time-based filtering is very promising for improving the performance. However, a finer and more accurate species distribution model should probably be used. Using occurrence data solely for learning species distribution model is actually often not possible because of strong sampling bias. Thus, in ecology, the prediction of the presence or absence of a given species at a given location, is usually based on environmental variables that characterize the environment encountered at that location (e.g. climatic data, topological data, occupancy data, etc.). All Colombia Peru TUCMI Run 3 0,144 0,146 0,026 TUCMI Run 1 0,099 0,101 0,003 TUCMI Run 2 0,119 0,121 0,007 TUCMI Run 4 0,061 0,059 0,158 Learning from metadata was not really conclusive: The attempt of Cynapse to use metadata as a context information passed to the neural network did not allow to outperform the purely audio-based runs of DYNI and TUCMI systems. However, as they used a less advanced network architecture it is difficult to conclude on the real benefit of metadata learning. A run without the use of metadata would have been required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presented the overview and the results of the LifeCLEF bird identification challenge 2017. The main outcome was that the best performing system was based on a purely image-based convolutional neural network architecture (Inception V4) applied to a standard time-frequency representation. This shows the convergence of the best performing methods whatever the targeted domain. As in many challenges, ensembles of networks also improved the performance consistently even if their practical use in real-world applications is still limited. Concerning the soundscapes-based passive monitoring scenario that was evaluated this year, few additional conclusions came out: (i) the performance improved over last year but remains globally low, (ii) few design considerations specific to that contents allow consistent improvements such as a lower FFT window size to construct the spectrograms or the use of a multi-label loss function instead of a softmax. Finally, it was shown by one of the participant that the use of location-based and time-based species filtering could be beneficial for a realworld monitoring device that would be fixed at a given place. Such approach is now facilitated by the huge volume of occurrences collected and shared by the eBird citizen science project. Even the raw spatial frequency of the occurrences gives a rather good estimate of the observable species at a given place. However, this might not help identifying the less abundant species that are often the ones that need a further follow-up.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,134.77,370.22,345.83,7.89;6,134.77,381.20,309.53,7.86;6,150.52,152.37,311.24,203.08"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. BirdCLEF 2017 results overview -Mean Average Precision. The orange dot line represents the last year's best system obtained by the CUBE system [2].</figDesc><graphic coords="6,150.52,152.37,311.24,203.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,134.77,222.61,345.82,20.69"><head>Table 1 :</head><label>1</label><figDesc>Results of TUCMI runs detailed by country (time-coded soundscapes only)</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_0" coords="3,144.73,657.44,179.37,7.47"><p>http://www.xeno-canto.org/contributors</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements The organization of the BirdCLEF task is supported by the <rs type="funder">Xeno-Canto foundation</rs> for nature sounds as well as the <rs type="funder">French CNRS</rs> project <rs type="projectName">SABIOD</rs>.<rs type="projectName">ORG</rs> and <rs type="funder">EADM GDR CNRS MADICS</rs>, <rs type="funder">BRILAAM STIC-AmSud</rs>, and <rs type="funder">Floris'Tic</rs>. The annotations of some soundscape were prepared with regreted wonderful <rs type="person">Lucio Pando</rs> at <rs type="person">Explorama Lodges</rs>, with the support of <rs type="person">Pam Bucur</rs>, <rs type="person">H. Glotin</rs> and <rs type="person">Marie Trone</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_xp3shkq">
					<orgName type="project" subtype="full">SABIOD</orgName>
				</org>
				<org type="funded-project" xml:id="_pZBACBW">
					<orgName type="project" subtype="full">ORG</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,258.15,337.64,7.86;9,151.52,269.11,329.07,7.86;9,151.52,280.07,329.07,7.86;9,151.52,291.03,127.58,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,388.47,258.15,92.13,7.86;9,151.52,269.11,329.07,7.86;9,151.52,280.07,84.33,7.86">The 9th mlsp competition: New methods for acoustic classification of multiple simultaneous bird species in noisy environment</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Raich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Eftaxias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,258.97,280.07,221.63,7.86;9,151.52,291.03,61.10,7.86">IEEE Workshop on Machine Learning for Signal Processing (MLSP)</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,301.99,337.64,7.86;9,151.52,312.95,300.61,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,352.53,301.99,128.06,7.86;9,151.52,312.95,154.28,7.86">Audio based bird species identification using deep learning techniques</title>
		<author>
			<persName coords=""><forename type="first">Elias</forename><surname>Sprengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,327.00,312.95,96.45,7.86">Working notes of CLEF</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,323.91,337.64,7.86;9,151.52,334.87,329.07,7.86;9,151.52,345.83,87.18,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,297.62,323.91,182.97,7.86;9,151.52,334.87,97.16,7.86">A multi-modal deep neural network approach to bird-song identication</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lidy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,268.97,334.87,211.62,7.86;9,151.52,345.83,58.51,7.86">Working Notes of CLEF 2017 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,356.78,337.63,7.86;9,151.52,367.74,329.07,7.86;9,151.52,378.70,58.51,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,309.26,356.78,171.33,7.86;9,151.52,367.74,65.74,7.86">Recognizing bird species in audio files using transfer learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fritzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,239.55,367.74,241.04,7.86;9,151.52,378.70,24.87,7.86">Working Notes of CLEF 2017 (Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,389.66,337.64,7.86;9,151.52,400.62,329.07,7.86;9,151.52,411.58,329.07,8.12;9,151.52,423.18,122.39,7.47" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,441.76,389.66,38.83,7.86;9,151.52,400.62,99.54,7.86">Bioacoustic challenges in icml4b</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Halkias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sueur</surname></persName>
		</author>
		<ptr target="http://sabiod.org/ICML4B2013_proceedings.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,290.27,400.62,190.32,7.86;9,151.52,411.58,62.74,7.86">Proc. of 1st workshop on Machine Learning for Bioacoustics</title>
		<meeting>of 1st workshop on Machine Learning for Bioacoustics<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,433.50,337.64,7.86;9,151.52,444.46,329.07,7.86;9,151.52,455.41,329.07,7.86;9,151.52,466.37,252.29,8.12" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,277.38,433.50,203.21,7.86;9,151.52,444.46,34.95,7.86">Overview of the 2nd challenge on acoustic bird classification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bas</surname></persName>
		</author>
		<ptr target="http://sabiod.univ-tln.fr/nips4b" />
	</analytic>
	<monogr>
		<title level="m" coord="9,207.81,444.46,272.78,7.86;9,151.52,455.41,34.46,7.86">Proc. Neural Information Processing Scaled for Bioacoustics. NIPS Int. Conf</title>
		<meeting>Neural Information essing Scaled for Bioacoustics. NIPS Int. Conf<address><addrLine>Halkias X., USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,477.33,337.63,7.86;9,151.52,488.29,160.42,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,398.20,477.33,82.39,7.86;9,151.52,488.29,64.20,7.86">Lifeclef bird identification task 2016</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,237.07,488.29,46.21,7.86">CLEF 2016</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,499.25,337.63,7.86;9,151.52,510.21,159.05,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,294.75,499.25,181.51,7.86">Distilling the knowledge in a neural network</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.96,521.17,337.63,7.86;9,151.52,532.13,329.07,7.86;9,151.52,543.09,74.87,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,168.54,532.13,293.34,7.86">Large-scale bird sound classification using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,543.09,46.21,7.86">CLEF 2017</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,554.04,337.98,7.86;9,151.52,565.00,329.07,7.86;9,151.52,575.96,235.14,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,306.26,554.04,174.33,7.86;9,151.52,565.00,247.39,7.86">Audio bird classification with inception-v4 extended with time and time-frequency attention mechanisms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sevilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bessonne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,420.63,565.00,59.96,7.86;9,151.52,575.96,201.49,7.86">Working Notes of CLEF 2017 (Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,586.92,337.98,7.86;9,151.52,597.88,329.07,7.86;9,151.52,608.84,25.60,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<title level="m" coord="9,343.86,586.92,136.74,7.86;9,151.52,597.88,189.48,7.86">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.62,619.80,337.97,7.86;9,151.52,630.76,329.07,8.12;9,151.52,642.36,122.39,7.47" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="9,404.33,619.80,76.26,7.86;9,151.52,630.76,167.55,7.86">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno>CoRR abs/1512.00567</idno>
		<ptr target="http://arxiv.org/abs/1512.00567" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
