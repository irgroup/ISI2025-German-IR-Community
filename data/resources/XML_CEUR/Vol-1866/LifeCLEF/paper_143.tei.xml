<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,165.62,152.67,263.84,12.64;1,202.25,170.67,190.37,12.64">Large-Scale Bird Sound Classification using Convolutional Neural Networks</title>
				<funder ref="#_en3SCWU">
					<orgName type="full">German Federal Ministry of Education and Research</orgName>
				</funder>
				<funder>
					<orgName type="full">European Union</orgName>
				</funder>
				<funder>
					<orgName type="full">European Social Fund for Germany</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,135.02,209.58,47.38,8.96"><forename type="first">Stefan</forename><surname>Kahl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Chemnitz</orgName>
								<address>
									<addrLine>Straße der Nationen 62</addrLine>
									<postCode>09111</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.73,209.58,93.65,8.96"><forename type="first">Thomas</forename><surname>Wilhelm-Stein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Chemnitz</orgName>
								<address>
									<addrLine>Straße der Nationen 62</addrLine>
									<postCode>09111</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.61,209.58,66.98,8.96"><forename type="first">Hussein</forename><surname>Hussein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Chemnitz</orgName>
								<address>
									<addrLine>Straße der Nationen 62</addrLine>
									<postCode>09111</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.85,209.58,57.38,8.96"><forename type="first">Holger</forename><surname>Klinck</surname></persName>
							<email>holger.klinck@cornell.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Bioacoustics Research Program</orgName>
								<orgName type="laboratory" key="lab2">Cornell Lab of Ornithology</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<addrLine>159 Sapsucker Woods Road</addrLine>
									<postCode>14850</postCode>
									<settlement>Ithaca</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,433.49,209.58,26.68,8.96;1,204.53,221.22,37.00,8.96"><forename type="first">Danny</forename><surname>Kowerko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Chemnitz</orgName>
								<address>
									<addrLine>Straße der Nationen 62</addrLine>
									<postCode>09111</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.87,221.22,46.34,8.96"><forename type="first">Marc</forename><surname>Ritter</surname></persName>
							<email>ritter@hs-mittweida.de</email>
							<affiliation key="aff2">
								<orgName type="institution">Hochschule Mittweida</orgName>
								<address>
									<addrLine>Technikumplatz 17</addrLine>
									<postCode>09648</postCode>
									<settlement>Mittweida</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.55,221.22,65.70,8.96"><forename type="first">Maximilian</forename><surname>Eibl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Chemnitz</orgName>
								<address>
									<addrLine>Straße der Nationen 62</addrLine>
									<postCode>09111</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,165.62,152.67,263.84,12.64;1,202.25,170.67,190.37,12.64">Large-Scale Bird Sound Classification using Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EDA3D4B8C8F7BF5796385B69173BCA8A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bioacoustics</term>
					<term>Large-Scale Classification</term>
					<term>Convolutional Neural Networks</term>
					<term>Audio Features</term>
					<term>Bird Sound Identification</term>
					<term>BirdCLEF 2017</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Identifying bird species in audio recordings is a challenging field of research. In this paper, we summarize a method for large-scale bird sound classification in the context of the LifeCLEF 2017 bird identification task. We used a variety of convolutional neural networks to generate features extracted from visual representations of field recordings. The BirdCLEF 2017 training dataset consist of 36.496 audio recordings containing 1500 different bird species. Our approach achieved a mean average precision of 0,605 (official score) and 0,687 considering only foreground species.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivation</head><p>Identifying bird species based on their calls, songs and sounds in audio recordings is an important task in wildlife monitoring for which the annotation is time consuming if done manually. With the arrival of convolutional neural networks (CNNs, ConvNets), automated processing of field recordings made a huge leap forward <ref type="bibr" coords="1,401.95,601.31,10.69,8.96" target="#b0">[1]</ref>. Nonetheless, processing large datasets containing hundreds of different classes is still very challenging. In the past years, many ground breaking CNN architectures evolved from evaluation campaigns such as TREC, CLEF or the ILSVRC <ref type="bibr" coords="1,369.31,637.31,11.26,8.96" target="#b1">[2]</ref>[3] <ref type="bibr" coords="1,391.83,637.31,11.26,8.96" target="#b3">[4]</ref>. Adapting those architectures for the purpose of audio event detection has become a common practice despite the very different domains of image and audio inputs. Generating deep features based on visual representations of audio recordings has proven to be very effective when applied to the classification of audio events such as bird sounds <ref type="bibr" coords="1,422.71,685.34,22.32,8.96">[5][6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Dataset</head><p>The BirdCLEF 2017 <ref type="bibr" coords="2,213.29,170.22,23.31,8.96">[7][8]</ref> training data is built from the Xeno-Canto collaborative database <ref type="foot" coords="2,159.14,180.03,3.24,5.83" target="#foot_0">1</ref> and contains 36.496 sound recordings with a total number of 1500 species (50% increase from the 2016 dataset). Most audio files are sampled at 44.1 kHz, 16 bits, mono and show a wide variety of recording quality, run length, bird count and background noise. The training set has a massive class imbalance with a minimum of four recordings for Laniocera rufescens and a maximum of 160 recordings for Henicorhina leucophrys. The training data is complemented with XML-files containing metadata such as foreground and background species, user quality ratings, time and location of the recording and author name and notes. We did not make use of any of the additional metadata except for the class id of foreground species. The presence of numerous background species distorts the training data and makes single label training particularly challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Workflow</head><p>Our workflow consists of four main steps. First, we extract spectrograms from all audio recordings. Secondly, we extend our training set through extensive dataset augmentation. Next, we try to find the best CNN architecture with respect to number of classes, sample count and data diversity. Finally, we train our models using consumer hardware and Open Source toolkits and frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generating Spectrograms</head><p>We decided to use magnitude spectrograms with a resolution of 512x256 pixels, which represent five-second chunks of audio signal. This (relatively large) input size is computationally expensive when training ConvNets but our experiments showed that high resolution spectrograms contain more valuable details and the overall classification performance benefits from larger inputs.</p><p>We extracted five-second spectrograms for each sound recording using a foursecond overlap, which resulted in 940.740 images. We implemented a heuristic to decide whether a signal chunk contains bird sounds or background noise only. We mainly adapted the approach of <ref type="bibr" coords="2,255.27,565.31,11.74,8.96" target="#b0">[1]</ref> and <ref type="bibr" coords="2,287.19,565.31,11.74,8.96" target="#b8">[9]</ref> and removed spectrograms with improper signal to noise ratio. Figure <ref type="figure" coords="2,239.16,577.31,4.52,8.96">1</ref>-3 visualize this process. We selected 869 spectrograms containing heavy background noise and no bird sounds for our dataset augmentation process.</p><p>Despite this method for signal and noise separation, the training data remains distorted. The classification error depends on clean, distinct classes, which is almost impossible to achieve if done automatically. Species present in the audio recordings are not time coded. Therefore, background species might interfere with feature learn-ing, especially for species with only few training samples. The amount of training samples greatly influences the generalization error. More samples significantly improve the detection rate; we noticed that classification of species with more than 1000 spectrograms performed best. Class imbalance affects generalization as well. We tried different techniques like cost-sensitive learning to counter this circumstance but noticed that those methods did not lead to a higher mean average precision. However, reducing class imbalances seems to benefit real world applications focused on rare species. Fig. <ref type="figure" coords="3,142.70,422.06,3.35,8.10">1</ref>. Magnitude spectrogram after signal preemphasis and value normalization. We use the framework "python_speech_features"<ref type="foot" coords="3,262.13,431.19,3.00,5.40" target="#foot_1">2</ref> for FFT with a window length of 0.05 and step size of 0.0097 for five-second chunks of the signal. We use a FFT length of 840 and crop high frequencies, which reduces the input size and is sufficient for most bird species. We did not perform any noise reduction. Fig. <ref type="figure" coords="3,142.34,636.16,3.35,8.10">2</ref>. Processed spectrogram after median blur, median threshold, spot removal and morphological closing. This approach suppresses background noise and highlights actual bird sounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3.</head><p>We only analyze the low frequency crop of the processed spectrogram (top) and decide whether it contains signal or noise (bottom). We use the sum of all rows containing signal (black) as threshold for bird presence or absence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset Augmentation</head><p>Dataset augmentation is vital to reduce the generalization error. However, most established augmentation methods like horizontal flip and random crop are not suitable for spectrograms as they might mask the original signal. Dataset augmentation should always target the properties of the test data, which are underrepresented or missing in the training data. We evaluated different augmentation methods using a local validation set consisting of ~50.000 samples from 100 species. We incorporated the following augmentations into our final runs:</p><p>Vertical Roll: Following <ref type="bibr" coords="4,240.60,461.25,11.76,8.96" target="#b0">[1]</ref> we implemented a random, pitch shifting vertical roll of maximum five percent, which had great impact on the generalization error. This seems to be by far the most beneficial dataset augmentation. We tried time shifting horizontal roll as well, but found that this augmentation harms generalization. This might be because we generated overlapping spectrograms and with that, already used time shifted spectrograms.</p><p>Gaussian Noise: Synthetic noise often helps convolutional neural networks to focus on salient image features. Most models will learn to ignore the noise over time, which makes them robust even against other (more realistic) noise sources. We simply added Gaussian noise of random intensity to our spectrograms and re-normalized the resulting images.</p><p>Noise Samples: In addition to random Gaussian noise, we added noise samples (spectrograms our heuristic rejected as bird sounds) which significantly improves the classification result and speeds up the entire training process. Most of the sound recordings show similar noise patterns; we tried to counter these patterns with the selection of 869 noisy spectrograms and randomly added them to our training images.</p><p>Batch Augmentation: Most sound recordings contain more than one bird species, which may vocalize at the same time. We tried to simulate this by randomly combining spectrograms of the same batch. Combining samples of the same class will not affect label distribution, whereas the combination of samples of different species results in multi-label targets that can be used to train sigmoid outputs.</p><p>We applied all augmentations at runtime, during training using CPU idle time. We implemented a multi-threaded batch loader, which significantly speeds up training. Our batch loader operates during a forward-backward pass iteration executed on the GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CNN Architecture</head><p>Finding the best CNN architecture is a time consuming task and often done purely by intuition. Current state-of-the-art approaches try to tackle this issue with automated hyperparameter search <ref type="bibr" coords="5,221.93,332.25,15.43,8.96" target="#b9">[10]</ref>. We decided to reduce the amount of possible design decisions and relied on current best practices for CNN layouts. All weighted layers (except for input and output layers) use Batch Normalization <ref type="bibr" coords="5,371.77,356.25,15.48,8.96" target="#b10">[11]</ref>, Exponential Linear Units (ELU) for unit activation <ref type="bibr" coords="5,258.17,368.25,16.76,8.96" target="#b11">[12]</ref> and are initialized using He-initialization <ref type="bibr" coords="5,451.42,368.25,15.42,8.96" target="#b12">[13]</ref>.</p><p>We wanted large receptive fields in our first convolutional layers, which have proven to be very effective for spectrograms during our experiments. We use filter sizes of 7x7 and 5x5 for larger inputs and 3x3 kernels for smaller input sizes in deeper layers.</p><p>Table <ref type="table" coords="5,151.55,416.25,4.98,8.96" target="#tab_0">1</ref> provides an overview of the three model designs we used for our submission.</p><p>Although the BirdCLEF classification task with 1500 classes, class imbalances and a distorted dataset is rather complex, shallow CNN architectures with classic layouts and only a few layers seem to be more effective than more complex highway networks with multiple tens of layers like DenseNet <ref type="bibr" coords="5,323.67,476.27,16.80,8.96" target="#b13">[14]</ref> or ResNet <ref type="bibr" coords="5,385.81,476.27,15.47,8.96" target="#b14">[15]</ref>. We tried different implementations of state-of-the-art convolutional networks but found them inferior to our simple CNN architectures. This might be due to the fact, that the image domain of spectrograms is very homogenous despite more than 1500 different signal types. Most spectrograms contain only little information, leaving most pixels blank. This observation is backed by the works of <ref type="bibr" coords="5,299.07,536.27,11.31,8.96" target="#b0">[1]</ref>[5] <ref type="bibr" coords="5,321.68,536.27,11.31,8.96" target="#b5">[6]</ref>.</p><p>Large input sizes are not common in current image classification publications. Most approaches reduce the input size to a maximum of 256x256 pixels. Current consumer GPUs are well suited for larger inputs. On the other hand, models with large input sizes are considerably harder to train and tune, training takes significantly more time and larger inputs do not always benefit generalization. Our experiments showed that non-square, high-resolution inputs of spectrograms do indeed achieve better classification results especially for large and diverse datasets. We used strided convolutions and pooling layers to cope with large inputs.</p><p>Additionally, a larger number of filters seems to be more effective than a larger number of hidden units. We found that 512 units per dense layer is sufficient even for 1500 classes. Determining the right amount of network parameters is crucial to avoid under-and overfitting. This process is also very time consuming considering the fact that less parameters might work well on small validation sets but usually underfit on larger datasets. Validation experiments should always show slight overfitting in order to have good generalization capacity when trained on more classes. Even though, models with a large number of weights did eventually overfit during our experiments with 1500 classes, so we decided to dial down the weight count. Most recent approaches at well-known evaluation campaigns use CNN ensembles to achieve their best classification results. Separate predictions are combined (bagging and boosting) to form the final ranking. We trained 19 convolutional neural networks and selected seven of them for our ensemble submission. Ensembles may not be ap-plicable for real world tasks such as real-time wildlife monitoring but effectively boost the overall classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>Time efficient training becomes crucial when training on 1500 classes with more than 940.000 samples. We tried to optimize our training process in order to save computation time and maintain a good overall performance at the same time. We evaluated different kinds of parameter settings and found the following to be very effective:</p><p>Learning Rate Schedule: The learning rate is one of the most important hyperparamters when training ConvNets. Fixed learning rates may hinder the optimization process from converging. Common practice uses learning rate steps, which reduce the learning rate on various occasions during training. Although batch normalization allows for larger learning rates, in order to achieve full convergence of the learning process, parameter changes have to be minimal near the end of training. We found that linear interpolation of the learning rate during training, with changes applied after each epoch, are very effective and can dramatically improve the classification result. We started our training process with a learning rate of 0.01 and decreased it over 55 epochs to a value of 0.00001.</p><p>Optimizer: Choosing the best optimizer for stochastic gradient descent parameter updates is vital for fast optimization convergence. We decided to use ADAM updates <ref type="bibr" coords="7,124.70,428.25,16.76,8.96" target="#b15">[16]</ref> (with the beta1 parameter set to 0.5) because of the high convergence speed the algorithm provides. In combination with our dynamic learning rate (which is still beneficial despite the adaptive nature of the optimizer), we achieved a significant speed-up compared to the Nesterov momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss function:</head><p>We use categorical cross entropy and binary cross entropy as loss functions for single and multi-label scenarios. We applied L2 regularization with a weight of 0.0001. Additionally, we experimented with different kinds of costsensitive loss functions, which increase the loss for misclassifications of rare species. Massive class imbalances may lead to a good overall classification accuracy just because of the dominance of single species. Incorporating class probability distributions into the loss function counters this effect if added to the loss alongside cross entropy and L2 distance (higher penalty if class is less probable). For the BirdCLEF 2017 challenge, this method turned out to be ineffective, but we observed a very clean confusion matrix for rare species, which might indicate a real world application of this approach.</p><p>Pre-trained Models: Re-using already trained models for new training iterations can cut the computation time needed until convergence by a great margin. Softmax classifier tend to be much more efficient when training ConvNets. Therefore, we trained models with single label outputs and used these pre-trained models as starting point for our multi-label scenarios with sigmoid outputs. Doing that, we were able to skip 20-30 epochs of training time per model. Some of our ensemble models were trained on different subsets of the training data. We made use of a pre-trained model every time we switched to new subsets. Batch Size: Increasing the size of batches for the training process is beneficial mostly due to the use of batch normalization. Smaller batches lead to more iterations per epoch and tend to perform better after the first few epochs. In the end, larger batches seem to provide better generalization. Choosing the best batch size always depends on the amount of VRAM the GPU provides. We had to set the batch size to 128, which was the largest we could fit in memory for all models, mainly constraint by the large receptive fields we used in the first layers of our ConvNets.</p><p>The implementation of our code is done purely in Python using NumPy, Theano <ref type="bibr" coords="8,454.18,294.21,16.52,8.96" target="#b16">[17]</ref> and Lasagne <ref type="bibr" coords="8,179.54,306.21,16.79,8.96" target="#b17">[18]</ref> for models, objectives and solvers, OpenCV for image processing, scikit-learn for metrics and Matplotlib for visualization. We did all of our experiments on a single PC with a NVIDIA Titan X graphics card. We switched to a NVIDIA P6000 GPU for the training of our final models, which provides 24GB of VRAM and two times faster training.</p><p>We used a local validation split of five percent of the training spectrograms to monitor the training process and limited the total number of samples per class to 1500. Training took between 15h and 80h per model on all 1500 classes and ~4h for our 100 class experimental models. We trained every model for 55 epochs and used early stopping to find the best parameter setting. Some models showed their best performance after 55 epochs, which indicates that longer training periods may have been beneficial. However, we did not proceed training these models due to time constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Performance on Local Test Set</head><p>We used a local test split of the given training data to evaluate our ConvNets after training. Therefore, we randomly separated 10% of all recordings (at least one file per species) for our test set. Our test set reflects the dataset distribution of species relatively well and results are comparable to the official scores. The local test data contains 3557 recordings of varying recording quality and length.</p><p>We used fixed random seeds and trained every CNN for 55 epochs, selecting the best performing snapshot according to the validation loss on a 5% validation split of the input spectrograms. Table <ref type="table" coords="8,254.06,635.27,4.98,8.96" target="#tab_4">2</ref> shows selected results of more than 100 different experiments, which we conducted. Due to time constraints, we did not manage to test all possible hyperparameter and dataset augmentation combinations, especially for our DenseNet and ResNet architectures. However, our experiments indicated that classic, carefully tuned CNN layouts outperform highway networks (our most com-petitive model was a DenseNet-32). On the other hand, CNNs with shortcuts need significantly less parameters and usually scale with increasing parameter count. There might still be a lot of potential laying in those architectures if carefully crafted. We selected the best models based on the results of our local test set evaluation for our submission. Additionally, we selected seven ConvNets for an ensemble. Predictions were pooled only by averaging the probabilities for every species based on the prediction for all five-second spectrograms of every recording. We tried numerous prediction pooling strategies like linear interpolation, thresholds or dilation but found none of them outperforming simple average pooling. However, fine-tuning the prediction process can lead to significantly better results for the same tested model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Official Scores</head><p>We submitted four runs, each of them pursuing a different strategy. Our submission contains the result of two single models (Run 1&amp;2) and the predictions of two ensembles (Run 3&amp;4). All runs are fully automatic with no manual interference. Only Run 4 uses additional metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TUCMI Run 1:</head><p>This run was composed of the predictions of a single model (Model 1, see Table <ref type="table" coords="9,211.80,645.47,4.20,8.96" target="#tab_0">1</ref>) with softmax activations to demonstrate our best performing model on a single label task. Prediction took an average of 833ms per sample recording (on a P6000 GPU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TUCMI Run 2:</head><p>We used the fully trained net from our first run as pre-trained model for this attempt of a multi label predictive CNN with sigmoid activations. We used batch augmentation with an average of two labels per sample of each batch to simulate simultaneously vocalizing bird species. Expectedly, this net did not score as good as our first model due to the distorted training set, which makes multi label predictions very challenging. It performed slightly better in the soundscape domain, which was the focus of this attempt. However, we expected a significant difference between both runs for the soundscape recordings, which was not the case. Prediction took an average of 950ms per sample recording. TUCMI Run 3: Ensembles of CNNs are widely used in evaluation campaigns such as TREC or CLEF. Despite their lack of real world application, ensembles often score best, which is also the case for our seven-model ensemble. Bagging predictions benefits from models trained on different portions of the training data. We decided to train four models on species containing up to 300, 500, 1000 and 2000 training samples (Model 3), one model trained on 256x128 pixel spectrograms (Model 2) and both models of our first two runs. This run is our best performing attempt; prediction took an average of 6s per sample recording due to sequential testing. TUCMI Run 4: Dedicated models tend to perform better if the number of expected audio events is fixed. We tried to estimate the most probable bird species present in the soundscape recordings based on the given geo-coordinates and the corresponding eBird frequency bar charts for the months of June, July and August. We ranked species based on the probability of occurrence in the Loreto/Peru area and trained a second ensemble for 100 selected species with different CNN layouts and multi label predictions. This is our only metadata assisted run, focused solely on soundscape prediction and performed similar to our models trained on 1500 species. Prediction took an average of 4s per sample recording due to sequential testing. Table <ref type="table" coords="10,149.30,498.04,3.41,8.10">3</ref>. Official scores (Mean average precision) of our submitted runs. Our ensemble scored better in all categories than our other runs; our dedicated model trained on selected species performed nearly as good as the models trained on all 1500 species for soundscape recordings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Additional Scores</head><p>The soundscape domain with multiple birds vocalizing at the same time, diverse and noisy backgrounds and, most importantly, no explicit training data is by far the most challenging test set. We tried to tackle these difficulties with a dedicated model ensemble, trained on specific bird species only. Considering the overall results for the 2017 time-coded soundscapes, our Run 4 did not perform as expected. However, additional evaluation results kindly provided by the organizers (Table <ref type="table" coords="11,413.22,174.18,4.19,8.96" target="#tab_6">4</ref>) show, how important the selection of the right bird species for neural net training can be. This is important for future real world applications of wildlife monitoring. The results verify that dedicated models specialized for the identification of bird species of a specific region outperform general models trained for the detection of a wide variety of bird species. This basically implies two detection strategies: Either limiting the bird species during the training of dedicated models or using probability measures based on species appearance for general models to refine classification results. The second option seems to be the most flexible, allowing for the adaption of one model to multiple scenarios such as changing seasons or the relocation of monitoring systems without the need to train a new model. Future experiments will have to show whether both methods perform equally good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Source Code</head><p>We made a refined and commented version of our source code alongside detailed instructions publicly available on GitHub<ref type="foot" coords="11,291.29,532.52,3.24,5.83" target="#foot_2">3</ref> . This repository enables everyone to reproduce our submissions, to train own models and evaluate results. We added our selected noise samples and a pre-trained model from our first run. We will keep the repository updated and will add some functionality for demo applications in the future. If you have any questions or remarks regarding the source code, please do not hesitate to contact us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future Work</head><p>There are a number of techniques that we think might help to improve bird sound classification upon our current results. Aside from better-crafted and tuned ConvNet architectures, extensive dataset augmentation and more training time, we would like to assess the following methods:</p><p>Reducing dataset distortion: A clean dataset with sharp classes is vital especially for multi label tasks like soundscape recordings. While this task could be done manually, we propose a more efficient way using neural nets trained to distinguish between noise and bird sounds. The excellent Warblr and FreeSound datasets <ref type="foot" coords="12,402.55,270.99,3.24,5.83" target="#foot_3">4</ref> provide several tens of thousands of samples for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D-Convolutions:</head><p>Mapping audio signal chunks to images via FFT is very effective but does not fully account for the sequential nature of continuous signals. With the rise of 3D-Convolutions <ref type="bibr" coords="12,239.69,333.21,15.43,8.96" target="#b18">[19]</ref>, we could think of sequence preserving image inputs like sequential stacks of spectrograms. Every input signal is split into chunks of 30 or more seconds, each second encoded as spectrum. All spectrograms of such a chunk form a 3D input (actually 5D: batch size, channels, stack size, width, height) which contains valuable information concerning bird sound occurrences over time. This approach will likely reduce the dataset distortion for birds with single calls in long time spans as well.</p><p>Snapshot Ensembles: Pooling the predictions of multiple CNNs is important for top scoring results in evaluation campaigns. Training ensembles is very time consuming and requires different datasets and/or network architectures. Snapshot Ensembles <ref type="bibr" coords="12,124.70,465.23,16.76,8.96" target="#b19">[20]</ref> try to reduce the amount of training time needed for an ensemble by using repeating learning rate cycles, which lead to independently converged models using the same dataset and architecture. Benchmarks show that those ensembles outperform state-of-the-art model architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We provided insights into our attempt of large-scale bird sound classification using various convolutional neural networks. After we conducted numerous experiments to identify the best techniques of dataset augmentation, training methods and network architectures, our best submission to the 2017 BirdCLEF challenge achieved a score of 0,605 MAP ranking second of all submissions. The results show that there is still a lot of room for improvements especially for the soundscape domain, which likely is the most important real-world application. Additionally, we provide a GitHub repository for the free use of our code base and with that, hope to offer a baseline for future BirdCLEF tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,124.70,245.99,346.04,242.03"><head>Table 1 .</head><label>1</label><figDesc>Model architectures used for our submitted runs. Every convolutional and every dense layer (except the final output layer) has batch normalization before its ELU activations, is Heinitialized and has shape preserving padding. Model 1 is our main model, used most in our runs. Model 2 and 3 are part of our CNN ensemble; Model 2 showed best generalization after few epochs but needs significantly more time to train than Model 1; Model 3 is used for the classification of subsets of the training data with a maximum of 500 classes. Shallow models perform better on small class selections and can be combined in ensembles if trained with sigmoid outputs.</figDesc><table coords="6,128.90,342.62,325.43,145.40"><row><cell>Model 1</cell><cell>Model 2</cell><cell>Model 3</cell></row><row><cell>8 weighted Layers</cell><cell>9 weighted Layers</cell><cell>8 weighted Layers</cell></row><row><cell>~4000s per Epoch</cell><cell>~5500s per Epoch</cell><cell>~1000s per Epoch</cell></row><row><cell>Conv1, 64x7x7, Stride 2</cell><cell>Conv1, 32x7x7, Stride 1</cell><cell>Conv1, 32x7x7, Stride 2</cell></row><row><cell>MaxPooling, Size 2</cell><cell>MaxPooling, Size 2</cell><cell>MaxPooling, Size 2</cell></row><row><cell></cell><cell>Conv2, 32x5x5, Stride 1</cell><cell></cell></row><row><cell></cell><cell>MaxPooling, Size 2</cell><cell></cell></row><row><cell>Conv2, 128x5x5, Stride 1</cell><cell>Conv3, 64x5x5, Stride 1</cell><cell>Conv2, 128x5x5, Stride 1</cell></row><row><cell>MaxPooling, Size 2</cell><cell>MaxPooling, Size 2</cell><cell>MaxPooling, Size 2</cell></row><row><cell>Conv3, 256x3x3, Stride 1</cell><cell>Conv4, 128x3x3, Stride1</cell><cell>Conv3, 256x3x3, Stride 1</cell></row><row><cell>MaxPooling, Size 2</cell><cell>MaxPooling, Size 2</cell><cell>MaxPooling, Size 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,128.90,494.32,325.43,22.14"><head>Conv4, 512x3x3, Stride 1 Conv5, 512x3x3, Stride 1 Conv4, 512x3x3, Stride 1</head><label></label><figDesc></figDesc><table coords="6,128.90,508.36,300.98,8.10"><row><cell>MaxPooling, Size 2</cell><cell>MaxPooling, Size 2</cell><cell>MaxPooling, Size 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,128.90,522.76,325.43,22.02"><head>Conv5, 1024x3x3, Stride 1 Conv6, 1024x3x3, Stride 1 Conv5, 512x3x3, Stride 1</head><label></label><figDesc></figDesc><table coords="6,128.90,536.68,300.98,8.10"><row><cell>MaxPooling, Size 2</cell><cell>MaxPooling, Size 2</cell><cell>MaxPooling, Size 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,128.90,551.20,321.58,78.78"><head>DenseLayer, 512 Units DenseLayer, 512 Units DenseLayer, 512 Units</head><label></label><figDesc></figDesc><table coords="6,128.90,565.12,321.58,64.86"><row><cell>Dropout, p=0,5</cell><cell>Dropout, p=0,5</cell><cell>Dropout, p=0,5</cell></row><row><cell>DenseLayer, 512 Units</cell><cell>DenseLayer, 512 Units</cell><cell>DenseLayer, 512 Units</cell></row><row><cell>Dropout, p=0,5</cell><cell>Dropout, p=0,5</cell><cell>Dropout, p=0,5</cell></row><row><cell>DenseLayer, 1500 Units</cell><cell>DenseLayer, 1500 Units</cell><cell>DenseLayer, &lt;500 Units</cell></row><row><cell>Softmax Output</cell><cell>Softmax Output</cell><cell>Sigmoid Output</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,124.70,197.99,346.04,238.41"><head>Table 2 .</head><label>2</label><figDesc>Model evaluation on local test split which contains at least one sample per species. Input size was 512x256 pixels (if not stated otherwise) of five-second chunks of each recording with no overlap taking ~800ms for each sample prediction. Overlapping consecutive spectrograms boost the MAP by ~2% but takes considerably more time to process. We were not able to evaluate DenseNet or ResNet architectures with more than 50 layers due to limited resources.</figDesc><table coords="9,128.18,273.59,335.07,148.41"><row><cell>CNN Type</cell><cell>Description</cell><cell>MAP FG+BG</cell><cell>MAP FG</cell></row><row><cell>Model 1</cell><cell>No dataset augmentation</cell><cell>0,481</cell><cell>0,553</cell></row><row><cell>Model 2</cell><cell>No dataset augmentation</cell><cell>0,468</cell><cell>0,537</cell></row><row><cell>Model 3</cell><cell>No dataset augmentation</cell><cell>0,455</cell><cell>0,527</cell></row><row><cell>Model 1*</cell><cell>Dataset augmentation</cell><cell>0,583</cell><cell>0,671</cell></row><row><cell>Model 1**</cell><cell>Dataset augmentation, sigmoid activations</cell><cell>0,559</cell><cell>0,643</cell></row><row><cell>Model 1</cell><cell>Dataset augmentation, 10s spectrograms</cell><cell>0,554</cell><cell>0,645</cell></row><row><cell>Model 1</cell><cell>Dataset augmentation, 256x128px specs</cell><cell>0,576</cell><cell>0,663</cell></row><row><cell>Model 2</cell><cell>Dataset augmentation</cell><cell>0,573</cell><cell>0,661</cell></row><row><cell>Ensemble***</cell><cell>Seven models, average pooling</cell><cell>0,629</cell><cell>0,711</cell></row><row><cell>DenseNet-32</cell><cell>Dataset augmentation</cell><cell>0,558</cell><cell>0,642</cell></row></table><note coords="9,136.10,428.30,271.02,8.10"><p>*used as model for Run 1 **used as model for Run 2 ***used for Run 3</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,124.70,257.99,346.04,118.53"><head>Table 4 .</head><label>4</label><figDesc>Additional results (MAP) provided by the organizers for time-coded soundscape recordings detailed by country. Our dedicated Run 4 (trained on samples for bird species most probable for Loreto/Peru) outperforms all other runs by a great margin for soundscapes recorded in Peru.</figDesc><table coords="11,157.82,311.66,282.46,64.86"><row><cell>Run</cell><cell>All</cell><cell>Colombia</cell><cell>Peru</cell></row><row><cell>1</cell><cell>0,099</cell><cell>0,101</cell><cell>0,003</cell></row><row><cell>2</cell><cell>0,119</cell><cell>0,121</cell><cell>0,007</cell></row><row><cell>3</cell><cell>0,144</cell><cell>0,146</cell><cell>0,026</cell></row><row><cell>4</cell><cell>0,061</cell><cell>0,059</cell><cell>0,158</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,129.98,686.23,97.62,8.10"><p>http://www.xeno-canto.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,129.98,686.23,159.99,8.10"><p>http://python-speech-features.readthedocs.io</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="11,129.98,686.23,145.74,8.10"><p>https://github.com/kahst/BirdCLEF2017</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="12,129.98,686.23,261.86,8.10"><p>http://machine-listening.eecs.qmul.ac.uk/bird-audio-detection-challenge/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>The <rs type="funder">European Union</rs> and the <rs type="funder">European Social Fund for Germany</rs> partially funded this research. This work was also partially funded by the <rs type="funder">German Federal Ministry of Education and Research</rs> in the program of <rs type="programName">Entrepreneurial Regions InnoProfile-Transfer</rs> in the project <rs type="projectName">group localizeIT</rs> (funding code <rs type="grantNumber">03IPT608X</rs>). We like to thank <rs type="person">Matt Medler</rs>, <rs type="person">Tom Schulenberg</rs>, and <rs type="person">Chris Wood</rs> from the <rs type="affiliation">Cornell Lab of Ornithology</rs> for their kind assistance and advice.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_en3SCWU">
					<idno type="grant-number">03IPT608X</idno>
					<orgName type="project" subtype="full">group localizeIT</orgName>
					<orgName type="program" subtype="full">Entrepreneurial Regions InnoProfile-Transfer</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>://github.com/kahst/BirdCLEF2017</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="13,132.67,294.02,338.07,8.10;13,141.74,305.06,234.71,8.10" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="13,353.83,294.02,116.91,8.10;13,141.74,305.06,139.44,8.10">Audio based bird species identification using deep learning techniques</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sprengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">K</forename><surname>Martin Jaggi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Working notes of CLEF</note>
</biblStruct>

<biblStruct coords="13,132.67,316.10,337.48,8.10;13,141.74,327.02,328.61,8.10;13,141.74,338.06,44.35,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,346.93,316.10,123.21,8.10;13,141.74,327.02,108.32,8.10">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,266.55,327.02,187.07,8.10">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,349.10,337.90,8.10;13,141.74,360.02,328.88,8.10;13,141.74,371.06,170.15,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,170.62,360.02,115.16,8.10">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">.</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,302.01,360.02,168.60,8.10;13,141.74,371.06,133.99,8.10">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,382.10,338.07,8.10;13,141.74,393.02,328.76,8.10;13,141.74,404.06,62.11,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,316.55,382.10,154.19,8.10;13,141.74,393.02,136.91,8.10">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,295.85,393.02,174.65,8.10;13,141.74,404.06,16.94,8.10">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,415.10,337.67,8.10;13,141.74,426.02,194.32,8.10" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="13,327.62,415.10,142.72,8.10;13,141.74,426.02,66.88,8.10">AENet: Learning Deep Audio Features for Video Analysis</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00599</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,132.67,437.06,337.80,8.10;13,141.74,448.10,223.10,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,211.38,437.06,259.08,8.10;13,141.74,448.10,56.05,8.10">Recognizing bird species in audio recordings using deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,213.79,448.10,147.28,8.10">Working notes of CLEF 2016 conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,459.02,338.03,8.10;13,141.74,470.08,328.70,8.10;13,141.74,481.12,231.38,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,312.53,470.08,157.91,8.10;13,141.74,481.12,115.14,8.10">LifeCLEF 2017 Lab Overview: multimedia species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planquè</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,272.45,481.12,77.90,8.10">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,492.04,338.19,8.10;13,141.74,503.08,179.90,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,389.23,492.04,81.63,8.10;13,141.74,503.08,65.75,8.10">LifeCLEF Bird Identification Task 2017</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planquè</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,223.61,503.08,75.47,8.10">CLEF working notes</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,514.12,338.19,8.10;13,141.74,525.04,328.41,8.10;13,141.74,536.08,255.24,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,259.50,514.12,211.36,8.10;13,141.74,525.04,126.85,8.10">Bird song classification in field recordings: winning solution for NIPS4B 2013 competition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,286.15,525.04,184.00,8.10;13,141.74,536.08,72.57,8.10">Proc. of int. symp. Neural Information Scaled for Bioacoustics, sabiod</title>
		<meeting>of int. symp. Neural Information Scaled for Bioacoustics, sabiod</meeting>
		<imprint>
			<date type="published" when="2013-12">2013, December</date>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
	<note>. org/nips4b, joint to NIPS, Nevada</note>
</biblStruct>

<biblStruct coords="13,132.40,547.12,338.34,8.10;13,141.74,558.04,328.81,8.10;13,141.74,569.08,68.07,8.10" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="13,381.13,547.12,89.61,8.10;13,141.74,558.04,262.53,8.10">Neural networks designing neural networks: multi-objective hyper-parameter optimization</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C</forename><surname>Smithson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">H</forename><surname>Meyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02120</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,132.40,580.12,337.99,8.10;13,141.74,591.04,251.83,8.10" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="13,262.16,580.12,208.23,8.10;13,141.74,591.04,124.99,8.10">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,132.40,602.08,338.05,8.10;13,141.74,613.12,275.96,8.10" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<title level="m" coord="13,353.71,602.08,116.74,8.10;13,141.74,613.12,149.12,8.10">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,132.40,624.04,338.34,8.10;13,141.74,635.08,329.12,8.10;13,141.74,646.12,182.98,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,310.85,624.04,159.89,8.10;13,141.74,635.08,179.52,8.10">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,336.81,635.08,134.05,8.10;13,141.74,646.12,119.96,8.10">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.40,657.07,338.24,8.10;13,141.74,668.11,210.62,8.10" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="13,401.83,657.07,68.81,8.10;13,141.74,668.11,83.03,8.10">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,132.40,149.99,337.77,8.10;14,141.74,161.03,328.89,8.10;14,141.74,172.07,35.35,8.10" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,306.07,149.99,160.48,8.10">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,151.92,161.03,301.79,8.10">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,132.40,182.99,337.90,8.10;14,141.74,194.03,63.51,8.10" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="14,251.16,182.99,161.27,8.10">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,132.40,205.07,337.90,8.10;14,141.74,215.99,328.88,8.10;14,141.74,227.03,159.53,8.10" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="14,212.24,215.99,258.38,8.10;14,141.74,227.03,32.26,8.10">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">.</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,132.40,238.07,338.13,8.10;14,141.74,248.99,220.68,8.10" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">.</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename></persName>
		</author>
		<title level="m" coord="14,170.26,248.99,77.17,8.10">Lasagne: First release</title>
		<meeting><address><addrLine>Zenodo; Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,132.40,260.03,338.22,8.10;14,141.74,271.07,328.62,8.10;14,141.74,282.02,180.34,8.10" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,397.40,260.03,73.22,8.10;14,141.74,271.07,169.23,8.10">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,329.01,271.07,141.35,8.10;14,141.74,282.02,117.19,8.10">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,132.40,293.06,338.46,8.10;14,141.74,304.10,265.26,8.10" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00109</idno>
		<title level="m" coord="14,449.86,293.06,21.00,8.10;14,141.74,304.10,138.22,8.10">Snapshot ensembles: Train 1, get m for free</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
