<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.48,116.90,326.39,12.68;1,162.90,134.83,289.56,12.68;1,150.47,154.69,314.42,10.57">Improving Model Performance for Plant Image Classification With Filtered Noisy Images FHDO Biomedical Computer Science Group (BCSG)</title>
				<funder>
					<orgName type="full">NVIDIA Corporation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,144.19,190.72,84.75,8.80"><forename type="first">Andreas</forename><forename type="middle">R</forename><surname>Ludwig</surname></persName>
							<email>andreasralf.ludwig003@stud.fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">University of Applied Sciences and Arts Dortmund (FHDO) Department of Computer Science</orgName>
								<address>
									<addrLine>Emil-Figge-Strasse 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.49,190.72,56.06,8.80"><forename type="first">Helga</forename><surname>Piorek</surname></persName>
							<email>h.piorek@stud.fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">University of Applied Sciences and Arts Dortmund (FHDO) Department of Computer Science</orgName>
								<address>
									<addrLine>Emil-Figge-Strasse 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.10,190.72,77.27,8.80"><forename type="first">Andreas</forename><forename type="middle">H</forename><surname>Kelch</surname></persName>
							<email>andreas.kelch001@stud.fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">University of Applied Sciences and Arts Dortmund (FHDO) Department of Computer Science</orgName>
								<address>
									<addrLine>Emil-Figge-Strasse 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,393.93,190.72,46.21,8.80"><forename type="first">David</forename><surname>Rex</surname></persName>
							<email>darex@live.de</email>
							<affiliation key="aff0">
								<orgName type="department">University of Applied Sciences and Arts Dortmund (FHDO) Department of Computer Science</orgName>
								<address>
									<addrLine>Emil-Figge-Strasse 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,450.69,190.72,20.48,8.80;1,222.71,202.67,28.78,8.80"><forename type="first">Sven</forename><surname>Koitka</surname></persName>
							<email>sven.koitka@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">University of Applied Sciences and Arts Dortmund (FHDO) Department of Computer Science</orgName>
								<address>
									<addrLine>Emil-Figge-Strasse 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">TU Dortmund University</orgName>
								<address>
									<addrLine>Otto-Hahn-Str. 14</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.75,202.67,100.42,8.80"><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
							<email>christoph.friedrich@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">University of Applied Sciences and Arts Dortmund (FHDO) Department of Computer Science</orgName>
								<address>
									<addrLine>Emil-Figge-Strasse 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.48,116.90,326.39,12.68;1,162.90,134.83,289.56,12.68;1,150.47,154.69,314.42,10.57">Improving Model Performance for Plant Image Classification With Filtered Noisy Images FHDO Biomedical Computer Science Group (BCSG)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">74B3C198C992AC7F55BB4AB536DD7B9F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>convolutional neural networks</term>
					<term>plant image classification</term>
					<term>plantCLEF</term>
					<term>oversampling</term>
					<term>transfer learning</term>
					<term>filtering noisy datasets</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The training of convolutional neural networks for image recognition usually requires large image datasets to produce favorable results. Those large datasets can be acquired by web crawlers that accumulate images based on keywords. Due to the nature of data in the web, these image sets display a broad variation of qualities across the contained items. In this work, a filtering approach for noisy datasets is proposed, utilizing a smaller trusted dataset. Hereby a convolutional neural network is trained on the trusted dataset and then used to construct a filtered subset from the noisy datasets. The methods described in this paper were applied to plant image classification and the created models have been submitted to the PlantCLEF 2017 competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The LifeCLEF plant identification task (PlantCLEF) <ref type="bibr" coords="1,371.03,585.32,10.95,8.80" target="#b4">[5,</ref><ref type="bibr" coords="1,381.98,585.32,7.30,8.80" target="#b6">7]</ref> is an annual competition and part of the LifeCLEF evaluation campaign. This year's PlantCLEF task dataset was comprised of 10,000 classes but contained only up to around 1,250 samples per class. Some classes were only represented by a handful of samples. There was a second, noisy dataset, which was derived from results of Bing and Google search queries. These results are partly representing images other than plants or have been labeled incorrectly.</p><p>Up to now, image classifiers have rarely been trained with datasets with more than 1,000 classes. In most of these cases, such datasets were only used to test the effectiveness and results of large distributed learning systems. In <ref type="bibr" coords="2,417.10,143.84,15.49,8.80" target="#b10">[11]</ref> a deep autoencoder that was trained on an ImageNet-10K <ref type="bibr" coords="2,349.09,155.80,15.49,8.80" target="#b14">[15]</ref> dataset comprising 10,000 classes reached a top-1 accuracy of 19.2 %. It was also tested on the ImageNet-21K dataset comprising 21,841 classes, where it reached a top-1 accurary of 15.8 % . These results were later improved by <ref type="bibr" coords="2,346.60,191.66,9.96,8.80" target="#b3">[4]</ref>, where a top-1 accuracy of 29.8 % on ImageNet-21K was reached with Alexnet <ref type="bibr" coords="2,374.68,203.62,9.96,8.80" target="#b8">[9]</ref>. With Nearest Class Mean classification a top-1 accuracy of 23.9 % has been achieved by <ref type="bibr" coords="2,434.26,215.57,15.49,8.80" target="#b11">[12]</ref> on the ImageNet-10K dataset. By using fisher vectors <ref type="bibr" coords="2,343.29,227.53,15.49,8.80" target="#b12">[13]</ref> a top-1 accuracy of 19.1 % was achieved by <ref type="bibr" coords="2,207.86,239.48,15.49,8.80" target="#b15">[16]</ref> on the same dataset.</p><p>This paper aims to show that modern architectures for convolutional neural networks, such as InceptionV4 or InceptionResNetV2 <ref type="bibr" coords="2,370.21,263.39,14.61,8.80" target="#b16">[17]</ref>, can achieve state-ofthe-art results on the given dataset. Furthermore it was assessed if the training results can be improved by training a subset of the noisy dataset. The subset was created by filtering the noisy dataset with an already trained neural network.</p><p>As seen in <ref type="bibr" coords="2,195.59,311.21,11.62,8.80" target="#b1">[2,</ref><ref type="bibr" coords="2,207.21,311.21,7.75,8.80" target="#b2">3,</ref><ref type="bibr" coords="2,214.96,311.21,11.62,8.80" target="#b13">14]</ref> transfer-learning can improve the training results of neural networks. Therefore the models in this work were trained by an approach similar to the two phases fine-tuning approach described in <ref type="bibr" coords="2,367.97,335.12,9.96,8.80" target="#b2">[3]</ref>. At the beginning, the weights of the output layer were randomly initialized and then trained with a small learning rate for a few epochs. Afterward, the entire network was trained. The methodologies of this paper are further described in Section 2. The training of the neural networks as well as the overall results of this paper are described in Section 3. The conclusion can be found in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset Split</head><p>In order to be able to evaluate the quality of the created classifiers, the data of the Encyclopedia of Life (EOL) dataset were randomly split into a training and a validation set. The training set consisted of 90 % of the EOL data and the validation set consisted of the remaining 10 %. In this way, the pretrained models' performance could be estimated during development. For the final submission, the models were trained on the complete EOL dataset in order to take advantage of the full training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Regular Training of the Model</head><p>The general approach to the task at hand was to utilize pretrained models provided on the Tensorflow Slim Git page 3 as a starting point for the training. Since the utilized models have been trained to recognize one thousand different classes, many of which are irrelevant for the PlantCLEF competition, these models were fine-tuned using the training set. As a result of the fine-tuning, there were models produced containing only the lower level filters of the pretrained model and output layers primed to the 10,000 classes of the PlantCLEF competition. Subsequent to the fine-tuning process, the models were trained further utilizing the training set and filtered web datasets. The general workflow is displayed in Figure <ref type="figure" coords="3,221.30,179.71,3.87,8.80" target="#fig_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Utilized CNN Architectures</head><p>The pretrained models on the Tensorflow Slim Git page were trained on the ILSVRC-2012-CLS dataset <ref type="bibr" coords="3,254.59,621.19,14.61,8.80" target="#b14">[15]</ref>. The model checkpoints were published with the accuracy that could be reached on the corresponding test set. Since the Incep-tionResNetV2 and InceptionV4 <ref type="bibr" coords="3,274.56,645.10,15.49,8.80" target="#b16">[17]</ref> architectures are evidently able to produce the best accuracies, 80.4 % top-1 for InceptionResNetV2 and 80.2 % top-1 for InceptionV4. The training was conducted with a fine-tuning procedure similar to the two-step fine-tuning approach described in <ref type="bibr" coords="4,342.99,131.89,9.96,8.80" target="#b2">[3]</ref>. The standard TF-slim data augmentation<ref type="foot" coords="4,194.25,142.29,3.97,6.16" target="#foot_1">4</ref> functions were used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Filtered Web Datasets</head><p>After visually examining the web dataset it became apparent, that apart from plant images in varying qualities, there were also a large number of images that displayed unrelated items such as postage stamps or music instruments.</p><p>Due to the noisy nature of the web dataset, the data were filtered to reduce the possibility that the classification results of the existing models are weakened. For this purpose, the whole web dataset was classified using a model that showed a good estimate performance. This model has been trained on the whole EOL set and was submitted as Run 1. Ultimately all the pictures that could be classified correctly within top-5 predictions were compiled to a web-top-5 dataset containing 556,584 samples. One could assume that using top-5 web subset for further training improves the accuracy of the trained model solely with regard to the classes that have already been learned sufficiently. It would be interesting to investigate, if filtering different proportions of the predicted classes (e.g. top-100) could improve the accuracy. However due to limitations of processing time only the filtering approach described above was pursued. Figure <ref type="figure" coords="4,414.86,355.35,4.98,8.80">2</ref> visualizes the procedure used for filtering the web dataset.</p><p>The plots in Figure <ref type="figure" coords="4,242.95,379.26,4.98,8.80">3</ref> show the number of occurrences of distinct classes within the training dataset and the filtered dataset sorted in descending order. The number of occurrences for every class was logarithmized. The filtered subset is missing any samples for 696 classes, which is a general problem of the presented filtering approach. Training data for classes that are already well trained can be selected from the noisy dataset, but all the images of classes that can not be classified within a margin will be removed along with the noise. The objective of further research would be to find a top-x which maximizes noise filtering without dropping too many classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Oversampling</head><p>To improve the results of the classification, multiple crops of an image were used to calculate an average classification per sample, as described in <ref type="bibr" coords="4,427.94,531.00,11.25,8.80" target="#b5">[6,</ref><ref type="bibr" coords="4,439.18,531.00,7.50,8.80" target="#b7">8,</ref><ref type="bibr" coords="4,446.68,531.00,11.25,8.80" target="#b16">17]</ref>. For each sample, ten crops were created, one at each corner, one centered and a mirrored version of these five crops. The final prediction was estimated as an average of the ten samples of every image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The estimated performance metrics were calculated using models that had not yet been trained with the complete EOL dataset. These models were used to receive an estimate of the performance through classification on the validation set. It is important to notice that in the EOL dataset, every image belongs to a unique observation. In contrast, the official test set contains a large number of observations with multiple images, allowing the model to predict the label of an image based on an average of multiple images. The official MRR score was calculated based on observations, whereas the presented validation MRR scores were estimated on images solely. Table <ref type="table" coords="6,337.49,450.99,4.98,8.80" target="#tab_0">1</ref> shows the training error of the finished models and the ensemble. The model performance and accuracy of the first three runs are presented in Table <ref type="table" coords="6,303.68,474.90,3.87,8.80" target="#tab_1">2</ref>. Run 4 was excluded from both Tables as it is considered to be broken.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FHDO BCSG Run 1 -InceptionResNetV2 Trained on EOL Dataset</head><p>The first submission was trained on the aforementioned pretrained model. For the fine-tuning training step, the logits and auxiliary logits were randomly initialized instead of being copied from the utilized checkpoint. These two layers had been trained using a relatively small learning rate of 0.0045 for five epochs. The two layers were chosen based on the assumption that the lower level layers have already been suitably prepared for classification tasks while the topmost layers needed to be primed onto the 10,000 different classes of the PlantCLEF competition. Subsequently, all the layers of this model were trained on the training set for fifty epochs with the parameters shown in Table <ref type="table" coords="6,348.04,657.05,3.87,8.80" target="#tab_2">3</ref>. The model that was trained for fifty epochs yielded an MRR of 0.4661 on the validation set and was therefore chosen as the model for the first submitted run. To finish the training the net was trained for another five epochs using the complete EOL dataset. Table <ref type="table" coords="8,475.61,143.84,4.98,8.80" target="#tab_3">4</ref> shows the effect of oversampling on the estimated MRR scores for Run 1. The second submission was trained analogously to the first submission. As mentioned before, a pretrained model was used and only the logits and auxiliary logits layer were trained for five epochs with a small learning rate of 0.0045. Eventually, all layers of the model were trained for thirty epochs using the parameters shown in Table <ref type="table" coords="8,245.04,405.24,3.87,8.80" target="#tab_2">3</ref>.</p><p>In order to examine the effect of the web filtering approach, a web subset was created using the completely trained Run 1 model. Images were added to this set if the annotated class of the noisy training set was in the top-5 predictions of the model trained for Run 1. With this, a web subset finally consisting of 556,584 images was assembled. Following the filter process, the Run 2 model was trained on the web-top-5 dataset for five epochs and afterwards trained for another five epochs on the training dataset. The parameters for this training were chosen analogously to the training of Run 1 with a starting learning rate of 0.000275 and five epochs. The learning rate was adopted from the last training epoch of the previous training step. To finish up the training for this model, the net was trained for another five epochs on the complete EOL dataset analogue to the procedure used for Run 1. Before executing the final training step, the MRR scores were estimated on the validation set, leading to a result of 0.503. This value could have been biased though, due to the web-top-5 subset being compiled with a network that was trained on the complete EOL dataset. Table <ref type="table" coords="8,134.77,596.90,4.98,8.80" target="#tab_4">5</ref> shows the effects of oversampling on the estimated MRR scores of Run 2. Oversampling improved the MRR scores slightly at the cost of higher computing demands.</p><p>In order to investigate the effectiveness of the filtering approach, the training procedure of Run 2 was modified and two more models were trained. The starting point for both models was an InceptionResNetV2 trained on the training set for thirty epochs. The training for the first evaluation model was conducted once again with 556,584 images from the web dataset. Instead of being selected by the filtering approach, they were chosen randomly. The training for the second evaluation model was conducted with the complete web dataset. The first model was trained for five epochs (86,966 steps) and the other model with an equivalent number of steps (86,966 steps). The training parameters were chosen analogously to the training of the Run 2 model. Afterwards the models were trained on the training dataset for another five epochs. The results on the validation set showed that using the filtered data for training improved the estimated MRR scores compared to using a randomly drafted dataset or the whole noisy dataset. The results of this analysis are displayed in Table <ref type="table" coords="9,333.81,239.48,3.87,8.80" target="#tab_5">6</ref>. Composing an ensemble run from multiple prediction models can improve the overall accuracy in comparison to a single prediction model <ref type="bibr" coords="9,407.99,585.32,14.61,8.80" target="#b9">[10]</ref>. In order to create an ensemble, the mean values of all the class predictions of the different models for every image were calculated and then assembled into a new set of predictions as shown in <ref type="bibr" coords="9,239.01,621.19,14.61,8.80" target="#b9">[10]</ref>. The reasoning behind this is that one model might be better at predicting certain classes while being worse than the other models on other classes and vice versa. Presumably, the ensemble would be better if it was composed from a number of accurate and diverse runs.</p><p>Due to the fact, that on the validation set, Run 2 was producing the higher MRR value of 0.503 compared to the value of 0.471 of Run 1, the models were weighted for the ensemble. In this way the presumably better model would have a higher influence on the final prediction. Run 1 contributed 1/3 and Run 2 contributed 2/3 to Run 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">FHDO_BCSG Run 4</head><p>Run 4 was based on an InceptionV4 architecture but did only achieve modest estimated MRR scores due to misconfigurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The utilized filtering approach improved the predictions of the resulting model on the validation set and the official score. The filtering approach increased computation costs. Recent studies suggest <ref type="bibr" coords="10,327.36,303.97,9.96,8.80" target="#b0">[1]</ref>, that neural networks can only recognize samples of unknown classes to a certain extent. Since there were only few samples available for some classes and since some classes were very similar to one another, there is a chance that samples belong to a known class other than the one they were labeled with.</p><p>The use of oversampling leads to a minimal increase in estimated MRR scores, as shown in <ref type="bibr" coords="10,189.46,375.70,14.61,8.80" target="#b16">[17]</ref>. Since multiple crops increase the processing time during classification, the oversampling method is not suited for every scenario.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,504.89,345.83,7.93;3,134.77,515.86,345.83,7.92;3,134.77,526.82,345.82,7.92;3,134.77,537.78,249.20,7.92"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The diagram shows the general workflow for Run 1. The EOL dataset was split in a training and a validation set. A pretrained InceptionResNetV2 checkpoint was fine-tuned on the training set and evaluated using the validation set. The training was finished by training the network with the entire EOL dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,529.21,345.82,7.93;5,134.77,540.18,345.83,7.92;5,134.77,551.14,345.82,7.92;5,134.77,562.10,136.60,7.92"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig.2. The diagram shows the filtering procedure. At first the noisy dataset is classified using the finished Run 1 model. Afterwards the images that could be classified correctly within top-5 predictions were accumulated to a filtered web subset. This web subset was then used for further training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,144.12,139.67,327.11,145.12"><head>Table 1 .</head><label>1</label><figDesc>Classification results on the training datasets using the finished models</figDesc><table coords="7,198.40,163.63,213.17,121.16"><row><cell>Datasets</cell><cell cols="2">Run ID MRR Top-1 Top-5</cell></row><row><cell></cell><cell>Run 1</cell><cell>0.898 0.856 0.949</cell></row><row><cell>Training (90 %)</cell><cell>Run 2</cell><cell>0.894 0.854 0.941</cell></row><row><cell></cell><cell cols="2">Run 3 0.900 0.860 0.946</cell></row><row><cell></cell><cell>Run 1</cell><cell>0.509 0.418 0.611</cell></row><row><cell>Validation (10 %)</cell><cell cols="2">Run 2 0.515 0.426 0.615</cell></row><row><cell></cell><cell>Run 3</cell><cell>0.513 0.423 0.614</cell></row><row><cell></cell><cell cols="2">Run 1 0.846 0.800 0.901</cell></row><row><cell>EOL</cell><cell>Run 2</cell><cell>0.835 0.789 0.889</cell></row><row><cell></cell><cell>Run 3</cell><cell>0.841 0.795 0.895</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,342.51,345.83,112.24"><head>Table 2 .</head><label>2</label><figDesc>Official scores on the test set and estimated performance and accuracies on training and validation set of Run 1-3</figDesc><table coords="7,139.65,377.43,336.07,77.32"><row><cell>Run ID</cell><cell cols="2">Estimated performance</cell><cell>Official score</cell></row><row><cell></cell><cell>Training</cell><cell>Validation</cell><cell>MRR</cell></row><row><cell></cell><cell cols="2">MRR Top-1 Top-5 MRR Top-1 Top-5</cell></row><row><cell>Run 1 EOL</cell><cell cols="2">0.889 0.845 0.942 0.471 0.382 0.570</cell><cell>0.792</cell></row><row><cell cols="3">Run 2 EOL + Filtered 0.887 0.845 0.939 0.503 0.421 0.594</cell><cell>0.806</cell></row><row><cell>Run 3 Ensemble</cell><cell cols="2">0.890 0.848 0.941 0.493 0.406 0.588</cell><cell>0.804</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,140.74,512.46,333.88,124.56"><head>Table 3 .</head><label>3</label><figDesc>Training parameters for Run 1 and 2</figDesc><table coords="7,140.74,536.42,333.88,100.60"><row><cell></cell><cell>Run 1</cell><cell>Run 2</cell></row><row><cell>Mini-batch size</cell><cell>32</cell><cell>32</cell></row><row><cell>Steps</cell><cell cols="2">50 epochs (334,700 steps) 30 epochs (200,820 steps)</cell></row><row><cell>Learning rate</cell><cell>0.045</cell><cell>0.045</cell></row><row><cell>Optimizer</cell><cell>rmsprop</cell><cell>rmsprop</cell></row><row><cell>Learning rate decay type</cell><cell>exponential</cell><cell>exponential</cell></row><row><cell>Learning rate decay factor</cell><cell>0.47</cell><cell>0.47</cell></row><row><cell cols="2">Number of epochs per decay 2</cell><cell>2</cell></row><row><cell>Weight decay</cell><cell>0.00004</cell><cell>0.00004</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.77,188.03,345.83,156.37"><head>Table 4 .</head><label>4</label><figDesc>Run 1: The results of classification on the validation set with one central crop in comparison to oversampling</figDesc><table coords="8,134.77,222.95,332.06,121.45"><row><cell>Crop method</cell><cell cols="2">Estimated performance</cell></row><row><cell></cell><cell>MRR Top-1</cell><cell>Top-5</cell></row><row><cell cols="2">Central Crop 0.466 0.375</cell><cell>0.567</cell></row><row><cell cols="2">Oversampling 0.471 0.382</cell><cell>0.570</cell></row><row><cell cols="3">3.2 FHDO_BCSG Run 2 -InceptionResNetV2 Trained on EOL</cell></row><row><cell>and Web-Top-5 Datasets</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,134.77,272.18,345.82,85.52"><head>Table 5 .</head><label>5</label><figDesc>Run 2: The results of classification with one central crop in comparison to classifaction with oversampling on the validation set</figDesc><table coords="9,218.37,307.10,177.21,50.61"><row><cell>Crop method</cell><cell cols="2">Estimated performance</cell></row><row><cell></cell><cell>MRR Top-1</cell><cell>Top-5</cell></row><row><cell cols="2">Central Crop 0.492 0.411</cell><cell>0.582</cell></row><row><cell cols="2">Oversampling 0.503 0.421</cell><cell>0.594</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,134.77,408.96,345.83,150.23"><head>Table 6 .</head><label>6</label><figDesc>The results of training with a filtered dataset in comparison to training with a randomly drafted dataset or the whole web dataset. The scores were calculated on the validation set</figDesc><table coords="9,134.77,453.09,297.29,106.09"><row><cell>Run variations</cell><cell cols="2">MRR Top-1 Top-5 Training steps</cell></row><row><cell>Run 2</cell><cell>0.503 0.421 0.594</cell><cell>86,966</cell></row><row><cell>Random images</cell><cell>0.487 0.397 0.588</cell><cell>86,966</cell></row><row><cell cols="2">Whole web dataset 0.495 0.402 0.598</cell><cell>86,966</cell></row><row><cell cols="3">3.3 FHDO_BCSG Run 3 -Ensemble of Run 1 and 2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,646.78,335.87,7.92;2,144.73,657.74,228.11,7.92"><p>Maintainer: Nathan Silberman and Sergio Guadarrama; [last access: 29.06.2017] https://github.com/tensorflow/models/tree/master/slim</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="4,144.73,646.78,335.87,7.92;4,144.73,657.74,228.11,7.92"><p>Maintainer: Nathan Silberman and Sergio Guadarrama; [last access: 29.06.2017] https://github.com/tensorflow/models/tree/master/slim</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>The authors gratefully acknowledge the support of <rs type="funder">NVIDIA Corporation</rs> with the donation of the Titan X Pascal GPU, which supported this research.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.95,505.30,337.64,7.92;10,151.52,516.26,329.07,7.92;10,151.52,527.22,153.04,7.92" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,320.61,505.30,140.44,7.92">Towards Open Set Deep Networks</title>
		<author>
			<persName coords=""><forename type="first">Abhijit</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,516.26,329.07,7.92">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1563" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,537.85,337.63,7.92;10,151.52,548.81,329.07,7.92;10,151.52,559.77,329.08,7.92;10,151.52,570.73,329.07,7.92;10,151.52,581.69,197.08,7.92" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,309.29,537.85,171.30,7.92;10,151.52,548.81,171.31,7.92">Analyzing the Performance of Multilayer Neural Networks for Object Recognition</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10584-0_22</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-10584-0_22" />
	</analytic>
	<monogr>
		<title level="m" coord="10,349.26,548.81,131.33,7.92;10,151.52,559.77,201.28,7.92">Proceedings of the 13th European Conference Computer Vision (ECCV 2014)</title>
		<meeting>the 13th European Conference Computer Vision (ECCV 2014)<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="page" from="329" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,592.32,337.64,7.92;10,151.52,603.28,329.07,7.92;10,151.52,614.23,126.77,7.92" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,359.10,592.32,121.50,7.92;10,151.52,603.28,174.09,7.92">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CoRR. vol. abs/1406.2952</idno>
		<ptr target="http://arxiv.org/abs/1406.2952" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,624.86,337.63,7.92;10,151.52,635.82,329.08,7.92;10,151.52,646.78,329.07,7.92;10,151.52,657.74,293.28,7.92" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,391.29,624.86,89.29,7.92;10,151.52,635.82,247.61,7.92">Project Adam: Building an Efficient and Scalable Deep Learning Training System</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Suzue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Apacible</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kalyanaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,421.62,635.82,58.98,7.92;10,151.52,646.78,329.07,7.92;10,151.52,657.74,49.77,7.92">Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 2014)</title>
		<meeting>the 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 2014)<address><addrLine>Broomfield, CO</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,120.62,337.64,7.92;11,151.52,131.58,329.07,7.92;11,151.52,142.54,329.08,7.92;11,151.52,153.49,246.74,7.92" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,290.82,120.62,189.77,7.92;11,151.52,131.58,245.33,7.92">Plant identification based on noisy web data: the amazing performance of deep learning (LifeCLEF 2017)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,419.78,131.58,60.81,7.92;11,151.52,142.54,255.93,7.92">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="11,248.29,153.49,121.31,7.92">CEUR-WS Proceedings Notes</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-14">11-14 September, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,164.39,337.63,7.92;11,151.52,175.35,329.07,7.92;11,151.52,186.31,134.74,7.92" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,302.16,164.39,178.43,7.92;11,151.52,175.35,14.74,7.92">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,188.93,175.35,291.67,7.92;11,151.52,186.31,47.73,7.92">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,197.20,337.63,7.92;11,151.52,208.16,329.07,7.92;11,151.52,219.12,329.08,7.92;11,151.52,230.08,329.07,7.92;11,151.52,241.03,329.07,7.92;11,151.52,251.99,275.99,7.92" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,355.93,208.16,124.65,7.92;11,151.52,219.12,174.04,7.92">LifeCLEF 2017 Lab Overview: multimedia species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,347.03,219.12,133.56,7.92;11,151.52,230.08,329.07,7.92;11,151.52,241.03,183.56,7.92">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the 8th International Conference of the CLEF Association, CLEF 2017</title>
		<title level="s" coord="11,175.57,251.99,170.01,7.92">Lecture Notes of Computer Science (LNCS</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,262.89,337.64,7.92;11,151.52,273.85,329.07,7.92;11,151.52,284.81,329.07,7.92;11,151.52,295.76,329.07,7.92;11,151.52,306.72,320.07,7.92" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,265.96,262.89,214.63,7.92;11,151.52,273.85,144.47,7.92">Optimized Convolutional Neural Network Ensembles for Medical Subfigure Classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,318.64,273.85,161.95,7.92;11,151.52,284.81,329.07,7.92;11,151.52,295.76,155.69,7.92">Proceedings of the 8th International Conference of the CLEF Association, CLEF 2017</title>
		<title level="s" coord="11,151.52,306.72,170.00,7.92">Lecture Notes of Computer Science (LNCS</title>
		<meeting>the 8th International Conference of the CLEF Association, CLEF 2017<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="11,142.95,317.62,337.64,7.92;11,151.52,328.58,329.08,7.92;11,151.52,339.53,329.07,7.92;11,151.52,350.49,44.02,7.92" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,337.07,317.62,143.52,7.92;11,151.52,328.58,124.38,7.92">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,233.36,339.53,214.75,7.92">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,361.39,337.98,7.92;11,151.52,372.35,45.55,7.92" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="11,211.45,361.39,227.80,7.92">Combining Pattern Classifiers: Methods and Algorithms</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kuncheva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Wiley</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>edn.</note>
</biblStruct>

<biblStruct coords="11,142.61,383.24,337.98,7.92;11,151.52,394.20,329.07,7.92;11,151.52,405.16,329.08,7.92;11,151.52,416.12,304.71,7.92" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,185.86,394.20,275.74,7.92">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,279.11,405.16,201.49,7.92;11,151.52,416.12,139.50,7.92">Proceedings of the 29th International Conference on Machine Learning (ICML 2012)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</editor>
		<meeting>the 29th International Conference on Machine Learning (ICML 2012)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07">July 2012</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,427.01,337.97,7.92;11,151.52,437.97,329.07,7.92;11,151.52,448.93,294.57,7.92" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,367.33,427.01,113.26,7.92;11,151.52,437.97,231.95,7.92">Distance-Based Image Classification: Generalizing to New Classes at Near-Zero Cost</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,391.01,437.97,89.58,7.92;11,151.52,448.93,170.97,7.92">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2624" to="2637" />
			<date type="published" when="2013-11">Nov 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,459.82,337.98,7.92;11,151.52,470.78,329.07,7.92;11,151.52,481.74,329.07,7.92;11,151.52,492.70,68.84,7.92" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,372.37,459.82,108.22,7.92;11,151.52,470.78,183.31,7.92">Towards Good Practice in Large-Scale Learning for Image Classification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,356.62,470.78,123.98,7.92;11,151.52,481.74,209.17,7.92">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012-06">2012. Jun 2012</date>
			<biblScope unit="page" from="3482" to="3489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,503.59,337.97,7.92;11,151.52,514.55,329.07,7.92;11,151.52,525.51,329.07,7.92;11,151.52,536.47,58.62,7.92" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,333.89,503.59,146.70,7.92;11,151.52,514.55,107.10,7.92">Fine-tuning deep convolutional networks for plant recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Camargo</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="11,187.70,525.51,91.94,7.92">CLEF (Working Notes)</title>
		<title level="s" coord="11,286.17,525.51,118.04,7.92">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1391</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,547.36,337.98,7.92;11,151.52,558.32,329.08,7.92;11,151.52,569.28,329.07,7.92;11,151.52,580.24,123.99,7.92" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,415.78,558.32,64.82,7.92;11,151.52,569.28,145.50,7.92">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,305.92,569.28,174.67,7.92;11,151.52,580.24,28.79,7.92">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,591.13,337.98,7.92;11,151.52,602.09,329.07,7.92;11,151.52,613.05,279.28,7.92" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,376.48,591.13,104.11,7.92;11,151.52,602.09,155.94,7.92">Image Classification with the Fisher Vector: Theory and Practice</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-013-0636-x</idno>
		<ptr target="http://dx.doi.org/10.1007/s11263-013-0636-x" />
	</analytic>
	<monogr>
		<title level="j" coord="11,314.07,602.09,166.52,7.92">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,623.95,337.98,7.92;11,151.52,634.90,329.07,7.92;11,151.52,645.86,263.06,7.92" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,355.28,623.95,125.31,7.92;11,151.52,634.90,209.31,7.92">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,381.39,634.90,99.21,7.92;11,151.52,645.86,180.14,7.92">International Conference on Learning Representations 2016 Workshop</title>
		<meeting><address><addrLine>ICLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
