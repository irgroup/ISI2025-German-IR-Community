<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.53,115.90,304.30,12.90;1,275.01,133.83,65.33,12.90">Automatic Whale Matching System using Feature Descriptor</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,184.55,172.17,59.23,8.64"><forename type="first">S</forename><forename type="middle">M</forename><surname>Jaisakthi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">VIT University</orgName>
								<address>
									<settlement>Vellore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.23,172.17,52.04,8.64"><forename type="first">P</forename><surname>Mirunalini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">VIT University</orgName>
								<address>
									<settlement>Vellore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">SSN College of Engineering</orgName>
								<address>
									<settlement>Kalavakkam, Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,314.71,172.17,56.53,8.64"><forename type="first">Rutuja</forename><surname>Jadhav</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">VIT University</orgName>
								<address>
									<settlement>Vellore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.53,115.90,304.30,12.90;1,275.01,133.83,65.33,12.90">Automatic Whale Matching System using Feature Descriptor</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FC0E99498A64BCACA7C8734D6FCA8F50</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Whale</term>
					<term>Matching</term>
					<term>Segmentation</term>
					<term>Recognition</term>
					<term>GrabCut</term>
					<term>FloodFill</term>
					<term>SIFT features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Whales play an important role in ocean ecosystem by maintaining a stable food chain. Whales continued to be endangered animals though they don't have direct predators. To ensure the survival of this endangered species, marine biologists are tracking them to know their status and health of the species at all times. Manual recognition of whale is most tricky and hence automated system helps biologist to develop conservation strategies for different species of whale. This can be primarily achieved through individual whale recognition and tracking their behavior by analyzing the data collected. In this paper we have proposed a method for finding the matching pairs of whales by analyzing the caudal fin images of whales in the data set. For finding the matching pairs we have segmented the caudal fins from the background images using GrabCut and FloodFill algorithm. From the segmented images key features are extracted using Scale Invariant Feature Transform (SIFT) and key features are matched using the FLANN (Fast Library for Approximate Nearest Neighbors) matcher. Finally the matched pairs are ranked using the confidence values.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To effectively manage and conserve animals, it is important to understand and predict the movement of animals. Recent study shows that the movement and behaviors of highly migratory marine species is challenging because of logistical and technological limitations. However automatic recognition and tracking tools significantly helps the scientist to study the movements and behavior of the animals. Automatic recognition of individual whales plays a crucial role for tracking target population, biological samples, acoustic recordings, behavior patterns, migration, sexual maturity and health assessments. To track and monitor the population, whales are photographed during aerial surveys and then manually matched to an online photo-identification catalog. The development of the photo-identification has been proven to be a useful tool for learning many species. Since the process is done manually it is impossible to get accurate count of all individuals in a given large collection of observations <ref type="bibr" coords="2,373.25,131.27,10.58,8.64" target="#b7">[9]</ref>. Automating the photoidentification process could speed-up and provide better result in large data collection of data. This can be done by comparing the features extracted between the whales. Generally whales have four fins : two pectoral fins, a caudal fin and a dorsal fin. The caudal fin is meant for propulsion of animals and the pectoral fins act as rudders and stabilizers. Caudal fins is the powerful fins with different shapes. The different shapes in caudal fins, patterns of coloration and patterns of marks designed by nature over the caudal fins can play a valuable contribution in recognition and tracking. So, analysis of caudal fins plays an important role in studying whales. Scientists compare and match the image taken, with images in database and locate known whales during their migration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks and Dataset</head><p>The report in this research work is a challenge in SeaClef 2017 which is a part of LifeCLef 2017 <ref type="bibr" coords="2,196.57,301.55,10.58,8.64" target="#b4">[6]</ref>. This task aims in automatically matching whale image pairs, over a large set of images, of same individual whales through the analysis of their caudal fins. The caudal fins are considered as the most discriminant pattern for distinguishing an individual whale from another. The data set shared for the task contains 2005 images of humbpack whales caudals.</p><p>Individual whales can be recognized by their pattern of markings and/or the scars that appear along the years <ref type="bibr" coords="2,248.05,373.28,10.58,8.64" target="#b5">[7]</ref>. It is very difficult to match the whales in whole data set because number of individual whales is high, features to discriminate the whales are very similar and similar water background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Methodology</head><p>To automatically detect the matching pairs of whale from the given data set we have first preprocessed the images to remove the noise like clouds, sea water and landscapes. From the noise removed images we have extracted the scale invariant key features and these extracted features are matched to obtain the matching pairs of whales. The steps involved in automatic whale matching system are :</p><p>-Preprocessing using Grabcut Segmentation and Flood fill algorithm -Key Feature Extraction using SIFT Algorithm -Feature Matching using FLANN Matcher -Ranking the Matched Pairs</p><p>The overall architecture of the proposed methodology is depicted in Figure <ref type="figure" coords="2,453.22,563.86,3.74,8.64" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>The data set contains images of caudal fins with some background details such as sea water, clouds, land cover, rock etc. The presence of these background details may reduce the performance of the system considerably. So we have preprocessed the images in the data set to segment the caudal fins alone from the background using GrabCut and FloodFill algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grab Cut Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flood Fill Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprocessing</head><p>Segmented ROI ROI segmentation using Grabcut method GrabCut algorithm <ref type="bibr" coords="3,404.46,250.80,11.62,8.64" target="#b0">[1]</ref> efficiently segments the foreground object from the background images using graph cut method. This algorithm takes foreground information in the form of rectangle and the information which lies outside the rectangle is considered as background. Using this foreground and background information Gaussian Mixture Models (GMM) are modeled. Each pixel in the foreground is assigned the most probable Gaussian component in the foreground GMMs. Similarly each pixel in the background is assigned the most probable Gaussian component in the background GMMs. From this pixels a graph is built with pixel being represented as nodes. Two more additional nodes Source node and Sink node are added where every foreground pixel is connected to Source node and every background pixel is connected to Sink node. The weights for edges between source and end node is defined based on the probability of a pixel being foreground/background. The weights between the pixels are defined by the edge information or pixel similarity. Using the mincut algorithm graph is segmented. After the cut, all the pixels connected to Source node become foreground and those connected to Sink node become background. The same process is repeated until the foreground image is segmented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Key Feature Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SIFT Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matching Feature Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FLANN Matcher</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking the Matched Whales</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matched Whales</head><p>In this work, we have used GrabCut segmentation to segment the caudal fins from the background. In order to automatically obtain the foreground information in the form of rectangle we have divided the image into grid like structure by dividing rows and columns of the image into 20 equal parts. Sample grid image is shown in Figure <ref type="figure" coords="3,473.11,478.68,3.74,8.64" target="#fig_1">2</ref>. From this grid like structure we have eliminated two topmost row strip and one bottom most row strip. Similarly one left most and right most column strip is eliminated. After eliminating some of the rows and columns the grids in the middle is formed into single rectangle and this rectangle is given as input foreground rectangle. The result of the GrabCut segmentation for two sample images are depicted in Figure <ref type="figure" coords="3,412.04,538.46,3.74,8.64">3</ref>.</p><p>For the first image foreground is perfectly segmented while in the second image foreground is not segmented properly. These images need further segmentation which is done using FloodFill algorithm.</p><p>Improving Segmentation Results using FloodFill Algorithm GrabCut method fails to give good segmentation results for some images. The segmentation process is further improved using floodFill algorithm which clusters same colored pixels. To obtain a seed point automatically for clustering, blue colored pixels are searched along the column stripes and row stripes. Once the seed points are selected FloodFill algorithm groups  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Key Feature Extraction using SIFT algorithm</head><p>After obtaining the ROI, whale recognition has been done by extracting the key features from the ROI and comparing them with the features of all other images. The key features of the whale image are obtained using SIFT <ref type="bibr" coords="5,308.85,419.91,11.62,8.64" target="#b1">[2]</ref> feature extraction algorithm since, SIFT features are scale and rotational invariants and also invariant to change in illumination.</p><p>The descriptors obtained rely on local pixel information which makes them robust to minor occlusions. The following steps are involved in generating the key features of the image.</p><p>-Scale-Space Construction : In this step several octaves (images of same size) of the original images is generated. The size of image in each octave is reduced to half the size of previous octave's image. The images within an octave is blurred out progressively. -Scale-space extrema detection: In this step, potential interest points that are invariant to scale and orientation are identified by estimating Difference-of-Gaussian (DoG) over all scales and image locations.</p><p>-Key point Localization: To find the key points first maxima/minima is located in DoG images and the subpixel maxima/minima is determined. This is achieved by comparing neighbouring pixels in the current scale, the scale "above" and the scale "below". -Orientation assignment: One or more orientations are assigned to each key point location based on local image gradient directions.</p><p>-Key point descriptor: The local image gradients are measured at the selected scale in the region around each key point. These are transformed into a representation called descriptors which encompasses position, scale, orientation and local image structure.</p><p>The descriptors are called as key features which are extracted from the ROI of the data set. To find the matching pairs, the extracted features of an image are compared with the features obtained from all other images in the data set. After feature extraction, feature matching is done using FLANN matcher. The sample image after key point extraction using SIFT algorithm is depicted in the Figure <ref type="figure" coords="6,364.02,200.86,3.74,8.64">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Matching using FLANN Matcher</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ranking the Matched Pairs</head><p>Matching Whales To find the best match, each image in the data set is matched with all other images in the data set using Flann matcher. While matching the whales, there may be duplicate pairs. In order to remove the duplicate pairs we have constructed matrix of size N X N, where N represents the number of images in the data set. The value v in the location (i,j) represents the number of good matches that matches image i and image j. The duplicate matching of the images are eliminated by considering only the values in lower triangular matrix and discarding the upper triangular matrix. Higher the number of good matches represents high possibility of matching pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence Value Calculation</head><p>The confidence value for the matching pairs should be in the interval of 0 to 1. To estimate the confidence value we have used min-max normalization method which is given by</p><formula xml:id="formula_0" coords="7,193.11,399.28,287.48,22.31">v = v -min max -min (new max -new min) + new min<label>(1)</label></formula><p>Min-max normalization transforms the obtained good matches v of an image to a new interval from new min to new max using min and max values of good matches.</p><p>Ranking the Whales Pairs The matched pairs are identified by fixing the threshold t for the confidence value. Generally the threshold is fixed as 95%. The pairs which is having confidence value more than the threshold are filtered out and then ranking is performed by sorting the confidence values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Resources</head><p>We have used the data set released by the SeaClef 2017 for whale matching and for implementation we have used Opencv in c++ environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>Using the proposed method we could able to obtain 0.01% average precision only. The accuracy can be further improved by using other matcher such as Brutforce and knnmatcher. The results can be further enhanced by obtaining best estimates of the matches using feature correspondence using homography technique.</p><p>In this paper we have proposed an automatic method to match the whales in a large data set which is the picture of caudal fins. Our method first segments the whale image from the background using grabcut segmentation. From the segmented foreground images key points are obtained using SIFT descriptors. subsequently, using FLANN matcher key features of an image is matched with the key features of all other images in data set and good matches are obtained. Using the obtained good matches confidence value is calculated using min-max normalization and the estimated confidence value is ranked.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,181.74,215.05,251.88,8.12"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture Diagram of the Proposed Whale matching System</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,177.39,324.17,260.59,8.12;4,167.26,160.16,248.03,138.95"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Extraction of Foreground Rectangle Input for GrabCut Algorithm</figDesc><graphic coords="4,167.26,160.16,248.03,138.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,237.58,638.23,140.19,8.12;4,157.54,497.29,146.46,97.40"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Result of Grabcut Segmentation</figDesc><graphic coords="4,157.54,497.29,146.46,97.40" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.13,270.40,342.46,7.77;8,146.47,281.36,334.12,7.77;8,146.47,292.31,44.72,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,366.80,270.40,113.79,7.77;8,146.47,281.36,127.28,7.77">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName coords=""><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,283.29,281.36,135.77,7.77">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2004">2004</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,303.27,342.46,7.77;8,146.47,314.23,334.12,7.77;8,146.47,325.19,211.68,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,239.05,303.27,241.54,7.77;8,146.47,314.23,21.11,7.77">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lowe</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:VISI.0000029664.99615</idno>
		<ptr target="http://dx.doi.org/10.1023/B:VISI.0000029664.99615" />
	</analytic>
	<monogr>
		<title level="j" coord="8,188.67,314.23,114.61,7.77">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004-11">November 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,347.11,342.46,7.77;8,146.47,358.07,69.98,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,211.22,347.11,123.70,7.77">Image segmentation using GrabCut</title>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Malmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,345.10,347.11,135.49,7.77;8,146.47,358.07,11.46,7.77">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,369.03,342.46,7.77;8,146.47,379.99,208.69,7.77" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,275.54,369.03,205.05,7.77;8,146.47,379.99,59.26,7.77">Image Segmentation using k-means clustering, EM and Normalized Cuts</title>
		<author>
			<persName coords=""><forename type="first">Suman</forename><surname>Tatiraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Avi</forename><surname>Mehta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>University Of California Irvine</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,390.94,342.46,7.77;8,146.47,401.90,334.12,7.77;8,146.47,412.86,334.12,7.77;8,146.47,423.82,215.67,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,318.78,412.86,161.81,7.77;8,146.47,423.82,115.85,7.77">LifeCLEF 2017 Lab Overview: multimedia species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Willem-Pier And</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Christophe And</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,266.13,423.82,75.83,7.77">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,434.78,342.46,7.77;8,146.47,445.74,334.12,7.77;8,146.47,456.70,29.88,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,410.94,434.78,69.65,7.77;8,146.47,445.74,221.03,7.77">Unsupervised Individual Whales Identification: Spot the Difference in the Ocean</title>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Christophe</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anjara</forename><surname>Saloma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,373.81,445.74,80.24,7.77">CLEF (Working Notes</title>
		<imprint>
			<biblScope unit="page" from="469" to="480" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,467.66,342.46,7.77;8,146.47,478.62,334.12,7.77;8,146.47,489.57,108.34,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,281.83,467.66,198.76,7.77;8,146.47,478.62,88.49,7.77">Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration</title>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,254.53,478.62,226.06,7.77;8,146.47,489.57,83.91,7.77">International Conference on Computer Vision Theory and Applications (VISAPP&apos;09)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,500.53,342.46,7.77;8,146.47,511.49,297.49,7.77" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="8,315.22,500.53,165.37,7.77;8,146.47,511.49,178.52,7.77">Object Detection, Classification, Tracking and individual Recognition for Sea Images and Videos</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Papp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Lovas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabor</forename><surname>Szucs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">CLEF</note>
	<note>Working Notes</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
