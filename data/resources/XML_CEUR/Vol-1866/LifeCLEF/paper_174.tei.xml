<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.14,152.67,306.88,12.64;1,202.25,170.67,190.37,12.64">Image-based Plant Species Identification with Deep Convolutional Neural Networks</title>
				<funder>
					<orgName type="full">BMUB (Bundesministerium für Umwelt, Naturschutz</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,268.01,209.70,59.04,8.96"><forename type="first">Mario</forename><surname>Lasseck</surname></persName>
							<email>mario.lasseck@mfn-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Museum für Naturkunde Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.14,152.67,306.88,12.64;1,202.25,170.67,190.37,12.64">Image-based Plant Species Identification with Deep Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">48C8CBF5804F9FB9B65F879BD54D8553</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Plant Species Identification</term>
					<term>Biodiversity</term>
					<term>Deep Learning</term>
					<term>Convolutional Neural Networks</term>
					<term>Fine-grained Image Classification</term>
					<term>Data Augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents deep learning techniques for image-based plant identification at very large scale. State-of-the-art Deep Convolutional Neural Networks (DCNNs) are fine-tuned to classify 10,000 species. To improve identification performance several models trained on different datasets with multiple image dimensions and aspect ratios are ensembled. Various data augmentation techniques have been applied to prevent overfitting and to further improve model accuracy and generalization. The proposed approach is evaluated in the LifeCLEF 2017 campaign. It provides the best system among all participating teams by achieving a mean reciprocal rank (MRR) of 92 % and a top-5 accuracy of 96 % on the official PlantCLEF test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image-based plant identification is a promising tool supporting agriculture automation and environmental conservation efforts. It can be used via mobile applications like Pl@ntNet <ref type="bibr" coords="1,167.30,495.71,11.69,8.96" target="#b1">[2]</ref> or Naturblick <ref type="bibr" coords="1,239.09,495.71,11.69,8.96" target="#b2">[3]</ref> for education, biodiversity monitoring and the collection of plant observation records either by professionals or in a citizen science context. It helps to bridge the taxonomic knowledge gab and offers new interactive and efficient ways of browsing large image collections of flora. Distinguishing between 10,000 individual plant species is a challenging finegrained classification problem. One has to deal with categories that are very similar and often share a common part structure leading to low inter-class variations. On the other hand, plants are extremely diverse in size, shape, color and texture. Furthermore images of a particular species can contain different plant organs or content types. A single image can either show an entire plant or just a small part of it (e.g. flower, fruit, branch, stem or leaf) with significant changes of appearance throughout the year leading to high intra-class variations.</p><p>The LifeCLEF 2017 plant identification challenge aims to evaluate image-based plant identification systems close to conditions of real-world biodiversity monitoring scenarios at a very large scale. This year the LifeCLEF evaluation campaign provides two main datasets and participants are encouraged to evaluate and compare classifica-tion results using either one or both of them for training. The "trusted" training set is based on the online collaborative Encyclopedia of Life <ref type="bibr" coords="2,354.79,162.18,11.69,8.96" target="#b0">[1]</ref> with ca. 260,000 images coming from several public databases (Wikimedia, iNaturalist, Flickr, etc.) and institutions or websites dedicated to botany. Additionally up to 100,000 labeled and "trusted" images from previous campaigns are also provided. The second much larger "noisy" training set is built by web crawlers (e.g. Google and Bing image search). It contains over 1.4 million images among them many with wrong content (wrong species, portrait of a botanist working on a species, drawings, herbarium sheet of a dry specimen, etc.). All in all over 1.7 million images of 10,000 species of wild, cultivated, ornamental and endangered plants mostly coming from Western Europe and North American flora with different types of views (branch, entire plant, flower, fruit, leaf, stem, bark, scans of leaf, etc.) are provided and can be used for training. For evaluation a test set of 25,170 images belonging to 17,868 plant observations is provided. The test images represent typical smartphone application queries from Pl@ntNet and need to be classified by identifying the correct species for each observation. More information on datasets and task can be found in the LifeCLEF 2017 Lab Overview <ref type="bibr" coords="2,124.70,342.21,11.72,8.96" target="#b3">[4]</ref> and the plant identification task summery <ref type="bibr" coords="2,306.95,342.21,10.83,8.96" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Implementation Details and Model Training</head><p>To address the task of plant identification deep learning techniques are applied that already proved to be very successful in other image-based object classification scenarios. Several models are trained and prediction results are later bagged to increase identification accuracy on the test set. For late fusion, ensembles tend to yield better results if there is a significant diversity among the models <ref type="bibr" coords="2,362.71,447.21,15.43,8.96" target="#b13">[14]</ref>, <ref type="bibr" coords="2,384.91,447.21,15.31,8.96" target="#b14">[15]</ref>. In order to generate a diverse set of models the following aspects are varied across them: </p><formula xml:id="formula_0" coords="2,124.70,479.29,4.58,9.05"></formula><formula xml:id="formula_1" coords="2,124.70,653.80,72.71,33.42"> GoogLeNet [6]  ResNet [7]  ResNeXT [8]</formula><p>GoogLeNet won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) <ref type="bibr" coords="3,124.70,174.18,11.69,8.96" target="#b8">[9]</ref> in 2014 and was previously used for plant identification by <ref type="bibr" coords="3,390.12,174.18,15.48,8.96" target="#b16">[17]</ref>, <ref type="bibr" coords="3,413.07,174.18,16.79,8.96" target="#b19">[20]</ref> and <ref type="bibr" coords="3,451.59,174.18,15.29,8.96" target="#b20">[21]</ref>.</p><p>ResNet won ILSVRC and the Common Objects in Contexts (COCO) Detection Challenge <ref type="bibr" coords="3,149.54,198.18,16.76,8.96" target="#b9">[10]</ref> in 2015. For plant species recognition it was successfully used by <ref type="bibr" coords="3,442.75,198.18,16.79,8.96" target="#b17">[18]</ref>    To fine-tune a model the last fully-connected layer of the network is replaced and adapted to the 10k classes problem of the plant identification task. Before traininginstead of random initializationall weights except the ones of the exchanged layer are initialized with the pre-trained parameters.</p><p>In case of GoogLeNet, NVIDIA Digits <ref type="bibr" coords="3,298.49,657.74,16.72,8.96" target="#b11">[12]</ref> in combination with the Caffe framework <ref type="bibr" coords="3,148.82,669.74,16.72,8.96" target="#b12">[13]</ref> is used for training, data preparation and classification. The residual networks are trained via the MXNet framework <ref type="bibr" coords="3,320.35,681.74,15.43,8.96" target="#b13">[14]</ref>. All models are using either 2 NVIDIA GeForce GTX 1080 or 2 NVIDIA GeForce GTX 1080 Ti GPUs in parallel. Batch sizes are chosen mostly as large as possible to not run out of GPU memory. For some GoogLeNet models Caffe's iter_size parameter is applied to accumulate gradients over 2 batches. A fixed learning rate policy is used starting with a base learning rate around 0.01. It is decreased by a factor between 2 and 10 whenever validation loss or accuracy is not improving any longer. This is usually done twice within the entire training process. Stochastic Gradient Descent (SGD) and for some GoogLeNet models Nesterov's Accelerated Gradient (NAG) is used for model optimization. Since GoogLeNet is not providing batch normalization a mean image is computed from the training set and subtracted from all images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Preparation</head><p>To evaluate to what extent DCNNs can learn from noisy data compared to trusted data, some models are trained using only images of the Encyclopedia of Life dataset (plus in some cases images of the PlantCLEF 2016 dataset) while others are trained with all available images. It was also tried to form a mixture of both main datasets (see section Submissions and Results). Besides that, random partitions (stratified folds) of the datasets are created, so each model can use a different fold for validation and the remaining folds for training. The bagging of models trained on different folds was previously successfully applied by <ref type="bibr" coords="4,286.85,396.21,16.76,8.96" target="#b17">[18]</ref> and the winning team of the PlantCLEF 2015 task <ref type="bibr" coords="4,167.90,408.21,15.43,8.96" target="#b21">[22]</ref>. Instead of applying advanced methods like Borda-fuse as in <ref type="bibr" coords="4,441.99,408.21,16.83,8.96" target="#b21">[22]</ref> or taking the species-wise maximum as in <ref type="bibr" coords="4,291.55,420.21,15.51,8.96" target="#b17">[18]</ref>, in this work only simple averaging is performed for model ensembling.</p><p>Original images for training and testing are of arbitrary dimensions and aspect ratios. Since the networks used in this work only accept fixed sized square images as input, images need to be preprocessed via rescaling and/or cropping. To gain diversity across models, training images are rescaled to various dimensions and sometimes aspect ratios are additionally changed. Ensembling models using different image scales already improved results in previous PlantCLEF tasks <ref type="bibr" coords="4,370.61,504.23,15.73,8.96" target="#b22">[23]</ref>, <ref type="bibr" coords="4,392.95,504.23,15.34,8.96" target="#b18">[19]</ref>. Scaling images to different dimensions followed by random cropping helps to improve generalization by letting the network see patches of slightly different sections and resolutions of the original image. For GoogLeNet models, images are scaled to the following dimensions before random cropping is applied on-the-fly during training:</p><formula xml:id="formula_2" coords="4,124.70,572.29,68.80,33.54"> 256x256 pixel  250x250 pixel  240x240 pixel</formula><p>When using Digits, different resize transformations are chosen to handle non-square images. Figure <ref type="figure" coords="4,187.75,628.91,4.98,8.96" target="#fig_3">2</ref> visualizes the four transformation options offered by Digits. To not lose too much information per image only option 3 Half crop, half fill and 4 Squash which includes a warping of images by changing its aspect ratio is used in submission models. For models trained via the MXNet framework all images are resized such that the shorter side becomes 256 pixel while preserving the original aspect ratio. This is also done for all images in the test set. For test images additionally 5 patches are extracted by cropping square patchesthe same size as used for model trainingfrom each corner plus the center of the image. Predictions of these patches are averaged to gain a more robust classification result per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation</head><p>During training square patches are cropped in real-time from each image at random positions to serve as network input. Most models use input patches of size 224x224 pixel except for one GoogLeNet model which uses input patches of 230x230 pixel. After cropping, horizontal flipping is applied to randomly chosen patches. For residual networks additional data augmentation techniques were explored. For submission models the following image manipulation methods are used on-the-fly during training:</p><p> rotation by random angle  random variation of saturation (S channel in HSL color space)  random variation of lightness (L channel in HSL color space) Image patches are rotated by an angle randomly chosen between ± 45°. Color variation is applied in the HSL color space by adding values randomly chosen between ± 32 for saturation and ± 20 for lightness to the S and L channel (originally ranging from 0 to 255). When learning rate is decreased during training the maximum values for rotation angle and color variation are also decreased, letting the network see patches closer to the original image at the end of each training procedure.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submissions and Results</head><p>Identification performance of different model ensembles is evaluated on the official PlantCLEF 2017 test set. All submitted runs are created by aggregating predictions of test images using models of all three network architectures trained with different datasets and configurations. For each observation predictions are averaged over:</p><p> all images belonging to the same observation  all 5 patches cropped from the resized image  all models within the ensemble</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 1</head><p>For the first run three models (one per network architecture) are ensembled using only images from the trusted datasets Encyclopedia of Life (E) and PlantCLEF 2016 (P) for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 2</head><p>For the second run an ensemble of six models containing four GoogLeNets, one Res-Net-152 and one ResNeXt-101-64x4d is trained using images from datasets E and P plus all images from the noisy web dataset (W).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 3</head><p>For the third run the trusted datasets E and P are augmented by "good" (trustworthy) images from the noisy web dataset, as well as some images from the test set. To accomplish this, the web set is filtered by using models from the first run to identify plant species on web images. Correctly classified files (highest prediction equals ground truth) are selected to form a new filtered web dataset (FW) of 508,802 images. Furthermore some images from the test set are also added to the training set. Here the residual networks of run 1 and 2 are used to gather images with a prediction score greater than 0.98. The resulting set of 18,217 images is additionally filtered by choosing only images with at least 6 out of 20 predictions (5 predictions per image of 4 networks) suggesting the same species without confusion by any other species. This way, 9934 images from the test set are selected (T) and also added to the training set. Two GoogLeNet models pre-trained with data from the second run and one ResNeXt model from the first run are fine-tuned with the new formed trusted training set (E, P, FW, T) and used to aggregate predictions for the third run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 4</head><p>For the final and best scoring fourth run, all 12 models from the previous runs are ensembled and their predictions bagged via averaging.</p><p>Table <ref type="table" coords="7,151.31,596.87,4.98,8.96" target="#tab_1">1</ref> gives an overview of number of models, datasets used for training and the official results for each submitted run. Identification performance is measured by evaluating mean reciprocal rank (MRR), top-1 and top-5 accuracy on the test set. More information on models, their individual configurations, training details, validation scores and datasets used for each run can be found in a separate excel sheet <ref type="bibr" coords="8,445.87,263.82,15.41,8.96" target="#b25">[27]</ref>.</p><p>It should be mentioned, that during training, scores on validation sets were in most cases significantly lower than scores on the PlantCLEF test set. Especially when using images from the noisy dataset for training and validation, accuracy dropped to around 52 %. When looking at validation scores in <ref type="bibr" coords="8,311.71,311.85,16.72,8.96" target="#b25">[27]</ref> one has to take into account that models use different datasets and folds for validation. Only scores of models using the exact same datasets are comparable. Table <ref type="table" coords="8,309.25,335.85,4.98,8.96" target="#tab_2">2</ref> illustrates three examples of models using identical subsets for training and validation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>State-of-the-art DCNNs are powerful tools to identify objects in images. By finetuning pre-trained models (originally trained to classify ImageNet categories like cars, dogs, cats, etc.) they can be adapted to identify a large number of different plant species. By bagging several diverse models, identification performance can be significantly increased. To gain diversity, models are trained on different folds using various datasets, image dimensions, aspect ratios and augmentation techniques. Submission results suggest that if the number of training examples is sufficiently high DCNNs are capable of handling quite a fairly large amount of noise in the training set. For the third run the trusted training set is augmented by adding "good" examples from the noisy web dataset. To sort out which images contain correct content of plant species, DCNNs previously trained with trusted datasets are used to filter the web set. Although increasing identification accuracy by almost 8 % for models using the augmented dataset, it still does not reach the performance of models using all available data for training including many images with wrong content. A different filter approach, focused on discarding "bad" images rather than including "good" ones, might lead to better results and would be worth investigating in the future.</p><p>Due to time constraints it was not possible to systematically investigate the influence of certain settings on single network architectures. For this to be done, individual parameters need to be changed while keeping all other configurations the same. This was given up in favour of producing models with high diversity.</p><p>Unfortunately training from scratch was only carried out for a relatively small number of iterations using the trusted dataset exclusively. It was abandoned in favour of fine-tuning pre-trained models which seemed to be more promising at that time. Nevertheless it would be interesting to train networks from scratch for a longer period of time using all available images and to compare results with fine-tuned networks.</p><p>The models trained in this work will be further developed and later integrated into the mobile application Naturblick. The application provides information about urban nature in Berlin and offers different species identification tools including audio-based bird identification using algorithms developed and evaluated in previous LifeCLEF campaigns <ref type="bibr" coords="10,169.79,342.21,15.39,8.96" target="#b27">[29]</ref>, <ref type="bibr" coords="10,191.56,342.21,15.31,8.96">[30]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,462.91,198.18,7.75,8.96;3,124.70,210.18,206.94,8.96;3,331.75,207.99,6.48,5.83;3,340.99,210.18,129.59,8.96;3,124.70,222.18,346.00,8.96;3,124.70,234.18,345.96,8.96;3,124.70,246.18,345.69,8.96;3,124.70,258.18,345.83,8.96;3,124.70,270.18,345.75,8.96;3,124.70,282.21,346.00,8.96;3,124.70,294.21,74.16,8.96;3,136.10,306.21,334.60,8.96;3,124.70,318.21,345.88,8.96;3,124.70,330.21,345.88,8.96;3,124.70,342.21,345.91,8.96;3,124.70,354.21,345.93,8.96;3,124.70,366.21,345.85,8.96;3,124.70,378.21,345.86,8.96;3,124.70,390.21,346.00,8.96;3,124.70,402.21,245.18,8.96"><head></head><label></label><figDesc>in the PlantCLEF 2016 challenge. ResNeXT ranked 2 nd place in the ILSVRC 2016 classification task. It has not been used for plant identification before. The latter two residual network architectures allow to efficiently train very deep networks where each layer does not need to learn the whole feature space transformation but only a residual correction to the previous layer. The models trained for the current plant identification task are using the 152 layer version of ResNet (ResNet-152) and a 101 layer version with cardinality = 64 and bottleneck width = 4d of the ResNeXt architecture (Res-NeXt-101-64x4d).Instead of starting from scratch all networks are trained via transfer learning by fine-tuning models pre-trained on the ImageNet dataset<ref type="bibr" coords="3,357.55,318.21,15.43,8.96" target="#b10">[11]</ref>. GoogLeNet and Res-NeXt-101-64x4d had been pre-trained on the ILSVRC version of the dataset containing 1.2 million images of 1000 object categories while ResNet-152 had been trained on the 10 times larger complete ImageNet dataset covering 11k object classes. All pre-trained models mentioned here are publically available and can be downloaded for the corresponding frameworks [24],<ref type="bibr" coords="3,289.95,378.21,15.31,8.96" target="#b23">[25]</ref>,[26]. Previous work<ref type="bibr" coords="3,397.32,378.21,16.83,8.96" target="#b18">[19]</ref> but also own experiments with the trusted training set suggested that starting training with pretrained models leads to better results and faster convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,372.55,402.21,97.94,8.96;3,124.70,414.21,345.76,8.96;3,124.70,426.21,305.45,8.96"><head>Figure 1</head><label>1</label><figDesc>shows progress of validation accuracy over 40 training epochs using a pre-trained GoogLeNet model compared to a model started from scratch with randomly initialized weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,132.62,586.60,329.78,8.10;3,176.77,447.40,241.71,130.35"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Progress in validation accuracy using pre-trained networks vs. training from scratch.</figDesc><graphic coords="3,176.77,447.40,241.71,130.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,134.78,223.67,325.77,8.10"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Digits resize transformation options: 1. Crop 2. Fill 3. Half crop, half fill 4. Squash</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,136.10,569.63,334.46,8.96;5,124.70,581.63,345.88,8.96;5,124.70,593.63,345.88,8.96;5,124.70,605.63,345.69,8.96;5,124.70,617.63,346.12,8.96;5,124.70,629.63,138.48,8.96"><head>Figure 4 to 7</head><label>7</label><figDesc>show augmentation examples combining random cropping, horizontal flipping, rotation and variations of saturation and lightness. Image sources with original aspect ratios are visualized in figure3. The corresponding plant species and Me-diaIds are from left to right: Leucanthemum vulgare (MediaId: 254374), Streptanthus polygaloides (MediaId: 351199), Wikstroemia uva-ursi (MediaId: 378991) and Ipomoea sagittata (MediaId: 243459).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,209.21,222.59,176.74,8.10;6,303.90,163.85,67.15,50.15"><head>Fig. 3 .Fig. 4 .Fig. 5 .Fig. 6 .Fig. 7 .</head><label>34567</label><figDesc>Fig. 3. Image examples with original aspect ratio</figDesc><graphic coords="6,303.90,163.85,67.15,50.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,124.70,150.18,346.00,8.96;9,124.70,162.18,345.46,8.96;9,124.70,174.18,345.95,8.96;9,124.70,186.18,345.97,8.96;9,124.70,198.18,317.71,8.96"><head>Figure 8</head><label>8</label><figDesc>Figure 8 compares scores of all submissions to the LifeCLEF 2017 plant identification task. Run 2, 3 and 4 (MarioTsaBerlin) belong to the best scoring submissions among all participating teams. Especially run 2 and 4 show outstanding performances with a MRR of over 91 % and a top-5 accuracy of 96 % on the test set. More results and evaluation details can be accessed via the PlantCLEF 2017 homepage [28].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="9,131.30,466.60,332.84,8.10;9,183.86,477.52,227.54,8.10;9,124.70,231.40,345.90,225.80"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Official scores [MRR] of the LifeCLEF 2017 plant identification task. The above described methods and submitted runs belong to MarioTsaBerlin.</figDesc><graphic coords="9,124.70,231.40,345.90,225.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,130.58,149.99,332.33,84.15"><head>Table 1 .</head><label>1</label><figDesc>Number of models, training datasets and performance results of submitted runs</figDesc><table coords="8,130.58,168.90,332.33,65.24"><row><cell>Run</cell><cell cols="2"># Models Datasets</cell><cell cols="3">MRR [%] Top1 Acc. [%] Top5 Acc. [%]</cell></row><row><cell>1</cell><cell>3</cell><cell>E,P</cell><cell>84.7</cell><cell>79.4</cell><cell>91.1</cell></row><row><cell>2</cell><cell>6</cell><cell>E,P,W</cell><cell>91.5</cell><cell>87.7</cell><cell>96.0</cell></row><row><cell>3</cell><cell>3</cell><cell>E,P,FW,T</cell><cell>89.4</cell><cell>85.7</cell><cell>94.0</cell></row><row><cell>4</cell><cell>12</cell><cell>E,P,W,FW,T</cell><cell>92.0</cell><cell>88.5</cell><cell>96.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,128.18,371.66,338.48,290.96"><head>Table 2 .</head><label>2</label><figDesc>Implementation details of three models using all available images of the trusted and noisy datasets and identical subsets for training and validation</figDesc><table coords="8,128.18,401.49,338.48,261.13"><row><cell>Model-ID</cell><cell>M5</cell><cell>M8</cell><cell>M9</cell></row><row><cell cols="2">Network architecture GoogLeNet</cell><cell>ResNet-152</cell><cell>ResNeXt-101-64x4d</cell></row><row><cell>Framework</cell><cell>Digits/Caffe</cell><cell>MXNet</cell><cell>MXNet</cell></row><row><cell>Pre-trained via</cell><cell>ImageNet-1k</cell><cell>ImageNet-11k</cell><cell>ImageNet-1k</cell></row><row><cell>Datasets</cell><cell>E, P, W</cell><cell>E, P, W</cell><cell>E, P, W</cell></row><row><cell>Validation subset</cell><cell>2 nd fold</cell><cell>2 nd fold</cell><cell>2 nd fold</cell></row><row><cell>Image size [px]</cell><cell>250x250</cell><cell cols="2">shorter side 256 shorter side 256</cell></row><row><cell cols="3">Resize transformation half crop, half fill -</cell><cell>-</cell></row><row><cell>Crop size [px]</cell><cell>224x224</cell><cell>224x224</cell><cell>224x224</cell></row><row><cell>Augmentation</cell><cell>crop, mirror</cell><cell>crop, mirror,</cell><cell>crop, mirror, rota-</cell></row><row><cell></cell><cell></cell><cell>rotation, satura-</cell><cell>tion, saturation,</cell></row><row><cell></cell><cell></cell><cell>tion, lightness</cell><cell>lightness</cell></row><row><cell>Learning rate</cell><cell>0.015 40 epochs,</cell><cell>0.01 26 epochs,</cell><cell>0.01 15 epochs,</cell></row><row><cell>schedule</cell><cell>0.001 35 epochs,</cell><cell>0.001 9 epochs,</cell><cell>0.001 12 epochs</cell></row><row><cell></cell><cell>0.0005 28 epochs,</cell><cell>0.0001 10 epochs</cell><cell></cell></row><row><cell></cell><cell>0.0001 26 epochs</cell><cell></cell><cell></cell></row><row><cell>Batch size</cell><cell>128</cell><cell>44</cell><cell>48</cell></row><row><cell>Solver Type</cell><cell>SGD</cell><cell>SGD</cell><cell>SGD</cell></row><row><cell cols="2">Val. set accuracy [%] 52.74</cell><cell>52.53</cell><cell>52.99</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="8,231.15,668.66,15.17,8.96;8,305.45,667.58,22.71,8.96"><p>&amp; 4 2 &amp; 4</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. I would like to thank <rs type="person">Hervé Goëau</rs>, <rs type="person">Alexis Joly</rs>, <rs type="person">Pierre Bonnet</rs> and <rs type="person">Henning Müller</rs> for organizing this task. I also want to thank the <rs type="funder">BMUB (Bundesministerium für Umwelt, Naturschutz</rs><rs type="institution">, Bau und Reaktorsicherheit)</rs>, the <rs type="institution">Museum für Naturkunde Berlin</rs> and especially the <rs type="institution">Naturblick</rs> project team for supporting my research.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,132.67,471.04,274.04,8.10" xml:id="b0">
	<monogr>
		<ptr target="http://eol.org/,lastaccessed2017/05/21" />
		<title level="m" coord="10,141.74,471.04,114.80,8.10">Encyclopedia of Life Homepage</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,482.08,337.82,8.10;10,141.74,493.12,233.71,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,289.94,482.08,77.08,8.10">Pl@ntNet mobile app</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joly</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,385.73,482.08,84.76,8.10;10,141.74,493.12,163.48,8.10">Proceedings of the 21st ACM international conference on Multimedia</title>
		<meeting>the 21st ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="423" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,504.04,337.84,8.10;10,141.74,515.08,41.07,8.10" xml:id="b2">
	<monogr>
		<ptr target="http://naturblick.naturkundemuseum.berlin/" />
		<title level="m" coord="10,141.74,504.04,102.63,8.10">Naturblick App Homepage</title>
		<imprint>
			<date type="published" when="2017-05-21">2017/05/21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,526.12,337.70,8.10;10,141.74,537.04,329.00,8.10;10,141.74,548.08,182.65,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,256.70,537.04,214.04,8.10;10,141.74,548.08,66.24,8.10">LifeCLEF 2017 Lab Overview: multimedia species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,226.14,548.08,77.89,8.10">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,559.12,337.83,8.10;10,141.74,570.04,297.72,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,269.77,559.12,200.73,8.10;10,141.74,570.04,119.93,8.10">Plant identification based on noisy web data: the amazing performance of deep learning</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,343.84,570.04,75.35,8.10">CLEF working notes</title>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,581.08,316.14,8.10" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m" coord="10,228.76,581.08,118.02,8.10">Going Deeper with Convolutions</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,592.12,338.06,8.10;10,141.74,603.04,45.99,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,281.38,592.12,172.66,8.10">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,141.74,603.04,20.71,8.10">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,614.08,337.93,8.10;10,141.74,625.12,187.95,8.10" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,319.09,614.08,151.50,8.10;10,141.74,625.12,80.91,8.10">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,636.04,337.66,8.10;10,141.74,647.08,83.74,8.10" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,254.77,636.04,197.89,8.10">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,658.15,338.19,8.10;10,141.74,668.98,328.94,8.19;10,141.74,680.11,173.15,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,217.61,658.15,169.95,8.10">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,249.88,668.98,163.57,8.19">Computer Vision -ECCV 2014. ECCV 2014</title>
		<title level="s" coord="10,419.90,669.07,50.78,8.10;10,141.74,680.11,73.92,8.10">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8693</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,149.99,338.34,8.10;11,141.74,161.03,328.77,8.10;11,141.74,172.07,93.83,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,359.75,149.99,110.99,8.10;11,141.74,161.03,77.83,8.10">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,238.37,161.03,228.29,8.10">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,182.99,296.61,8.10" xml:id="b11">
	<monogr>
		<ptr target="https://developer.nvidia.com/digits" />
		<title level="m" coord="11,141.74,182.99,60.76,8.10">Digits Homepage</title>
		<imprint>
			<date type="published" when="2017-05-21">2017/05/21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,194.03,338.17,8.10;11,141.74,205.07,83.82,8.10" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="11,217.49,194.03,234.68,8.10">Caffe: Convolutional Architecture for Fast Feature Embedding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,215.99,338.10,8.10;11,141.74,227.03,328.71,8.10;11,141.74,238.07,132.17,8.10" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,218.79,215.99,251.71,8.10;11,141.74,227.03,116.77,8.10">MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,279.99,227.03,190.46,8.10;11,141.74,238.07,107.73,8.10">Neural Information Processing Systems, Workshop on Machine Learning Systems</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,248.99,337.97,8.10;11,141.74,260.03,116.57,8.10" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,266.17,248.99,165.30,8.10">Measures of diversity in classifier ensembles</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Whitaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,438.79,248.99,31.57,8.10;11,141.74,260.03,30.78,8.10">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="181" to="207" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,271.07,338.22,8.10;11,141.74,282.02,288.07,8.10" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,243.12,271.07,206.53,8.10">Learning with ensembles: How overfitting can be useful</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sollich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,456.25,271.07,14.36,8.10;11,141.74,282.02,177.26,8.10">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="190" to="196" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,293.06,338.46,8.10;11,141.74,304.10,288.59,8.10" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,310.66,293.06,160.20,8.10;11,141.74,304.10,172.21,8.10">Open-set Plant Identification Using an Ensemble of Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,332.81,304.10,97.52,8.10">CLEF2016 Working Notes</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,314.93,338.32,8.19;11,141.74,326.06,205.56,8.10" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,274.25,315.02,196.47,8.10;11,141.74,326.06,89.43,8.10">Very Deep Residual Networks with MaxOut for Plant Identification in the Wild</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,249.89,326.06,97.41,8.10">CLEF2016 Working Notes</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,337.10,338.21,8.10;11,141.74,348.02,328.97,8.10;11,141.74,359.06,97.52,8.10" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,331.51,337.10,139.10,8.10;11,141.74,348.02,309.52,8.10">Plant Identification System based on a Convolutional Neural Network for the LifeClef 2016 Plant Classification Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,141.74,359.06,97.52,8.10">CLEF2016 Working Notes</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,370.10,338.22,8.10;11,141.74,381.02,215.75,8.10" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,275.00,370.10,195.61,8.10;11,141.74,381.02,100.25,8.10">Feature Learning via Mixtures of DCNNs for Fine-Grained Plant Classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,259.96,381.02,97.52,8.10">CLEF2016 Working Notes</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,392.06,338.34,8.10;11,141.74,403.10,156.06,8.10" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,269.65,392.06,201.09,8.10;11,141.74,403.10,39.98,8.10">Floristic participation at LifeCLEF 2016 Plant Identification Task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,200.33,403.10,97.47,8.10">CLEF2016 Working Notes</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,414.02,337.90,8.10;11,141.74,425.06,298.64,8.10" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="11,195.90,414.02,274.40,8.10;11,141.74,425.06,112.63,8.10">Plant identification with deep convolutional neural network: Snumedinfo at lifeclef plant identification task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,291.44,425.06,148.94,8.10">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,436.10,337.98,8.10;11,141.74,447.02,182.39,8.10;11,124.82,458.06,59.25,8.10;11,211.44,458.06,23.60,8.10;11,262.49,458.06,15.37,8.10;11,305.31,458.06,10.43,8.10;11,343.09,458.06,11.07,8.10;11,381.48,458.06,19.95,8.10;11,428.87,458.06,41.58,8.10;11,141.74,469.12,243.98,8.10;11,406.27,469.12,12.47,8.10;11,439.27,469.12,31.26,8.10;11,141.74,480.04,41.08,8.10" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="11,292.32,436.10,178.06,8.10;11,141.74,447.02,66.45,8.10">Bluefield (KDE TUT) at LifeCLEF 2016 Plant Identification Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tatsuma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
		<ptr target="2017/05/21" />
	</analytic>
	<monogr>
		<title level="m" coord="11,226.72,447.02,97.41,8.10;11,124.82,458.06,59.25,8.10;11,211.44,458.06,23.60,8.10;11,262.49,458.06,15.37,8.10;11,305.31,458.06,10.43,8.10;11,343.09,458.06,11.07,8.10;11,381.48,458.06,19.95,8.10;11,428.87,458.06,37.42,8.10">CLEF2016 Working Notes 24. GoogLeNet Model files for the Caffe framework</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,491.08,84.41,8.10;11,237.77,491.08,22.41,8.10;11,280.95,491.08,15.37,8.10;11,317.29,491.08,10.43,8.10;11,348.60,491.08,11.07,8.10;11,380.50,491.08,27.56,8.10;11,428.95,491.08,41.47,8.10;11,141.74,502.12,205.97,8.10;11,124.82,513.04,7.58,8.10" xml:id="b23">
	<monogr>
		<ptr target="http://data.mxnet.io/models/imagenet/resnext/101-layers/26" />
		<title level="m" coord="11,141.74,491.08,75.07,8.10;11,237.77,491.08,22.41,8.10;11,280.95,491.08,15.37,8.10;11,317.29,491.08,10.43,8.10;11,348.60,491.08,11.07,8.10;11,380.50,491.08,27.56,8.10;11,428.95,491.08,37.32,8.10">ResNeXt-101-64x4d model files for the MXNet framework</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,513.04,338.22,8.10;11,141.74,524.08,150.50,8.10" xml:id="b24">
	<monogr>
		<ptr target="http://data.mxnet.io/models/imagenet-11k/resnet-152/,lastaccessed2017/05/21" />
		<title level="m" coord="11,141.74,513.04,184.37,8.10">ResNet-152 model files for the MXNet framework</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,535.12,338.31,8.10;11,141.74,546.04,89.51,8.10" xml:id="b25">
	<monogr>
		<ptr target="http://www.animalsoundarchive.org/RefSys/PlantCLEF2017" />
		<title level="m" coord="11,141.74,535.12,99.64,8.10">Model and training details</title>
		<imprint>
			<date type="published" when="2017-05-21">2017/05/21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,557.08,338.12,8.10;11,141.74,568.12,41.07,8.10" xml:id="b26">
	<monogr>
		<ptr target="/05/21" />
		<title level="m" coord="11,141.74,557.08,103.35,8.10">PlantCLEF 2017 Homepage</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,579.04,338.34,8.10;11,141.74,590.08,328.56,8.10;11,141.74,601.12,284.78,8.10;11,124.82,612.04,345.87,8.10;11,141.74,623.08,251.14,8.10" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="11,212.81,579.04,257.92,8.10;11,141.74,590.08,29.62,8.10;11,209.99,612.04,260.70,8.10;11,141.74,623.08,135.62,8.10">Improving Bird Identification using Multiresolution Template Matching and Feature Selection during Training</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,268.20,590.08,202.10,8.10;11,141.74,601.12,53.13,8.10;11,295.61,623.08,97.27,8.10">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<title level="s" coord="11,200.48,601.12,126.80,8.10">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</editor>
		<meeting><address><addrLine>Lasseck M</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015. 9283. 2016</date>
		</imprint>
	</monogr>
	<note>CLEF2016 Working Notes</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
