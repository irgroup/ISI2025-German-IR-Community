<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.24,152.99,283.57,13.60;1,198.12,170.99,199.50,13.60">Marine Animal Detection and Recognition with Advanced Deep Learning Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,174.24,210.18,56.34,9.63"><forename type="first">Peiqin</forename><surname>Zhuang</surname></persName>
							<email>pq.zhuang@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shenzhen key Lab. Of CVPR</orgName>
								<orgName type="institution">Shenzhen Institutes of Advanced Technology Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.84,210.18,44.33,9.63"><forename type="first">Linjie</forename><surname>Xing</surname></persName>
							<email>lj.xing@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shenzhen key Lab. Of CVPR</orgName>
								<orgName type="institution">Shenzhen Institutes of Advanced Technology Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.20,210.18,41.97,9.63"><forename type="first">Yanlin</forename><surname>Liu</surname></persName>
							<email>yl.liu@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shenzhen key Lab. Of CVPR</orgName>
								<orgName type="institution">Shenzhen Institutes of Advanced Technology Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.80,210.18,42.22,9.63"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
							<email>sheng.guo@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shenzhen key Lab. Of CVPR</orgName>
								<orgName type="institution">Shenzhen Institutes of Advanced Technology Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.60,210.18,34.01,9.63"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shenzhen key Lab. Of CVPR</orgName>
								<orgName type="institution">Shenzhen Institutes of Advanced Technology Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.24,152.99,283.57,13.60;1,198.12,170.99,199.50,13.60">Marine Animal Detection and Recognition with Advanced Deep Learning Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">617F9AE86419656BFA470071866EEEF9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Fish Detection</term>
					<term>Weakly-labelled</term>
					<term>Image Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper summarizes SIATMMLAB's contributions in SEACLEF-2017 task [1]. We took part in three subtasks with advanced deep learning models. In Automatic Fish Identification and Species Recognition task, we exploited different frameworks to detect the proposal boxes of foreground fish, then fine-tuned a pre-trained neural network to classify the fish. In Automatic Frame-level Salmon Identification task, we utilized the BN-Inception [2] network to identify whether a video frame contains salmons or not. In Marine Animal Recognition task, we examined different neural networks to make classification based on weakly-labelled images. Our methods achieve good results in both task1 and task3.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Driven by the increasing demand of ecological surveillance and biodiversity monitoring under the water, more sea-related multimedia data were collected with the aid of advanced imaging systems. However, with the exponential growth of searelated visual data, it is prohibitive to count on the manual handling by the experts to annotate these datasets. Therefore, automatic analyzing the contents of underwater image is key to make use of the exponentially increasing underwater data. SEACLEF2017 <ref type="bibr" coords="1,190.80,555.06,11.71,9.63" target="#b0">[1]</ref> have launched four tasks to explore suitable methods for handling these multimedia data.</p><p>In this paper, we elaborate our methods applied in SEACLEF2017, and the remainder of this paper is organized as follows. In Section 2, we present our approach for task 1, including the method of proposing foreground boxes, reducing background boxes and classifying the boxes we got. In Section 3, we report our approach for frame-level salmon identification in task 2. In Section 4, we describe the procedure for handling weakly-labeled fish data in task 3. Finally, we provide a comprehensive analysis of our works and discuss the directions for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Species Recognition on Coral Reef Videos</head><p>In this task, we automatically identify and recognize fish species by giving a bounding box and a corresponding label. To reach this goal, we employed a traditional pipeline <ref type="bibr" coords="2,208.44,201.66,11.45,9.63" target="#b2">[3,</ref><ref type="bibr" coords="2,219.89,201.66,7.63,9.63" target="#b3">4,</ref><ref type="bibr" coords="2,227.52,201.66,7.63,9.63" target="#b4">5]</ref> for detecting fish. Firstly, we used different detection architectures based on deep neural networks to generate potential bounding boxes, and then fine-tuned a pre-trained neural network with training data to classify bounding boxes. Section 2.1 will present the two methods we used to detect bounding boxes, while Section 2.2 describes the procedure of classification. We show our final result in Section 2.3. At the end, we make some discussions on how to improve detection performance in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Foreground Detection</head><p>Inspired by the past participants of SEACLEF <ref type="bibr" coords="2,318.00,321.66,10.84,9.63" target="#b7">[8,</ref><ref type="bibr" coords="2,328.84,321.66,7.23,9.63" target="#b8">9]</ref>, we separate species recognition process into detection step and classification step. In the past three years, end-to-end <ref type="bibr" coords="2,124.68,345.66,11.45,9.63" target="#b2">[3,</ref><ref type="bibr" coords="2,136.13,345.66,7.63,9.63" target="#b3">4,</ref><ref type="bibr" coords="2,143.76,345.66,7.63,9.63" target="#b4">5]</ref> detection methods have prevailed in most of detection task because of their dramatic performance, so we selected two latest models as our detector. Firstly, we chose SSD <ref type="bibr" coords="2,229.92,369.66,11.71,9.63" target="#b4">[5]</ref> to differentiate the regions between foreground fish and background. With the advance of producing predictions from different features maps of different scales <ref type="bibr" coords="2,200.52,393.66,10.84,9.63" target="#b4">[5,</ref><ref type="bibr" coords="2,211.36,393.66,7.23,9.63" target="#b5">6]</ref>, we got acceptable results by using SSD to generate potential bounding boxes. Besides, we also used another detection architecture PVANET <ref type="bibr" coords="2,460.32,405.66,11.71,9.63" target="#b6">[7]</ref> to detect foreground fish. At detection step, we extracted all true positive frames with annotations and then chose one tenth of positive frames at each video to formulate validation dataset. The rest of positive frames were used for training. A detection result example is shown in Fig. <ref type="figure" coords="2,252.12,453.66,3.76,9.63" target="#fig_0">1</ref>. Afterwards to remove false positives, we adopted Sungbin Chois's <ref type="bibr" coords="3,413.28,150.66,11.71,9.63" target="#b7">[8]</ref> method to compute a background image (Fig. <ref type="figure" coords="3,270.96,162.66,4.18,9.63">2</ref>) by selecting the median value of at each pixel position for every video, then used background subtraction and erosion to create a mask (Fig. <ref type="figure" coords="3,172.44,186.66,4.18,9.63">3</ref>) for each frame. If the area of background in bounding box is greater than a threshold we set, we consider it as background, and consequently discard it. Compared to other background subtraction methods <ref type="bibr" coords="3,344.40,210.66,10.84,9.63" target="#b7">[8,</ref><ref type="bibr" coords="3,355.24,210.66,7.23,9.63" target="#b8">9]</ref>, we adopted it as a postprocessing means after detection, not directly for detecting bounding boxes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Species Classification</head><p>In the classification step, we extracted true positives from the training dataset with the annotation information. Besides, we also added some background as a new class to further reduce the amounts of false positives in our final results. Given species imbalance in each video, we extracted all videos together and then separated those patches into training set and validation set according to the labels. The ratio between training dataset and validation dataset is the same as in the detection step.</p><p>For the sake of attaining a high performance of classification, we chose the ResNet-10 [10] as our classifier. According to the relevant information of this task, we set the number of output classes as 16, and used the training dataset to fine-tune this ResNet-10 network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results</head><p>The test dataset contains 73 videos, similar scenario to the training dataset. By employing the procedure of detection and classification, we finally submitted two results. SIATMMLAB_run1 used the SSD framework to detect foreground fish, while SIATMMLAB_run2 utilized the PVANET to generate potential bounding boxes as well. Both classification of two above results used ResNet-10 network as classifier. Our normalized scores of two results are respectively 0.66 and 0.71 (Table <ref type="table" coords="4,433.44,330.66,3.63,9.63" target="#tab_0">1</ref>), nearly equal to the best result <ref type="bibr" coords="4,217.20,342.66,11.71,9.63" target="#b7">[8]</ref> in 2015. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Discussion</head><p>Although our approaches achieve a good performance on task 1, there still exist some aspects need to be further improved. By analyzing the results, we found that several fish were not detected in some particular video clips , which influences the counting score and precision. Besides, our detector often mistook some background regions with abundant texture for foreground fish. Since the video naturally has context information between frames, it will help us to fix the discontinuity of video sequence in detection results and achieve a higher performance in the future.</p><p>In task 1, we combined both detection and classification methods to recognize fish species on coral reef videos, and achieved an exciting results in the end. In the future, we will try more fusions of detection and classification, use some pre-processing methods to augment video resolution, reduce noise influence caused by the illumination changes, and utilize more video context information. We believe that effective incorporation of these methods will provide an opportunity for ecologist to monitor biodiversity more accurately than manual handling in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame-level Salmon Identification</head><p>In this task, we need identify the appearance of salmon in each frame <ref type="bibr" coords="5,416.64,177.66,10.59,9.63" target="#b0">[1]</ref>. Since the ratio of frame which salmon appeared is rare and the salmons are often small, this task appears to be challenging for us. In Section 3.1, we will analyze some relevant information about the training data; Section 3.2 presents the method we used and our final result. Finally, we will make a discussion about task 2 in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Analysis</head><p>The training data contains 8 videos about salmon. By analyzing the frames extracted from all videos, it is notable that the amounts of positives are much less than the counterpart of negatives. There are nearly 1300 positives, while the negatives numbers are almost 59k, which creates a large data imbalance between positive samples and negative samples. Besides, most of the frames are only filled with black background, which means information about salmon in the fame is rare (Fig. <ref type="figure" coords="5,458.52,333.66,3.32,9.63" target="#fig_2">4</ref>).. Some frames even contain a green static water turbine in it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments and Results</head><p>In order to solve the imbalance between positives and negatives, we used all positives and randomly selected 1500 negatives to formulate our training dataset and validation dataset. We chose nine tenths of those as training set , while the rest were used as validation set.</p><p>We regarded frame-level salmon identification as a binary classification problem, so we use the BN-Inception <ref type="bibr" coords="5,246.24,617.22,11.71,9.63" target="#b1">[2]</ref> network as our classifier. Through fine-tuning our network with training dataset, we achieved 97% accuracy in our validation dataset. We also tested the model at the whole dataset, which showed only 2723 images were incorrectly classified within 59956 images.</p><p>The test dataset contains 8 videos. Compared to the training dataset, the test dataset seems to be a little different from training dataset. The green water turbine in most of frames is always revolving, which causes illumination changes and increases much noise. In this case, our model often mistook many frames with illumination changes for positives and created many false positives. We submitted one run as our final result. The precision for our result is 0.04 (Table <ref type="table" coords="6,344.64,186.66,3.63,9.63">2</ref>), which means most of our positives are incorrect. Our model performance suffers a huge decline in test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2. Our results on task2 measured by precision, recall and F-measure metric</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision</head><p>Recall F-measure SIATMMLAB_run1 0.04 0.82 0.07</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>We adopted BN-Inception network to identify whether a video frame contains the salmons or not. Although our model successfully reached the goal of this task on the training dataset, the accuracy of test dataset performed poorly with challenging constraints. Firstly, since the salmon is so small and most of frames are filled with black background, it is challenging for us to choose frames, which essentially represent the properties of positives and negatives. Next, our model finally mistook many negatives for positives because of the illumination changes brought by the water turbine revolving. Given these constraints, more work will be carried out to help improve models performance. Strengthening feature representation and introducing data-mining tricks may further solve the problem of frames selection. Besides, more pre-processing technologies will be employed to improve the quality of frames and reduce influence of illumination changes. According to the two above aspects, we will incorporate more methods to increase our model generalization and accuracy in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Marine Animal Species Recognition</head><p>Task 3 aims to classify marine animals from weakly-labelled images collected by keyword queries on the internet <ref type="bibr" coords="6,254.52,540.54,10.68,9.63" target="#b0">[1]</ref>. With the difficulty of high similarity between two species and weakly-labelled annotations, making exact recognition seems to be challenging for us. Section 4.1 will analyze the training dataset and its existing difficulties. Section 4.2 shows the details of our experiments and final results. Finally, we will make a slight discussion about our work in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Analysis</head><p>The training dataset includes nearly 13k images, but some of species have little numbers of images compared to other species. During the period of our experiment, we also found some other flaws in the training dataset: some maps describing the distribution region of corresponding species were added to the training dataset and sometimes two identical images both appeared in different species, which often caused classification mistakes. In this case, we made a slight examination of training dataset to pick out the bad data. Besides, we also added nearly 2k images extracted from YouTube videos to increase the generalization of our model. We split all the data into training data and validation data and selected 10 images per species as validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments and Results</head><p>Driven by the high performance of deep neural networks <ref type="bibr" coords="7,360.84,246.78,11.93,9.63" target="#b1">[2,</ref><ref type="bibr" coords="7,372.77,246.78,11.93,9.63" target="#b9">10,</ref><ref type="bibr" coords="7,384.70,246.78,11.93,9.63" target="#b10">11,</ref><ref type="bibr" coords="7,396.62,246.78,11.93,9.63" target="#b11">12]</ref>, we decided to use two latest architectures of neural network: BN-Inception <ref type="bibr" coords="7,374.64,258.78,11.71,9.63" target="#b1">[2]</ref> and ResNet-50 <ref type="bibr" coords="7,453.00,258.78,15.43,9.63" target="#b9">[10]</ref>. We did many experiments on training data, and evaluated our performance on the validation set with the top-1 and top-5 metric. Some detail information are shown in Table <ref type="table" coords="7,150.00,294.78,3.76,9.63" target="#tab_1">3</ref>.  <ref type="bibr" coords="7,188.64,484.86,11.45,9.63" target="#b1">[2,</ref><ref type="bibr" coords="7,200.09,484.86,11.45,9.63" target="#b9">10]</ref>, resizing the input image size, using different crop size <ref type="bibr" coords="7,442.80,484.86,16.70,9.63" target="#b12">[13,</ref><ref type="bibr" coords="7,459.50,484.86,12.53,9.63" target="#b13">14]</ref> to augment the training dataset. All our experiments achieved high performance in the validation data in spite of the challenge this task brings. The accuracies measured by top-1 metric were all higher than 0.8, and the counterparts measured by Top-5 metric were also higher than 0.93. We finally submitted three runs. According to the source of test dataset, we separated test dataset into two parts. Those images cropped from videos, which have similar scenario with task 1's data, were tested by a new RestNet-50 network trained with task 1's video data. The rest of test data were tested by the ablation of different models <ref type="bibr" coords="7,158.64,592.86,16.63,9.63" target="#b14">[15]</ref> we have described before. SIATMMLAB_run1 used the ablation of experiment 1 and experiment 4. The ablation of experiment 1, experiment 3 and experiment 4 provided the result of SIATMMLAB_run2. SIATMMLAB_run3 includes the result of experiment 1 and experiment 3. The metric used to measure final runs is average precision, the result of our runs are shown in Table <ref type="table" coords="7,416.76,640.86,3.76,9.63" target="#tab_2">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>Although our model achieved more than 0.8 accuracy with top-1 metric on the validation dataset, it seems its performance slightly decreased in the test dataset.</p><p>Recalling the whole procedure of our experiment, we have not used the relevance ranking information when we trained our model. Since the images are weaklylabelled and the metric used to measure our results is average precision, using the relevance information may help us to recognize the species more accurately in the future. Furthermore, some species have high degree of similarity to each other, even our human beings can not classify them easily without professional knowledge. How to find an effective method to essentially represent these species is a key problem we want to solve in our future work. Besides, some fish species were so small compared to the whole image. In this case, we will employ salience map to primarily locate the positions of fish, preparing for the recognition step.</p><p>Automatically recognizing fish species is essential for handling sea-related multimedia data.We hope to further improve classifier with more advanced models ablation and auxiliary methods so as to recognize fish species more accurately when the ecologist use these methods to monitor biodiversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Perspectives</head><p>This paper has described our participation in SEACLEF2017. All our approaches are based on the architectures of deep neural network, and achieved high performance both in task 1 and task 3. Although the methods using with deep neural network have good effect in this competition, there still exist some constrains, like low-resolution, illumination changes and complicated background, when we handle these underwater multimedia data. Based on these constraints, we will respectively investigate more effective methods to solve these aspects and try to raise our performance to a new level. More related work will be carried out to improve our model performance and help to promote these methods to be used in the real-world application.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,124.68,639.66,328.07,9.63;2,124.68,652.53,300.93,8.71;2,154.68,474.36,263.52,160.68"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A detection result example, fish were enclosed by red bounding boxes with their corresponding scores, which shows the confidence of being detected as foreground.</figDesc><graphic coords="2,154.68,474.36,263.52,160.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,124.68,405.66,319.53,9.63;3,124.68,418.53,29.10,8.71;3,154.68,438.36,274.56,149.88"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. An example background computed by selecting the median value at each pixel position</figDesc><graphic coords="3,154.68,438.36,274.56,149.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,124.68,498.09,347.22,8.71;5,124.68,510.09,13.50,8.71;5,136.08,366.48,318.84,126.24"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Two different images show the appearance of salmon in the frame with red bounding box</figDesc><graphic coords="5,136.08,366.48,318.84,126.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,124.68,367.53,340.87,90.52"><head>Table 1 .</head><label>1</label><figDesc>Our final results on task1 measured by counting score, precision and normalized score</figDesc><table coords="4,129.36,393.69,336.19,64.36"><row><cell></cell><cell>Counting score</cell><cell>Precision</cell><cell>Normalized score</cell></row><row><cell>SIATMMLAB_run1</cell><cell>0.87</cell><cell>0.76</cell><cell>0.66</cell></row><row><cell>SIATMMLAB_run2</cell><cell>0.88</cell><cell>0.80</cell><cell>0.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,124.68,319.65,347.15,174.84"><head>Table 3 .</head><label>3</label><figDesc>Our experiment description on task 3 by using different neural networks,changing input size and crop size</figDesc><table coords="7,124.68,345.81,347.15,148.68"><row><cell>Model</cell><cell cols="2">Input size Crop size</cell><cell>Top-1 Acc.</cell></row><row><cell cols="2">Experiment1 BN-Inception 360×260</cell><cell>224</cell><cell>0.840</cell></row><row><cell cols="2">Experiment2 BN-Inception 600×400</cell><cell>224</cell><cell>0.818</cell></row><row><cell cols="2">Experiment3 BN-Inception 600×400</cell><cell>336</cell><cell>0.845</cell></row><row><cell>Experiment4 ResNet-50</cell><cell>360×260</cell><cell>224</cell><cell>0.837</cell></row><row><cell>Experiment5 ResNet-50</cell><cell>600×400</cell><cell>224</cell><cell>0.800</cell></row><row><cell cols="4">We did such 5 experiments on the training dataset by changing the deep neural</cell></row><row><cell>network model</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,129.84,151.53,303.78,79.00"><head>Table 4 .</head><label>4</label><figDesc>Our final result measured by the metric of average precision</figDesc><table coords="8,129.84,165.57,303.78,64.96"><row><cell></cell><cell>Pr@1</cell><cell>Pr@2</cell><cell>Pr@3</cell></row><row><cell>SIATMMLAB_run1</cell><cell>0.61</cell><cell>0.71</cell><cell>0.74</cell></row><row><cell>SIATMMLAB_run2</cell><cell>0.61</cell><cell>0.71</cell><cell>0.75</cell></row><row><cell>SIATMMLAB_run3</cell><cell>0.61</cell><cell>0.72</cell><cell>0.76</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,137.08,178.53,334.63,8.71;9,124.68,190.53,347.13,8.71;9,124.68,202.53,347.37,8.71;9,124.68,214.53,67.05,8.71" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,124.68,202.53,284.31,8.71">LifeCLEF 2017 Lab Overview: multimedia species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Willem-Pier</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Christophe</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,416.28,202.53,55.77,8.71;9,124.68,214.53,22.05,8.71">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,137.44,225.90,334.57,9.63;9,124.68,238.53,245.97,8.71" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,281.04,226.53,190.98,8.71;9,124.68,238.53,116.09,8.71">Batch normalization: Accelerating deep network by reducing internal covariate shift</title>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,255.84,238.53,19.08,8.71">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,137.44,249.90,170.69,9.63" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,198.96,250.53,32.69,8.71">Fast-rcnn</title>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,247.20,250.53,18.12,8.71">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,137.44,261.90,334.30,9.63;9,124.68,274.53,267.69,8.71" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,345.72,262.53,126.02,8.71;9,124.68,274.53,167.42,8.71">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,308.16,274.53,16.97,8.71">NIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009">2015. 8,9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,137.44,285.90,334.27,9.63;9,124.68,298.53,341.01,8.71" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,234.84,298.53,124.89,8.71">SSD: single shot multibox detector</title>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,375.12,298.53,20.23,8.71">ECCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.40,309.66,333.40,9.63;9,124.68,321.66,347.27,9.63;9,124.68,333.66,103.05,9.63" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,196.92,321.66,205.60,9.63">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName coords=""><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,137.44,345.90,333.89,9.63;9,124.68,358.53,347.58,8.71;9,124.68,370.53,123.45,8.71" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,167.40,358.53,275.84,8.71">Deep but Lightweight Neural Networks for Real-time Object Detection</title>
		<author>
			<persName coords=""><forename type="first">Kye-Hyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Byungseok</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yeongjae</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minjie</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pvanet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08588</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,137.44,381.90,334.01,9.63;9,124.68,394.53,347.13,8.71;9,124.68,406.53,347.13,8.71;9,124.68,418.53,90.32,8.71" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,201.84,382.53,269.61,8.71;9,124.68,394.53,157.98,8.71">Fish identification in underwater video with deep convolutional neural nerwork : Snumedinfo at lifeclef fish task</title>
		<author>
			<persName coords=""><forename type="first">Sungbin</forename><surname>Choi</surname></persName>
		</author>
		<idno>nbn:de:0074-1391-8</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,321.24,394.53,150.57,8.71;9,124.68,406.53,142.86,8.71">Working Notes of the 6th International Conference of the CLEF Initiative</title>
		<title level="s" coord="9,278.04,406.53,118.07,8.71">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">1391</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,137.08,430.53,334.97,8.71;9,124.68,442.53,347.22,8.71;9,124.68,454.53,347.34,8.71;9,124.68,466.53,201.20,8.71" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,124.68,442.53,332.11,8.71">SeaCLEF 2016: Object Proposal Classification for Fish Detection in Underwater Videos</title>
		<author>
			<persName coords=""><forename type="first">Jonas</forename><surname>Jäger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Viviane</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus</forename><surname>Fricke-Neuderth</surname></persName>
		</author>
		<idno>nbn:de:0074-1609-5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,124.68,454.53,277.62,8.71">Working Notes of the 7th International Conference of the CLEF Initiative</title>
		<title level="s" coord="9,408.72,454.53,63.30,8.71;9,124.68,466.53,43.31,8.71">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1609</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,141.18,478.53,330.33,8.71;9,124.68,490.53,175.41,8.71" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,349.68,478.53,121.83,8.71;9,124.68,490.53,39.68,8.71">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,179.88,490.53,19.85,8.71">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,141.18,502.53,330.72,8.71;9,124.68,514.53,167.49,8.71" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,352.56,502.53,119.34,8.71;9,124.68,514.53,107.66,8.71">Image classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Krizhevsky</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sutskever</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,248.40,514.53,19.02,8.71">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,141.18,526.53,330.78,8.71;9,124.68,538.53,148.41,8.71" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,293.28,526.53,178.68,8.71;9,124.68,538.53,88.87,8.71">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,229.44,538.53,16.97,8.71">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,143.02,549.66,328.64,9.63;9,124.68,561.66,113.01,9.63" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,358.80,549.66,112.85,9.63;9,124.68,561.66,36.91,9.63">ParseNet: Looking wider to see better</title>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,178.56,561.66,19.27,9.63">ILCR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,143.02,573.66,328.31,9.63;9,124.68,585.66,347.45,9.63;9,124.68,597.66,270.09,9.63" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,425.40,573.66,45.93,9.63;9,124.68,585.66,347.45,9.63;9,124.68,597.66,23.30,9.63">Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNS</title>
		<author>
			<persName coords=""><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,156.36,597.66,163.25,9.63">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2055" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,143.02,609.66,328.52,9.63;9,124.68,621.66,347.13,9.63;9,124.68,633.66,35.85,9.63" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,368.28,609.66,103.25,9.63;9,124.68,621.66,148.33,9.63">Locally-Supervised Deep Hybrid Model for Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,280.20,621.66,164.57,9.63">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="808" to="820" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
