<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.94,152.79,309.23,12.64;1,150.74,170.79,293.63,12.64;1,202.25,188.79,190.37,12.64">UPB HES SO @ PlantCLEF 2017: Automatic Plant Image Identification using Transfer Learning via Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.74,228.42,65.18,8.96"><forename type="first">Alexandru</forename><surname>Toma</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Multimedia Lab -CAMPUS</orgName>
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.85,228.32,50.73,9.05"><forename type="first">Daniel</forename><surname>Liviu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Multimedia Lab -CAMPUS</orgName>
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.07,228.32,61.66,9.06"><forename type="first">Bogdan</forename><surname>È˜tefan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Multimedia Lab -CAMPUS</orgName>
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,377.20,228.42,31.11,8.96"><surname>Ionescu</surname></persName>
							<email>bionescu@alpha.imag.pub.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Multimedia Lab -CAMPUS</orgName>
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.94,152.79,309.23,12.64;1,150.74,170.79,293.63,12.64;1,202.25,188.79,190.37,12.64">UPB HES SO @ PlantCLEF 2017: Automatic Plant Image Identification using Transfer Learning via Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EAF72BC11E4F560225DCCFFDF3635E72</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>plant identification</term>
					<term>deep learning</term>
					<term>transfer learning</term>
					<term>convolutional neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in computer vision have made possible the use of neural networks in large scale image retrieval tasks. An example application is the automated plant classification. However, training a network from scratch takes a lot of computational effort and turns out to be very time consuming. In this paper, we investigate a transfer learning approach in the context of the 2017 PlantCLEF task, for automatic plant image classification. The proposed approach is based on the well-known AlexNet Convolutional Neural Network (CNN) model. The network was fine-tuned using the 2017 PlantCLEF Encyclopedia of Life (EOL) training data, which consists of approximately 260,000 plant images belonging to 10,000 species. The learning process was sped up in the upper layers leaving original features almost untouched. Our best proposed official run scored 0,361 in terms of the Mean Reciprocal Rank (MRR) when evaluated on the test dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Plants are one of the most vital life forms on Earth, having a tremendous contribution to the well-functioning of the ecosystems. Hence plant taxonomy plays a key role in preservation of plant species across the globe. Nevertheless, plant taxonomy is problematic and often results in duplicate identifications, given the difficulty of the classification task and the error of the human operator performing the manual classification. Therefore, the plant identification challenge of the Conference and Labs of the Evaluation Forum (CLEF) <ref type="bibr" coords="1,213.39,595.55,11.67,8.96" target="#b4">[5,</ref><ref type="bibr" coords="1,225.06,595.55,7.78,8.96" target="#b5">6,</ref><ref type="bibr" coords="1,232.84,595.55,7.78,8.96" target="#b6">7,</ref><ref type="bibr" coords="1,240.62,595.55,7.78,8.96" target="#b7">8,</ref><ref type="bibr" coords="1,248.40,595.55,7.78,8.96" target="#b8">9,</ref><ref type="bibr" coords="1,256.18,595.55,11.67,8.96" target="#b9">10]</ref> in conjunction with initiatives such as LeafSnap <ref type="bibr" coords="1,124.70,607.55,16.76,8.96" target="#b10">[11]</ref> and Pl@ntNet <ref type="bibr" coords="1,205.51,607.55,16.86,8.96" target="#b11">[12]</ref> aim to bring together computer vision enthusiasts and botanists to exploit big amounts of raw image queries in a fully automatic way and therefore to ensure a sustainable approach when comes to ecological monitoring studies and environmental conservation. Convolutional neural networks came to the attention of the research groups when a team led by Geoffrey Hinton and Alex Krizhevsky won the ImageNet Large Scale Visual Recognition Competition (ILSVRC) <ref type="bibr" coords="1,305.84,679.58,16.96,8.96" target="#b12">[13]</ref> with record-breaking results <ref type="bibr" coords="1,444.55,679.58,10.68,8.96" target="#b0">[1]</ref>. In order to train the model, they used a subset of the ImageNet database. Therefore, the model was trained on more than one million images and was able to classify images into 1,000 objects categories.</p><p>Lately, a trend has been observed that industry together with research groups are using more and more in their work deep convolutional networks architectures, due to the high performance outputs. The same trend was also observed among the research groups participating in the LifeCLEF plant identification challenge with outstanding performance <ref type="bibr" coords="2,178.99,234.42,11.00,8.96" target="#b1">[2,</ref><ref type="bibr" coords="2,190.00,234.42,7.34,8.96" target="#b2">3,</ref><ref type="bibr" coords="2,197.33,234.42,7.34,8.96" target="#b3">4]</ref>. Likewise, transfer learning became a common practice among researchers when comes to the use of convolutional neural networks <ref type="bibr" coords="2,400.15,246.42,16.83,8.96" target="#b13">[14,</ref><ref type="bibr" coords="2,416.98,246.42,12.62,8.96" target="#b14">15,</ref><ref type="bibr" coords="2,429.60,246.42,12.62,8.96" target="#b15">16]</ref> mainly because fine-tuning a network is much faster and easier than training from scratch. Moreover, the pre-trained network has already learned a rich set of features that can be applied to a wider range of tasks.</p><p>PlantCLEF 2016 campaign has brought together 94 research groups. Among these only 8 research groups succeeded in submitting their runs <ref type="bibr" coords="2,371.65,306.45,15.66,8.96" target="#b9">[10]</ref>. They had to build robust plant classification systems in order to solve a multi-organ plant classification problem, i.e., identification of 1,000 species of plants corresponding to 7 different organs, along with an open-set recognition problem, i.e., automatic detection of invasive species from unknown classes.</p><p>PlantCLEF 2017 campaign, continues the challenge and aims to automatically detect in the Pl@ntNet raw query images, specimens of plants belonging to the provided training data. Another objective for this year is to evaluate the performance of a system built with noisy data against one built using trusted data. Therefore, two main training sets are provided, each being part of the same list of 10,000 plant species: a "trusted" training set based on the online collaborative Encyclopedia of Life (EoL) and a "noisy" training set built using web crawlers <ref type="bibr" coords="2,329.09,438.35,16.08,9.05" target="#b19">[20,</ref><ref type="bibr" coords="2,345.16,438.35,12.06,9.05" target="#b20">21]</ref>.</p><p>In this paper, we present the participation of the UPB HES SO team to the task. Our proposal is a transfer learning approach, adapted to plant image classification. We use a pre-trained model of the AlexNet CNN <ref type="bibr" coords="2,332.43,474.47,10.72,8.96" target="#b0">[1]</ref>, accelerating the learning process in the upper layers. A number of 4 runs were sent for evaluation, each run based on the same model, but having different set of hyper parameters.</p><p>The rest of this paper is organized as follows. Section 2 describes the proposed method based on the fine-tuning of the AlexNet model for plant identification. Section 3 describes the training process, experiments and the results. Conclusions and future challenges are presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method Description</head><p>In order to build our plant identification system we make use of transfer learning. For that we considered a pre-trained deep neural network model which is part of the Matlab Neural Network Toolbox <ref type="bibr" coords="2,259.08,627.47,15.46,8.96" target="#b16">[17]</ref>. We employed AlexNet. AlexNet can be split into two distinct parts, based on the role they accomplish in the neural network. The first part of AlexNet is responsible with feature learning, being comprised of five convolutional layers from which the first, the second and the fifth are followed by max-pooling layers; while the second part of AlexNet encloses three fully-connected layers with an output layer of 1000 neurons for classification, representing each class in the neural network (see Fig. <ref type="figure" coords="3,343.99,162.42,3.63,8.96" target="#fig_0">1</ref>).</p><p>For fine-tuning of AlexNet we used the PlantCLEF 2017 Encyclopedia of Life (EOL) training set, having a base learning rate of 0.001 and a batch size of 512 without weight decay. In order to prevent the overfitting, two strategies were used: firstly, we set a threshold so that the training stops if the mean accuracy of the previous 50 iterations is greater than 99%; and secondly we imposed a L2 regularization factor of 0.001. The training process was sped up in the last fully connected layer, by multiplying the base learning rate with a factor of 10.</p><p>Also, the last fully connected layer was modified to fit our needs: its output size was increased to 10,000, representing our classes. Therefore, the network will be able to distinguish between 10,000 different species of plants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and results</head><p>This section gives an overview on experiments we have conducted for both validation of the method and for final training itself. We firstly sought for the optimal hyper parameters in order to fine-tune the neural network model by conducting short experiments on a selected group of 1,000 categories of plants. Then, we took another validation step towards final training phase by using all the 10,000 categories of plants together with the optimal set of hyper parameters. Thereafter, we have fine-tuned the network in order to obtain our final runs. These aspects will be depicted in the following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary experiments on development data</head><p>We have considered the PlantCLEF 2017 Encyclopedia of Life (EOL) training dataset containing about 260,000 images from which some were in grayscale and others in CMYK. We have removed all the grayscale images since most of them were representing sketches of plants, low quality images, or even maps (see Fig. <ref type="figure" coords="3,411.55,683.54,3.63,8.96" target="#fig_1">2</ref>). In this way a total of 65 images from 45 categories were removed from the database. We also found 19 CMYK images which have been reduced to RGB, i.e., removing the 4th dimension, and further used in training. All the images were resized to 227 x 227 pixels due to the AlexNet input layer requirements.</p><p>In our experiments we have used the Matlab Neural Network Toolbox <ref type="bibr" coords="4,432.43,198.42,15.32,8.96" target="#b16">[17]</ref>. Our models were trained using Stochastic Gradient Descent with momentum 0.9 (SGDM). To find the optimal hyper parameters to use for fine-tuning the network, we have selected the first 1,000 categories, in ascending order, from the training dataset. The main reason for choosing only a small sub-set from the training dataset was that in this way we could get a quick estimate of the final results by varying several hyper parameters.</p><p>Experiments thus consisted in choosing the first 1,000 categories from the training set, representing 25,094 images from which 80% (per class) were used for training and the rest of 20% (per class) were used in evaluation.</p><p>We split our preliminary experiments in two phases. In the first phase we conducted a coarse exploration of data by varying several hyper parameters: weight learn rate along with the bias learn rate multiplying factors in the last fully connected layer, batch size and threshold used in the early stopping strategy. The purpose of this step was to find the best approach which could score the highest accuracy when evaluated on validation data. In the second phase we took the winner from the previous one and started to vary the L2 regularization factor to see what contribution could bring to the performance of the network. Throughout the experiments the base learning rate was constant.</p><p>Experiments have shown that weight and bias learn rates from the last fully connected layer as well as the strategies used to prevent the overfitting have a major impact on the performance of the network. Therefore the best approach having a multiplying factor of 10 for weight and bias learn rates and a batch size of 512 has reached the early stopping threshold of 98% in 2,066 iterations, i.e., about 53 epochs, scoring 45.6% accuracy on the validation data (see Table <ref type="table" coords="4,322.25,597.88,3.27,8.10" target="#tab_0">1</ref>).</p><p>Then, in the second phase the best approach having the same hyper parameters as the winner from the first one but with a L2 regularization factor of 0.001 has reached the early stopping threshold of 99% in 3,117 iterations, i.e., about 80 epochs, scoring 47% accuracy on the validation data (see Table <ref type="table" coords="4,316.37,645.23,3.63,8.96" target="#tab_1">2</ref>).</p><p>We have carried out yet another preliminary experiment before we trained the network for our submission: we validated the optimal hyper parameters on the entire training set from which 80% of images (per class) were used for training and the rest of 20% of images (per class) were used for validation. The main objective of this experiment was to give us an estimate about the network performance when trained on the entire dataset. We therefore trained the network with the optimal hyper parameters having a base learning rate of 0.001, weight and bias learning rate multiplying factor in the last fully connected layer of 10, batch size of 512 along with an early stopping threshold of 99% and L2 regularization factor of 0.001. The model has reached the threshold in 16,668 iterations, i.e., about 42 epochs, and scored 30.55% accuracy on the validation set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on the testing data</head><p>Following the previous experiments, we have used the above mentioned hyper parameters in order to train four models which have been submitted for evaluation. Each of these runs is detailed below as follows:</p><p>â”€ UPB HES-SO Run 1: This is our primary submission hence is the starting point for the other runs. In this run we fine-tuned the AlexNet neural networks model using the optimal hyper parameters from the validation step without weight decay:</p><p>â€¢ Base learning rate 0.001 â€¢ Weight learn rate factor 10 â€¢ Bias learn rate factor 10</p><formula xml:id="formula_0" coords="6,136.10,150.56,152.81,33.54">â€¢ Batch size 512 â€¢ Threshold 94% â€¢ L2 regularization factor 0.001</formula><p>This model has reached the early stopping threshold in 51,943 iterations, i.e., about 104 epochs, and scored 0.326 in terms of the Mean Reciprocal Rank (MRR) when evaluated on the test dataset. It took the 11 th place amongst the models which were trained only with trusted dataset <ref type="bibr" coords="6,255.53,231.06,15.43,8.96" target="#b19">[20]</ref>.</p><p>â”€ UPB HES-SO Run 2: This model was trained for about 18 epochs using Run 1 as a starting point and therefore having the same hyper parameters but learning rate factor in the last fully connected layer was set to 20. No updates on base learning rate were during training. It achieved a score of 0.305 in terms of MRR when evaluated on the test dataset and took the 12 th place amongst the models which were trained only with trusted dataset <ref type="bibr" coords="6,288.89,311.13,15.43,8.96" target="#b19">[20]</ref>.</p><p>â”€ UPB HES-SO Run 3: In this run we wanted to see to what extent the network performance can be improved by decreasing the learning rate factor in the last fully connected layer and further by adding a learning rate schedule. We used Run 1 as a starting point with the learning rate factor in the last fully connected layer set to 5.</p><p>We have trained the network for about 14 epochs without weight decay obtaining 96.88% accuracy on the training set. After that we dropped the learning rate by a factor of 0.5 each epoch until the 18th epoch having 99.22% accuracy on the training set. This model achieved a score of 0.361 in terms of MRR when evaluated on the test dataset, proving that adding a learning rate schedule within training stage could have a positive impact on the performance of the network. It took the 9 th place amongst the models which were trained only with trusted dataset <ref type="bibr" coords="6,421.99,455.13,15.34,8.96" target="#b19">[20]</ref>.</p><p>â”€ UPB HES-SO Run 4: The setup was similar to Run 3 but having the learning rate factor in the last fully connected layer set to 2. We have trained the network for about 13 epochs without weight decay obtaining 97.85% accuracy on the training set. After that we dropped the learning rate by a factor of 0.5 each epoch until the 18th epoch having 99.02% accuracy on the training set. However, when evaluated on the test dataset we achieved the same score of 0.361 as for Run 3 and this model took the 10 th place amongst the models which were trained only with trusted dataset <ref type="bibr" coords="6,157.58,563.15,15.43,8.96" target="#b19">[20]</ref>. In this case lowering the learning rate factor in the last fully connected layer did not bring any improvement on the performance but still this hyper parameter has a key role in transfer learning and its value cannot be neglected.</p><p>We have considered the PlantCLEF 2017 EOL training dataset, mainly because we wanted to validate our method aiming for the highest performance and we thought that this couldn't be achieved on the noisy dataset alone.</p><p>All the models were trained using a NVIDIA Quadro M4000 GPU. Training for the first run took about 7 days to complete, while for the other runs took about 27 hours, the latter being derived from the first run.</p><p>The results for these four runs are depicted within Table <ref type="table" coords="6,363.43,679.22,3.77,8.96" target="#tab_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and future work</head><p>In this paper, we presented the participation of UPB HES SO team to the 2017 PlantCLEF challenge. Choosing the optimal hyper parameters when fine-tuning a pretrained neural network model is a sensitive topic. Although transfer learning might be very convenient when comes to deep convolutional neural networks, it could lead to poor system performance if hyper parameters are wrongly selected. Another important aspect is the choice of the pre-trained model which should satisfy both the requirements of the task and the hardware limitations. We explored several hyper parameters in context of transfer learning to find to what extent each one would impact the performance of the network. Thereby we found that weight and bias learn rates from the last fully connected layer as well as the strategies used to prevent the overfitting influence the network's ability to adapt to new tasks. Besides early stopping threshold and L2 regularization factor, a learn rate schedule has to be considered in order to improve the network performance. Thus our system scored 0,326 MRR when trained without weight decay, respectively 0.361 MRR when the base learning rate was dropped to half every epoch.</p><p>As previously mentioned, the chosen pre-trained model is a particularly important aspect. Therefore we believe that our method could be improved if we choose for instance one of the VGG Neural Network models <ref type="bibr" coords="7,327.76,530.15,16.72,8.96" target="#b17">[18]</ref> and also if we take earlier into account a learn rate schedule.</p><p>We deeply understand the tremendous impact of plant taxonomy for environmental conservation. We intend to continue our research activity over the plant classification problem, complementing our approach with more advanced techniques such as data augmentation or even make use of Probabilistic Neural Networks (PNN). In this way, we aim to provide a sustainable approach when comes to ecological monitoring studies and environmental conservation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,124.70,446.42,345.87,8.10;3,124.70,457.46,212.90,8.10;3,137.15,315.40,320.87,122.05"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of the AlexNet model. The number inside curly braces represents the number of filters with dimensions mentioned above it [19].</figDesc><graphic coords="3,137.15,315.40,320.87,122.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,135.98,310.22,322.87,8.10;4,140.65,231.40,108.00,69.75"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of grayscale images (from left to right: sketch, low quality image, map).</figDesc><graphic coords="4,140.65,231.40,108.00,69.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,124.70,258.35,345.85,137.86"><head>Table 1 .</head><label>1</label><figDesc>Preliminary results in the first evaluation phase. BLR denotes the base learning rate, WLR and BiLR denotes the weight, respectively the bias learn rate factor in the last fully connected layer.</figDesc><table coords="5,149.64,297.44,307.77,98.78"><row><cell>No.</cell><cell>BLR</cell><cell cols="4">WLR Factor BiLR Factor Batch Size Threshold Accuracy</cell></row><row><cell>1</cell><cell>0.001</cell><cell>100</cell><cell>100</cell><cell>256</cell><cell>95% 35.59%</cell></row><row><cell>2</cell><cell>0.001</cell><cell>100</cell><cell>100</cell><cell>512</cell><cell>95% 39.96%</cell></row><row><cell>3</cell><cell>0.001</cell><cell>100</cell><cell>100</cell><cell>512</cell><cell>98% 41.38%</cell></row><row><cell>4</cell><cell>0.001</cell><cell>100</cell><cell>100</cell><cell>512</cell><cell>98% 40.56%</cell></row><row><cell>5</cell><cell>0.001</cell><cell>10</cell><cell>10</cell><cell>512</cell><cell>98% 45.67%</cell></row><row><cell>6</cell><cell>0.001</cell><cell>1</cell><cell>1</cell><cell>512</cell><cell>98% 45.18%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,130.22,423.98,334.42,93.02"><head>Table 2 .</head><label>2</label><figDesc>Preliminary results in the second evaluation phase (note that the 3rd experiment has not reached the threshold of 99%, being stopped at 100 epochs, reaching only 97.85%).</figDesc><table coords="5,176.39,460.62,254.46,56.38"><row><cell>No.</cell><cell>L2</cell><cell>Regularization</cell><cell>Number</cell><cell>of</cell><cell>Epochs</cell><cell>Accuracy</cell></row><row><cell>1</cell><cell></cell><cell>0.0005</cell><cell></cell><cell></cell><cell>80</cell><cell>45.28%</cell></row><row><cell>2</cell><cell></cell><cell>0.001</cell><cell></cell><cell></cell><cell>80</cell><cell>47.03%</cell></row><row><cell>3</cell><cell></cell><cell>0.005</cell><cell></cell><cell></cell><cell>100</cell><cell>44.46%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,124.70,150.35,344.15,160.70"><head>Table 3 .</head><label>3</label><figDesc>Results on the testing data. The official score is in terms of MRR and the official rank represents the place obtained amongst the models which were trained only with trusted dataset.</figDesc><table coords="7,124.70,178.96,280.94,132.10"><row><cell>Run</cell><cell>Official</cell><cell>Official</cell></row><row><cell></cell><cell>score</cell><cell>rank</cell></row><row><cell>UPB HES-SO Run1</cell><cell>0.326</cell><cell>11</cell></row><row><cell>UPB HES-SO Run2</cell><cell>0.305</cell><cell>12</cell></row><row><cell>UPB HES-SO Run3</cell><cell>0.361</cell><cell>9</cell></row><row><cell>UPB HES-SO Run4</cell><cell>0.361</cell><cell>10</cell></row><row><cell>4</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,132.67,177.35,337.88,8.10;8,141.74,188.39,329.12,8.10;8,141.74,199.31,46.51,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,311.19,177.35,159.35,8.10;8,141.74,188.39,80.03,8.10">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,241.50,188.39,187.03,8.10">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,210.35,337.92,8.10;8,141.74,221.39,183.10,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,277.94,210.35,192.65,8.10;8,141.74,221.39,50.38,8.10">Bluefield (KDE TUT) at LifeCLEF 2016 Plant Identification Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tatsuma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,210.65,221.39,84.16,8.10">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,232.31,337.92,8.10;8,141.74,243.35,277.08,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,290.56,232.31,180.02,8.10;8,141.74,243.35,144.33,8.10">Open-set Plant Identification Using an Ensemble of Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Berrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Erchan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,304.79,243.35,83.89,8.10">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,254.39,338.12,8.10;8,141.74,265.31,221.89,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,268.43,254.39,202.35,8.10;8,141.74,265.31,89.42,8.10">Very Deep Residual Networks with MaxOut for Plant Identification in the Wild</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,249.71,265.31,83.89,8.10">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,276.35,337.46,8.10;8,141.74,287.42,241.45,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,337.07,276.35,133.06,8.10;8,141.74,287.42,43.05,8.10">The CLEF 2011 plant images classification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,202.74,287.42,150.12,8.10">CLEF (Notebook Papers/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,298.34,337.79,8.10;8,141.74,309.38,256.10,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,337.05,298.34,133.41,8.10;8,141.74,309.38,37.65,8.10">The ImageCLEF 2012 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,197.23,309.38,166.23,8.10">CLEF (Online Working Notes/Labs/Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,320.42,337.51,8.10;8,141.74,331.34,145.20,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,318.94,320.42,151.24,8.10;8,141.74,331.34,13.32,8.10">The ImageCLEF 2013 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,173.02,331.34,83.89,8.10">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,342.38,337.59,8.10;8,141.74,353.42,92.08,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,284.10,342.38,144.70,8.10">LifeCLEF plant identification task 2014</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,448.35,342.38,21.91,8.10;8,141.74,353.42,59.67,8.10">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,364.34,337.92,8.10;8,141.74,375.38,66.30,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,258.21,364.34,124.26,8.10">LifeCLEF plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,419.53,364.34,51.05,8.10;8,141.74,375.38,36.27,8.10">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.40,386.42,338.20,8.10;8,141.74,397.34,136.41,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,264.29,386.42,202.42,8.10">Plant identification in an open-world (LifeCLEF 2016)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,154.08,397.34,93.88,8.10">CLEF working notes 2016</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.40,408.38,337.96,8.10;8,141.74,419.42,328.91,8.10;8,141.74,430.34,177.91,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,357.70,408.38,112.66,8.10;8,141.74,419.42,183.73,8.10">Leafsnap: A Computer Vision System for Automatic Plant Species Identification</title>
		<author>
			<persName coords=""><forename type="first">Neeraj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arijit</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,344.94,419.42,125.71,8.10;8,141.74,430.34,147.14,8.10">Proceedings of the 12th European Conference on Computer Vision (ECCV)</title>
		<meeting>the 12th European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.40,441.38,338.16,8.10;8,141.74,452.42,219.33,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,262.52,441.38,78.18,8.10">Pl@ntNet mobile app</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,360.77,441.38,109.79,8.10;8,141.74,452.42,140.83,8.10">Proceedings of the 21st ACM international conference on Multimedia</title>
		<meeting>the 21st ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="423" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.40,463.34,338.29,8.10;8,141.74,474.40,329.06,8.10;8,141.74,485.44,42.06,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,294.04,463.34,176.65,8.10;8,141.74,474.40,18.07,8.10">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,180.09,474.40,179.65,8.10">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.40,496.36,337.91,8.10;8,141.74,507.40,328.76,8.10;8,141.74,518.44,52.52,8.10" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,310.83,496.36,159.48,8.10;8,141.74,507.40,246.01,8.10">Deep Learning and SVM Classification for Plant Recognition in Content-Based Large Scale Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">P</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Osvath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Papp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Szucs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,409.51,507.40,60.99,8.10;8,141.74,518.44,22.49,8.10">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.40,529.36,337.82,8.10;8,141.74,540.40,328.64,8.10;8,141.74,551.44,114.15,8.10" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,338.04,529.36,132.18,8.10;8,141.74,540.40,310.80,8.10">Plant Identification System based on a Convolutional Neural Network for the LifeCLEF 2016 Plant Classification Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,141.74,551.44,83.89,8.10">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.40,562.36,338.24,8.10;8,141.74,573.40,200.48,8.10" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,255.35,562.36,215.30,8.10;8,141.74,573.40,69.08,8.10">Feature Learning via Mixtures of DCNNs for Fine-Grained Plant Classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,228.29,573.40,83.89,8.10">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.40,584.44,130.31,8.10;8,141.74,595.36,236.87,8.10" xml:id="b16">
	<monogr>
		<ptr target="https://www.mathworks.com/products/neural-network.html" />
		<title level="m" coord="8,141.74,584.44,116.87,8.10">Matlab Neural Network Toolbox</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.40,606.40,338.18,8.10;8,141.74,617.44,112.80,8.10" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,253.81,606.40,216.78,8.10;8,141.74,617.44,42.40,8.10">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,202.52,617.44,25.76,8.10">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.40,628.36,338.16,8.10;8,141.74,639.40,329.06,8.10;8,141.74,650.44,238.18,8.10" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,354.30,628.36,116.26,8.10;8,141.74,639.40,146.09,8.10">A Taxonomy of Deep Convolutional Neural Nets for Computer Vision</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Sarvadevabhatla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Mopuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,307.66,639.40,163.13,8.10;8,141.74,650.44,16.58,8.10;8,248.42,650.44,101.24,8.10">Vision Systems Theory, Tools and Applications</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Frontiers in Robotics and AI</note>
</biblStruct>

<biblStruct coords="8,132.40,661.39,338.16,8.10;8,141.74,672.43,315.02,8.10" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="8,260.45,661.39,210.11,8.10;8,141.74,672.43,170.08,8.10">Plant identification based on noisy web data: the amazing performance of deep learning (LifeCLEF 2017)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,332.59,672.43,93.93,8.10">CLEF working notes 2017</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.40,150.35,338.19,8.10;9,141.74,161.39,328.66,8.10;9,141.74,172.31,87.76,8.10" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,282.89,150.35,187.70,8.10;9,141.74,161.39,88.08,8.10">LifeCLEF 2017 Lab Overview: multimedia species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,237.06,161.39,89.42,8.10">CLEF 2017 Proceedings</title>
		<title level="s" coord="9,333.82,161.39,136.58,8.10;9,141.74,172.31,27.98,8.10">Springer Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
