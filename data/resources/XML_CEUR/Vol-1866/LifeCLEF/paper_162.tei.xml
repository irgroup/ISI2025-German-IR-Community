<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.71,115.96,313.95,12.62;1,167.32,133.89,280.72,12.62;1,235.00,151.82,145.35,12.62">LifeClef 2017 Plant Identification Challenge: Classifying Plants Using Generic-Organ Correlation Features</title>
				<funder ref="#_cNwD3hB">
					<orgName type="full">Postgraduate</orgName>
					<orgName type="abbreviated">PPP</orgName>
				</funder>
				<funder>
					<orgName type="full">NVIDIA Corporation</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Malaya</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.33,189.68,53.52,8.74"><forename type="first">Sue</forename><forename type="middle">Han</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centre of Image &amp; Signal Processing</orgName>
								<orgName type="department" key="dep2">Fac. Comp. Sci. &amp; Info. Tech</orgName>
								<orgName type="institution">University of Malaya</orgName>
								<address>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.62,189.68,81.59,8.74"><forename type="first">Yang</forename><forename type="middle">Loong</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centre of Image &amp; Signal Processing</orgName>
								<orgName type="department" key="dep2">Fac. Comp. Sci. &amp; Info. Tech</orgName>
								<orgName type="institution">University of Malaya</orgName>
								<address>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.08,189.68,71.95,8.74"><forename type="first">Chee</forename><forename type="middle">Seng</forename><surname>Chan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centre of Image &amp; Signal Processing</orgName>
								<orgName type="department" key="dep2">Fac. Comp. Sci. &amp; Info. Tech</orgName>
								<orgName type="institution">University of Malaya</orgName>
								<address>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.71,115.96,313.95,12.62;1,167.32,133.89,280.72,12.62;1,235.00,151.82,145.35,12.62">LifeClef 2017 Plant Identification Challenge: Classifying Plants Using Generic-Organ Correlation Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7DA87D4F5AE115E9D8568DDF1F8DE59F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Plant classification</term>
					<term>deep learning</term>
					<term>convolutional neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our proposal in the multi-organ plant identification task (LifeClef2017 challenge <ref type="bibr" coords="1,332.36,297.37,9.52,7.86" target="#b7">[8]</ref>). The objective of the challenge is to evaluate to what extent machine learning and computer vision can learn from noisy data compared to trusted data. To address the challenge, we employ our recent proposed hybrid generic-organ convolutional neural network, abbreviated  to train on different composition of plant datasets. Overall, all the submitted runs obtained comparable results in the LifeClef2017 plant classification task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Plant classification has received particular attention in the computer vision field <ref type="bibr" coords="1,134.77,464.65,15.50,8.74" target="#b9">[10]</ref> due to its important implications in agriculture automation and environmental conservation. Along with the recent advances in science and technology, automatic plant species recognition has been made possible to assist botanists in plant identification tasks <ref type="bibr" coords="1,257.34,500.51,14.61,8.74" target="#b9">[10]</ref>. For example: development of an efficient plant recognition system using the Local Binary Pattern <ref type="bibr" coords="1,359.65,512.47,15.50,8.74" target="#b12">[13]</ref> allows the classification of medical plants <ref type="bibr" coords="1,211.84,524.42,14.61,8.74" target="#b11">[12]</ref>. Robotic weed control system drives studies on automatic plant identification in agronomic research aimed at crop improvement by recognition of crop plants and elimination of weeds <ref type="bibr" coords="1,332.91,548.33,9.96,8.74" target="#b4">[5]</ref>. Despite these, automatic plant recognition, a foundational capability in this context, is nevertheless still in its early stages.</p><p>In 2013, LifeClef challenge <ref type="bibr" coords="1,270.56,584.39,10.52,8.74" target="#b3">[4]</ref> provided the first multi-organ plant dataset. This was the first multi-organ plant classification benchmark for the computer vision community. This year, LifeClef 2017 <ref type="bibr" coords="1,325.36,608.30,10.52,8.74" target="#b7">[8]</ref> offered a bigger amount of plant biodiversity data <ref type="bibr" coords="1,212.44,620.25,9.96,8.74" target="#b2">[3]</ref>. The objective is to identify 10000 species from images of plants collected based on two different channels: a "trusted" training set and a "noisy" training set. The trusted training set is collected from the online collaborative Encyclopedia Of Life (EoL) such as Wikipedia, iNaturalist and Flickr Fig. <ref type="figure" coords="2,228.21,334.86,3.87,8.74">1</ref>: Our proposed HGO-CNN architecture while the noisy training set is collected based on the Google and Bing image search results. In this challenge, we employ our recently proposed convolutional neural network (CNN) architecture -namely the HGO-CNN <ref type="bibr" coords="2,413.74,395.30,15.50,8.74" target="#b10">[11]</ref> with small refinements. Specifically, it integrates both the generic and organ-specific information for the multi-organ plant classification task.</p><p>The rest of the working note is organized as follows. In Section 2, we present the methodology of our proposed architecture. Section 3 illustrates its training scheme. Section 4 shows the experiments and results for both the validation and testing set. Lastly, Section 5 presents conclusion and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method Description</head><p>Unlike previous approaches <ref type="bibr" coords="2,260.84,524.46,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,273.02,524.46,7.75,8.74" target="#b1">2]</ref> that trained CNN to capture solely generic representation from the plantation images, HGO-CNN <ref type="bibr" coords="2,371.20,536.42,15.50,8.74" target="#b10">[11]</ref> is able to encapsulate both the organ and generic information prior to the plant classification. We consider features from the organ because plant organs in general, are known prior to the exploration of its characteristics. For instant, when botanists study a leaf, they focus on the leaf characters such as margin or venation, and, when they study a flower, they focus on the characteristics of petals, sepals and stamen to identify the plant species. So, we believe that a better recognition method for plant species requires prior information of their respective organs.</p><p>The proposed HGO-CNN comprises of four layers or components: (i) a shared layer, (ii) an organ layer, (iii) generic layer, and (iv) a species layer. We introduce shared layer for both the generic and organ components. The reasons are threefold. First, <ref type="bibr" coords="3,208.73,118.99,15.50,8.74" target="#b16">[17,</ref><ref type="bibr" coords="3,225.89,118.99,12.73,8.74" target="#b15">16]</ref> demonstrated that preceding layers in deep networks response to low-level features such as corners and edges. Since both the higher level generic and organ components require low-level features to build higher level features, we introduce shared preceding layers for these components. Second, according to <ref type="bibr" coords="3,217.17,166.81,14.61,8.74" target="#b15">[16]</ref>, the shared layer may reduce floating point operations and memory footprint of the network execution, which are of importance for real world application. Lastly, using shared layer will help to reduce the number of training parameters, which is beneficial to the architecture's computational efficiency. Fig. <ref type="figure" coords="3,200.46,214.64,4.98,8.74">1</ref> depicts the configuration of our proposed model. Input to our proposed model is a color image of 224 × 224 pixels. For the convolutional layer, we utilise 3 × 3 convolution filters with spatial resolution preserved using stride 1. Max pooling is performed using a 2 × 2 pixel window with stride 2. Three fully connected layers, which have 4096, 4096 and 10000 channels respectively, follow behind the stacks of convolutional layers. Finally, the HGO-CNN output is fed into a softmax layer to produce the softmax output. Note that for the q-th class, the softmax output is defined as</p><formula xml:id="formula_0" coords="3,330.07,296.18,58.81,17.82">P (q) n = e sq M m=1</formula><p>e sm where M stands for the total number of classes and s stands for the class prediction score. After performing the softmax operation, softmax loss L is computed as follows:</p><formula xml:id="formula_1" coords="3,258.81,347.13,217.54,30.20">L = 1 B B n=1 -log(P (T ) n ) (<label>1</label></formula><formula xml:id="formula_2" coords="3,476.35,357.55,4.24,8.74">)</formula><p>where B is the batch size and T is the ground truth class label for the n-th input image.</p><p>In this challenge, we refine some of the configurations in the original HGO-CNN architecture: (1) the data layer normalization technique, called Batch Normalization (BN) <ref type="bibr" coords="3,210.23,432.17,10.52,8.74" target="#b5">[6]</ref> is included. We added BN from the last convolution layer of both the generic and organ components respectively until the fully connected layers. This is to enhance the correlation of representation learning between the two components, so that it is more robust to non-linearities. (2) During feature fusion, features summation is performed instead of concatenation to further amplify the correspondences of these features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training</head><p>Pre-Training Two-Path CNN We design a two-path CNN as shown in Fig. <ref type="figure" coords="3,134.77,560.41,4.98,8.74">2</ref> for the purpose of training two different components: the generic and organ specific features. This two-path CNN is initially pre-trained using the ImageNet challenge dataset <ref type="bibr" coords="3,212.58,584.32,14.61,8.74" target="#b13">[14]</ref>.</p><p>Organ layer After we obtained the pre-trained two-path CNN, one of the CNN path is repurposed for the organ task. This organ layer is trained together with the shared layer, using seven kinds of predefined organ labels. We obtain organbased feature maps, x org ∈ R H×W ×Z where H, W and Z are the height, width and number of channels of the respective feature maps. Since PlantClef2017 Fig. <ref type="figure" coords="4,246.42,265.69,7.75,8.74">2:</ref> A two-path CNN architecture dataset does not provide organ information for every plant image, we train the organ layer based on the previous PlantClef2015 training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generic layer</head><p>After training the organ layer, another CNN path is repurposed for the generic task. This generic layer is trained using the species labels, regardless of organ information. We obtain generic-based feature maps, x gen ∈ R H×W ×Z . To allow both the organ and generic layers to share the common proceeding layer, we keep the shared layer's weights to be consistent. This is achieved by setting their learning rate to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Species layer</head><p>To introduce correlation between both the organ and generic components, a fusion function y = g(x org , x gen ) is employed at stage L (after the last convolutional layer for both components as shown in Fig. <ref type="figure" coords="4,422.00,446.38,4.43,8.74">1</ref>) to produce the organ and generic correlation feature maps, y ∈ R H×W ×Z . In our model, g performs summation of these two sets of features:</p><formula xml:id="formula_3" coords="4,248.53,493.11,232.06,11.74">y i,j,k = x org i,j,k + x gen i,j,k<label>(2)</label></formula><p>where 1</p><formula xml:id="formula_4" coords="4,176.00,514.14,183.11,8.74">≤ i ≤ H, 1 ≤ j ≤ W , 1 ≤ k ≤ Z.</formula><p>The feature maps, y will then go through two convolution layers to learn the combined representation of generic and organ features. Since these two convolution layers are new randomlyinitialised, we set their learning rate to be 10 times higher than the other layers during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>Our architecture is trained using the Caffe <ref type="bibr" coords="4,334.08,620.25,10.52,8.74" target="#b6">[7]</ref> framework. The networks are trained with back-propagation, using stochastic gradient descent <ref type="bibr" coords="4,429.44,632.21,9.96,8.74" target="#b8">[9]</ref>. For the training parameter setting, we employed the fixed learning policy. We set the learning rate to 0.01, and then decreased it by a factor of 10 when the validation set accuracy stops improving. The momentum was set to 0.9 and the weight decay to 0.0001. We run the experiments using a NVIDIA K40 graphics card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Preparation</head><p>For the trusted training set, we first downloaded all 256287 images. We then randomly selected 208878 images for training and 47409 images for validation.</p><p>To increase the robustness of the system in recognising multi-organ plant images, a multi-scale training was adopted. We isotropically rescaled the training images into three different sizes: 256, 385 and 512, then randomly cropped 224*224 pixels from the rescaled images to feed into the network for training. By doing this, the crop from the larger scaled images will correspond to a small part of the image or particularly subpart of the organ; while the crop from the smaller scaled images will correspond to the global structure of a plant. Besides that, we also increased the data size by mirroring the input image during training. After the data augmentation, we obtained 626634 training images and a validation set of 142227 images. However, for the noisy dataset, we only managed to crawl up to 918216 number of images which is about 60% of the total number of images from the web due to resource limitations. We then separated it into 738716 images for training and 179500 images for validation. We performed the same data augmentation to produce another training set that contains 2216148 images and a validation set of 538500 images. For the testing set, all 25170 images are downloaded and similarly augmented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental results on validation set</head><p>For the evaluation of our validation set, the softmax output from our CNN model for each image was first collected. An averaging fusion was then used to combine the softmax scores of the augmented validation set. In this experiment, we computed the top-1 classification result (Acc) to infer the robustness of the system. We compared our method to the generic network, VGG-16 net <ref type="bibr" coords="5,447.56,492.80,14.61,8.74" target="#b14">[15]</ref>. Table <ref type="table" coords="5,176.07,608.30,4.98,8.74" target="#tab_0">1</ref> shows the comparison of the performance results. We can observe that the VGG-16 net performed better in the noisy dataset while our proposed HGO-CNN performed better in the trusted dataset. There are two possible reasons:</p><p>(1) the organ layer in HGO-CNN that was trained on previous PlantClef2015 dataset might not be robust enough to model such a huge and diverse data, <ref type="bibr" coords="5,467.86,656.12,12.73,8.74" target="#b1">(2)</ref> noisy dataset in this case is better modeled using generic features regardless of the organ information as the generic features might include many independent plant features that help in the classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results on Test set</head><p>We submitted four runs to the LifeClef 2017 challenge. We finetuned all models using both the training and validation set to increase the robustness of the models. To obtain the observation level predictions, an averaging fusion method was employed to combine the results of the testing images that have the same observation id. Their performance were evaluated based on Mean Reciprocal Rank (MRR). The characteristics of each run are stated below:</p><p>-UM Run 1: Proposed HGO-CNN that trained with the trusted set only.</p><p>-UM Run 2: VGG-16 net that trained with the noisy set only.   <ref type="figure" coords="6,170.18,608.30,4.98,8.74" target="#fig_1">3</ref> shows the overall results of the LifeClef2017 multi-organ plant classification task. We observed that Run 2 which is ranked at 12th out of a total of 28 runs is the best among the submitted runs while Run 1 which is ranked at 19th shows the lowest result. Henceforth, we make a deduction that the fusion model HGO-CNN that we currently trained is not generalized enough to predict  unseen testing images. Run 3 (ranked at 13th) and Run 4 (ranked at 15th), both are the combined results of Run 1 and Run 2 respectively, are ranked lower compared to Run 2. This is clearly due to the poor performance of the Run 1 model. Furthermore, Run 3 is ranked higher than Run 4, an indication that the averaging fusion method performs better than the max voting method.</p><p>Next, we compared the results of our submitted runs based on different composition of the dataset. Table <ref type="table" coords="7,289.69,584.39,4.98,8.74" target="#tab_1">2</ref> shows the results of the trusted training set(EOL). We observe that our UM Run 1 provides a comparable result, where it is ranked at 5th out of a total of 11 runs. We believe that the performance could have been better if the organ layer in the HGO-CNN is trained with the latest Plantclef2017 dataset. However, it was restricted as the organ information is not provided for most of the images. In Table <ref type="table" coords="7,346.89,644.16,3.87,8.74" target="#tab_2">3</ref>, we have the lowest rank but we used only 60% of the noisy WEB dataset. Furthermore, there are only two participants in this category which is hardly a thorough comparison. Lastly, in Table <ref type="table" coords="8,162.84,130.95,3.87,8.74" target="#tab_3">4</ref>, we observed comparable results for our submitted runs. However, we believe that our performance can be improved. In the current experiments, we separately trained the CNN models using two different datasets and inferred the results using average fusion. It is possible that the performance could have been better if both of the datasets are trained in one single end-to-end CNN model without the prerequisite of external fusion to infer the species. Moreover, training on 100% of noisy images might be able to boost up the classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future work</head><p>This working note explains the implementation the HGO-CNN for the Plant-clef2017 challenge. We described the methodology of our proposed architecture and analyzed the results based on both validation and testing set. We observed that our current HGO-CNN model is not generalized enough to predict unseen testing images. This might due to the lack of robustness at the organ layer trained using the previous PlantClef2015 dataset. In the future, we will revise our proposed model to increase its robustness by re-training all layers using the latest datasets that incorporate both trusted and noisy images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,140.99,295.48,339.60,8.77;6,151.70,307.47,92.21,8.74;6,140.99,319.34,339.60,8.77;6,151.70,331.32,93.53,8.74"><head></head><label></label><figDesc>-UM Run 3: Combined results of UM Run 1 and UM Run 2 based on averaging fusion at image level. -UM Run 4: Combined results of UM Run 1 and UM Run 2 based on max voting at image level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,154.42,572.12,306.52,8.74;6,134.76,370.37,345.58,190.23"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Results of the LifeClef2017 multi-organ plant classification task</figDesc><graphic coords="6,134.76,370.37,345.58,190.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,134.77,115.74,345.78,207.60"><head></head><label></label><figDesc></figDesc><graphic coords="2,134.77,115.74,345.78,207.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,193.21,536.20,225.88,44.91"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison    </figDesc><table coords="5,193.21,548.54,225.88,32.57"><row><cell>Method</cell><cell cols="2">Trusted (Acc) Noisy (Acc)</cell></row><row><cell>Finetuned VGG-16 top layer</cell><cell>0.44</cell><cell>0.44</cell></row><row><cell>HGO-CNN</cell><cell>0.45</cell><cell>0.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,169.21,127.36,276.93,147.12"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison for trusted training set(EOL)</figDesc><table coords="7,240.60,139.70,131.08,134.79"><row><cell>Method</cell><cell>MRR</cell></row><row><cell>CMP Run 3</cell><cell>0.807</cell></row><row><cell>FHDO BCSG Run 1</cell><cell>0.792</cell></row><row><cell>KDETUT Run 1</cell><cell>0.772</cell></row><row><cell>CMP Run 4</cell><cell>0.733</cell></row><row><cell>UM Run 1</cell><cell>0.700</cell></row><row><cell>PlantNet Run 1</cell><cell>0.613</cell></row><row><cell cols="2">SabanciUGebzeTU Run 2 0.581</cell></row><row><cell>UPB HES SO Run 3</cell><cell>0.361</cell></row><row><cell>UPB HES SO Run 4</cell><cell>0.361</cell></row><row><cell>UPB HES SO Run 1</cell><cell>0.326</cell></row><row><cell>UPB HES SO Run 2</cell><cell>0.305</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,171.85,300.36,271.65,44.91"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison for noisy training set(WEB)</figDesc><table coords="7,258.43,312.70,95.43,32.57"><row><cell>Method</cell><cell>MRR</cell></row><row><cell cols="2">KDETUT Run 2 0.824</cell></row><row><cell cols="2">UM Run 2 0.799</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,157.60,371.15,300.15,113.05"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison for noisy training set(WEB+EOL)</figDesc><table coords="7,240.60,383.49,131.08,100.71"><row><cell>Method</cell><cell>MRR</cell></row><row><cell cols="2">MarioTsaBerlin Run 4 0.920</cell></row><row><cell>KDETUT Run 4</cell><cell>0.853</cell></row><row><cell>KDETUT Run 3</cell><cell>0.837</cell></row><row><cell>UM Run 3</cell><cell>0.798</cell></row><row><cell>UM Run 4</cell><cell>0.789</cell></row><row><cell cols="2">SabanciUGebzeTU Run 4 0.638</cell></row><row><cell cols="2">SabanciUGebzeTU Run 1 0.636</cell></row><row><cell cols="2">SabanciUGebzeTU Run 3 0.622</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This research is supported by the <rs type="funder">Postgraduate (PPP)</rs> Grant <rs type="grantNumber">PG007-2016A</rs>, from <rs type="funder">University of Malaya</rs>; and the used K40 GPU was donated by <rs type="funder">NVIDIA Corporation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_cNwD3hB">
					<idno type="grant-number">PG007-2016A</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,473.63,337.64,7.86;8,151.52,484.59,329.07,7.86;8,151.52,495.55,154.49,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,338.89,473.63,141.70,7.86;8,151.52,484.59,329.07,7.86;8,151.52,495.55,16.79,7.86">A comparative study of fine-grained classification methods in the context of the lifeclef plant identification challenge 2015</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,189.66,495.55,44.57,7.86">CLEF 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1391</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,505.88,337.64,7.86;8,151.52,516.84,329.07,7.86;8,151.52,527.80,25.60,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,189.11,505.88,291.48,7.86;8,151.52,516.84,151.31,7.86">Plant identification with deep convolutional neural network: Snumedinfo at lifeclef plant identification task 2015</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,322.36,516.84,158.23,7.86">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,538.13,337.64,7.86;8,151.52,549.09,329.07,7.86;8,151.52,560.05,25.60,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,282.89,538.13,197.70,7.86;8,151.52,549.09,205.65,7.86">Plant identification based on noisy web data: the amazing performance of deep learning (lifeclef 2017)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,377.22,549.09,82.46,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,570.38,337.64,7.86;8,151.52,581.34,271.28,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,173.03,581.34,175.40,7.86">The imageclef 2013 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,369.42,581.34,24.70,7.86">CLEF</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,591.67,337.64,7.86;8,151.52,602.63,329.07,7.86;8,151.52,613.59,314.96,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,357.94,591.67,122.65,7.86;8,151.52,602.63,189.66,7.86">Plant classification system for crop/weed discrimination without segmentation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Haug</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Michaels</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Biber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ostermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,361.74,602.63,118.85,7.86;8,151.52,613.59,51.91,7.86;8,233.48,613.59,100.78,7.86">Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1142" to="1149" />
		</imprint>
	</monogr>
	<note>IEEE Winter Conference</note>
</biblStruct>

<biblStruct coords="8,142.96,623.92,337.64,7.86;8,151.52,634.88,329.07,7.86;8,151.52,645.84,329.07,7.86;8,151.52,656.80,97.23,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,239.24,623.92,241.36,7.86;8,151.52,634.88,126.07,7.86">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,297.31,634.88,183.28,7.86;8,151.52,645.84,149.21,7.86">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<title level="s" coord="8,361.38,645.84,119.21,7.86;8,151.52,656.80,68.56,7.86">JMLR Workshop and Conference Proceedings</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,119.67,337.63,7.86;9,151.52,130.63,329.07,7.86;9,151.52,141.59,329.07,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,237.79,130.63,238.19,7.86">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,165.15,141.59,232.47,7.86">Proc. of the ACM International Conference on Multimedia</title>
		<meeting>of the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,152.55,337.63,7.86;9,151.52,163.51,329.07,7.86;9,151.52,174.47,329.07,7.86;9,151.52,185.43,192.22,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,348.80,163.51,131.79,7.86;9,151.52,174.47,151.82,7.86">Lifeclef 2017 lab overview: multimedia species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,324.15,174.47,95.45,7.86">CLEF 2017 Proceedings</title>
		<title level="s" coord="9,426.78,174.47,53.81,7.86;9,151.52,185.43,128.87,7.86">Springer Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,196.39,337.63,7.86;9,151.52,207.34,329.07,7.86;9,151.52,218.30,86.01,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,328.09,196.39,152.50,7.86;9,151.52,207.34,103.94,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,275.64,207.34,200.74,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,229.26,337.98,7.86;9,151.52,240.22,319.26,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,353.94,229.26,126.65,7.86;9,151.52,240.22,166.18,7.86">How deep learning extracts and learns leaf features for plant classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,324.34,240.22,81.42,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,251.18,337.98,7.86;9,151.52,262.14,269.87,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,302.56,251.18,178.04,7.86;9,151.52,262.14,200.91,7.86">Hgo-cnn: Hybrid generic-organ convolutional neural network for multi-organ plant classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,373.13,262.14,19.58,7.86">ICIP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,273.10,337.97,7.86;9,151.52,284.06,329.07,7.86;9,151.52,295.02,25.60,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,288.91,273.10,191.68,7.86;9,151.52,284.06,191.54,7.86">Classification of medicinal plants: an approach using modified lbp with symbolic representation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Naresh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nagendraswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,349.91,284.06,67.10,7.86">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="1789" to="1797" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,305.98,337.98,7.86;9,151.52,316.93,329.07,7.86;9,151.52,327.89,25.60,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,321.34,305.98,159.25,7.86;9,151.52,316.93,228.87,7.86">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,387.57,316.93,30.85,7.86">TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,338.85,337.97,7.86;9,151.52,349.81,329.07,7.86;9,151.52,360.77,329.07,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,345.21,349.81,135.39,7.86;9,151.52,360.77,61.52,7.86">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,219.68,360.77,166.76,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,371.73,337.97,7.86;9,151.52,382.69,193.35,7.86" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="9,278.92,371.73,201.67,7.86;9,151.52,382.69,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR, abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,393.65,337.97,7.86;9,151.52,404.61,329.07,7.86;9,151.52,415.56,329.07,7.86;9,151.52,426.52,70.14,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,465.49,393.65,15.10,7.86;9,151.52,404.61,329.07,7.86;9,151.52,415.56,22.38,7.86">Hdcnn: Hierarchical deep convolutional neural networks for large scale visual recognition</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,196.04,415.56,264.06,7.86">Proc. of the IEEE International Conference on Computer Vision</title>
		<meeting>of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2740" to="2748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,437.48,337.97,7.86;9,151.52,448.44,255.46,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,256.42,437.48,219.92,7.86">Visualizing and understanding convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,165.60,448.44,117.84,7.86">Computer vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
