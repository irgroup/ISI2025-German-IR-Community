<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.00,115.96,335.37,12.62;1,215.01,133.89,185.35,12.62">A Multi-modal Deep Neural Network approach to Bird-song identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,134.77,171.56,68.25,8.74"><forename type="first">Botond</forename><surname>Fazekas</surname></persName>
							<email>botond.fazekas@tuwien.ac.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Vienna University of Technology Institute of Software Technology and Interactive Systems</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,213.39,171.56,87.62,8.74"><forename type="first">Alexander</forename><surname>Schindler</surname></persName>
							<email>alexander.schindler@ait.ac.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Vienna University of Technology Institute of Software Technology and Interactive Systems</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Austrian Institute of Technology Center for Digital Safety and Security</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.73,171.56,57.87,8.74"><forename type="first">Thomas</forename><surname>Lidy</surname></persName>
							<email>lidy@ifs.tuwien.ac.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Vienna University of Technology Institute of Software Technology and Interactive Systems</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,405.20,171.56,70.92,8.74"><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
							<email>rauber@ifs.tuwien.ac.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Vienna University of Technology Institute of Software Technology and Interactive Systems</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.00,115.96,335.37,12.62;1,215.01,133.89,185.35,12.62">A Multi-modal Deep Neural Network approach to Bird-song identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A7A22952DD510AD82364F5075DEE199D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>bird song</term>
					<term>deep neural network</term>
					<term>exponential linear unit</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a multi-modal Deep Neural Network (DNN) approach for bird song identification. The presented approach takes both audio samples and metadata as input. The audio is fed into a Convolutional Neural Network (CNN) using four convolutional layers. The additionally provided metadata is processed using fully connected layers. The flattened convolutional layers and the fully connected layer of the metadata are joined and fed into a fully connected layer. The resulting architecture achieved 2., 3. and 4. rank in the BirdCLEF2017 task in various training configurations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Approach</head><p>We present our multi-modal Deep Neural Network (DNN) submission to the BirdCLEF 2017 task <ref type="bibr" coords="1,223.59,476.79,12.24,8.74" target="#b0">[1]</ref> which is part of the LifeCLEF <ref type="bibr" coords="1,374.18,476.79,10.52,8.74" target="#b5">[6]</ref> Multimedia Retrieval of biodiversity data evaluation campaign. The presented system is an adoption of the approach introduced in <ref type="bibr" coords="1,263.81,500.70,10.52,8.74" target="#b7">[8]</ref> which was extensively evaluated in <ref type="bibr" coords="1,427.86,500.70,9.96,8.74" target="#b6">[7]</ref>. The presented approach extends the originally audio-only based model to include further modalities as input. The original part is based on the provided samples of field recorded audio content. This information is converted to the frequency domain and sequentially processed before being fed into a custom Convolutional Neural Network (CNN) using four convolutional layers. The additionally provided metadata is processed using fully connected layers. The flattened convolutional layers and the fully connected layer of the metadata were joined and fed into a large fully-connected layer.</p><p>To achieve better convergence of the neural networks and to improve model accuracy several pre-processing steps are applied to the provided data. The field-recordings are split into bird-song and noise parts. For the training of the models a random audio segment is selected from the sound file. Various dataaugmentation steps detailed below are applied to the Mel-scaled spectrograms and fed into the network. From the provided metadata, longitude, latitude, elevation and the part of the day are used as additional information. Each feature is flagged with an extra bit in case of missing data.</p><p>For the final calculation of the results, sequential audio segments with 50% overlap are taken from the sound files and predictions are retrieved from the trained model. To asses the final classification, the average predictions for each segment of a sound file is calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preprocessing</head><p>This section describes data transformations, especially in the audio domain, including data manipulation methods to augment the provided training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sound preprocessing</head><p>For the sound preprocessing a similar method formulated in <ref type="bibr" coords="2,401.41,320.78,10.52,8.74" target="#b1">[2]</ref> is applied. The audio recordings are split in sound, noise and irrelevant segments. In order to do that we compute the spectrogram of the sound file using short-time Fourier transform (STFT) with a Hanning window function (size 512, 74% overlap). We normalize the resulting spectrogram to the interval [0,1]. The spectrogram is treated then as a grayscale image.</p><p>As in most of the recordings the foreground bird singing/calling has higher amplitude than the background noise, in order to distinguish the relevant sound from the background noise, each STFT frequency bin is set to 1 if it is above three times the median of the corresponding row and three times the median of its corresponding column, otherwise it is set to 0. However, as this step results in a noisy spectrogram, binary erosion and dilation filter is applied to it. We have used a 4 by 4 filter as suggested in <ref type="bibr" coords="2,296.38,464.84,9.96,8.74" target="#b1">[2]</ref>. A one dimensional indicator vector is created from this image in which the i-th column is set to 1 if the corresponding column in the spectrogram contains at least one 1, other it is set to 0. This vector is further binary dilated twice. The indicator vector is then scaled to the original length of the recording and it is used as a mask to extract the relevant sound part. For separating the noise the same method is applied with a threshold of 3 instead of 2.5 for the median clipping and the resulting image is then inverted. Columns containing pixels which don't have an amplitude larger than 3 times or smaller than 2.5 times the row and column median are considered as irrelevant as in this case they cannot be distinguished clearly from the sound or noise part. However, with very noisy recordings or in ones that contain only bird songs without any quiet parts this approach can result in very short or even empty segments, as none or very few pixels will be above the median threshold. To overcome the problem of short segments, a minimum segment length of 32.768 samples is selected, as this is the minimum sound chunk size in our network architecture. The noise/sound separation threshold is iteratively lowered by 0.1 until the length of the sound part is over this limit.</p><p>Since the Deep Learning network needs a fix sized input during the training, for composing the batches we randomly select 16 (our batch size) files from which we randomly select segments. If the files contain less than 32768 samples, instead of padding we loop the files. The selected segments are then converted to the time-domain using Short-term Fourier Transform (STFT). In a subsequent step a log-normalized Mel-scale Transform with with 80 Mel-bands is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Augmentation</head><p>Most of the data augmentation steps are similar to the ones used in <ref type="bibr" coords="3,425.10,238.59,9.96,8.74" target="#b1">[2]</ref>. However, we found that in addition to the data augmentation steps proposed small variations in the amplitude, overlaying other birds from neighboring areas can further improve the accuracy, leading to the data augmentation process described below.</p><p>Noise overlay During the training up to 4 random noise samples are taken from the noise files of the training set and each is added With 75% probability to the sound sample. This results in having some segments containing no noise overlay at all, while others having four different. Adding more noise to the sound samples results in a worse performance. We found that greatly dampening the volume of the noise (as described in <ref type="bibr" coords="3,414.07,373.11,10.52,8.74" target="#b1">[2]</ref> reduces the accuracy. Thus the overlay volume is only changed by ±10%.</p><p>Combining same class audio files With a probability of 70%, recordings of birds from the same class are overlayed with a random damping factor between 20% and 60%.</p><p>Combining birds from the neighboring area In addition to the noise, with a probability of 30%, a bird singing/calling of a different class that can be found in a distance of 1 • East/West/North/South is overlayed on the sample with 30% ± 5% damping.</p><p>Random cut After applying one of the above described overlays the spectrogram is randomly cut into two parts and the two parts concatenated again after switching the order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Volume shift</head><p>The volume of the input audio is randomly changed by ±5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pitch shift</head><p>The pitch of the input audio is randomly changed by ±5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Metadata preprocessing</head><p>To incorporate the available metadata in the model some preprocessing is required due to missing or inapplicable values. For the missing values we use other instances of the same species where these attributes are available. We calculate the mean and the variance of the respective attribute distribution and generate a normal distributed random value.</p><p>Apart from the date and the geo-coded coordinates, the time of the day is available. If this information is missing it is randomly generated as above. It has been shown that bird song intensity correlates with the melatonin levels in the birds and thus with the daylight <ref type="bibr" coords="4,315.59,154.86,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="4,327.77,154.86,7.01,8.74" target="#b3">4]</ref>. As the time of the sunrise and sunset varies during the year, the time of the day is not directly related to the amount of light. Thus, instead of directly using the time values we decided to divide the day into six different categories of sunlight exposure corresponding to different positions of the sun on the sky. The time of the sunrise and the sunset is dependent on the coordinates and on the day of the year (and partially on the elevation, however this is ignored in our implementation) and it is approximated with the algorithm formulated in <ref type="bibr" coords="4,282.05,238.55,9.96,8.74" target="#b4">[5]</ref>. We define the following parts of a day:</p><p>-Night 1 -From midnight until the sun is 9 • below the horizon (BTH) -Dawn -From 9 • BTH until 4 • above the horizon (ATH)</p><formula xml:id="formula_0" coords="4,134.77,282.63,179.46,67.93">-Forenoon -From 4 • ATH until noon -Afternoon -From noon until 4 • ATH -Dusk -From 4 • ATH to 9 • BTH -Night 2 -9 • BTH until midnight 9 •</formula><p>BTH is selected because it lies between the nautical twilight (i.e. the horizon is visible) and the civil twilight (i.e. terrestrial objects are visible to the human eye), 4 • ATH is selected arbitrarily as a point where the sun is already clearly above the horizon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Network architecture</head><p>The network has two types of input: one for the spectrogram and one for the metadata. The metadata input is a vector of 7 elements: The metadata input is fed into a fully connected layer of 100 neurons. The spectrogram input layer (80 × 512 units) is followed by four convolutional layers with Exponential Linear Unit (ELU) activation, each followed by a max-pooling layer. We found that ELUs yield the same results as using a rectifying activation function but without the need of batch normalization and reducing the training times by about 300%. A dropout of 0.2 is used on the input layer, after flattening the convolutional layers (0.4) and after the fully connected layer (0.4).</p><p>We use either an FFT window of 256 with an 32768 sample long sound segment or an FFT window of 512 with an 65536 sample long sound segment.</p><p>For both of them the Mel scale is calculated with 80 bands. Thus, the input layer is a matrix of 80 × 512. It is important to note that having 80 Mel bands with an FFT windows size of 256 means that some Mel bands are empty. However, these are filtered anyway in the first convolutional / max-pooling layer. Thus, instead of using differently configured input layers for FFT sizes 256 and 512 we are using a single input layer configuration covering all bands. For the training we use a batch size of 16, and a learning rate of 0.001, with Nesterov momentum of 0.9. For detailed scenario descriptions and a full report on the BirdCLEF evaluation campaign results, please refer to the BirdCLEF 2017 Web-page<ref type="foot" coords="5,436.25,570.75,3.97,6.12" target="#foot_0">3</ref> . We also tested a more complicated architecture with three inputs: an FFT 256 spectrogram, an FFT 512 spectrogram and the metadata which are then co-joined in a fully connected layer. This architecture yielded better results as the FFT 512only network, but worse than the FFT-256 network, however the training time is considerably longer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cynapse</head><p>The presented approach harnesses information deriving from multiple modalities. Cynapse Run 3 performed the best for the time-coded soundscapes, which may contain longer parts without any relevant sound. However for the traditional records with less noise it had the worst performance. On the other hand, Cynapse Run 4 that incorporated the results of Cynapse Run 3 and the Cynapse Run 2 results with higher frequency resolutions performed the best for traditional record, but only third best for the time-coded soundscapes. A possible explanation could be that the higher frequency resolution is a more distinguishing feature than the higher temporal resolution but only if there are long enough sound segments available. Future work would focus on regarding ornithological relationships instead of treating each species as an isolated class.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,272.04,345.83,257.77"><head>Table 1 .</head><label>1</label><figDesc>Run 1: In this run we use an 256 FFT window, and we train the network with 90% of the training set for 4 days. Cynapse Run 2: We use an 512 FFT window, and the training is kept running for 3 days with 90% of the training set. Then, for 1 day it is trained on the whole training set. Cynapse Run 3: The network from Cynapse Run 1 is kept training for an additional day with the whole training set. Cynapse Run 4: The predictions of Cynapse Run 2 and Cynapse Run 3 were taken and averaged for each class. Results of the Cynapse Runs (CR) on the four BirdCLEF evaluation tasks:</figDesc><table coords="5,134.77,434.53,345.83,95.29"><row><cell cols="5">MAP '16 Soundscapes with time-codes (MAP w TC), MAP Soundscapes without time-</cell></row><row><cell cols="5">codes (MAP wo TC), MAP Traditional records only main species (MAP o MS), MAP</cell></row><row><cell cols="4">Traditional records with background species (MAP w BS)</cell><cell></cell></row><row><cell>CR</cell><cell cols="3">MAP w TC MAP wo TC MAP o MS</cell><cell>MAP w BS</cell></row><row><cell>1</cell><cell>0,165</cell><cell>0,008</cell><cell>0,486</cell><cell>0,432</cell></row><row><cell>2</cell><cell>0,069</cell><cell>0,012</cell><cell>0,562</cell><cell>0,493</cell></row><row><cell>3</cell><cell>0,168</cell><cell>0,008</cell><cell>0,514</cell><cell>0,456</cell></row><row><cell>4</cell><cell>0,142</cell><cell>0,010</cell><cell>0,579</cell><cell>0,511</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="5,144.73,657.44,202.42,7.47"><p>http://www.imageclef.org/lifeclef/2017/bird</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,317.93,342.24,7.86;6,146.91,328.89,333.68,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,186.01,328.89,165.99,7.86">Alexis LifeCLEF Bird Identification Task</title>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Willem-Pier</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joly</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>CLEF working notes</note>
</biblStruct>

<biblStruct coords="6,138.35,339.85,342.24,7.86;6,146.91,350.81,333.68,7.86;6,146.91,361.77,59.97,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,465.49,339.85,15.10,7.86;6,146.91,350.81,280.19,7.86">Audio Based Bird Species Identification using Deep Learning Techniques</title>
		<author>
			<persName coords=""><forename type="first">Elias</forename><surname>Sprengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yannic</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,434.42,350.81,46.17,7.86;6,146.91,361.77,59.97,7.86">CLEF 2016 Working Notes</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,372.73,342.25,7.86;6,146.91,383.68,333.68,7.86;6,146.91,394.64,34.81,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,283.32,372.73,197.28,7.86;6,146.91,383.68,184.20,7.86">Neuroendocrine mechanisms regulating reproductive cycles and reproductive behavior in birds</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">F</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Balthazart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,349.86,383.68,126.48,7.86">Hormones, Brain, and Behavior</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">649798</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,405.60,342.24,7.86;6,146.91,416.56,333.68,7.86;6,146.91,427.52,333.68,7.86;6,146.91,438.48,150.07,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,354.00,405.60,126.59,7.86;6,146.91,416.56,190.46,7.86">Seasonal neuroplasticity in the songbird telencephalon: A role for melatonin</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">J</forename><surname>Vant Hof</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">F</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,361.13,416.56,119.46,7.86;6,146.91,427.52,333.68,7.86;6,146.91,438.48,83.51,7.86">Proceedings of the National Academy of Sciences of the United States of America. Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4674" to="4679" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,449.44,342.25,7.86;6,146.91,460.40,330.82,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,351.01,449.44,129.59,7.86;6,146.91,460.40,58.25,7.86">Low-precision formulae for planetary positions</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Van Flandern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">F</forename><surname>Pulkkinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,223.58,460.40,183.65,7.86">The Astrophysical Journal Supplement Series</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="391" to="411" />
			<date type="published" when="1979">1979</date>
			<publisher>APA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,471.36,342.24,7.86;6,146.91,482.31,333.68,7.86;6,146.91,493.27,333.68,7.86;6,146.91,504.23,95.78,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,177.31,482.31,254.18,7.86">LifeCLEF 2016: mMltimedia life species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,448.32,482.31,32.27,7.86;6,146.91,493.27,333.68,7.86">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="286" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,515.19,342.24,7.86;6,146.91,526.15,333.68,7.86;6,146.91,537.11,273.11,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,321.68,515.19,158.91,7.86;6,146.91,526.15,253.85,7.86">Comparing shallow versus deep neural network architectures for automatic music genre classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lidy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,420.99,526.15,59.60,7.86;6,146.91,537.11,182.23,7.86">Proceedings of the 9th Forum Media Technology (FMT2016)</title>
		<meeting>the 9th Forum Media Technology (FMT2016)<address><addrLine>St. Poelten, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,548.07,342.24,7.86;6,146.91,559.03,333.67,7.86;6,146.91,569.99,263.26,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,270.35,548.07,210.24,7.86;6,146.91,559.03,74.19,7.86">CQT-based convolutional neural networks for audio scene classification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lidy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,239.81,559.03,240.78,7.86;6,146.91,569.99,139.05,7.86">Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop</title>
		<meeting>the Detection and Classification of Acoustic Scenes and Events 2016 Workshop<address><addrLine>DCASE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="60" to="64" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
