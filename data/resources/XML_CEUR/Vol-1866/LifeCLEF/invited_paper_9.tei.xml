<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.77,115.96,345.83,12.62;1,172.90,133.89,269.54,12.62;1,248.29,151.82,118.78,12.62">Plant identification based on noisy web data: the amazing performance of deep learning (LifeCLEF 2017)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,199.25,189.49,56.55,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<email>herve.goeau@cirad.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.36,189.49,60.95,8.74"><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<email>pierre.bonnet@cirad.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.23,189.49,48.07,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inria ZENITH team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.77,115.96,345.83,12.62;1,172.90,133.89,269.54,12.62;1,248.29,151.82,118.78,12.62">Plant identification based on noisy web data: the amazing performance of deep learning (LifeCLEF 2017)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9471D1923BF5CFFE60C772BF59B51DD4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>plant</term>
					<term>leaves</term>
					<term>leaf</term>
					<term>flower</term>
					<term>fruit</term>
					<term>bark</term>
					<term>stem</term>
					<term>branch</term>
					<term>species</term>
					<term>retrieval</term>
					<term>images</term>
					<term>collection</term>
					<term>species identification</term>
					<term>citizen-science</term>
					<term>finegrained classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The 2017-th edition of the LifeCLEF plant identification challenge is an important milestone towards automated plant identification systems working at the scale of continental floras with 10.000 plant species living mainly in Europe and North America illustrated by a total of 1.1M images. Nowadays, such ambitious systems are enabled thanks to the conjunction of the dazzling recent progress in image classification with deep learning and several outstanding international initiatives, such as the Encyclopedia of Life (EOL), aggregating the visual knowledge on plant species coming from the main national botany institutes. However, despite all these efforts the majority of the plant species still remain without pictures or are poorly illustrated. Outside the institutional channels, a much larger number of plant pictures are available and spread on the web through botanist blogs, plant lovers web-pages, image hosting websites and on-line plant retailers. The LifeCLEF 2017 plant challenge presented in this paper aimed at evaluating to what extent a large noisy training dataset collected through the web and containing a lot of labelling errors can compete with a smaller but trusted training dataset checked by experts. To fairly compare both training strategies, the test dataset was created from a third data source, i.e. the Pl@ntNet mobile application that collects millions of plant image queries all over the world. This paper presents more precisely the resources and assessments of the challenge, summarizes the approaches and systems employed by the participating research groups, and provides an analysis of the main outcomes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Thanks to the long term efforts made by the biodiversity informatics community, it is now possible to aggregate validated data about tens of thousands species world wide. The international initiative Encyclopedia of Life (EoL), in particular, is one of the largest repository of plant pictures. However, despite these efforts, the majority of plant species living on earth are still very poorly illustrated with typically only few pictures of a single specimen or herbarium scans. More pictures are available on the Web through botanist blogs, plant lovers web-pages, image hosting websites and on-line plant retailers. But this data is much harder to be structured and contains a high degree of noise. The LifeCLEF 2017 plant identification challenge proposes to study to what extent such noisy web data is competitive with a relatively smaller but trusted training set checked by experts. As a motivation, a previous study conducted by <ref type="bibr" coords="2,341.48,214.64,41.39,8.74">Krause et</ref> al. <ref type="bibr" coords="2,398.52,214.64,15.50,8.74" target="#b9">[10]</ref> concluded that training deep neural networks on noisy data was unreasonably effective for finegrained recognition. The PlantCLEF challenge completes their work in several points:</p><p>1. it extends their result to the plant domain whose specificity is that the available data on the web is scarcer, the risk of confusion higher and, finally, the degree of noise higher. 2. it scales the comparison between trusted and noisy training data to 10K of species whereas the trusted training sets used in their study were actually limited to few hundreds of species. 3. it uses a third-party test dataset that is not a subset of either the noisy dataset or the trusted dataset. This allows a more fair comparison. More precisely, the test data is composed of images submitted by the crowd of users of the mobile application Pl@ntNet <ref type="bibr" coords="2,306.23,377.71,13.87,8.74" target="#b7">[8]</ref>. Consequently, it exhibits different properties in terms of species distribution, pictures quality, etc.</p><p>In the following subsections, we synthesize the resources and assessments of the challenge, summarize the approaches and systems employed by the participating research groups, and provide an analysis of the main outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>To evaluate the above mentioned scenario at a large scale and in realistic conditions, we built and shared three datasets coming from different sources. As training data, in addition to the data of the previous PlantCLEF challenge <ref type="bibr" coords="2,467.31,512.66,9.96,8.74" target="#b2">[3]</ref>, we provided two new large data sets both based on the same list of 10,000 plant species (living mainly in Europe and North America):</p><p>Trusted Training Set EoL10K : a trusted training set based on the online collaborative Encyclopedia Of Life (EoL). The 10K species were selected as the most populated species in EoL data after a curation pipeline (taxonomic alignment, duplicates removal, herbaria sheets removal, etc.). The training set contains 256,287 pictures in total but has a strong class imbalance with a minimum of 1 picture for Achillea filipendulina and a maximum of 1245 pictures for Taraxacum laeticolor.</p><p>Noisy Training Set Web10K : a noisy training set built through Web crawlers (Google and Bing image search engines) and containing 1.1M images. This training set is also imbalanced with a minimum of 4 pictures for Plectranthus sanguineus and a maximum of 1732 pictures for Fagus grandifolia. The main objective of providing both datasets was to evaluate to what extent machine learning techniques can learn from noisy data compared to trusted data (as usually done in supervised classification). Pictures of EoL are themselves coming from different sources, including institutional databases as well as public data sources such as Wikimedia, iNaturalist, Flickr or various websites dedicated to botany. This aggregated data is continuously revised and rated by the EoL community so that the quality of the species labels is globally very good. On the other side, the noisy web dataset contains more images but with several types and levels of noise: some images are labeled with the wrong species name (but sometimes with the correct genus or family), some are portraits of a botanist specialist of the targeted species, some are labeled with the correct species name but are drawings or herbarium sheets, etc.</p><p>Pl@ntNet test set: the test data to be analyzed within the challenge is a large sample of the query images submitted by the users of the mobile application Pl@ntNet (iPhone<ref type="foot" coords="3,235.54,332.61,3.97,6.12" target="#foot_0">4</ref> &amp; Androd<ref type="foot" coords="3,288.50,332.61,3.97,6.12" target="#foot_1">5</ref> ). It contains a large number of wild plant species mostly coming from the Western Europe Flora and the North American Flora, but also plant species used all around the world as cultivated or ornamental plants including some endangered species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>Based on the previously described testbed, we conducted a system-oriented evaluation involving different research groups who downloaded the data and ran their system.</p><p>Each participating group was allowed to submit up to 4 run files built from different methods (a run file is a formatted text file containing the species predictions for all test items). Semi-supervised, interactive or crowdsourced approaches were allowed but had to be clearly signaled within the submission system. None of the participants employed such methods.</p><p>The main evaluation metric is the Mean Reciprocal Rank (MRR), a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer. The MRR is the average of the reciprocal ranks for the whole test set:</p><formula xml:id="formula_0" coords="4,257.81,139.71,98.05,30.79">M RR = 1 |Q| Q i=1 1 rank i</formula><p>where |Q| is the total number of query occurrences in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participants and methods</head><p>80 research groups registered to the LifeCLEF plant challenge 2017. Among this large raw audience, 8 research groups finally succeeded in submitting run files. Details of the used methods and evaluated systems are synthesized below and further developed in the working notes of the participants (CMP <ref type="bibr" coords="4,428.88,273.55,14.61,8.74" target="#b15">[16]</ref>, FHDO BCSG <ref type="bibr" coords="4,166.79,285.51,14.61,8.74" target="#b13">[14]</ref>, KDE TUT <ref type="bibr" coords="4,242.35,285.51,9.96,8.74" target="#b3">[4]</ref>, Mario MNB <ref type="bibr" coords="4,318.35,285.51,14.60,8.74" target="#b11">[12]</ref>, Sabanci Gebze <ref type="bibr" coords="4,401.41,285.51,13.31,8.74" target="#b1">[2]</ref>, UM <ref type="bibr" coords="4,444.62,285.51,15.50,8.74" target="#b12">[13]</ref> and UPB HES SO <ref type="bibr" coords="4,202.01,297.46,14.76,8.74" target="#b18">[19]</ref>). Table <ref type="table" coords="4,256.79,297.46,4.98,8.74" target="#tab_0">1</ref> reports the results achieved by each run as well as a brief synthesis of the methods used in each of them. Complementary, the following paragraphs give a few more details about the methods and the overall strategy employed by each participant. CMP, Czech Republic, 4 runs, <ref type="bibr" coords="4,292.69,357.21,16.47,8.77" target="#b15">[16]</ref>: this participant based his work on the Inception-ResNet-v2 architecture <ref type="bibr" coords="4,282.58,369.19,15.50,8.74" target="#b16">[17]</ref> which introduces inception modules with residual connections. An additional maxout fully-connected layer with batch normalization was added on top of the network, before the classification fullyconnected layer. Hard bootstrapping was used for training with noisy labels. A total of 17 models were trained using different training strategies such as: with or without maxout, with or without pre-training on ImageNet, with or without bootstrapping, with and without filtering of the noisy web dataset. CMP Run 1 is the combination of all the 17 networks by averaging their results. CMP Run 3 is the combination of the 8 networks that were trained on the trusted EOL data solely. CMP Run2 and CMP Run 4 are post-processings of CMP Run1 and CMP Run 3 aimed at compensating the asymmetry of class distributions between the test set and the training sets. FHDO BCSG, Germany, 4 runs, <ref type="bibr" coords="4,300.42,524.58,16.47,8.77" target="#b13">[14]</ref>: this participant also used Inception-ResNet-v2 architecture. The Run 1 is based exclusively on the trusted EOL dataset following a two phases fine-tuning approach: in a first phase only the last output layer is trained for a few epochs with a small learning rate starting from randomly initialized weights, and in a second phase the entire network is trained with numerous epochs and a larger learning rate. For the test set, they used an oversampling technique for increasing the number of test samples with 10 crops (1 center, 4 corners and the mirrored crops). For the Run 2, they kept the same architecture but extended the trusted dataset with a filtered subset of the noisy dataset: images from the web were added if their species label were in the top-5 predictions from the model used in Run 1. Run 3 is the combination of Run 1&amp;2. PlantNet, France, 1 run: The PlantNet team provided a baseline for the task with the system used in Pl@ntNet app, based on a slightly modified version of inception v1 model <ref type="bibr" coords="6,230.10,382.01,15.50,8.74" target="#b17">[18]</ref> as described in <ref type="bibr" coords="6,314.51,382.01,9.96,8.74" target="#b0">[1]</ref>. The system also includes a number of thresholding and rejection mechanisms that are useful within the mobile app but that also degrade the raw classification performance. This team submitted only one run trained on the trusted EOL dataset.</p><p>Sabanci-Gebze, Turkey, 4 runs, <ref type="bibr" coords="6,297.40,441.75,11.14,8.77" target="#b1">[2]</ref>: inspired by the good results achieved last year with a combination of a GoogleLeNet <ref type="bibr" coords="6,349.91,453.74,15.50,8.74" target="#b17">[18]</ref> and a VGGNet <ref type="bibr" coords="6,441.98,453.74,14.61,8.74" target="#b14">[15]</ref>, this team ran an ensemble classifier of 9 VGGNets. Each network was trained with data augmentation techniques using random crops and random horizontal, focusing only the two last layers for fine-tuning due to technical limitations. The submitted Run 2 used models learned only on the EOL trusted dataset. For the remaining runs, the models were trained for supplementary epochs introducing complementary training images selected from the noisy dataset (about 60.000 images which were matching the ground truth according to the models trained for the Run 2). Run 1, 3, 4 used respectively a Borda count, a maximum confident rule and a weighted combination of the output of the classifiers.</p><p>Mario TSA Berlin, Germany, 4 runs, <ref type="bibr" coords="6,321.67,585.22,16.47,8.77" target="#b11">[12]</ref>: In Run3, two additional GoogLeNet's and one ResNeXT were trained using a filtered version of the web dataset and images of the test set that received a probability higher than 0.98 in Run1. The last and "winning" run MarioTsaBerlin Run 4 finally combined all the 12 trained models.</p><formula xml:id="formula_1" coords="6,345.23,585.25,16.11,8.74">this</formula><p>UM, Malaysia &amp; UK, 4 runs, <ref type="bibr" coords="7,292.22,250.47,16.47,8.77" target="#b12">[13]</ref>: this participant proposed an original architecture called Hybrid Generic-Organ (HBO-CNN) that was trained on the trusted dataset (UM Run 1). Unfortunately, it performed worst than a standard VGGNet model learned on the noisy dataset (UM Run 2). This can be partially explained by the fact that the HBO-CNN model need tagged images (flower, fruit, leaf,...), a missing information for the noisy dataset and partially available for the trusted dataset.</p><p>UPB HES SO, Switzerland, 4 runs, <ref type="bibr" coords="7,324.14,346.11,16.47,8.77" target="#b18">[19]</ref>: this team trained the historical AlexNet model <ref type="bibr" coords="7,221.65,358.10,15.50,8.74" target="#b10">[11]</ref> by using exclusively the trusted training dataset, and focused the experiments on the solver part. Run 1 didn't used weight decay for the regularization. Run 2 applied an important learning rate factor on the last layer without updating this value during the training. Run 3 and 4 used a usual learning rate schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We report in Figure <ref type="figure" coords="7,224.72,464.84,4.98,8.74" target="#fig_1">1</ref> the performance achieved by the 29 collected runs. Table <ref type="table" coords="7,134.77,476.79,4.98,8.74" target="#tab_0">1</ref> provides the results achieved by each run as well as a brief synthesis of the methods used in each of them.</p><p>Trusted or noisy ? As a first noticeable remark, the measured performances are very high despite the difficulty of the task with a median Mean Reciprocal Rank (MRR) around 0.8, and a highest MRR of 0.92 for the best system Mario MNB Run 4. A second important remark is that the best results were obtained mostly by systems that were learned on both the trusted and the noisy datasets. Only two runs (KDE TUT Run 2 and UM Run 2) used exclusively the noisy dataset but gave better results than most of the methods using only the trusted dataset. Several teams also tried to filter the noisy dataset, based on the prediction of a preliminary system trained only on the trusted dataset (i.e. by rejecting pictures whose label is contradictory with the prediction). However, this strategy did not improve the final predictor and even degraded the results. For instance Mario MNB Run 2 (using the raw Web dataset) performed better than Mario MNB Run 3 (using the filtered Web dataset). Succeeding strategies with CNN models: Regarding the used methods, all submitted runs were based on Convolutional Neural Networks (CNN) confirming definitively the supremacy of this kind of approach over previous methods.</p><p>A wide variety of popular architectures were trained from scratch or fine-tuned from pre-trained weights on the ImageNet dataset: GoogLeNet <ref type="bibr" coords="8,408.05,452.88,19.99,8.74" target="#b17">[18]</ref> and its improved inception v2 <ref type="bibr" coords="8,219.82,464.84,12.45,8.74" target="#b6">[7]</ref> and v4 <ref type="bibr" coords="8,270.30,464.84,15.50,8.74" target="#b16">[17]</ref> versions, inception-resnet-v2 <ref type="bibr" coords="8,410.87,464.84,16.85,8.74" target="#b16">[17]</ref>, ResNet-50 and ResNet-152 <ref type="bibr" coords="8,208.58,476.79,9.96,8.74" target="#b4">[5]</ref>, ResNeXT <ref type="bibr" coords="8,261.24,476.79,15.15,8.74" target="#b4">[5]</ref>, VGGNet <ref type="bibr" coords="8,317.91,476.79,21.64,8.74" target="#b14">[15]</ref> and even the AlexNet <ref type="bibr" coords="8,436.38,476.79,17.99,8.74" target="#b10">[11]</ref>. One can notice that inception v3 was not experimented despite the fact that it is a Besides the use of ensemble of classifiers, some teams also tried to propose modifications of existing models. KDE TUT, in particular, modified the architecture of the first convolutional layers of ResNet-50 and report consistent improvements in their validation experiments <ref type="bibr" coords="9,260.08,226.59,9.96,8.74" target="#b3">[4]</ref>. CMP also reported slight improvements on the inception-resnet-v2 by using a maxout activation function instead of RELU. The UM team proposed an original architecture called Hybrid Generic-Organ learned on the trusted dataset (UM Run 1). Unfortunately, it performed worst than a standard VGGNet model learned on the noisy dataset (UM Run 2). This can be partially explained by the fact that the HBO-CNN model need tagged images (flower, fruit, leaf,...), a missing information for the noisy dataset and partially available for the trusted dataset.</p><p>The race for the most recent model?: one can suppose that the most recent models such as inception-resnet-v2 or inception-v4 should lead to better results than older ones such as AlexNet, VGGNet and GoogleNet. For instance, the runs with GoogleNet and VGGNet by Sabanci <ref type="bibr" coords="9,342.37,370.05,9.96,8.74" target="#b1">[2]</ref>, or with a PReLU version of inception-v1 by the PlantNet team, or with the historical AlexNet architecture by the UPB HES SO team <ref type="bibr" coords="9,249.56,393.96,15.50,8.74" target="#b18">[19]</ref> performed the worst results. However, one can notice that the "winning" team used also numerous GoogLeNet models, while the old VGGNet used in UM run 2 gave quite high and intermediate results around a MRR of 0.8. This highlights how much the training strategies are important and how ensemble classifiers, bagging and data augmentation can greatly improve the performance even without the most recent architectures from the state of the art.</p><p>The race for GPUs: Like discussed above, best performances were obtained with ensembles of very deep networks trained over millions of images and heavy data augmentation techniques. In the case of the best run Mario MNB Run 4, test images were also augmented so that the prediction of a single image finally relies on the combination of 60 probability distributions (5 patches x 12 models).</p><p>Overall, the best performing system requires a huge GPU consumption so that their use in data intensive contexts is limited by cost issues (e.g. the Pl@ntNet mobile application accounts for millions of users). A promising solution towards this issue could be to rely on knowledge distilling <ref type="bibr" coords="9,353.82,573.29,9.96,8.74" target="#b5">[6]</ref>. Knowledge distilling consists in transferring the generalization ability of a cumbersome model to a small model by using the class probabilities produced by the cumbersome model as soft targets for training the small model. Alternatively, more efficient architectures and learning procedures should be devised. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Complementary Analysis</head><p>Performances by organs: the main idea here is to evaluate which kind of organ and association of organs provide the best performances. The figure <ref type="figure" coords="10,451.32,369.19,4.98,8.74" target="#fig_3">2</ref> gives the min, max and average MRR scores for all runs detailed by the 10 most representative organ combinations (with at least 100 observations in the test dataset). Surprisingly, the graph reveals that the majority of organ combinations share more or less the same MRR scores around 0.7 on average, highlighting how much the systems based on CNNs tend to be robust to any combination of pictures. However, as we yet noticed the previous years in LifeCLEF, the majority o f the systems performed clearly better when a test observation contains one or several picture of flowers exclusively. Using at least a picture of flower in a test observation with other types of organs guaranties in a sense good identification performances if we look at the three next organ combinations (flower-fruit-leaf, flower-leaf and flower-leaf-stem). On the opposite side, the systems have more difficulties when a test observation contains pictures of leaves without any flowers. It is getting worse when an observation combines only pictures of leaves and stems. This could be explained by the fact that stems are visually very different from leaves and both these two kinds of pictures produce dissimilar and non complementary sets of species on the outputs of the CNNs. We can notice as a complementary remark, that generally combining several pictures from different types of organs causes wider ranges of min and max scores, highlighting how much can be sensitive the combination of organs with an inappropriate rule.</p><p>Biodiversity-friendly evaluation metric: Like in <ref type="bibr" coords="10,366.01,620.25,10.52,8.74" target="#b8">[9]</ref> the main idea here is to evaluate how automated identification system deals with the long tail problem, i.e how much an automated system performs along the long-tailed distribution, where basically very few species are well populated in terms of observations while the vast majority of species contain few images. We therefore split the species into 3 categories according to the number of observations populating these species in the datasets. The figure <ref type="figure" coords="11,254.95,142.90,4.98,8.74" target="#fig_4">3</ref> gives the detailed MRR scores of three categories of species: with a low, intermediate and high number of (trusted and noisy) training images, respectively between 4 and 161 images, between 162 and 195 images, and between 196 and 1583 images (the three categories are balanced in terms of total number of training images). First we can notice that, as we can expect, for the majority of the systems, the performances are clearly lower on the "intermediate" species than the "high" species, and even more on the "low" species category. For instance, the FHDO BCSG Run3 is very affected by the long tail distribution problem with a difference of MRR scores about 0.5 between the "high" and the "low" categories. However, on the opposite side, some runs like Mario TSA Berlin runs 2&amp;4, KDE TUT runs 2&amp;3, or to a lesser extent UM run 2, are definitely "biodiversity-friendly" since they are quite few affected by the long tail distribution and are able to maintain more or less equivalent MRR scores for the three species categories. We can specifically highlight the Run 2 from KDE TUT which, while it is using "only" three ResNet-50 models learned from scratch, is able to guaranty a MRR score around 0.79±0.4 almost independently from the number of training images by species. Moreover, we can notice that all these remarkable runs produced by KDE TUT, Mario TSA Berlin and UM teams, share the fact they used all the entire noisy datasets without any filtering process. All the attempts of filtering the noisy dataset seem to degrade the performances on the "intermediate" and "low" categories, like for instance for the Mario TSA Berlin Run 2 and 3 (resp. without an with filtering).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper presented the overview and the results of the LifeCLEF 2017 plant identification challenge following the six previous ones conducted within CLEF evaluation forum. This year the task was performed on the biggest plant images dataset ever published in the literature. This dataset was composed of two distinct sources: a trusted set built from the online Encyclopedia of Life and a noisy dataset illustrating the same 10K species with more than 1M images crawled from the web without any filtering. The main conclusion of our evaluation was that convolutional neural networks (CNN) appear to be amazingly effective in the presence of noise in the training set. All networks trained solely on the noisy dataset did outperform the same models trained on the trusted data. Even at a constant number of training iterations (i.e. at a constant number of images passed to the network), it was more profitable to use the noisy training data. This means that diversity in the training data is a key factor to improve the generalization ability of deep learning. The noise itself seems to act as a regularization of the model. Beyond technical aspects, this conclusion is of high importance in botany and biodiversity informatics in general. Data quality and data validation issues are of crucial importance in these fields and our conclusion is somehow disruptive. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,364.31,585.25,116.28,8.74;6,134.77,597.20,352.49,8.74;6,134.77,609.16,345.83,8.74;6,134.77,621.11,345.83,8.74;6,134.77,633.07,345.82,8.74;6,134.77,645.02,345.82,8.74;7,134.77,118.99,345.82,8.74;7,134.77,130.95,345.82,8.74;7,134.77,142.90,345.83,8.74;7,134.77,154.86,345.83,8.74;7,134.77,166.81,345.83,8.74;7,134.77,178.77,345.83,8.74"><head></head><label></label><figDesc>participant used ensembles of fine-tuned CNNs pre-trained on ImageNet based on 3 architectures (GoogLeNet, ResNet-152 and ResNeXT) each trained with bagging techniques. Data augmentation techniques multiplied by 5 the number of training images with random cropping, horizontal flipping, variations of saturation, lightness and rotation, these three last transformations following a decreasing degree correlated the diminution of the learning rate during training to let the CNNs see patches closer to the original image at the end of each training process. Test images were also augmented and the resulting predictions averaged. MarioTsaBerlin Run 1 results from the combination of the 3 architectures trained on the trusted datasets only (EOL and PlantCLEF2016). Run 2 exploited both the trusted and the noisy dataset to train four GoogLeNet's, one ResNet-152 and one ResNeXT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,134.77,345.05,345.83,7.89;8,134.77,356.03,61.43,7.86;8,141.88,115.83,328.54,214.44"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Scores achieved by all systems evaluated within the plant identification task of LifeCLEF 2017</figDesc><graphic coords="8,141.88,115.83,328.54,214.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,134.77,500.70,345.83,8.74;8,134.77,512.66,345.82,8.74;8,134.77,524.61,345.83,8.74;8,134.77,536.57,345.83,8.74;8,134.77,548.52,345.82,8.74;8,134.77,560.48,345.83,8.74;8,134.77,572.43,345.82,8.74;8,134.77,584.39,345.82,8.74;8,134.77,596.34,345.82,8.74;8,134.77,608.30,345.83,8.74;8,134.77,620.25,345.82,8.74;8,134.77,632.21,345.83,8.74;8,134.77,644.16,345.83,8.74;8,134.77,656.12,345.83,8.74;9,134.77,118.99,345.82,8.74;9,134.77,130.95,345.82,8.74;9,134.77,142.90,345.82,8.74;9,134.77,154.86,345.83,8.74;9,134.77,166.81,345.82,8.74;9,134.77,178.77,309.44,8.74"><head></head><label></label><figDesc>recent model giving state of art performances in other image classification benchmarks. It is important to notice that the best results of each team were obtained with classifier ensembles (in particular Mario TSA Run 4, KDE TUT Run 4 and CMP Run 1). Bootstrap aggregating (bagging) was very efficient in this context to extend the number of classifiers by learning several models with the same architecture but on different training and validation subsets. This is the case of the best run Mario TSA Run 4 that combined 7 GoogLeNet, 2 ResNet-152, 3 ResNeXT trained on different datasets. The CMP team also combined numerous models (up to 17 in Run 1) with various subsets of the training data and bagging strategies, but all with the same inception-resnet-v2 architecture. Another key for succeeding the task was the use of data augmentation with usual transformations such as random cropping, horizontal flipping, rotation, for increasing artificially the number of training samples and helping the CNNs to generalize better. The two best teams used data augmentation in both the training and the test phase. Mario MNB team added two more interesting transformations, i.e. color saturation and lightness modifications. They also correlated the intensity of these transformations with the diminution of the learning rate during training to let the CNNs see patches closer to the original image at the end of each training process. Last but not least, Mario MNB is the only team who used exactly the same transformation in the training and test phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,158.49,300.90,298.38,7.89;10,141.88,115.84,328.53,170.29"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Min, max and average MRR scores detailed by organ combination</figDesc><graphic coords="10,141.88,115.84,328.53,170.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="12,134.77,345.05,345.83,7.89;12,134.77,356.03,122.72,7.86;12,141.88,115.83,328.54,214.44"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. MRR scores detailed by 3 categories of species: with an high, intermediate and low number of training images</figDesc><graphic coords="12,141.88,115.83,328.54,214.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,137.76,121.73,352.64,532.47"><head>Table 1 :</head><label>1</label><figDesc>Results of the LifeCLEF 2017 Plant Identification Task KDE TUT, Japan, 4 runs,<ref type="bibr" coords="6,273.56,130.92,11.15,8.77" target="#b3">[4]</ref>: this participant introduced a modified version of the ResNet-50 model. Three of the intermediate convolutional layers used for downsampling were modified by changing the stride value from 2 to 1 and preceding it by max-pooling with a stride of 2, to optimize the coverage of the inputs. Additionally, they switched the downsampling operation with the convolution for delaying the downsampling operation. This has been shown to improve performance by the authors of the ResNet architecture themselves. During the training they used data augmentation based on random crops, rotations and optional horizontal flipping. Test images were also augmented through a single flip operation and the resulting predictions averaged. Since the original ResNet-50 architecture was modified, no fine-tuning was used and the weights were learned from scratch starting with a big learning rate value of 0.1. The learning rates were dropped twice (to 0.01 and then 0.01) over 100 epochs according to a schedule ratio 4:2:1 indicating the number of iterations using the same learning rate (limited to a total number of 350 000 iterations in the case of the big noisy dataset due to technical limitations). Run 1, 2, 3 were trained respectively on the trusted dataset, noisy dataset, and both datasets. The final run 4 is a combination the outputs of the 3 runs.</figDesc><table coords="5,137.76,138.99,352.64,515.21"><row><cell>Run</cell><cell>Method</cell><cell>Training</cell><cell cols="3">MRR Top1 Top5</cell></row><row><cell>Mario TSA Berlin Run4</cell><cell>Average of many fine-tuned NNs</cell><cell>EOL, WEB</cell><cell cols="3">0.92 0.885 0.962</cell></row><row><cell>Mario TSA Berlin Run2</cell><cell>Average of 6 fine-tuned NNs</cell><cell>EOL, WEB, PlantCLEF2016</cell><cell cols="3">0.915 0.877 0.96</cell></row><row><cell>Mario TSA Berlin Run3</cell><cell>Average of 3 fine-tuned NNs</cell><cell>EOL, filtered WEB, scoring test data PlantCLEF2016, high</cell><cell cols="3">0.894 0.857 0.94</cell></row><row><cell>KDE TUT Run4</cell><cell>ResNet50 (modified)</cell><cell>EOL, WEB</cell><cell cols="3">0.853 0.793 0.927</cell></row><row><cell>Mario TSA Berlin Run3</cell><cell>Average of 3 fine-tuned NNs</cell><cell cols="4">EOL, PlantCLEF2016 0.847 0.794 0.911</cell></row><row><cell>CMP Run1</cell><cell>Inception-ResNet-v2</cell><cell cols="4">EOL, filtered WEB 0.843 0.786 0.913</cell></row><row><cell>KDE TUT Run3</cell><cell>ResNet50 (modified)</cell><cell>EOL, WEB</cell><cell cols="3">0.837 0.769 0.922</cell></row><row><cell>CMP Run3</cell><cell>Inception-ResNet-v2</cell><cell>EOL</cell><cell cols="3">0.807 0.741 0.887</cell></row><row><cell>FHDO BCSG Run2</cell><cell>Inception-ResNet-v2</cell><cell cols="4">EOL, filtered WEB 0.806 0.738 0.893</cell></row><row><cell>FHDO BCSG Run3</cell><cell>Inception-ResNet-v2</cell><cell cols="4">EOL, filtered WEB 0.804 0.736 0.891</cell></row><row><cell>UM Run2</cell><cell>VGGNet</cell><cell>WEB</cell><cell cols="3">0.799 0.726 0.888</cell></row><row><cell>UM Run3</cell><cell>VGGNet multi-organ</cell><cell>EOL, WEB</cell><cell cols="3">0.798 0.727 0.886</cell></row><row><cell>FHDO BCSG Run1</cell><cell>Inception-ResNet-v2</cell><cell>EOL</cell><cell cols="3">0.792 0.723 0.878</cell></row><row><cell>UM Run4</cell><cell>UM Run1&amp;2 max voting</cell><cell>EOL, WEB</cell><cell cols="3">0.789 0.715 0.882</cell></row><row><cell>KDE TUT Run1</cell><cell>ResNet50 (modified)</cell><cell>EOL</cell><cell cols="3">0.722 0.707 0.85</cell></row><row><cell>CMP Run2</cell><cell>Inception-ResNet-v2</cell><cell cols="4">EOL, filtered WEB 0.765 0.68 0.87</cell></row><row><cell>CMP Run4</cell><cell>Inception-ResNet-v2</cell><cell>EOL</cell><cell cols="3">0.733 0.641 0.849</cell></row><row><cell>UM Run1</cell><cell>VGGNet multi-organ</cell><cell>EOL</cell><cell cols="3">0.7 0.621 0.795</cell></row><row><cell>SabanciU GebzeTU Run 4</cell><cell>VGGNets</cell><cell cols="4">EOL, filtered WEB 0.638 0.557 0.738</cell></row><row><cell>SabanciU GebzeTU Run 1</cell><cell>VGGNets</cell><cell cols="4">EOL, filtered WEB 0.636 0.556 0.737</cell></row><row><cell>SabanciU GebzeTU Run 3</cell><cell>VGGNets</cell><cell cols="4">EOL, filtered WEB 0.622 0.537 0.728</cell></row><row><cell>PlantNet Run1</cell><cell>Inception v1</cell><cell>EOL</cell><cell cols="3">0.613 0.513 0.734</cell></row><row><cell>SabanciU GebzeTU Run2</cell><cell>VGGNets</cell><cell>EOL</cell><cell cols="3">0.581 0.508 0.68</cell></row><row><cell>UPB HES SO Run3</cell><cell>AlexNet</cell><cell>EOL</cell><cell cols="3">0.361 0.293 0.442</cell></row><row><cell>UPB HES SO Run4</cell><cell>AlexNet</cell><cell>EOL</cell><cell cols="3">0.361 0.293 0.442</cell></row><row><cell>UPB HES SO Run1</cell><cell>AlexNet</cell><cell>EOL</cell><cell cols="3">0.326 0.26 0.406</cell></row><row><cell>UPB HES SO Run2</cell><cell>AlexNet</cell><cell>EOL</cell><cell cols="3">0.305 0.239 0.383</cell></row><row><cell>FHDO BCSG Run4</cell><cell>Inception v4</cell><cell>PlantCLEF2016, WEB</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="3,144.73,646.48,268.32,7.47"><p>https://itunes.apple.com/fr/app/plantnet/id600547573?mt=8</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="3,144.73,657.44,273.03,7.47"><p>https://play.google.com/store/apps/details?id=org.plantnet</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,408.78,337.63,7.86;12,151.52,419.74,329.07,7.86;12,151.52,430.70,218.26,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,400.92,408.78,79.67,7.86;12,151.52,419.74,76.93,7.86">Pl@ntnet app in the era of deep learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Affouard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,248.52,419.74,232.07,7.86;12,151.52,430.70,49.65,7.86">5th International Conference on Learning Representations (ICLR 2017)</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">April 24-26 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,441.16,337.63,7.86;12,151.52,452.11,329.07,7.86;12,151.52,463.07,91.93,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,310.46,441.16,170.13,7.86;12,151.52,452.11,205.94,7.86">Plant identification with large number of classes: Sabanciu-gebzetu system in plantclef 2017</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Atito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,380.94,452.11,99.65,7.86;12,151.52,463.07,63.25,7.86">Working notes of CLEF 2017 conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,473.53,337.63,7.86;12,151.52,484.49,329.07,7.86;12,152.35,493.18,233.20,10.13" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,277.55,473.53,198.94,7.86">Plant identification in an open-world (lifeclef 2016)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,166.10,484.49,310.10,7.86">Working Notes of CLEF 2016-Conference and Labs of the Evaluation forum</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09-08">5-8 September, 2016. 2016</date>
			<biblScope unit="page" from="428" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,505.90,337.63,7.86;12,151.52,516.86,309.51,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,245.33,505.90,235.26,7.86;12,151.52,516.86,97.25,7.86">Residual network with delayed max pooling for very large scale plant identification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,269.59,516.86,162.78,7.86">Working notes of CLEF 2017 conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,527.31,337.63,7.86;12,151.52,538.27,329.07,7.86;12,151.52,549.23,76.80,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,290.95,527.31,172.55,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,538.27,324.88,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,559.68,337.63,7.86;12,151.52,570.64,159.05,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="12,294.75,559.68,181.51,7.86">Distilling the knowledge in a neural network</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.96,581.10,337.63,7.86;12,151.52,592.05,329.07,8.12;12,151.52,603.66,84.73,7.47" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,244.89,581.10,235.70,7.86;12,151.52,592.05,138.66,7.86">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CoRR abs/1502.03167</idno>
		<ptr target="http://arxiv.org/abs/1502.03167" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,613.47,337.63,7.86;12,151.52,624.43,329.08,7.86;12,151.52,635.39,177.27,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,344.18,624.43,136.41,7.86;12,151.52,635.39,22.40,7.86">A look inside the pl@ ntnet experience</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dufour-Kowalski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Affouard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carré</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,180.73,635.39,82.03,7.86">Multimedia Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,645.84,337.64,7.86;12,151.52,656.80,329.07,7.86;13,151.52,119.67,329.07,7.86;13,151.52,130.63,329.07,7.86;13,151.52,142.24,197.71,7.47" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,269.66,656.80,210.94,7.86">Are species identification tools biodiversity-friendly?</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1145/2661821.2661826</idno>
		<ptr target="http://doi.acm.org/10.1145/2661821.2661826" />
	</analytic>
	<monogr>
		<title level="m" coord="13,165.75,119.67,314.84,7.86;13,151.52,130.63,77.26,7.86;13,283.27,130.63,43.28,7.86">Proceedings of the 3rd ACM International Workshop on Multimedia Analysis for Ecological Data</title>
		<meeting>the 3rd ACM International Workshop on Multimedia Analysis for Ecological Data<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
	<note>MAED &apos;14</note>
</biblStruct>

<biblStruct coords="13,142.62,152.55,337.98,7.86;13,151.52,163.51,329.07,7.86;13,151.52,174.47,306.97,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,183.89,163.51,292.77,7.86">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,165.60,174.47,169.56,7.86">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="301" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,185.43,337.98,7.86;13,151.52,196.39,329.07,7.86;13,151.52,207.34,86.01,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,328.09,185.43,152.50,7.86;13,151.52,196.39,103.94,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,275.64,196.39,200.74,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,218.30,337.97,7.86;13,151.52,229.26,260.66,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,203.54,218.30,277.05,7.86;13,151.52,229.26,47.81,7.86">Image-based plant species identification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,220.74,229.26,162.78,7.86">Working notes of CLEF 2017 conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,240.22,337.97,7.86;13,151.52,251.18,329.07,7.86;13,151.52,262.14,119.70,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,304.70,240.22,175.89,7.86;13,151.52,251.18,235.95,7.86">Lifeclef 2017 plant identification challenge: Classifying plants using generic-organ correlation features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,410.16,251.18,70.43,7.86;13,151.52,262.14,91.03,7.86">Working notes of CLEF 2017 conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,273.10,337.97,7.86;13,151.52,284.06,329.07,7.86;13,151.52,295.02,205.53,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,466.51,273.10,14.08,7.86;13,151.52,284.06,324.82,7.86">Improving model performance for plant image classification with filtered noisy images</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Piorek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Kelch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rex</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,165.60,295.02,162.78,7.86">Working notes of CLEF 2017 conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,305.98,337.97,7.86;13,151.52,316.93,190.79,7.86" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="13,278.92,305.98,201.67,7.86;13,151.52,316.93,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,325.63,337.98,10.13;13,151.52,338.85,255.72,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,234.92,327.89,245.67,7.86;13,151.52,338.85,43.20,7.86">Learning with noisy and trusted labels for fine-grained plant recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Śulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,215.80,338.85,162.78,7.86">Working notes of CLEF 2017 conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,349.81,337.98,7.86;13,151.52,360.77,329.07,7.86;13,151.52,371.73,25.60,7.86" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<title level="m" coord="13,343.86,349.81,136.74,7.86;13,151.52,360.77,189.48,7.86">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.62,382.69,337.98,7.86;13,151.52,393.65,329.07,7.86;13,151.52,404.61,329.07,7.86;13,151.52,415.56,25.60,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,282.96,393.65,127.48,7.86">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,432.52,393.65,48.07,7.86;13,151.52,404.61,290.18,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,426.52,337.98,7.86;13,151.52,437.48,329.07,7.86;13,151.52,448.44,191.46,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,297.60,426.52,182.99,7.86;13,151.52,437.48,310.62,7.86">Upb hes so @ plantclef 2017: Automatic plant image identification using transfer learning via convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,151.52,448.44,162.78,7.86">Working notes of CLEF 2017 conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
