<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.99,116.95,301.39,12.62;1,140.37,134.89,334.63,12.62">Opinion polarity detection in Twitter data combining sequence mining and topic modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,189.07,172.56,75.63,8.74"><forename type="first">Asma</forename><surname>Ouertatani</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIPAH</orgName>
								<orgName type="institution" key="instit1">ENSI</orgName>
								<orgName type="institution" key="instit2">University of Manouba</orgName>
								<address>
									<settlement>Tunis</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.26,172.56,59.98,8.74"><forename type="first">Ghada</forename><surname>Gasmi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">INSAT</orgName>
								<orgName type="laboratory">LISI</orgName>
								<orgName type="institution">University of Carthage</orgName>
								<address>
									<settlement>Tunis</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,365.17,172.56,56.65,8.74"><forename type="first">Chiraz</forename><surname>Latiri</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">LIPAH</orgName>
								<orgName type="laboratory" key="lab2">FST</orgName>
								<orgName type="institution">University of Tunis El Manar</orgName>
								<address>
									<settlement>Tunis</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.99,116.95,301.39,12.62;1,140.37,134.89,334.63,12.62">Opinion polarity detection in Twitter data combining sequence mining and topic modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">66F5DDE84DC3A5FFC785809F304E3FAC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>topic modeling</term>
					<term>LDA</term>
					<term>opinion analysis</term>
					<term>sequence mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a pipeline process to analyze opinion about festivals and cultural events by automatically detecting polarity in Twitter data. Previous studies have focused in the polarity classification of individual tweets. However, to understand the polarity of opinion on a domain, it is important to find themes or topics that occur in the corpus. The first phase is to find the optimal number of topics and to identify the major topics via the latent Dirichlet analysis (LDA) topic model. The second stage is to detect polarity in tweets using the sequence mining approach mainly founded on sequences extracted from tweets using a LCM-seq algorithm <ref type="bibr" coords="1,248.67,354.69,9.22,7.86" target="#b8">[9]</ref>. The results showed that the polarity detection accuracy of the sequence mining was 84.78%, indicating that the proposed method was valid in most cases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the advent of web 2.0 and social network service evolution, users generated a massive amount of information stored in unstructured online reviews that can not simply be used for further processing by computers. Various researchers have conducted analyses focusing on the exchange of opinions that occurs on social network platforms. Twitter is an online social network where users post and interact with messages, "tweets", restricted to 140 characters. However, discovering sentiments and opinions through manual analysis of a large volume of textual data is extremely difficult. For that reason, specific preprocessing methods and algorithms are needed in order to mine useful patterns. Hence, in recent years, there have been much interests in the natural language processing community to develop novel text mining techniques with the capability of accurately extracting users' opinions from large volumes of information like Twitter data. Among various opinion mining tasks, one of them is polarity analysis, i.e. whether the semantic orientation of a text is positive or negative, which focuses on classifying the polarity of individual texts (e.g., web reviews or tweets) by selecting important features through methods such as n-grams <ref type="bibr" coords="2,367.68,119.99,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="2,384.84,119.99,11.62,8.74" target="#b10">11]</ref>, word subsequence <ref type="bibr" coords="2,134.77,131.95,14.61,8.74" target="#b11">[12]</ref>, information gain <ref type="bibr" coords="2,232.68,131.95,9.96,8.74" target="#b5">[6]</ref>, and recursive feature elimination <ref type="bibr" coords="2,397.07,131.95,9.96,8.74" target="#b0">[1]</ref>. When applying machine learning to opinion classification, most existing approaches rely on supervised learning models trained from labeled corpora where each document has been labeled as positive or negative prior to training. A tweet is then classified via algorithms, such as the nave naïve, maximum entropy <ref type="bibr" coords="2,384.95,179.77,14.61,8.74" target="#b10">[11]</ref>, or support vector machine (SVM) algorithms. However, sentiment classification models trained on one domain might not work at all when moving to another domain. Furthermore, in a more fine-grained opinion classification problem (e.g finding users opinions for a particular film festival), topic detection and opinion classification are often performed in a two-stage pipeline process, by first detecting a topic and later assigning a polarity label to that particular topic. We propose a pipeline process to analyze opinion about festivals and cultural events by automatically detecting polarity in Twitter data. Previous studies have focused on the polarity classification of individual tweets. However, to understand the polarity of opinion on a domain, it is important to find themes or topics that occur in the corpus. Our goal here is to find the optimal number of topics and to identify the major topics via the latent Dirichlet analysis (LDA) topic model. The second stage detects polarity in tweets using the sequence mining approach mainly founded on sequences extracted from tweets using a LCM-seq algorithm. The remainder of this paper is organized as follows. Section 2 details the proposed method, which includes a data-preprocessing step; Section 3 presents the analysis results; and Section 4 presents the conclusion of this study and discusses directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>The MC2@CLEF2017 lab has released a collection of 70 000 000 microblogs over 18 months dealing with cultural events <ref type="bibr" coords="2,308.35,501.70,9.96,8.74" target="#b6">[7]</ref>. Microblogs are in all languages. We used just 5 000 000 tweets from the collection. Simple and intiutive techniques in the preprocessing phase were evoked as removal links, twitter identifiers, pontuations and stop words. Clearly cannot be performed without knowing the underlying language detection. Therefore, modern text processing tools heavily rely on highly effective algorithms for language. We employed the Cavnar and Trenkle <ref type="bibr" coords="2,414.48,573.43,10.52,8.74" target="#b4">[5]</ref> approach to text categorization based on character n-gram frequencies that have been particularly successful. We used the implementation in the R extension package textcat aims at both exibility and convenience. After the preprocessing phrase we chosed the first 320000 english tweets to be our dataset. Figure <ref type="figure" coords="2,347.82,633.20,4.98,8.74" target="#fig_0">1</ref> presents a words cloud from our dataset. The word cloud principle is based on a text analysis method that allows us to highlight the most frequently used keywords ( like : music, Film..) in a text paragraph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topic modelling</head><p>Topic modeling is a type of statistical model in natural language processing that aims to find topics in a corpus, group topics together by looking for similarity and co-occurence, and categorize documents in the corpus based on the topic probabilities assigned. We are specifically using a statistical method called the latent Dirichlet allocation (LDA). Latent Dirichlet Allocation (LDA) is one of the most popular topic models <ref type="bibr" coords="3,168.77,470.58,9.96,8.74" target="#b2">[3]</ref>. In the context of LDA, a topic is composed of terms with creation probabilities. For each term position in a document, LDA identifies a topic, and the topic is composed of the terms included in the topic, measured probabilistically. Given a set of documents, LDA provides an algorithm that learns the topics and the terms associated with each topic. LDA requires one input parameter: the number of topics to extract. And now the question then arises as: What is the best way to determine k (number of topics) in topic modeling?</p><p>Optimal number of topics for LDA model : Before going right into generating the topic model and analysing the output, we need to decide on the number of topics that the model should use. We used 3 metrics to estimate the best fitting number of topics:</p><p>-Method based on the harmonic mean :</p><p>This method has first been applied by Griffiths and Steyvers <ref type="bibr" coords="3,419.74,633.20,9.96,8.74" target="#b7">[8]</ref>.</p><p>We calculated the harmonic mean of a the values sets of p(w|z, k). The model that we will retain by varying k will be the one which will have the highest value. z : Per word topic assignment. w : word. k : number of topic. -Density-based method <ref type="bibr" coords="4,252.13,168.38,10.52,8.74" target="#b3">[4]</ref> The principle is to calculate the similarity (or distance) between all pairs of themes for different models obtained by varying the number of themes. Themes are more independent if the similarity between themes is small. -Method based on the Kullback-Leibler divergence (KL) <ref type="bibr" coords="4,396.72,216.77,10.52,8.74" target="#b1">[2]</ref> The measure of divergence is a measure of how the topic1 distribution for document m and the word distribution for topic1 diverges from a second topics expected probability distribution.</p><p>The optimal k is the one with the lowest divergence. The three methods required to train multiple LDA models to select one with the best performance. So, the best way is to calculate all metrics at once, the figure <ref type="figure" coords="4,410.66,298.18,4.98,8.74">2</ref> represents the Results calculated on the whole dataset: The three methods agree that somewhere between 75 and 100 topics is optimal Fig. <ref type="figure" coords="4,278.01,526.55,4.13,7.89">2</ref>. number of topics for this dataset. To find the best value of the number of topics hyperparameter k we used the perplexity measure for the applicability of a topic model to new data and the 5 folds cross validation over the range of k [75..100]. Perplexity is a measure of how well a probability model predicts a sample. We opted to fit a model with 85 topics. In the figure <ref type="figure" coords="4,289.27,609.29,4.98,8.74" target="#fig_1">3</ref> the plot of the results: Terms are assigned to a topic with probabilities, so every term in the corpus is given a probability per topic. However, we can use the top terms to get a sense for what each topic covers. Figure <ref type="figure" coords="4,292.79,645.16,4.98,8.74">4</ref> shows the topics names. For the second stage of our approach we used the films topic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Polarity detection</head><p>Before starting the phase of analysis of polarity one must go through the stage of the analysis of subjectivity to remove the objective tweets of our collection. To do this, we used the subjectivity lexicon 1 , and N-gram as features and the naïve bayes as classifier. For the polarity detection, we used lexique Wordnet sentiment, T f * idf and the algorithm LCM-seq<ref type="foot" coords="6,226.10,142.33,3.97,6.12" target="#foot_1">2</ref> to extract all frequent item sequences. to use it as features.</p><p>Lcm-seq : is an efficient algorithm for enumerating frequent sequence patterns from a sequential database. In addition to its high speed, LCM-seq can be applied in a variety of ways, as it can assign a positive or negative weight to each sequence and only extract frequent sequence patterns that appear in a specified window width <ref type="bibr" coords="6,199.25,215.86,9.96,8.74" target="#b8">[9]</ref>. For a vocabulary V , the set of finite sequences on V is expressed by V * . A sequence pattern is an arbitrary sequence s = a 1 ....a n V * , and P = V * expresses the set of all sequence patterns on V . The sequence database on V is the sequence set S = s 1 , ..., s m . We denote the the size of S by |S|. For sequence pattern p ∈ P , a sequence database including p is called an occurrence of p. The denotation of p, denoted by θ(p) is the set of the occurrences of p. |θ(p)| is called the frequency of p, and denoted byF req. For given constant α ∈ N , called a minimum support, sequence pattern p is frequent if F req(p) ≥ α. In our approach, we used a value min sup equal to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental validation</head><p>For the phase of the subjectivity analysis we used as a training corpus introduced in Pang/Lee ACL 2004<ref type="foot" coords="6,266.41,425.00,3.97,6.12" target="#foot_2">3</ref> we used the Subjectivity lexicon and N-gram as features.</p><p>For the polarity detection we used the sentiment140 data as a training data <ref type="foot" coords="6,473.36,448.91,3.97,6.12" target="#foot_3">4</ref> , and we used the frequent item sequences as features for naïve bayes classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation protocol</head><p>As evaluation meteric we used the classifier Accuracy. The accuracy can be defined as the percentage of correctly classified instances :</p><formula xml:id="formula_0" coords="6,225.14,547.36,255.45,22.31">Accuracy = (T P + T N ) (T P + T N + F P + F N )<label>(1)</label></formula><p>Where TP, FN, FP and TN represent the number of true positives, false negatives, false positives and true negatives, respectively. The following table illustrates the results for the naïve bayes classifier : </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The polarity detection aims to automatically classify the customer opinion and provide comprehensive understanding of customer feedback from raw data on the Web. In all of the social network platforms, Twitter has been one of the most popular sources for marketing information research and sentiment classification. The work described in this paper is a step towards efficient classification of tweets using the topic modelling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,232.98,326.78,149.40,7.89;3,180.12,161.12,255.12,150.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. words cloud from our dataset</figDesc><graphic coords="3,180.12,161.12,255.12,150.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,205.84,306.90,203.67,7.89;5,194.29,327.41,226.77,215.19"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. 5-fold cross-validations of a topic modeling</figDesc><graphic coords="5,194.29,327.41,226.77,215.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,134.77,354.64,411.03,157.14"><head></head><label></label><figDesc></figDesc><graphic coords="4,134.77,354.64,411.03,157.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,224.21,118.75,166.79,141.18"><head>Table 2 .</head><label>2</label><figDesc>polarity detection results</figDesc><table coords="7,224.21,118.75,166.79,130.27"><row><cell></cell><cell>accuracy</cell></row><row><cell>Subjectivity lexique</cell><cell>75.14%</cell></row><row><cell>N-gram</cell><cell>80.2%</cell></row><row><cell cols="2">Subjectivity lexique + N-gram 81.5%</cell></row><row><cell cols="2">Table 1. subjectivity analysis results</cell></row><row><cell></cell><cell>accuracy</cell></row><row><cell>lexique wordnet</cell><cell>75.14%</cell></row><row><cell>T f  *  idf</cell><cell>79.80%</cell></row><row><cell cols="2">frequent item sequences 82.5%</cell></row><row><cell>All</cell><cell>84.78%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,144.73,657.79,184.99,7.86"><p>http://mpqa.cs.pitt.edu/lexicons/subj-lexicon</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.73,635.88,204.42,7.86"><p>http://research.nii.ac.jp/uno/code/LCM-seq.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,144.73,646.84,274.37,7.86"><p>http://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,144.73,657.79,175.02,7.86"><p>http://help.sentiment140.com/for-students]</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.96,448.86,337.63,7.86;7,151.52,459.82,329.07,7.86;7,151.52,470.78,145.04,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,339.19,448.86,141.40,7.86;7,151.52,459.82,135.34,7.86">Affect analysis of web forums and blogs using correlation ensembles</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Thoms</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,298.55,459.82,182.05,7.86;7,151.52,470.78,46.13,7.86">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1168" to="1180" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,481.86,337.64,7.86;7,151.52,492.82,329.07,7.86;7,151.52,503.78,298.50,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,437.09,481.86,43.50,7.86;7,151.52,492.82,325.06,7.86">On finding the natural number of topics with latent dirichlet allocation: Some observations</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Veni Madhavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Narasimha</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,151.52,503.78,206.97,7.86">Advances in Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="391" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,514.85,337.64,7.86;7,151.52,525.81,205.19,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,324.25,514.85,104.78,7.86">Latent dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,438.65,514.85,41.95,7.86;7,151.52,525.81,105.97,7.86">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01">Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,536.89,337.64,7.86;7,151.52,547.85,245.62,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,333.09,536.89,147.50,7.86;7,151.52,547.85,75.98,7.86">A density-based method for adaptive lda model selection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,235.28,547.85,62.67,7.86">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1775" to="1781" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,558.93,337.64,7.86;7,151.52,569.89,113.68,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,296.93,558.93,133.02,7.86">N-gram-based text categorization</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Cavnar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Trenkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,437.67,558.93,42.92,7.86;7,151.52,569.89,9.57,7.86">Ann Arbor MI</title>
		<imprint>
			<biblScope unit="volume">48113</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="175" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,580.96,337.63,7.86;7,151.52,591.92,329.07,7.86;7,151.52,602.88,329.07,7.86;7,151.52,613.84,329.07,7.86;7,151.52,624.80,100.85,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,298.49,591.92,182.11,7.86;7,151.52,602.88,329.07,7.86;7,151.52,613.84,142.41,7.86">Bulk heterojunction solar cells using thieno [3, 4-c] pyrrole-4, 6-dione and dithieno [3, 2-b: 2, 3-d] silole copolymer with a power conversion efficiency of 7.3%</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Beaupré</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-R</forename><surname>Pouliot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wakim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Leclerc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,304.54,613.84,172.20,7.86">Journal of the American Chemical Society</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4250" to="4253" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,635.88,337.63,7.86;7,151.52,646.84,329.07,7.86;7,151.52,657.79,329.07,7.86;8,151.52,120.67,329.07,7.86;8,151.52,131.63,243.32,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,151.52,646.84,324.94,7.86">CLEF 2017 Microblog Cultural Contextualization Lab Overview (regular paper)</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<ptr target="http://www.springerlink.com" />
	</analytic>
	<monogr>
		<title level="m" coord="7,163.06,657.79,282.87,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<title level="s" coord="8,389.27,120.67,91.32,7.86;8,151.52,131.63,52.28,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>CLEF, Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-09-11">11/09/2017-14/09/2017. 2017</date>
			<biblScope unit="volume">10456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,142.59,337.64,7.86;8,151.52,153.55,245.79,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,296.10,142.59,97.33,7.86">Finding scientific topics</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,405.80,142.59,74.80,7.86;8,151.52,153.55,117.84,7.86">Proceedings of the National academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">suppl 1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,164.51,337.63,7.86;8,151.52,175.46,329.07,7.86;8,151.52,186.42,167.01,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,296.06,164.51,184.53,7.86;8,151.52,175.46,127.66,7.86">Extracting promising sequential patterns from rfid data using the lcm sequence</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nakahara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Uno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,298.38,175.46,182.22,7.86;8,151.52,186.42,99.16,7.86">Knowledge-Based and Intelligent Information and Engineering Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,197.38,337.98,7.86;8,151.52,208.34,141.99,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,258.24,197.38,222.35,7.86;8,151.52,208.34,25.89,7.86">Twitter as a corpus for sentiment analysis and opinion mining</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Paroubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,197.33,208.34,20.42,7.86">LREc</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,219.30,337.98,7.86;8,151.52,230.26,329.07,7.86;8,151.52,241.22,329.07,7.86;8,151.52,252.18,133.34,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,310.14,219.30,170.45,7.86;8,151.52,230.26,110.09,7.86">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,278.77,230.26,201.81,7.86;8,151.52,241.22,166.75,7.86">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,263.14,337.97,7.86;8,151.52,274.09,300.27,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,265.26,263.14,215.33,7.86;8,151.52,274.09,105.93,7.86">Ensemble of feature sets and classification algorithms for sentiment classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,265.09,274.09,83.38,7.86">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1138" to="1152" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
