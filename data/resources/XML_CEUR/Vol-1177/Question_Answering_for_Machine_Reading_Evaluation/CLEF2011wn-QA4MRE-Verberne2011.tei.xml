<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,173.23,100.02,256.53,12.62;1,187.36,117.95,228.27,12.62">Retrieval-based Question Answering for Machine Reading Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,267.30,155.62,68.41,8.74"><forename type="first">Suzan</forename><surname>Verberne</surname></persName>
							<email>s.verberne@cs.ru.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Foraging Lab/Centre for Language and Speech Technology</orgName>
								<orgName type="institution">Radboud University Nijmegen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,173.23,100.02,256.53,12.62;1,187.36,117.95,228.27,12.62">Retrieval-based Question Answering for Machine Reading Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3758388536639F3325ADE2AA34061C5C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Question Answering for Machine Reading (QA4MRE) task was set up as a reading comprehension test consisting of 120 multiple-choice questions pertaining to twelve target texts (the test documents) grouped in three different topics. Since this is the first year that we participate in the task, we decided to follow a relatively knowledge-poor approach that is mainly based on Information Retrieval (IR) techniques. We participated in the English task only. In its most basic version, our system takes the question as a query and uses a standard word-based ranking model to retrieve the most relevant fragments from the test document. Then it matches each of the multiple-choice answer candidates against the set of retrieved text fragments to select the answer with the highest summed similarity (again measured using a standard ranking model). We investigated two forms of information expansion to improve over this baseline: (1) statistical expansion of the test document with sentences from the topical background corpus, and (2) expansion of the question with automatically gathered facts from the background corpus. Our best-performing experimental setting reaches an overall c@1 score of 0.37. We found that statistical expansion of the test documents gives very different results for the three topics but overall it gives an improvement over the document-only baseline. We could not gain any improvements from question-to-facts expansion. More experiments are needed to find a good implementation of fact expansion. In the near future, we will follow up on the current work with more experiments related to expansion of questions and documents for the purpose of question answering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>After a long tradition of tracks at CLEF in which Question Answering (QA) was a task in its own right, in 2010 and 2011 the organization prepared the QA task as an evaluation method for another task: Machine Reading <ref type="bibr" coords="1,230.59,563.48,9.96,8.74" target="#b2">[3]</ref>. According to the track guidelines, "Machine Reading can be defined as the automatic understanding of text. One way of evaluating the understanding of a text is to assess a system's ability to answer a set of questions about it."</p><p>The Question Answering for Machine Reading (QA4MRE) task was set up as a reading comprehension test. We participated in the English task, which consists of 120 multiple-choice questions pertaining to twelve target texts (the test documents). The test documents are grouped by four in three different topics: AIDS, Climate change and Music &amp; Society. For each of the three topics, a separate background corpus of respectively 29,000, 37,000 and 33,000 documents from a diverse range of sources was provided. In the reading test, five answer candidates are given per question. The participating systems have the choice of not answering a question if they are not sure of the selected answer.</p><p>Answering the reading test questions may require more information than the information that is explicitly contained in the test documents. Some examples are:</p><p>-A seemingly simple question such as "Who is the founder of the SING campaign? " (topic AIDS, document 1, question 2) can be answered by the sentence "And this is the name of my campaign, SING Campaign." but only in combination with the knowledge that 'my' refers to Annie Lennox, the author of the text. -The answer to the more complex question "What event caused Annie Lennox to commit herself to the fight against AIDS? " (topic AIDS, document 1, question 1) has to be distilled from the first 250 words of the text. The answer "Nelson Mandela's conference to the world press" is a summarization of Annie Lennox' motivation. -The question "In which European cities has Annie Lennox performed? " (topic AIDS, document 1, question 7) has as answer alternatives several cities where Annie Lennox performed but which are not located in Europe. Thus, this question can only be answered with knowledge of which cities are in Europe. -The answer to the question "In Frankie's jail, how was diamorphine taken? " is in the fragment "Frankie, at that time, was a heroin addict, and he was in jail. So his choice was either to accept that dirty needle or not to get high." but it is difficult to recognize because the synonym diamorphine is used instead of heroin in the question, and the correct answer "by intravenous injection" does not contain the word needle.</p><p>Considering these difficulties, the QA4MRE task asks for a number of natural language processing techniques including question analysis (answer type determination), named entity recognition, coreference resolution, synonym/hypernym mapping, etc. Since this is the first year that we participate in the task, we decided to follow a relatively knowledge-poor approach that is mainly based on Information Retrieval (IR) techniques.</p><p>In its most basic version, our system takes the question as a query and uses a standard wordbased ranking model to retrieve the most relevant fragments from the test document. Then it matches each of the multiple-choice answer candidates against the set of retrieved text fragments to select the answer with the highest summed similarity (again measured using a standard ranking model). We investigated two forms of information expansion to improve over this baseline: (1) statistical expansion of the test document with sentences from the topical background corpus, and (2) expansion of the question with automatically gathered facts from the background corpus.</p><p>In Section 2, we will explain our approach and the experiments that we carried out. In Section 3 we describe the results that we obtained. Our conclusions are in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our approach</head><p>We implemented the QA4MRE task as a pipeline process with five modules: (1) data preparation, (2) test document expansion, (3) question-to-facts expansion, (4) text fragment retrieval and (5) answer selection. Of these, modules 2 and 3 are optional. We implemented all modules in Perl. Then we ran four different experiments:</p><p>1. no expansion; 2. test document expansion; 3. question-to-facts expansion; 4. test document expansion and question-to-facts expansion.</p><p>In all these experiments, modules 1, 4 and 5 are used. Module 2 is active in experiment 2 and 4; module 3 in experiment 3 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data preparation</head><p>We prepared the test documents and the background corpus as follows: In the test documents, we replaced the words 'I', 'me', and 'my' by the name of the author, which we simply assumed to be the first name of the document. We split the test documents in target fragments of three sentences each, and the background corpora in single-sentence units. Then we lowercased all texts and removed punctuation. We created an inverted index of each topic's background corpus and we removed all words from the index that occur in fewer than three sentences. <ref type="foot" coords="3,432.11,113.43,3.97,6.12" target="#foot_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Test document expansion</head><p>In Section 1, we showed that some of the questions of the reading comprehension task cannot be answered with only the information contained in the corresponding test document; additional information from the background corpus is required to answer these questions. Our document expansion step in experiment 2 and 4 is based on the idea that the required additional information is in some way related to the information that is already present in the test document. We implemented a statistical expansion module that searches the most similar sentences in the topic's background corpus index for each of the three-sentence fragments (the 'target fragments') from the test document.</p><p>The similarity between the target fragments and the background sentences is measured using the BM25 ranking function <ref type="bibr" coords="3,216.11,273.13,9.96,8.74" target="#b0">[1]</ref>: For each target fragment T , containing keywords t 1 , ..., t n , the BM25 score of a background sentence S is:</p><formula xml:id="formula_0" coords="3,173.03,304.33,339.97,30.97">score(S, T ) = n i=1 ISF (t i ) • f (t i , S) • (k 1 + 1) f (t i , S) + k 1 • (1 -b + b • |S| avgsl )<label>(1)</label></formula><p>in which f (t i , S) is t i 's term frequency in S, |S| is the length of S in words, and avgsl is the average sentence length in the background collection. For the parameters k 1 and b, we used k 1 = 2.0 and b = 0.75. ISF (t i ) is the Inverse Sentence Frequency of the target term t i :</p><formula xml:id="formula_1" coords="3,235.84,387.80,277.16,23.23">ISF (t i ) = log N -n(t i ) + 0.5 n(t i ) + 0.5<label>(2)</label></formula><p>where N is the total number of sentences in the collection, and n(t i ) is the number of sentences containing t i .</p><p>Our document expansion module expands each target fragment with the ten highest-scoring sentences from the background collection that are at least four words long. <ref type="foot" coords="3,414.77,452.92,3.97,6.12" target="#foot_1">2</ref> Table <ref type="table" coords="3,449.68,454.50,4.98,8.74">1</ref> shows an example target fragment with the ten most related sentences that are retrieved from the background corpus. The result is a set of expanded target fragments ET for each test document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Question-to-facts expansion</head><p>In the previous subsection we described a statistical method for expanding the test document with information from the background corpus. A second type of expansion that we investigated, is expanding the question itself with structured information from the corpus. The rationale behind this is that each question contains names and domain-related terms about which the background corpus contains factual information. This factual information might help for answering the question. For example, the corpus contains facts about needle exchange programs, e.g. "needle exchange programs, which have proved to reduce the spread of blood-borne viruses" (document AIDS EN 10645.txt). This informtion can help answering the question "Why did a former UK Prime Minister start a needle exchange program?' (answer: "in order to prevent the spread of HIV")</p><p>In the remainder of the current subsection, we explain how we extracted facts from the corpus; in the next subsection we explain how we used the question-to-facts expansion in the QA process.</p><p>We implemented a fact extractor for English that is based on dependency relations, similar to the method proposed in <ref type="bibr" coords="3,195.71,684.41,9.96,8.74" target="#b5">[6]</ref>. Each fact has the general form subject|verb|object|complements or Table 1. Target fragment expansion example. The first line contains the target fragment: the first three sentences from document 1 on topic 'AIDS', lowercased and without punctuation. The next lines contain the most similar sentences from the background corpus on AIDS, together with the IDs of the document they come from and their BM25 score. Note that during matching, the words 'I', 'me' and 'my' have been replaced by the name of the author of the document, 'Annie Lennox'. why i am an hiv aids activist im going to share with you the story as to how i have become an hiv/aids campaigner and this is the name of my campaign We used the AEGIR parser <ref type="bibr" coords="4,225.48,479.03,10.52,8.74" target="#b4">[5]</ref> to generate the dependency relations needed for the fact extractor. AEGIR was developed for applications in Information Retrieval rather than in Linguistics. For that reason, its dependency model expresses the aboutness <ref type="bibr" coords="4,345.42,502.94,10.52,8.74" target="#b3">[4]</ref> of a sentence rather than describing its complete syntactic structure. AEGIR's output representation is comparable to the Stanford typed dependencies representation <ref type="bibr" coords="4,243.79,526.85,9.96,8.74" target="#b1">[2]</ref>, in the sense that it generates a set of binary relations between words for an input sentence, thereby converting some function words (such as prepositions) to relations. In addition to that, AEGIR performs a number of normalizing transformations, such as passive-to-active transformation. For example, the clause "an inflammatory reaction, caused by the bowel tissue" leads to the same analysis as "the bowel tissue causes an inflammatory reaction": "the bowel tissue | cause | an inflammatory reaction | "</p><p>We parsed the background corpora with AEGIR (version 1.8.2) and extracted the facts using a Perl script. We performed some automatic postprocessing to filter out 'empty' facts of which the subject or the object is a pronoun<ref type="foot" coords="4,235.00,640.18,3.97,6.12" target="#foot_2">3</ref> and facts that come from metadata instead of actual document content (e.g. containing the word 'Wikipedia').</p><p>We considered several options for expanding the input questions with facts from the background corpus, and we decided on a rather basic approach: We indexed the facts twice, by subject (first field) and by object (third field). Then, while processing the question Q, if any substring of the question acts as subject or object in an indexed fact F i , this fact is added to the question expansion set EF Q . We set a threshold on the frequency of the fact in the background corpus that has to be exceeded for the fact to be included in EF Q .</p><p>For example, the question "Why did a former UK Prime Minister start a needle exchange program?" (topic AIDS, document 3, question 6) contains three substrings that act as subject or object in facts from the background corpus: 'prime minister', 'needle exchange' and 'needle exchange program'. Example background facts containing these terms are: EF Q , the set of facts generated for a question, can be very large if frequent substrings (e.g. 'problem', 'people') from the question act as subject or object in facts. Therefore, we set a maximum sentence frequency of 2,000 and a minimum length of 7 characters on each substring. If a substring is more frequent or shorter, then no facts are generated for this substring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Text fragment retrieval</head><p>The retrieval module retrieves the most relevant target fragments for each question. If set to active, the document expansion module has before expanded each three-sentence fragment in the test document with ten sentences from the background corpus.</p><p>For measuring the similarity between the question and an (expanded) target score(Q, ET i ) we again use the BM25 ranking function. For each question, up to ten expanded fragments are retrieved for which the BM25 score is higher than a fixed threshold. The result is a set of (expanded) target fragments that are relevant to the question Q: ET Q .</p><p>If the question-to-facts expansion module is active (in experiment 3 and 4), then we add the selected facts EF Q to ET Q . For example, in experiment 2, two (non-expanded) fragments from the test document are retrieved for the question "Why did a former UK Prime Minister start a needle exchange program?" (topic AIDS, document 3, question 6). In addition to this, the question-tofacts expansion module selected 12 facts related to the question terms 'prime minister', 'needle exchange' and 'needle exchange program'. The two fragments and the twelve facts together form the question expansion set E Q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Answer selection</head><p>The last step of our pipeline is to select one of the multiple-choice answer options from the reading test. We implemented a simple voting system, again based on BM25, for selecting the most likely answer given E Q<ref type="foot" coords="5,166.88,542.96,3.97,6.12" target="#foot_3">4</ref> :</p><p>We treat each answer candidate as a query, and calculate its BM25 score for each of the elements in E Q,i . Then we sum this score over all elements to get a score for each answer:</p><formula xml:id="formula_2" coords="5,225.15,587.85,283.61,30.32">score(A j , E Q ) = n i=1 score(A, E Q,i ) (<label>3</label></formula><formula xml:id="formula_3" coords="5,508.75,598.26,4.24,8.74">)</formula><p>where n is the number of (expanded) target fragments and facts for Q.</p><p>We set a threshold on the answer selection score: If the score(A j , Q) for none of the answer candidates A j exceeds this threshold, then the question is not answered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>As introduced in the previous section, we submitted four experiments: 1. no expansion; 2. test document expansion; 3. question-to-facts expansion; 4. test document expansion and question-to-facts expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The effects of selective thresholds</head><p>In Section 2, we mentioned that we used thresholds at several points in our method.</p><p>In the question-to-facts expansion module, we applied a threshold to the frequency of a fact in the corpus for it to be selected for a question's expansion set. We noticed that there is quite some noise among the facts that only occur once in the corpus. Therefore, we require that the absolute fact frequency be ≥ 2. Table <ref type="table" coords="6,281.35,231.46,4.98,8.74" target="#tab_1">2</ref> shows the number of questions that are expanded with at least one fact from the background corpus, and the average number of facts per expanded question per topic. Considering these statistics, it is clear that more work is needed on sensible expansion of questions with facts. The low numbers of questions that get expanded with facts, especially for the AIDS and Music &amp; Society topics, suggests that our method for selecting substrings for expansion is too rigid. We will look into this in more detail before following up with additional experiments. Moreover, the Climate Change topic has an extremely high number of facts per expanded question. This is because the question substrings 'carbon dioxide' and 'climate change' are both very frequent in the Climate Change corpus, and generate large amounts of facts, e.g. The first of these facts may be relevant for the question "How could the reduction of carbon dioxide emissions be promoted?", but the second fact is not informative, and the third has incorrectly been generated due to parsing errors. An interesting challenge for future work is to estimate the informativeness of facts for question expansion.</p><p>Secondly, we applied a threshold on the BM25 score in the text fragment retrieval module. Up to ten fragments were retrieved per question, but if fewer than ten exceed the threshold then only those are retrieved that do. We used a threshold of score(Q, ET i ) = 2. On average, 2.0 fragments were retrieved per question in experiment 1 and 3 (without document expansion); 4.9 fragments were retrieved per question in experiments 2 and 4 (when the fragments have been expanded with information from the corpus).</p><p>Finally, we applied a threshold on the BM25 score in the answer selection module, which we use to decide whether or not our system answers the question: if it is not certain enough of the highest-scoring answer, then it decides not to answer the question. We used a threshold of score(A j , E Q ) = 1. Table <ref type="table" coords="6,204.57,654.70,4.98,8.74">3</ref> shows the number of unanswered questions per experimental setting per topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation of the runs</head><p>The main evaluation measure used for the QA4MRE runs is c@1 <ref type="bibr" coords="6,370.17,712.77,9.96,8.74" target="#b6">[7]</ref>. This measure not only counts the number of correctly answered questions, but also takes into account the number of questions that are not answered by a system:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,106.34,182.84,179.46,8.74;5,106.34,194.80,216.80,8.74;5,106.34,206.75,216.08,8.74"><head></head><label></label><figDesc>the country | have | prime minister | now an order | permit | needle exchange | in three city needle exchange program | decrease | the spread |</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,106.34,455.37,159.84,8.74;6,106.34,467.32,180.27,8.74;6,106.34,479.28,142.88,8.74"><head></head><label></label><figDesc>fossil fuel | generate | carbon dioxide the requirement | reduce | carbon dioxide the causal | link | carbon dioxide</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,91.40,285.94,416.89,50.98"><head>Table 2 .</head><label>2</label><figDesc>Statistics on question-to-facts expansion. The reading test consists of 40 questions per topic.</figDesc><table coords="6,91.40,306.74,319.71,30.18"><row><cell></cell><cell cols="3">AIDS Climate change Music &amp; Society</cell></row><row><cell>No. of expanded questions</cell><cell>2</cell><cell>11</cell><cell>4</cell></row><row><cell cols="2">Avg. no. of facts per expanded question 14.5</cell><cell>99.7</cell><cell>3.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,99.96,704.48,413.03,7.86;3,99.96,715.44,413.04,7.86;3,99.96,726.40,73.03,7.86"><p>Removing words from the index that have a sentence frequency below three also had a practical reason: Perl's data structures are not memory-efficient enough to store an inverted index including all hapaxes and near-hapaxes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,99.96,737.36,405.46,7.86"><p>Without the length requirement, BM25 gives us too many 'sentences' of only one or two words long.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,99.96,726.40,413.04,7.86;4,99.96,737.36,182.03,7.86"><p>Of course, better would be to replace these anaphors by their antecedent but we did not add an anaphora resolution step in this version of our method.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,99.96,726.40,413.04,7.86;5,99.96,737.36,357.96,7.86"><p>Recall that EQ contains text fragments from the test document; depending on which experiment we run these are statistically expanded to ETQ, and/or extended with the set of facts EFQ.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where nr is the number of correctly answered questions, nu is the number of unanswered questions and n is the total number of questions.</p><p>Table <ref type="table" coords="7,132.34,366.54,4.98,8.74">4</ref> shows the results for our runs in terms of c@1.</p><p>We make several observations from these results. First, experimental setting 2 gives the best results over all topics. Music &amp; Society is an exception to this trend; here the settings without expansion of the test documents outperform the settings with document expansion. We had a look at the expanded fragments from the source documents and we suspect that the specificity of the test documents (e.g. one specific music video) turns the most similar sentences from the background corpus (which are about other music videos) irrelevant for answering the questions. For the other two topics, expansion of the test document with sentences from the background corpus gives an improvement. This improvement is very large for the Climate Change topic (from c@1 = 0.29 to c@1 = 0.45).</p><p>Our second observation is that question-to-facts expansion does not improve the performance of our system on the reading tests. As we explained in Section 3.1, we expect that there is a lot to gain from sensible expansion of questions with facts: First, in the quality of the facts generated from the background corpus (which depends on the quality of the dependency parser), second in the selection of informative facts to be included in the expandion set, and third in the selection of the substrings (terms) that are expanded to facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>The QA4MRE task is a reading comprehension task: the goal is to answer a number of multiplechoice questions pertaining to a test document, possibly with use of a background corpus on the topic of the document.</p><p>We implemented a relatively knowledge-poor approach to the reading comprehension task. Our approach is based on retrieval techniques in two steps: retrieval of relevant fragments from the test document for the input question, and matching of the multiple-choice answer candidates against the retrieved fragments in order to select the most likely answer.</p><p>In a series of experiments, we investigated two forms of information expansion: (1) statistical expansion of the test document with sentences from the topical background corpus, and (2) expansion of the question with automatically gathered facts from the background corpus.</p><p>We found that statistical expansion of the test documents gives very different results for the three topics but overall it gives an improvement over the document-only baseline. We could not gain any improvements from question-to-facts expansion. More experiments are needed to find a good implementation of fact expansion.</p><p>In the near future, we will follow up on the current work with more experiments related to expansion of questions and documents for the purpose of question answering.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,93.58,194.39,419.42,7.86;8,102.15,205.35,410.85,7.86;8,102.15,216.31,320.63,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,292.95,194.39,220.06,7.86;8,102.15,205.35,91.60,7.86">Term Proximity Scoring for Ad-hoc Retrieval on Very Large Text Collections</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,216.38,205.35,296.62,7.86;8,102.15,216.31,202.61,7.86">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="621" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.58,227.27,419.42,7.86;8,102.15,238.23,410.85,7.86;8,102.15,249.19,216.15,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,266.90,227.27,197.24,7.86">The Stanford typed dependencies representation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,487.12,227.27,25.88,7.86;8,102.15,238.23,381.06,7.86">Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.58,260.15,419.41,7.86;8,102.15,271.11,410.85,7.86;8,102.15,282.06,147.44,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,273.73,260.15,64.72,7.86">Machine reading</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,357.11,260.15,155.89,7.86;8,102.15,271.11,98.72,7.86">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence<address><addrLine>Menlo Park, CA; Cambridge, MA; London</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999. 2006</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1517" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.58,293.02,419.42,7.86;8,102.15,303.98,283.17,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,327.35,293.02,158.98,7.86">Phrase-Based Document Categorization</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H A</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Beney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Verberne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,494.30,293.02,18.70,7.86;8,102.15,303.98,192.24,7.86">Current Challenges in Patent Information Retrieval</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="263" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.58,314.94,419.41,7.86;8,102.15,325.90,410.85,7.86;8,102.15,336.86,402.04,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,347.45,314.94,165.55,7.86;8,102.15,325.90,131.95,7.86">Constructing a broad coverage lexicon for text mining in the patent domain</title>
		<author>
			<persName coords=""><forename type="first">Nelleke</forename><surname>Oostdijk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suzan</forename><surname>Verberne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">A</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Koster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,253.71,325.90,259.30,7.86;8,102.15,336.86,372.91,7.86">Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC 2010). European Language Resources Association (ELRA)</title>
		<meeting>the Seventh conference on International Language Resources and Evaluation (LREC 2010). European Language Resources Association (ELRA)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.58,347.82,419.41,7.86;8,102.15,358.78,410.85,7.86;8,102.15,369.74,287.78,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,202.24,347.82,231.02,7.86">Semantic enrichment of text with background knowledge</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,455.73,347.82,57.27,7.86;8,102.15,358.78,410.85,7.86;8,102.15,369.74,29.93,7.86">Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading</title>
		<meeting>the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="15" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.58,380.69,419.42,7.86;8,102.15,391.65,410.85,7.86;8,102.15,402.61,312.46,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,259.47,380.69,165.03,7.86">A Simple Measure to Assess Non-response</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Penas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Del Rosal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,442.42,380.69,70.58,7.86;8,102.15,391.65,410.85,7.86">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
