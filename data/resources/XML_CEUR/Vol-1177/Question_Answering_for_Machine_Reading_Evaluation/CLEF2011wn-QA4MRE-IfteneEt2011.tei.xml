<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.28,142.31,318.84,11.85;1,241.32,158.75,129.00,11.85">Question Answering for Machine Reading Evaluation on Romanian and English</title>
				<funder ref="#_Mjg2ybJ">
					<orgName type="full">Sector Operational Program for Human Resources Development</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,157.08,194.91,50.35,8.37"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,215.16,194.91,97.27,8.37"><forename type="first">Alexandru-Lucian</forename><surname>Gînscă</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.16,194.91,45.67,8.37"><forename type="first">Alex</forename><surname>Moruz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">Romanian Academy Iasi Branch</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.92,194.91,62.84,8.37"><forename type="first">Diana</forename><surname>Trandabăț</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">Romanian Academy Iasi Branch</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.76,205.95,62.83,8.37"><forename type="first">Maria</forename><surname>Husarciuc</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Center of Biblical-Philological Studies Monumenta linguae Dacoromanorum</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.28,142.31,318.84,11.85;1,241.32,158.75,129.00,11.85">Question Answering for Machine Reading Evaluation on Romanian and English</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">043C4CD67A1F8AC8242E66F8928152E6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Question Answering for Machine Reading Evaluation, Information Retrieval</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes UAIC 1 's Question Answering for Machine Reading Evaluation systems participating in the QA4MRE 2011 evaluation task. The system is designed to extract knowledge from large volumes of text and to use this knowledge to answer questions in Romanian and English monolingual tasks. Our systems were built on the architecture of a Question Answering system, customized for this new task. Thus, the new system used from our previous question answering systems the question processing and information retrieval components, adapted for new requests. Additionally, a new component was added in order to detect the most probable answer of a question, from a list of possible answers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question Answering for Machine Reading Evaluation (QA4MRE<ref type="foot" coords="1,393.96,477.27,2.81,5.04" target="#foot_1">2</ref> ) is the exercise of developing a methodology for evaluating Machine Reading systems through Question Answering and Reading Comprehension Tests.</p><p>In 2011, the QA4MRE task focused on reading a single document and correctly identifying the answer from a set of possible answers, using some inference and the previously acquired background knowledge. The competitors received test data and background knowledge related to three topics: AIDS, Climate Change and Music and Society. An important note is that, for all involved languages (English, Spanish, German, Italian and Romanian), the test data was the same (parallel translations) and the background knowledge was available to all participants.</p><p>Preparing the 2011 exercise, we started from the systems built for the 2009 and 2010 QA@CLEF editions <ref type="bibr" coords="1,243.96,596.55,9.94,8.37" target="#b0">[1]</ref>, <ref type="bibr" coords="1,259.80,596.55,9.94,8.37" target="#b1">[2]</ref>.</p><p>The general architecture of our Question Answering for Machine Reading Evaluation system, similar for the two considered languages, is described in Section 2. Section 3 is concerned with the presentation of the results, while the last Section discusses the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System components</head><p>The system we participated with in QA4MRE 2011 uses some components from the system we used in 2010 <ref type="bibr" coords="2,244.80,245.43,9.94,8.37" target="#b1">[2]</ref>, adapted for the new task (question analysis, corpus indexing and snippet extraction) and some new components (mainly for the identification of the correct answer). The architecture of the current Romanian system is presented in Figure <ref type="figure" coords="2,226.68,277.95,4.67,8.37">1</ref> and the main components are detailed in next subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: UAIC system used in QA4MRE</head><p>The English system is almost similar with the Romanian system. Due to technical problems in handling the English background knowledge, we skipped over using the components presented in subsections 2.1 and 2.3, and the component detailed in subsection 2.4 was only partially used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background knowledge indexing</head><p>The Romanian background knowledge consists of a collection of 161,279 documents in text format (25,033 correspond to the AIDS topic, 51,130 to Climate Change topic and 85,116 to Music and Society topic). The indexing component was responsible for taking the name of the file and the text from it and adding both to the Lucene index 1 using Lucene<ref type="foot" coords="3,193.68,204.87,2.81,5.04" target="#foot_2">3</ref> libraries <ref type="bibr" coords="3,231.96,205.23,10.91,8.37" target="#b2">[3]</ref> (see Figure <ref type="figure" coords="3,289.80,205.23,4.67,8.37">1</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Test data processing</head><p>The test data consists in an XML file with 12 test documents (4 documents for each of the three topics), 10 questions for each document (120 questions in total) and 5 possible answers for each question (600 possible answers in total). Test data processing involved 3 operations: (a) extracting documents, (b) processing questions and (c) processing possible answers. For (a), we extract the content of the tag &lt;doc&gt; from the XML with the test data, and save it in a relative path corresponding to the current &lt;topic id&gt; and &lt;reading test id&gt; tags. In this way, we can use later this document to build Lucene index 2, as presented in Figure <ref type="figure" coords="3,439.20,333.51,3.51,8.37">1</ref>.</p><p>For (b) and (c), we used our question processing module from <ref type="bibr" coords="3,398.40,344.31,10.91,8.37" target="#b0">[1]</ref> and performed the following important steps: i. Stop words elimination; ii.</p><p>Lemmatization; iii.</p><p>Named Entity identification; iv.</p><p>Lucene query building. For the first three steps, we used this year the web services available both for Romanian and English from the Sentimatrix<ref type="foot" coords="3,309.48,433.83,2.81,5.04" target="#foot_3">4</ref> project <ref type="bibr" coords="3,343.20,434.19,9.94,8.37" target="#b3">[4]</ref>.</p><p>For instance, in the case of the question "Ce a spus Nelson Mandela la conferința de presă?" (En: What did Nelson Mandela say at the press conference?) the execution of the above steps has the following results:</p><p>-in the first step, the following stop words are eliminated: ce, a, la, de (En: what, the, at); -in the next step, lemmas for the words spus, conferința, presă (En: say, conference, press) are identified;</p><p>-in the third step, Nelson Mandela is identified as a Named Entity;</p><p>-in the last step, the Lucene query is build: "(spus^2 spune) Nelson^3 Mandela^3 (conferința^2 conferință) (presă^2 presa)".</p><p>From the above Lucene query, one can notice that we consider named entities to be of most relevance (hence receiving a boot of 3), and the inflected and lemmatized form of the words existing in the question receive a lower boost value (2 in the example above).</p><p>Additionally for (c), we used from the ontology presented in <ref type="bibr" coords="4,404.64,140.79,10.79,8.37" target="#b4">[5]</ref> the relations between regions and cities and the relations between cities and countries, in order to eliminate the answers with low probability to be the final required answer. Thus, for the question În ce orașe europene a cântat Annie Lennox? (En: In which European cities has Annie Lennox performed?), we eliminate from the list of possible answers the answers with non-European cities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Information Retrieval on Background Knowledge</head><p>The purpose of this module is to retrieve, for every question, the relevant documents from the background knowledge. For this task, similar to our previous approach from 2010 and 2009, we used the Lucene search over the index presented in section 2.1 using the Lucene queries presented in section 2.2.</p><p>The result of this step is a list of documents from the background knowledge, with associated relevance score obtained after performing Lucene search using the queries obtained after processing the questions. Thus, we have Score(d, q), the relevance score for a document d when we search the background knowledge with the Lucene query associated to question q.</p><p>Similar to 2.2 (a), we copy the content of all the files from this list in a relative path with the name obtained from the &lt;topic id&gt;, &lt;reading test id&gt; and the &lt;question id&gt; tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Indexing and searching using relevant documents for questions</head><p>This module takes all documents from 2.2 (a) and 2.3 (using the relative path obtained from &lt;topic id&gt;, &lt;reading test id&gt; and &lt;question id&gt; tags) and puts them in a separate index. The results of this step are 120 separate indexes for every question from the initial test data (Lucene index 2 in Figure <ref type="figure" coords="4,340.80,451.47,3.38,8.37">1</ref>). Because of how we saved the relevant files using relative paths, files from the Lucene index 2 are relevant to the corresponding question for the specific relative path.</p><p>Then in every index, we performed searches using Lucene queries obtained at 2.2 (c) and, for every answer, a list of documents with Lucene relevance scores are returned, where Score(d, a) is the relevance score for document d when we search with the Lucene query associated to the answer a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Identifying of most probable answer</head><p>The results of this step are the runs submitted by our group. In order to do this for every question, we combine the Lucene scores from 2.3 and 2.4 using the following formula:</p><formula xml:id="formula_0" coords="4,176.40,612.00,233.69,10.32">‫)ܽ(݁ݎܿܵ‬ = ∑ ‫,݀(݁ݎܿܵ‬ ‫)ݍ‬ × ‫,݀(݁ݎܿܵ‬ ܽ) ௗ∈ோ௩௧_ௗ௦</formula><p>where a is the current answer, q is the corresponding question from the test data and d is a document from the list of relevant documents returned by the search in Lucene index 2.</p><p>After we calculate the above value for all answers associated to a question, we consider the answer with the highest value as being the most probable answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Evaluation</head><p>For the QA4MRE 2011 task, our team submitted 10 runs, out of which 9 were for the Romanian-Romanian language pair and one for the English-English pair.</p><p>The evaluation of this year's results is done from two different perspectives. The first one is equivalent to a traditional evaluation in which all the answers are gathered in a single set which is then compared to a gold standard, not taking into account the document associated with a particular answer. On the other hand, the reading perspective offers insight on how well the system "understands" a particular document. At first, the C@1 measures <ref type="bibr" coords="5,291.24,321.15,11.03,8.37" target="#b5">[6]</ref> of each test comprising of 10 questions per document are taken into consideration. These results are then used to obtain statistical measures, such as the mean, median and standard deviation over values grouped by topic or as an overall view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation at the question answering level</head><p>We grouped our runs for Romanian based on the threshold used to consider a NOA response. For the first group of runs, we imposed the condition that NOA should be used for the case in which the Lucene index searcher didn't return any documents. For the second and third group of runs, the threshold was determined by the value given by the Lucene score associated with each document found. For the second group, the threshold was set at 0.05 and for the third one, at 0.02. The best results obtained by runs from each of the three groups are presented in Table <ref type="table" coords="5,364.08,471.03,4.67,8.37" target="#tab_1">1</ref> and Table <ref type="table" coords="5,411.72,471.03,4.67,8.37" target="#tab_2">2</ref> in the Ro-Ro column, which has three sub columns. For the English run, the threshold was fixed at 0.02. As can be seen in Table <ref type="table" coords="6,245.76,140.79,3.51,8.37" target="#tab_1">1</ref>, the best result of our system in terms of C@1 measure is obtained for the English run. For Romanian, the best run is the one in the first group, and the worst results are from the group with the 0.05 threshold.</p><p>We can observe the influence of the correctly unanswered questions in the C@1 measure when comparing the number of right answers for the best run for Romanian, the one in the first column, with the one for the English run. Although in the Ro-Ro run, a higher number of questions were correctly answered (30 right answers) than in the En-En run (25 right answers), the better C@1 measure is obtained for the English run. This is explained by the difference in the number of correctly unanswered questions. Thus, for the Romanian version of the system, we have decided to only return an empty answer when our information retrieval module did not return any result (this was empirically determined); this greatly improved the precision of determining empty answers, but significantly reduced our recall for them.</p><p>In the case of the English system, we have empirically established that the Lucene threshold below which we can safely assume that no answer can be provided is the score 0.02. As can be seen from the table given above, this resulted in many more unanswered questions, which greatly decreased precision but improved the recall, and also resulted in a significant increase in the C@1 measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation at the reading test level</head><p>In Table <ref type="table" coords="6,180.48,377.19,3.45,8.37" target="#tab_2">2</ref>, we present the median and mean for each of the three topics, Topic1 (AIDS), Topic2 (Climate Change) and Topic3 (Music and society) and their overall values. The three columns in the Romanian part of the table correspond to the best means given by runs in each of the three groups described in the previous section. These results are consistent with the trend introduced in Table <ref type="table" coords="6,400.92,554.07,3.45,8.37" target="#tab_1">1</ref>. The best mean was obtained for the English run, followed by the best Romanian run from the first threshold group of runs.</p><p>One anomaly can be observed in the results of the English run. In all the Romanian runs, the median is significantly lower than the mean, but in the English run, the order is reversed. We can therefore consider that our system performs uniformly well on the majority of the test for the English run, with fewer spikes in the C@1 values distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Error analysis</head><p>In extension to the analysis carried out above, we have also performed error analysis over the reported results (only the top scoring runs were analyzed). One of the most common error sources arises from our attempt to take into account all of the supporting snippets that our information retrieval procedure returns. Instead of comparing the results for all the snippets independently, we have combined the results for each candidate answer by calculating the average score, which is to say, if a candidate answer had more than one supporting snippet, the score for the entire candidate was the average of all the scores of its supporting snippets. Examples of this type of error can be seen for questions 8, 9, 2 and 3 in Topic 1, Reading Test 1. The solution to this type of error is to only take into account the highest scoring snippet for each candidate, instead of combining the scores.</p><p>Another error source we have indentified is incorrect query building, especially in the case of long queries, as can be seen in the case of question 5 in Topic 1, Reading Test 1. The mistake in query building arises from the fact that for the second candidate, the preposition and article construct "într-un" (En: in a) is not recognized as a stop word, and as such is included in the query, thus artificially boosting the query score (the Lucene query we have used is "asistentă (medicală^2 medical) într-un^2 spital"). The solution to this error is to perform more accurate POS tagging and to exclude functional words from queries.</p><p>Ambiguity in terms of answer extraction is also a cause for errors, as can be seen in question 6 in Topic 1, Reading Test 1. After the information retrieval step, two candidates (the first and the third) are determined to have identical top scores, supported by identically scoring snippets. In this case the QA system defaults to choosing the first candidate, which is incorrect. The solution to this type of error is twofold: performing information extraction at the paragraph or sentence level, in order to only take into account those parts of the knowledge base which refer to the question focus, and to perform an additional step of determining the distance between each candidate and the focus of the question in the knowledge base.</p><p>The same solution as the one described above can be applied to error cases such as those found for question 4 in Topic 2, Reading Test 8. This error comes from the fact that the answer candidates are all country names, and the top scoring snippet is obtained for the name that has the highest Tf/Idf value, regardless of the relevance to the original question. This type of error is quite common, and is not limited to single entity candidates, as can be seen in the case of questions 7, 9 and 10 in Topic 2, Reading Test 8 and for question 7, Topic 3, Reading Test 12 (for the En-En task). Another possible solution for this issue is to increase the score of those candidates which can be found in multiple supporting documents in the knowledge base.</p><p>In some cases, query generation requires the use of semantic equivalents for candidates in order to determine the correct answer, as is the case for question 10 in Topic 1, Reading Test 1. The system chooses an incorrect answer because the correct candidate, in this case "categoric da" (En: definitely yes) cannot be found in the supporting document or in the relevant articles of background knowledge in the same surface form, but it can be found as a semantic equivalent. In order to compensate for this problem, query generation should also take into account semantic equivalents of words. Some error cases are due to the fact that, in some cases, the answer extraction module does not choose the top scoring snippet, and therefore misses the correct answer by discarding it, as can be seen for question 2 in Topic 3, Reading Test 12 (for the En-En task).</p><p>An error that is only encountered in the case of the En-En task is that of missing background information. Because of time constraints, we were unable to make use of the BK available for the English task, and, because of this, we were unable to find some answers, as can be seen in question 3, Topic 3, Reading Test 12, where the correct answer, "five", is not present at all in the supporting document. This can also be seen in the case of question 8, Topic 3, Reading Test 12, where the reference to a person born in 1889 is missed because the supporting document makes no reference to that year.</p><p>Numbers are also a major cause of errors, mainly because they can be written either with letters or with digits, as can be seen in question 4, Topic 3, Reading Test 12. This can be solved at the question processing stage, where the numbers can be transformed in both formats, in order to cover all possibilities. Some errors also come from the fact that the Lucene indexer and query system treats the query words and the indexed text as a bag of words, and disregards the fact that, in some cases, if the query words are in different sentences, they lose their meaning as an answer candidate. This is the case for question 9, Topic 3, Reading Test 12, where instead of searching for "35 years" or "50 years", the system instead searches for "35" "years" and "50" "years". Since "50" appears in the text twice, and "35" only once, the selected candidate is "50 years" (on the basis of higher Tf/Idf), regardless of the act that "50" is in no way connected to "years".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper presents our systems built for the Question Answering for Machine Reading Evaluation task within CLEF 2011 labs. The evaluation shows an overall accuracy of 0.25 for the Ro-Ro monolingual task and 0.21 for the En-En task, and a C@1 measure of 0.26 for Ro-Ro and 0.29 for En-En. The thresholds used to obtain the NOA answers were properly selected for English (from this reason the C@1 measure is higher for English, even if the overall accuracy is higher for Romanian).</p><p>The presented systems were built starting from the main components of our QA systems (the question processing and information retrieval modules) to which new components were added for identifying the most probable answer from a set of possible answers.</p><p>What is interesting is that, although we did not use for the English monolingual task the background knowledge, the results were better in terms of the C@1 measure for English, because of the threshold we considered for the NOA answers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,194.64,514.59,219.74,118.36"><head>Table 1 :</head><label>1</label><figDesc>Results of UAIC's runs at question answering level</figDesc><table coords="5,194.64,531.39,219.74,101.56"><row><cell></cell><cell></cell><cell>Ro-Ro</cell><cell></cell><cell>En-En</cell></row><row><cell>answered right</cell><cell>30</cell><cell>11</cell><cell>19</cell><cell>25</cell></row><row><cell>answered wrong</cell><cell>85</cell><cell>19</cell><cell>43</cell><cell>47</cell></row><row><cell>total answered</cell><cell>115</cell><cell>30</cell><cell>62</cell><cell>72</cell></row><row><cell>unanswered right</cell><cell>0</cell><cell>19</cell><cell>11</cell><cell>12</cell></row><row><cell>unanswered wrong</cell><cell>0</cell><cell>66</cell><cell>42</cell><cell>34</cell></row><row><cell>unanswered empty</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>2</cell></row><row><cell>total unanswered</cell><cell>5</cell><cell>90</cell><cell>58</cell><cell>48</cell></row><row><cell>Overall accuracy</cell><cell>0.25</cell><cell>0.09</cell><cell>0.16</cell><cell>0.21</cell></row><row><cell>C@1 measure</cell><cell>0.26</cell><cell>0.16</cell><cell>0.23</cell><cell>0.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,198.72,431.55,215.98,107.92"><head>Table 2 :</head><label>2</label><figDesc>Results of UAIC's runs at reading test level</figDesc><table coords="6,198.72,448.35,215.98,91.12"><row><cell></cell><cell></cell><cell>RO-RO</cell><cell></cell><cell>EN-EN</cell></row><row><cell>Topic1 median</cell><cell>0.10</cell><cell>0.00</cell><cell>0.07</cell><cell>0.23</cell></row><row><cell>Topic2 median</cell><cell>0.40</cell><cell>0.00</cell><cell>0.29</cell><cell>0.31</cell></row><row><cell>Topic3 median</cell><cell>0.30</cell><cell>0.32</cell><cell>0.33</cell><cell>0.36</cell></row><row><cell>Overall median</cell><cell>0.20</cell><cell>0.00</cell><cell>0.16</cell><cell>0.31</cell></row><row><cell>Topic1 mean</cell><cell>0.10</cell><cell>0.04</cell><cell>0.08</cell><cell>0.25</cell></row><row><cell>Topic2 mean</cell><cell>0.39</cell><cell>0.08</cell><cell>0.26</cell><cell>0.27</cell></row><row><cell>Topic3 mean</cell><cell>0.29</cell><cell>0.30</cell><cell>0.31</cell><cell>0.32</cell></row><row><cell>Overall mean</cell><cell>0.26</cell><cell>0.14</cell><cell>0.22</cell><cell>0.28</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,148.32,635.32,140.67,7.56"><p>University "Al. I. Cuza" of Iasi, Romania</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,148.32,645.28,171.98,7.56"><p>QA4MRE: http://celct.fbk.eu/QA4MRE/index.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,148.32,635.32,112.87,7.56"><p>Lucene: http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,148.32,645.28,134.95,7.56"><p>Sentimatrix: http://www.sentimatrix.eu/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. The research presented in this paper was funded by the <rs type="funder">Sector Operational Program for Human Resources Development</rs> through the project "<rs type="projectName">Development of the innovation capacity and increasing of the research impact through post</rs><rs type="programName">-doctoral programs</rs>" <rs type="grantNumber">POSDRU/89/1.5/S/49944</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Mjg2ybJ">
					<idno type="grant-number">POSDRU/89/1.5/S/49944</idno>
					<orgName type="project" subtype="full">Development of the innovation capacity and increasing of the research impact through post</orgName>
					<orgName type="program" subtype="full">-doctoral programs</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,146.37,166.84,321.89,7.56;9,156.00,176.68,312.31,7.56;9,156.00,186.40,312.19,7.56;9,156.00,196.12,285.17,7.56" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,437.76,166.84,30.50,7.56;9,156.00,176.68,169.83,7.56">Question Answering on English and Romanian Languages</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trandabăţ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Pistol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Husarciuc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cristea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,425.40,176.68,39.13,7.56">CLEF 2009</title>
		<title level="s" coord="9,232.91,186.40,154.96,7.56;9,410.76,186.40,57.43,7.56;9,156.00,196.12,40.27,7.56">Multilingual Information Access Evaluation</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6241</biblScope>
			<biblScope unit="page" from="229" to="236" />
		</imprint>
	</monogr>
	<note>I Text Retrieval Experiments</note>
</biblStruct>

<biblStruct coords="9,146.37,205.84,321.94,7.56;9,156.00,215.44,312.31,7.56;9,156.00,225.28,302.33,7.56" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,345.24,205.84,123.07,7.56;9,156.00,215.44,108.29,7.56">Question Answering on Romanian, English and French Languages</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trandabăţ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Husarciuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,272.40,215.44,191.78,7.56">Notebook Paper for the CLEF 2010 LABs Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">September. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.37,235.00,167.38,7.56" xml:id="b2">
	<monogr>
		<ptr target="http://lucene.apache.org/java/docs/" />
		<title level="m" coord="9,156.00,235.00,30.61,7.56">LUCENE</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.37,244.48,322.06,7.56;9,156.00,254.56,312.39,7.56;9,156.00,264.16,312.28,7.56;9,156.00,273.88,312.31,7.56;9,156.00,283.60,22.49,7.56" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,198.72,254.56,188.04,7.56">Sentimatrix -Multilingual Sentiment Analysis Service</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Gînscă</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Boroș</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trandabăţ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Toader</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Corîci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">A</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cristea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,403.44,254.56,64.95,7.56;9,156.00,264.16,312.28,7.56;9,156.00,273.88,69.37,7.56">Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (ACL-WASSA2011)</title>
		<meeting>the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (ACL-WASSA2011)<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">June 19-24. (2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.37,293.44,321.89,7.56;9,156.00,303.16,312.31,7.56;9,156.00,312.88,299.57,7.56" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,279.72,293.44,173.46,7.56">Named Entity Relation Mining Using Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Balahur-Dobrescu</surname></persName>
		</author>
		<idno>EAN: 9782951740846. 28-30</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,156.00,303.16,308.26,7.56">Proceedings of the Sixth International Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the Sixth International Language Resources and Evaluation (LREC&apos;08)<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-05">May. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.37,322.48,322.01,7.56;9,156.00,332.32,312.39,7.56;9,156.00,342.04,260.57,7.56" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,238.20,322.48,146.29,7.56">A Simple Measure to Assess Non-response</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,400.20,322.48,68.18,7.56;9,156.00,332.32,312.39,7.56;9,156.00,342.04,106.86,7.56">Proceedings of 49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)</title>
		<meeting>49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">June 19-24. (2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
