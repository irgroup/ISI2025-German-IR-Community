<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.20,115.82,330.96,12.93;1,195.00,133.82,225.16,12.93">The DI@UE&apos;s participation in QA4MRE: from QA to multiple choice challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,238.44,171.07,44.00,9.96"><forename type="first">José</forename><surname>Saias</surname></persName>
							<email>jsaias@di.uevora.pt</email>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Informática</orgName>
								<orgName type="institution">ECT Universidade de Évora</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.04,171.07,72.06,9.96"><forename type="first">Paulo</forename><surname>Quaresma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Informática</orgName>
								<orgName type="institution">ECT Universidade de Évora</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.20,115.82,330.96,12.93;1,195.00,133.82,225.16,12.93">The DI@UE&apos;s participation in QA4MRE: from QA to multiple choice challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C3AD92DD7EBDAD2157C8DF075C219BCB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This QA4MRE edition brought two challenges to the DI@UE team: the absence of Portuguese as a working language and the different nature of the task when compared with previous participation in QA@CLEF. We addressed this multiple choice answering problem by assessing answer candidates in a text surface based manner, without a deep linguistic processing. This system employs a Lucene based search engine and Wordnet to assist in synonym check and morphological normalization. Answer analysis and the criteria for the answering decision are fundamentally based on superficial analysis of document text, enriched with semantic validation of compatibility between terms. The solution we describe answered to 73 from 120 questions, having 18 correct answers and an accuracy of 0.15.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the participation of the Informatics Department of the University of Évora (DI@UE) team at Question Answering for Machine Reading Evaluation (QA4MRE) <ref type="foot" coords="1,252.12,492.54,3.97,6.19" target="#foot_0">1</ref> of Cross Language Evaluation Forum (CLEF2011). In previous editions of CLEF, DI@UE focused on the Portuguese monolingual Question Answering (QA) task <ref type="foot" coords="1,272.40,516.42,3.97,6.19" target="#foot_1">2</ref> . In QA@CLEF 2008 we used the Senso system <ref type="bibr" coords="1,154.44,529.39,10.56,9.96" target="#b3">[4]</ref> for open domain QA, featuring a portuguese stemmer, a text indexation engine and an answer validation module. In this system's latest evolution <ref type="bibr" coords="1,464.32,541.39,12.30,9.96" target="#b4">[5]</ref>, candidate answers for each question are contextualized over time, space and semantic dimension. This organization enables a multiple perspective differentiation over the answer list and supports the answer appreciation process, but it is not multilingual. It incorporates tools for the Portuguese language, which was not included in the list of languages for this QA4MRE edition. Even with the introduction of tools for the English language, Senso system did not seem the most suitable for processing QA4MRE questions.</p><p>The task in which participated until 2008 was substantially different from that required this year. For QA@CLEF, the purpose was to automatically find the answer to a set of questions. Systems were required to detect the possible answers by themselves inside the document collections (Wikipedia and news corpus) <ref type="bibr" coords="2,170.28,166.27,9.99,9.96" target="#b8">[9]</ref>.</p><p>QA4MRE main task aims to test systems' ability to understand the meaning communicated by a text <ref type="bibr" coords="2,243.72,190.75,14.69,9.96" target="#b9">[10]</ref>. The task proposes a reading comprehension exercise where each question about a document will have five choices, from which systems will identify the correct answer. Despite the complexity of the process, with the need for a justification with the elements that support the answer and the need for textual inference, these task rules give rise to specialized approaches. Using the benefit of having an answer among five possibilities, the effort can be directed to assessing answer candidates. Such may be seen as a subtask of full QA, in the sense that it does not need an answer extraction phase.</p><p>The next section presents the main resources employed and the system architecture. The approach used in QA4MRE is described with examples in section 3. The obtained results are presented in section 4. Finally, some conclusions and future work are pointed out in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Resources and Architecture</head><p>The main system components and their interactions are presented in figure <ref type="figure" coords="2,472.80,396.43,3.90,9.96" target="#fig_0">1</ref>. The XML Layer is a component responsible for parsing the input, sorting out the questions with their multiple choice answers and maintaining a connection to their particular reading test document. When all questions are processed, this component generates the XML output and makes sure the syntax is correct and conforms to the DTD.</p><p>The Question Classifier module was thought to determine the type of the question, which is later considered in assessing each response. The Local KB has a starting knowledge base containing common sense facts about places, entities and events. Its content is important for example in Named Entity Recognition (NER) process.</p><p>The Libs Module contains collections of text documents, refered as Background Collections (BC). The English version for all three topics (AIDS; Climate Change; Music and Society) corresponds to approximately 2 gigabytes of text. This module also includes a Lucene<ref type="foot" coords="2,321.00,563.94,3.97,6.19" target="#foot_2">3</ref> based text search engine, used to index all BC text and subsequent document retrieval operations. Working with English written text, Wordnet <ref type="bibr" coords="2,260.33,588.79,14.47,9.96" target="#b1">[2]</ref> is a significant external tool for assisting in synonym check, term base form normalization or to find definitions. This resource is consulted through the Java API for WordNet Searching 4 . The Answer Analyzer is responsible for assess each answer choice for a question. This check is fundamentally based on superficial analysis of text, enriched with semantic validation of compatibility between terms. Besides the reading test document content, the system also examines BC documents that are retrieved from the question text and answer choice's text. Possibly relevant phrases are highlighted for further consideration when deciding between the various choices. With the information collected for each candidate answer to a question, the Answer Selector module applies a set of criteria to choose the most plausible answer. Next section explains how the system processes each QA4MRE question. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Although Machine Reading can be defined as the automatic understanding of text <ref type="bibr" coords="3,155.16,631.63,9.99,9.96" target="#b5">[6]</ref>, our participation in QA4MRE is not based on a deep linguistic processing. Our approach to identify the correct answer choice to a question comprises the following steps:</p><p>-Named Entity Recognition -prior identification of any entity names, dates, quantities or other expressions that influence question interpretation. -Question Classification -determine the category of the question in order to adopt specific procedures in the treatment of answers. The question: "How many orchestras were mentioned in the London Times? " is a Quantity subtype of Factoid category. -Document Retrieval -search for documents in Background Collections that can support one of the possible answers to the question. It uses the Lucene tool with a search expression according to the question category and each answer candidate. Search expression avoids stop words and can be expanded using synonyms or morphological normalization. -Passage Retrieval -to minimize text area where the more time consuming techniques must be applied, the retrieved documents and the reading test document are divided into text segments. -Answer Analysis -all possible answers are analyzed in the text segments.</p><p>For each of the multiple choices we intend to verify: A-Is there a textual answer pattern to this question category that is verified, in the current text segment, for this answer choice? B-If both question and answer key elements are present in the segment, what is the (minimal) distance between them? In both cases, the question key element to find in the text segments is the question focus. That is the entity or object that the question refers to, about which some information is to be determined. Question focus is identified with the category of the question. When question classification fails, the default procedure is to look for terms in the question text, filtering out stop words. The presence of those key elements on a text segment is based on term exact match, firstly, but also through semantic compatibility (synonym, hyperonym, base form). -Answer Selection -considering the cases A and B detected for each answer, the system will identify the most plausible answer. The decision is based on the following ordered criteria: 1. If there is one single answer that verified the case A, then that response is chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">If multiple answers verify the case A:</head><p>(a) If one of them has more occurrences of the case A, then that answer is chosen. (b) Otherwise, between the answers having the same number of occurrences of case A, the tie is resolved by the criteria that follow. 3. If there is one single answer that verified the case B, then that answer is chosen. 4. If multiple answers verify the case B and one (only) of them has the minimal distance observed in a text segment, then that answer is chosen. 5. If multiple answers verify the case B and one (only) of them has the minimum value for the medium distance observed in the segments where it had occurences, then that answer is chosen. 6. If none of the above applies, then the question remains unanswered.</p><p>The fifth criterion is the main difference between the two submitted runs. For the first run, it was used only to break ties resulting from the criterion 4, whereas in the last run it is applied more broadly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This methodology was applied to the 120 questions. Two full executions were completed and their results were submitted. For the second run, the chart in figure <ref type="figure" coords="5,164.04,231.19,4.98,9.96" target="#fig_1">2</ref> denotes the proportion of correct and incorrect answers on the one hand, and the amount unanswered questions on the other side. It makes clear that the approach leads to a large number of wrong answers. However, if we add the right answers to the unanswered questions we get more than the number of wrong answers. A more specific assessment in each of the runs can be found in table <ref type="bibr" coords="5,455.40,488.23,3.90,9.96" target="#b0">1</ref>. We can see that the system answered 74 questions in the first run. The remaining 46 questions were left unanswered without selecting candidate answer. In cases where response was submitted, only 15 were classified as correct. In the second run, one more question was left unanswered. The system was correct in 18 of the 73 responses submitted. We note that the system classification for the second run is more favorable, because while it increased the number of correct answers, it also decreased the number of erroneous results. This improvement is confirmed by the measure in the second column on the right of the table. The accuracy is calculated as the number of responses submitted and classified as correct divided by the number of questions. On run 02, accuracy is 18/120 = 0.15, which is better than the former.</p><p>C@1 is a balanced measure rewarding systems that, for the same number of correct answers, perform better over the remaing answers <ref type="bibr" coords="5,391.80,643.63,9.99,9.96" target="#b0">[1]</ref>. By leaving some questions unanswered, a system can decrease the number of incorrect results. This measure is calculated using the formula in equation 1. The C@1 value for each run is shown in the rightmost column of the table 1. Again, the system got the best result in the second run, with C@1 = (18+47(18/120))/120 = 0.21.</p><p>The next section has some conclusions about these results and considerations about our QA4MRE participation.</p><formula xml:id="formula_0" coords="6,205.92,299.82,274.71,27.25">C@1 = #correct + #unanswered * #correct #questions #questions (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We believe that the outcome of this task is not comparable with our previous work in QA@CLEF. The aim of this work is to automatically understand the meaning of each question and its response hypotheses in order to determine the answer.</p><p>We found that answer analysis with variant A did not contributed to any answer. This may be due to problems with the question classifier, who managed to correctly assign a category to only 9 questions and some of those still had problems in detecting the question focus. Thus, the criteria 3, 4, 5 and 6 turned out to be the dominant to the process of multiple choice answering.</p><p>Looking at the questions and despite our text surface based approach can be much improved, it seems that there is a limit beyond which only a deeper semantic analysis may reach the answers. This QA4MRE edition brought us two challenges: the absence of Portuguese as a working language and the different nature of the task and its objectives. Having a lack of time to implement a more robust solution, we consider that the results are satisfactory. Moreover, we believe this experiment constitutes a basis for a future semantically more advanced system, enabled to English, that can be tested in an upcoming participation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,251.04,540.52,113.30,8.66;3,158.38,256.65,298.37,259.23"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. System Architecture</figDesc><graphic coords="3,158.38,256.65,298.37,259.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,207.48,455.44,200.47,8.66;5,145.42,310.92,324.00,120.00"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Evaluation at QA level for the second run</figDesc><graphic coords="5,145.42,310.92,324.00,120.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,142.20,117.64,330.98,65.54"><head>Table 1 .</head><label>1</label><figDesc>QA level evaluation for each run</figDesc><table coords="6,142.20,117.64,330.98,44.66"><row><cell></cell><cell></cell><cell></cell><cell>unanswered</cell><cell></cell><cell></cell><cell>answered</cell><cell></cell><cell>all</cell><cell></cell></row><row><cell cols="10">Run # Right Wrong Empty # Right Wrong Accuracy C@1</cell></row><row><cell>01</cell><cell>46</cell><cell>0</cell><cell>0</cell><cell>46</cell><cell>74</cell><cell>15</cell><cell>59</cell><cell>0.13</cell><cell>0.17</cell></row><row><cell>02</cell><cell>47</cell><cell>0</cell><cell>0</cell><cell>47</cell><cell>73</cell><cell>18</cell><cell>55</cell><cell>0.15</cell><cell>0.21</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.72,644.80,123.76,8.66"><p>http://celct.fbk.eu/QA4MRE/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,144.72,654.28,335.79,10.94"><p>University of Évora previous work at CLEF: 2004<ref type="bibr" coords="1,345.36,656.56,9.12,8.66" target="#b6">[7]</ref>, 2005<ref type="bibr" coords="1,381.36,656.56,9.12,8.66" target="#b7">[8]</ref>, 2007<ref type="bibr" coords="1,417.36,656.56,9.64,8.66" target="#b2">[3]</ref> and 2008<ref type="bibr" coords="1,468.36,656.56,9.12,8.66" target="#b3">[4]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.72,634.72,335.80,8.66;2,144.72,645.64,140.92,8.66"><p>Apache Lucene is an open source project with advanced indexing and searching features. http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,144.72,656.56,321.03,8.66"><p>Java API for WordNet Searching: http://lyle.smu.edu/˜tspell/jaws/index.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.32,623.68,341.86,8.66;6,142.32,634.72,338.23,8.66;6,142.32,645.64,338.32,8.66;6,142.32,656.56,74.32,8.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,303.36,623.68,172.51,8.26">A Simple Measure to Assess Non-response</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alvaro</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,142.32,634.72,338.23,8.66;6,142.32,645.64,165.31,8.66">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.32,119.44,342.30,8.66;7,142.32,130.48,52.78,8.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,221.04,119.44,157.92,8.26">Wordnet: A lexical database for English</title>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,387.00,119.44,93.62,8.66;7,142.32,130.48,18.21,8.66">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.32,141.40,342.23,8.66;7,142.32,152.32,338.06,8.66;7,142.32,163.36,322.36,8.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,286.32,141.40,194.23,8.66;7,142.32,152.32,82.76,8.66">The senso question answering approach to portuguese qa@clef-2007</title>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Saias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Quaresma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,303.60,152.32,176.78,8.66;7,142.32,163.36,113.83,8.66">CLEF 2007 Working Notes, Cross-Language Evaluation Forum Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="7,138.32,174.28,342.22,8.66;7,142.32,182.92,338.23,10.94;7,142.32,196.24,288.04,8.66" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,284.04,174.28,196.50,8.66;7,142.32,185.20,16.64,8.66">The senso question answering system at qa@clef 2008</title>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Saias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Quaresma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,324.97,182.92,155.58,10.94;7,142.32,196.24,162.87,8.66">Multiple Language Question Answering @ Cross-Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Universidade de Évora</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="7,138.32,207.16,342.25,8.66;7,142.32,218.08,320.20,8.66" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Saias</surname></persName>
		</author>
		<idno>hdl.handle.net/10174/2505</idno>
		<title level="m" coord="7,194.64,207.16,285.92,8.26;7,142.32,218.08,122.54,8.26">Contextualização e Activação Semântica na Selecção de Resultados em Sistemas de Pergunta-Resposta</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Phd Thesis</note>
</biblStruct>

<biblStruct coords="7,138.32,229.12,342.23,8.66;7,142.32,240.04,178.66,8.66" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,233.28,229.12,200.36,8.26">Answering and Questioning for Machine Reading</title>
		<author>
			<persName coords=""><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,441.96,229.12,38.59,8.66;7,142.32,240.04,146.34,8.66">American Association for Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.32,250.96,342.08,8.66;7,142.32,259.72,338.20,10.94;7,142.32,272.92,25.42,8.66" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,142.32,259.72,216.88,10.54">The University of Évora approach to QA@CLEF-2004</title>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Quaresma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Quintano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Irene</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Saias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pedro</forename><surname>Salgueiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,368.04,262.00,108.25,8.66">CLEF 2004 Working Notes</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.32,283.84,342.30,8.66;7,142.32,294.88,226.90,8.66" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,308.76,283.84,171.86,8.26;7,142.32,294.88,79.15,8.26">A Logic Programming Based Approach To QA@CLEF05 Track</title>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Quaresma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Irene</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,228.96,294.88,107.53,8.66">CLEF 2005 Working Notes</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.32,305.80,342.11,8.66;7,142.32,316.72,308.58,8.66" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,146.88,305.80,257.17,8.66">Guidelines for the participants in QA@CLEF</title>
		<ptr target="http://clef-qa.fbk.eu/2008/download/QA@CLEF08Guidelines-for-Participantsnew.pdf" />
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
	<note>QA@CLEF</note>
</biblStruct>

<biblStruct coords="7,142.54,327.64,304.74,8.66" xml:id="b9">
	<monogr>
		<ptr target="http://celct.fbk.eu/QA4MRE/" />
		<title level="m" coord="7,151.56,327.64,163.73,8.66">Track Guidelines</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>QA4MRE@CLEF</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
