<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,171.60,203.77,252.00,12.64;1,240.96,223.81,113.65,12.64">Question Answering for Machine Reading with Lexical Chain</title>
				<funder ref="#_fAW5rK2 #_mV7YS9C">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_QMamZQw">
					<orgName type="full">Shanghai Committee of Science and Technology</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,215.76,261.88,35.29,8.96"><forename type="first">Ling</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Technology Fudan University</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.24,261.88,46.86,8.96"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<email>xpqiu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Technology Fudan University</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.60,261.88,66.30,8.96"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<email>xjhuang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Technology Fudan University</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,171.60,203.77,252.00,12.64;1,240.96,223.81,113.65,12.64">Question Answering for Machine Reading with Lexical Chain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A2A4F15E004B9C9376CA53F3BC2D89F2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering</term>
					<term>Machine Reading</term>
					<term>Lexical Chain</term>
					<term>WordNet</term>
					<term>Natural Language Processing 1</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Question answering for machine reading (QA4MR) is a task to understand the meaning communicated by a text. In this paper, we present our system in QA4MRE 1 . The system follows the steps of reading comprehension as a language learner. Lexical chain is used to estimate the semantic relation between texts. Natural language processing (NLP) techniques are also widely used, such as: POS tagging, name entity recognition, coreference. On the QA4MRE test dataset, our system achieves the c@1 measure of 0.28 and 0.26 for the two submissions, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Machine Reading <ref type="bibr" coords="1,201.38,489.16,11.69,8.96" target="#b0">[1]</ref> is the automatic, unsupervised understanding of texts, which builds a bridge between natural language and knowledge understandable by machines. The Machine Reading task focus on the deep understanding of small number of texts, which is different from text mining <ref type="bibr" coords="1,291.13,523.73,10.68,8.96" target="#b1">[2]</ref>, where the system reads and extract knowledge from hundreds or thousands of texts.</p><p>Question answering for machine reading (QA4MR) <ref type="bibr" coords="1,362.65,546.66,16.75,8.96" target="#b11">[12]</ref> is a task to answer questions by reading of single documents. To understand the meaning of a text on semantic level, system should identify a set of multiple choices related to it, where correct answers require inference in all kinds, i.e., lexical (acronymy, synonymy, hyperonymy), syntactic (nominalization / verbalization, causative, paraphrase, active/passive), discourse (coreference, anaphora ellipsis) <ref type="bibr" coords="1,357.74,604.28,10.68,8.96" target="#b2">[3]</ref>.</p><p>Here is an example for QA4MRE task: Text: Annie Lennox: Why I am an HIVAIDS activist, I'm going to share with you the story as to how I have become an HIV/AIDS campaigner. And this is the name of my campaign, SING Campaign.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>Who is the founder of the SING campaign? Candidate Answers:</p><formula xml:id="formula_0" coords="2,134.74,195.65,76.63,54.94">1) Nelson Mandela 2)Youssou N'Dour 3)Michel Sidibe 4)Zackie Archmat 5)Annie Lennox</formula><p>By machine reading, the answer "Annie Lennox" could be chosen.</p><p>In this paper, we propose a method for question answering for machine reading system with lexical chain. Our system is similar to the scenario of humans learning a new language and dealing with reading comprehension tests. Humans always do reading comprehensions in 3 steps:</p><p>1. Locating: Reading the question and extracting sentences from the passage which may related to the question. 2. Answering: Reading these sentences in details to select which sentences are most likely to be the answer. 3. Choosing: Reading all the choices and choosing the one has the same meaning as the answer. So our system explores the possibilities of following the above steps as a language learner doing Reading Comprehensions. Lexical chain <ref type="bibr" coords="2,368.64,391.16,10.68,8.96" target="#b5">[6]</ref>, proposed by LCC, performs well in finding topic relations between words based on WordNet. Our system uses lexical chain to estimate the semantic relation between texts.</p><p>The rest of the paper is organized in the following way: Section 2 is to present a brief overview of related works. Section 3 provides the architecture of our system. Section 4 introduced lexical chain in details. We present our experiments and results in section 5, and conclude in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>QA4MR <ref type="bibr" coords="2,164.52,527.20,16.75,8.96" target="#b11">[12]</ref> is related to some topics in the fields of information retrieval and natural language processing (NLP), such as question answering <ref type="bibr" coords="2,414.71,538.72,15.43,8.96" target="#b10">[11]</ref>, reading comprehension <ref type="bibr" coords="2,188.26,550.13,11.71,8.96" target="#b8">[9]</ref> and recognizing textual entailment <ref type="bibr" coords="2,343.53,550.13,10.68,8.96" target="#b6">[7]</ref>.</p><p>Question answering (QA) <ref type="bibr" coords="2,246.19,561.65,11.71,8.96" target="#b7">[8]</ref> is the task of automatically answering a question posed in natural language. In contrast to QA4MR, QA systems are designed to extract answers in large corpus. QA systems seldom focus on the deeply understanding of corpus. What's more, QA systems tend to answer every question as they can even though they might not confident about the correctness of the answers.</p><p>Reading comprehension (RC) <ref type="bibr" coords="2,260.60,619.15,11.70,8.96" target="#b8">[9]</ref> system attempts to understand a document and returns an answer sentence when posed with a question. RC resembles the ad hoc question answering (QA) task that aims to extract an answer from a collection of documents when posed with a question. However, since RC focuses only on a single document, the system needs to draw upon external knowledge sources to achieve deep analysis of passage sentences for answer sentence extraction.</p><p>Recognizing Textual Entailment (RTE) <ref type="bibr" coords="3,296.40,149.56,11.71,8.96" target="#b6">[7]</ref> has been proposed recently as a generic task that captures major semantic inference needs across many NLP applications. This task requires to recognize, given two text fragments, whether the meaning of one text is entailed (can be inferred) from the other text. Semantic understanding and logic understanding of text is indispensable for RTE system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Overview</head><p>The framework of our QA4MR system is presented in Fig. <ref type="figure" coords="3,362.29,262.72,3.76,8.96" target="#fig_0">1</ref>. In our system, text (including passage, question and choices) is sent to preprocessing module initially. (1) Sentence segmentation for each passage.</p><p>(2) Tokenization for each sentence, question and choice.</p><p>(3) POS and parsing.</p><p>(4) Named entities recognition.</p><p>(5) Coreference annotation for each pronominal or abbreviate expression. Our system uses Illinois coreference package<ref type="foot" coords="3,315.96,473.17,3.24,5.83" target="#foot_1">2</ref>  <ref type="bibr" coords="3,324.24,475.36,11.71,8.96" target="#b3">[4]</ref> to preprocess text. After preprocessing, annotated text is separated into 3 pieces: passage, question and choices. The passage and question are then sent to locating module. As described in section 1, locating module extracts sentences from passage which are related to the question.</p><p>While doing reading comprehension, language learners always locate sentences in a passage by the name entities mentioned in the question, such as a name of a person, a special place or an organization. For instance, in test set of QA4MRE task <ref type="bibr" coords="3,456.47,555.90,10.68,8.96" target="#b2">[3]</ref>, passage1, question 2:</p><p>Who is the founder of the SING campaign? To answer this question, language learners will focus on the organization "SING campaign". While reading, they will only pay attention to sentences talking about "SING campaign" to find the answer and neglect the others. So in locating module, system extracted all these sentences such as:</p><p>And this is the name of my campaign, SING Campaign. And yes, my SING Campaign has supported Treatment Action Campaign in the way that I have tried to raise awareness and to try to also raise funds. SING Campaign is basically just me and about three or four wonderful people who help to support me.</p><p>It is worth to say that we can find out that the founder of SING campaign is the author of the passage (Annie Lennox) only in the sentences above. If without the recognition of name entities, the following sentence might mislead the answer of this question semantically, which is talking about the foundation of a campaign, but not SING campaign: I was very very fortunate, a couple of years later, to have met Zackie Achmat, the founder of Treatment Action Campaign, an incredible campaigner and activist.</p><p>Sentences extracted by locating module are then submitted to answering module, with the question. Answering module, as its name implies, gives the answer to the question. Instead of giving an answer directly, answering module reads every sentence from input and select which one is most likely to be the answer. The method of the selection is by lexical chain. Details of lexical chain are presented in section 3.</p><p>In the last step, answer sentences extracted by answering module are submitted to choosing module with the choices of the question. Choosing module gives the final result by name entity matching and lexical chain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Lexical Chain</head><p>As is mention in previous, lexical chain <ref type="bibr" coords="4,286.32,446.68,11.71,8.96" target="#b5">[6]</ref> is used in answering module and choosing module to estimate semantic relation between texts. WordNet <ref type="bibr" coords="4,397.19,458.20,11.71,8.96" target="#b4">[5]</ref> [10] relations including: Hypernym, Hyponym, Synonym, Meronym, Holonym, Attribute, Cause, Entailment. In addition, to build connection between synsets of different POS, gloss relation, defined by 6 is used.</p><p>Following lexical chain <ref type="bibr" coords="4,239.14,504.18,10.59,8.96" target="#b5">[6]</ref>, which is put forward by LCC, system scores the semantic relation between words by the total score of each relation path. Equation (1) shows how system scores words: Relation( , ) Score( ) (1)</p><formula xml:id="formula_1" coords="4,247.60,543.35,73.57,19.41">i j i i s s r = ∑</formula><p>where:</p><formula xml:id="formula_2" coords="4,234.32,586.89,146.02,26.25">( ) 1 ( ) ( * ) (<label>2</label></formula><formula xml:id="formula_3" coords="4,211.06,586.89,173.23,26.26">) i i length r R C i Score r I W MG = = × ∏</formula><p>where: I is the initial score. WRi is the weight of the relation presented in table <ref type="table" coords="4,331.31,639.64,3.76,8.96" target="#tab_0">1</ref>.</p><p>MGci can be calculated by equation (3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONST MG</head><p>CONST N - = + where Nr-gloss is the total amount of r-gloss relation of a word (r-gloss relation is reverse to gloss relation). Semantic relation between texts can be estimated by the score of their key words. For example, in passage 1, question 5:</p><p>What is Annie Lennox's profession? Sentences are extracted by locating module, among them, following two sentences are most likely to be the answer:</p><p>……because I am a woman, and I am a mother…… ……talking and using my platform as a musician, with my commitment to Mandela...... With WordNet gloss relation, system finds out that musician is a kind of profession. As the gloss of musician#2 is "artist who composes or conducts music as a profession". While mother is not a kind of profession.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results</head><p>The evaluation measure of QA4MR task is C@13. This measure rewards systems that, while maintaining the number of correct answers are able to reduce the incorrect ones by leaving some questions unanswered. C@1 measure is presented in Equation ( <ref type="formula" coords="5,286.46,569.09,3.63,8.96">4</ref> nr: is the number of correctly answered questions nu: is the number of unanswered questions n: is the total number of questions</p><p>We evaluates 2 runs on the dataset of QA4MRE 2011 <ref type="bibr" coords="6,347.57,184.13,12.90,8.96" target="#b2">[3]</ref>. The significant difference between two runs is the processing of locating module. The first run keeps the question unanswered if system failed in locating, that is, failed to recognize any name entity in the question, while the second run send all sentences in the passage to answering module. And also, the length of the lexical chain in the second run is less than the first run by 1.  Table <ref type="table" coords="7,150.84,251.19,4.98,8.96" target="#tab_1">2</ref> shows the overall C@1 measure of our two results, comparing with the best run, the worst run and the average C@1 measure for QA4MRE. Table <ref type="table" coords="7,423.11,262.72,4.98,8.96" target="#tab_2">3</ref> illustrates more details about the C@1 measure of each topic. Table <ref type="table" coords="7,365.64,274.24,3.76,8.96" target="#tab_3">4</ref>, Fig. <ref type="figure" coords="7,395.63,274.24,4.98,8.96">2</ref> and Fig. <ref type="figure" coords="7,440.97,274.24,4.98,8.96">3</ref> show the result of both run at question-answering level. According to Fig. <ref type="figure" coords="7,415.81,285.65,4.98,8.96">2</ref> and Fig. <ref type="figure" coords="7,463.09,285.65,3.76,8.96">3</ref>, system does not performance well without the locating of name entities. Notice that 30 questions are not answered in both runs, for two reasons. One is that current approach gives up all questions about date, time and digit such as: For how long did people applaud at performances of the Bolivar Youth Orchestra? The other is our current approach do not use background collection, which leads to the failure in answering questions such as: What is Nelson Mandela's country of origin? ( post-apartheid Rainbow Nation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Future Works</head><p>This paper introduced the architecture of our system in QA4MRE task. System chooses correct answers by name entity locating and lexical chain. Experiments revealed that name entity locating reduce the rate of wrong answers misleaded by lexical relation, and lexical chain estimate the semantic relation of texts by scoring the lexical relation of words.</p><p>Our current approach gets an overall c@1 measure as 0.28. In the future, we will have two further works to do. One is developing a module recognizing date, time and digit. The other is trying to use world knowledge in our system, including background collection, and web-based resources such as Wikipedia.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,245.76,393.45,103.96,8.10;3,124.80,406.36,172.07,8.96;3,142.80,417.88,180.93,8.96;3,142.80,429.40,229.54,8.96;3,142.81,440.93,87.57,8.96;3,142.82,452.45,128.73,8.96;3,142.83,463.86,297.81,8.96;3,134.79,475.38,180.60,8.96;3,315.96,473.17,3.24,5.83;3,324.24,475.36,87.07,8.96;3,134.75,486.88,335.70,8.96;3,124.79,498.40,345.89,8.96;3,124.78,509.93,345.93,8.96;3,124.78,521.45,36.20,8.96;3,134.74,532.86,335.75,8.96;3,124.78,544.38,345.70,8.96;3,124.78,555.90,345.93,8.96;3,124.78,567.43,85.22,8.96;3,216.46,578.95,172.49,8.96;3,134.74,590.36,335.71,8.96;3,124.78,601.88,345.76,8.96;3,124.78,613.40,345.68,8.96;3,124.77,624.93,177.62,8.96;3,191.21,295.61,213.00,92.40"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. System architecture Preprocessing module do following works:(1) Sentence segmentation for each passage.(2) Tokenization for each sentence, question and choice.(3) POS and parsing.(4) Named entities recognition.(5) Coreference annotation for each pronominal or abbreviate expression. Our system uses Illinois coreference package 2<ref type="bibr" coords="3,324.24,475.36,11.71,8.96" target="#b3">[4]</ref> to preprocess text. After preprocessing, annotated text is separated into 3 pieces: passage, question and choices. The passage and question are then sent to locating module. As described in section 1, locating module extracts sentences from passage which are related to the question.While doing reading comprehension, language learners always locate sentences in a passage by the name entities mentioned in the question, such as a name of a person, a special place or an organization. For instance, in test set of QA4MRE task<ref type="bibr" coords="3,456.47,555.90,10.68,8.96" target="#b2">[3]</ref>, passage1, question 2:Who is the founder of the SING campaign? To answer this question, language learners will focus on the organization "SING campaign". While reading, they will only pay attention to sentences talking about "SING campaign" to find the answer and neglect the others. So in locating module, system extracted all these sentences such as:</figDesc><graphic coords="3,191.21,295.61,213.00,92.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,290.09,569.09,7.26,8.96;5,251.80,583.53,3.41,9.23;5,286.01,583.53,38.26,9.23;5,226.85,589.99,14.87,9.23;5,364.98,589.99,11.88,9.23;5,255.47,583.53,61.87,9.23;5,218.38,589.99,6.82,9.23;5,285.53,598.00,5.12,9.23;5,266.83,583.36,5.62,9.44;5,243.16,589.82,5.62,9.44;5,124.80,614.07,27.13,8.96;5,124.80,637.73,3.04,5.63;5,132.36,640.59,315.21,7.24;5,124.80,650.19,321.54,7.24;5,124.80,659.43,335.12,7.24;5,124.80,668.66,308.86,7.24;5,124.80,677.78,321.67,7.24;5,124.79,687.02,292.40,7.24"><head>3</head><label></label><figDesc>Anselmo Peñas, Pamela Forner, Richard Sutcliffe, Alvaro Rodrigo, Corina Forascu, Iñaki Alegria, Danilo Giampiccolo, Nicolas Moreau and Petya Osenova. Overview of ResPubliQA 2009: Question Answering Evaluation over European Legislation. In C. Peters, G. Di Nunzio, M. Kurimo, Th Mandl, D. Mostefa, A. Peñas, G. Roda. (Eds) Multilingual Information Access Evaluation I. Text Retrieval Experiments, 10th Workshop of the Cross-Language Evaluation Forum, CLEF 2009, Corfu, Greece, Revised Selected Papers. Lecture Notes in Computer Science 6241. Springer-Verlag, 2010.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,240.24,365.01,115.06,8.10;6,180.82,268.77,234.10,89.55"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Result of submission 1</figDesc><graphic coords="6,180.82,268.77,234.10,89.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,183.36,152.25,228.46,154.93"><head>Table 1 .</head><label>1</label><figDesc>Weight of WordNet relations</figDesc><table coords="5,183.36,174.21,228.46,132.97"><row><cell cols="4">WordNet Relation Weight WordNet Relation Weight</cell></row><row><cell>Hypernym</cell><cell>0.8</cell><cell>Attribute</cell><cell>0.5</cell></row><row><cell>Hyponym</cell><cell>0.7</cell><cell>Cause</cell><cell>0.5</cell></row><row><cell>Synonym</cell><cell>0.9</cell><cell>Entailment</cell><cell>0.7</cell></row><row><cell>Meronym</cell><cell>0.5</cell><cell>Gloss</cell><cell>0.6</cell></row><row><cell>Holonym</cell><cell>0.5</cell><cell>R-Gloss</cell><cell>0.2</cell></row><row><cell>C</cell><cell></cell><cell>(3)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>R Gloss</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,205.80,533.01,183.51,46.14"><head>Table 2 .</head><label>2</label><figDesc>Overall C@1 measure</figDesc><table coords="6,205.80,554.97,183.51,24.18"><row><cell></cell><cell cols="5">Run 1 Run 2 Best Worst Average</cell></row><row><cell>C@1</cell><cell>0.28</cell><cell>0.26</cell><cell>0.57</cell><cell>0.02</cell><cell>0.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,231.61,599.73,132.17,77.46"><head>Table 3 .</head><label>3</label><figDesc>C@1 measure by topics</figDesc><table coords="6,231.61,621.81,132.17,55.38"><row><cell>Topics</cell><cell cols="2">Run 1 Run 2</cell></row><row><cell>AIDS</cell><cell>0.28</cell><cell>0.26</cell></row><row><cell>Climate Change</cell><cell>0.19</cell><cell>0.17</cell></row><row><cell>Music and Society</cell><cell>0.36</cell><cell>0.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,210.96,152.25,173.40,77.46"><head>Table 4 .</head><label>4</label><figDesc>Evaluation at question-answering level</figDesc><table coords="7,229.92,174.21,135.43,55.50"><row><cell></cell><cell cols="2">Run 1 Run 2</cell></row><row><cell>Correctly answered</cell><cell>22</cell><cell>25</cell></row><row><cell>Wrongly answered</cell><cell>38</cell><cell>65</cell></row><row><cell>unanswered</cell><cell>60</cell><cell>30</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,132.36,686.35,107.05,8.45"><p>http://celct.fbk.eu/QA4MRE/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,133.56,673.83,171.78,7.24"><p>http://cogcomp.cs.illinois.edu/page/software_view/18</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was (partially) funded by <rs type="funder">NSFC</rs> (No. <rs type="grantNumber">61003091</rs> and No. <rs type="grantNumber">61073069</rs>) and <rs type="funder">Shanghai Committee of Science and Technology</rs> (No. <rs type="grantNumber">10511500703</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fAW5rK2">
					<idno type="grant-number">61003091</idno>
				</org>
				<org type="funding" xml:id="_mV7YS9C">
					<idno type="grant-number">61073069</idno>
				</org>
				<org type="funding" xml:id="_QMamZQw">
					<idno type="grant-number">10511500703</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,128.21,181.77,342.53,8.10;8,145.81,197.37,217.05,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,300.98,181.77,61.00,8.10">Machine reading</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cafarella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,380.65,181.77,90.08,8.10;8,145.81,197.37,194.86,8.10">Proceedings of the 21th National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 21th National Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.22,212.97,342.38,8.10;8,145.82,228.56,311.85,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,200.05,212.97,187.26,8.10">Text Mining: The state of the art and the challenges</title>
		<author>
			<persName coords=""><forename type="first">Ah-Hwee</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,402.99,212.97,67.61,8.10;8,145.82,228.56,285.31,8.10">Proceedings of the PAKDD 1999 Workshop on Knowledge Disocovery from Advanced Databases</title>
		<meeting>the PAKDD 1999 Workshop on Knowledge Disocovery from Advanced Databases</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.22,244.16,342.54,8.10;8,145.82,259.76,105.47,8.10" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,145.82,244.16,292.90,8.10">Question Answering for Machine Reading Evaluation at CLEF</title>
		<ptr target="http://celct.fbk.eu/QA4MRE/" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.23,275.36,342.38,8.10;8,145.83,290.95,141.09,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,266.56,275.36,204.06,8.10;8,145.83,290.95,37.62,8.10">Understanding the Value of Features for Coreference Resolution</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Bengtson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,199.12,290.95,60.27,8.10">proc. EMNLP-08</title>
		<meeting>EMNLP-08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.24,306.55,342.41,8.10;8,145.84,322.15,59.24,8.10" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,265.25,306.55,156.39,8.10">WordNet : An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.25,337.74,342.54,8.10;8,145.85,353.34,181.66,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,315.19,337.74,141.15,8.10">Lexical chains for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Novischi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,145.85,353.34,86.98,8.10">Proceedings of COLING</title>
		<meeting>COLING<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-08">2002. August</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.26,368.94,342.36,8.10;8,145.86,384.53,324.90,8.10;8,145.86,400.13,324.77,8.10;8,145.86,415.73,34.90,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,430.48,368.94,40.14,8.10;8,145.86,384.53,216.43,8.10">The Third PASCAL Recognizing Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,390.91,384.53,79.85,8.10;8,145.86,400.13,260.30,8.10">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.27,431.33,342.52,8.10;8,145.87,446.92,200.43,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,221.23,431.33,209.48,8.10">Overview of the TREC 2002 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,145.87,446.92,200.43,8.10">Proceedings of the Eleventh Text REtrieval Conference</title>
		<meeting>the Eleventh Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.28,462.52,342.36,8.10;8,145.88,478.12,325.03,8.10;8,145.88,493.71,315.32,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,386.57,462.52,84.07,8.10;8,145.88,478.12,80.98,8.10">Deep Read: a reading comprehension system</title>
		<author>
			<persName coords=""><forename type="first">Lynette</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,243.20,478.12,227.71,8.10;8,145.88,493.71,203.85,8.10">Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</title>
		<meeting>the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1999">June 20-26, 1999</date>
			<biblScope unit="page" from="325" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.47,509.31,338.40,8.10;8,145.88,524.91,120.09,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,239.73,509.31,153.44,8.10">WordNet : A Lexical Database for English</title>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,399.33,509.31,71.54,8.10;8,145.88,524.91,33.69,8.10">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.46,540.50,338.33,8.10;8,145.87,556.10,324.78,8.10;8,145.87,571.70,302.02,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,385.03,540.50,85.76,8.10;8,145.87,556.10,74.91,8.10">Question-answering by predictive annotation</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anni</forename><surname>Coden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,236.35,556.10,234.31,8.10;8,145.87,571.70,198.66,8.10">Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.46,587.30,338.13,8.10;8,145.87,602.89,185.84,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,228.56,587.30,196.63,8.10">Answering and Questioning for Machine Reading</title>
		<author>
			<persName coords=""><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,435.26,587.30,35.33,8.10;8,145.87,602.89,134.04,8.10">American Association for Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2007-03">March 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
