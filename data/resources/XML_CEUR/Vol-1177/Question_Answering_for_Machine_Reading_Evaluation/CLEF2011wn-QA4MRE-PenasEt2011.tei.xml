<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,127.56,74.05,375.71,12.72;1,210.60,90.13,174.36,12.72">Overview of QA4MRE at CLEF 2011: Question Answering for Machine Reading Evaluation</title>
				<funder ref="#_Quf6hZC">
					<orgName type="full">Research Network MA2VICMR</orgName>
				</funder>
				<funder ref="#_whg2HEp">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,108.72,129.01,61.83,11.04"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
						</author>
						<author>
							<persName coords="1,178.80,129.01,53.70,11.04"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@isi.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,240.72,129.01,56.89,11.04"><forename type="first">Pamela</forename><surname>Forner</surname></persName>
							<email>forner@celct.it</email>
							<affiliation key="aff2">
								<orgName type="institution">CELCT</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.72,129.01,63.06,11.04"><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
							<email>alvarory@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">NLP&amp;IR group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.12,129.01,68.38,11.04"><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
							<email>richard.sutcliffe@ul.ie</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,454.92,129.01,61.38,11.04"><forename type="first">Corina</forename><surname>Forascu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Al. I</orgName>
								<orgName type="institution">Cuza University of Iasi</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.04,140.53,75.79,11.04"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
							<email>csporled@coli.uni-sb.de</email>
							<affiliation key="aff5">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,127.56,74.05,375.71,12.72;1,210.60,90.13,174.36,12.72">Overview of QA4MRE at CLEF 2011: Question Answering for Machine Reading Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">10A7CC636FAF521F6FFDDA3C1B4F023E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the first steps towards developing a methodology for testing and evaluating the performance of Machine Reading systems through Question Answering and Reading Comprehension Tests. This was the attempt of the QA4MRE challenge which was run as a Lab at CLEF 2011. This year a major innovation was introduced, as the traditional QA task was replaced by a new Machine Reading task whose intention was to ask questions which required a deep knowledge of individual short texts and in which systems were required to choose one answer, by analysing the corresponding test document in conjunction with the background collections provided by the organization. Beside the main task, also one pilot task was offered, namely, Processing Modality and Negation for Machine Reading. This task was aimed at evaluating whether systems were able to understand extra-propositional aspects of meaning like modality and negation. This paper describes the preparation of the data sets, the creation of the background collections to allow systems to acquire the required knowledge, the metric used for the evaluation of the systems' submissions, and the results of this first attempt. Twelve groups participated in the task submitting a total of 62 runs in three languages: English, German and Romanian.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Machine Reading (MR) is defined as a task that deals with the automatic understanding of texts. The evaluation of this "automatic understanding" can be approached in two ways: the first one is to define a formal language (target ontology), ask the systems to translate texts into the formal language representation, and then evaluate systems by using structured queries formulated in the formal language. The second approach is agnostic with any particular representation of the text. Systems are inquired about the text with natural language questions. The first option is approached by Information Extraction. The second is related to how Question Answering (QA) is being articulated during the last decade. In this evaluation we follow the second approach but with a significant change with respect to previous QA campaigns. Why?</p><p>By 2005 we realized that there was an upper bound of 60% of accuracy in systems performance, despite more than 80% of the questions were answered by at least one participant. We understood that we had a problem of error propagation in the traditional QA pipeline (Question Analysis, Retrieval, Answer Extraction, Answer Selection/Validation). Thus, in 2006 we proposed a pilot task called Answer Validation Exercise (AVE). The aim was to produce a change in QA architectures giving more responsibility to the validation step. In AVE we assumed there was a previous step of hypothesis over-generation and the hard work was in the validation step. This is a kind of classification task that could take advantage of Machine Learning. The same idea is behind the architecture of IBM's Watson (DeepQA project) that successfully participated at Jeopardy (Ferrucci et al., 2010).</p><p>After the three editions of AVE we tried to transfer our conclusions to the main QA task at CLEF 2009 and 2010. The first step was to introduce the option of leaving questions unanswered. This is related to the development of validation technologies. We needed a measure able to reward systems that reduce the number of questions answered incorrectly without affecting systems accuracy, by leaving unanswered the questions they estimated they couldn't answer. The measure was an extension of accuracy called c@1 (Peñas and Rodrigo,  2011), tested during 2009 and 2010 QA campaigns at CLEF, and used also in the current evaluation.</p><p>However, this change wasn't enough. Almost all systems continued using IR engines to retrieve relevant passages and then try to extract the exact answer from that. This is not the change in the architecture we expected, and again, results didn't go beyond the 60% pipeline upper bound. Finally, we understood that the change in the architecture requires a previous development of answer validation/selection technologies. For this reason, in the current formulation of the task, the step of retrieval is put aside for a while, focusing on the development of technologies able to work with a single document, and answer questions about it.</p><p>The idea of hypothesis generation and validation architecture is applicable to the new setting were only one document is considered, but of course the generation of hypotheses would be very limited if one only considers the given document. Systems should consider a large collection related to the given document in the task of hypothesis generation. Then, the validation must be performed according to the given document.</p><p>In the new setting, we started again decompounding the problem into generation and validation. Thus, in this first edition, we will test the systems only for the validation step. Together with the questions the organization provides a set of candidate answers. Besides, in this first edition, systems know there is one and only one correct answer among the candidates. This gives the evaluation the format of traditional Multiple Choice Reading Comprehension tests. From this starting point, a natural roadmap could be the following:</p><p>1. Focus on validation: Questions have attached a set of candidate answers. a.</p><p>Step 1. All questions have one and only one correct candidate answer. b.</p><p>Step 2. Introduce questions that require inference (e.g. about time and space). c.</p><p>Step 3. Introduce questions with no correct candidate answer. d.</p><p>Step 4. Introduce questions that require textual inference after reading a large set of documents related to the test (e.g. expected actions of agents with a particular role, etc.) 2. Introduce hypothesis generation: Organization provides reference collections of documents related to the tests. a.</p><p>Step 5. Questions about a single document, but no candidate answers are provided. b. Step 6. Full setting of QA were systems have to generate hypothesis considering the reference collection and provide the answer together with the set of documents that support the answer. We are just at the beginning of this roadmap, giving space and resources for the evaluation of new QA systems with new architectures. The success of this new initiative is only measurable by the development of these new architectures able to produce a qualitative jump in performance. This vision will guide the concrete definition of the tasks year by year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TASK DESCRIPTION</head><p>The QA4MRE 2011 task focuses on the reading of single documents and the identification of the answers to a set of questions. Questions are in the form of multiple choice, each having five options, and only one correct answer. The detection of correct answers might require eventually various kinds of inference and the consideration of previously acquired background knowledge from reference document collections. Although the additional knowledge obtained through the background collection may be used to assist with answering the questions, the principal answer is to be found among the facts contained in the test documents given. Thus, reading comprehension tests do not require only semantic understanding but they assume a cognitive process which involves using implications and presuppositions, retrieving the stored information, performing inferences to make implicit information explicit. Many different forms of knowledge take part in this process: linguistic, procedural, world-and-common-sense knowledge. All these forms coalesce in the memory of the reader and it is sometimes difficult to clearly distinguish and reconstruct them in a system which needs additional knowledge and inference rules in order to understand the text and to give sensitive answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Main Task</head><p>By giving only a single document per test, systems are required to understand every statement and to form connections across statement in case the answer is spread over more than one sentence. Systems are requested to (i) understand the test questions, (ii) analyze the relation among entities contained in questions and entities expressed by the candidate answers, (iii) understand the information contained in the documents, (iv) extract useful pieces of knowledge from the background collections, (v) and select the correct answer from the five alternatives proposed.</p><p>Tests were divided into: -3 topics, namely "Aids", "Climate change" and "Music and Society" -Each topic had 4 reading test -Each reading test consisted of one single document, with 10 questions and a set of five choices per question. In global, the evaluation had in this campaign -12 test documents (4 documents for each of the three topics) -120 questions (10 questions for each document) with -600 choices/options (5 for each question) Test documents and questions were made available in English, German, Italian, Romanian, and Spanish. These materials were exactly the same in all languages, created using parallel translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pilot Exercises</head><p>Beside the main task, also one pilot task was offered this year at QA4MRE; i.e. Processing Modality and Negation for Machine Reading [11]. It was coordinated by CLiPS, a research center associated with the University of Antwerp, Belgium. The task was aimed at evaluating whether systems are able to understand extra-propositional aspects of meaning like modality and negation. Modality is a grammatical category that allows expressing aspects related to the attitude of the speaker towards his/her statements. Modality understood in a broader sense is also related to the expression of certainty, factuality, and evidentiality. Negation is a grammatical category that allows changing the truth value of a proposition. Modality and negation interact to express extra-propositional aspects of meaning. More information at http://www.cnts.ua.ac.be/BiographTA/qa4mre.html The Pilot task exploited the same topics and background collections of the main exercise. Test documents, instead, were specifically selected in order to ensure the properties required for the questionnaires. The pilot task was offered in English only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE BACKGROUND COLLECTIONS</head><p>One focus of the task is the ability to extract different types of knowledge and to combine them as a way to answer the questions. In order to allow systems to acquire the same background knowledge, ad-hoc collections were created. At an early stage, a background collection related to the renewable energy domain was first released to participants together with some sample data. The background collection for the sample, of about 11,000 documents, was in English only. For the real test, three background collections -one for each of the topics -were released in all the languages involved in the exercise, i.e., English, German, Italian, Spanish and Romanian. Overall, fifteen large repositories as source of "background knowledge" were created to enable inferring information that is implicit in the text. These background collections are comparable (but not identical) topic-related (but not specialized) collections made available to all participants at the beginning of April by signing a license agreement. Thus, systems could "learn" and acquire knowledge in one language or several.</p><p>The only way to acquire big comparable corpora in the three domains we were interested, was crawling the web. Crawling refers to the acquisition of material specific to a given subject from the Web. The Web, with its vast volumes of data in almost any domain and language, offers a natural source for naturally occurring texts. To this end, a web crawler was specifically created by CELCT in order to gather domain-specific texts from the Web.</p><p>As for the distribution of documents among the collections, the final number of documents fetched for each language collection was different, but this is supposed to reflect the real distribution. Table <ref type="table" coords="3,472.20,503.05,4.98,11.04" target="#tab_0">1</ref> depicts the sizes of the corpora which were acquired and the number of documents contained in each language background collection for each of the three topics. The corpora obtained from the process of crawling contain a set of documents which are related to the test documents. Unfortunately, the degree of noisy documents introduced is unknown. As a final step, in order to ensure that each language background collection really contained documents which supported the inferences of the questions, each language organizer was also asked to manually search on the web for the documents, in their own language, which were to be manually added to each language collection. A list of the respective docs that should be looked for was provided by question creators to each language group. Once all collections were ready in all languages, the zipped files were transferred to CELCT ftp server. All documents inside each collection were then re-numbered giving them a progressive unique identifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Keywords and Crawling</head><p>A web crawler is a relatively simple automated program, or script, that methodically scans or "crawls" through Internet pages to create an index of the data it's looking for.</p><p>The QA4MRE crawler is a flexible application designed to download a large number of documents from the World Wide Web around a specified list of keywords. It was developed using Google API, downloading documents in a ranked order, and obeying the Robot Exclusion Standard. After downloading, documents are converted in .txt format and each text is named according to the sources from which it has been downloaded, for example: "articles.latimes.com_68".</p><p>Keywords play a central role in the crawling process as they are used in acquiring the seed URLs. Before fixing the final set of keywords all people in charge of the creation of the respective language collection experimented with a preliminary pool of keywords and suggested changes to the others. Then, once the sets of keywords were standardised in English, they were translated into the other languages and loaded into CELCT's crawler. Keywords mustn't be too generic, and combination of keywords useful to restrict the domain helped to retrieve relevant documents. Synonyms or words which have very similar meaning -like for example, "climate change" and "climate variability"; "carbon dioxide" and "C02" -were kept as separate queries, as the documents which could be obtained could be different. Also, acronyms were always solved, -like for example Joint United Nations Programme on HIV (UNAIDS) -and were entered in the same query into the crawler.</p><p>In addition, as building a comparable corpus requires control over the selection of source texts in the various languages, each language group was asked to prepare a list of (trusted) web sites -indicatively a number of 40 -which were more likely to have plenty of documents related to the topic in their own language. This was required as a way to increase the number of relevant documents avoiding introducing noise (or virus files). The longer the list of domains was, the higher the number of documents which could be downloaded for each single query. Texts were drawn from a variety range of sources e.g.: newspapers, newswire, web, journals, blogs, Wikipedia entries, etc.</p><p>All keywords and all domains were entered in one crawling run. This solution allowed the removal of duplicate URLs retrieved making different queries, as the encountered URLs were kept in memory, so that every URL was visited only once. On average, it took 2-3 days to build one background collection for one topic.</p><p>Other parameters could also be set, namely the number of documents to be downloaded for each single query. By default it was set to 1000, since, due to Google restrictions, it is the maximum number of documents per query which can be downloaded for a specified source/domain. For the English language, this parameter was set to 500. In an attempt, to reduce the number of indices, and other useless files from the corpus lists, the documents which are too short were automatically discarded, by setting the minimum length of the document to 1000 characters. For the English language it was set to 1500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TEST SET PREPARATION</head><p>As we have seen, the task this year was to answer a series of multiple choice tests, each based on a short document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Test Documents</head><p>In order to allow participants to tune their systems, a set of pilot data was first devised. This consisted of three English documents concerned with the topic of renewable energy taken from Green Blog (http://www.greenblog.org/) together with three sets of questions, one for each document, and a background collection of about 11,000 documents. For each document there were ten multiple choice questions; each question had five candidate answers, one clearly correct answer and four clearly incorrect answers. The task of each system was therefore to choose one answer for each question, by analysing the corresponding test document in conjunction with the background collection.</p><p>Following the creation of the pilot data, attention was turned to the materials for the actual evaluation. The languages this year were English, German, Italian, Romanian and Spanish. The intention was to set identical questions for these five languages. This implied that we had access to a suitable parallel collection of documents so that each test document was exactly translated into each language of the task. Unfortunately, even after decades of interest in parallel corpora, very few publicly available high quality collections exist in these five languages. The main possibilities available to us were "Eurobabble" and technical manuals, but each was somewhat unsuitable for the task. Another option was for us to commission special translations of selected documents in, say, English, just for the purposes of QA4MRE.</p><p>After some consideration, we took up a suggestion of Igal Gabbay to use documents taken from the Technology, Entertainment, Design (TED) conferences (www.ted.com). Each TED event consists of a series of invited presentations by prestigious speakers, from fields such as politics, entertainment and industry. The speakers are fluent, persuasive, and mostly speak from memory with no repetition or hesitation. Each talk lasts for twenty minutes or less and is aimed at a non-specialised but reasonably educated audience. The organisers provide for each talk a high-quality text transcription. In the case of the talks used, this ranges in length between 1125 and 3580 words. However, they also provide an infrastructure for the transcriptions to be translated by volunteers. These translations are carefully refereed and are generally of very high quality. The number of languages in which a talk is available varies, depending on its popularity, but is typically 20-40. From the perspective of QA4MRE, TED transcriptions have some good points and some bad ones. On the one hand, they are of high typographical and syntactic quality, they discuss clearly-defined topics, they are at a reasonable intellectual level, they are available translated accurately into many languages and they are of course publicly available. On the other hand, they are on the short side, and, length-for-length contain less facts amenable to the generation of questions than might be the case for other kinds of document. They may also contain jokes or digressions, or material which can only be comprehended in the context of film clips, photographs or recordings which are used in the talk but which of course do not appear in the transcription. Finally, the transcriptions can contain phrases such as "laughter", "applause" or "music" from time to time. These, of course, are describing events at the talk itself and are thus not a transcription of anything that was said.</p><p>Having decided on the source of documents, three topics were then chosen, AIDS, Climate Change, and Music and Society. For each topic, four TED talks were selected, each having transcripts available in English, German, Italian, Romanian and Spanish. Table <ref type="table" coords="5,225.72,526.45,4.98,11.04" target="#tab_1">2</ref> lists the selected talks. Ten multiple-choice questions were then devised for each talk. As in the pilot materials, a question always had five candidate answers from which to choose, with one clearly correct answer and four clearly incorrect answers. Once the questions had been composed in the language of the original author, each was then translated into English. The English versions of the questions and candidate answers were carefully checked by a referee to verify that they were clear, that the intended answer was clearly correct, that the intended answer was in the test document, and that the other candidate answers were clearly incorrect. Questions were modified accordingly. The English versions were then used to translate each question into each of the five languages of the task. The same process was used to translate each candidate answer (five per query) into the five languages.</p><p>The result of this process was a set of 120 questions in five languages, each with five multiple-choice answers, also in those five languages. The final step was to check that the answer to each question was in fact present in the test document for all the languages of the task. Occasionally, certain parts of the original English text were left out of the translation in a particular target language, or perhaps modified or interpreted in a particular manner which made the question impossible to answer in that language. In such cases, the question had to be withdrawn from all languages and a new one devised to take its place.</p><p>In parallel with the above activity, a background collection was created for each of the three topics, as described in Section 3 above. The questions, test documents and background collections were now ready to be used in the QA4MRE task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Questions</head><p>Unlike previous campaigns, where the aim was mainly to ask factoid questions involving the extraction of simple information (mainly Named Entities) from large collections of long documents, the intention in QA4MRE was to ask more searching questions which required a deep knowledge of individual short texts. Concerning test queries, as is usual practice in the QA campaign, they were artificially constructed from portions of the text to match the criteria we wanted to test in this task. The QA4MRE questions were also created taking into consideration different levels of difficulty. They may refer to:</p><p>facts that (as in traditional QA evaluation) are explicitly present in the text facts that are explicitly present but are not explicitly related (for example, they do not appear in the same sentence, although any human would understand they are connected)</p><p>facts that are not explicitly mentioned in the text, but that are one inferential step away (as in the RTE challenge)</p><p>facts that are explicitly mentioned in the text but that require some inference to be connected to form the answer Out of the 120 questions given in the test set, 44 of them needed some extra information from the background collection in order to be answered, while for 76 questions the information present in the text document alone was enough to select the correct answer. More in details, as Table <ref type="table" coords="6,320.76,366.01,4.98,11.04" target="#tab_2">3</ref> shows, 38 questions had the answer contained in the same sentence/paragraph; while for 38 questions the system had to assemble information from different paragraphs in order to answer the question. In addition, questions were also posed so that the answers were not merely a mechanical repetition of the input question, but all kinds of textual inferences could be requested, i.e., lexical (acronym, synonymy, hyperonymy-hyponymy), syntactic (nominalization-verbalization, causative, paraphrase, active-passive), discourse (co-reference, anaphora ellipsis).  <ref type="table" coords="7,519.60,443.53,4.98,11.04" target="#tab_3">4</ref> with a breakdown by frequency in Table <ref type="table" coords="7,242.64,455.05,3.76,11.04" target="#tab_4">5</ref>. Unlike in previous campaigns, questions were not required to fall into the ten types in a pre-determined distribution. As can be seen in Table <ref type="table" coords="7,375.36,466.45,3.76,11.04" target="#tab_4">5</ref>, about half the questions (64 out of 120) were FACTOID, 17 were CAUSE and 16 were WHICH-IS-TRUE. There were between one and five instances of each of the remaining types. Total 120</p><p>Table <ref type="table" coords="7,97.56,693.25,4.98,11.04" target="#tab_5">6</ref> shows the proportion of correct answers and of NoA answers given by all systems to each different question type. Degree of truth seem to be the easiest type of question to be answered while composite and hypothetical questions appear to be the most difficult to be approached. However, system seem to be less confident in answering methods and opinion questions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Tools and Infrastructure</head><p>Also this year, CELCT developed a series of infrastructures to help the management of the QA4MRE exercise.</p><p>Many processes and requirements were to be dealt with:</p><p>o the need to develop a proper and coherent tool for the management of the data produced during the campaign, to store it and to make it re-usable, as well as to facilitate the analysis and comparison of results o the necessity of assisting the different organizing groups in the various tasks of the data set creation and to facilitate the process of collection and translation of questions o the possibility for the participants to directly access the data, submit their own runs (this also implied some syntax checks of the format), and later, get the detailed viewing of the results and statistics.</p><p>A series of automatic web interfaces were specifically designed for each of these purposes, with the aim of facilitating the data processing and, at the same time, showing the users only what they needed for the task they had to accomplish. So, the main characteristics of these interfaces are the flexibility of the system specifically centred on the user's requirements. While designing the interfaces for question collection and translation one of the first issues which was to be dealt with, was the fact of having many assessors, a big amount of data, and a long process. So tools must ensure an efficient and consistent management of the data, allowing:</p><p>1.</p><p>Edition of the data already entered at any time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Revision of the data by the users themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Consistency propagation ensuring that modifications automatically re-model the output in which they are involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Statistics and evaluation measures are calculated and updated in real time.</p><p>In particular, ensuring the consistency of data is a key feature in data management. For example, if a typo is corrected in the Translation Interface, the modification is automatically updated also in the GoldStandard files, in the Test Set files, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATION</head><p>Participating systems could give one of two possible responses for each question in the test collection:</p><p>• To give one answer selected from the five candidate answers of the question</p><p>• not to answer the question if a system considered that it did not have enough evidences for selecting one of the candidate answers as the correct one. This option is called NoA answer. In order to evaluate the ability of validating its answers, the system could return in this case the candidate answer that it would select in case of having to answer the question.</p><p>Taking into consideration these two possible responses, each question receives one (and only one) of the three following assessments:</p><p>• correct if the system selected the correct answer among the five candidate ones of the given question</p><p>• incorrect if the system selected one of the wrong answers • NoA if the system chose not to answer the question The evaluation of the output given by participating systems was performed automatically by comparing the answers of systems against the gold standard collection with human-made annotations. No manual assessment was required. The task developed this year allowed us to evaluate systems from two different perspectives:</p><p>1. A question-answering evaluation, as the traditional evaluation performed in past campaigns. In this evaluation, we just accounted answers without grouping them.</p><p>2. On the other hand, we can perform a reading-test evaluation, obtaining figures for each particular reading test, and as a part of a topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Measure</head><p>The purpose of allowing NoA answers is to reduce the amount of incorrect responses, while keeping the number of correct ones, by leaving some questions unanswered. As the main evaluation measure for this year's campaign c@1 was used, which takes into account the option of not answering certain questions. c@1 was firstly introduced in ResPubliQA 2009 [8] and is fully described in (Peñas and Rodrigo, 2011). The formulation of c@1 is given in (1).</p><formula xml:id="formula_0" coords="9,234.12,333.51,128.83,28.61">) ( 1 1 @ n n n n n c R U R + = (1)</formula><p>where n R : number of questions correctly answered. n U : number of questions unanswered. n: total number of questions c@1 acknowledges the option of giving NoA answers in the proportion that a system answers questions correctly, which is measured using accuracy. Thus, a higher accuracy over answered questions would give more value to unanswered questions, and therefore, a higher final c@1 value.</p><p>As a secondary measure, we also provided scores according to accuracy (2), the traditional measure applied to QA evaluations that does not distinguish between answered and unanswered questions. We used also the candidate answer given to unanswered questions to obtain accuracy values. where n R : number of questions correctly answered. n UR : number of unanswered questions whose candidate answer was correct. n: total number of questions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Question Answering perspective evaluation</head><p>A question-answering evaluation has been performed over the whole test collection. This evaluation measures the overall performance of a system, without analyzing the behaviour over a particular reading test. The information taken into account for each system at this level is:</p><p>• number of questions ANSWERED o number of questions ANSWERED with RIGHT answer o number of questions ANSWERED with WRONG answer</p><p>• number of questions UNANSWERED o number of questions UNANSWERED with RIGHT candidate answer o number of questions UNANSWERED with WRONG candidate answer o number of questions UNANSWERED with EMPTY candidate answer More in detail, the evaluation at this level includes:</p><p>• Overall c@1(over the 120 questions of the test collection)</p><p>• c@1 per topic (over the 40 questions of each topic)</p><p>• Overall accuracy (over the 120 questions of the test collection, considering also the candidate answers given to unanswered questions)</p><p>• Proportion of answers correctly discarded (see (3))</p><formula xml:id="formula_1" coords="10,183.96,209.70,228.99,31.74">UE UW UR UE UW n n n n n discarded correctly + + + = _ (3)</formula><p>where:</p><p>n UR : number of unanswered questions whose candidate answer was correct n UW : number of unanswered questions whose candidate answer was incorrect n UE : number of unanswered questions whose candidate answer was empty</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Reading perspective evaluation</head><p>The objective of the reading-test evaluation is to offer information about the performance of a system "understanding" the meaning of each single document. This understanding is evaluated by means of multiplechoice tests consisting of ten questions per document. This evaluation is performed taking as reference the c@1 values achieved for each test (one document with ten questions about it). Then, the c@1 values were aggregated at topic and global levels:</p><p>• Median, average and standard deviation of c@1 values at test level, grouped by topic.</p><p>• Overall median, average and standard deviation of c@1 values at test level.</p><p>The median c@1 has been provided under the consideration that it can be more informative at reading-test level than average values. This is because median is less affected by outliers than average, and therefore, it offers more information about the ability of a system to understand a text. For example, if we have three high c@1 values in a topic, but the last one is very low, the median is not affected by this low result (because it is an isolated result in comparison with the other three), while average accounts for this bad behaviour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Random Baselines</head><p>In order to offer some baselines for this task, it must be considered that participating systems can decide to answer or not to answer a given question. Then, we firstly propose the use of a random baseline where all the questions are answered. This baseline has five possibilities when trying to answer a question: it can select the correct answer to the question, or it can select one of the four incorrect answers. In this case, the overall result is 0.2 (both for accuracy and for c@1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">PARTICIPATION and RESULTS</head><p>Out of the 25 groups which had previously registered and signed the license agreement to download the background collections, a total of 12 groups participated in the QA4MRE tasks submitting 62 runs in 3 different languages (German, English, and Romanian). Table <ref type="table" coords="10,283.44,658.93,4.98,11.04" target="#tab_6">7</ref> shows the runs submitted in each language. No runs were submitted either in Italian, or -quite surprisingly -in Spanish (usually the second most chosen language). All runs were monolingual; no team attempted a cross-language task. This was probably due to the fact that crossing the language boundary is currently not core to the task, even though multiliguality is directly addressed through the provision of collections and tests in five languages.</p><p>Participants were allowed to submit a maximum of 10 runs. The first run was to be produced using nothing more than the knowledge provided in the background collections. Additional runs could include other sources of information, e.g. ontologies, rule bases, web, Wikipedia, etc., or other types of inferences. All resources used to acquire the knowledge were to be listed in the submission file.</p><p>Beside specifying the resources used, systems were required to list also the document(s) and sentence(s) that helped them (directly or indirectly) to identify the correct answer. Such provenance was not used for formal evaluation, but for informal analysis and discussion. As usual, the vast majority of the runs were in English, as Table <ref type="table" coords="11,331.56,281.89,4.98,11.04" target="#tab_6">7</ref> shows. The list of participating teams and the reference to their reports are shown in Table <ref type="table" coords="11,260.76,293.29,3.76,11.04" target="#tab_7">8</ref>. Beside Europe, participants came also from USA, China and India. Table <ref type="table" coords="11,96.84,516.01,4.98,11.04" target="#tab_8">9</ref> illustrates the mean scores for each of the 12 reading tests considering all systems (all the values in the following tables refer to c@1) . This shows the difficulty of each particular test. Test 3 (Topic 1) at 0.09 is lower than all the others. So, this appeared to be a very hard test. On the contrary, Test 9 (Topic 3) at 0.32 is higher than all the others by 0.05. So, this test seems to be somewhat easier. Concerning the overall difficulty of the exercize, Topic 3 was the easiest and Topic 1 was the hardest but the range of difficulty is not huge, as Table <ref type="table" coords="11,233.04,652.09,9.90,11.04" target="#tab_0">10</ref> demonstrates. So, the three topics look fairly balanced. Also, average performances do not exceed too much the random baseline (0.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 10: Mean Scores for each Topic</head><p>Topic 1 Topic 2 Topic 3 Mean 0.18 0.20 0.25</p><p>The following three tables (14-15-16) show the best run for each participating group, reporting the mean of the tests for each topic. Except for one case, the overall mean is higher that the baseline.  As for system performances at the question-answering evaluation level we can generally see that only one team (jucs) is above 50%, showing a large room for improvement. From a reading test perspective, in general no group passed the reading tests, and all system seem to be very close to random guessing. Overall results at reading test level, i.e., median, average, and standard deviation for all runs are given in Appendix 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">RELATED WORK</head><p>The current state of development of the NLP technologies offers a good opportunity for proposing an evaluation of MR systems. The opportunity arises from the clear evolution of NLP systems towards a deeper level of text analysis that allows a better understanding of documents. In fact, the interest in MR among different research groups over the world has increased recently as the creation of the MR program at DARPA<ref type="foot" coords="14,442.20,103.48,4.02,8.91" target="#foot_0">1</ref> testifies. The large community involved in Machine Reading is searching a way to evaluate their systems. But the problem of how to evaluate these machines is still an open research issue. Over the last years, the QA Track at CLEF has changed its evaluation methodology in order to promote deeper text understanding. Clearly, the task of retrieving just text excerpts (facts, sentences, paragraphs or documents) is not enough to develop the technology. Besides QA, other evaluation activities were also performed which required deeper analyses of texts, for example Recognizing Textual Entailment (RTE), Answer Validation (AV), and Knowledge Base Population (KBP). Question Answering: a system receives questions formulated in natural language and returns one or more exact answers to these questions, possibly with the locations from which the answers were drawn as justification. The evaluation of QA systems began at the Text Retrieval Conference (TREC) <ref type="bibr" coords="14,381.12,222.42,3.00,6.65" target="#b1">2</ref> , and was continued at the Cross Language Evaluation Forum (CLEF) <ref type="bibr" coords="14,226.20,233.94,3.00,6.65" target="#b2">3</ref> in the EU, and at the NII-NACSIS Test Collection for IR Systems (NTCIR) <ref type="bibr" coords="14,107.64,245.34,3.00,6.65" target="#b3">4</ref> in Japan. Most of the questions used in these evaluations ask about facts (i.e. Who is the president of XYZ?) or dentitions (i.e. What does XYZ mean?). Since systems could search for answers among several documents (using IR engines), it was generally possible to find in some document a "system-friendly" statement that contained exactly the answer information stated in an easily matched form. This made QA both shallow and relatively easy. Recognizing of Textual Entailment (RTE): a system must decide whether the meaning of a text (the Text T) entails the meaning of another text (the Hypothesis H): whether the meaning of the hypothesis can be inferred from the meaning of the text [4]. RTE systems have been evaluated at the RTE Challenges, whose first competition was proposed in 2005. The RTE Challenges encourage the development of systems that have to treat different semantic phenomena. Answer Validation Exercise (AVE) [5.6.7]. A combination of QA and RTE evaluations. Answer Validation (AV) is the task of deciding. given a question and an answer from a QA system, whether the answer is correct or not. AVE was a task focused on the evaluation of AV systems and it was defined as a problem of RTE in order to promote a deeper analysis in QA. Another application of RTE, similar to AVE, in the context of Information Extraction was performed in a pilot task at the RTE-6 <ref type="bibr" coords="14,142.56,417.90,3.00,6.65" target="#b4">5</ref> with the aim of studying the impact of RTE systems in Knowledge Base Population (KBP) <ref type="bibr" coords="14,519.00,417.90,3.00,6.65" target="#b5">6</ref> . The objective of this pilot task is to validate the output of participant systems at the KBP slot illing task that was celebrated at the Text Analysis Conference (TAC) <ref type="bibr" coords="14,280.20,440.82,3.00,6.65" target="#b6">7</ref> . Systems participating at the KBP slot filling task must extract from documents some values for a set of attributes of a certain entity. Given the output of participant systems at KBP, the RTE KBP validation pilot consists of deciding whether each of the values detected for an entity is correct according to the supporting document. For taking this decision, participant systems at the RTE KBP validation pilot receive a set of T-H pairs, where the hypothesis is built combining an entity, an attribute and a value. Other efforts closer to our proposal for evaluating systems understanding took place, as the "ANLP/NAACL 2000 Workshop on Reading comprehension tests as evaluation for computer-based language understanding systems" <ref type="bibr" coords="14,107.04,532.86,3.00,6.65" target="#b7">8</ref> . This workshop proposed to evaluate understanding systems by means of Reading Comprehension (RC) tests. The evaluation consisted of a set of texts and a series of questions about each text. Quite interestingly, most of the approaches presented at that workshop showed how to adapt QA systems to such kind of evaluation. A more complete evaluation methodology of MR systems has been reported in [7], where the authors proposed to use also RC tests. However, the objective of these tests was to extract correct answers from documents, which is similar to QA without an IR engine.</p><p>A natural step in this area is an evaluation methodology that requires a deeper level of inference and of analysis of text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">CONCLUSIONS</head><p>This year, the QA @ CLEF task was characterized by a major innovation, namely the transition from the traditional Question Answering (QA) task, proposed in the last eight QA challenges at CLEF, to a new evaluation focus on the reading of a single document. The main reason behind this choice was the feeling that most systems were ready to make a definitive move towards a deeper understanding of the text. Along the years, the QA challenges adopted simple questions which required almost no inferences to find the correct answers. These surface-level evaluations have promoted QA architectures based on Information Retrieval (IR) techniques, in which the final answer(s) is/are obtained after focusing on selected portions of retrieved documents and matching sentence fragments or sentence parse trees. No real understanding of documents was performed, since none was required by the evaluation. Machine Reading (MR), instead, requires the automatic understanding of texts at a deeper level, so this methodology encourages the development of systems able to perform a deep analyses of the text.</p><p>One way of evaluating the understanding of a text is to assess the ability to answer a set of questions about it. In particular, reading comprehension tests are designed to measure how well human readers understand what they read. Each text comes with a set of questions about information that is stated or implied in the text.</p><p>The objectives of the task are twofold: (i) to propose a task where a deeper level of understanding is required (ii) to extract the knowledge contained in texts as a way to improve the performance of systems where some kinds of reasoning are required. Hence, the development of MR technologies should be fostered and the number of groups interested in the task should increase. This is also an opportunity to create a common framework and community in the field of text understanding.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,78.24,553.55,438.90,99.89"><head>Table 1 : Size of the acquired background collections in the various languages for the three topics</head><label>1</label><figDesc></figDesc><table coords="3,78.24,565.38,438.90,88.06"><row><cell>TOPICS</cell><cell># docs</cell><cell>DE</cell><cell>KB</cell><cell># docs</cell><cell>EN KB</cell><cell># docs</cell><cell>ES KB</cell><cell># docs</cell><cell>IT</cell><cell>KB</cell><cell># docs</cell><cell>RO KB</cell></row><row><cell>AIDS</cell><cell cols="8">25,521 226,008 28,862 535,827 27,702 312,715 32,488</cell><cell cols="4">759,525 25,033 344,289</cell></row><row><cell>CLIMATE CHANGE</cell><cell cols="12">73,057 524,519 42,743 510,661 85,375 677,498 82,722 1238,594 51,130 374,123</cell></row><row><cell>MUSIC &amp; SOCIETY</cell><cell cols="12">81,273 754,720 46,698 733,898 130,000 922,663 92, 036 1274,581 85,116 564,604</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,86.64,211.19,430.26,176.30"><head>Table 2 : TED Test Documents</head><label>2</label><figDesc></figDesc><table coords="5,86.64,223.19,430.26,164.30"><row><cell>Topic</cell><cell>No.</cell><cell>Author</cell><cell>Title</cell><cell>Wor ds</cell></row><row><cell>AIDS</cell><cell>1</cell><cell>Annie Lennox</cell><cell>Why I am an HIV/AIDS activist</cell><cell>1378</cell></row><row><cell>AIDS</cell><cell>2</cell><cell>Bono</cell><cell>Bono's call to action for Africa</cell><cell>3580</cell></row><row><cell>AIDS</cell><cell>3</cell><cell>Elizabeth Pisani</cell><cell>Sex, drugs and HIV --let's get rational</cell><cell>3178</cell></row><row><cell>AIDS</cell><cell>4</cell><cell>Emily Oster</cell><cell>Emily Oster flips our thinking on AIDS in Africa</cell><cell>3299</cell></row><row><cell>Climate Change</cell><cell>5</cell><cell>Al Gore</cell><cell>Al Gore warns on latest climate trends</cell><cell>1235</cell></row><row><cell>Climate Change</cell><cell>6</cell><cell>Al Gore</cell><cell>Al Gore's new thinking on the climate crisis</cell><cell>3190</cell></row><row><cell>Climate Change</cell><cell>7</cell><cell>David Keith</cell><cell>David Keith's unusual climate change idea</cell><cell>3314</cell></row><row><cell>Climate Change</cell><cell>8</cell><cell>Lee Hotz</cell><cell>Inside an Antarctic time machine</cell><cell>1308</cell></row><row><cell>Music &amp; Society</cell><cell>9</cell><cell>Adam Sadowsky</cell><cell>Adam Sadowsky engineers a viral music video</cell><cell>1125</cell></row><row><cell>Music &amp; Society</cell><cell cols="2">10 Ben Cameron</cell><cell>The true power of the performing arts</cell><cell>2000</cell></row><row><cell>Music &amp; Society</cell><cell cols="2">11 David Byrne</cell><cell>How architecture helped music evolve</cell><cell>2213</cell></row><row><cell>Music &amp; Society</cell><cell cols="2">12 Jose Abreu</cell><cell>Jose Abreu on kids transformed by music</cell><cell>1679</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,111.72,277.07,364.26,49.61"><head>Table 3 : Number of Questions which need background knowledge to be answered</head><label>3</label><figDesc></figDesc><table coords="6,111.72,288.90,364.26,37.78"><row><cell></cell><cell></cell><cell>no extra knowledge is needed : 76</cell></row><row><cell>number of questions : 120</cell><cell>info from background collection required : 44</cell><cell>info from different paragraphs: 38 answer in the same sentence/para. : 38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,75.00,448.67,444.30,300.38"><head>Table 4 : Examples of Questions</head><label>4</label><figDesc></figDesc><table coords="6,75.00,460.67,444.30,288.38"><row><cell>Type</cell><cell>Topic</cell><cell cols="2">Test Question</cell><cell>Answers (Correct One in Bold)</cell></row><row><cell>CAUSE</cell><cell>Climate</cell><cell>5</cell><cell>What could be a</cell><cell>higher sea level / more atmospheric</cell></row><row><cell></cell><cell>Change</cell><cell></cell><cell>consequence of a</cell><cell>pollution / less drinking water / more fires /</cell></row><row><cell></cell><cell></cell><cell></cell><cell>reduction in Arctic ice?</cell><cell>less droughts</cell></row><row><cell cols="2">COMPOSITE Climate</cell><cell>7</cell><cell>What solution not tested</cell><cell>the use of renewable energies / the reduction</cell></row><row><cell></cell><cell>Change</cell><cell></cell><cell>by humans could</cell><cell>of CO2 emissions / investment in climate</cell></row><row><cell></cell><cell></cell><cell></cell><cell>contribute to reducing the</cell><cell>change by the governments of developed</cell></row><row><cell></cell><cell></cell><cell></cell><cell>climate change problem?</cell><cell>countries / the introduction of signed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>particles into the stratosphere / the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>protection of the environment</cell></row><row><cell>DEGREE-</cell><cell>AIDS</cell><cell>1</cell><cell>Do people agree that</cell><cell>definitely yes / definitely no / unknown /</cell></row><row><cell>OF-</cell><cell></cell><cell></cell><cell>governments should be</cell><cell>sometimes / only one person agrees</cell></row><row><cell>TRUTH</cell><cell></cell><cell></cell><cell>committed to fighting</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>AIDS?</cell><cell></cell></row><row><cell>FACTOID-</cell><cell>AIDS</cell><cell>2</cell><cell>Which African country did</cell><cell>Somalia / Horn of Africa / Sudan / Abyssinia</cell></row><row><cell>LOCATION</cell><cell></cell><cell></cell><cell>Bono Vox visit?</cell><cell>/ Eritrea</cell></row><row><cell>FACTOID-</cell><cell>Music &amp;</cell><cell>9</cell><cell>How many times was OK</cell><cell>more than 50 million / fifty / a million /</cell></row><row><cell>NUMBER</cell><cell>Society</cell><cell></cell><cell>Go's video viewed?</cell><cell>10,000 / 85</cell></row><row><cell>FACTOID-</cell><cell>AIDS</cell><cell>3</cell><cell>Who wrote "People do</cell><cell>Elizabeth Pisani / Frankie / a friend of</cell></row><row><cell>PERSON</cell><cell></cell><cell></cell><cell>stupid things. That's what</cell><cell>Elizabeth Pisani / the brother of Elizabeth</cell></row><row><cell></cell><cell></cell><cell></cell><cell>spreads HIV"?</cell><cell>Pisani / a drug addict</cell></row><row><cell>FACTOID-</cell><cell>Music &amp;</cell><cell>10</cell><cell>What are two difficulties</cell><cell>jeans and set curtain times / parking and set</cell></row><row><cell>LIST</cell><cell>Society</cell><cell></cell><cell>associated with attending a</cell><cell>curtain times / internet and set curtain times</cell></row><row><cell></cell><cell></cell><cell></cell><cell>live performance?</cell><cell>/ customisation and set curtain times / body</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>types and set curtain times</cell></row></table><note coords="7,337.20,351.01,172.99,11.04;7,337.20,362.53,169.90,11.04;7,337.20,374.05,127.39,11.04;7,70.92,397.45,453.46,11.04;7,70.92,408.97,453.61,11.04;7,70.92,420.49,453.69,11.04;7,70.92,432.01,453.55,11.04;7,70.92,443.53,446.02,11.04"><p>the enjoyment of music / the satisfaction of playing / the feeling of being no-one / the lack of food / the lack of shelter Concerning the types of questions which would be asked, it had originally been proposed that there would be four: FACTOID, CAUSE, HYPOTHETICAL and COMPOSITE. However, following the creation of the pilot materials, six further question types were suggested: DEGREE-OF-TRUTH, METHOD, OPINION, PURPOSE, RESULTS and WHICH-IS-TRUE. Furthermore, FACTOIDs are broken down into LOCATION, NUMBER-CALC, PERSON, STATED-LIST, TIME and UNKNOWN-TYPE Examples of the types can be seen in Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,187.80,514.67,201.79,152.30"><head>Table 5 : Distribution of question types Question type Total number of questions</head><label>5</label><figDesc></figDesc><table coords="7,187.80,547.93,171.90,119.04"><row><cell>CAUSE</cell><cell>17</cell></row><row><cell>DEGREE-OF-TRUTH</cell><cell>1</cell></row><row><cell>COMPOSITE</cell><cell>2</cell></row><row><cell>FACTOID</cell><cell>64</cell></row><row><cell>HYPOTHETICAL</cell><cell>4</cell></row><row><cell>METHOD</cell><cell>5</cell></row><row><cell>OPINION</cell><cell>3</cell></row><row><cell>PURPOSE</cell><cell>4</cell></row><row><cell>RESULTS</cell><cell>4</cell></row><row><cell>WHICH-IS-TRUE</cell><cell>16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,114.48,73.19,366.34,140.90"><head>Table 6 : Percentage of Correct and NoA answers according to different question type</head><label>6</label><figDesc></figDesc><table coords="8,120.48,85.19,348.27,128.90"><row><cell>Question type</cell><cell>% of correct answers</cell><cell>% of NoA answers</cell></row><row><cell>CAUSE</cell><cell>0,18%</cell><cell>0,39%</cell></row><row><cell>DEGREE-OF-TRUTH</cell><cell>0,40%</cell><cell>0,40%</cell></row><row><cell>COMPOSITE</cell><cell>0,15%</cell><cell>0,30%</cell></row><row><cell>FACTOID *</cell><cell>0,30%</cell><cell>0,38%</cell></row><row><cell>HYPOTHETICAL</cell><cell>0,16%</cell><cell>0,31%</cell></row><row><cell>METHOD</cell><cell>0,28%</cell><cell>0,50%</cell></row><row><cell>OPINION</cell><cell>0,23%</cell><cell>0,49%</cell></row><row><cell>PURPOSE</cell><cell>0,24%</cell><cell>0,38%</cell></row><row><cell>RESULTS</cell><cell>0,31%</cell><cell>0,33%</cell></row><row><cell>WHICH-IS-TRUE</cell><cell>0,29%</cell><cell>0,37%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,107.39,142.19,367.94,125.66"><head>Table 7 : Tasks and corresponding numbers of submitted runs. Target languages (corpus and answer)</head><label>7</label><figDesc></figDesc><table coords="11,107.39,174.37,367.94,93.48"><row><cell></cell><cell></cell><cell></cell><cell>DE</cell><cell>EN</cell><cell>ES</cell><cell>IT</cell><cell>RO</cell><cell>Total</cell></row><row><cell>Source languages</cell><cell>(questions)</cell><cell>DE EN ES IT RO Total</cell><cell>11 11</cell><cell>42 42</cell><cell>0</cell><cell>0</cell><cell>9 9</cell><cell>11 42 0 0 9 62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,92.04,329.99,401.10,173.54"><head>Table 8 : Teams with the reference to their reports</head><label>8</label><figDesc></figDesc><table coords="11,92.04,343.43,401.10,160.10"><row><cell>Team</cell><cell>Reference</cell></row><row><cell>Jadavpur University, India</cell><cell>Pakray et al.</cell></row><row><cell>Ca' Foscari University, Italy</cell><cell>-</cell></row><row><cell>LIMSI-CNRS, France</cell><cell>-</cell></row><row><cell>Universidade de Évora, Portugal</cell><cell>Saias and Quaresma</cell></row><row><cell>NEC Laboratories, USA</cell><cell>-</cell></row><row><cell>Daiict, India</cell><cell>Arora</cell></row><row><cell>AL.I.Cuza University, Romania</cell><cell>Iftene et al.</cell></row><row><cell>University of Hagen, Germany</cell><cell>Glockner et al.</cell></row><row><cell>Radboud University Nijmegen, The Netherlands</cell><cell>Verberne</cell></row><row><cell>University of Heidelberg, Germany</cell><cell>Babych et al.</cell></row><row><cell>UNED, Spain</cell><cell>Martinez-Romo and Araujo</cell></row><row><cell>Fudan University, China</cell><cell>Cao et al.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,87.96,575.63,419.46,41.09"><head>Table 9 : Mean Scores for each Reading Test</head><label>9</label><figDesc></figDesc><table coords="11,87.96,585.51,419.46,31.21"><row><cell></cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell></row><row><cell>Mean</cell><cell>0.24</cell><cell>0.19</cell><cell>0.09</cell><cell>0.17</cell><cell>0.18</cell><cell>0.22</cell><cell>0.16</cell><cell>0.24</cell><cell>0.32</cell><cell>0.27</cell><cell>0.18</cell><cell>0.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="12,73.80,84.71,150.93,254.60"><head>Table 14 : Results for English</head><label>14</label><figDesc></figDesc><table coords="12,73.80,96.13,150.93,243.18"><row><cell></cell><cell></cell><cell>Topic</cell><cell>Topic</cell><cell>Topic</cell></row><row><cell></cell><cell>Overall</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>RUN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NAME</cell><cell>Mean</cell><cell>Mean</cell><cell cols="2">Mean Mean</cell></row><row><cell>jucs110</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6enen</cell><cell>0.58</cell><cell>0.80</cell><cell>0.53</cell><cell>0.42</cell></row><row><cell>ifln1102</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>enen</cell><cell>0.37</cell><cell>0.28</cell><cell>0.45</cell><cell>0.37</cell></row><row><cell>uaic111</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0enen</cell><cell>0.28</cell><cell>0.25</cell><cell>0.27</cell><cell>0.32</cell></row><row><cell>fdcs110</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2enen</cell><cell>0.27</cell><cell>0.27</cell><cell>0.20</cell><cell>0.34</cell></row><row><cell>uned11</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>01enen</cell><cell>0.27</cell><cell>0.19</cell><cell>0.36</cell><cell>0.26</cell></row><row><cell>base110</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1enen</cell><cell>0.26</cell><cell>0.23</cell><cell>0.21</cell><cell>0.35</cell></row><row><cell>swai110</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1enen</cell><cell>0.25</cell><cell>0.24</cell><cell>0.21</cell><cell>0.29</cell></row><row><cell>iles1108</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>enen</cell><cell>0.24</cell><cell>0.25</cell><cell>0.13</cell><cell>0.33</cell></row><row><cell>diue110</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2enen</cell><cell>0.20</cell><cell>0.12</cell><cell>0.31</cell><cell>0.17</cell></row><row><cell>random</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>baseline</cell><cell>0.20</cell><cell>0.20</cell><cell>0.20</cell><cell>0.20</cell></row><row><cell>vens110</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1enen</cell><cell>0.18</cell><cell>0.17</cell><cell>0.06</cell><cell>0.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="12,239.52,84.71,144.09,103.40"><head>Table 15 : Results for German</head><label>15</label><figDesc></figDesc><table coords="12,239.52,96.13,144.09,91.98"><row><cell></cell><cell></cell><cell>Topic</cell><cell>Topic</cell><cell>Topic</cell></row><row><cell></cell><cell>Overall</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>RUN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NAME</cell><cell>Mean</cell><cell cols="3">Mean Mean Mean</cell></row><row><cell>uhei110</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2dede</cell><cell>0.23</cell><cell>0.18</cell><cell>0.34</cell><cell>0.16</cell></row><row><cell>loga110</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2dede</cell><cell>0.21</cell><cell>0.20</cell><cell>0.24</cell><cell>0.19</cell></row><row><cell>random</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>baseline</cell><cell>0.20</cell><cell>0.20</cell><cell>0.20</cell><cell>0.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="12,397.32,84.71,148.89,84.56"><head>Table 16 : Results for Romanian</head><label>16</label><figDesc></figDesc><table coords="12,397.32,96.13,148.89,73.14"><row><cell></cell><cell></cell><cell>Topic</cell><cell>Topic</cell><cell>Topic</cell></row><row><cell></cell><cell>Overall</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>RUN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NAME</cell><cell cols="4">Mean Mean Mean Mean</cell></row><row><cell>uaic1107</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>roro</cell><cell>0.26</cell><cell>0.10</cell><cell>0.39</cell><cell>0.29</cell></row><row><cell>random</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>baseline</cell><cell>0.20</cell><cell>0.20</cell><cell>0.20</cell><cell>0.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="12,162.96,422.87,264.54,333.17"><head>Table 11 : Results for English</head><label>11</label><figDesc></figDesc><table coords="12,162.96,434.87,264.54,321.17"><row><cell>System</cell><cell cols="3">c@1 #R #W #NoA #NoA</cell><cell>#NoA</cell><cell>#NoA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>R</cell><cell>W</cell><cell>empty</cell></row><row><cell>combination</cell><cell>0.95</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>jucs1106enen</cell><cell>0.57 58 40</cell><cell>22</cell><cell>0</cell><cell>0</cell><cell>22</cell></row><row><cell>jucs1107enen</cell><cell>0.47 52 57</cell><cell>11</cell><cell>0</cell><cell>0</cell><cell>11</cell></row><row><cell>ifln1102enen</cell><cell>0.37 42 71</cell><cell>7</cell><cell>0</cell><cell>0</cell><cell>7</cell></row><row><cell>ifln1105enen</cell><cell>0.35 40 73</cell><cell>7</cell><cell>0</cell><cell>0</cell><cell>7</cell></row><row><cell>ifln1101enen</cell><cell>0.34 32 56</cell><cell>32</cell><cell>0</cell><cell>0</cell><cell>32</cell></row><row><cell>ifln1104enen</cell><cell>0.33 31 57</cell><cell>32</cell><cell>0</cell><cell>0</cell><cell>32</cell></row><row><cell>jucs1104enen</cell><cell>0.32 38 82</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>jucs1105enen</cell><cell>0.32 38 82</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>uaic1110enen</cell><cell>0.29 25 47</cell><cell>48</cell><cell>12</cell><cell>34</cell><cell>2</cell></row><row><cell>fdcs1102enen</cell><cell>0.28 22 38</cell><cell>60</cell><cell>0</cell><cell>0</cell><cell>60</cell></row><row><cell>base1101enen</cell><cell>0.27 26 64</cell><cell>30</cell><cell>0</cell><cell>0</cell><cell>30</cell></row><row><cell>uned1101enen</cell><cell>0.27 24 53</cell><cell>43</cell><cell>0</cell><cell>0</cell><cell>43</cell></row><row><cell>fdcs1103enen</cell><cell>0.26 25 65</cell><cell>30</cell><cell>0</cell><cell>0</cell><cell>30</cell></row><row><cell>swai1101enen</cell><cell>0.26 24 62</cell><cell>34</cell><cell>0</cell><cell>0</cell><cell>34</cell></row><row><cell>iles1108enen</cell><cell>0.24 28 91</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1</cell></row><row><cell>uned1109enen</cell><cell>0.24 20 47</cell><cell>53</cell><cell>0</cell><cell>0</cell><cell>53</cell></row><row><cell>iles1107enen</cell><cell>0.23 27 93</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>iles1110enen</cell><cell>0.22 26 94</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>diue1102enen</cell><cell>0.21 18 55</cell><cell>47</cell><cell>0</cell><cell>0</cell><cell>47</cell></row><row><cell>jucs1103enen</cell><cell>0.21 25 95</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>uned1102enen</cell><cell>0.21 17 46</cell><cell>57</cell><cell>0</cell><cell>0</cell><cell>57</cell></row><row><cell>iles1109enen</cell><cell>0.20 24 96</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>uned1103enen</cell><cell>0.20 16 44</cell><cell>60</cell><cell>0</cell><cell>0</cell><cell>60</cell></row><row><cell cols="2">random baseline 0.20</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>iles1106enen</cell><cell>0.19 14 34</cell><cell>72</cell><cell>0</cell><cell>0</cell><cell>72</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="13,162.96,309.11,264.54,183.17"><head>Table 12 : Results for German</head><label>12</label><figDesc></figDesc><table coords="13,162.96,321.11,264.54,171.17"><row><cell>System</cell><cell cols="5">c@1 #R #W #NoA #NoA</cell><cell>#NoA</cell><cell>#NoA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>R</cell><cell>W</cell><cell>empty</cell></row><row><cell>combination</cell><cell>0.34</cell><cell></cell><cell></cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>uhei1109dede</cell><cell cols="3">0.24 22 60</cell><cell>38</cell><cell>0</cell><cell>0</cell><cell>38</cell></row><row><cell>uhei1102dede</cell><cell cols="3">0.23 19 43</cell><cell>58</cell><cell>0</cell><cell>0</cell><cell>58</cell></row><row><cell>loga1101dede</cell><cell cols="3">0.22 21 67</cell><cell>32</cell><cell>0</cell><cell>0</cell><cell>32</cell></row><row><cell>loga1102dede</cell><cell cols="3">0.22 21 67</cell><cell>32</cell><cell>0</cell><cell>0</cell><cell>32</cell></row><row><cell>uhei1103dede</cell><cell cols="3">0.22 18 45</cell><cell>57</cell><cell>0</cell><cell>0</cell><cell>57</cell></row><row><cell cols="2">random baseline 0.20</cell><cell></cell><cell></cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>uhei1106dede</cell><cell cols="3">0.19 13 20</cell><cell>87</cell><cell>0</cell><cell>0</cell><cell>87</cell></row><row><cell>uhei1104dede</cell><cell cols="3">0.18 14 43</cell><cell>63</cell><cell>0</cell><cell>0</cell><cell>63</cell></row><row><cell>uhei1108dede</cell><cell cols="3">0.18 14 36</cell><cell>70</cell><cell>0</cell><cell>0</cell><cell>70</cell></row><row><cell>uhei1105dede</cell><cell cols="3">0.17 13 43</cell><cell>64</cell><cell>0</cell><cell>0</cell><cell>64</cell></row><row><cell>uhei1107dede</cell><cell cols="3">0.16 11 14</cell><cell>95</cell><cell>0</cell><cell>0</cell><cell>95</cell></row><row><cell>uhei1101dede</cell><cell>0.13</cell><cell>9</cell><cell>18</cell><cell>93</cell><cell>0</cell><cell>0</cell><cell>93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="13,70.92,529.79,453.66,194.90"><head>Table 13 : Results for Romanian System c@1 #R #W #NoA #NoA R</head><label>13</label><figDesc></figDesc><table coords="13,364.80,541.79,62.70,20.54"><row><cell>#NoA</cell><cell>#NoA</cell></row><row><cell>W</cell><cell>empty</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="17,70.92,85.56,475.98,667.15"><head>APPENDIX 1: Overall results at reading test level: Median, Average, and Standard Deviation for all runs</head><label></label><figDesc></figDesc><table coords="17,73.80,153.49,473.10,599.22"><row><cell>uaic1101roro</cell><cell>0.20</cell><cell>0.23</cell><cell>0.17</cell><cell>0.12</cell><cell>0.11</cell><cell>0.08</cell><cell>0.35</cell><cell>0.36</cell><cell>0.15</cell><cell>0.27</cell><cell>0.23</cell><cell>0.18</cell></row><row><cell cols="6">uaic1102roro APPENDIX 2: SYSTEM DESCRIPTIONS 0.00 0.13 0.19 0.00 0.00</cell><cell>0.00</cell><cell>0.10</cell><cell>0.13</cell><cell>0.16</cell><cell>0.24</cell><cell>0.26</cell><cell>0.24</cell></row><row><cell>uaic1103roro</cell><cell>0.16</cell><cell>0.21</cell><cell>0.20</cell><cell>0.00</cell><cell>0.05</cell><cell>0.10</cell><cell>0.35</cell><cell>0.33</cell><cell>0.16</cell><cell>0.26</cell><cell>0.26</cell><cell>0.24</cell></row><row><cell>uaic1104roro</cell><cell>0.13</cell><cell>0.19</cell><cell>0.22</cell><cell>0.00</cell><cell>0.05</cell><cell>0.10</cell><cell>0.22</cell><cell>0.22</cell><cell>0.20</cell><cell>0.33</cell><cell>0.31</cell><cell>0.29</cell></row><row><cell>uaic1105roro</cell><cell>0.00</cell><cell>0.13</cell><cell>0.21</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.08</cell><cell>0.16</cell><cell>0.32</cell><cell>0.30</cell><cell>0.26</cell></row><row><cell>RUN_NAME uaic1106roro uaic1107roro uaic1108roro uaic1109roro uaic1110enen</cell><cell>Overall Median 0.13 0.20 0.00 0.16 0.31</cell><cell>Overall Average 0.19 0.26 0.14 0.22 0.28</cell><cell>0.22 0.20 Overall Standard 0.20 0.21 Deviation 0.11</cell><cell cols="2">Topic 1 0.05 0.10 0.04 0.08 Median Average 0.00 0.10 0.00 0.07 0.23 0.25</cell><cell cols="3">Topic 2 0.22 0.39 0.08 0.26 Deviation Median Average 0.10 0.22 0.08 0.40 Standard 0.08 0.00 0.10 0.29 0.12 0.31 0.27</cell><cell cols="3">Topic 3 0.31 0.29 0.30 0.31 Deviation Median Average 0.20 0.33 0.14 0.30 Standard 0.16 0.32 0.19 0.33 0.09 0.36 0.32</cell><cell>0.29 0.24 Standard 0.26 0.29 Deviation 0.14</cell></row><row><cell>uhei1101dede</cell><cell>0.16</cell><cell>0.13</cell><cell>0.13</cell><cell>0.17</cell><cell>0.16</cell><cell>0.13</cell><cell>0.09</cell><cell>0.13</cell><cell>0.16</cell><cell>0.09</cell><cell>0.09</cell><cell>0.10</cell></row><row><cell>base1101enen uhei1102dede</cell><cell>0.27 0.22</cell><cell>0.26 0.23</cell><cell>0.17 0.15</cell><cell>0.22 0.22</cell><cell>0.23 0.18</cell><cell>0.10 0.12</cell><cell>0.22 0.39</cell><cell>0.21 0.34</cell><cell>0.16 0.14</cell><cell>0.44 0.16</cell><cell>0.35 0.16</cell><cell>0.24 0.13</cell></row><row><cell>diue1101enen uhei1103dede</cell><cell>0.13 0.23</cell><cell>0.16 0.21</cell><cell>0.15 0.13</cell><cell>0.00 0.23</cell><cell>0.00 0.18</cell><cell>0.00 0.13</cell><cell>0.26 0.32</cell><cell>0.28 0.30</cell><cell>0.15 0.13</cell><cell>0.19 0.16</cell><cell>0.20 0.16</cell><cell>0.08 0.13</cell></row><row><cell>diue1102enen uhei1104dede</cell><cell>0.15 0.16</cell><cell>0.2 0.17</cell><cell>0.14 0.13</cell><cell>0.14 0.08</cell><cell>0.12 0.10</cell><cell>0.08 0.13</cell><cell>0.27 0.31</cell><cell>0.31 0.28</cell><cell>0.17 0.10</cell><cell>0.14 0.16</cell><cell>0.17 0.12</cell><cell>0.08 0.08</cell></row><row><cell>fdcs1102enen uhei1105dede</cell><cell>0.25 0.17</cell><cell>0.27 0.16</cell><cell>0.19 0.13</cell><cell>0.27 0.08</cell><cell>0.27 0.10</cell><cell>0.23 0.13</cell><cell>0.15 0.27</cell><cell>0.20 0.22</cell><cell>0.10 0.15</cell><cell>0.43 0.17</cell><cell>0.34 0.16</cell><cell>0.24 0.12</cell></row><row><cell>fdcs1103enen uhei1106dede</cell><cell>0.22 0.17</cell><cell>0.25 0.18</cell><cell>0.17 0.12</cell><cell>0.24 0.17</cell><cell>0.27 0.18</cell><cell>0.07 0.15</cell><cell>0.14 0.26</cell><cell>0.17 0.25</cell><cell>0.06 0.10</cell><cell>0.34 0.17</cell><cell>0.33 0.13</cell><cell>0.29 0.08</cell></row><row><cell>ifln1101enen uhei1107dede</cell><cell>0.38 0.17</cell><cell>0.32 0.16</cell><cell>0.20 0.12</cell><cell>0.25 0.18</cell><cell>0.26 0.18</cell><cell>0.15 0.15</cell><cell>0.31 0.18</cell><cell>0.28 0.22</cell><cell>0.22 0.10</cell><cell>0.41 0.09</cell><cell>0.42 0.09</cell><cell>0.23 0.10</cell></row><row><cell>ifln1102enen uhei1108dede</cell><cell>0.35 0.18</cell><cell>0.37 0.18</cell><cell>0.16 0.12</cell><cell>0.23 0.18</cell><cell>0.28 0.16</cell><cell>0.11 0.12</cell><cell>0.45 0.22</cell><cell>0.45 0.19</cell><cell>0.24 0.14</cell><cell>0.40 0.22</cell><cell>0.37 0.19</cell><cell>0.10 0.14</cell></row><row><cell>ifln1104enen uhei1109dede</cell><cell>0.27 0.23</cell><cell>0.31 0.23</cell><cell>0.18 0.13</cell><cell>0.21 0.21</cell><cell>0.23 0.17</cell><cell>0.11 0.12</cell><cell>0.24 0.28</cell><cell>0.29 0.30</cell><cell>0.16 0.09</cell><cell>0.41 0.14</cell><cell>0.42 0.21</cell><cell>0.23 0.15</cell></row><row><cell>ifln1105enen uned1101enen</cell><cell>0.35 0.27</cell><cell>0.35 0.27</cell><cell>0.13 0.13</cell><cell>0.23 0.19</cell><cell>0.28 0.19</cell><cell>0.11 0.08</cell><cell>0.40 0.35</cell><cell>0.40 0.36</cell><cell>0.18 0.11</cell><cell>0.40 0.23</cell><cell>0.37 0.26</cell><cell>0.10 0.14</cell></row><row><cell>iles1101enen uned1102enen</cell><cell>0.00 0.24</cell><cell>0.07 0.21</cell><cell>0.12 0.16</cell><cell>0.10 0.23</cell><cell>0.10 0.20</cell><cell>0.11 0.16</cell><cell>0.09 0.06</cell><cell>0.13 0.14</cell><cell>0.16 0.21</cell><cell>0.00 0.32</cell><cell>0.00 0.28</cell><cell>0.00 0.09</cell></row><row><cell>iles1102enen uned1103enen</cell><cell>0.09 0.17</cell><cell>0.12 0.19</cell><cell>0.17 0.18</cell><cell>0.09 0.24</cell><cell>0.20 0.23</cell><cell>0.28 0.09</cell><cell>0.17 0.00</cell><cell>0.13 0.11</cell><cell>0.09 0.21</cell><cell>0.00 0.21</cell><cell>0.05 0.23</cell><cell>0.09 0.22</cell></row><row><cell>iles1103enen uned1104enen</cell><cell>0.15 0.16</cell><cell>0.18 0.15</cell><cell>0.14 0.12</cell><cell>0.20 0.09</cell><cell>0.18 0.12</cell><cell>0.05 0.15</cell><cell>0.10 0.16</cell><cell>0.08 0.12</cell><cell>0.05 0.08</cell><cell>0.30 0.23</cell><cell>0.30 0.20</cell><cell>0.18 0.14</cell></row><row><cell>iles1104enen uned1105enen</cell><cell>0.20 0.16</cell><cell>0.17 0.15</cell><cell>0.11 0.16</cell><cell>0.20 0.08</cell><cell>0.18 0.09</cell><cell>0.05 0.10</cell><cell>0.05 0.15</cell><cell>0.08 0.12</cell><cell>0.10 0.08</cell><cell>0.20 0.23</cell><cell>0.25 0.26</cell><cell>0.10 0.24</cell></row><row><cell>iles1105enen uned1106enen</cell><cell>0.20 0.14</cell><cell>0.17 0.13</cell><cell>0.11 0.15</cell><cell>0.20 0.09</cell><cell>0.18 0.13</cell><cell>0.05 0.16</cell><cell>0.05 0.07</cell><cell>0.08 0.07</cell><cell>0.10 0.08</cell><cell>0.20 0.16</cell><cell>0.25 0.20</cell><cell>0.10 0.20</cell></row><row><cell>iles1106enen uned1107enen</cell><cell>0.15 0.16</cell><cell>0.18 0.13</cell><cell>0.16 0.11</cell><cell>0.21 0.09</cell><cell>0.25 0.09</cell><cell>0.24 0.10</cell><cell>0.16 0.08</cell><cell>0.16 0.08</cell><cell>0.02 0.09</cell><cell>0.08 0.21</cell><cell>0.12 0.23</cell><cell>0.15 0.08</cell></row><row><cell>iles1107enen uned1108enen</cell><cell>0.20 0.00</cell><cell>0.22 0.03</cell><cell>0.14 0.07</cell><cell>0.20 0.00</cell><cell>0.23 0.05</cell><cell>0.13 0.10</cell><cell>0.10 0.00</cell><cell>0.13 0.00</cell><cell>0.05 0.00</cell><cell>0.30 0.00</cell><cell>0.33 0.04</cell><cell>0.15 0.09</cell></row><row><cell>iles1108enen uned1109enen</cell><cell>0.20 0.21</cell><cell>0.24 0.23</cell><cell>0.12 0.18</cell><cell>0.25 0.20</cell><cell>0.25 0.21</cell><cell>0.13 0.07</cell><cell>0.10 0.41</cell><cell>0.13 0.33</cell><cell>0.05 0.23</cell><cell>0.37 0.09</cell><cell>0.33 0.16</cell><cell>0.09 0.21</cell></row><row><cell>iles1109enen vens1101enen</cell><cell>0.20 0.11</cell><cell>0.20 0.18</cell><cell>0.10 0.17</cell><cell>0.20 0.11</cell><cell>0.23 0.17</cell><cell>0.13 0.19</cell><cell>0.10 0.06</cell><cell>0.13 0.06</cell><cell>0.05 0.06</cell><cell>0.20 0.28</cell><cell>0.25 0.31</cell><cell>0.10 0.12</cell></row><row><cell>iles1110enen</cell><cell>0.20</cell><cell>0.22</cell><cell>0.10</cell><cell>0.25</cell><cell>0.25</cell><cell>0.13</cell><cell>0.10</cell><cell>0.15</cell><cell>0.10</cell><cell>0.25</cell><cell>0.25</cell><cell>0.06</cell></row><row><cell>jucs1101enen</cell><cell>0.15</cell><cell>0.16</cell><cell>0.13</cell><cell>0.15</cell><cell>0.15</cell><cell>0.06</cell><cell>0.15</cell><cell>0.18</cell><cell>0.17</cell><cell>0.15</cell><cell>0.15</cell><cell>0.17</cell></row><row><cell>jucs1102enen</cell><cell>0.15</cell><cell>0.16</cell><cell>0.13</cell><cell>0.15</cell><cell>0.15</cell><cell>0.06</cell><cell>0.15</cell><cell>0.18</cell><cell>0.17</cell><cell>0.15</cell><cell>0.15</cell><cell>0.17</cell></row><row><cell>jucs1103enen</cell><cell>0.20</cell><cell>0.21</cell><cell>0.12</cell><cell>0.15</cell><cell>0.18</cell><cell>0.10</cell><cell>0.20</cell><cell>0.25</cell><cell>0.17</cell><cell>0.20</cell><cell>0.20</cell><cell>0.08</cell></row><row><cell>jucs1104enen</cell><cell>0.30</cell><cell>0.32</cell><cell>0.15</cell><cell>0.25</cell><cell>0.30</cell><cell>0.14</cell><cell>0.20</cell><cell>0.25</cell><cell>0.19</cell><cell>0.40</cell><cell>0.40</cell><cell>0.08</cell></row><row><cell>jucs1105enen</cell><cell>0.30</cell><cell>0.32</cell><cell>0.15</cell><cell>0.25</cell><cell>0.30</cell><cell>0.14</cell><cell>0.20</cell><cell>0.25</cell><cell>0.19</cell><cell>0.40</cell><cell>0.40</cell><cell>0.08</cell></row><row><cell>jucs1106enen</cell><cell>0.74</cell><cell>0.58</cell><cell>0.37</cell><cell>0.81</cell><cell>0.80</cell><cell>0.18</cell><cell>0.68</cell><cell>0.53</cell><cell>0.36</cell><cell>0.39</cell><cell>0.42</cell><cell>0.49</cell></row><row><cell>jucs1107enen</cell><cell>0.45</cell><cell>0.48</cell><cell>0.28</cell><cell>0.81</cell><cell>0.80</cell><cell>0.18</cell><cell>0.20</cell><cell>0.25</cell><cell>0.19</cell><cell>0.40</cell><cell>0.40</cell><cell>0.08</cell></row><row><cell>loga1101dede</cell><cell>0.14</cell><cell>0.21</cell><cell>0.17</cell><cell>0.18</cell><cell>0.20</cell><cell>0.18</cell><cell>0.23</cell><cell>0.24</cell><cell>0.12</cell><cell>0.13</cell><cell>0.19</cell><cell>0.23</cell></row><row><cell>loga1102dede</cell><cell>0.14</cell><cell>0.21</cell><cell>0.17</cell><cell>0.18</cell><cell>0.20</cell><cell>0.18</cell><cell>0.23</cell><cell>0.24</cell><cell>0.12</cell><cell>0.13</cell><cell>0.19</cell><cell>0.23</cell></row><row><cell>swai1101enen</cell><cell>0.22</cell><cell>0.25</cell><cell>0.19</cell><cell>0.20</cell><cell>0.24</cell><cell>0.13</cell><cell>0.11</cell><cell>0.21</cell><cell>0.28</cell><cell>0.26</cell><cell>0.29</cell><cell>0.17</cell></row><row><cell>swai1102enen</cell><cell>0.00</cell><cell>0.06</cell><cell>0.09</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.18</cell><cell>0.17</cell><cell>0.01</cell></row><row><cell>swai1103enen</cell><cell>0.00</cell><cell>0.02</cell><cell>0.05</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.05</cell><cell>0.10</cell></row><row><cell>swai1104enen</cell><cell>0.00</cell><cell>0.08</cell><cell>0.11</cell><cell>0.00</cell><cell>0.05</cell><cell>0.09</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.17</cell><cell>0.20</cell><cell>0.08</cell></row><row><cell>swai1105enen</cell><cell>0.16</cell><cell>0.16</cell><cell>0.14</cell><cell>0.00</cell><cell>0.04</cell><cell>0.08</cell><cell>0.16</cell><cell>0.15</cell><cell>0.11</cell><cell>0.31</cell><cell>0.28</cell><cell>0.13</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="19,63.24,133.91,460.74,614.66"><head>Table 17 : Methods used by participating systems Table 18: Use of Knowledge by participating systems System name Question Analyses Linguistic Processing</head><label>17</label><figDesc></figDesc><table coords="19,63.24,171.10,460.74,577.47"><row><cell cols="2">No Question Analyses</cell><cell>Manually done Patterns</cell><cell>Automatically acquired</cell><cell>patterns</cell><cell></cell><cell>Other</cell><cell>Part Of Speech Tagging</cell><cell>Chunking</cell><cell>n-grams</cell><cell>Named Entity Recognition</cell><cell>(NER)</cell><cell>Temporal expressions</cell><cell>Numerical expressions</cell><cell cols="2">Phrase transformations</cell><cell>Dependency analysis</cell><cell></cell><cell>Functions (sub. obj. etc)</cell><cell>Syntactic transformations</cell><cell cols="2">Semantic parsing</cell><cell>Semantic role labeling</cell><cell></cell><cell>Predefined Sets Of</cell><cell>Relation</cell><cell>Frames</cell><cell>logic representation</cell><cell>Theorem prover</cell><cell>None</cell><cell>Other</cell></row><row><cell>base</cell><cell></cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">x x</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>diue</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>fdcs</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell cols="3">x x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ifln</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>factoid extract.</cell></row><row><cell>iles</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">x x x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>jucs</cell><cell></cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stem</cell></row><row><cell>loga</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x x</cell></row><row><cell>swai</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>uaic</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">x x x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>uhei</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">x x x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>uned</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Graph Analysis</cell><cell cols="2">x x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell></row><row><cell>vens</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">x x</cell><cell></cell><cell cols="9">x x x x x x</cell><cell></cell><cell cols="3">x x</cell><cell></cell><cell></cell><cell></cell><cell cols="2">x x</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="11">Knowledge Resources Used</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tools</cell></row><row><cell cols="2">System name</cell><cell></cell><cell>captured from the</cell><cell>background collection</cell><cell>Lexical DB</cell><cell>Thesaurus</cell><cell>Encyclopedia</cell><cell>Ontology</cell><cell>Collection of paraphrases</cell><cell>Word List</cell><cell></cell><cell>Gazzetteers</cell><cell>Categorial-Variation DB</cell><cell>Synonym-Acronym</cell><cell>Dictionary</cell><cell>Dependency Similarity</cell><cell>Dictionary</cell><cell>Proximity Similarity</cell><cell>Lexical Reference Rule-</cell><cell>Base</cell><cell>Collection of word</cell><cell>Knowledge propositions</cell><cell>Collection of entailment</cell><cell>rules</cell><cell cols="2">Coreference Resolver</cell><cell>Named Entities Recognition</cell><cell>POS Tagger</cell><cell>Parser</cell><cell>Name Normalization</cell></row><row><cell>base</cell><cell></cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x x</cell></row><row><cell>diue</cell><cell></cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell></row><row><cell>fdcs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">x x x x x</cell></row><row><cell>ifln</cell><cell></cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell></row><row><cell>iles</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">x x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x x x</cell></row><row><cell>jucs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell>x</cell></row><row><cell>loga</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">x x</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">x x x</cell><cell>x</cell></row><row><cell cols="2">swai</cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell></row><row><cell>uaic</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell></row><row><cell>uhei</cell><cell></cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell></row><row><cell cols="2">uned</cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x x</cell></row><row><cell cols="2">vens</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">x x x x x</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="20,144.60,73.19,298.33,278.30"><head>Table 19 : Techniques used for the Answer Validation component System name</head><label>19</label><figDesc></figDesc><table coords="20,144.60,88.92,298.33,262.57"><row><cell></cell><cell>No answer validation</cell><cell>Machine Learning</cell><cell>Web redundancies</cell><cell>Redundancies in the collection</cell><cell>Lexical similarity (term</cell><cell>overlapping)</cell><cell>Syntactic similarity</cell><cell>Sematic similarity</cell><cell>Theorem prooving or similar</cell><cell>Other</cell></row><row><cell>base</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>diue</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell></row><row><cell>fdcs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">x</cell><cell>x</cell><cell>x</cell><cell></cell></row><row><cell>ifln</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>iles</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>jucs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell></cell></row><row><cell>loga</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell></row><row><cell>swai</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell></cell></row><row><cell>uaic</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">x</cell><cell></cell><cell></cell><cell></cell></row><row><cell>uhei</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">x</cell><cell>x</cell><cell></cell><cell>x</cell></row><row><cell>uned</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>vens</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>base</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="14,76.20,670.23,169.62,9.97"><p>http://www.darpa.mil/ipto/programs/mr/mr.asp</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="14,76.20,680.55,68.50,9.97"><p>http://trec.nist.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="14,76.20,690.99,111.46,9.97"><p>http://www.clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="14,76.20,701.31,104.26,9.97"><p>http://research.nii.ac.jp/ntcir/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="14,76.20,711.63,166.18,9.97"><p>http://www.nist.gov/tac/2010/RTE/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="14,76.20,721.95,127.54,9.97"><p>http://nlp.cs.qc.cuny.edu/kbp/2010/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="14,78.48,732.39,108.10,9.97"><p>http://www.nist.gov/tac/2010/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="14,76.20,742.71,177.54,9.97"><p>http://www.aclweb.org/anthology/W/W00/#0600</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>Special thanks are also due <rs type="person">Giovanni Moretti</rs> (<rs type="affiliation">CELCT. Trento. Italy</rs>) for the technical support in the management of all data of the campaign. This work has been partially supported by the <rs type="funder">Research Network MA2VICMR</rs> (<rs type="grantNumber">S2009/TIC-1542</rs>) and <rs type="projectName">Holopedia</rs> project (<rs type="grantNumber">TIN2010-21128-C02</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Quf6hZC">
					<idno type="grant-number">S2009/TIC-1542</idno>
					<orgName type="project" subtype="full">Holopedia</orgName>
				</org>
				<org type="funding" xml:id="_whg2HEp">
					<idno type="grant-number">TIN2010-21128-C02</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="15,74.68,494.53,449.68,11.04;15,70.92,506.05,453.54,11.04;15,70.92,517.45,332.01,11.04" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,287.64,494.53,185.86,11.04">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><forename type="middle">Felisa</forename><surname>Álvaro</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,474.12,506.05,50.34,11.04;15,70.92,517.45,204.62,11.04">Advances in Multilingual and Multimodal Information Retrieval</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><surname>Jijkoun</surname></persName>
		</editor>
		<editor>
			<persName><surname>Th</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">H</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">D W</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">A</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">V</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><surname>Santos</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09">2007. September 2008</date>
			<biblScope unit="volume">5152</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,74.68,534.97,449.69,11.04;15,70.92,546.49,453.43,11.04;15,70.92,558.01,453.46,11.04;15,70.92,569.53,395.01,11.04" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,344.16,534.97,180.22,11.04">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Álvaro Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">Felisa</forename><surname>Valentín Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,100.68,558.01,423.70,11.04;15,70.92,569.53,69.99,11.04">Evaluation of Multilingual and Multi-modal Information Retrieval. 7th Workshop of the Cross-Language Evaluation Forum</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">F C</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">J</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">B</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">D W</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">M</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><surname>Stempfhuber</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF; Alicante. Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09-20">2006. 2006. September 20-22. 2006</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="15,74.68,587.05,449.80,11.04;15,70.92,598.45,453.43,11.04;15,70.92,609.97,453.45,11.04;15,70.92,621.49,326.97,11.04" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,287.64,587.05,185.86,11.04">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><forename type="middle">Anselmo</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">Felisa</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,430.92,598.45,93.43,11.04;15,70.92,609.97,448.59,11.04">Evaluating Systems for Multilingual and Multimodal Information Access. 9th Workshop of the Cross-Language Evaluation Forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><surname>Th</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">V</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">A</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">H</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">D</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">V</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">D</forename><surname>Jijkoun</surname></persName>
		</editor>
		<editor>
			<persName><surname>Santos</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF; Aarhus. Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09-17">2008. 2008. September 17-19. 2008</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="15,74.69,639.01,449.80,11.04;15,70.92,650.53,343.53,11.04" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,290.40,639.01,229.78,11.04">The PASCAL Recognising Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Oren Glickman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="15,81.72,650.53,140.99,11.04">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3944</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,74.69,667.93,449.72,11.04;15,70.92,679.45,171.90,11.04" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,308.76,667.93,66.11,11.04">Machine reading</title>
		<author>
			<persName coords=""><forename type="first">Oren</forename><forename type="middle">Michele</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cafarella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,392.76,667.93,131.65,11.04;15,70.92,679.45,145.43,11.04">Proceedings of the 21st National Conference on Artificial Intelligence</title>
		<meeting>the 21st National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,74.69,696.97,449.72,11.04;15,70.92,708.49,170.37,11.04" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,240.84,696.97,207.97,11.04">The TREC-8 Question Answering Track Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,466.80,696.97,57.61,11.04;15,70.92,708.49,79.72,11.04">Text Retrieval Conference TREC-8</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="83" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,74.69,726.01,449.65,11.04;15,70.92,737.53,259.14,11.04" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,310.80,726.01,213.54,11.04;15,70.92,737.53,102.96,11.04">Reading Comprehension Tests for Computer-based Understanding Evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">L</forename><surname>Wellner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">W</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Greiff</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hirschman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,180.60,737.53,63.45,11.04">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="305" to="334" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,74.68,71.05,449.65,11.04;16,70.92,82.57,453.54,11.04;16,70.92,93.97,453.46,11.04;16,70.92,105.49,453.55,11.04;16,70.92,117.13,453.69,11.04;16,70.92,128.53,262.89,11.04" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,263.04,82.57,261.42,11.04;16,70.92,93.97,105.94,11.04">Overview of ResPubliQA 2009: Question Answering Evaluation over European Legislation</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><forename type="middle">Pamela</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">Richard</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Corina</forename><surname>Álvaro Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Iñaki Alegria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">Petya</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,100.80,105.49,423.67,11.04;16,70.92,117.13,113.55,11.04">Multilingual Information Access Evaluation Vol. I Text Retrieval Experiments. Workshop of the Cross-Language Evaluation Forum</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</editor>
		<editor>
			<persName><surname>Kurimo</surname></persName>
		</editor>
		<editor>
			<persName><surname>Th</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">D</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">A</forename><surname>Mostefa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">G</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><surname>Roda</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF; Corfu. Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009-09-30">2009. 30 September -2 October. 2010</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers. Lecture Notes in Computer Science 6241</note>
</biblStruct>

<biblStruct coords="16,74.69,146.05,449.77,11.04;16,70.92,157.57,453.44,11.04;16,70.92,169.09,199.05,11.04" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,240.48,146.05,179.23,11.04">A Simple Measure to Assess Non-response</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alvaro</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,441.00,146.05,83.46,11.04;16,70.92,157.57,453.44,11.04;16,70.92,169.09,21.57,11.04">Proceedings of 49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)</title>
		<meeting>49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)<address><addrLine>Portland. Oregon. USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">June 19-24. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,186.61,445.16,11.04;16,70.92,198.01,453.49,11.04;16,70.92,209.53,235.29,11.04" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,453.48,198.01,70.93,11.04;16,70.92,209.53,147.30,11.04">Building Watson: An Overview of the DeepQA Project</title>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">Eric</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jennifer Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>James Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>David Gondek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aditya</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">J</forename><surname>Adam Lally</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>William Murdock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nico</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Schlaefer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Welty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,224.64,209.53,50.05,11.04">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,227.05,445.16,11.04;16,70.92,238.57,453.46,11.04;16,70.92,250.09,134.37,11.04" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,265.20,227.05,259.26,11.04;16,70.92,238.57,41.54,11.04">Annotating Modality and Negation for a Machine Reading Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Roser</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,123.60,238.57,235.91,11.04">CLEF 2011 Labs and Workshop -Notebook Papers</title>
		<title level="s" coord="16,124.20,250.09,76.80,11.04">Online Proceedings</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date>19-22 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,267.61,445.16,11.04;16,70.92,279.01,453.57,11.04;16,70.92,290.53,203.97,11.04" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,267.72,267.61,256.74,11.04;16,70.92,279.01,144.23,11.04">Graph-based Word Clustering Applied to Question Answering and Reading Comprehension Tests</title>
		<author>
			<persName coords=""><forename type="first">Juan</forename><surname>Martinez-Romo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lourdes</forename><surname>Araujo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,223.56,279.01,219.23,11.04">CLEF 2011 Labs and Workshop -Notebook Papers</title>
		<title level="s" coord="16,193.92,290.53,76.69,11.04">Online Proceedings</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date>19-22 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,308.05,445.16,11.04;16,70.92,319.57,453.55,11.04;16,70.92,331.09,351.33,11.04" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,488.28,308.05,36.18,11.04;16,70.92,319.57,298.37,11.04">Question Answering for Machine Reading Evaluation on Romanian and English</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandru-Lucian</forename><surname>Gînscă</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Moruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Trandabăt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Husarciuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,378.12,319.57,146.35,11.04;16,70.92,331.09,66.97,11.04">CLEF 2011 Labs and Workshop -Notebook Papers</title>
		<title level="s" coord="16,341.16,331.09,76.80,11.04">Online Proceedings</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date>19-22 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,348.61,445.16,11.04;16,70.92,360.01,453.57,11.04;16,70.92,371.53,203.97,11.04" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,147.60,348.61,376.86,11.04;16,70.92,360.01,152.33,11.04">Cosine similarity as Machine Reading Technique. Question Answering for Machine Reading Evaluation on Romanian and English</title>
		<author>
			<persName coords=""><forename type="first">Gaurav</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,231.12,360.01,213.35,11.04">CLEF 2011 Labs and Workshop -Notebook Papers</title>
		<title level="s" coord="16,193.92,371.53,76.69,11.04">Online Proceedings</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date>19-22 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,389.05,445.25,11.04;16,70.92,400.57,418.05,11.04" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,158.64,389.05,286.67,11.04">Retrieval-based Question Answering for Machine Reading Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Suzan</forename><surname>Verberne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,453.24,389.05,71.31,11.04;16,70.92,400.57,133.91,11.04">CLEF 2011 Labs and Workshop -Notebook Papers</title>
		<title level="s" coord="16,408.00,400.57,76.70,11.04">Online Proceedings</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date>19-22 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,418.09,445.23,11.04;16,70.92,429.49,453.67,11.04;16,70.92,441.01,351.33,11.04" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,141.48,429.49,236.16,11.04">JU_CSE_TE: System Description QA4MRE@CLEF 2011</title>
		<author>
			<persName coords=""><forename type="first">Partha</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pinaki</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Somnath</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bidhan</forename><surname>Chandra Pal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,385.32,429.49,139.27,11.04;16,70.92,441.01,66.97,11.04">CLEF 2011 Labs and Workshop -Notebook Papers</title>
		<title level="s" coord="16,341.16,441.01,76.80,11.04">Online Proceedings</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date>19-22 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,458.53,445.16,11.04;16,70.92,470.05,453.58,11.04;16,70.92,481.57,134.37,11.04" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,368.28,458.53,156.18,11.04;16,70.92,470.05,47.60,11.04">Dependency-Based Answer Validation for German</title>
		<author>
			<persName coords=""><forename type="first">Svitlana</forename><surname>Babych</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Henn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Pawellek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Padò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,129.84,470.05,231.97,11.04">CLEF 2011 Labs and Workshop -Notebook Papers</title>
		<title level="s" coord="16,124.20,481.57,76.80,11.04">Online Proceedings</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date>19-22 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,499.09,445.07,11.04;16,70.92,510.49,453.46,11.04;16,70.92,522.01,51.21,11.04" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="16,269.64,499.09,250.35,11.04">Question Answering for Machine Reading with Lexical Chain</title>
		<author>
			<persName coords=""><forename type="first">Ling</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,70.92,510.49,214.79,11.04">CLEF 2011 Labs and Workshop -Notebook Papers</title>
		<title level="s" coord="16,497.16,510.49,27.22,11.04;16,70.92,522.01,46.94,11.04">Online Proceedings</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date>19-22 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,79.30,539.53,445.13,11.04;16,70.92,551.05,418.05,11.04" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="16,277.08,539.53,171.12,11.04">The LogAnswer Project at QA4MRE 2011</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glockner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bjorn</forename><surname>Pelzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tiansi</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,455.40,539.53,69.03,11.04;16,70.92,551.05,133.91,11.04">CLEF 2011 Labs and Workshop -Notebook Papers</title>
		<title level="s" coord="16,408.00,551.05,76.70,11.04">Online Proceedings</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date>19-22 September</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
