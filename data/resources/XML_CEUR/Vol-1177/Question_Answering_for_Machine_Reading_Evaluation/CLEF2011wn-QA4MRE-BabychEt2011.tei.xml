<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.57,116.95,294.54,12.62;1,279.23,134.89,56.90,12.62">Dependency-Based Answer Validation for German</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.27,172.56,71.26,8.74;1,214.53,170.98,1.36,6.12"><forename type="first">Svitlana</forename><surname>Babych</surname></persName>
							<email>svitlana.babych@ims.uni-stuttgart.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="department" key="dep2">Institut für Computerlinguistik</orgName>
								<orgName type="institution" key="instit1">Universität Stuttgart</orgName>
								<orgName type="institution" key="instit2">Universität Heidelberg</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,225.19,172.56,70.60,8.74;1,295.79,170.98,6.54,6.12"><forename type="first">Alexander</forename><surname>Henn #</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="department" key="dep2">Institut für Computerlinguistik</orgName>
								<orgName type="institution" key="instit1">Universität Stuttgart</orgName>
								<orgName type="institution" key="instit2">Universität Heidelberg</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.92,172.56,56.73,8.74;1,365.66,170.98,6.54,6.12"><forename type="first">Jan</forename><surname>Pawellek #</surname></persName>
							<email>pawellek@cl.uni-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="department" key="dep2">Institut für Computerlinguistik</orgName>
								<orgName type="institution" key="instit1">Universität Stuttgart</orgName>
								<orgName type="institution" key="instit2">Universität Heidelberg</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,398.16,172.56,66.88,8.74"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
							<email>pado@cl.uni-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="department" key="dep2">Institut für Computerlinguistik</orgName>
								<orgName type="institution" key="instit1">Universität Stuttgart</orgName>
								<orgName type="institution" key="instit2">Universität Heidelberg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.57,116.95,294.54,12.62;1,279.23,134.89,56.90,12.62">Dependency-Based Answer Validation for German</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3F592E7D246CD27FBF38C323A3C6C14F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>QA4MRE</term>
					<term>German</term>
					<term>machine reading</term>
					<term>question answering</term>
					<term>logical inference</term>
					<term>knowledge extraction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the Heidelberg contribution to the CLEF 2011 QA4MRE task for German. We focus on the objective of not using any external resources, building a system that represents questions, answers and texts as formulae in propositional logic derived from dependency structure. Background knowledge is extracted from the background corpora using several knowledge extraction strategies. We answer questions by attempting to infer answers from the test documents complemented by background knowledge, with a distance measure as fall-back. The main challenge is to specify the translation from dependency structure into a logical representation. For this step, we suggest different rule sets and evaluate various configuration parameters that tune accuracy and coverage. All of runs exceed a random baseline, but show different coverage/accuracy profiles (accuracy up to 44%, coverage up to 65%).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">The CLEF 201QA4MRE task</head><p>The long-term goal of NLP is to build computer systems that can communicate with humans. Arguably, an important part of this enterprise is the development of semantic analysis components which allow systems to understand the information contained in the text and reason with it. This task is often called machine reading <ref type="bibr" coords="1,172.36,537.56,14.90,8.74" target="#b14">[15]</ref>. CLEF 2011 introduced a track on machine reading, phrasing it as a multiple-choice question answering task <ref type="bibr" coords="1,332.92,549.52,15.65,8.74" target="#b20">[21]</ref> under the name of QA4MRE. This represents a simplified version of full machine reading in the sense that systems do not have to generate answers, but only have to discriminate among a given set of possible answers (answer validation <ref type="bibr" coords="1,357.66,585.38,15.00,8.74" target="#b17">[18]</ref>). At the same time, the QA4MRE task was designed in a way that emphasized the role of developing a comprehensive understanding of a small text. Questions were drawn from three domains (AIDS, Climate Change, and Music and Society). For each domain, there was a small set of test documents supposed to contain the answer, and a very large background collection of documents gathered from the web. Questions were designed to involve considerable surface variation compared to the texts, and thus</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>Was ist das Ziel von UNAIDS? What is the goal of UNAIDS? Answer Candidate 1 ein Abkommen mit dem Gesundheitsminister zu treffen to come to an agreement with the minister of health Answer Candidate 2 ein Konzert mit dem African Children's Choir zu geben to give a concert with the African Children's Choir Answer Candidate 3 zu verhindern, dass HIV-positive Frauen schwanger werden to avoid that HIV-positive women become pregnant Answer Candidate 4 UNAIDS' Außenwirkung zu vergrößern to increase publicity for UNAIDS Answer Candidate 5 zu vermeiden, dass HIV von Müttern auf Kinder übertragen wird <ref type="bibr" coords="2,136.16,239.33,34.86,7.86">(correct)</ref> to avoid that HIV is transmitted from mothers to children Answer Sentence Die Botschaft, die UNAIDS derzeit in der Welt verbreitet, ist, dass wir es schaffen wollen, die Übertragung des Virus von Mutter zu Kind bis 2015 praktisch zu eliminieren.</p><p>The message that UNAIDS is currently spreading is that we want to manage to virtually eliminate the transmission of the virus from mother to child until 2015. Table <ref type="table" coords="2,175.00,320.69,4.13,7.89">1</ref>. An example question with answer candidates and the answer sentence not to be answerable by simple lexical matching. Answer validation in this setting can be seen as a textual inference task <ref type="bibr" coords="2,311.91,375.62,10.73,8.74" target="#b7">[8]</ref> with potentially complex inference steps: An answer candidate answers a question if the statement obtainable by substituting the answer into the question can be inferred from the text. For the first (primary) of ten QA4MRE runs, the use of external resources like WordNet or paraphrase resources was prohibited. This constraint emphasizes the need for deeper analysis of the texts to (a) consolidate the semantic representations of the test documents and (b) acquire additional knowledge from the background collection. Table <ref type="table" coords="2,207.20,459.31,4.88,8.74">1</ref> shows an example question with its answer candidates and the sentence providing the answer in the test document. The example demonstrates the properties discussed above. None of the answer candidates are contained literally in the answer. To build an inference chain from the text to the correct answer candidate, systems need to acquire the knowledge that "having a goal" has something to do with "wanting to", that "avoiding" something can mean to "eliminate" it; that "HIV" is a "virus", and so forth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Strategy and Architecture</head><p>At the time of QA4MRE 2011, there was no full-fledged general-purpose textual entailment system for German, only for the related task of question grading <ref type="bibr" coords="2,134.77,609.29,14.90,8.74" target="#b13">[14]</ref>. We therefore approached question validation with a fairly simple system. In the spirit of <ref type="bibr" coords="2,202.42,621.25,10.08,8.74" target="#b8">[9]</ref>, we build mainly on dependency-based normalized syntactic representations (predicate-argument relations) which abstract away from the surface structure, complemented by inference rules encoding synonymy and hyponymy knowledge acquired from the background collection. More specifically,  we assume that every relevant piece of knowledge can be expressed as a ternary relation. This includes both syntagmatic relations from actual text, e.g., dependency relations (Peter,subj,sleeps) and paradigmatic relations, i.e. type-level semantic relations (natural gas,hypernym,energy source) and semantic relatedness (book,relatedTo,story) acquired from the background corpus. We treat complex structures (like dependency graphs) simply as sets of such binary relations, and use the YAML file format as a universal exchange format among modules <ref type="bibr" coords="3,459.27,392.74,9.96,8.74" target="#b2">[3]</ref>.</p><p>The overall structure of the system is shown in Figure <ref type="figure" coords="3,388.82,404.83,3.86,8.74" target="#fig_0">1</ref>. It consists of three modules. In the first module, all types of language input (questions, answers, test and background documents) are preprocessed, dependency-parsed, and normalized, to meet the first need outlined in the introduction (consolidated representation). The second module extracts semantic relations from the background document collection based on distributional similarity and shallow rules, thus addressing the second need from the introduction (acquisition of additional information). Finally, the third module attempts to infer each answer candidate, combined with the question, from the test document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Modules in Detail</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Syntactic Analysis and Normalization</head><p>This module creates dependency-syntactic structures for the German input texts (questions, answers, test and background documents). We first perform sentence splitting with the regular expression-based tokenizer by Sebastian Nagel<ref type="foot" coords="3,456.30,600.56,3.97,6.12" target="#foot_0">1</ref> and then run the MATE dependency parsing toolkit <ref type="bibr" coords="3,355.77,614.09,10.73,8.74" target="#b3">[4]</ref> which is among the best available dependency parsers for German. After parsing, we perform a number of normalization steps whose general motivation is to bring the dependency output of the parser closer to our semantic intuitions, similar to <ref type="bibr" coords="4,376.34,317.90,14.32,8.74" target="#b12">[13]</ref>. Our goal is to make the representations of answers and their supporting textual evidence more similar to one another, abstracting away from surface variability. All normalization steps are realized as dependency tree transformations.</p><p>Specifically, we deal with the four most frequent phenomena that we identified. The first one is passive sentences, which we transform into an active representation, including changing the edge labels. The second is prepositional phrases, where we delete the preposition node and encode this information in the edge label. The third one is coordinations, where we expand the second conjunct which is by default only realized in a reduced form. These three phenomena can be treated in German fairly similar to English <ref type="bibr" coords="4,319.56,437.68,14.32,8.74" target="#b12">[13]</ref>. However, our final phenomenon -German verb complexes -is language-specific and warrants discussion. In English declarative sentences, verb complexes with one finite and (at least) one infinite verb -e.g., auxiliary + participle, or modal + infinitive -are, as a rule, realized contiguously. In contrast, German main clauses must realize the finite verb in second position and the infinite verb in clause-final position. The MATE parser is trained on a version of the German TIGER treebank <ref type="bibr" coords="4,405.76,509.41,10.36,8.74" target="#b5">[6]</ref> converted into dependencies. According to the TIGER annotation guidelines, sentence-initial arguments are attached to the second-position verb, while all others are attached to the clause-final verb. The left-hand side of figure <ref type="figure" coords="4,372.14,545.27,5.08,8.74" target="#fig_1">2</ref> shows the dependency tree for "Der Arzt hat eine Operation durchgeführt", where the sentence-initial subject (doctor) is attached to the finite auxiliary and the object (operation) to the participle. Following our our semantic intuition, we move all arguments to attach to the semantic head of the sentence (cf. the tree on the right-hand side).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Knowledge Extraction from the Background Collection</head><p>We decided to concentrate on extracting two types of inference rules from the background corpus, namely hyponymy and synonymy. These two relations frequently contribute to bridging the "lexical gap" between answer candidates and textual evidence, and can be acquired using well-established methods. The first approach we followed was the extraction of hypernym-hyponym pairs with so-called Hearst Patterns <ref type="bibr" coords="5,273.74,382.03,14.90,8.74" target="#b10">[11]</ref>. Their adaptation to German <ref type="bibr" coords="5,427.17,382.03,15.81,8.74" target="#b9">[10]</ref> consists of six regular expression patterns (listed in table 2) which we applied to the background corpora. Our second approach to knowledge extraction was based on vector space models <ref type="bibr" coords="5,239.61,417.90,14.76,8.74" target="#b19">[20]</ref>. Given the large size of the background corpus, we constructed dependency-based vectors to take advantage of their better ability to identify close semantic similarity when sparsity is not an issue <ref type="bibr" coords="5,423.68,441.81,14.74,8.74" target="#b15">[16]</ref>. To limit computation time and memory consumption, all words occurring at least 50 times in one of our task's background corpora have been included as target words in the vector space. To extract synonyms from the spaces, we used a range of symmetrical similarity measures to compute vector similarity, including Cosine, Dice, GCM, Hindle, Jaccard, and Lin, and added all pairs above some threshold, optimized for recall, to a database of inference rules (cf. Table <ref type="table" coords="5,409.23,513.54,3.87,8.74" target="#tab_2">3</ref>).</p><p>In contrast to synonymy, hyponymy is an asymmetrical relationship which should therefore not be amenable to extraction with symmetrical similarity measures. We therefore identified hypernymy using the asymmetrical Balanced Average Precision similarity measure <ref type="bibr" coords="5,304.21,561.36,14.90,8.74" target="#b11">[12]</ref>. This measure judges the relevance of the broader term's features for the narrower term (based on feature ranks) and penalizes short (e.g., vague) vectors. Again, we added pairs above a certain threshold (Table <ref type="table" coords="5,209.84,597.23,4.43,8.74" target="#tab_2">3</ref>) to a database of inference rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>We take the classical "logical inference" approach to deciding whether an answer follows from the text <ref type="bibr" coords="5,225.06,657.11,9.76,8.74" target="#b4">[5]</ref>. We represent the answer candidate C, the test document T , and the background knowledge base B (i.e., our inference rules), as logical formulae and test the validity of the following formula:</p><formula xml:id="formula_0" coords="6,281.52,152.83,199.08,8.74">(T ∧ B) C<label>(1)</label></formula><p>The main problem is how to represent T and C in logical terms. In an ideal world, we would obtain a complete representation of the sentence meaning as provided by a full syntax-semantics interface. However, wide-coverage translation of natural language into logics is still essentially an open research problem. Additionally, if T and A are represented by strong logical representations, then we need an equally strong background knowledge base B which can, for example, license paraphrastic variation between T and C (invent X → be the first to think of X ). This problem has been approached e.g. by acquiring meaning postulates from WordNet <ref type="bibr" coords="6,177.75,269.35,14.82,8.74" target="#b18">[19]</ref>, but in the absence of manually vetted knowledge sources, as we assume, Formula (1) will be valid for only a small fraction of all cases <ref type="bibr" coords="6,442.17,281.31,9.96,8.74" target="#b4">[5]</ref>.</p><p>We address this problem by deriving weaker representations of the linguistic structures that essentially encode their dependency relations in propositional logic, experimenting with different parameters in the process. We take advantage of the fact that we have to solve only a multiple-choice task by searching for construction methods in which exactly one of the answers can be proved: if no answer can be proved, the representation is too strong; if more than one answer can be proved, it is too weak. The names of the binary parameters of this process are marked in boldface and will be used in the next chapter to explain the evaluation results.</p><p>Turning test documents into propositional formulae T . Our goal is to translate dependency structures into propositional logical formulae. The first, optional, step is to prune the dependency structures by removing nodes with no semantic content (option DropPOS). If this option is selected, we remove all leaf nodes with the parts of speech ART (article) and APPR (prepositions<ref type="foot" coords="6,439.94,464.21,3.97,6.12" target="#foot_1">2</ref> ) as well as PWAT, PWAV, PWS (question words -see below for details). The next step is to decide on the shape of the literals. We developed four different rulesets which we will demonstrate on the example of the node "durchführen/perform" from the right-hand side of Figure <ref type="figure" coords="6,262.74,513.60,4.94,8.74" target="#fig_1">2</ref> and its two outgoing edges (SB to "Arzt/doctor" and OA to "Operation/operation").</p><p>-The Unary ruleset builds a literal for each dependency edge that consists only of the head and the dependent. For the above-mentioned edges, we obtain perform(doctor) and perform(operation). -The LabeledUnary ruleset builds a literal for each dependency edge like Unary, but also includes the edge label. For the example: SB(perform,doctor) and OA(perform,operation). -The Binary ruleset builds a literal for each pair of dependency edges with the same head without taking edges into account: perform(doctor,operation).</p><p>-The Unrestricted ruleset turns each dependency subtree of depth one (i.e., each head with all of its dependents) into one n-ary literal. For the current example, the result is identical to the output of Binary.</p><p>The logical representation of a test document is the conjunction of all its literals. Several rulesets are activated, which leads to a certain amount of redundancy.</p><p>Turning a question-answer pair into a propositional formula C. In QA4MRE, answers are usually not complete propositions, but only words or short phrases. Such answers must be combined with the questions in order to obtain a complete representation of the proposition conveyed by the answer candidate, such as "Is it the goal of UNAIDS to increase publicity for itself" for answer candidate 4 in Table <ref type="table" coords="7,260.21,264.27,3.87,8.74">1</ref>. We therefore translate the linguistic realizations of question and answer candidates into propositional logic as detailed above, and combine them into a single formula in a way that is influenced by two parameters. The first parameter is WhSubstitution (question word substitution) concerns the handling of single-word answers. If this option is true, then we simply replace the interrogative pronoun ("who"/"what") in the question with the answer word. If it is false, then we attempt to reconstruct a logical representation, i.e., a literal, for the answer. Recall that our literals must correspond to dependency edges. We therefore search in the dependency representation of the test document for the most frequent head which had the answer word as its argument. As an example, assume that the test document mentioned medicine being an academic discipline. If the question concerns somebody's occupation, and the answer is "medicine", we then obtain as representation for this answer candidate the literal discipline(medicine), corresponding to the more detailed multi-word answer candidate "the discipline of medicine". We also construct answers by searching for heads which had hyponyms and synonyms of the answer as their argument. Additionally we also construct an answer by combining the answer word with the question's head (which in general leads to very similar results as question word substitution).</p><p>The second parameter is ANDonly which decides how the literals of question and answer candidate are combined into the formula C. If ANDonly is true, C is the conjunction of all literals of answers in the answer candidate, a representation that is optimized for precision. If ANDonly is false, representations for the answer and question are first formed as disjunctions over the respective literals, and then combined by conjunction. This formulae is not very precise (since it is valid as soon as one literal of question and answer each is valid), but it may be informative enough for a multiple-choice selection task (cf. the discussion above).</p><p>Proving validity and the role of background knowledge. Finally, we use the theorem prover CVC3 <ref type="bibr" coords="7,257.09,616.36,10.73,8.74" target="#b1">[2]</ref> to test the validity of the resulting formulae. <ref type="foot" coords="7,476.32,614.79,3.97,6.12" target="#foot_2">3</ref>Run ID 1 2 3 4 5 6 7 8 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unary T T T T T T T T T LabeledUnary T T T F F F F F T Binary T T T T T T T F T Unrestricted T T T T T T T F T WhSubstitution</head><formula xml:id="formula_1" coords="8,215.77,173.57,180.75,78.84">F F F F T F F T T ANDonly F F F F F T T F F AllKnowledgeAtOnce T T F F F F F F F Threshold F F F F F F T F F DropPOS T T T T T T T F T RankUnprovable F F F F F F F F T Table 4.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submitted runs and their features</head><p>As discussed above, the basis of our approach is to attempt to prove the five formulae in parallel for a growing body of background knowledge B, i.e., synonymy and hyponymy relations. Specifically, we divide the relations generated by the knowledge extraction component (Section 3.2) into five categories according to decreasing semantic similarity. In other words, we first attempt to prove the answer candidates when taking only fairly certain background knowledge into account, and if this is not possible, we proceed to less certain background knowledge. As soon as at least one of the answers can be proved, we stop. This process is skipped by setting the option AllKnowledgeAtOnce, which adds the complete body of background knowledge at once. At the end of this process, we can have different outcomes. If none of the answers can be proved, we do not make a prediction. If exactly one answer can be proved, we return this answer. If more than one answer can be proved, we require a tie-breaker to choose among these candidates. For this end, we score each derivation based on the similarity scores of the background knowledge that was employed, effectively interpreting similarity scores as confidence values. Questions where more than one answer receives exactly the same score must however still be left unanswered. We also experimented with the option Threshold which introduced a fixed threshold that answer scores had to exceed.</p><p>Finally, we experimented with a fallback mode called RankUnprovable where answers C that could not be proved (and were thus normally discarded) were included in the output of the system. The decision between these answers was made based on a simple distance metric that quantifies the "closeness" of answer candidates to test documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We submitted a total of nine runs, as permitted by the QA4MRE guidelines. Preprocessing and knowledge extraction were held constant across runs; we only varied the parameters of the inference step described in Section 3.3, as shown in Table <ref type="table" coords="8,161.74,657.11,3.86,8.74">4</ref>. We originally planned to vary only one parameter at a time to observe  the impact of parameters, but had to deviate from this idea to sample a larger part of the parameter space. Consequently, we decided to use a quantitative approach to analyze the influence of run parameter on run performance. We analyzed our runs with a logistic regression model which have previously been used successfully to explain the influence of features in data <ref type="bibr" coords="9,426.68,327.63,10.11,8.74" target="#b6">[7]</ref>. We used the runs' properties (cf. Table <ref type="table" coords="9,271.98,339.58,4.52,8.74">4</ref>) as predictors x, and the run performance as the response variable y to be predicted. Logistic regression models have the form</p><formula xml:id="formula_2" coords="9,226.21,371.90,250.14,26.65">p(y = 1) = 1 1 + e -z with z = i β i x i (<label>2</label></formula><formula xml:id="formula_3" coords="9,476.35,378.63,4.24,8.74">)</formula><p>where p is the probability of the response variable taking some value and β i the coefficient assigned to predictor x i . Model estimation sets the parameters β to maximize the likelihood of the data. In the current setup, we are interested in analyzing the predictor weights: for each predictor x i , we can test the hypothesis that it significantly contributes to predicting the response. We first attempted to fit a model to the official evaluation metric of QA4MRE 2011, namely C@1. However, we failed to find any significant predictors. While the fact that we have relatively few runs can definitely play a role, we attribute this failure to the property of C@1 to combine coverage and accuracy into a single figure of merit <ref type="bibr" coords="9,200.03,518.42,14.51,8.74" target="#b16">[17]</ref>. We found that we were more successful by investigating the influence of our predictors for coverage and accuracy separately (cf. Table <ref type="table" coords="9,460.24,530.37,3.87,8.74" target="#tab_3">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Predicting coverage</head><p>Column 5 in Table <ref type="table" coords="9,217.84,585.38,4.88,8.74" target="#tab_3">5</ref> (Coverage) shows the coverage figures for our different runs. We see that the runs fall into three groups: relatively high coverage (run 9, 68%), medium coverage (runs 2-5 and 8, 40-55%), and low coverage (runs 1, 6, and 7, &lt; 30%). The model with which we analysed these coverage figures revealed two significant predictors with a negative influence on coverage when set to true, namely ANDonly and AllKnowledgeAtOnce. There was also one highly significant predictor with a positive influence on coverage, namely RankUnprovable. These results tie in well with our intuition about the inference approach that we use. Our model does does not make a prediction if one of two complementary situations take place: (a) none of the answers can be inferred, or (b) there are two or more answers with the same score among which we cannot decide. Since ANDonly means that the answer candidate C is a pure conjunction of the question and answer literals, setting it to true results in more cases of type (a). Conversely, RankUnprovable attempts to answer also questions where none of the answers is provable as a whole, reducing the number of (a) cases. Finally, if AllKnowledgeAtOnce is true, more ties occur, thus more cases of type (b).</p><p>To gain a better qualitative understanding of these cases, we make the simplifying, but largely warranted, assumption that our runs coincide on the questions that they can and cannot cover. This gives rise to four classes of questions with respect to coverage, shown on the left-hand side of Figure <ref type="figure" coords="10,447.60,361.92,3.80,8.74" target="#fig_3">3</ref>. Class 1 consists of questions that are always covered, even in an ANDonly setting. An example<ref type="foot" coords="10,170.46,384.26,3.97,6.12" target="#foot_3">4</ref> is question 1-1-3, What country is Nelson Mandela from?.</p><p>Class 2 consists of questions where the conjunction of answer and question cannot be inferred. An example is question 2-7-10, What could happen if the amount of CO2 in the atmosphere is not reduced?. This question, paired with the correct answer, results in a fairly complex formula for C, with more than ten literals. If not all literals are connected by conjunctions (ANDonly), if cannot be proved, but it can if ANDonly is set to false.</p><p>Class 3 coverts cases that cannot be decided by the prover, but can be decided by the distance metric when RankUnprovable is true. An example is 2-3-5, What could be a consequence of the reduction of arctic ice?. We cannot prove this question together with any answer because the question contains a description of a causal relation, "a consequence of", which is not expressed in the same way in the test document. The distance metric can, however, make a prediction.</p><p>Finally, there are questions (Class 4) which none of our runs covered, such as 1-1-5, What is Annie Lennox' profession? The correct answer would be musician. However, none of the answers can be proved, which means that runs 1 to 8 abstain from answering. Run 9 attempts to apply the distance metric, but runs into a distance tie between the answers musician and dancer both of which are equally close to the test document. A further contributing factor for this question was that the parser returned an incorrect analysis of the question, which directly led to an inappropriate representation for C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Predicting accuracy</head><p>The fourth column in Table <ref type="table" coords="11,258.13,141.65,4.94,8.74" target="#tab_3">5</ref> lists accuracy figures for the different runs, where accuracy is computed as the ratio of correctly answered questions to answered questions. Again, we see three groups, although the overall accuracy is quite low, and the groups are much closer together. One run has very low accuracy (run 5 at 23%), most runs show middling accuracy (runs 1-4, 8, 9) and two runs have fair accuracy (runs 6 and 7, the best run, with 44% accuracy). In terms of accuracy, all of our runs beat a random baseline (at 20%). Due to the fact that the accuracy figures are clustered together more closely, our logistic regression model yielded only one predictor with significant (positive) impact on accuracy, namely ANDonly. As discussed above, ANDonly leads to stronger answer representations which reduce the risk for false positives. Thus, ANDonly is a true precision/recall trade-off: activating it yields higher-precision runs, deactivating it higher-recall runs. Another factor which is not significant in the regression but which we consider important to explain the performance of run 5 is WhSubstitution (question word substitution). The heuristic we employed to deal with single-word answers for interrogative pronouns (cf. Section 3.3) yielded only mixed results: too frequently, the occurrences of the answer candidate in the test document that form the basis for its interpretation were not related to the question.</p><p>In sum, we can distinguish again, though at the risk of oversimplification, four groups of questions, shown on the right-hand side of Figure <ref type="figure" coords="11,409.11,381.10,3.80,8.74" target="#fig_3">3</ref>: (1), those that are answered correctly by all runs; (2), those where question word substitution leads to wrong answers; (3), those that can be answered correctly if ANDonly is true; and (4), those that no run manages to answer correctly.</p><p>An interesting example of group ( <ref type="formula" coords="11,294.90,429.27,4.16,8.74">3</ref>) is question 1-3-10, Who wrote 'People do stupid things -that's what spreads HIV' ?, with the correct answer 1: Elizabeth Pisani. Two other answer candidates show a lot of overlap with the correct answer (a friend of Elizabeth Pisani's and Elizabeth Pisani's brother ). If ANDonly is false, and consequently not the complete answer has to be proved, some of the runs mistakenly return one of the two confounds as the correct answer.</p><p>An analysis of the group (4) errors made by run 7, our highest-accuracy run, highlights the limitations of our current system architecture. One important limitation is the fact that we consider literals (i.e., individual dependency edges) largely in isolation. This makes our system vulnerable to questions where (parts of) answers are already contained in the test document. For example, one of the answer candidates for question 2-5-4, Why are there more wildfires now?, is Global warming, a term which occurs in the test document several times. Since for why-questions, we in effect try to prove a conjunction of question and answer, we check whether the test document contains There are more wildfires and Global warming. This is the case, and we return Global warming as the (incorrect) answer -the correct answer would have been Less water.</p><p>A second frequent problem is the inadequacy of our background knowledge extraction. For example, question 2-5-2 is Where is the third largest ice mass in the world?, with the correct answer candidate being Asia. The test document mentions only the Himalaya (mountains). If we had had access to the meronymy (or location) relation the Himalaya is a part of (or located in) Asia, we would have been able to prove this answer, but we did not attempt to extract meroynym relations. Consequently, our systems attempted to prove increasingly weaker versions of the answers and ended up with incorrect answers. More generally speaking, the problem is that our current system has a very impoverished notion of background knowledge. Even within the covered relation (hyponymy), coverage is far from perfect. Outside hyponymy, a large number of relevant semantic relations are excluded both at the lexical level (e.g., meronymy or entailment relations between verbs) and at the lexico-syntactic level (paraphrases), all of which can play an important role in entailment <ref type="bibr" coords="12,343.95,239.54,9.96,8.74" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In our contribution to the CLEF 2011 QA4MRE task, we focussed on answer validation completely without the use of any external knowledge resources, merely extracting some background knowledge from the provided background corpora. Our approach was fairly straightforward and consisted in a fairly direct translation of dependency trees into proposional logic formulae, with each edge contributing one literal. Several parameters determine the strength of the resulting formulae. We implemented a back-off proving setup where we proceeded from stronger to weaker logical representations until we were able to prove at least one of the answers. Cases of ties are resolved by a simple distance metric.</p><p>We were happy to find that our best runs showed accuracies of around 40%; however there is a clear inverse relation between coverage and accuracy, with the best runs showing the lowest coverage. This clearly shows that improvements are necessary with respect to both the precision and the recall of our approach, and our data analysis has yielded a number of clear directions for improvements. As for recall, we mainly need to improve the knowledge acquisition techniques with which we extract background knowledge from large corpora and scale them up to a larger set of semantic relations, notably paraphrase. With regard to precision, we feel that we need to improve the handling of different question types and go beyond simply matching the linguistic material in the question with the test document. In particular for causal questions, our current approach is much too restricted. More generally, we would like to generalize our current text normalization step into a more sophisticated syntax-semantics interface that maps input texts onto "more semantic" knowledge representation structures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,182.28,287.00,250.79,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Structure of the Heidelberg system for QA4MRE 2011</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,261.80,345.82,7.89;4,134.77,272.79,347.61,7.86;4,134.77,283.75,105.95,7.86;4,345.70,121.33,126.30,125.70"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. German sentence in perfective tense (auxiliary + participle): "Der Arzt hat eine Operation durchgeführt The doctor has performed an operation". Left: parser output. Right: normalized version.</figDesc><graphic coords="4,345.70,121.33,126.30,125.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,149.12,118.38,301.54,7.86"><head></head><label></label><figDesc>Run ID # Correct (c) # Answered (a) Accuracy (c/a) Coverage (a/120) C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,160.21,183.10,294.93,7.89"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Question classes according to coverage (left) and accuracy (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.39,118.38,347.99,176.75"><head>Table 2 .</head><label>2</label><figDesc>The patterns used to extract hyponymy relations. The patterns assume that each word is followed by a slash and its part of speech, such as Energie/NN -energy/N.</figDesc><table coords="5,149.66,118.38,312.50,176.75"><row><cell>Regular Expression Pattern</cell><cell></cell><cell cols="2">hypernym RE group</cell></row><row><cell cols="3">(\S+)/N. wie/\S+ zum/\S+ Beispiel/\S+ (\S+)/N.</cell><cell>1</cell></row><row><cell cols="2">(\S+)/N. ist/\S+ eine?/\S+ (\S+)/N.</cell><cell></cell><cell>2</cell></row><row><cell cols="2">(\S+)/N. wie/\S+ etwa/\S+ (\S+)/N.</cell><cell></cell><cell>1</cell></row><row><cell cols="2">(\S+)/N. einschließlich/\S+ (\S+)/N.</cell><cell></cell><cell>1</cell></row><row><cell cols="2">(\S+)/N. und/\S+ andere/\S+ (\S+)/N.</cell><cell></cell><cell>2</cell></row><row><cell cols="2">(\S+)/N. oder/\S+ andere/\S+ (\S+)/N.</cell><cell></cell><cell>2</cell></row><row><cell>Similarity Measure</cell><cell cols="3">Threshold Similarity Measure Threshold</cell></row><row><cell>Cosine distance</cell><cell>0.8</cell><cell>Dice coefficient</cell><cell>0.7</cell></row><row><cell>GCM</cell><cell>0.7</cell><cell>Hindle</cell><cell>300.0</cell></row><row><cell>Jaccard</cell><cell>0.7</cell><cell>Lin</cell><cell>0.7</cell></row><row><cell>Balanced Average Precision</cell><cell>0.6</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,216.87,303.42,181.62,7.89"><head>Table 3 .</head><label>3</label><figDesc>Thresholds for similarity measures.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,139.07,118.38,337.22,123.57"><head>Table 5 .</head><label>5</label><figDesc>Results for runs (120 questions). Best result for each statistic in boldface.</figDesc><table coords="9,450.66,118.38,12.29,7.86"><row><cell>@1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,658.44,202.91,9.21"><p>http://www.cis.uni-muenchen.de/ ~wastl/misc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.73,646.84,335.87,7.86;6,144.73,657.79,96.83,7.86"><p>Note that prepositions should have been re-encoded as edge labels during the preceding syntactic normalization.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="7,144.73,635.88,335.86,7.86;7,144.73,646.84,337.14,7.86;7,144.73,657.79,309.00,7.86"><p>CVC3 is in fact a solver for predicate logic, even though currently all of our formulae are in propositional logic. We use CVC3 for future compatibility to allow, for example, the formulation of PL1 inference rules for predicates with valency mappings.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="10,144.73,646.84,335.87,7.86;10,144.73,657.79,171.10,7.86"><p>For convenience, we present all examples in English. The IDs that we provide have the shape [TopicID]-[DocID]-[QuestionID].</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,613.95,337.64,7.86;12,151.52,624.91,329.07,7.86;12,151.52,635.87,293.66,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,323.92,613.95,156.68,7.86;12,151.52,624.91,65.48,7.86">Definition and analysis of intermediate entailment levels</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,237.44,624.91,243.16,7.86;12,151.52,635.87,151.36,7.86">Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment</title>
		<meeting>the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment<address><addrLine>Ann Arbor, MI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,646.84,337.82,7.86;12,151.52,657.79,292.78,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,245.53,646.84,21.27,7.86">CVC3</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tinelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,288.55,646.84,76.21,7.86">Proceedings of CAV</title>
		<title level="s" coord="12,372.98,646.84,107.79,7.86;12,151.52,657.79,27.78,7.86">Lecture Notes in Computer Science</title>
		<meeting>CAV<address><addrLine>berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4590</biblScope>
			<biblScope unit="page" from="298" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,120.67,339.42,7.86;13,151.52,131.63,207.55,8.12" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ben-Kiki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<ptr target="http://www.yaml.org/spec/1.2/spec.html" />
		<title level="m" coord="13,256.68,120.67,171.37,7.86">Yaml ain&apos;t markup language specification</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,142.59,338.92,7.86;13,151.52,153.55,235.52,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,201.38,142.59,262.29,7.86">Top accuracy and fast dependency parsing is not a contradiction</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,151.52,153.55,95.59,7.86">Proceedings of COLING</title>
		<meeting>COLING<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,164.51,338.92,7.86;13,151.52,175.46,243.41,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,243.05,164.51,219.84,7.86">Recognising textual entailment with logical inference</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Markert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,151.52,175.46,91.18,7.86">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,186.42,339.42,7.86;13,151.52,197.38,330.35,7.86;13,151.52,208.34,62.86,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,390.67,186.42,87.44,7.86">The TIGER treebank</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dipper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lezius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,165.76,197.38,274.85,7.86">Proceedings of the Workshop on Treebanks and Linguistic Theories</title>
		<meeting>the Workshop on Treebanks and Linguistic Theories<address><addrLine>Sozopol, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,219.30,339.43,7.86;13,151.52,230.26,329.41,7.86;13,151.52,241.22,68.35,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,348.71,219.30,129.82,7.86">Predicting the dative alternation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bresnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cueni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nikitina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Baayen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,164.94,230.26,155.03,7.86">Cognitive Foundations of Interpretation</title>
		<imprint>
			<publisher>Royal Netherlands Academy of Science</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="69" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,252.18,337.89,7.86;13,151.52,263.14,330.35,7.86;13,151.28,274.09,157.73,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,304.09,252.18,176.75,7.86;13,151.52,263.14,35.79,7.86">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,208.61,263.14,118.57,7.86">Machine Learning Challenges</title>
		<title level="s" coord="13,334.46,263.14,143.38,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3944</biblScope>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,285.05,339.17,7.86;13,151.52,296.01,298.11,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="13,350.01,285.05,132.12,7.86;13,151.52,296.01,140.23,7.86">MindNet: acquiring and structuring semantic information from text</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">D</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Microsoft Research</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct coords="13,142.62,306.97,337.98,7.86;13,151.52,317.93,308.99,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="13,373.43,306.97,107.16,7.86;13,151.52,317.93,105.16,7.86">Taxonomy extraction from german encyclopedic texts</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Augustin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kienreich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sabol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Graz University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct coords="13,142.62,328.89,339.26,7.86;13,151.52,339.85,193.10,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,212.60,328.89,249.83,7.86">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,151.52,339.85,95.59,7.86">Proceedings of COLING</title>
		<meeting>COLING<address><addrLine>Nantes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,350.81,339.52,7.86;13,151.52,361.77,316.78,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,409.95,350.81,72.18,7.86;13,151.52,361.77,154.26,7.86">Directional distributional similarity for lexical inference</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kotlerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhitomirsky-Geffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,312.66,361.77,65.02,7.86">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="389" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,372.73,339.76,7.86;13,151.52,383.68,329.07,7.86;13,151.52,394.64,207.52,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,290.69,372.73,187.79,7.86">The Stanford typed dependencies representation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,165.34,383.68,315.25,7.86;13,151.52,394.64,70.41,7.86">Proceedings of the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation</title>
		<meeting>the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation<address><addrLine>Manchester, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,405.60,339.52,7.86;13,151.52,416.56,330.86,7.86;13,151.52,427.52,330.86,7.86;13,151.52,438.48,166.53,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,313.89,405.60,168.25,7.86;13,151.52,416.56,327.04,7.86">Evaluating answers to reading comprehension questions in context: Results for German and the role of information structure</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Meurers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ziai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,165.73,427.52,312.31,7.86">Proceedings of the EMNLP TextInfer 2011 Workshop on Textual Entailment</title>
		<meeting>the EMNLP TextInfer 2011 Workshop on Textual Entailment<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,449.44,339.76,7.86;13,151.52,460.40,77.31,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,197.43,449.44,123.49,7.86">Inference in text understanding</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,342.15,449.44,81.88,7.86">Proceedings of AAAI</title>
		<meeting>AAAI<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="561" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,471.36,339.77,7.86;13,151.52,482.31,197.34,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,243.28,471.36,234.66,7.86">Dependency-based construction of semantic space models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,151.52,482.31,106.72,7.86">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="199" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,493.27,337.98,7.86;13,151.52,504.23,204.80,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,248.58,493.27,162.60,7.86">A simple measure to assess non-response</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,432.51,493.27,48.08,7.86;13,151.52,504.23,49.69,7.86">Proceedings of ACL/HLT</title>
		<meeting>ACL/HLT<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,512.92,337.97,10.13;13,151.52,526.15,306.76,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,342.69,515.19,137.90,7.86;13,151.52,526.15,81.22,7.86">Testing the reasoning for question answering validation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,239.65,526.15,139.79,7.86">Journal of Logic and Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="459" to="474" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,537.11,339.76,7.86;13,151.52,548.07,257.49,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,254.69,537.11,223.52,7.86">A semantic approach to recognizing textual entailment</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tatu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,165.60,548.07,91.18,7.86">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="371" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,559.03,337.98,7.86;13,151.52,569.99,290.26,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,259.87,559.03,220.73,7.86;13,151.52,569.99,37.18,7.86">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,195.91,569.99,167.03,7.86">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,580.94,337.98,7.86;13,151.52,591.90,217.42,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,223.80,580.94,188.30,7.86">Answering and questioning for machine reading</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,433.09,580.94,47.50,7.86;13,151.52,591.90,126.98,7.86">Proceedings of the AAAI Spring Symposium</title>
		<meeting>the AAAI Spring Symposium<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
