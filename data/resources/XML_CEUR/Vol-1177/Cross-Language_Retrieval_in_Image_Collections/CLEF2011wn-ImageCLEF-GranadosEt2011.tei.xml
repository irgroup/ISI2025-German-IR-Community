<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.82,151.67,327.10,12.54;1,206.81,169.07,181.76,12.54">Multimodal information approaches for the Wikipedia collection at ImageCLEF 2011</title>
				<funder ref="#_TbthtWs #_vkpFKvJ #_EUBbKFw">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.78,207.32,49.91,9.05"><forename type="first">R</forename><surname>Granados</surname></persName>
							<email>rgranados@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidad Nacional de Educación a Distancia</orgName>
								<orgName type="institution" key="instit2">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.75,207.32,46.54,9.05"><forename type="first">J</forename><surname>Benavent</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universitat de Valencia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.59,207.32,50.02,9.05"><forename type="first">X</forename><surname>Benavent</surname></persName>
							<email>xaro.benavent@uv.es</email>
						</author>
						<author>
							<persName coords="1,323.93,207.32,38.60,9.05"><forename type="first">E</forename><surname>De Ves</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universitat de Valencia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.85,207.32,80.10,9.05"><forename type="first">Ana</forename><surname>García-Serrano</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidad Nacional de Educación a Distancia</orgName>
								<orgName type="institution" key="instit2">UNED</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universitat de Valencia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.82,151.67,327.10,12.54;1,206.81,169.07,181.76,12.54">Multimodal information approaches for the Wikipedia collection at ImageCLEF 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5BD3CDD632A36CCBCF7E68A8B511B75F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Retrieval</term>
					<term>Textual-based Retrieval</term>
					<term>Content-Based Image Retrieval</term>
					<term>Relevance feedback</term>
					<term>Merge Results Lists</term>
					<term>Indexing</term>
					<term>Multimedia</term>
					<term>Multimodal</term>
					<term>Fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The main goal of this paper it is to present our experiments in ImageCLEF 2011 Campaign (Wikipedia retrieval task). This edition we focused on applying different strategies of merging multimodal information, textual and visual, following both early and late fusion approaches. Our best runs are in the top ten of the global list, at positions 8, 9 and 10 with MAP 0.3405, 0.3367 and 0.323, being the second best group of the contest. Moreover, 18 of the 20 runs submitted are above the average MAP of its own modality (textual or mixed). In our system, the TBIR module works firstly and acts as a filter, and the CBIR system works only with the filtered sub-collection. The two ranked lists are fused using its own probability in a final ranked list. The best run of the TBIR system is in position 14 with a MAP of 0.3044, and uses subsystems IDRA and Lucene, fusing monolingual experiments carried out with IDRA preprocessing and Lucene search engine, taking into account extra information from Wikipedia articles. The best result at the CBIR system is obtained by using a logistic regression relevance feedback algorithm and CEDD low-level features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The UNED-UV is a research group with researchers from two different universities in Spain, the Universidad Nacional de Educación a Distancia (UNED) and the Valencia University (UV). This research group is working together <ref type="bibr" coords="1,393.83,598.00,11.69,9.05" target="#b0">[1]</ref> [2] <ref type="bibr" coords="1,431.59,598.00,11.69,9.05" target="#b6">[7]</ref> since ImageCLEF08 edition.</p><p>Two kinds of experiments were submitted to the 2011 Wikipedia Retrieval edition <ref type="bibr" coords="1,124.82,632.44,10.90,9.05" target="#b2">[3]</ref>: pure textual runs (TBIR), and mixed (with visual, CBIR). For textual experiments different approaches (stemming, use of articles info, and named entities recognition) were tested in order to evaluate the differences among them. For the mixed runs, as in 2010 presented ones <ref type="bibr" coords="1,210.28,667.00,10.78,9.05" target="#b6">[7]</ref>, the TBIR system works firstly over the whole database as a filter and then the CBIR only works over the filtered collection. Finally, the fusion module gets a ranked list, merging the textual and visual lists taking into account the probabilities obtained by each of the modules individually. The merging module for the multimodal information is the main goal of study of our group for this edition.</p><p>The TBIR subsystem includes the UNED own implemented tool IDRA (InDexing and Retrieving Automatically) <ref type="bibr" coords="2,262.97,195.56,11.69,9.05" target="#b3">[4]</ref> which includes several functionalities: text extraction and preprocessing, indexation following a Vector Space Model (VSM) approach using TF-IDF weighted vectors, retrieval based on the cosine function, a connection to a basic Lucene <ref type="bibr" coords="2,243.49,230.03,17.03,9.05" target="#b11">[12]</ref> configuration, and some merges utilities. The CBIR subsystem uses its own low-level features or the CEDD ones <ref type="bibr" coords="2,378.07,241.55,15.42,9.05" target="#b10">[11]</ref>, depending on the experiment in order to test the influence of the low-level features in the final results. Two different algorithms have also been used: a logistic regression relevance feedback algorithm and an automatic algorithm with the Tanimoto distance. A more detailed presentation of the system, the submitted experiments, and the obtained results are included in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>To carry out the Wikipedia retrieval task, it has been used a three modules architecture, as shown at Fig. <ref type="figure" coords="2,257.30,375.85,3.76,9.05">1</ref>. The figure illustrates the global system, which includes the TBIR (text based image retrieval) module, the CBIR (content based image retrieval) module, and the fusion one.</p><p>As the conceptual meaning of a topic is initially better captured by the text module itself than by the visual one, the textual module works first as a filter for the visual one, which works only with the sub-collection filtered by the textual module. Each module gets a ranked list based on a similarity score or probability (the TBIR module uses the textual information to obtain these scores while the CBIR one uses the visual one). From now on, we call textual probability (Pt) to the probability given by the textual module and image probability (Pi) to the probability given by the visual module. The way of merging these two probabilities is studied at the fusion module.</p><p>The TBIR subsystem is based on the IDRA tool, which allows to preprocess the textual information associated with the images in the collection, and to index and retrieve using both its own implemented search engine (based on a VSM approach), and a basic configuration of Lucene <ref type="bibr" coords="2,270.57,536.92,15.39,9.05" target="#b11">[12]</ref>.</p><p>The CBIR subsystem uses its own low-level features or the CEDD features depending on the experiment (in order to test the influence of the kind of low-level features in the final result), and its own logistic regression relevance feedback algorithm. Also, an automatic algorithm, which uses the Tanimoto distance as the score for ranking images in the collection, has been implemented in order to compare the performance of this distance with the logistic regression algorithm.</p><p>Each of the two subsystems, TBIR and CBIR, generates a ranked list with a certain probability and this multimodal information is merged at the fusion module. Different ways of merging this information is tested. Moreover, merging algorithms are used inside the TBIR subsystem to fuse different textual result lists from monolingual experiments in order to obtain multilingual results, as other fusing techniques are used inside the CBIR subsystem. All details of the different levels of merging information and the algorithms used are explained in the following sections. Fig. <ref type="figure" coords="3,269.29,459.91,3.40,8.18">1</ref>. System overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text-based Index and Image Retrieval (TBIR)</head><p>This module is in charge of the index and search of the images in the collection, based on the textual information in the metadata files associated with each of these images. IDRA tool is used to extract, select and preprocess this information. Additional resources, as STILUS <ref type="bibr" coords="3,218.81,549.40,11.84,9.05" target="#b8">[9]</ref> or Snowball <ref type="bibr" coords="3,288.22,549.40,15.48,9.05" target="#b9">[10]</ref>, are required in the preprocessing step. Finally, both IDRA and Lucene search engines are used for the index and retrieval of the preprocessed textual info, obtaining a ranked results list with the retrieved relevant images for each query.</p><p>The components shown in Fig. <ref type="figure" coords="3,262.20,595.48,4.98,9.05">1</ref> within the TBIR subsystem are the followings:</p><p>Textual Info Extraction. Two different textual information sources can be differentiated in the collection: the metadata files and the articles files. The metadata XML tags extracted, using the JDOM Java API, are &lt;name&gt; and the general &lt;comment&gt; for all languages, and &lt;description&gt;, &lt;comment&gt; and &lt;caption&gt; for each particular language (English, French and Dutch). The &lt;caption&gt; tag from metadata files may include a link to the article/s from Wikipedia where images appear. This information is taken into account in some of the experiments, extracting the title and the categories from the linked articles.</p><p>The final output of this component will be the selected textual information describing the images, coming from both the metadata and the articles, and separated depending on the language: text (EN), text (FR) and text (DE).</p><p>Language-dependent IDRA Preprocess. This component processes the selected text in three steps: 1) special characters deletion: characters with no statistical meaning, like punctuation marks or accents, are eliminated; 2) stopwords detection: exclusion of semantic empty words from specifics lists for each language; and 3) stemming: for reducing inflected or derived words to their stem, base or root form. A different algorithm is needed to perform stemming for each one of the languages. Stemmers from Snowball <ref type="bibr" coords="4,186.89,299.03,16.88,9.05" target="#b9">[10]</ref> are used in the experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NER (STILUS).</head><p>The Named Entities Recognition is carried out by the "List Entities" functionality of the STILUS-Core API <ref type="bibr" coords="4,289.54,328.07,10.73,9.05" target="#b8">[9]</ref>. Different forms of the detected entities (from general and variants) are took into account and considered as textual information in the corresponding experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index&amp;Search.</head><p>Once completed extraction and preprocess, both IDRA tool and Lucene will be used to index the selected text, and to retrieve relevant images for the proposed queries.</p><p>IDRA indexation is based on the VSM approach using TF-IDF (term frequencyinverse document frequency) weighted vectors. This approach consists in calculating the weights vectors for each one of the images selected texts. Each vector is compounded by the TF-IDF weights values of the different words in the collection. TF-IDF weight is a statistical measure used to evaluate how important a word is to a text in a concrete collection. These weights are normalized using the Euclidean distance.</p><p>IDRA search will launch the textual queries from the topics (English, French or Dutch) against a concrete index, obtaining this way the corresponding "TXT Results List". For each one of the queries, IDRA calculates its corresponding weights vector in the same way as in the index. Then, the similarity between the query and an image text will depend on the proximity of their associated vectors calculated by the cosine measure. This similarity value will be calculated between the query and all the images associated text indexed. Then images are ranked in descending order of relevance in the "TXT Results List".</p><p>Lucene indexation and search can be executed from IDRA tool. Selected texts already preprocessed with IDRA are indexed with Lucene following a basic implementation that uses the WhiteSpaceAnalyzer which just separates tokens and doesn"t apply any other linguistic preprocess.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Content-Based Information and Visual Retrieval</head><p>The VISION-Team at the Computer Science Department of the University of Valencia has its own CBIR system, and that has been used in previous ImageCLEF editions since our first participation in 2008. Last edition, the focus of the work was in testing three different visual algorithms applied to the results retrieved by the text module: the automatic, the relevance feedback and the query expansion obtaining the best results with the relevance feedback algorithm. Therefore, this edition we have used the relevance feedback algorithm and the work has been focus on testing the behavior of our own low-level features with the low-level features given by the organization (the CEDD algorithm described in <ref type="bibr" coords="5,317.56,218.48,15.12,9.05" target="#b10">[11]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extraction of low level features.</head><p>As in most CBIR systems, a feature vector represents each image. The first step at the Visual Retrieval system is extracting these features for all the images on the database and for each image in the query topic. We use different low-level features describing color and texture to build a vector of 293 components based on color and texture information.</p><p> Color information: Color information has been extracted calculating both local and global histograms of the images using 10x3 bins on the HS color system. Local histograms have been calculated dividing the images in four fragments of the same size. Therefore, a feature vector of 222 components represents the color information of the image.  Texture information: Two types of texture features are computed: the granulometric distribution function, using the coefficients that result of fitting the distribution function with a B-spline basis. And, the Spatial Size Distribution. We have used two different versions of it by using as the structuring elements for the morphological operation that get size both a horizontal and a vertical segment <ref type="bibr" coords="5,300.29,415.93,10.69,9.05" target="#b0">[1]</ref>. This gives us a texture feature vector of 71 components.</p><p>We assume that the conceptual meaning of a question is better captured by the text module than by a visual module when they work individually. Therefore, the task of the visual module is to re-rank the textual result list taking into account the information of the query images given at each topic.</p><p>Automatic algorithm. This is a classical algorithm in a CBIR system. Each image in the database has an associated low level feature vector. Concretely, we have used for this algorithm the low level features given by the organization (CEDD).</p><p>The second step is to calculate the similarity measurement between the feature vectors of each image on the database and the N query images. The distance metric applied in our experiments is the Tanimoto. As we have N query images, we will obtain N visual result lists, one for each query image in the topic. These N result lists are merged by using an average OWA operator.</p><p>Relevance feedback algorithm based on logistic regression. This algorithm works differently to the two previous ones. Therefore, we will explain the concept of relevance feedback and the adjustments made to get a good performance of the algorithm for the proposed tasks <ref type="bibr" coords="5,261.53,629.44,10.86,9.05" target="#b4">[5]</ref>. Relevance feedback is a term used to describe the actions performed by a user to interactively improve the results of a query by reformulating it. An initial query formulated by a user may not fully capture his/her wishes. Users then typically change the query manually and re-execute the search until they are satisfied. By using relevance feedback, the system learns a new query that better captures the user"s need for information. The user enters his/her preferences at each iteration through the selection of relevant and non-relevant images.</p><p>We will explain the way the logistic regression relevance feedback algorithm works. Let us consider the (random) variable Y giving the user evaluation where Y=1 means that the image is positively evaluated and Y=0 means a negative evaluation. Each image in the database has been previously described by using low-level features in such a way that the j-th image has the k-dimensional feature vector xj associated. Our data will consist of (xj, yj), with j=1,…,n, where n is the total number of images, xj is the feature vector and yj the user evaluation (1=positive and 0=negative). The image feature vector x is known for any image and we intend to predict the associated value of Y. In this work, we have used a logistic regression where P(Y=1|x) i.e. the probability that Y=1 (the user evaluates the image positively) given the feature vector x, is related with the systematic part of the model (a linear combination of the feature vector) by means of the logit function. For a binary response variable Y and p explanatory variables X1,…,Xp, the model for π(x)=P(Y=1|x) at values x=(x1,…,xp) of predictors is logit[π(x)]=α+β1x1+…+βpxp, where logit[π(x)]=ln(π(x)/(1-π(x))). The model parameters are obtained by maximizing the likelihood function given by:</p><formula xml:id="formula_0" coords="6,224.51,362.52,246.30,18.34">    n y i y i i i x x l 1 )] ( 1 [ ) ( ) (   <label>(1)</label></formula><p>The maximum likelihood estimators (MLE) of the parameter vector β are calculated by using an iterative method.</p><p>We have a major difficulty when having to adjust a global regression model in which we take the whole set of variables into account, because the number of selected images (the number of positive plus negative images) is typically smaller than the number of characteristics. In this case, the regression model adjusted has as many parameters as the number of data and many relevant variables could be not considered. In order to solve this problem, our proposal is to adjust different smaller regression models: each model considers only a subset of variables consisting of semantically related characteristics of the image. Consequently, each sub-model will associate a different relevance probability to a given image x, and we face the question of how to combine them in order to rank the database according to the user"s preferences. This problem has been solved by means of an ordered averaged weighted operator (OWA) <ref type="bibr" coords="6,193.60,545.56,10.66,9.05" target="#b5">[6]</ref>.</p><p>In our case, we have adapted the manual relevance feedback to an automatic performance. The examples and the counter-examples (positive and negative images) are automatically selected for each topic. The examples are the query images of the topic plus N images taken from the first positions of the textual result list. The M counter-examples are obtained by applying a procedure which chooses J random images from the whole database (without images in the textual list). This J images are ranked by the Euclidean distance and the latest M images are taken as negative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multimodal Fusion</head><p>Different types of fusion are needed in different steps of the experimentation. Depending on the point the fusion is carried out, it is called early fusion (at feature level) or late fusion (at decision level) <ref type="bibr" coords="7,287.69,194.96,10.69,9.05" target="#b7">[8]</ref>. Moreover, fusion can be applied among resources from different modes (text and image), or the same (different sources of text). Late fusion algorithms are used when fusing multiple modalities in the semantic space, at decision level, and when fusing textual results from monolingual experiments. All mixed runs submitted fuse together textual and visual runs at this level, combining the mono-modal decisions (results lists) from each modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Fusion in textual runs</head><p>Several fusion approaches are used within the textual module, following both early and late fusion techniques. Fusion at feature level (early fusion) is used when combining text in different languages <ref type="bibr" coords="7,282.33,316.07,10.83,9.05" target="#b7">[8]</ref>, or from different processes (i.e. NER). A late fusion approach is used when fusing together the decisions from each of the monolingual processes. The different implemented algorithms, and the purposes they were built for, are explained bellow: JOIN. Early fusion approach which just concatenates several lists of terms coming from different sources, obtaining only one. The level of fusion of this method is feature (early fusion), as it merges the components of the representation vectors of each image.</p><p>This approach is used in two kinds of experiments: 1) to merge the terms from the different languages (EN, FR, DE) describing each image in the collection into a unique multilingual representation (run9); and 2) to fuse at feature level the terms coming from the metadata textual information, with those obtained from the NER process (run10).</p><p>MAXmerge. Used to fuse together different results lists. This algorithm is included in IDRA tool and, for each query, selects the results from the different lists which have a higher relevance/similarity value, independently of the list in which the results appear in. The merging of the results corresponds to a decision level fusion (late fusion), where individual decisions working with each language are mixed in a unique results list, which is multilingual. (run4, run8) Enrich. Late fusion algorithm used to merge two results lists, also included in IDRA. The algorithm fuses together a main list with a support one. If a retrieved image appears in both lists for the same query, the relevance of this result in the merged list will be increased in the following way (normalized from 0 to 1): Every results appearing in the support list but not in the main one (for each query), will be added at the end of the results for each query. In this case, relevance values will be normalized according with the lower value in this moment.</p><p>This method is used (run7) to enrich the results of one of the English monolingual experiments with those from a NE-based experiment. The improvement (or not) of these fusion will be appreciable just in those queries (a total of 9) where named entities where detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Fusion in the Visual Module</head><p>There are two points where a fusion algorithm is needed inside the visual procedures.</p><p>The first one is in relevance feedback algorithm because, as we have explained before, our proposal is to adjust different smaller regression models. Consequently, each sub-model will associate a different relevance probability to a given image x. An ordered averaged weighted operator (OWA) with an orness of 0.5 has been used for this purpose, as explained in section 2.2.</p><p>On the other hand, the automatic algorithm generates N result visual lists depending on the number of query images. These N lists are merged in one result final list by using the Mathematical aggregation operators OWA with an orness of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Multimodal Fusion. Merging textual and visual lists</head><p>The late fusion module is focused on merging the two probabilities obtained for each of the images from the textual and from the visual module independently. Different ways of merging these two probabilities have been tested:  Pi. Using only the image probability to rank the filtered textual list. In this experiment the textual probability is used only on the first step to filter a subcollection of the most similar images to the topic, and then the re-ranking is only based on the visual information.  Pi*Pt: The ranking is made by using the score computed by the product of the two probabilities. At these experiments both textual and visual information is used to obtain the final re-ranked list.The OWA operator has been in the late fusion process too. This operator transforms a finite number of inputs into a single output (in our case the inputs are the Pi and Pt probabilities). With the OWA operator no weight is associated with any particular input; instead, the relative magnitude of the input decides which weight corresponds to each input. The aggregation weights used for these experiments are the weights which correspond to an orness with values 0.3 (this means that a weight of 0.3 is given to the higher probability value) and 0.5 (this is like an average operator).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments (submitted runs)</head><p>A total of 20 runs were finally submitted to the task, 10 text-based and 10 mixed combining both textual and visual techniques. A schematic description of all the 20 runs is available in the followings tables.</p><p>Table <ref type="table" coords="9,148.93,223.76,3.41,8.04">1</ref>. Submitted textual experiments.</p><formula xml:id="formula_1" coords="9,125.66,247.11,335.84,176.91">ID Lang Details System md stem Art Fusion NER run1 EN IDRA  - - - - run2 EN IDRA   - - - run3 EN IDRA    - - run4 ALL IDRA    MAXmerge (3en, 3fr, 3de) - run5 EN IDRA+Lucene    - - run6 FR IDRA+ Lucene    - - run7 EN IDRA+ Lucene    Enrich (5,NEs)  run8 ALL IDRA+ Lucene    MAXmerge (5en, 5fr, 5de) - run 9 ALL IDRA+ Lucene    JOIN - run10 EN IDRA+ Lucene    JOIN </formula><p>Experiments 1, 2, 3 and 4 are fully run using the IDRA tool (pre-processing, indexing and retrieval). These runs try to compare different configuration possibilities: 1) applying stemming or not; 2) adding the articles textual information; 3) using of the textual information from the languages different from English (FR, DE). The stemming process for the different languages was carried out using Snowball.</p><p>The way we take into account the textual information in the related articles provided with the collection consists on extracting the title and the Wikipedia categories of the corresponding article. Run 4 uses metadata, stemming and articles (as run 3 in English) independently for each one of the 3 languages, and applies a late fusion algorithm (MAXmerge) in order to merge the obtained monolingual results.</p><p>Runs 5 and 6 use the same configuration as run 3, but using IDRA just to preprocess the data, and Lucene for index and search. Run 8 also uses IDRA and Lucene, and follows the same late fusion approach among monolingual experiments carried out with IDRA pre-processing and Lucene search engine. Run 9 fuses together the text from different languages at feature level, following an early fusion method (JOIN).</p><p>Runs 7 and 10 add the use of a named entities recognisor based in the STILUS-Core API. This information is mixed with the textual info already available in two ways: 1) concatenating the identified entities for each image with its own associated text, before indexing, that is, merging at feature level (the same concatenation is done with the text of the queries); 2) searching independently using only entities for indexation and search (just in 9 queries with named entities detected), and using these results to enrich (late fusion algorithm) the results list from run 5 based in english textual information. Two textual based algorithms have been used for testing the different ways of merging the textual and the visual information, named as the run8 and the run9 (see table <ref type="table" coords="10,147.96,431.89,4.98,9.05">1</ref> for textual detail algorithms). Therefore, the block of runs 14, 15 and 16 is designed to be compared with runs 18, 19 and 20 in order to see the influence of the textual based algorithm used. In each of the group, we have test three ways of merging the textual and the visual information: the product of the textual and visual probabilities, an aggregation OWA operator using an ORNESS(0.3) or ORNESS(0.5) as weights.</p><p>Runs 11 and 12 have been made to test the influence of the low-level features used (UV or the CEDD), and also to test the fusion algorithm that only uses the visual information, image probability, to re-rank the final list. Finally, run13 is designed to test the two different vision algorithms used: the relevance feedback and the automatic algorithm with the Tanimoto distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>After the evaluation by the task organizers, our results for each of the submitted experiments are presented in Table <ref type="table" coords="10,267.83,635.20,3.76,9.05" target="#tab_1">2</ref>. The table shows how our best results are for the mixed runs 18, 20 and 19 (at positions 8, 9 and 10 of the global result list, this is at the 10% first results). For the text modality, the best result is run 8 at position 14 (at the 15% first results). It is worth pointing out, that all our runs except two of them are above the average of each modality (textual and mixed runs), and that for group classification we are the second group in the global result list. Textual runs 1, 2 and 3 show how the application of stemming and the use of the textual information coming from the articles categories from Wikipedia have a positive influence in the final image retrieval. Analyzing results from runs 7 and 10, it can be observed that the recognition of named entities is not a very useful in terms of MAP, may be because only 9 of the queries contain any entity (results per topic should be detailed analyzed). Only two runs (from XRCE) have obtained better results than our best one, and these both use query expansion or feedback techniques.</p><p>Regarding to the textual fusion, it can be observed in our two best textual runs how late fusion (run8, 0.3044) obtains better results than early fusion (run9, 0.2758). The only difference between these two runs is that run9 fuses together the textual information from the three languages at the beginning of the process (at feature level), while run8 works independently with each language and combines the results at the end of the process (at decision level).</p><p>With respect to the mixed runs, our best result is run18 with a MAP of 0.3405, ranked in position number 8. This is also our best global result. This experiment uses the images filtered by run8 (our best textual result), low level features given by the organization (CEDD), and the regression algorithm. The merging is performed by ranking the final list with probabilities Pi*Pt. Runs number 11 and 12, with MAP of 0.0553 and 0.0516, are our worst global results (position 107 and 108 of the global ranked MAP list). These two runs use only the image probability Pi for ranking the final result list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks and Future Work</head><p>Our best results are for the mixed modality, and they are at the position 8, 9 and 10, this is at the top ten of the contest. Moreover, 18 of the 20 runs submitted have their MAP above the average of its own modality (textual or mixed). These results mean that our strategy of fusing multimodal information as the textual and visual algorithms is on the top of the retrieval information strategies.</p><p>The best mixed run mentioned uses at the textual module IDRA and Lucene with fusion approach among monolingual experiments, and the visual module uses CEDD features as low-level features and the logistic regression relevance feedback algorithm; and fuses the two lists, textual and visual multiplying both probabilities. Our group will continue working tuning the two modules independently, in order to improve the results obtained.</p><p>In the textual modality we have discover the positive influence of taking into account the textual information extracted from Wikipedia categories, and the better performance when fusing together the textual information from different languages at decision level (late fusion, run8) than doing it at feature level (early fusion, run9).</p><p>Regarding the merging strategies, the combination of the multimodal information (textual and visual) at the decision level gives always better results than using only textual or visual information individually (runs 18,19 and 20 against run8 or run12). Among the different merging strategies presented, the best results are always obtained multiplying both probabilities, followed by the aggregation OWA operator at different values (runs 18, 20 and 19 respectively).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,267.71,648.26,4.90,9.54;7,244.42,648.26,10.73,9.54;7,249.75,633.28,10.73,9.54;7,235.04,633.28,13.64,9.54;7,203.26,639.95,10.73,9.54;7,157.32,639.95,10.73,9.54;7,261.33,648.16,5.38,9.64;7,220.15,639.84,5.38,9.64;7,174.84,639.84,5.38,9.64;7,256.15,648.26,2.72,9.54;7,229.57,648.26,13.36,9.54;7,261.49,633.28,2.72,9.54;7,214.98,639.95,2.72,9.54;7,182.70,639.95,19.22,9.54;7,169.04,639.95,2.72,9.54;7,140.81,639.95,15.29,9.54;7,291.41,630.85,135.60,7.24;7,291.41,639.97,133.70,7.24;7,291.41,649.21,130.62,7.24;7,291.41,663.94,24.10,1.00;7,317.47,658.45,83.52,7.24;7,452.26,644.24,11.72,8.89"><head></head><label></label><figDesc>relevance value in the merged list supRel: relevance value in the support list mainRel: relevance value in the main list posRel: position in the support list (2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,124.82,218.78,330.19,175.96"><head>Table 2 .</head><label>2</label><figDesc>Submitted mixed experiments.</figDesc><table coords="10,140.18,242.31,314.83,152.43"><row><cell>ID</cell><cell>TXT run</cell><cell>Low-level Features</cell><cell>Relevance feedback</cell><cell>Details Distance</cell><cell>Fusion</cell></row><row><cell>run11</cell><cell>run9</cell><cell>UV</cell><cell></cell><cell>-</cell><cell>Pi</cell></row><row><cell>run12</cell><cell>run9</cell><cell>CEDD</cell><cell></cell><cell>-</cell><cell>Pi</cell></row><row><cell>run13</cell><cell>run9</cell><cell>CEDD</cell><cell>Automatic</cell><cell>Tanimoto</cell><cell>OWA(Med)</cell></row><row><cell>run14</cell><cell>run9</cell><cell>CEDD</cell><cell></cell><cell>-</cell><cell>Pt*Pi</cell></row><row><cell>run15</cell><cell>run9</cell><cell>CEDD</cell><cell></cell><cell>-</cell><cell>OWA(Med)</cell></row><row><cell>run16</cell><cell>run9</cell><cell>CEDD</cell><cell></cell><cell>-</cell><cell>OWA(Orness0.3)</cell></row><row><cell>run17</cell><cell>run9</cell><cell>UV</cell><cell></cell><cell>-</cell><cell>Pt*Pi</cell></row><row><cell>run18</cell><cell>run8</cell><cell>CEDD</cell><cell></cell><cell>-</cell><cell>Pt*Pi</cell></row><row><cell>run19</cell><cell>run8</cell><cell>CEDD</cell><cell></cell><cell>-</cell><cell>OWA(Med)</cell></row><row><cell>run20</cell><cell>run8</cell><cell>CEDD</cell><cell></cell><cell>-</cell><cell>OWA(Orness0.3)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,124.82,184.34,318.32,310.75"><head>Table 2 .</head><label>2</label><figDesc>Results for the submitted experiments</figDesc><table coords="11,149.42,208.07,293.72,287.02"><row><cell>Po</cell><cell>Run</cell><cell>Mode</cell><cell>MAP</cell><cell>P@10</cell><cell>P@20</cell><cell>R-prec.</cell><cell>Bpref</cell></row><row><cell>93</cell><cell cols="7">run1 Textual 0.1727 0.3040 0.2380 0.2140 0.1786</cell></row><row><cell>75</cell><cell cols="7">run2 Textual 0.2056 0.3700 0.2900 0.2518 0.2097</cell></row><row><cell>63</cell><cell cols="7">run3 Textual 0.2243 0.3980 0.3270 0.2702 0.2287</cell></row><row><cell>51</cell><cell cols="7">run4 Textual 0.2489 0.3800 0.3290 0.2913 0.2450</cell></row><row><cell>45</cell><cell cols="7">run5 Textual 0.2601 0.4560 0.3670 0.3014 0.2600</cell></row><row><cell>98</cell><cell cols="7">run6 Textual 0.1561 0.3580 0.2600 0.2111 0.1813</cell></row><row><cell>50</cell><cell cols="7">run7 Textual 0.2515 0.4460 0.3660 0.2997 0.2541</cell></row><row><cell>14</cell><cell cols="7">run8 Textual 0.3044 0.5060 0.4040 0.3435 0.3012</cell></row><row><cell>36</cell><cell cols="7">run9 Textual 0.2758 0.4520 0.3550 0.3154 0.2771</cell></row><row><cell cols="8">55 run10 Textual 0.2403 0.4520 0.3510 0.2908 0.2458</cell></row><row><cell cols="8">107 run11 Mixed 0.0553 0.1180 0.1030 0.0816 0.0631</cell></row><row><cell cols="8">108 run12 Mixed 0.0516 0.0880 0.0950 0.0802 0.0579</cell></row><row><cell cols="8">21 run13 Mixed 0.2869 0.5040 0.4060 0.3306 0.2909</cell></row><row><cell cols="8">15 run14 Mixed 0.3006 0.5200 0.4030 0.3379 0.2983</cell></row><row><cell cols="8">20 run15 Mixed 0.2869 0.5040 0.4060 0.3306 0.2909</cell></row><row><cell cols="8">17 run16 Mixed 0.2980 0.5000 0.4030 0.3338 0.2954</cell></row><row><cell cols="8">16 run17 Mixed 0.3006 0.4960 0.3960 0.3376 0.2996</cell></row><row><cell>8</cell><cell cols="7">run18 Mixed 0.3405 0.5420 0.4500 0.3752 0.3378</cell></row><row><cell cols="8">10 run19 Mixed 0.3233 0.5400 0.4230 0.3586 0.3217</cell></row><row><cell>9</cell><cell cols="7">run20 Mixed 0.3367 0.5460 0.4410 0.3673 0.3314</cell></row><row><cell cols="2">Average</cell><cell cols="6">Textual 0.2169 0.3973 0.3228 0.2668 0.2246</cell></row><row><cell cols="8">Best (pos11) Textual 0.3141 0.5160 0.4270 0.3504 0.3107</cell></row><row><cell cols="2">Average</cell><cell cols="6">Mixed 0.2558 0.4542 0.3678 0.3049 0.2648</cell></row><row><cell cols="8">Best (pos 1) Mixed 0.3880 0.6320 0.5100 0.4162 0.3847</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work has been partially supported by projects <rs type="projectName">BUSCAMEDIA</rs> (<rs type="grantNumber">CEN-20091026</rs>), <rs type="projectName">MA2VICMR</rs> (<rs type="grantNumber">S2009/TIC-1542</rs>) and project <rs type="grantNumber">MCYT TEC2009-12980</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_TbthtWs">
					<idno type="grant-number">CEN-20091026</idno>
					<orgName type="project" subtype="full">BUSCAMEDIA</orgName>
				</org>
				<org type="funded-project" xml:id="_vkpFKvJ">
					<idno type="grant-number">S2009/TIC-1542</idno>
					<orgName type="project" subtype="full">MA2VICMR</orgName>
				</org>
				<org type="funding" xml:id="_EUBbKFw">
					<idno type="grant-number">MCYT TEC2009-12980</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,128.23,602.02,342.42,8.18;12,136.22,612.34,334.57,8.18;12,136.22,622.66,334.39,8.18;12,136.22,632.98,302.88,8.18" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,450.22,602.02,20.42,8.18;12,136.22,612.34,334.57,8.18;12,136.22,622.66,34.31,8.18">Some results using different approaches to merge visual and text-based features in CLEF&quot;08 photo collection</title>
		<author>
			<persName coords=""><forename type="first">Ana</forename><surname>García-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xaro</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruben</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miguel</forename><surname>Goñi-Menoyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,317.39,632.98,15.44,8.18">Págs</title>
		<idno type="ISSN">0302-9743</idno>
		<imprint>
			<biblScope unit="volume">5706</biblScope>
			<biblScope unit="page" from="568" to="571" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.23,643.42,342.41,8.18;12,136.22,653.74,334.20,8.18;12,136.22,664.06,334.38,8.18;12,136.22,674.38,334.37,8.18;12,136.22,684.82,334.54,8.18;13,136.22,149.30,334.61,8.18;13,136.22,159.74,73.23,8.18" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,136.22,653.74,334.20,8.18;12,136.22,664.06,75.91,8.18">Multimedia Retrieval by Means of Merge of Results from Textual and Content Based Retrieval Subsystems</title>
		<author>
			<persName coords=""><forename type="first">Ana</forename><surname>García-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xaro</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruben</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Esther</forename><surname>De Ves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jose</forename><surname>Miguel Goñi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-15751-6</idno>
		<idno>#pp: 8</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,218.71,664.06,251.88,8.18;12,136.22,674.38,140.16,8.18;12,409.77,674.38,60.82,8.18;12,136.22,684.82,173.58,8.18">Lecture Notes in Computer Science. Multilingual Information Access Evaluation II. Multimedia Experiments</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10-02">September 30 -October 2, 2009. 2010</date>
			<biblScope unit="page" from="142" to="149" />
		</imprint>
	</monogr>
	<note>Workshop of the Cross-Language Evaluation Forum, CLEF 2009</note>
</biblStruct>

<biblStruct coords="13,128.23,170.06,342.22,8.18;13,136.22,180.38,325.81,8.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,317.83,170.06,152.61,8.18;13,136.22,180.38,88.44,8.18">Overview of the wikipedia image retrieval task at ImageCLEF 2011</title>
		<author>
			<persName coords=""><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jana</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,231.02,180.38,94.29,8.18">CLEF 2011 working notes</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,128.23,190.70,342.47,8.18;13,136.22,201.14,334.40,8.18;13,136.22,211.46,334.25,8.18;13,136.22,221.81,190.10,8.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,391.41,190.70,79.28,8.18;13,136.22,201.14,154.22,8.18">La herramienta IDRA (Indexing and Retrieving Automatically)</title>
		<author>
			<persName coords=""><forename type="first">Ruben</forename><surname>Granados Muñoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><forename type="middle">García</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><forename type="middle">M Goñi</forename><surname>Menoyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,214.48,211.46,255.99,8.18;13,136.22,221.81,108.35,8.18">XXV Conferencia de la Sociedad Española para el Procesamiento del Lenguaje Natural (SEPLN&quot;09)</title>
		<meeting><address><addrLine>San Sebastián</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">Septiembre de 2009. 2009</date>
			<biblScope unit="volume">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,128.23,232.13,342.10,8.18;13,136.22,242.57,334.59,8.18;13,136.22,252.89,26.34,8.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,358.53,232.13,111.80,8.18;13,136.22,242.57,164.25,8.18">Applying logistic regression to relevance feedback in image retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zuccarello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ayala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>De Ves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Domingo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,306.58,242.57,70.22,8.18">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2621" to="2632" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,128.23,263.21,342.43,8.18;13,136.22,273.53,325.84,8.18" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,175.42,263.21,295.24,8.18;13,136.22,273.53,24.96,8.18">On ordered weighted averaging aggregation operators in multi criteria decision making</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Yager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,167.62,273.53,179.16,8.18">IEEE Transactions Systems Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="183" to="190" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,128.23,283.85,342.59,8.18;13,136.22,294.29,334.50,8.18;13,136.22,304.61,334.31,8.18;13,136.22,314.93,120.60,8.18" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,136.22,294.29,330.39,8.18">Experiences at ImageCLEF 2010 using CBIR and TBIR Mixing Information Approaches</title>
		<author>
			<persName coords=""><forename type="first">Joan</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xaro</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Esther</forename><surname>De Ves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruben</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><surname>García-Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,136.22,304.61,125.15,8.18">CLEF 2010 LABs and Workshops</title>
		<title level="s" coord="13,268.57,304.61,60.97,8.18">Notebook Papers</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">September 2010. 2010</date>
			<biblScope unit="page" from="22" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,128.23,325.25,342.18,8.18;13,136.22,335.69,275.41,8.18" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,382.66,325.25,87.75,8.18;13,136.22,335.69,105.48,8.18">Multimodal fusion for multimedia analysis: a survey</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">E</forename><surname>Saddik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,247.85,335.69,74.24,8.18">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="345" to="379" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,128.23,346.01,312.68,8.18" xml:id="b8">
	<monogr>
		<ptr target="http://www.daedalus.es/productos/stilus/stilus-core.html" />
		<title level="m" coord="13,136.22,346.01,95.68,8.18">STILUS-Core de Daedalus</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.39,356.33,179.09,8.18" xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Snowball</forename><surname>Stemmer</surname></persName>
		</author>
		<ptr target="http://snowball.tartarus.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.40,366.65,338.20,8.18;13,136.22,377.11,334.61,8.18;13,136.22,387.43,334.60,8.18;13,136.22,397.75,226.13,8.18" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,411.51,366.65,59.08,8.18;13,136.22,377.11,326.97,8.18">Accurate Image Retrieval based on Compact Composite Descriptors and Relevance Feedback Information</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zagoris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papamarkos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,136.22,387.43,298.02,8.18">International Journal of Pattern Recognition and Artificial Intelligence (IJPRAI)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2010-02">February, 2010</date>
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.40,408.07,151.07,8.18" xml:id="b11">
	<monogr>
		<ptr target="http://lucene.apache.org" />
		<title level="m" coord="13,136.22,408.07,54.31,8.18">Apache Lucene</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
