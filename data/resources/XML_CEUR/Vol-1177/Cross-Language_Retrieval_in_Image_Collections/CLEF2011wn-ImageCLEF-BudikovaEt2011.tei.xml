<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.77,116.95,345.82,12.62">MUFIN at ImageCLEF 2011: Success or Failure?</title>
				<funder ref="#_Y9zMRea">
					<orgName type="full">METACentrum</orgName>
				</funder>
				<funder ref="#_anESQvH">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_NrnUNPR">
					<orgName type="full">Brno PhD Talent Financial Aid</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,201.53,154.62,65.26,8.74"><forename type="first">Petra</forename><surname>Budikova</surname></persName>
							<email>budikova@fi.muni.cz</email>
							<affiliation key="aff0">
								<orgName type="institution">Masaryk University</orgName>
								<address>
									<settlement>Brno</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.90,154.62,56.52,8.74"><forename type="first">Michal</forename><surname>Batko</surname></persName>
							<email>batko@fi.muni.cz</email>
							<affiliation key="aff0">
								<orgName type="institution">Masaryk University</orgName>
								<address>
									<settlement>Brno</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.89,154.62,54.93,8.74"><forename type="first">Pavel</forename><surname>Zezula</surname></persName>
							<email>zezula@fi.muni.cz</email>
							<affiliation key="aff0">
								<orgName type="institution">Masaryk University</orgName>
								<address>
									<settlement>Brno</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.77,116.95,345.82,12.62">MUFIN at ImageCLEF 2011: Success or Failure?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2A489889C30F4D27DE5389DBD57BAA74</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In all fields of research it is important to discuss and compare various methods that are being proposed to solve given problems. In image retrieval, the ImageCLEF competitions provide such comparison platform. We have participated in the Photo Annotation Task of the ImageCLEF 2011 competition with a system based on the MUFIN Annotation Tool. Our approach is, in contrast to typical classifier solutions, based on a general annotation system for web images that provides general keywords for arbitrary image. However, the free-text annotation needs to be transformed into the 99 concepts given by the competition task. The transformation process is described in detail in the first part of this paper. In the second part, we discuss the results achieved by our solution. Even though the free-text annotation approach was not as successful as the classifier-based approaches, the results are competitive especially for the concepts involving real-world objects. On the other hand, our approach does not require training and is scalable to any number of concepts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the course of the past few decades, multimedia data processing has become an integral part of many application fields, including medicine, art, security, etc. This poses a number of challenges to the computer science -we need techniques for efficient data representation, storing and retrieval. In many applications, it is also necessary to understand the semantic meaning of a multimedia object, i.e. to know what is represented in a picture or what a video is about. Such information is usually expressed in a textual form, e.g. as a text description that accompanies the multimedia object. The semantic description of an object can be obtained manually or (semi)-automatically. Since the manual annotation is an extremely labor-intensive and time-consuming task for larger data collections, automatic annotation or classification of multimedia objects is of high importance. Perhaps the most intensive is the research on automatic annotation of images, which is essential for semantic image retrieval <ref type="bibr" coords="1,298.85,585.38,9.96,8.74" target="#b3">[4]</ref>.</p><p>In all fields of research it is important to discuss and compare various methods that are being proposed to solve given problems. In image retrieval, the ImageCLEF competitions provide such comparison platform. Each year, a set of specific tasks is defined that reflects the most challenging problems of current research. In 2011 as well as in the two previous years, one of the challenges was the Photo Annotation Task. This paper presents the techniques used by the MUFIN group to handle the Annotation Task. We believe that our methods will be interesting for the community since our approach is different from the solutions presented in previous years <ref type="bibr" coords="2,160.30,155.86,14.61,8.74" target="#b9">[10]</ref>. Rather than creating a solution tailored for this specific task we employed a general purpose annotation system and used the returned keywords to identify the relevant concepts. The results show that while this approach logically lags behind the precision of the more finely tuned solutions, it is still capable of solving some instances pretty well.</p><p>The paper is organized as follows. First, we briefly review the related work on image annotation and distinguish two important classes of annotation methods. Next, the ImageCLEF Annotation Task is described in more detail and the quality of the training data is discussed. Finally, we present the methods used by the MUFIN group, analyze their performance and discuss the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The reason why we are interested in annotation is to simplify access to the multimedia data. Depending on a situation, different types of metadata may be needed, both in content and form. In <ref type="bibr" coords="2,294.30,346.55,9.96,8.74" target="#b5">[6]</ref>, three forms of annotation are discussed: free text, keywords chosen from a dictionary, and concepts from some ontology. While the free text annotation does not require any structure, the other two options pose some restrictions on the terminology used, and in particular make the selection of keywords smaller. On certain conditions, we then begin to call the task classification or categorization rather than annotation.</p><p>Even though the conditions are not strictly defined, the common understanding is that classification task works with a relatively small number of concepts and typically uses machine learning to create specialized classifiers for the given concepts. To train the classifiers, a sufficiently large training dataset with labeled data needs to be available. The techniques that can be engaged in the learning are numerous, including SVMs, kNN classifiers, or probabilistic approaches <ref type="bibr" coords="2,467.31,478.37,9.96,8.74" target="#b6">[7]</ref>. Study <ref type="bibr" coords="2,164.83,490.33,15.50,8.74" target="#b9">[10]</ref> describes a number of classification setups using different types of concept learning.</p><p>On the contrary, annotation usually denotes a task where a very large or unlimited number of concepts is available and typically no training data is given. The solution to such task needs to exploit some type of data mining, in case of image annotation it is often based on content-based image retrieval over collections of images with rich metadata. Such system is described for example in <ref type="bibr" coords="2,467.31,562.37,9.96,8.74" target="#b7">[8]</ref>. Typically, the retrieval-based annotation systems exploit tagged images from photo-sharing sites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ImageCLEF Photo Annotation Task</head><p>In the ImageCLEF Photo Annotation Task, the participants were asked to assign relevant keywords to a number of test images. The full setup of the contest is described in the Task overview <ref type="bibr" coords="3,284.47,119.99,14.61,8.74" target="#b10">[11]</ref>. From our perspective, two facts are important: (1) the keywords to be assigned were chosen from a fixed set of 99 concepts, and (2) a set of labeled training images was available. Following our definition of terms from the previous section, the task thus qualifies as a classification problem. As such, it is most straightforwardly solved by machine learning approaches, using the training data to tune the parameters of the model. The quality of the training data is then crucial for the correctness of the classification.</p><p>As explained in <ref type="bibr" coords="3,219.53,203.68,14.61,8.74" target="#b9">[10]</ref>, it is difficult to obtain a large number of labeled images both for training and contest evaluation. It is virtually impossible to gather such data only with the help of a few domain experts. Therefore, only a part of the data was labeled by domain experts. The rest was annotated in a crowdsourcing way, using workers from the Amazon Mechanical Turk portal. Even though the organizers of the contest did their best to ensure that only sane results would be accepted, the gathered data still contain some errors. In the following, we would like to comment on some of them that we have noticed, so that they could be corrected in the future. Also, we will discuss later how these errors may have influenced the performance of the annotation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training data deficiencies</head><p>During the preparation of our solution for the Photo Annotation Task, we have identified to following types of errors in the labeled training data:</p><p>-Logical nonsense: Some annotations in the training dataset contradict the laws of the real world. The most significant nonsense we found was a number of images with the following triplet of concepts: single person, man, woman. Such combination appeared for 784 images. Similarly, still life and active are concepts that do not match together. Though the emotional annotations are more subjective and cannot be so easily discarded as nonsense, we also believe that annotating an image by both cute and scary concepts is an oxymoron. -Annotation inconsistence: Since the contest participants were not provided by any explanation of the concepts, it was not always clear to us what counts as relevant for a given concept and what does not. One such unclear concept was single person. Should a part of a body be counted as a person or not? It seems that this question was also unclear to the trainset annotators as the concept bodypart sometimes co-occurred with single person and sometimes with no person. -Concept overuse: The emotional and abstract concepts are definitely difficult to assign. Even within our research group, we could not decide what determines the "cuteness" of an image or what is the definition of a "natural" image. Again, the labeled training data were not of much help as we could not discover any inner logic in them. We suspect that the Amazon Turk workers solved this problem by using the emotional and abstract terms as often as possible. For illustration, from among the 8000 training images 3910 were labeled cute, 3346 were considered to be visual arts and 4594 images were perceived as natural.</p><p>Each error type is illustrated by a few examples in Figure <ref type="figure" coords="4,390.23,119.99,3.87,8.74" target="#fig_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our solution</head><p>The MUFIN Annotation Tool came into existence in the beginning of this year as an extension of the MUFIN Image Search<ref type="foot" coords="4,318.04,528.20,3.97,6.12" target="#foot_0">1</ref> , a content-based search engine that we have been developing for several years. Our aim was to provide an online annotation system for arbitrary web images. The first prototype version of the system is available online<ref type="foot" coords="4,242.28,564.06,3.97,6.12" target="#foot_1">2</ref> and was presented in <ref type="bibr" coords="4,340.66,565.64,9.96,8.74" target="#b2">[3]</ref>. As the first experiments with the tool provided promising results, we decided to try our tool in the ImageCLEF Annotation Task. The fundamental difference in the basic orientation of MUFIN Annotation Tool and the Annotation Task is that our system provides annotations while the task asks for classification. Our system provides free-text annotation of images, using any keywords that seem relevant using the content-based searching. To be able to use our tool for the task, we needed to transform the provided keywords into the restricted set of concepts given by the task. Moreover, even though the MUFIN tool is quite good at describing the image content it does not give much information about emotions and technical-related concepts (we will discuss the reasons later). Therefore, we also had to extend our system and add new components for specialized processing of some concepts. The overall architecture of the processing engaged in the Annotation Task is depicted in Figure <ref type="figure" coords="5,443.90,203.68,3.87,8.74" target="#fig_1">2</ref>. In the following sections, we will describe the individual components in more detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MUFIN Annotation Tool</head><p>The MUFIN Annotation Tool is based on the MUFIN Image Search engine, which retrieves the nearest neighbors of a given image based on visual and text similarity. The image search system, described in full detail in <ref type="bibr" coords="6,412.50,164.16,9.96,8.74" target="#b0">[1]</ref>, enables fast retrieval of similar images from very large collections. The visual similarity is defined by five MPEG-7 global descriptors -Scalable Color, Color Structure, Color Layout, Edge Histogram, and Region Shape -and their respective distance functions <ref type="bibr" coords="6,204.16,211.98,9.96,8.74" target="#b8">[9]</ref>. If some text descriptions of images are available, their tf-idf similarity score is also taken into consideration. In the ImageCLEF contest, freetext image descriptions and EXIF tags were available for some images. Together with the image, these were used as the input of the retrieval engine.</p><p>To obtain an annotation of some input image, we first evaluate the nearest neighbor query over a large collection of high-quality images with rich and trustworthy text metadata. In particular, we are currently using the Profimedia dataset, which contains 20M images from a microstock site <ref type="bibr" coords="6,393.92,295.73,9.96,8.74" target="#b1">[2]</ref>. When the query is processed by the MUFIN search, we obtain a set of images with their respective keywords. In the Profimedia dataset, each image is accompanied by a set of title words (typically 3 to 10 words) and keywords (about 20 keywords per image in average). Both the title words and the keywords of all images in the result set are merged together (the title words receiving a higher weight) and the frequencies of individual lemmas are identified. A list of stopwords and the WordNet lexical database <ref type="bibr" coords="6,250.82,379.42,10.52,8.74" target="#b4">[5]</ref> are used to remove irrelevant word types, names, etc. The Annotation Tool then returns the list of the most frequent keywords with the respective frequencies, which express the confidence of the annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Annotation to concept transformation</head><p>To transform the free-text annotation into the ImageCLEF concepts it was necessary to find the semantic relations between the individual keywords and concepts. For this purpose, we used the WordNet lexical database, which provides structured semantic information for English nouns, verbs, adjectives, and adverbs. The individual words are grouped into sets of cognitive synonyms (called synsets), each expressing a distinct concept. These synsets are interlinked by different semantic relations, such as hypernym/hyponym, synonym, meronym, etc. It is thus possible to find out whether any two synsets are related and how. In our case, we were interested in the relationships between the synsets of our annotation keywords and the synsets of the ImageCLEF concepts. To obtain them, we first needed to find the relevant synsets for our keywords and the task concepts.</p><p>The WordNet synset is defined as a set of words with the same meaning, accompanied by a short description of the respective semantic concept. Very often, a word has several meanings and therefore is contained in several synsets. For instance, the word "cat" appears both in a synset describing the domestic animal and a synset describing an attractive woman. If we have only a keyword and no other information about the word sense or context, we need to consider all synsets that contain this keyword. This is the case of keywords returned by the MUFIN Annotation Tool. We do not try here to determine whether the synsets are really relevant but rely on the majority voting of a large number of keywords that are processed.</p><p>The situation is however different in case of the ImageCLEF concepts where it is much more important to know the correct synsets. Fortunately, we have actually two possible ways of determining the relevant synsets. First, we can sort them out manually since the number of concepts is relatively small. The other, more systematic solution, will run the whole annotation process with all the candidate synsets of the concepts, log the contributions of the respective individual synsets, evaluate their performance, and rule out those with a low success rate.</p><p>Once the synsets are determined, we can look for the relationships. Again, there are different types of relations and some of them are relevant for one concept but irrelevant for another. Again, we have the same two possibilities of choosing the relevant relationships as in the case of synsets. In our implementation, we have used the manual cleaning approach for synsets and automatic selection approach for relationships.</p><p>With the relevant synsets and relationships, we count a relevance score of each ImageCLEF concept during the processing of each image. The score is increased each time a keyword-synset is related to concept-synset. The increase is proportional to the confidence score of the keyword as produced by the MUFIN Annotation Tool.</p><p>Finally, the concepts are checked against the OWL ontology provided within the Annotation Task. The concepts are visited in a decreasing order of their scores and whenever a conflict between two concepts is detected, the concept with a lower score is discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Additional image processing</head><p>The mining in keywords of similar images allows us to obtain such information as is usually contained in the image descriptions. This is most often related to image content, so the concepts related to nature, buildings, vehicles, etc. can be identified quite well. However, the Annotation Task considers also concepts that are less often described in the text. To get some more information about these, we employed the following three additional information sources:</p><p>-Face recognition: The face recognition algorithms are well-known in the image processing. We employ face recognition to determine the number of persons in an image. -EXIF tag processing: Some of the input photos are accompanied by EXIF tags that provide information about various image properties. When available, we use these tags to decide the relevance of concepts related to illumination, focus, and time of the day. -MUFIN visual search: Apart from the large Profimedia collection, we also have the training dataset that can be searched with respect to the visual similarity of images. However, since the trainset is rather small and some concepts are represented by only a few images, there is quite a high probability that the nearest neighbors will not be relevant. Therefore, we only consider neighbors within a small range of distances (determined by experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Trainset statistics input</head><p>Definitely the most difficult concepts to assign are the ones related to user's emotions and also the abstract concepts such as technical, overall quality, etc. As discussed in Section 3.1, it is not quite clear either to us or to the people who annotated the trainset what these concepts precisely mean. Therefore, it is very difficult to determine their relevance using the image visual content. The text provided with the images is also not helpful in most cases.</p><p>We finally decided to rely on the correlations between image content and the emotions it most probably evokes. For example, images of babies or nature are usually deemed cute. A set of such correlation rules was derived from the trainset and used to choose the emotional and abstract concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Our submissions at ImageCLEF</head><p>In our primary run, all the above-described components were integrated as depicted in Figure <ref type="figure" coords="8,209.77,376.27,3.87,8.74" target="#fig_1">2</ref>. In addition, we submitted three more runs where we tried various other settings to find out whether the proposed extensions really provided some added quality to the search results. The components that were left out in some experiments are depicted by dashed lines in Figure <ref type="figure" coords="8,418.26,412.14,3.87,8.74" target="#fig_1">2</ref>. We also experimented with the transformation of our concept scores into the confidence values expressed as percentage, which was done either as concept-specific or concept-independent. The individual run settings were as follows:</p><p>-MufinSubmission100: In this run, all the available information was exploited including photo tags, EXIF tags, visual image descriptors, and trainset statistics. Concept-specific mapping of annotation scores to confidence values was applied. -MufinSubmission101: The same settings were used for this run as in the previous case but concept-independent mapping of annotation scores was applied. -MufinSubmission110: In this run, we did not use the EXIF tags for the processing of concepts related to daytime and illumination as described in Section 4.3. The MUFIN visual search in the trainset was omitted as well. However, the textual EXIF tags were used as a part of the input for the MUFIN Annotation Tool. Concept-independent mapping of annotation scores was applied. -MufinSubmission120: In this run, the EXIF tags were not applied at all, neither as a part of the text-and-visual query in the basic annotation step nor in the additional processing. Again, the MUFIN visual search was skipped. Concept-independent mapping of annotation scores was applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion of results</head><p>As detailed in <ref type="bibr" coords="9,196.46,148.47,14.61,8.74" target="#b10">[11]</ref>, the following three quality metrics were evaluated to compare the submitted results: Mean interpolated Average Precision (MAP), F-measure (F-ex), and Semantic R-Precision (SR-Precision). As we expected, our best submission was MufinSubmission100 which achieved 0.299 MAP, 0.462 F-ex, and 0.628 SR-precision. The other submissions that we have tried received slightly worse scores. After the task evaluation and the release of the algorithm for computing the MAP metric, we also re-evaluated our system with better settings of the MUFIN Annotation Tool that we have improved since the ImageCLEF task submission. Using these, we were able to gain one or two percent increase of the MAP score. With respect to the MAP measure, our solution ranked at position 13 among the 18 participating groups.</p><p>Apart from the overall results, it is also interesting to take a closer look at the performance of the various solutions for individual concepts. The complete list of concept results for each group is available on the ImageCLEF web pages<ref type="foot" coords="9,449.92,303.23,3.97,6.12" target="#foot_2">3</ref> . Here we focus on the categories and particular examples of concepts where MUFIN annotation performed either well or poorly and discuss the possible reasons.</p><p>First of all, we need to define what we consider a good result. Basically, there are two ways: either we only look at the performance, e.g. following the MAP measure, or we consider the performance in relation to the difficulty of assigning the given concept. The assigning difficulty can be naturally derived from the competition results -when no group was able to achieve high precision with some concept, then the concept is problematic. Since we believe that the second way is more suitable, we express our results as a percentage of the best MAP achieved for the given concept. Table <ref type="table" coords="9,302.96,425.27,4.98,8.74">1</ref> and Figure <ref type="figure" coords="9,363.55,425.27,4.98,8.74" target="#fig_2">3</ref> summarize the results of MufinSubmission100 expressed in this way. that it is worth further studies to sort out the different types of concepts as well as annotation approaches and try to establish some relationships between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this study, we have described the MUFIN solution of the ImageCLEF Photo Annotation Task, which is based on free-text annotation mining, and compared it to more specialized, classifier-based approaches. The method we presented has its pros and cons. Mining information from annotated web collections is complicated by a number of features related to the way the annotations are created. As discussed in <ref type="bibr" coords="11,205.31,245.75,14.61,8.74" target="#b11">[12]</ref>, we need to expect errors, typing mistakes, synonyms, etc. However, there are also ways of overcoming these difficulties. In our approach, we have exploited a well-annotated collection, the semantical information provided by WordNet, and a specialized ontology. Using these techniques, we have been able to create an annotation system that shows precision comparable to average classifiers, which are usually trained for specific purposes only. The main advantage of our solution lies in the fact that it requires minimum training (and is therefore less dependent on the availability of high-quality training data) and is scalable to any number of concepts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,424.47,345.83,7.89;4,134.77,435.46,65.87,7.86;4,152.06,152.27,311.24,257.43"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Trainset labeling errors: a) logical nonsense, b) annotation inconsistence, c) concept overuse.</figDesc><graphic coords="4,152.06,152.27,311.24,257.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,238.64,624.59,134.99,7.89;5,143.41,264.56,328.55,345.26"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Concept retrieval schema.</figDesc><graphic coords="5,143.41,264.56,328.55,345.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,193.47,635.73,225.35,7.89;9,134.77,459.42,345.84,168.92"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. MUFIN relative MAP performance per concept.</figDesc><graphic coords="9,134.77,459.42,345.84,168.92" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.73,646.84,141.89,7.86"><p>http://mufin.fi.muni.cz/imgsearch/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,657.79,145.66,7.86"><p>http://mufin.fi.muni.cz/annotation/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="9,144.73,657.79,133.26,7.86"><p>http://imageclef.org/2011/Photo</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been partially supported by <rs type="funder">Brno PhD Talent Financial Aid</rs> and by the national research projects <rs type="grantNumber">GAP 103/10/0886</rs> and <rs type="grantNumber">VF 20102014004</rs>. The hardware infrastructure was provided by the <rs type="funder">METACentrum</rs> under the programme <rs type="grantNumber">LM 2010005</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NrnUNPR">
					<idno type="grant-number">GAP 103/10/0886</idno>
				</org>
				<org type="funding" xml:id="_Y9zMRea">
					<idno type="grant-number">VF 20102014004</idno>
				</org>
				<org type="funding" xml:id="_anESQvH">
					<idno type="grant-number">LM 2010005</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table" coords="10,177.05,368.92,4.98,8.74">1</ref> shows the results averages in groups of semantically close categories as specified by the ontology provided for ImageCLEF. We can observe that the MUFIN approach is most successful in categories that are (1) related to visual image content rather than higher semantics, and (2) probable to be reflected in image tags. These are, in particular, the categories describing the depicted elements, landscape, seasons, etc. Categories related to impressions, events, etc. represent the other end of the spectrum; they are difficult to decide using only the visual information and (especially the impressions) are rarely described via tags.</p><p>However, the average MAP values do not differ that much between categories. The reason for this is revealed if we take a closer look at the results for individual concepts, as depicted in Figure <ref type="figure" coords="10,277.82,501.06,3.87,8.74">3</ref>. Here we can notice low peaks in otherwise well performing categories and vice versa. For instance, the clouds concept in the landscapes category performs rather poorly. This is caused by the fact that clouds appear in many images but only as a part of a background, which is not important enough to appear in the annotation. On the contrary, airplanes are more interesting and thus regularly appear in the annotations. In fact, we again encounter the difference between the annotation and classification tasksin annotation we are usually interested in the most important/interesting tags while in classification all relevant tags are wanted.</p><p>Several more extremes are pointed out in Figure <ref type="figure" coords="10,361.22,609.29,3.87,8.74">3</ref>. For instance, the concept cute performs well because of its high frequency in the dataset. On the other hand, for the concept overexposed a specialized classifier is much more suitable than the annotation mining. The detailed discussion of the best fitting methods for individual categories is beyond the scope of this paper. However, we believe</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,472.82,337.64,7.86;11,151.52,483.78,329.07,7.86;11,151.52,494.73,329.07,7.86;11,151.52,505.69,138.28,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,225.09,483.78,202.56,7.86">Building a web-scale image similarity search system</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lucchese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rabitti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sedmidubsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-009-0339-z,10.1007/s11042-009-0339-z</idno>
		<ptr target="http://dx.doi.org/10.1007/s11042-009-0339-z,10.1007/s11042-009-0339-z" />
	</analytic>
	<monogr>
		<title level="j" coord="11,434.25,483.78,46.34,7.86;11,151.52,494.73,91.81,7.86">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="599" to="629" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,516.43,337.63,7.86;11,151.52,527.39,329.07,7.86;11,151.52,538.35,120.85,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,300.14,516.43,180.45,7.86;11,151.52,527.39,64.98,7.86">Evaluation platform for content-based image retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Budikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,236.59,527.39,212.27,7.86">To appear in Theory and Practice of Digital Libraries</title>
		<imprint>
			<date type="published" when="2011-09-28">2011. 26-28 September 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,549.09,337.63,7.86;11,151.52,560.05,329.07,7.86;11,151.52,571.01,42.49,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,306.63,549.09,100.52,7.86">Online image annotation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Budikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,430.78,549.09,49.81,7.86;11,151.52,560.05,229.06,7.86">4th International Conference on Similarity Search and Applications</title>
		<meeting><address><addrLine>SISAP</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="109" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,581.74,337.64,7.86;11,151.52,592.70,229.72,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,323.66,581.74,156.93,7.86;11,151.52,592.70,85.45,7.86">Image retrieval: Ideas, influences, and trends of the new age</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,244.02,592.70,84.49,7.86">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,603.44,337.64,7.86;11,151.52,614.40,25.60,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,234.71,603.44,173.00,7.86">WordNet: An Electronic Lexical Database</title>
		<editor>Fellbaum, C.</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,625.14,337.64,7.86;11,151.52,636.10,87.55,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,207.80,625.14,172.93,7.86">A survey of methods for image annotation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,388.60,625.14,91.99,7.86">J. Vis. Lang. Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="617" to="627" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,646.84,337.64,7.86;11,151.52,657.79,279.73,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,287.52,646.84,193.08,7.86;11,151.52,657.79,41.89,7.86">Machine learning methods in automatic image annotation</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kwasnicka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Paradowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,214.75,657.79,133.49,7.86">Advances in Machine Learning II</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="387" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,120.67,337.64,7.86;12,151.52,131.63,329.07,7.86;12,151.52,142.59,274.10,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,350.92,120.67,129.68,7.86;12,151.52,131.63,119.82,7.86">Image annotation by large-scale content-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">;</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename></persName>
		</author>
		<editor>Nahrstedt, K., Turk, M., Rui, Y., Klas, W., Mayer-Patel, K.</editor>
		<imprint>
			<date type="published" when="2006">0001. 2006</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="607" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,153.55,337.64,7.86;12,151.52,164.51,80.38,7.86" xml:id="b8">
	<monogr>
		<idno>ISO/IEC 15938-3:2002</idno>
		<title level="m" coord="12,151.53,153.55,283.56,7.86">MPEG-7: Multimedia content description interfaces. Part 3: Visual</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,175.46,337.97,7.86;12,151.52,186.42,329.07,7.86;12,151.52,197.38,246.43,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,260.32,175.46,220.27,7.86;12,151.52,186.42,162.85,7.86">New strategies for image annotation: Overview of the photo annotation task at imageclef 2010</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,189.08,197.38,175.34,7.86">CLEF (Notebook Papers/LABs/Workshops</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Pianta</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,208.34,337.98,7.86;12,151.52,219.30,277.49,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,311.08,208.34,169.51,7.86;12,151.52,219.30,122.32,7.86">The CLEF 2011 Photo Annotation and Concept-based Retrieval Tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liebetrau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,295.18,219.30,105.15,7.86">CLEF 2011 working notes</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,230.26,337.97,7.86;12,151.52,241.22,92.54,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,194.43,230.26,270.40,7.86">Studying social tagging and folksonomy: A review and framework</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Trant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,473.29,230.26,7.30,7.86;12,151.52,241.22,39.81,7.86">J. Digit. Inf</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
