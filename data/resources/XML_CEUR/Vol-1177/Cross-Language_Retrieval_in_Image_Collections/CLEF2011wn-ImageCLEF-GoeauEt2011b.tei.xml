<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,169.27,115.96,276.83,12.62;1,135.09,133.89,345.18,12.62">Participation of INRIA &amp; Pl@ntNet to ImageCLEF 2011 plant images classification task</title>
				<funder>
					<orgName type="full">Agropolis</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,150.95,172.29,56.55,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">IMEDIA team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,218.06,172.29,48.08,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">IMEDIA team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,276.69,172.29,65.62,8.74"><forename type="first">Itheri</forename><surname>Yahiaoui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">IMEDIA team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,352.86,172.29,60.95,8.74"><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<email>pierre.bonnet@cirad.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,443.73,172.29,20.67,8.74;1,284.91,184.24,41.07,8.74"><forename type="first">Elise</forename><surname>Mouysset</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tela Botanica</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,169.27,115.96,276.83,12.62;1,135.09,133.89,345.18,12.62">Participation of INRIA &amp; Pl@ntNet to ImageCLEF 2011 plant images classification task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2A35519EF4E5A6329C818D38D7D88E5B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pl@ntNet</term>
					<term>IMEDIA</term>
					<term>INRIA</term>
					<term>ImageCLEF</term>
					<term>plant</term>
					<term>leaves</term>
					<term>images</term>
					<term>collection</term>
					<term>identification</term>
					<term>classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the participation of INRIA IMEDIA group and the Pl@ntNet project to ImageCLEF 2011 plant identification task. ImageCLEF's plant identification task provides a testbed for the system-oriented evaluation of tree species identification based on leaf images. The aim is to investigate image retrieval approaches in the context of crowdsourced images of leaves collected in a collaborative manner. IMEDIA submitted two runs to this task and obtained the best evaluation score for two of the three image categories addressed within the benchmark. The paper presents the two approaches employed, and provides an analysis of the obtained evaluation results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents the participation of INRIA IMEDIA group and the Pl@ntNet<ref type="foot" coords="1,480.97,490.58,3.97,6.12" target="#foot_0">4</ref> project to the plant identification task that was organized within ImageCLEF 2011 <ref type="foot" coords="1,154.69,514.49,3.97,6.12" target="#foot_1">5</ref> for the system-oriented evaluation of visual based plant identification. This first year pilot task was more precisely focused on tree species identification based on leaf images. The task was organized as a classification task over 70 tree species with visual content being the main available information. Three types of image content were considered: leaf scans, leaf photographs with a white uniform background (referred as scan-like pictures) and unconstrained leaf's photographs acquired on trees with natural background. IMEDIA group, in collaboration with the Pl@ntNet project submitted two runs, one based on large-scale local features matching and rigid geometrical models, the other one based on segmentation and shape boundary features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task description</head><p>The task was evaluated as a supervised classification problem with tree species used as class labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training and Test data</head><p>A part of Pl@ntLeaves dataset was provided as training data whereas the remaining part was used later as test data. The training subset was built by randomly selecting 2/3 of the individual plants of each species (and not by randomly splitting the images themselves). So that pictures of leaves belonging to the same individual tree cannot be split across training and test data. This prevents identifying the species of a given tree thanks to its own leaves and that makes the task more realistic. In a real world application, it is indeed much unlikely that a user tries to identify a tree that is already present in the training data. Detailed statistics of the composition of the training and test data are provided in Table <ref type="table" coords="2,472.84,298.23,3.87,8.74" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nb of pictures Nb of individual plants Nb of contributors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task objective and evaluation metric</head><p>The goal of the task was to associate the correct tree species to each test image. Each participant was allowed to submit up to 3 runs built from different methods. As many species as possible can be associated to each test image, sorted by decreasing confidence score. Only the most confident species was however used in the primary evaluation metric described below. But providing an extended ranked list of species was encouraged in order to derive complementary statistics (e.g. recognition rate at other taxonomic levels, suggestion rate on top k species, etc.). The primary metric used to evaluate the submitted runs was a normalized classification rate evaluated on the 1st species returned for each test image. Each test image is attributed with a score of 1 if the 1st returned species is correct and 0 if it is wrong. An average normalized score is then computed on all test images. A simple mean on all test images would indeed introduce some bias with regard to a real world identification system. Indeed, we remind that the Pl@ntLeaves dataset was built in a collaborative manner. So that few contributors might have provided much more pictures than many other contributors who provided few. Since we want to evaluate the ability of a system to provide correct answers to all users, we rather measure the mean of the average classification rate per author. Furthermore, some authors sometimes provided many pictures of the same individual plant (to enrich training data with less efforts). Since we want to evaluate the ability of a system to provide the correct answer based on a single plant observation, we also decided to average the classification rate on each individual plant. Finally, our primary metric was defined as the following average classification score S:</p><formula xml:id="formula_0" coords="3,234.63,284.24,245.97,31.28">S = 1 U U u=1 1 P u Pu p=1 1 N u,p Nu,p n=1 s u,p,n<label>(1)</label></formula><p>U : number of users (who have at least one image in the test data) P u : number of individual plants observed by the u-th user N u,p : number of pictures taken from the p-th plant observed by the u-th user s u,p,n : classification score (1 or 0) for the n-th picture taken from the p-th plant observed by the u-th user It is important to notice that while making the task more realistic, the normalized classification score also makes it more difficult. Indeed, it works as if a bias was introduced between the statistics of the training data and the one of the test data. It highlights the fact that bias-robust machine learning and computer vision methods should be preferred to train such real-world collaborative data. Finally, to isolate and evaluate the impact of the image acquisition type (scan, scan-like, photograph), a normalized classification score S was computed for each type separately. Participants were therefore allowed to train distinct classifiers, use different training subsets or use distinct methods for each data type.</p><p>3 Description of used methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Large-scale local features matching and rigid geometrical models → inria imedia plantnet run1</head><p>State-of-the-art methods addressing leaf-based identification of leaves are mostly based on leaf segmentation and shape boundary features <ref type="bibr" coords="3,376.36,596.34,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="3,388.54,596.34,12.73,8.74" target="#b11">12,</ref><ref type="bibr" coords="3,402.93,596.34,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="3,412.33,596.34,12.73,8.74" target="#b14">15,</ref><ref type="bibr" coords="3,426.73,596.34,7.01,8.74" target="#b0">1]</ref>. Segmentationbased approaches have however several strong limitations including the presence of clutter and background information as well as other acquisition shortcomings (shadows, leaflets occlusion, holes, cropping, etc.). These issues are particularly critical in a crowdsourcing environment where we do not control accurately the acquisition protocol. Alternatively, our first run is based on local features and large-scale matching. Indeed, we realized that large-scale object retrieval methods <ref type="bibr" coords="4,152.87,130.95,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="4,170.03,130.95,11.62,8.74" target="#b9">10]</ref>, usually aimed at retrieving rigid objects (buildings, logos, etc.), do work surprisingly well on leaves. This can be explained by the fact that even if only a small fraction of the leaf remains affine invariant, this is sufficient to discriminate it from other species. Concretely, our system is based on the following steps: (i) Local features extraction (mixed texture &amp; shape features computed around Harris points) (ii) Local features matching with an efficient hashing-based indexing scheme (iii) Spatially consistent matches filtering with a RANSAC algorithm using a rigid transform model (iv) Basic top-K decision rule as classifier: for each species, the number of occurrences in the top-K images returned is used as its score.</p><p>Besides clutter robustness, the method has several advantages: it does not require any complex training phase allowing fast dynamic insertion of new crowdsourced training data, and it is weakly affected by unbalanced class distribution thanks to the selectivity of the spatial consistency filtering.</p><p>Mixed texture &amp; shape local features Rather than using classical SIFT features computed around DoG points, we employed multi-resolution color Harris points ( <ref type="bibr" coords="4,185.27,339.74,10.79,8.74" target="#b4">[5]</ref> and <ref type="bibr" coords="4,221.81,339.74,10.30,8.74" target="#b7">[8]</ref>). Indeed, we remarked that Harris corners were much more representative of relevant patterns of the leaves than the DoG points. Leaf boundary corners detected by Harris detector are notably much more stable than the blobs detected by DoG (which are visually mainly noise). We used the color version of Harris detector <ref type="bibr" coords="4,249.26,387.56,10.52,8.74" target="#b4">[5]</ref> that has usually a better repeatability. Finally we extracted the points at four distinct resolutions (as in <ref type="bibr" coords="4,376.05,399.51,10.79,8.74" target="#b7">[8]</ref>) to deal with scaling and blurring (with a scale factor equal to 0.8 between each resolution). The number of Harris points extracted per image was limited to 500 (with a log-scale maximum number of points per resolution).</p><p>Local features: hough 4 4, eoh 8, fourier 8 32 are extracted around each Harris point from an image patch oriented according to the principal orientation and scaled according to the resolution at which the Harris corner was detected. hough 4 4 is a 16 dimensional histogram based on ideas inspired from the Hough transform and is used to represent simple shapes in an image <ref type="bibr" coords="4,405.09,495.15,9.96,8.74" target="#b3">[4]</ref>. fourier 8 32 is a Fourier histogram used as a texture descriptor describing the distribution of the spectral power density within the complex frequency plane. It can differentiate between the low, middle and high frequencies and between different angles the salient features have in a patch <ref type="bibr" coords="4,360.25,542.97,9.96,8.74" target="#b3">[4]</ref>. eoh 8 is a 8 dimensional classical Edge Orientation Histogram used for describing shapes in images and gives here the distribution of gradients on 8 directions in a patch. Finally, we use as local features the concatenation of these 3 local features, resulting in a 280-dimensional feature vector extracted around each Harris point.</p><p>Local features compression with RMMH Random Maximum Margin Hashing (RMMH) <ref type="bibr" coords="4,196.00,644.16,10.52,8.74" target="#b6">[7]</ref> is a new data dependent hashing method that we recently introduced for the efficient embedding of high-dimensional feature vectors. The main idea of RMMH is to train balanced and independent binary partitions of the high-dimensional space by training svm's on purely random splits of the data, regardless the closeness of the training samples and without any form of supervision. It allows to generate consistently more independent hash functions than previous data dependent hashing methods while keeping a better embedding than classical data independent random projections such as LSH <ref type="bibr" coords="5,436.64,178.77,9.96,8.74" target="#b6">[7]</ref>. In this work, each local feature vector was embedded into a 256-bits hash code using RMMH with a linear kernel (inner product) and M=32 training samples per hash function (i.e. per bit). The distance between two local features is finally computed as a Hamming distance between their two hash codes.</p><p>Local features indexing and matching with AMP-RMMH We also used RMMH for indexing purposes using the multi-probe hashing method described in <ref type="bibr" coords="5,146.70,280.17,9.96,8.74" target="#b8">[9]</ref>. The 20 first bits of the hash codes were used to create a hash table and all binary hash codes of the full training set were mapped into it (resulting in about 2 millions 256-bits hash codes mapped in a 2 20 size hash table). At query time, each local feature of the query image is compressed with RMMH through a 256-bit hash code and its approximate 600-nearest neighbors are searched by probing multiple neighboring buckets in the hash table (according to the a posteriori multi-probe algorithm described in <ref type="bibr" coords="5,335.78,351.90,10.30,8.74" target="#b8">[9]</ref>). This step returns a large set of candidate local feature matches than can be reorganized image by image to finally obtain a set of candidate images each with a set of candidate matches.</p><p>Reranking with rigid geometrical models A last step is finally applied to re-rank the candidate images (retrieved from the training set) according to their geometrical consistency with the query local features (as in <ref type="bibr" coords="5,426.26,429.40,15.50,8.74" target="#b13">[14]</ref> or <ref type="bibr" coords="5,458.45,429.40,14.76,8.74" target="#b9">[10]</ref>).</p><p>We therefore estimate a translation-scale-rotation geometric model between the query image and each retrieved image. This is done using a RANSAC-like algorithm working only on points positions, so that it uses random pairs of matches to build candidate transform parameters. The final score for each image is computed as the number of inlier matches (i.e. the ones that respect the estimated translation-scale-rotation geometric model). All images that were returned by the former step are finally re-ranked according to this geometrical consistency score. This can be explained by the fact that scan images do not contain any noisy background so that all local features included in the trained index are actually parts of the leave and not distractors as in unconstrained photographs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification with a top-k decision rule</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Directional Fragment Histogram and geometric parameters on shape boundary→ inria imedia plantnet run2</head><p>The method used in the second run is very distinct from the first one and is closer from state-of-the-art methods based on leaf segmentation and shape boundary features. We use a shape boundary descriptor called Directional Fragment Histogram introduced in a previous work on botanical images database <ref type="bibr" coords="6,431.40,269.00,14.61,8.74" target="#b14">[15]</ref>, and to combine it with usual geometric parameters on shapes. The method described below focuses on scans and scan-like images. For photographs, results were produced by using classical global descriptors (Fourier histogram, Hough histogram, HSV color histogram, Global and Local Edge Orientation Histograms) implemented in the framework developed in IMEDIA team (more details of these global descriptors can be found in <ref type="bibr" coords="6,286.03,340.73,10.52,8.74" target="#b3">[4]</ref> and <ref type="bibr" coords="6,319.24,340.73,10.30,8.74" target="#b5">[6]</ref>).</p><p>As almost boundary-based shape description methods, the first step deals with image segmentation. We use the classical Otsu adaptive thresholding method <ref type="bibr" coords="6,134.77,376.60,14.61,8.74" target="#b12">[13]</ref>, applied widely in the literature due to its content-independent characteristic. Then two distinct feature extractions are applied in order to obtain a set of two vector descriptors, one containing the boundary description with a Directional Fragment Histogram, and the other containing 8 distinct geometric parameters.</p><p>Boundary description with Directional Fragment Histogram This method was introduced and applied successfully in a previous work on botanical data in 2006 <ref type="bibr" coords="6,157.78,474.51,14.61,8.74" target="#b14">[15]</ref>. The main idea is to consider that each element of a contour has a relative orientation with respect to its neighbors. The method consists in to slide a segment over the contour of the shape and to identify groups of elements having the same direction within the segment. Such groups are called Directional Fragments, and then the DFH codes the frequency distribution and relative length of groups of elements. Figure <ref type="figure" coords="6,264.45,534.29,4.98,8.74" target="#fig_0">1</ref> gives an example of the extraction of the Directional Fragment Histogram during one position of the sliding segment (colored in three fragments green, red and blue) along the contour. In this example the DFH is a 32-dimensional histogram given by 8 orientations d 0 to d 7 combined with 4 balanced ranges of relative lengths, (the lengths of the fragments seen as a percentage of the segment length). In this position the sliding segment is counted 3 times at 3 distinct orientations and lengths. At the end of the procedure DFH is finally normalized by the number of all the possible segments.</p><p>Boundary description with geometric parameters In order to improve performances in plant identification, we chose to combine the DHF descriptor with 8 morphological features used in plant identification literature, like Aspect Ratio, Rectangularity, Convex Area Ratio, Convex Perimeter Ratio, Sphericity, Eccentricity and Form Factor. The table 2 gives the 8 geometric parameters used for the task. Most of these parameters were succesfuly experimented in <ref type="bibr" coords="7,134.77,385.99,14.61,8.74" target="#b10">[11]</ref>, but on a limited numbers of 6 species related in fact to 6 very distinct morphological categories of simple leaf shapes. The Plant Identification task was the opportunity to experiment these shape parameters on much more species, on much more morphological categories of leaf shapes, with simple and compound leaves, and for certain with more visual ambiguities between species.</p><p>Classification with a top-k decision rule Finally, the boundary is described by two vectors, one 8-dimensional vector containing the shape parameters, and a DFH histogram. A balanced weighted sum of L1 distances on these two vectors is used as similarity measure between an image test and a training image. Best species label is finally computed by voting on the top-10 returned training images, as in the previous first run. <ref type="bibr" coords="7,134.77,584.07,42.19,8.74">Figures 2,</ref><ref type="bibr" coords="7,179.34,584.07,30.80,8.74">3 and 4</ref> present the normalized classification scores of the 20 submitted runs for each of the three image types. Figure <ref type="figure" coords="7,334.04,596.02,4.98,8.74" target="#fig_6">5</ref> presents the mean performances averaged over the 3 image types. Table <ref type="table" coords="7,311.70,607.98,4.98,8.74">3</ref> finally presents the same results but with detailed numerical values.</p><p>The two runs submitted by IMEDIA, in spite of theirs theoretical differences, gave both good results, and obtained the best evaluation scores for two</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diameter</head><p>Maximum length contained in the shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aspect Ratio</head><p>Ratio between the maximum length Dmax and the minimum lenght Dmin of the minimum bounding box of the shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dmax D min</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rectangularity</head><p>Ratio between the area As of the shape and the area A b of the minimal bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As A b</head><p>Convex Area Ratio Ratio between the shape area As and the convex hull area A h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As A h</head><p>Convex Perimeter Ratio Ratio between the shape perimeter Ps and the convex hull perimeter P h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ps P h</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Form Factor</head><p>The form factor can be interpreted as the "roundness" of the shape and is a ratio between the area As and the (squared) perimeter Ps of the shape.  Table <ref type="table" coords="8,170.74,523.02,4.13,7.89">2</ref>. The eight geometric parameters used in the second run inria imedia plantnet run2.</p><p>of the three image categories addressed within the benchmark, on scans for the run inria imedia plantnet run1, and on scan-like images for the second run inria imedia plantnet run2.</p><p>Considering the first run, the approach based on large-scale local features matching and rigid geometrical models gives surprisingly better results on scans than state of the arts methods based on shape boundary features.</p><p>Considering the image types and the results for all teams, performances are degrading with the complexity of the acquisition image type. Indeed, scans are more easy to identify than scan-like photos and unconstrained photos are much more difficult. This is can be seen in figure <ref type="figure" coords="9,328.73,154.86,4.98,8.74" target="#fig_6">5</ref> where the relative scores of each image type are highlighted by distinct colors. However, if this "rule" is true for the first run inria imedia plantnet run1 , it is not for the second run inria imedia plantnet run2 (and also for 5 other runs). It is difficult to give a precise reason of these results, but numerous unsuccessful scan tests have a relatively poor quality, coming from a low resolution original scan, noisy with a non uniform and gradually yellow colored background with blurred content. These unsuccessful scans maybe indicate a weakness at the very first step of automatic segmentation. RMIT 0,071 0,000 0,098 0,056 RMIT run2 RMIT 0,061 0,032 0,043 0,045 daedalus run1 DAEDALUS 0,043 0,025 0,055 0,041 UAIC2011 Run02 UAIC 0,000 0,000 0,042 0,014 Table <ref type="table" coords="9,164.75,522.17,4.13,7.89">3</ref>. Normalized classification scores for each run and each image type. Top 3 results per image type are highlighted in bold <ref type="bibr" coords="9,134.77,606.21,6.72,10.52" target="#b4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>For IMEDIA these results are very promising considering the complementarity of the two very distinct methods. Surprisingly, the matching approach gives the best evaluation score of the task on scans than state of the arts methods    Initially aimed at retrieving rigid objects, this original approach for plant leaf identification can be certainly improved in order to be more robust to other kind of images as the scan-like pictures and photographs, maybe by considering a part-based model approach. The very good results with the second method based on shape boundary description, let us to plan improvement by combining it with the matching approach. Indeed, by considering all test images, only 22% of the images are successful at the same time for the two methods, which let us to aim a significant room for improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,134.77,292.34,345.82,7.89;7,134.77,303.33,261.13,7.86;7,186.64,115.83,242.08,151.78"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Extraction of the Directional Fragment Histogram during one position of the sliding segment (colored in three fragments green, red and blue).</figDesc><graphic coords="7,186.64,115.83,242.08,151.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,472.41,392.55,17.07,5.24;8,476.44,398.65,9.02,6.61;8,481.10,402.37,3.34,4.37;8,136.56,448.74,40.47,7.86;8,237.07,437.73,169.85,7.86;8,237.07,448.69,148.34,7.86;8,237.07,459.65,84.81,7.86"><head>4πAs P 2 s</head><label>2</label><figDesc>SphericityRatio between the radius ri of the incircle of the shape and the radius re of the excircle of the shape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,477.88,446.68,6.12,6.31;8,477.53,453.82,6.84,5.24;8,136.56,495.52,48.02,7.86;8,237.07,489.99,164.90,7.86;8,237.07,500.95,108.70,7.86;8,477.05,493.58,7.79,6.20;8,477.05,500.61,7.79,6.20"><head></head><label></label><figDesc>major principal axis λ1 over the minor principal axis λ2 λ 1 λ 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,196.53,349.66,222.30,7.89;10,136.49,115.84,342.37,209.09"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Normalized classification scores for scan images</figDesc><graphic coords="10,136.49,115.84,342.37,209.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,188.34,606.79,238.69,7.89;10,136.49,373.15,342.36,208.91"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Normalized classification scores for scan-like photos</figDesc><graphic coords="10,136.49,373.15,342.36,208.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="11,195.52,349.48,224.32,7.89;11,136.49,115.84,342.36,208.91"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Normalized classification scores for photographs</figDesc><graphic coords="11,136.49,115.84,342.36,208.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,168.12,606.70,279.11,7.89;11,136.49,373.06,342.36,208.91"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Normalized classification scores averaged over all image types</figDesc><graphic coords="11,136.49,373.06,342.36,208.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,136.56,353.59,316.64,99.10"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the composition of the training and test data</figDesc><table coords="2,136.56,353.59,316.64,88.19"><row><cell>Scan</cell><cell>Train Test</cell><cell>2349 721</cell><cell>151 55</cell><cell>17 13</cell></row><row><cell>Scan-like</cell><cell>Train Test</cell><cell>717 180</cell><cell>51 13</cell><cell>2 1</cell></row><row><cell>Photograph</cell><cell>Train Test</cell><cell>930 539</cell><cell>72 33</cell><cell>2 3</cell></row><row><cell>All</cell><cell>Train Test</cell><cell>3996 1440</cell><cell>269 99</cell><cell>17 14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,554.71,345.83,110.14"><head></head><label></label><figDesc>Best species label is finally computed by voting on the top-10 returned training images (ranked by geometrical consistency score). effective than other strategies (e.g. using all training images for all test image types or using only the same image type for training and testing).</figDesc><table coords="5,134.77,608.27,345.83,56.59"><row><cell>might be more</cell></row><row><cell>Training data strategy Since training and test leaf images are categorized in</cell></row><row><cell>three distinct image types (scans, scan-like photos and unconstrained photos),</cell></row><row><cell>an important question is which training images types should be used for which</cell></row><row><cell>test image type. Few leave-one-out experiments performed on the training set</cell></row><row><cell>itself did show us that using only scans as training images for all test images</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="1,144.73,645.84,233.07,7.86"><p>http://www.plantnet-project.org/papyrus.php?langue=en</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="1,144.73,656.80,126.99,7.86"><p>http://www.imageclef.org/2011</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was funded by the <rs type="funder">Agropolis</rs> fundation through the project Pl@ntNet (http://www.plantnet-project.org/)</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,297.98,337.64,7.86;12,151.52,308.94,273.59,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,323.70,297.98,156.89,7.86;12,151.52,308.94,97.75,7.86">A complex network-based approach for boundary shape analysis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,256.14,308.94,81.42,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="67" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,319.66,337.63,7.86;12,151.52,330.62,329.07,7.86;12,151.52,341.58,327.05,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,358.37,330.62,122.22,7.86;12,151.52,341.58,197.22,7.86">Searching the world&apos;s herbaria: A system for visual identification of plant species</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kress</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sheorey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,369.65,341.58,23.24,7.86">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="116" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,352.31,337.63,7.86;12,151.52,363.26,324.16,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,408.54,352.31,72.05,7.86;12,151.52,363.26,118.69,7.86">Fractal dimension applied to plant identification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>De Oliveira Plotze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Falvo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,276.96,363.26,83.53,7.86">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2722" to="2733" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,373.99,337.63,7.86;12,151.52,384.95,329.07,7.86;12,151.52,395.91,88.11,7.86" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ferecatu</surname></persName>
		</author>
		<title level="m" coord="12,207.65,373.99,272.94,7.86;12,151.52,384.95,105.36,7.86">Image retrieval with active relevance feedback using both visual and keyword-based descriptors</title>
		<imprint>
			<date type="published" when="2005-07">jul 2005</date>
		</imprint>
		<respStmt>
			<orgName>Université de Versailles Saint-Quentinen-Yvelines</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="12,142.96,406.63,337.64,7.86;12,151.52,417.59,329.07,7.86;12,151.52,428.55,25.60,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,265.00,406.63,215.59,7.86;12,151.52,417.59,213.36,7.86">Object-based queries using color points of interest. Content-Based Access of Image and Video Libraries</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gouet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,372.60,417.59,79.92,7.86">IEEE Workshop on</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,439.27,337.64,7.86;12,151.52,450.23,329.07,7.86;12,151.52,461.19,318.89,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,272.61,439.27,207.98,7.86;12,151.52,450.23,43.37,7.86">Image annotation: which approach for realistic databases?</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hervé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,214.70,450.23,265.89,7.86;12,151.52,461.19,74.31,7.86;12,286.22,461.19,37.49,7.86">Proceedings of the 6th ACM international conference on Image and video retrieval</title>
		<meeting>the 6th ACM international conference on Image and video retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="170" to="177" />
		</imprint>
	</monogr>
	<note>CIVR &apos;07</note>
</biblStruct>

<biblStruct coords="12,142.96,471.91,337.64,7.86;12,151.52,482.87,60.92,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,244.77,471.91,143.64,7.86">Random maximum margin hashing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,411.88,471.91,47.56,7.86">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="573" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,493.59,337.64,7.86;12,151.52,504.55,329.07,7.86;12,151.52,515.51,25.60,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,190.34,493.59,207.59,7.86">New local descriptors based on dissociated dipoles</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,420.98,493.59,59.61,7.86;12,151.52,504.55,273.70,7.86">Proceedings of the 6th ACM international conference on Image and video retrieval</title>
		<meeting>the 6th ACM international conference on Image and video retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="573" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,526.23,337.63,7.86;12,151.52,537.19,329.07,7.86;12,151.52,548.15,25.60,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,240.86,526.23,200.66,7.86">A posteriori multi-probe locality sensitive hashing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,463.04,526.23,17.56,7.86;12,151.52,537.19,271.06,7.86">Proceeding of the 16th ACM international conference on Multimedia</title>
		<meeting>eeding of the 16th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,558.87,337.97,7.86;12,151.52,569.83,329.07,7.86;12,151.52,580.79,60.92,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,242.13,558.87,219.73,7.86">Logo retrieval with a contrario visual query expansion</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,569.83,307.81,7.86">Proceedings of the seventeen ACM international conference on Multimedia</title>
		<meeting>the seventeen ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="581" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,591.52,337.98,7.86;12,151.52,602.47,87.07,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,294.83,591.52,185.77,7.86;12,151.52,602.47,40.02,7.86">Automatic plant leaf classification for a mobile field guide</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potter</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct coords="12,142.62,613.20,337.97,7.86;12,151.52,624.16,329.07,7.86;12,151.52,635.12,93.69,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,370.25,613.20,110.34,7.86;12,151.52,624.16,155.20,7.86">Plant species identification using elliptic fourier leaf shape analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Neto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Samal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,313.19,624.16,167.40,7.86">Computers and Electronics in Agriculture</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="134" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,645.84,337.98,7.86;12,151.52,656.80,146.49,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,190.25,645.84,233.72,7.86">A Threshold Selection Method From Gray-Level Histogram</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,430.73,645.84,49.86,7.86;12,151.52,656.80,81.48,7.86">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,119.67,337.97,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,212.82,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,376.47,119.67,104.12,7.86;13,151.52,130.63,156.11,7.86">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,330.64,130.63,149.95,7.86;13,151.52,141.59,184.15,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,152.55,337.97,7.86;13,151.52,163.51,329.07,7.86;13,151.52,174.47,100.85,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,313.68,152.55,166.90,7.86;13,151.52,163.51,40.41,7.86">Shape-based image retrieval in botanical collections</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Herve</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,213.04,163.51,245.80,7.86">Advances in Multimedia Information Processing -PCM 2006</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4261</biblScope>
			<biblScope unit="page" from="357" to="364" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
