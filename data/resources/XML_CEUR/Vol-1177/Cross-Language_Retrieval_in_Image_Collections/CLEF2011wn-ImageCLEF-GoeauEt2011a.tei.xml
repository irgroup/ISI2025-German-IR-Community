<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.99,115.96,335.37,12.62">The CLEF 2011 plant images classification task</title>
				<funder>
					<orgName type="full">Agropolis</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.72,153.63,56.55,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">IMEDIA team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.83,153.63,60.95,8.74"><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<email>pierre.bonnet@cirad.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">INRA</orgName>
								<orgName type="institution" key="instit2">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.33,153.63,48.08,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">IMEDIA team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.95,153.63,74.03,8.74"><forename type="first">Nozha</forename><surname>Boujemaa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">IMEDIA team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,434.54,153.63,28.09,8.74;1,142.51,165.58,50.25,8.74"><forename type="first">Daniel</forename><surname>Barthelemy</surname></persName>
							<email>daniel.barthelemy@cirad.fr</email>
							<affiliation key="aff2">
								<orgName type="department">Direction and INRA</orgName>
								<orgName type="laboratory" key="lab1">CIRAD</orgName>
								<orgName type="laboratory" key="lab2">BIOS</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<postCode>F-34398</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,203.32,165.58,93.07,8.74"><forename type="first">Jean-François</forename><surname>Molino</surname></persName>
							<email>jean-francois.molino@ird.fr</email>
							<affiliation key="aff3">
								<orgName type="laboratory">IRD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.94,165.58,83.05,8.74"><forename type="first">Philippe</forename><surname>Birnbaum</surname></persName>
							<email>philippe.birnbaum@cirad.fr</email>
							<affiliation key="aff4">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,400.55,165.58,65.07,8.74"><forename type="first">Elise</forename><surname>Mouysset</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Tela Botanica</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,286.66,177.54,56.92,8.74"><forename type="first">Marie</forename><surname>Picard</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Tela Botanica</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.99,115.96,335.37,12.62">The CLEF 2011 plant images classification task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">478CFC0999FDCAA9E95E82AAEF2BE88A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>plant</term>
					<term>leaves</term>
					<term>images</term>
					<term>collection</term>
					<term>identification</term>
					<term>classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ImageCLEF' plant identification task provides a testbed for the system-oriented evaluation of tree species identification based on leaf images. The aim is to investigate image retrieval approaches in the context of crowdsourced images of leaves collected in a collaborative manner. This paper presents an overview of the resources and assessments of the plant identification task at ImageCLEF 2011, summarizes the retrieval approaches employed by the participating groups, and provides an analysis of the main evaluation results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convergence of multidisciplinary research is more and more considered as the next big thing to answer profound challenges of humanity related to health, biodiversity or sustainable energy. The integration of life sciences and computer sciences has a major role to play towards managing and analyzing cross-disciplinary scientific data at a global scale. More specifically, building accurate knowledge of the identity, geographic distribution and uses of plants is essential if agricultural development is to be successful and biodiversity is to be conserved. Unfortunately, such basic information is often only partially available for professional stakeholders, teachers, scientists and citizens, and often incomplete for ecosystems that possess the highest plant diversity. A noticeable consequence, expressed as the taxonomic gap, is that identifying plant species is usually impossible for the general public, and often a difficult task for professionals, such as farmers or wood exploiters and even for the botanists themselves. The only way to overcome this problem is to speed up the collection and integration of raw observation data, while simultaneously providing to potential users an easy and efficient access to this botanical knowledge. In this context, content-based visual identification of plant's images is considered as one of the most promising solution to help bridging the taxonomic gap. Evaluating recent advances of the IR community on this challenging task is therefore an important issue. This paper presents the plant identification task that was organized within Im-ageCLEF 2011<ref type="foot" coords="2,199.80,201.11,3.97,6.12" target="#foot_0">7</ref> for the system-oriented evaluation of visual based plant identification. This first year pilot task was more precisely focused on tree species identification based on leaf images. Leaves are far from being the only discriminant visual key between tree species but they have the advantage to be easily observable and the most studied organ in the computer vision community. The task was organized as a classification task over 70 tree species with visual content being the main available information. Additional information only included contextual meta-data (author, date, locality name) and some EXIF data. Three types of image content were considered: leaf scans, leaf photographs with a white uniform background (referred as scan-like pictures) and unconstrained leaf's photographs acquired on trees with natural background. The main originality of this data is that it was specifically built through a citizen sciences initiative conducted by Telabotanica<ref type="foot" coords="2,297.34,344.57,3.97,6.12" target="#foot_1">8</ref> , a French social network of amateur and expert botanists. This makes the task closer to the conditions of a real-world application: (i) leaves of the same species are coming from distinct trees living in distinct areas (ii) pictures and scans are taken by different users that might not used the same protocol to collect the leaves and/or acquire the images (iii) pictures and scans are taken at different periods in the year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task resources</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Pl@ntLeaves dataset</head><p>Building effective computer vision and machine learning techniques is not the only side of the taxonomic gap ptoblem. Speeding-up the collection of raw observation data is clearly another crucial one. The most promising approach in that way is to build real-world collaborative systems allowing any user to enrich the global visual botanical knowledge <ref type="bibr" coords="2,304.81,530.63,9.96,8.74">[9]</ref>. To build the evaluation data of Im-ageCLEF plant identification task, we therefore set up a citizen science project around the identification of common woody species covering the Metropolitan French territory. This was done in collaboration with TelaBotanica social network and with researchers specialized in computational botany. Technically, images and associated tags were collected through a crowd-sourcing web application [9] and were all validated by expert botanists. Several cycles of such collaborative data collection and taxonomical validation occurred. Scans of leaves were first collected over two seasons, between July and September 2009 Fig. <ref type="figure" coords="3,199.37,424.61,4.13,7.89">1</ref>. List of tree species included in the Pl@ntLeaves dataset and between June and September 2010 thanks to the work of active contributors from TelaBotanica social network. The idea of collecting only scans during this first period was to initialize the training data with limited noisy background and to focus on plant variability rather than mixed plant and view conditions variability. This allowed to collect 2228 scans over 55 species. A public version of the application<ref type="foot" coords="3,214.75,528.45,3.97,6.12" target="#foot_2">9</ref> was then opened in October 2010 and additional data were collected up to March 2011. The new collected images were either scans, or photographs with uniform background (referred as scan-like photos), or unconstrained photographs with natural background. They involved 15 new species from the previous set of 55 species. The Pl@ntLeaves dataset used within Im-ageCLEF finally contained 5436 images with 3070 scans, 897 scan-like photos and 1469 photographs. Figure <ref type="figure" coords="3,271.19,601.76,4.98,8.74" target="#fig_0">2</ref> displays samples of these 3 image types for 4 distinct tree species. The full list of species is provided in Figure <ref type="figure" coords="3,419.59,613.71,3.87,8.74">1</ref>.  These meta-data are stored in independent xml files, one for each image. Figure <ref type="figure" coords="5,134.77,400.76,4.98,8.74" target="#fig_1">3</ref> displays an example image with its associated xml data. Additional but partial meta-data information can be found in the image's EXIF, and might include the camera or the scanner model, the image resolution and dimension, the optical parameters, the white balance, the light measures, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pl@ntLeaves variability</head><p>The main originality of Pl@ntLeaves compared to previous leaf datasets, such as the Swedish dataset <ref type="bibr" coords="5,237.63,512.66,15.50,8.74" target="#b1">[13]</ref> or the Smithsonian one <ref type="bibr" coords="5,361.85,512.66,9.96,8.74" target="#b0">[1]</ref>, is that it was built in a collaborative manner through a citizen sciences initiative. This makes it closer to the conditions of a real-world application: (i) leaves of the same species are coming from distinct trees living in distinct areas (ii) pictures and scans are taken by different users that might not used the same protocol to collect the leaves and/or acquire the images (iii) pictures and scans are taken at different periods in the year. Intra-species visual variability and view conditions variability are therefore more stressed-out which makes the identification more realistic but more complex. Figures 4 to 9 provide illustrations of the intra-species visual variability over several criteria including leaf's color, leaf's global shape, leaf's margin appearance, number and relative positions of leaflets and number of lobes. On the other side, Figure <ref type="figure" coords="5,285.21,644.16,9.96,8.74" target="#fig_5">10</ref> illustrates the light reflection and shadows variations of scan-like photos. It shows that this acquisition protocol is actu- ally very different than pure scans. Both share the property of a limited noisy background but scan-like photos are much more complex due to the lighting conditions variability (flash, sunny weather, etc.) and the unflatness of leaves. Finally, the variability of unconstraint photographs acquired on the tree and with natural background is definitely a much more challenging issue as illustrated in Figure <ref type="figure" coords="6,166.20,455.09,8.49,8.74">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task description</head><p>The task was evaluated as a supervised classification problem with tree species used as class labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training and Test data</head><p>A part of Pl@ntLeaves dataset was provided as training data whereas the remaining part was used later as test data. The training subset was built by randomly selecting 2/3 of the individual plants of each species (and not by randomly splitting the images themselves). So that pictures of leaves belonging to the same individual tree cannot be split across training and test data. This prevents identifying the species of a given tree thanks to its own leaves and that makes the task more realistic. In a real world application, it is indeed much unlikely that a   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task objective and evaluation metric</head><p>The goal of the task was to associate the correct tree species to each test image. Each participant was allowed to submit up to 3 runs built from different methods. As many species as possible can be associated to each test image, sorted by decreasing confidence score. Only the most confident species was however used in the primary evaluation metric described below. But providing an extended ranked list of species was encouraged in order to derive complementary statistics (e.g. recognition rate at other taxonomic levels, suggestion rate on top k species, etc.). The primary metric used to evaluate the submitted runs was a normalized classification rate evaluated on the 1st species returned for each test image. Each test image is attributed with a score of 1 if the 1st returned species is correct and 0 if it is wrong. An average normalized score is then computed on all test images. A simple mean on all test images would indeed introduce some bias with regard to a real world identification system. Indeed, we remind that the Pl@ntLeaves dataset was built in a collaborative manner. So that few contributors might have provided much more pictures than many other contributors who provided few. Since we want to evaluate the ability of a system to provide correct answers to all users, we rather measure the mean of the average classification rate per author. Furthermore, some authors sometimes provided many pictures of the same individual plant (to enrich training data with less efforts). Since we want to evaluate the ability of a system to provide the correct answer based on a single plant observation, we also decided to average the classification rate on each individual plant. Finally, our primary metric was defined as the following average classification score S:</p><formula xml:id="formula_0" coords="9,234.63,470.63,245.97,31.28">S = 1 U U u=1 1 P u Pu p=1 1 N u,p Nu,p n=1 s u,p,n<label>(1)</label></formula><p>U : number of users (who have at least one image in the test data) P u : number of individual plants observed by the u-th user N u,p : number of pictures taken from the p-th plant observed by the u-th user s u,p,n : classification score (1 or 0) for the n-th picture taken from the p-th plant observed by the u-th user</p><p>It is important to notice that while making the task more realistic, the normalized classification score also makes it more difficult. Indeed, it works as if a bias was introduced between the statistics of the training data and the one of the test data. It highlights the fact that bias-robust machine learning and computer vision methods should be preferred to train such real-world collaborative data. Finally, to isolate and evaluate the impact of the image acquisition type (scan, scan-like, photogragh), a normalized classification score S was computed for each type separately. Participants were therefore allowed to train distinct classifiers, use different training subsets or use distinct methods for each data type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participants and techniques</head><p>A total of 8 groups submitted 20 runs, which is a successful participation rate for a first year pilot task on a new topic. Participants were mainly academics, specialized in computer vision and multimedia information retrieval, coming from all around the world: Australia (1), Brazil (1), France (2), Romania (1), Spain (1), Turkey (1) and UK <ref type="bibr" coords="10,244.65,236.79,11.62,8.74" target="#b0">(1)</ref>. We list below the 8 participants and give a brief overview of the techniques they used to run the plant identification task. We remind here that ImageCLEF benchmark is a system-oriented evaluation and not a formal evaluation of the underlying methods. Readers interested by the scientific and technical details of any of these methods should refer to the CLEF2011 working note of each participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IFSC (3 runs) [6]</head><p>The two best runs obtained by this participant (IFSC USP run2 &amp; IFSC USP run1) are mainly based on a new shape boundary analysis method they introduced recently [3]. It is based on the complex network theory [2]. A shape is modeled into a small-world complex network and it uses degree and joint degree measurements in a dynamic evolution network to compose a set of shape descriptors. This method is claimed to be robust, noise tolerant, scale invariant and rotation invariant and proved to provide better performances than Fourier shape descriptors, curvature-based descriptors, Zernike moments and multiscale fractal dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIRIS (4 runs) [7]</head><p>This participant also used a classification scheme based on shape boundary analysis. The main originality however is that they used a model-driven approach for the segmentation and shape estimation. Their four runs differ in the parameters of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UAIC (3 runs)</head><p>This participant was the only one trying to benefit from metadata associated to the images (location, date, author, etc.). They submitted therefore 3 runs to evaluate the contribution of metadata compared to using visual content only. Their first run (UAIC2011 Run01) is based on visual content only, the second one (UAIC2011 Run02) uses only metadata based features in the classification process, and the third one uses both (UAIC2011 Run03). <ref type="bibr" coords="10,284.28,584.36,17.82,8.77" target="#b4">[16]</ref> The system consists of two separate subparts for: i) scan-pseudoscan images and ii) photos. The features used for the scan categories are computed using basic color and shape descriptors such as color moments and convexity, as well as more complex ones such as the Fourier descriptors of the contour and several morphological texture descriptors based on covariance extensions. Since some of these features are not meaningful for the photo category, a subset with color and texture features was used there. For training, Support Vector Machines and classifier combination using only image content and none of the meta-data were used. All of the scan-pseudoscan images were used for part i), while for part ii), all of the images were used. The system was fully automatic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SABANCI-OKAN (1 run)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INRIA (2 runs) [10]</head><p>This participant submitted two runs based on two radically different methods. Their second run (inria imedia plantnet run2) is based on a shape boundary feature, called DFH <ref type="bibr" coords="11,325.98,202.68,14.61,8.74" target="#b3">[15]</ref>, that they introduced in 2006. Their first run (inria imedia plantnet run1) is more surprising for a supervised classification task of leaves since it is based on local features matching with rigid geometrical models . Such generalist method is usually more dedicated to largescale retrieval of rigid objects and this is the only participant who used such approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RMIT (2 runs) [11]</head><p>RMIT mainly focused on comparing two distinct machine learning algorithms: an instance-based learning, implemented on Weka as IB1 (nearest-neighbor classifier) (RMIT run1), and a decision tree technique implemented on Weka as J48 (RMIT run2). For both, all training data were used, without complementary data. The features used were GIFT 166 colour histograms.</p><p>DAEDALUS (1 run) <ref type="bibr" coords="11,261.06,370.02,17.82,8.77" target="#b2">[14]</ref> This participant used a generalist image retrieval framework based on SIFT features and a nearest-neighbor classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KMIMMIS (4 runs)</head><p>This participant also used a generalist image retrieval framework based on local features and a nearest-neighbor classifier. They compared different configurations: basic clustered SIFT with 1-NN label transfer (kmimmis run1 and kmimmis run4), simple edge and corner point detection with 1-NN vote label transfer (kmimmis run2), simple edge and corner point detection with 10-NN vote label transfer (kmimmis run3). Over all runs, the most frequently used class of methods is shape boundary analysis. 8 runs among 20 are based on some boundary shape features.This is not surprising since state-of-the-art methods addressing leaf-based identification in the literature are mostly based on leaf segmentation and shape boundary features [4, 12, 5, 15, 3]. On the other side, it was a good news that the majority of the runs were based on other various approaches so that more relevant conclusions can be expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Global analysis</head><p>Figures 12, 13 and 14 present the normalized classification scores of the 20 submitted runs for each of the three image types. Alternatively, Figure <ref type="figure" coords="11,432.06,656.12,9.96,8.74">15</ref> presents the overall performances averaged over the 3 image types. Table <ref type="table" coords="12,408.72,118.99,4.98,8.74">2</ref> finally presents the same results but with detailed numerical values. A first global remark is that, as expected, the performances are degrading with the complexity of the acquisition image type. Scans are more easy to identify than scan-like photos and unconstrained photos are much more difficult. This is can be easily seen in Figure <ref type="figure" coords="12,258.73,178.77,9.96,8.74">15</ref> where the relative scores of each image type are highlighted by distinct colors. A second global remark is that no method provide the best score for all image types. None of the run even belongs to the top-3 runs of all image types (as shown in Table <ref type="table" coords="12,205.07,226.59,3.87,8.74">2</ref>). This is somehow disappointing from the genericity point of view but not surprising regarding the nature of the different image types. One could expect that scans and scan-like photos lead to similar conclusions but this is actually not the case. The only runs that give quite stable and good performances over the three image types are the two runs of IFSC based on complex network shape boundary analysis method (IFSC USP run1 &amp; IFSC USP run2). This justifies their excellent ranking when averaging the classification scores over the three image types. All other methods fail in providing as good results for the unconstrained photographs as shown in Figure <ref type="figure" coords="12,338.95,322.23,8.49,8.74" target="#fig_8">14</ref>. This score gap between IFSC run and others has however to be mitigated by a bias introduced by the author normalization of the classification score. Indeed, their high score is mostly due to excellent performances on the images of one of the 3 contributors. All these images are very similar and less cluttered than the average of the unconstrained photos (actually they are all close-up of a juda's tree leaf). But still, it seems that this is the only method that have at least perform very well on these images.</p><p>A third important remark is that shape boundary analysis methods do not provide the best results on the scan images whereas they are usually considered as being state-of-the-art on such data. They all provide good classification scores between 48% and 56% but they are consistently outperformed by two more generic image retrieval approaches (as shown in Figure <ref type="figure" coords="12,398.37,476.79,8.30,8.74" target="#fig_6">12</ref>). The best score is achieved by INRIA's run using large-scale matching of local features with rigid geometrical models (inria imedia plantnet run1, 68% classification rate). This suggests that modeling leaves as part-based rigid and textured objects might be an interesting alternative to shape boundary approaches that do not characterize well margin details, ribs or limb texture. Second best score on scans is obtained by the run of SABANCI-OKAN (Sabanci-okan-run1) which uses a supervised classification approach based on support vector machine (SVM) and a combination of 3 global visual features. This suggests that combining shape boundary features with other color and shape textures is also a promising direction. Global conclusions on the scan-like photos are quite different (Figure <ref type="figure" coords="12,443.11,608.30,8.30,8.74" target="#fig_7">13</ref>). The best score is obtained by INRIA's run (inria imedia plantnet run2) purely based on a global shape boundary feature (DFH <ref type="bibr" coords="12,318.45,632.21,14.76,8.74" target="#b3">[15]</ref>). It is followed closely by the four runs of LIRIS also based on boundary shape features but using a model-driven approach for the segmentation and the shape estimation. Then come the two runs of INRIA and SABANCI-OKAN that ranked first on the scan images (the first one based on rigid objects matching and the second one training combined features) and finally the shape boundary method of IFSC. One conclusion is that shape boundary methods appear to provide more stable results for scans and scan-like photos. On the other side the rigid object matching method of IN-RIA degrades much more from scans to scan-like pictures. This can be explained by the fact that it is more discriminant regarding the leave's morphology but less robust to light reflections and shadows. These lighting variations might also explain the degrading performances of the combined features used by SABANCI-OKAN. RMIT 0,071 0,000 0,098 0,056 RMIT run2 RMIT 0,061 0,032 0,043 0,045 daedalus run1 DAEDALUS 0,043 0,025 0,055 0,041 UAIC2011 Run02 UAIC 0,000 0,000 0,042 0,014 Table <ref type="table" coords="13,164.75,507.31,4.13,7.89">2</ref>. Normalized classification scores for each run and each image type. Top 3 results per image type are highlighted in bold</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">About using metadata</head><p>Using metadata to help the identification, and more particularly geo-tags, is definitely something that has to be studied. We were therefore very enthusiastic to see the runs of UAIC, aimed at evaluating the potential contributions of Pl@ntLeaves metadata. Unfortunately, results clearly show that adding metadata degrades their identification performances. Metadata alone even give a null classification success rate. Besides technical details that could probably slightly improve such species filtering based on metadata, it still shows that Pl@ntLeaves metadata might be intrinsically not very useful for identification purposes. The main reason is probably that the geographic spread of the data is limited (French mediterranean area). So that most species of the dataset might be identically and uniformly distributed in the covered area. Geo-tags would be for sure more useful at a global scale (continent, countries). But at a local scale, the geographical distribution of plants is much more complex. It usually depends on localized environmental factors such as sun exposition or water proximity that would require much more data to be modeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performances per species</head><p>To evaluate which species are more difficult to identify than others, we averaged the performances over the runs of all participants (for each specie). It is however difficult to understand precisely the score variations. They can be due to morphological variations but also to different view conditions or other statistical bias in the data such as the number of training images. Figure <ref type="figure" coords="14,414.29,608.30,9.96,8.74">16</ref> presents the obtained graph for the scan images only (in order to limit view conditions bias). The only global trend we discovered so far is that simple leaves are on average easier to identify than compound leaves. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performances per plant morphology</head><p>In botany, species are described and categorized by morphological features, frequently on leaves <ref type="bibr" coords="15,211.48,427.23,9.96,8.74">[8]</ref>. These features are very numerous and concern for instance the leaf organization, the margin type, the shapes, the venation, the presence of spines, etc. We attempt here to evaluate which kinds of morphological features make more difficult the species identification, through three essential kinds of features: the leaf organization, the global shape (for simple leaves only) and the margin type. Table <ref type="table" coords="15,237.99,487.01,9.96,8.74" target="#tab_1">17</ref> gives a detailed description of these features for each species involved in the test image dataset.</p><p>To evaluate which leaf organization makes more difficult the species identification, we averaged the performances over the runs of all participants for each kind of organization: simple or compound and palmately compound. Figure <ref type="figure" coords="15,151.89,548.52,9.96,8.74">18</ref> presents the obtained scores for each kind of image. This graph confirms that species with simple leaves are on average easier. However, compound leaves can be subdivided into sub-categories describing how leaflets are organized: pinnately compound (with leaflets arranged along a the main axis called rachis), or palmately compound (with leaflets attached at one same basal point). Figure <ref type="figure" coords="15,134.77,608.30,9.96,8.74" target="#fig_10">19</ref> presents the obtained scores and illustrates that species with palmately compound leaves can be easier than species with simple leaves, at least for the two tested species "Aesculus hippocastanum" and "Vitex agnus-castus". Another interesting point is the difference on scores between scan and scan-like for pinnate compound leaves (there were no scan-like images of palmately compound leaves in the test image dataset). One explanation is that scan-like of compound leaves are often in relief involving disturbing shadows and a global shape .</p><p>In the case of simple leaves, to evaluate which kind of shape makes more difficult the species identification, we averaged the performances over the runs of all participants for each category of shape identified in the test image dataset: asymmetrical, elliptic, lanceolate, linear, lobate, obovate and orbicular. Figure <ref type="figure" coords="16,134.77,453.59,9.96,8.74" target="#fig_0">20</ref> presents the obtained scores for each kinds of shape and image. Results show that species with an orbicular shape are easier to identify. This is confirmed in the graph 16 where three of the six orbicular species in the test image dataset ("acer campestre", "cercis siliquastrum", "corylus avellana") give good results (the best one for "corylus avellana").</p><p>One last important morphological feature studied by botanists is the margin type. In this case, to evaluate which type of margin makes more difficult the species identification, we averaged the performances over the runs of all participants for each category of margin identified in the test image dataset: "untoothed", "dentate", "serrate" and "crenate". Figure <ref type="figure" coords="16,393.37,561.18,9.96,8.74" target="#fig_0">21</ref> presents the obtained scores for each kinds of margin and image. Results show that the margin type weakly affect species identification, except maybe for species with crenate margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Performances per image</head><p>To qualitatively assess which kind of images causes good results and which one makes all methods failed, we sorted all test pictures by the number of runs in Fig. <ref type="figure" coords="17,185.10,349.48,9.04,7.89">15</ref>. Normalized classification scores averaged over all image types which they were correctly identified. The obtained ranking confirms that scan images are much easier for the identification. 99% of 100-top ranked images are actually scans (with 11 to 17 successful runs). Figure <ref type="figure" coords="17,381.02,407.44,9.96,8.74" target="#fig_0">22</ref> displays the 4 best identified images (with 17/20 successful runs). They all are very standard leaf images similar to the one found in books illustrations. On the other side, 260 images were not identified by any run with a majority of unconstrained photos (63 scans, 27 scan-like photos, 168 unconstrained photographs). The scans and scan-like photos belonging to this category of most difficult images are very interesting. As illustrated in Figure <ref type="figure" coords="17,288.53,479.17,8.49,8.74" target="#fig_1">23</ref>, most of them correspond to outlier leaves with defaults or unusual morphology (ill or torn leaves, missing leaflets, etc.). Figure <ref type="figure" coords="17,166.20,503.08,9.96,8.74" target="#fig_1">23</ref> displays 8 of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper presented the overview and the results of ImageCLEF 2011 plant identification testbed. A challenging collaborative dataset of tree's leaf images was specifically built for this evaluation and 8 participants coming from different countries submitted a total of 20 runs. A first conclusion was that identification performances are close from mature when using scans or photos with uniform background but that unconstrained photos are still much more challenging. More data and evaluation are clearly required to progress on such data. Another important conclusion was that sate-of-the-art methods based on shape boundary analysis were not the best ones on leaf scans. Better performances were no-Fig. <ref type="figure" coords="18,154.40,349.26,9.04,7.89">16</ref>. Mean classification score per species averaged over all participant runs -scan test images only tably obtained with a local features matching technique usually more dedicated to the large-scale retrieval of rigid objects. On the other side, shape boundary analysis methods remain better on scan-like photos due to their better robustness to light reflections and shadow effects. This suggests that combining shape boundary features with part-based rigid object models might be an interesting direction. Adding texture and color information also showed some improvements. On the contrary, using additional metadata such as geo-tags was not concluding on the evaluated dataset. Probably because the geographic spread of the data was limited.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,181.60,465.62,252.16,7.89;4,134.77,115.84,345.83,325.05"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the 3 image type categories for 4 species</figDesc><graphic coords="4,134.77,115.84,345.83,325.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,166.63,354.62,282.10,7.89;5,134.77,115.83,345.84,214.05"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An image of Pl@ntLeaves dataset and its associated metadata</figDesc><graphic coords="5,134.77,115.83,345.84,214.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,162.94,219.91,289.48,7.89;6,152.06,240.23,311.24,96.18"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Color variation of Cotinus coggygria Scop. (Eurasian smoketree)</figDesc><graphic coords="6,152.06,240.23,311.24,96.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,183.08,243.81,249.19,7.89;7,186.64,275.47,242.07,91.23"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Leaf's margin variation of Quercus ilex L. (Holm oak)</figDesc><graphic coords="7,186.64,275.47,242.07,91.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,146.31,229.89,322.73,7.89;8,221.22,250.30,172.91,111.49"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Leaflets relative position variation of Vitex agnus-castus L. (Judas Tree)</figDesc><graphic coords="8,221.22,250.30,172.91,111.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,134.77,228.52,345.82,7.89;9,134.77,239.51,113.27,7.86;9,134.77,260.67,345.83,79.92"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Light reflection and shadows variation of scan-like photos of Magnolia Grandiflora (Southern Magnolia)</figDesc><graphic coords="9,134.77,260.67,345.83,79.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="14,193.88,349.66,227.60,7.89;14,136.49,115.84,342.37,209.09"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Normalized classification scores for scan images</figDesc><graphic coords="14,136.49,115.84,342.37,209.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="15,185.68,349.48,243.99,7.89;15,136.49,115.84,342.36,208.91"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Normalized classification scores for scan-like photos</figDesc><graphic coords="15,136.49,115.84,342.36,208.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="16,192.87,349.48,229.62,7.89;16,136.49,115.84,342.36,208.91"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Normalized classification scores for photographs</figDesc><graphic coords="16,136.49,115.84,342.36,208.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="19,134.77,327.05,345.82,7.89;19,134.77,338.04,17.48,7.86;19,186.64,115.84,242.08,186.49"><head>Fig. 17 .Fig. 18 .</head><label>1718</label><figDesc>Fig. 17. Mean classification score per leaf organization averaged over all participant runs</figDesc><graphic coords="19,186.64,115.84,242.08,186.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="21,134.77,397.89,345.83,7.89;21,134.77,408.88,17.48,7.86;21,136.49,426.21,342.38,202.47"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Mean classification score per simple leaf shape averaged over all participant runs</figDesc><graphic coords="21,136.49,426.21,342.38,202.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,134.77,115.84,345.84,284.04"><head></head><label></label><figDesc></figDesc><graphic coords="3,134.77,115.84,345.84,284.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="17,136.49,115.84,342.36,208.91"><head></head><label></label><figDesc></figDesc><graphic coords="17,136.49,115.84,342.36,208.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="18,136.49,115.84,342.38,208.69"><head></head><label></label><figDesc></figDesc><graphic coords="18,136.49,115.84,342.38,208.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="20,169.35,115.84,276.65,189.96"><head></head><label></label><figDesc></figDesc><graphic coords="20,169.35,115.84,276.65,189.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="22,169.35,364.44,276.66,220.03"><head></head><label></label><figDesc></figDesc><graphic coords="22,169.35,364.44,276.66,220.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,136.16,448.93,353.37,154.82"><head>Table 1 .</head><label>1</label><figDesc>Nb of pictures Nb of individual plants Nb of contributors</figDesc><table coords="7,136.16,515.96,314.65,87.79"><row><cell>Scan</cell><cell>Train Test</cell><cell>2349 721</cell><cell>151 55</cell><cell>17 13</cell></row><row><cell>Scan-like</cell><cell>Train Test</cell><cell>717 180</cell><cell>51 13</cell><cell>2 1</cell></row><row><cell>Photograph</cell><cell>Train Test</cell><cell>930 539</cell><cell>72 33</cell><cell>2 3</cell></row><row><cell>All</cell><cell>Train Test</cell><cell>3996 1440</cell><cell>269 99</cell><cell>17 14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,171.10,606.77,273.15,7.89"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the composition of the training and test data</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_0" coords="2,144.73,645.84,126.99,7.86"><p>http://www.imageclef.org/2011</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_1" coords="2,144.73,656.80,123.92,7.86"><p>http://www.tela-botanica.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_2" coords="3,144.73,656.80,194.91,7.86"><p>http://combraille.cirad.fr:8080/demo plantscan/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was funded by the <rs type="funder">Agropolis</rs> fundation through the project Pl@ntNet (http://www.plantnet-project.org/) and the EU through the CHORUS+ Coordination action (http://avmediasearch.eu/)</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="18,142.96,623.92,337.63,7.86;18,151.52,634.88,329.07,7.86;18,151.52,645.84,329.07,7.86;18,151.52,656.80,60.92,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="18,213.12,645.84,218.97,7.86">First steps toward an electronic field guide for plants</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Kress</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shirdhonkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,439.83,645.84,25.08,7.86">Taxon</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="597" to="610" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,375.90,337.98,7.86;20,151.52,386.86,329.07,7.86;20,151.52,397.82,102.51,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="20,231.51,375.90,245.10,7.86">Computer Vision Classification of Leaves from Swedish Trees</title>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">J O</forename><surname>Söderkvist</surname></persName>
		</author>
		<idno>liTH-ISY-EX-3132</idno>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
			<pubPlace>Linköping, Sweden</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Linköping University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="20,142.62,408.77,337.98,7.86;20,151.52,419.73,329.07,7.86;20,151.52,430.69,191.46,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="20,413.97,408.77,66.62,7.86;20,151.52,419.73,311.27,7.86">Daedalus at imageclef 2011 plant identification task: Using sift keypoints for object detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Villena-Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lana-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>González-Cristóbal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,151.52,430.69,162.78,7.86">Working notes of CLEF 2011 conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,441.65,337.98,7.86;20,151.52,452.61,329.07,7.86;20,151.52,463.57,100.85,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="20,313.78,441.65,166.81,7.86;20,151.52,452.61,40.41,7.86">Shape-based image retrieval in botanical collections</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hervé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,213.04,452.61,245.80,7.86">Advances in Multimedia Information Processing -PCM 2006</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4261</biblScope>
			<biblScope unit="page" from="357" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,474.53,337.98,7.86;20,151.52,485.49,308.36,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="20,317.21,474.53,163.38,7.86;20,151.52,485.49,95.91,7.86">Sabanci-okan system at imageclef 2011: Plant identification task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tirkaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,268.43,485.49,162.78,7.86">Working notes of CLEF 2011 conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
