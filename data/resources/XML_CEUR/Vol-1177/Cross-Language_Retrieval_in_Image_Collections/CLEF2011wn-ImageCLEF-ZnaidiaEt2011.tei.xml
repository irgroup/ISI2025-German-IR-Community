<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.84,115.90,333.68,12.90;1,229.23,133.83,156.89,12.90">CEA LIST&apos;s participation to Visual Concept Detection Task of ImageCLEF 2011</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,200.56,171.88,53.44,8.64"><forename type="first">Amel</forename><surname>Znaidia</surname></persName>
							<email>amel.znaidia@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<addrLine>18 route du Panorama, BP6, Fontenay-aux-Roses</addrLine>
									<postCode>F-92265</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,260.60,171.88,66.95,8.64"><forename type="first">Hervé</forename><surname>Le Borgne</surname></persName>
							<email>herve.le-borgne@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<addrLine>18 route du Panorama, BP6, Fontenay-aux-Roses</addrLine>
									<postCode>F-92265</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.44,171.88,63.36,8.64"><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
							<email>adrian.popescu@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<addrLine>18 route du Panorama, BP6, Fontenay-aux-Roses</addrLine>
									<postCode>F-92265</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.84,115.90,333.68,12.90;1,229.23,133.83,156.89,12.90">CEA LIST&apos;s participation to Visual Concept Detection Task of ImageCLEF 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CF252C0B65C38A08691F83FACF29A447</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the CEA LIST participation in the ImageCLEF 2011 Photo Annotation challenge. This year, our motivation was to investigate the annotation performance by using provided Flickr-tags as additionnal information. First, we present an overview of our local and global visual features used in this work. Second, we present a new method, that we call "Fuzzy-tfidf", which takes into account the uncertainty of user tags. Our textual descriptor is based on semantic similarity between tags and visual concepts. To compute this similarity, we used two distances: the first one is based on Wordnet ontology and the second is based on social networks. We perform a late fusion to combine scores from visual and textual modalities. Our best model, a late fusion trained on global visual features and user tags, obtains 38.3 % MAP, almost a 8 % MAP absolute improvement compared to our best visual-only system. The results show that the combination of Flickr-tags with visual features improves the results of the run using only visual features. It corroborates the importance of taking into account the uncertainty of user tags and the complementarity between visual and textual modalities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEF Photo Annotation Task <ref type="bibr" coords="1,303.82,473.04,16.60,8.64" target="#b10">[11]</ref> is a multi-label classification problem, with 8.000 image for training, 10.000 for testing and 99 concepts to detect. The image are extracted from the MIR Flickr dataset <ref type="bibr" coords="1,311.75,496.95,11.62,8.64" target="#b5">[6]</ref> and the Flickr user tags and/or EXIF information are available for most photos.</p><p>In our participation to the ImageCLEF Photo Annotation Task, we focus on how to use the tags associated to the images to enhance the annotation performance. We propose three different models: visual only, textual only and multimodal models. The last model takes the mean of the predicted score of the textual and visual classifiers.</p><p>This paper is organized as follows. In Section 2 we describe our local and global visual features. In Section 3 we give an overview of our "Fuzzy-tfidf" method which uses user tags. Then in Section 4 we present in more detail the experiments we did, the submitted runs and the obtained results. Finally, we conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Visual features</head><p>We used two sets of descriptors, named fklsp and piria5 in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Local descriptors (fklsp)</head><p>This set was based on a non parametric estimation of Fisher vector to agregate local descriptors, as explained in detail in <ref type="bibr" coords="2,280.44,149.15,10.58,8.64" target="#b0">[1]</ref>.</p><p>Fisher kernel, score and vector Let X = {x t , t = 1 . . . T } a set of vectors used to describe an image (i.e a collection of local features). It can be seen as resulting from a generative probability model with density f (X|θ). To derive a kernel function from such a generative model, being able to exhibit discriminative properties as well, Jaakola <ref type="bibr" coords="2,134.77,224.82,11.62,8.64" target="#b7">[8]</ref> proposed to use the gradient of the log-likelihood with respect to the parameters, called the Fisher score:</p><formula xml:id="formula_0" coords="2,258.32,248.42,222.27,9.65">U X (θ) = θ log f (X|θ)<label>(1)</label></formula><p>This transforms the variable length of the sample X into a fixed length vector that can feed a classical learning machine. In the original work of <ref type="bibr" coords="2,374.30,277.09,11.62,8.64" target="#b7">[8]</ref> the Fisher information matrix F λ is suggested to normalize the vector:</p><formula xml:id="formula_1" coords="2,223.51,306.03,257.09,11.72">F λ = E X [ θ log f (X|θ) θ log f (X|θ) T ]<label>(2)</label></formula><p>It then results into the Fisher vector:</p><formula xml:id="formula_2" coords="2,244.54,343.71,236.05,14.30">G X (θ) = F -1/2 λ θ log f (X|θ)<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Density estimation</head><p>The traditional way of modeling a distribution density is to assume a classical parametric model such as normal, gamma or Weibull. For instance in <ref type="bibr" coords="2,461.50,386.96,15.27,8.64" target="#b11">[12]</ref>, the vocabularies of visual words are represented with a Gaussian Mixture Models, for which the parameters (weight, mean and variance of each Gaussian) are estimated by maximum likelihood. Alternatively, we can use a nonparametric estimate of the density, such as a histogram or a kernel-based method <ref type="bibr" coords="2,271.22,446.74,15.27,8.64" target="#b17">[17]</ref>. A histogram density estimation can be seen as modeling the unknown log-density function by a piecewise constant function and estimating the unknown coefficients by maximum likelihood. In this vein, Kooperberg <ref type="bibr" coords="2,463.99,470.65,16.60,8.64" target="#b9">[10]</ref> proposed to model the log-density function by cubic spline, resulting into the so-called logspline density estimation.</p><p>Let consider the space S consisting of the twice-continuously differentiable function f s (natural cubic splines), such that the restriction of f s to some intervals [t 1 , t 2 ] . . . [t K-1 , t K ] is a cubic polynomial and linear at the extremities. Let 1, B 1 , . . . , B p a set of basis functions that span the space S. Given θ = (θ 1 , . . . , θ p ) ∈ R p such that:</p><formula xml:id="formula_3" coords="2,225.67,560.13,254.92,26.29">U L exp (θ 1 B 1 (y)+, . . . , θ p B p (y)dy) &lt; ∞<label>(4)</label></formula><p>We can thus consider the exponential family of distribution based on this basis function:</p><formula xml:id="formula_4" coords="2,209.97,612.84,270.63,9.65">f (y, θ) = exp (θ 1 B 1 (y)+, . . . , θ p B p (y) -C(θ))<label>(5)</label></formula><p>Where C(θ)is a normalizing constant such that f (y, θ) is a density. As shown in <ref type="bibr" coords="2,453.57,632.53,15.27,8.64" target="#b9">[10]</ref>, it is possible to determine the maximum likelihood estimate of θ with a Newton-Raphson method with step-halving.</p><p>Signature derivation Each feature dimension x i (i ∈ [1 . . . D]) of a local descriptor can be thought of as arising as a random sample from a distribution having a density h i for a particular image and f i for a set of images. Modelling the log-density function by a cubic spline and deriving the corresponding Fisher score lead to <ref type="bibr" coords="3,398.75,155.18,10.79,8.64" target="#b0">[1]</ref>:</p><formula xml:id="formula_5" coords="3,214.53,179.10,266.06,29.73">∂L(Y, θ) ∂θ i j θ i j ≈ b θ i j = E h i B i j (y) -E f i B i j (y)<label>(6)</label></formula><p>Where h i (.) is the density of the image descriptor and f i (.) the density class descriptor (dimension i), this last being estimated from local descriptor extracted from several learning images. The full gradient vector U Y (θ) is a concatenation of these partial derivatives with respect to all parameters. Its number of components is D i=1 p i , where p i is the number of non-constant polynomial of the basis of S for dimension i.</p><p>The equation ( <ref type="formula" coords="3,209.19,279.38,3.87,8.64" target="#formula_5">6</ref>) simply reflects the way a specific image (with density h i ) differs from the average world (i.e density f i ), through a well chosen polynomial basis, at each dimension. The average world E f i B i j (y) can be seen as a codebook. If one uses linear polynomials B i j (y) = α j y i , equation ( <ref type="formula" coords="3,329.21,316.71,3.87,8.64" target="#formula_5">6</ref>) relates to the VLAD signature <ref type="bibr" coords="3,465.01,316.71,11.68,8.64" target="#b8">[9]</ref>, with an important difference since all vectors are used (i) during learning to estimate the codeword (ii) during test to compute the signature, while (i) K-means uses the closest vectors of a codeword (cluster center) to re-estimate it at each step (ii) VLAD uses only nearest neighbours to compute the signature component (see eq. ( <ref type="formula" coords="3,396.51,364.53,3.87,8.64" target="#formula_0">1</ref>) in <ref type="bibr" coords="3,416.99,364.53,10.45,8.64" target="#b8">[9]</ref>).</p><p>In his seminal work, Jaakola <ref type="bibr" coords="3,273.02,376.63,11.62,8.64" target="#b7">[8]</ref> proposed to normalize the Fisher score by the Fisher information matrix. In <ref type="bibr" coords="3,257.25,388.58,15.27,8.64" target="#b11">[12]</ref>, it was noted that such an operation improved the efficiency of the method in term of discrimination, by normalizing the dynamic range of the different dimensions of the gradient vector. Although some normalisation of the signature were proposed in <ref type="bibr" coords="3,244.88,424.45,10.58,8.64" target="#b0">[1]</ref>, they were not used in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficient implementation</head><p>The set of basis functions 1, B 1 , . . . , B p that span the space S introduced in section 2.1, are defined according to intervals</p><formula xml:id="formula_6" coords="3,383.37,466.72,97.23,9.65">[t i , t i+1 ] (i ∈ [1 . . . K]),</formula><p>where the t i are named knots. We fixed a given number of knots and placed them according to statistic order of the learning data. Hence, at each dimension, the amount of information is regularly distributed between knots. For low-level features such as those presented in section 2.1, the knots are approximately placed according to a logarithmic distribution.</p><p>Several choices are possible to defined the basis B k . In this work, we used the following basis, that is very efficient to implement:</p><formula xml:id="formula_7" coords="3,224.58,572.33,256.01,47.21">B 0 (y) = 1 (not used) B 1 (y) = y B k&gt;1 (y) = |y-t k |+y-t k 2 for y &lt; t k+1 0 for y &gt; t k+1<label>(7)</label></formula><p>Such an implementation is equivalent to compute only (y -t k ) on the interval [t k , t k+1 ] since the polynomial is null elsewhere and y &gt; t k on the interval. Moreover, we used a binary weighting scheme, that does not consider the value of |y -t k | in the computation but only its existence. In other word, one can only count +1 each time a pixel activity y is between t k and t k+1 . Such a binary weighting scheme is commonly used in the design of BOV, in particular when the codebook is large <ref type="bibr" coords="4,360.52,143.22,15.27,8.64" target="#b18">[18]</ref>.</p><p>Independent low level features According to theory, the signature derivation requires to use independent low-level features, such that the image description density could be expressed as a factorial code. Such features can be obtained with Independent Component Analysis (ICA) <ref type="bibr" coords="4,218.90,208.86,10.79,8.64" target="#b3">[4,</ref><ref type="bibr" coords="4,231.34,208.86,8.30,8.64" target="#b6">7]</ref> that is a class of methods that aims at revealing statistically independent latent variables of observed data. In comparison, the well-known Principal Component Analysis (PCA) would reveal uncorrelated sources, i.e with null moments up to the order two only. In its simplest form, ICA defines a generative model which consider multivariate data X as a linear mixtures of some unknown sources S, and the mixture A is also unknown. Under the assumption that the sources are mutually independent and at most one is Gaussian, <ref type="bibr" coords="4,282.09,280.59,11.62,8.64" target="#b3">[4]</ref> showed that it is possible to solve this ill-posed problem. For this one must compute a separating matrix w that lead to an estimate Y of the sources:</p><formula xml:id="formula_8" coords="4,268.01,316.14,212.58,8.96">Y = W X = W AS<label>(8)</label></formula><p>Many algorithms were proposed to achieve such an estimation, that are well reviewed in <ref type="bibr" coords="4,134.77,346.26,10.58,8.64" target="#b6">[7]</ref>. These authors proposed the fast-ICA algorithm that searches for sources that have a maximal nongaussianity. When applied to natural image patches of fixed size (e.g ∆ = 16×16 = 256), ICA results into a generative model composed of localized and oriented basis functions <ref type="bibr" coords="4,196.03,382.12,10.58,8.64" target="#b6">[7]</ref>. Its inverse, the separating matrix, is composed of independent filters w 1 , . . . , w D (size ∆) that can be used as feature extractors, giving a new representation with mutually independent dimensions. The number of filters (D) extracted by ICA is less or equal to the input data dimension (∆). This can be reduced using a PCA previously to the ICA. The responses of the D filters to some pixels (p 1 , . . . , p T ) of an image I(.) are thus independent realizations of the D-dimensional random vector Y . As a consequence, the density can be factorized as expected:</p><formula xml:id="formula_9" coords="4,210.77,474.31,269.83,30.32">h ica (I(p t )) = D i=1 h i ica (I(p y )) = D i=1 w i * I(p t )<label>(9)</label></formula><p>Where * is the convolution product. These independent low-level features can be further used according to the method presented into section 2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Global descriptors (piria5)</head><p>We concatenated five descriptors to form a single global descriptor of size 1341:</p><p>-A descriptor that is itself the concatenation of a Local Edge Pattern (LEP) descriptor (derived from <ref type="bibr" coords="4,210.21,608.67,11.20,8.64" target="#b2">[3]</ref>) and a color histogram, with a global normalisation on the 576 dimensions.</p><p>-A compact histogram that count how many pixels are 4-connected according to their colors <ref type="bibr" coords="4,199.29,644.51,15.27,8.64" target="#b13">[14]</ref>. -A classic color histogram of size 64.</p><p>-A RGB color histogram of size 125.</p><p>-A HSV color histogram.</p><p>The first descriptor (LEP) gives a piece of information on the texture of the image and the second a weak one on the spatial organisation of the pixels. All other descriptors mainly give information on the colors present in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Textual features</head><p>To improve visual concept annotation, image associated tags can be used. The key idea is to project the tags in the visual concept space. Each tag will be associated with one or more concepts according to their semantic similarities. In this manner, the concept voted by several tags is then considered appropriate to describe the content of the image. For example in Fig. <ref type="figure" coords="5,198.16,285.67,3.74,8.64" target="#fig_0">1</ref>, "strawberry, sugar, spoon, frutella, fresa" will be associated with the visual concept "food" which will be relevant to this image. To compute the similarity between user tags and visual concepts, we use two different distances. The first one is based on Wordnet ontology and the second is based on social networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic similarity</head><p>Wordnet-based similarity First, we rely on the Wu-Palmer measure <ref type="bibr" coords="5,415.34,588.24,15.27,8.64" target="#b19">[19]</ref>, which provides a similarity function for two given concepts, defined by how closely they are related in the hierarchy, i.e., their structural relations as shown in Fig. <ref type="figure" coords="5,413.56,612.15,3.74,8.64" target="#fig_1">2</ref>.</p><p>The conceptual similarity between two concepts C 1 et C 2 is given by: Where C 3 is the least common superconcept of C 1 and C 2 . N 1 , N 2 and N 3 represent respectively the number of nodes on the path from C 1 to C 3 , from C 2 to C 3 and from C 3 to Root. This measure is based on WordNet structure <ref type="bibr" coords="6,344.88,331.30,10.58,8.64" target="#b4">[5]</ref>. This can be seen as a semantic network where each node represents a concept of the real world. Each node consists of a set of synonyms that represent the same concept, this set is called synset. These synsets are connected by arcs that describe relations between concepts. This measure is defined between two synsets s 1 and s 2 by:</p><formula xml:id="formula_10" coords="5,225.33,646.03,255.26,23.23">ConSim(C 1 , C 2 ) = 2 * N 3 N 1 + N 2 + 2 * N 3<label>(10)</label></formula><formula xml:id="formula_11" coords="6,221.40,400.19,259.19,23.23">sim wup (s 1 , s 2 ) = 2 * depth(lcs(s 1 , s 2 )) depth(s 1 ) + depth(s 2 )<label>(11)</label></formula><p>where lcs(s 1 , s 2 ) denotes the least common subsumer (most specific ancestor node) of the two synsets s 1 and s 2 in a WordNet taxonomy, and depth(s) is the length of the path from s to the taxonomy Root. Since a word can belong to more than one synset in WordNet that is, it can have more than one conceptual meaning. We opt to determine the similarity between tags and concepts as the maximum similarity between all their synsets. Let syns(t) denotes the set of synsets that contain the tag t, we define the similarity between a tag t k and a concept C i as:</p><formula xml:id="formula_12" coords="6,144.73,532.93,341.37,9.65">sim W ordnet (t k , C i ) = max{sim wup (s k , s i )|(s k , s i ) ∈ syns(t k )Xsyns(C i )} (12)</formula><p>Flickr-based similarity Second, we rely on the work of Popescu et al. <ref type="bibr" coords="6,426.21,565.15,16.60,8.64" target="#b12">[13]</ref> to define a semantic measure between tags and visual concepts according to their social relatedness. Given two terms T and Q, their social relatedness is defined as follows:</p><formula xml:id="formula_13" coords="6,215.18,608.86,265.42,22.31">SocRel(T, Q) = users(Q, T ) * 1 log(pre(T ))<label>(13)</label></formula><p>where users(Q, T ) is the number of distinct users which associate tag T to a query Q; and pre(T ) is the number of distinct users from a prefetched subset of Flickr users that have tagged photos with tag T . The model obtained from Flickr for a tag t k can be expressed by:</p><formula xml:id="formula_14" coords="7,229.51,140.87,246.93,12.69">M F lickr (t k ) = ∪ N x=1 (weight(T x ), T x ) (<label>14</label></formula><formula xml:id="formula_15" coords="7,476.44,143.26,4.15,8.64">)</formula><p>where N is the number of retained Flickr socially related tags and weight(T x ) is the social normalized social weight of T x using relation <ref type="bibr" coords="7,348.58,173.18,15.27,8.64" target="#b12">(13)</ref>. In this context, we define a semantic similarity between a tag t k and a visual concept C i as:</p><formula xml:id="formula_16" coords="7,217.67,205.43,262.92,23.23">sim F lickr (t k , C i ) = dot(t k , C i ) norm(t k ) * norm(C i )<label>(15)</label></formula><p>where dot(., .) represents the scalar product and norm(.) the vector norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Textual descriptors</head><p>Term weighting is a key method in the context of text classification. As in the vector space model introduced by Salton et al. <ref type="bibr" coords="7,297.60,302.16,16.60,8.64" target="#b16">[16]</ref> to represent text document, we represent the visual concepts as a vector of weights (w i,1 , ..., w i,j , ..., w i,|C| ). In our case, the weight w i,j for a considered concept C i in a document d j is obtained by the product of tf i,j and idf i . The term frequency characterizes the frequency of a concept in the given image and it is calculated as follows:</p><formula xml:id="formula_17" coords="7,273.90,366.88,206.69,24.72">tf i,j = n i,j k n k,j<label>(16)</label></formula><p>where n i,j is the number of occurrences of the considered concept C i in document d j and the denominator is the sum of number of occurrences of all visual concepts in the document. The inverse document frequency is a measure of the general importance of the visual concept and it is given by:</p><formula xml:id="formula_18" coords="7,256.58,453.85,219.86,23.22">idf i = log( |D| |j : C i ∈ d j | ) (<label>17</label></formula><formula xml:id="formula_19" coords="7,476.44,460.91,4.15,8.64">)</formula><p>where |D| is the total number of images in the corpus and |j : C i ∈ d j | is the number of images where the concept C i appears. In this manner, we should perform a hard assignment to determine the presence or the absence of a concept (1 or 0). Or the semantic similarity between tags and a visual concept is not equal. Moreover, users usually do not use the same visual concepts to tag their photos. Then, it is more appropriate to proceed on a soft assignment in which a tag is matched to a visual concept with some confidence value. This confidence value represents the uncertainty of the presence of a visual concept. Ideally, if the user use the same visual concept to tag his photo, this value is equal to 1. Else, it is a value between 1 and 0 depending on how similar they are.</p><p>In this context, we propose a new version of tf idf , that we call "Fuzzy-tfidf". In this method, instead of hard assignment of a concept to a given tag, we add a confidence score. This score is the semantic similarity between a tag t k and a concept C i using formulas <ref type="bibr" coords="7,160.98,644.48,16.60,8.64" target="#b11">(12)</ref> or ccc. In this way, we take into account the ammount of similarity between tags and visual concepts. Let s k,i denotes the conceptual similarity between a tag t k and visual concept C i . T represents the set of tags in document d j . C and D represent respectively the set of visual concepts and documents in the dataset. The fuzzy term frequency is obtained by:</p><formula xml:id="formula_20" coords="8,236.28,158.51,244.31,25.38">f uzzy -tf i,j = k∈T s k,i i∈C k∈T s k,i<label>(18)</label></formula><p>The Fuzzy inverse document frequency is computed as follows:</p><formula xml:id="formula_21" coords="8,228.37,204.67,252.22,28.30">f uzzy -idf i = log( |D| j∈D P k∈T s k,i ni,j )<label>(19)</label></formula><p>where n i,j is the number of occurrences of the considered concept C i in document d j .</p><p>The f uzzy -tf idf is obtained by the product of the obove two frequencies. In case s k,i is equal to 1, we found the same formula as the classic tf idf . In this method, we consider only concepts that are similar to the considered tag in a neighborhood. This neighborhood is deterimed by cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We evaluated our annotation methods on the MIR Flickr dataset <ref type="bibr" coords="8,398.25,351.58,11.62,8.64" target="#b5">[6]</ref> containing 8.000 images for training and 10.000 for testing belonging to 99 concept classes. This year a special focus is laid to the detection of sentiment concepts (funny, scary, unpleasant, active, happy ...). Fig. <ref type="figure" coords="8,228.16,387.44,4.98,8.64" target="#fig_2">3</ref> shows samples of images taken from the ImageCLEF 2011 Photo Annotation Task Dataset with their annotated concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Submitted runs</head><p>We submitted five runs to the campaign, allowing relevant comparison between the methods: CEALIST text Fsb use only the textual feature computed according to the method presented in section 3.2. In this run, we use the Wordnet-based semantic similarity and we use the Fuzzy-tfidf to compute the textual descriptor. Two terms are considered semantically similar if their sim W ordnet is upper than a threshold α obtained by cross validation. In our experiments, this threshold is fixed to 0.8. The Fast Shared Boosting algorithm <ref type="bibr" coords="8,176.00,524.93,11.62,8.64" target="#b1">[2]</ref> is applied for classification.</p><p>CEALIST piria5 FsbRdsa use only global visual descriptors presented in section 2.2. It is the concatenation of five descriptors of color and texture.</p><p>CEALIST piria5 FsbRdsa text is a multimodal run where a late fusion process is performed. Scores of the late fusion are obtained by averaging the scores of the two previous runs. Both visual and textual scores are normalized before fusion.</p><p>CEALIST piria5 FsbRdsa text2 is our best run and it is also a multimodal one. It is a late fusion of the visual scores of the second run and the textual scores of the same method as the first run but this time using Flickr-based similarity.</p><p>CEALIST fklsp FsbRdsa text2 is a multimodal run where a late fusion process is performed between a visual classifier based on local visual descriptors presented in section 2.1 and the same textual classifier used in the previous run.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of the Results</head><p>Performance evaluation To determine the quality of the annotations five measures were used, three for the evaluation per concept and two for the evaluation per photo. For the concept based evaluation the mean Average Precision (MAP), the equal-error-rate (EER), and the area-under the curve (AUC) are used, using the confidence scores. For the example based evaluation, F-measure (F-ex) <ref type="bibr" coords="9,333.41,493.07,16.60,8.64" target="#b15">[15]</ref> and Semantic R-Precision (SR-Precision) are used. The SR-Precision is a novel performance measure derived from the example-based R-Precision measure. In contrast to R-Precision, it considers the Flickr Tag Similarity measure to determine the semantic relatedness of misclassified concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview of Results</head><p>In Table <ref type="table" coords="9,254.45,540.89,3.74,8.64" target="#tab_1">1</ref>, we list the performance of our submitted runs. We can notice that the use of user tags improves significantly the results (≈ 8 % MAP). The textual run (run 1) based on WordNet gave the same MAP as our best visual only run (run 2). We tested also run 1 with the Flickr-based similarity and it gaves 0.31 MAP. Fig. <ref type="figure" coords="10,184.83,155.18,4.98,8.64">4</ref>   Our goal for the ImageCLEF 2011 Photo Annotation challenge was to take advantage of the available user tags as additional information. Results have shown that all our methods combining visual and textual modalities outperform our visual only classiers. Our best scoring classier obtains 38.3 % in MAP, ≈ 8 % higher than our best visual-only system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgment</head><p>This work was supported by grants from DIGITEO and Rgion Ile-de-France.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,213.05,486.40,189.25,8.12;5,202.74,338.07,173.94,127.94"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example of image with its associated tags.</figDesc><graphic coords="5,202.74,338.07,173.94,127.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,237.64,272.30,140.07,8.12;6,222.64,115.83,170.09,141.73"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The concept similarity measure.</figDesc><graphic coords="6,222.64,115.83,170.09,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,134.77,357.35,345.83,8.12;9,134.77,368.66,89.40,7.77"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Samples of images taken from the ImageCLEF 2011 Photo Annotation Task Dataset with their annotated concepts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,231.45,622.39,152.45,8.12"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The Average Precision per concept.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,136.16,580.94,322.13,73.83"><head>Table 1 .</head><label>1</label><figDesc>Overview of the different submissions.</figDesc><table coords="9,136.16,580.94,322.13,62.97"><row><cell>Run</cell><cell cols="2">Modality MAP EER</cell><cell>AUC F-ex</cell><cell>SR-Pr</cell></row><row><cell>1: CEALIST text Fsb</cell><cell>T</cell><cell cols="3">0.292 0.356 0.684 0.478 0.675</cell></row><row><cell>2: CEALIST piria5 FsbRdsa</cell><cell>V</cell><cell cols="3">0.300 0.290 0.774 0.503 0.700</cell></row><row><cell cols="2">3: CEALIST piria5 FsbRdsa text V&amp; T</cell><cell cols="3">0.372 0.259 0.808 0.497 0.704</cell></row><row><cell cols="2">4: CEALIST piria5 FsbRdsa text2 V&amp; T</cell><cell cols="3">0.383 0.250 0.819 0.508 0.710</cell></row><row><cell cols="2">5: CEALIST fklsp FsbRdsa text2 V&amp; T</cell><cell cols="3">0.347 0.283 0.784 0.484 0.693</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,134.77,155.18,345.82,447.95"><head></head><label></label><figDesc>and 5 show the Average Precision (AP) of our best ImageCLEF run through different classes. Ov er ex po se d Un de re xp os ed Ne ut ra l_I llu m in at io n M ot io n_ Bl ur Ou t_ of _fo cu s Pa rtl y_ Bl ur re d No _B lu r Si ng le _P er so n Sm al l_G ro up Bi g_ Gr ou p No _P er so ns An im al s Fo od Ve hi cle Ae st he tic _Im pr es sio n Ov er al l_Q ua lit yFig. 4. The Average Precision per concept.</figDesc><table coords="10,141.58,210.52,331.90,392.61"><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0,8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0,6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0,4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0,2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pa rty lif e Fa m ily _F rie nd s Be ac h_ Ho lid ay s Bu ild in g_ Si gh ts Sn ow Ci ty lif e La nd sc ap e_ Na tu re Sp or ts De se rt Sp rin g Su m m er Au tu m n W in te r In do or Ou td oo r Pl an ts Flo we rs Tre es</cell><cell cols="3">Sk y Cl ou ds W at er La ke Ri ve r Se a M ou nt ai ns</cell><cell>Da y Ni gh t Su nn y Su ns et _S un ris e St ill_ Lif e M ac ro Po rtr ai t</cell><cell></cell><cell>Fa nc y</cell></row><row><cell cols="2">Ar ch ite ct ur e St re et Ch ur ch Br idg e Pa rk _G ar de n Ra in Mu sic alI ns tru m en t To y Sh ad ow bo dy pa rt Tra ve l W or k Bi rth da y Vi su al_ Ar ts Gr affi ti Pa int ing ar tifi cia l na tu ra l te ch nic al ab str ac t bo rin g cu te</cell><cell>do g</cell><cell cols="2">ca t bir d ho rs e fis h ins ec t ca r bic yc le sh ip tra in air pla ne sk at eb oa rd fe m ale</cell><cell>m ale</cell><cell>Ba by Ch ild Te en ag er Ad ult old _p er so n ha pp y fu nn y eu ph or ic ac tiv e sc ar y un ple as an t m ela nc ho lic ina ct ive</cell><cell>ca lm</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.61,298.05,337.98,7.77;11,150.95,309.01,329.64,7.77;11,150.95,319.97,44.58,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,279.62,298.05,200.97,7.77;11,150.95,309.01,62.71,7.77">Nonparametric estimation of fisher vectors to aggregate image descriptors</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Muñoz-Fuentes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,232.46,309.01,180.32,7.77">Proc ACIVS. Lecture Notes in Computer Science</title>
		<meeting>ACIVS. Lecture Notes in Computer Science<address><addrLine>Ghent, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6915</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,330.73,337.98,7.77;11,150.95,341.69,329.63,7.77;11,150.95,352.65,172.08,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,254.98,330.73,225.61,7.77;11,150.95,341.69,31.60,7.77">Fast shared boosting: Application to large-scale visual concept detection</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">L</forename><surname>Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Honnorat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,260.83,341.69,219.76,7.77;11,150.95,352.65,36.07,7.77">International Workshop on Content Based Multimedia Indexing, CBMI</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Quénot</surname></persName>
		</editor>
		<meeting><address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,363.41,337.98,7.77;11,150.95,374.37,66.50,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,241.93,363.41,184.89,7.77">Image classification using color, texture and regions</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,432.60,363.41,47.99,7.77;11,150.95,374.37,40.36,7.77">Image Vision Computing</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,385.12,337.98,7.77;11,150.95,396.08,39.60,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,192.88,385.12,177.25,7.77">Independent component analysis, a new concept?</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,372.40,385.12,64.04,7.77">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="314" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,406.84,337.98,7.77;11,150.95,417.80,146.43,7.77" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,220.32,406.84,260.27,7.77;11,150.95,417.80,41.78,7.77">WordNet: An Electronic Lexical Database (Language, Speech, and Communication)</title>
		<editor>Fellbaum, C.</editor>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,428.56,337.98,7.77;11,150.95,439.52,329.64,7.77;11,150.95,450.47,82.28,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,248.74,428.56,121.59,7.77">The mir flickr retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,389.47,428.56,91.12,7.77;11,150.95,439.52,281.92,7.77">MIR &apos;08: Proceedings of the 2008 ACM International Conference on Multimedia Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,461.23,337.98,7.77;11,150.95,472.19,42.58,7.77" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<title level="m" coord="11,286.37,461.23,120.53,7.77">Independent Component Analysis</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,482.95,337.98,7.77;11,150.95,493.91,53.04,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,240.64,482.95,201.75,7.77">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jaakola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,458.92,482.95,21.67,7.77">NIPS</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,504.67,337.98,7.77;11,150.95,515.62,236.34,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,316.75,504.67,163.84,7.77;11,150.95,515.62,73.66,7.77">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,242.59,515.62,20.53,7.77">CVPR</title>
		<meeting><address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">june 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,526.38,338.35,7.77;11,150.95,537.34,192.02,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,252.64,526.38,164.59,7.77">Logspline density estimation for censored data</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kooperberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,422.54,526.38,58.05,7.77;11,150.95,537.34,123.28,7.77">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="301" to="328" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,548.10,338.35,7.77;11,150.95,559.06,180.77,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,283.89,548.10,196.70,7.77;11,150.95,559.06,41.51,7.77">The clef 2011 photo annotation and concept-based retrieval tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liebetrau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,210.28,559.06,95.30,7.77">CLEF 2011 working notes</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,569.82,338.35,7.77;11,150.95,580.77,93.15,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,252.43,569.82,224.63,7.77">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,163.16,580.77,25.66,7.77">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,591.53,338.35,7.77;11,150.95,602.49,329.64,7.77;11,150.95,613.45,101.22,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,259.35,591.53,125.34,7.77">Social media driven image retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno>ICMR &apos;11</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,401.84,591.53,78.75,7.77;11,150.95,602.49,203.07,7.77">Proceedings of the 1st ACM International Conference on Multimedia Retrieval</title>
		<meeting>the 1st ACM International Conference on Multimedia Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="33" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,624.21,338.35,7.77;11,150.95,635.17,329.64,7.77;11,150.95,646.13,259.42,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,304.98,624.21,175.61,7.77;11,150.95,635.17,158.68,7.77">A compact and efficient image retrieval approach based on border/interior pixel classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">O</forename><surname>Stehling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">X F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,329.57,635.17,151.02,7.77;11,150.95,646.13,203.55,7.77">Proceedings of the eleventh international conference on Information and knowledge management</title>
		<meeting>the eleventh international conference on Information and knowledge management</meeting>
		<imprint>
			<biblScope unit="page" from="102" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,413.71,646.13,66.88,7.77;11,150.95,657.08,44.08,7.77" xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Virginia</forename><surname>Mclean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,119.96,299.17,7.77" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="12,227.77,119.96,76.27,7.77">Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>Butterworths</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
	<note>2 edn.</note>
</biblStruct>

<biblStruct coords="12,142.24,130.92,338.35,7.77;12,150.95,141.88,148.04,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,276.06,130.92,162.54,7.77">A vector space model for automatic indexing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,444.97,130.92,35.62,7.77;12,150.95,141.88,20.07,7.77">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975-11">November 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,152.84,338.35,7.77;12,150.95,163.80,23.90,7.77" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="12,218.10,152.84,184.91,7.77">Density estimation for statistics and data analysis</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Chapman and Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,174.76,338.35,7.77;12,150.95,185.71,158.31,7.77" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,237.37,174.76,239.56,7.77">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,163.16,185.71,22.51,7.77">ICCV</title>
		<imprint>
			<date type="published" when="2003-04">April 2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,196.67,338.35,7.77;12,150.95,207.63,329.64,7.77;12,150.95,218.59,329.64,7.77;12,150.95,229.55,236.70,7.77" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,251.10,196.67,155.30,7.77">Verb semantics and lexical selection</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<ptr target="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.1869" />
	</analytic>
	<monogr>
		<title level="m" coord="12,437.58,196.67,43.01,7.77;12,150.95,207.63,280.80,7.77">32nd. Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Las Cruces, New Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>New Mexico State University</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
