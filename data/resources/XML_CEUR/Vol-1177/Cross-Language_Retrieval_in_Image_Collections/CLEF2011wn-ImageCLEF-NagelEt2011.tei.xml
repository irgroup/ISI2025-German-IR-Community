<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.83,115.96,307.69,12.62;1,224.74,133.89,165.88,12.62">The Fraunhofer IDMT at ImageCLEF 2011 Photo Annotation Task</title>
				<funder ref="#_8nhhqcY">
					<orgName type="full">Ministry of Economics</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,172.59,172.29,58.83,8.74"><forename type="first">Karolin</forename><surname>Nagel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Technology (IDMT)</orgName>
								<orgName type="institution">Fraunhofer Institute for Digital Media</orgName>
								<address>
									<addrLine>Ehrenbergstr. 31</addrLine>
									<postCode>98693</postCode>
									<settlement>Ilmenau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.31,172.29,64.66,8.74"><forename type="first">Stefanie</forename><surname>Nowak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Technology (IDMT)</orgName>
								<orgName type="institution">Fraunhofer Institute for Digital Media</orgName>
								<address>
									<addrLine>Ehrenbergstr. 31</addrLine>
									<postCode>98693</postCode>
									<settlement>Ilmenau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,312.65,172.29,57.03,8.74"><forename type="first">Uwe</forename><surname>KÃ¼hhirt</surname></persName>
							<email>uwe.kuehhirt@idmt.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="department">Technology (IDMT)</orgName>
								<orgName type="institution">Fraunhofer Institute for Digital Media</orgName>
								<address>
									<addrLine>Ehrenbergstr. 31</addrLine>
									<postCode>98693</postCode>
									<settlement>Ilmenau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,392.38,172.29,50.39,8.74"><forename type="first">Kay</forename><surname>Wolter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Technology (IDMT)</orgName>
								<orgName type="institution">Fraunhofer Institute for Digital Media</orgName>
								<address>
									<addrLine>Ehrenbergstr. 31</addrLine>
									<postCode>98693</postCode>
									<settlement>Ilmenau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.83,115.96,307.69,12.62;1,224.74,133.89,165.88,12.62">The Fraunhofer IDMT at ImageCLEF 2011 Photo Annotation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3431186F82E095B5C4E1955A4538C329</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image annotation</term>
					<term>multi-modal fusion</term>
					<term>tag features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the participation of the Fraunhofer IDMT in the ImageCLEF 2011 Photo Annotation Task. Our approach is focused on text-based features and strategies to combine visual and textual information. First, we apply a pre-processing step on the provided Flickr tags to reduce noise. For each concept, tf-idf values per tag are computed and used to construct a text-based descriptor. Second, we extract RGB-SIFT descriptors using the codebook approach. Visual and text-based features are combined, once with early fusion and once with late fusion. The concepts are learned with SVM classifiers. Further, a post-processing step compares tags and concept names to each other. Our submission consists of one text-only and four multi-modal runs. The results show, that a combination of text-based and visual-features improves the result. Best results are achieved with the late fusion approach. The post-processing step only improves the results for some concepts, while others worsen. Overall, we scored a Mean Average Precision (MAP) of 37.1% and an example-based F-Measure (F-ex) of 55.2%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEF 2011 Photo Annotation Task challenges participants to evaluate their multi-label image annotation approaches on a set of Flickr images with the goal to achieve the most accurate annotation of these images. The images belong to 99 different concepts. These range from scene descriptions such as place and time over abstract categories, e.g., partylife to very specific concepts such as dog or car. This year's newly added concepts focus on emotions that the images convey, e.g., happy or melancholic. In addition to the images and concept associations, the participants are provided with the Flickr user tags and EXIF data of the images. A detailed overview of the data set and the task can be found in <ref type="bibr" coords="1,187.90,631.48,9.96,8.74" target="#b0">[1]</ref>.</p><p>Our main objective to solve this task is to explore how tags can be combined with visual features in order to optimize the annotation result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DATASET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>In Figure <ref type="figure" coords="2,181.88,324.66,3.87,8.74" target="#fig_0">1</ref>, an overview of our annotation system is shown. We use visual and textual information of the training data to learn models. These are then employed to annotate the test data. Afterwards, a post-processing step is applied.</p><p>The following sections describe each step in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Extraction</head><p>Visual Features: As our focus lies on the text-based features and the combination of different modalities, we only use one visual descriptor. The baseline makes use of dense-sampled RGB-SIFT descriptors <ref type="bibr" coords="2,369.90,434.69,9.96,8.74" target="#b1">[2]</ref>. These scale-invariant features describe the form and shape of a region around a certain pixel using edge orientation histograms <ref type="bibr" coords="2,259.70,458.60,9.96,8.74" target="#b2">[3]</ref>. They are extracted on a 6 pixel wide grid and post-processed with a k -means algorithm to generate a dictionary which contains 2,000 visual words.</p><p>Text-based Features: We use the Flickr user tags to construct text-based feature descriptors. As tagging on Flickr is relatively free, tags exist in different languages and word variations. In order to reduce this redundancy, we pre-process the tags prior to the generation of textual features. First, all Flickr user tags are translated into English by using the Google Translate API [4]. Afterwards, tags are stemmed with the help of the Porter Stemming Algorithm <ref type="bibr" coords="2,432.08,572.39,10.52,8.74" target="#b3">[5]</ref> in order to merge word variations like explorer -explored into one tag. We employ a supervised approach which learns tag frequencies on the concepts of the training set. Similar to the group of Meiji University <ref type="bibr" coords="2,427.00,608.30,9.96,8.74" target="#b4">[6]</ref>, conceptbased tf-idf weights <ref type="bibr" coords="2,223.62,620.25,10.52,8.74" target="#b5">[7]</ref> are assigned to each tag. A tag's term frequency (tf ) is detected by counting the number of times the tag occurs in a certain concept. The document frequency (df ) term is equivalent to the fraction of concepts the tag t appears in, as shown in Equation <ref type="formula" coords="2,304.64,656.12,3.87,8.74" target="#formula_1">1</ref>. Therefore, tags that appear very often in only a few concepts get higher weights assigned than tags that appear fairly often in many concepts:</p><formula xml:id="formula_0" coords="3,226.74,157.92,22.43,9.65">df t =</formula><p>number of concepts with tag t total number of concepts .</p><p>Finally, the inverse document frequency (idf ) is calculated as log(df t ).</p><p>For each concept, the tf-idf values of the tags of an image are summed up. This leads to a feature vector containing 99 elements with scores normalized in the range of [0; 1]. These features are then employed in the learning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Concept Learning and Annotation</head><p>For each concept, a SVM with RBF kernel is learned using the one-against-all strategy and optimized with the concept-based F-Measure on the training set. To combine visual and textual features, we employ two different approaches: early fusion and late fusion.</p><p>For the early fusion approach, both, visual and text-based features, are considered simultaneously to learn the SVM models. The late fusion approach learns SVM models for each modality separately and then combines the classification results using the geometric mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Post-processing</head><p>To further optimize the annotation result, we apply a simple post-processing step. Each image's tags are again translated and stemmed and afterwards compared to the concept names, which are stemmed as well. In case a concept consist of more than one word, the tags are compared to each of these words. If a tag and at least one word of the concept match, the image is assigned to that concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submission</head><p>We submitted five different runs in total. One run uses only textual information, the other four runs make use of multi-modal information sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â¢ Tags only â¢ Early fusion of RGB-SIFT and tags â¢ Early fusion of RGB-SIFT and tags with post-processing step â¢ Late fusion of RGB-SIFT and tags â¢ Late fusion of RGB-SIFT and tags with post-processing step 4 Results and Discussion</head><p>The results are evaluated with concept-based and example-based performance measures. Detailed information about the evaluation process can be found in <ref type="bibr" coords="3,467.30,656.12,9.96,8.74" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Early Fusion of RGB-SIFT and Tags</head><p>Late Fusion of RGB-SIFT and Tags Fig. <ref type="figure" coords="5,430.86,501.84,4.37,7.75">3</ref>: Comparison of the results of early fusion and late fusion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation per Concept</head><p>In Table <ref type="table" coords="6,174.97,314.19,3.87,8.74" target="#tab_0">1</ref>, the final scores for the concept-based evaluation with the MAP are presented. Overall, our system scored a best run of 37.1% MAP for the multimodal approach. The text-only approach results in a MAP of 32.6%. The late fusion approach outperformed the early fusion one by about 3% (37.1% versus 34.7%). The post-processing step does not improve the result of the late fusion approach, though it increases the results for the early fusion run. Figure <ref type="figure" coords="6,166.41,387.24,4.98,8.74">2</ref> shows that the post-processing actually works well for some concepts, while the detection performance for others worsens. Concepts that suffer the most from the post-processing step are those whose names consist of more than one word, e.g., park or garden, small group or old person. Meanwhile, concepts like cat, horse, airplane, or skateboard improve significantly. The main reason for this is the rather simple approach of the post-processing step. The consideration of composite concepts should help to improve the performance.</p><p>For most of the concepts, early and late fusion perform quite similarly. The main difference can be found for the concepts abstract, boring and cute as well as the different kinds of animals and vehicles. Here, late fusion outperforms early fusion, as can be seen in Figure <ref type="figure" coords="6,275.40,508.11,3.87,8.74">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation per Example</head><p>Table <ref type="table" coords="6,162.52,571.11,4.98,8.74" target="#tab_1">2</ref> shows the overall results of the example-based evaluation. Best results are achieved with a late fusion of RGB-SIFT and tag features and the postprocessing step, scoring an F-Measure of 55.2%. Early fusion of RGB-SIFT and tags resulted in the best Semantic R-Precision (SR-Precision) with 71.3%.</p><p>Using the example-based F-Measure, late fusion performs slightly better than early fusion, whereas the results for early fusion are better using the SR-Precision. Furthermore, the post-processing step seems to improve the results marginally. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>The first participation of Fraunhofer IDMT in the ImageCLEF Photo Annotation Task reveals promising results. Using our textual descriptor in combination with one visual descriptor, we achieve annotation results that can compete well with other systems. The textual features work especially well for rather specific concepts that describe objects in an image. A combination of different textual and visual features is likely to result in a very stable annotation. Future work will consider relations between tags as well as concepts more intently. Additionally, the inclusion of more visual features and text-based descriptors will be a main objective.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,215.11,254.26,185.14,8.74"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Overview of the annotation system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.77,127.36,345.82,129.88"><head>Table 1 :</head><label>1</label><figDesc>Results of the runs for the evaluation per conceptin terms of MAP. The best run is marked in bold letters.</figDesc><table coords="6,162.10,160.61,19.74,7.89"><row><cell>Run</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,127.36,345.83,151.80"><head>Table 2 :</head><label>2</label><figDesc>Results of the runs for the evaluation per example. Evaluation measures are the F-ex and the SR-Precision. The best run is marked in bold letters.</figDesc><table coords="7,162.10,160.61,287.91,118.56"><row><cell>Run</cell><cell>F-ex</cell><cell>SR-Precision</cell></row><row><cell>Tags</cell><cell>0.5254</cell><cell>0.6767</cell></row><row><cell>Early fusion RGB-SIFT &amp; tags</cell><cell>0.5413</cell><cell>0.7128</cell></row><row><cell>Early fusion RGB-SIFT &amp; tags +</cell><cell>0.5416</cell><cell>0.7121</cell></row><row><cell>post-processing</cell><cell></cell><cell></cell></row><row><cell>Late fusion RGB-SIFT &amp; tags</cell><cell>0.5512</cell><cell>0.7014</cell></row><row><cell>Late fusion RGB-SIFT &amp; tags +</cell><cell>0.5519</cell><cell>0.7014</cell></row><row><cell>post-processing</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was partly supported by grant <rs type="grantNumber">01MQ07017</rs> of the <rs type="programName">German research program THESEUS</rs> funded by the <rs type="funder">Ministry of Economics</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8nhhqcY">
					<idno type="grant-number">01MQ07017</idno>
					<orgName type="program" subtype="full">German research program THESEUS</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,557.55,342.24,7.86;7,146.91,568.51,333.68,7.86;7,146.91,579.47,79.69,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,311.27,557.55,169.32,7.86;7,146.91,568.51,124.10,7.86">The CLEF 2011 Photo Annotation and Concept-based Retrieval Tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liebetrau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,296.92,568.51,106.45,7.86">CLEF 2011 working notes</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,590.64,342.25,7.86;7,146.91,601.59,333.68,7.86;7,146.91,612.53,144.24,7.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,365.87,590.64,114.73,7.86;7,146.91,601.59,130.29,7.86">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,287.49,601.59,193.11,7.86;7,146.91,612.55,45.57,7.86">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,623.72,342.24,7.86;7,146.91,634.65,267.35,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,201.18,623.72,237.23,7.86">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,448.32,623.72,32.27,7.86;7,146.91,634.68,138.96,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004-11">November 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,119.67,342.24,7.86;8,146.91,130.61,170.52,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,201.36,119.67,135.87,7.86">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,349.39,119.67,131.20,7.86;8,146.91,130.63,81.07,7.86">Program: electronic library and information systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,141.59,342.24,7.86;8,146.91,152.55,333.68,7.86;8,146.91,163.51,146.42,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,312.53,141.59,168.06,7.86;8,146.91,152.55,252.74,7.86">Meiji University at the ImageCLEF2010 Visual Concept Detection and Annotation Task: Working notes</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Motohashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Izawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,420.93,152.55,59.66,7.86;8,146.91,163.51,55.06,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,174.47,342.24,7.86;8,146.91,185.43,141.14,7.86" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>SchÃ¼tze</surname></persName>
		</author>
		<title level="m" coord="8,325.71,174.47,151.00,7.86">Introduction to Information Retrieval</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
