<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,133.77,164.80,343.72,15.15;1,142.40,186.72,326.44,15.15">The University of Amsterdam&apos;s Concept Detection System at ImageCLEF 2011</title>
				<funder>
					<orgName type="full">STW SEARCHER</orgName>
				</funder>
				<funder>
					<orgName type="full">FES COMMIT</orgName>
				</funder>
				<funder ref="#_5b2WsdR">
					<orgName type="full">Department of Interior National Business Center</orgName>
				</funder>
				<funder>
					<orgName type="full">IARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,199.78,220.61,44.27,8.74"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems Lab Amsterdam</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.50,220.61,57.84,8.74;1,333.03,220.61,43.16,8.74"><forename type="first">Cees</forename><forename type="middle">G M</forename><surname>Van De Sande</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems Lab Amsterdam</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,385.45,220.61,26.02,8.74"><surname>Snoek</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems Lab Amsterdam</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,133.77,164.80,343.72,15.15;1,142.40,186.72,326.44,15.15">The University of Amsterdam&apos;s Concept Detection System at ImageCLEF 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">98478E3F03CC7797F22BB3490418FA61</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The University of Amsterdam participated in the photo annotation task and the concept-based retrieval task of ImageCLEF 2011. In the per-image evaluation of the photo annotation task, we achieve the highest score overall. For the concept-based retrieval task, we submitted the best visual-only run. For the concept-based retrieval task, we considered three ways to perform visual retrieval: fully automatic, human topic mapping and human topic inspection. For a fully automatic system, including more random negatives to train a topic model improves results. For a human selecting relevant concepts to the topic, multiplication fusion works better than summation. For human topic inspection, a relevance feedback scheme on the train data gives an 8-fold increase in the number of positive examples per topic. Depending on the topic, the human topic mapping (best for 21 topics) and inspection (best for 17 topics) give the best results. An oracle fusion of the different methods would increase MAP from 0.100 for our best run to 0.128 overall.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The University of Amsterdam participated in the photo annotation task and the concept-based retrieval task of ImageCLEF 2011. The Large-Scale Visual Concept Detection Task <ref type="bibr" coords="1,225.50,532.54,10.52,8.74" target="#b4">[5]</ref> evaluates visual concept detectors. The concepts used are from the personal photo album domain: beach holidays, snow, plants, indoor, mountains, still-life, small group of people, portrait. For more information on the dataset and concepts used, see the overview paper <ref type="bibr" coords="1,372.07,568.41,9.96,8.74" target="#b4">[5]</ref>. Our participation in the last two years, in ImageCLEF 2009/2010, focussed on increasing the robustness of the individual concept detectors based on the bag-of-words approach, and less on the per-image evaluation.</p><p>Last years experiments <ref type="bibr" coords="1,254.87,616.23,8.19,8.74" target="#b5">[6]</ref><ref type="bibr" coords="1,263.06,616.23,4.10,8.74" target="#b6">[7]</ref><ref type="bibr" coords="1,263.06,616.23,4.10,8.74" target="#b7">[8]</ref><ref type="bibr" coords="1,267.16,616.23,8.19,8.74" target="#b8">[9]</ref><ref type="bibr" coords="1,277.01,616.23,12.73,8.74" target="#b10">11]</ref> emphasize in particular the role of visual sampling, the value of color invariant features, the influence of codebook construction, and the effectiveness of kernel-based learning parameters. This was successful, resulting in the best visual only run for the photo annotation task in terms of MAP. Speedups using parallel computing were investigated in <ref type="bibr" coords="1,444.82,664.05,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="1,461.98,664.05,11.62,8.74" target="#b11">12]</ref>.</p><p>In 2009, the per-image evaluation suggested that the assignment of concept tags to images leaves room for improvement. The primary evaluation metric used in 2010 and beyond for the per-image evaluation was the average example-based F-measure. We have looked into optimizing this measure with our system.</p><p>A new task for this year is the concept-based retrieval task. By extending the test set to 200,000 images, this ensures that systems need to have reasonable computation times. Another difference in this task is that there are no predefined concepts, but a collection of 40 topics. These topics are typically combinations of several existing ImageCLEF concepts, but can have complex boolean expressions within them. They come in the form of a textual description and up to 5 example images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Photo Annotation</head><p>Our concept detection system is an improved version of the system from the ImageCLEF book <ref type="bibr" coords="2,218.78,314.23,9.96,8.74" target="#b3">[4]</ref>, where we have performed additional experiments <ref type="bibr" coords="2,466.96,314.23,10.52,8.74" target="#b7">[8]</ref> which give insight into the effect of different sampling methods, color descriptors and spatial pyramid levels within the bag-of-words model. Our runs this year roughly correspond to Harris-Laplace and dense sampling every 6 pixels (multi-scale) with 4-SIFT and Harris-Laplace and dense sampling every pixel (single-scale) with 4-SIFT from this book chapter <ref type="bibr" coords="2,359.19,374.01,9.96,8.74" target="#b7">[8]</ref>. However, instead of 4-SIFT, we only consider three ColorSIFT variants this year. One of these three is an optimized color descriptor which allows these three to perform as good as 4-SIFT. Please refer to the cited papers <ref type="foot" coords="2,323.46,408.30,3.97,6.12" target="#foot_0">1</ref> for implementation details of the system.</p><p>To achieve better results in the per-image evaluation, where we need to perform a binary assignment of a tag to an image, we use the probabilistic output of the SVM. In a cross-validation experiment, we have found a threshold of 0.3 to be good for most concepts: the default threshold of 0.5 would be too conservative when evaluating with an example-based F-measure where precision and recall are weighted equally. Optimizing the threshold on a per-concept basis instead of a single threshold was found to be less stable. Instead of a single parameter, 99 parameters need to be chosen (one per concept), and this estimation is done on the data of a single concept (instead of over 99 concepts).</p><p>New this year is our inclusion of textual information based on the image tags. As a textual representation of the image, we use a binary vector signaling whether a tag is present or absent among the provided Flickr tags. We select all words which occur at least 25 times. Tags consisting of multiple words, split by spaces are turned into multiple words. Also, words consisting of only digits are discarded. This gives us a lexicon of 1008 words. The binary feature vectors are L2-normalized. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Photo Annotation Runs</head><p>We have submitted five different runs. All runs use both Harris-Laplace and dense sampling with the SVM classifier.</p><p>â€¢ Core. Harris-Laplace and dense sampling every 6 pixels (multi-scale) with 3-SIFT.</p><p>â€¢ CoreA. Harris-Laplace and dense sampling every pixel (single-scale) with 3-SIFT.</p><p>â€¢ CoreFast. Harris-Laplace and dense sampling every 6 pixels (multi-scale) with 3-SIFT and fast intersection kernel <ref type="bibr" coords="3,331.35,396.76,9.96,8.74" target="#b1">[2]</ref>: instead of a Ï‡ 2 kernel, this run allows classification of test images whose computation time is independent of the number of support vectors.</p><p>â€¢ Multimodal-CoreA. Combination of the CoreA visual features with our text features; equally weighed at the SVM kernel level.</p><p>â€¢ Multimodal-CoreA-MKL. Combination of the CoreA visual features with our text features; weighed at the kernel level by multiple kernel learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation Per Concept</head><p>In table 1, the overall scores for the evaluation of concept detectors are shown.</p><p>The features with sampling at every pixel instead of every 6 pixels perform better (0.375 versus 0.368), which is similar to the result obtained in <ref type="bibr" coords="3,441.49,554.61,9.96,8.74" target="#b7">[8]</ref>. The use of a fast intersection kernel SVM <ref type="bibr" coords="3,306.91,566.57,10.52,8.74" target="#b1">[2]</ref> slightly reduces accuracy (0.368 to 0.364), but brings significant speed gains (useful for the concept-based retrieval task). The two final runs perform better than the others by including the textual modality, as was seen in ImageCLEF last year, for example in <ref type="bibr" coords="3,409.33,602.44,9.96,8.74" target="#b2">[3]</ref>. We confirm that including textual information based on the image tags improves results by 0.05 MAP. Indeed, numerous images are tagged directly with the name of a concept, or a synonym thereof (e.g. Graffiti or Sky). It should come as no surprise that this information is highly relevant for those concepts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Per Image</head><p>For the per-image evaluation, overall results are shown in table 2. Our emphasis on optimizing the threshold for tag assignment has resulted in the best overall run in terms of example-based F-measure and SR-precision over all submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Concept-Based Retrieval</head><p>The use of topics in the concept-based retrieval task, instead of concepts, poses a new problem to concept detection: what do we use as a starting point? Each topic has up to 5 example images, which could also be used to start visual retrieval. Since the topics are primarily combinations of several existing Im-ageCLEF concepts, we could use existing concept detectors. However, to do the latter fully automatic, we would need language parsing tools with support for boolean logic. An alternative is to add a 'manual' component to the system where a human maps topics to existing topics. But, a human can go a step further in their inspection of the topic. The concept-based retrieval task states that the training set of the annotation task (8,000 images annotated with 99 visual concepts) can be used to train the concept detectors. Therefore, we have extended the formulation of the topic by using relevance feedback on this training set. Overall, we have explored 3 approaches:</p><p>â€¢ Fully automatic retrieval. We use only the provided example images as positive examples to train a new concept detector. We combine these positive examples with either 10, 33 or 100 random negatives from the photo annotation train set. These are runs auto10, auto33 and auto100.</p><p>â€¢ Human topic mapping. A human reads the topic and then selects relevant concept(s). For run 1concept, the human can only select a single concept. For 2conceptsum and 2conceptmul, the human can select two concepts. The probability scores of these concepts are then combined using either summation or multiplication.</p><p>â€¢ Human topic inspection. A human can give quick feedback on whether images are relevant for a certain topic. Therefore, we have taken the concept models trained for the fully automatic retrieval, and applied them to the training set. A human was then given up to 7.5 minutes per topic to check the top ranked images for additional positive examples, and allowed to mark negative examples as well. Besides the output from the fully automatic system, the human was also allowed to look at the positive examples for one of the 99 existing concepts, and get additional positives from there. We also include a run with 100 negatives randomly added besides the negatives selected by a human.</p><p>The concept detectors used for concept-based retrieval are trained using the Core system from the photo annotation task, unless the word fast is in the name. In the latter case, the CoreFast system was used. It is of interest to note that we have only used visual information for the concept-based retrieval, where other participants have also included information from the tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>In Figure <ref type="figure" coords="5,178.13,349.58,3.87,8.74" target="#fig_0">1</ref>, we show results for our 3 concept-based retrieval approaches. For the fully automatic system, including more random negatives improves results. The fully automatic system achieves 0.043 MAP with 100 negative examples.</p><p>Additional negative examples might improve results further, but this also increases the chances that there are true positives among the random negatives. For the human concept mapping, selecting two concepts (where possible) results in a large improvement over selecting a single concept. This is expected, as the topics are designed to be boolean combinations of existing concepts Topics which directly map to a single concept have been left out on purpose. When combining two concepts, the multiplication fusion (0.089 MAP) works better than the summation fusion (0.080 MAP). For the human topic inspection, results are much better than the automatic system: the number of positives has increased to 42 on average, and 228 negatives have been selected. We find that including 100 random negatives still improves results; apparently the negatives selected by a human are not sufficient. To check whether selecting negatives is necessary at all, an interesting experiment would be to leave out the negatives selected by the human completely, and to only use random negatives. See also <ref type="bibr" coords="5,427.86,540.86,9.96,8.74" target="#b0">[1]</ref>.</p><p>The human concept mapping achieves the best results for 21 out of 40 topics. The human concept inspection achieves the best results for 17 out of 40 topics. Had we used the best approach per topic (oracle fusion), we would have increased MAP from 0.100 for our best run to 0.128 overall. Further analysis is needed to determine the relationship between how closely the topic maps to existing concepts, accuracy and the specificity of the topic. The submissions from our visual concept detection system in the ImageCLEF 2011 photo annotation task have resulted in the best run in the per-image evaluation. In the concept-based retrieval task, it was the best visual-only system. For the concept-based retrieval task, we considered three ways to perform visual retrieval: fully automatic, human topic mapping and human topic inspection. For a fully automatic system, including more random negatives to train a topic model improves results. For a human selecting relevant concepts to the topic, multiplication fusion works better than summation. For human topic inspection, a relevance feedback scheme on the train data gives an 8-fold increase in the number of positive examples per topic. Depending on the topic, the human topic mapping (best for 21 topics) and inspection (best for 17 topics) give the best results. An oracle fusion of the different methods would increase MAP from 0.100 for our best run to 0.128 overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully automatic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human topic mapping</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human topic inspection</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,491.32,644.63,8.74,28.12;6,491.32,633.16,8.74,7.75;6,491.32,596.14,8.74,31.80;6,491.32,580.48,8.74,11.93;6,491.32,562.93,8.74,13.84;6,491.32,498.27,8.74,60.95;6,491.32,458.80,8.74,35.75;6,491.32,434.26,8.74,20.81;6,491.32,403.29,8.74,25.35;6,491.32,383.77,8.74,15.80;6,491.32,328.68,8.74,51.37;6,491.32,316.11,8.74,8.86;6,491.32,307.40,8.74,4.98;6,491.32,279.33,8.74,24.35;6,491.32,261.57,8.74,13.84;6,491.32,214.68,8.74,43.17;6,491.32,187.57,8.74,23.39;6,491.32,162.19,8.74,21.67;6,491.32,144.33,8.74,14.13;6,491.32,124.80,8.74,15.80;6,503.27,658.30,8.74,14.45;6,503.27,649.73,8.74,4.98;6,503.27,619.30,8.74,26.85;6,503.27,562.55,8.74,53.16;6,503.27,546.25,8.74,11.08;6,503.27,528.83,8.74,13.84;6,503.27,490.65,8.74,34.60;6,503.27,473.16,8.74,13.84;6,503.27,437.17,8.74,32.41;6,503.27,400.62,8.74,32.96;6,503.27,389.01,8.74,8.03;6,503.27,310.93,8.74,74.50;6,503.27,267.70,8.74,39.63;6,503.27,249.98,8.74,14.13;6,503.27,224.81,8.74,21.58;6,503.27,194.93,8.74,26.28;6,503.27,184.65,8.74,6.70;6,503.27,157.75,8.74,23.30;6,503.27,124.80,8.74,29.36;6,515.23,631.76,8.74,40.99"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Results for the concept-based retrieval task. Every row corresponds to a topic; the maximum MAP score per row has a yellow background. At the bottom, the average number of positive/negative examples per topic model is listed (where relevant).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,133.77,134.88,343.71,108.22"><head>Table 1 :</head><label>1</label><figDesc>Overall results of the our runs evaluated over all concepts in the Photo Annotation task with Average Precision.</figDesc><table coords="3,188.03,162.24,235.53,80.87"><row><cell>Run name</cell><cell>Type</cell><cell>AP</cell></row><row><cell>Core</cell><cell>Visual</cell><cell>0.368</cell></row><row><cell>CoreA</cell><cell>Visual</cell><cell>0.375</cell></row><row><cell>CoreFast</cell><cell>Visual</cell><cell>0.364</cell></row><row><cell>Multimodal-CoreA</cell><cell cols="2">Visual+Tags 0.433</cell></row><row><cell cols="2">Multimodal-CoreA-MKL Visual+Tags</cell><cell>0.415</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,133.77,134.88,343.71,107.03"><head>Table 2 :</head><label>2</label><figDesc>Results using the per-image evaluation measures for our runs in the Photo Annotation Task. Measures are the average example-based F-measure and SR-precision.</figDesc><table coords="4,155.45,174.19,300.67,67.71"><row><cell>Run name</cell><cell>Type</cell><cell cols="2">F-measure SR-precision</cell></row><row><cell>Core</cell><cell>Visual</cell><cell>0.608</cell><cell>0.732</cell></row><row><cell>CoreA</cell><cell>Visual</cell><cell>0.612</cell><cell>0.734</cell></row><row><cell>CoreFast</cell><cell>Visual</cell><cell>0.605</cell><cell>0.730</cell></row><row><cell cols="2">Multimodal-CoreA Visual+Tags</cell><cell>0.622</cell><cell>0.742</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,149.01,630.11,214.00,7.21"><p>Papers available from http://www.colordescriptors.com</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgements</head><p>This work is supported by <rs type="funder">STW SEARCHER</rs>, <rs type="funder">FES COMMIT</rs>, and the <rs type="funder">IARPA</rs> via <rs type="funder">Department of Interior National Business Center</rs> contract number <rs type="grantNumber">D11PC20067</rs>. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of <rs type="institution">IARPA</rs>, <rs type="institution">DoI/NBC</rs>, or the <rs type="institution">U.S. Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5b2WsdR">
					<idno type="grant-number">D11PC20067</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,154.25,462.55,323.24,8.74;7,154.25,474.51,323.23,8.74;7,154.25,486.46,185.52,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,451.74,462.55,25.74,8.74;7,154.25,474.51,212.04,8.74">Social negative bootstrapping for visual categorization</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,392.58,474.51,84.90,8.74;7,154.25,486.46,155.36,8.74">ACM International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.25,506.39,323.24,8.74;7,154.25,518.34,323.24,8.74;7,154.25,530.30,166.18,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,310.33,506.39,167.15,8.74;7,154.25,518.34,157.59,8.74">Classification using intersection kernel support vector machines is efficient</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,337.50,518.34,139.98,8.74;7,154.25,530.30,135.79,8.74">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.25,550.22,323.23,8.74;7,154.25,562.18,323.23,8.74;7,154.25,574.13,239.66,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,457.94,550.22,19.54,8.74;7,154.25,562.18,296.74,8.74">Lear and xrces participation to visual concept detection task -imageclef</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>SÃ¡nchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,166.70,574.13,196.10,8.74">Working Notes for the CLEF 2010 Workshop</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.25,594.06,323.23,8.74;7,154.25,606.01,323.24,8.74;7,154.25,617.97,98.64,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,401.29,594.06,50.09,8.74">ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,200.74,606.01,276.74,8.74;7,154.25,617.97,24.74,8.74">Lecture Notes in Computer Science: The Information Retrieval Series</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.25,637.90,323.24,8.74;7,154.25,649.85,304.66,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,322.93,637.90,154.55,8.74;7,154.25,649.85,123.95,8.74">The clef 2011 photo annotation and concept-based retrieval tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liebetrau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,299.19,649.85,129.08,8.74">Working Notes of CLEF 2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,154.25,127.96,323.24,8.74;8,154.25,139.92,323.23,8.74;8,154.25,151.87,323.23,8.74;8,154.25,163.83,251.36,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,265.20,151.87,212.28,8.74;8,154.25,163.83,26.09,8.74">The mediamill trecvid 2010 semantic video search engine</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>De Rooij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Huurnink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Odijk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">C</forename><surname>Koelma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,201.56,163.83,172.94,8.74">Proceedings of the TRECVID Workshop</title>
		<meeting>the TRECVID Workshop</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,154.25,183.75,323.23,8.74;8,154.25,195.71,323.23,8.74;8,154.25,207.66,22.69,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,407.10,183.75,70.38,8.74;8,154.25,195.71,91.06,8.74">Real-time visual concept classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J H</forename><surname>Scha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,255.68,195.71,149.82,8.74">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="665" to="681" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,154.25,227.59,323.23,8.74;8,154.25,239.54,323.24,8.74;8,154.25,251.50,323.23,8.74;8,154.25,263.45,22.69,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,336.01,227.59,141.47,8.74;8,154.25,239.54,212.56,8.74">University of Amsterdam at the Visual Concept Detection and Annotation Tasks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,246.10,251.50,147.49,8.74">The Information Retrieval Series</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="343" to="358" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,154.25,283.38,323.23,8.74;8,154.25,295.33,323.23,8.74;8,154.25,307.29,323.23,8.74;8,154.25,319.24,323.23,8.74;8,154.25,331.20,323.23,8.74;8,154.25,343.15,103.01,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,426.34,283.38,51.14,8.74;8,154.25,295.33,257.28,8.74">The university of amsterdam&apos;s concept detection system at imageclef</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,457.09,295.33,20.39,8.74;8,154.25,307.29,323.23,8.74;8,154.25,319.24,323.23,8.74;8,154.25,331.20,52.83,8.74">Multilingual Information Access Evaluation Vol. II Multimedia Experiments: Proceedings of the 10th Workshop of the Cross-Language Evaluation Forum (CLEF</title>
		<title level="s" coord="8,355.14,331.20,122.34,8.74;8,154.25,343.15,30.03,8.74">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009. 2009. 2010</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="8,154.25,363.08,323.23,8.74;8,154.25,375.03,323.23,8.74;8,154.25,386.99,50.65,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,395.44,363.08,82.04,8.74;8,154.25,375.03,118.57,8.74">Accelerating visual categorization with the gpu</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,294.91,375.03,182.57,8.74;8,154.25,386.99,18.48,8.74">ECCV Workshop on Computer Vision on GPU</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,154.25,406.91,323.23,8.74;8,154.25,418.87,323.23,8.74;8,154.25,430.82,254.43,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,405.95,406.91,71.53,8.74;8,154.25,418.87,184.09,8.74">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,345.70,418.87,131.78,8.74;8,154.25,430.82,148.02,8.74">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,154.25,450.75,323.24,8.74;8,154.25,462.70,323.23,8.74;8,154.25,474.66,79.14,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,423.34,450.75,54.14,8.74;8,154.25,462.70,154.88,8.74">Empowering visual categorization with the GPU</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,322.44,462.70,150.36,8.74">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="70" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
