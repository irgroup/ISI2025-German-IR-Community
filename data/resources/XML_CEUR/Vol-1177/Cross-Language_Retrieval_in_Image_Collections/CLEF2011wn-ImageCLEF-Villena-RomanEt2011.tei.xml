<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.80,151.57,321.72,12.64;1,149.52,168.97,296.40,12.64">DAEDALUS at ImageCLEF 2011 Plant Identification Task: Using SIFT Keypoints for Object Detection</title>
				<funder ref="#_a4Rg9Fx #_S5nrC2F #_vUS7WgG #_Ze2EtXu">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.24,207.88,83.58,8.96"><forename type="first">Julio</forename><surname>Villena-Román</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Decisions and Language</orgName>
								<orgName type="institution">DAEDALUS -Data</orgName>
								<address>
									<country>S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,234.96,207.88,74.58,8.96"><forename type="first">Sara</forename><surname>Lana-Serrano</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Decisions and Language</orgName>
								<orgName type="institution">DAEDALUS -Data</orgName>
								<address>
									<country>S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.80,207.88,126.01,8.96"><forename type="first">José</forename><forename type="middle">Carlos</forename><surname>González-Cristóbal</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Decisions and Language</orgName>
								<orgName type="institution">DAEDALUS -Data</orgName>
								<address>
									<country>S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.80,151.57,321.72,12.64;1,149.52,168.97,296.40,12.64">DAEDALUS at ImageCLEF 2011 Plant Identification Task: Using SIFT Keypoints for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BAF8CCFF6AAC5B05DC3F05465046DECE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Plant identification task</term>
					<term>image retrieval</term>
					<term>Scale-Invariant Feature Transform</term>
					<term>SIFT</term>
					<term>keypoints</term>
					<term>classifier</term>
					<term>training</term>
					<term>test</term>
					<term>Pl@ntLeaves</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of DAEDALUS at ImageCLEF 2011 Plant Identification task. The task is evaluated as a supervised classification problem over 71 tree species from the French Mediterranean area used as class labels, based on visual content from scan, scan-like and natural photo images. Our approach to this task is to build a classifier based on the detection of keypoints from the images extracted using Lowe's Scale Invariant Feature Transform (SIFT) algorithm. Although our overall classification score is very low as compared to other participant groups, the main conclusion that can be drawn is that SIFT keypoints seem to work significantly better for photos than for the other image types, so our approach may be a feasible strategy for the classification of this kind of visual content.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the participation of DAEDALUS research team at the Plant Identification task <ref type="bibr" coords="1,199.92,538.00,10.69,8.96" target="#b0">[1]</ref>, a new pilot task within ImageCLEF 2011 whose objective is to research on the application of image retrieval technologies for identifying plant species. Specifically, this first year the focus is on tree species identification based on leaf images. Leaves are easily observable and the most studied organ in the computer vision community, although they are known to not be the only discriminant key between tree species. The task is evaluated as a supervised classification problem over 71 tree species from the French Mediterranean area used as class labels, based on visual content from Pl@ntLeaves dataset, published under a creative commons license within the Pl@ntNet project <ref type="bibr" coords="1,201.84,643.84,10.69,8.96" target="#b1">[2]</ref>, containing 3070 leaf scans, 897 leaf pictures with a white uniform background (referred as scan-like pictures) and 2469 leaf pictures in natural conditions (taken on the tree) provided by Telabotanica <ref type="bibr" coords="1,355.20,666.88,10.60,8.96" target="#b2">[3]</ref>, a French social network of amateur and expert botanists.</p><p>In addition to the image file itself, the dataset contains a series of meta-data attributes apart from the full taxon name (species, genus, family…) and French or English vernacular names (common names), including the acquisition type (scan, pseudoscan or photograph), content type (single leaf, single dead leaf or several leaves on tree visible in the picture), date, locality and GPS coordinates, and information about the author, all encoded in XML files. An example is shown in Figure <ref type="figure" coords="2,153.36,218.56,3.76,8.96" target="#fig_0">1</ref>. A part of Pl@ntLeaves dataset is provided as training data whereas the remaining part is used later as test data. The training data finally results in 4004 images and the test data results in 1432 images. The goal of the task is to associate the correct tree species to each test image. Each participant was allowed to submit up to 3 runs built from different methods. As many species as possible could be associated to each test image, sorted by decreasing confidence score.</p><p>In the following sections we will describe our approach, the experiments that we submitted, the results that we achieved on this task, and some preliminary conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>We approach this task with the construction of a classifier based on keypoints that represent objects within the images, extracted using Lowe's Scale-Invariant Feature Transform (SIFT) algorithm <ref type="bibr" coords="2,240.48,598.12,25.00,8.96">[4] [5]</ref>.</p><p>The fundamentals of SIFT algorithm are to extract interesting points for a given training image that model the objects depicted in it, so that those objects can be identified in a given test image containing many other objects. To perform reliable recognition, those features extracted from the training image must be detectable under changes in image scale, noise and illumination. In addition, the relative positions between these features in the original scene should not change from one image to another. Such interesting points usually lie on high-contrast regions of the image, such as object edges.</p><p>Our classifier is trained by first extracting SIFT keypoints from all images in the training set. Each set of keypoints is stored in a database, associated to the tree species that corresponds to such training image.</p><p>The number of extracted keypoints can be controlled by scaling the image resolution. Image resolution must not be very high as it is the larger scale keypoints that are most reliable and this is also much more efficient than processing large images. According to Lowe, an image of size 500 pixels square will typically give over 1000 keypoints depending on image content, which is plenty for most applications. For this purpose, each training image is rescaled to a width of 200 pixels. Moreover, as required by the Lowe's implementation that is used to obtain the SIFT keypoints <ref type="bibr" coords="3,193.92,287.56,10.69,8.96" target="#b5">[6]</ref>, images are converted to greyscale PGM format prior to the extraction.</p><p>Once all the training set is processed, an object is recognized in a test image by individually comparing each feature from the test image to this database and finding candidate matching features based on Euclidean distance of their feature vectors. Test images are also downscaled, in this case to a width of 400 pixels to be able to find more keypoints, and then also converted to greyscale PGM format.</p><p>From the full set of matches, subsets of keypoints that agree on the object and its location, scale and orientation in the new image are identified to filter out good matches. The same criteria as proposed by Lowe is used <ref type="bibr" coords="3,364.44,391.12,10.69,8.96" target="#b5">[6]</ref>, in which matches are identified by finding the 2 nearest neighbours of each keypoint from the training image among those in the test image, and only accepting a match if the distance to the closest neighbour is less than 0.6 of that to the second closest neighbour. This threshold can be adjusted up to select more matches or down to select only the most reliable.</p><p>Then the probability that a particular set of features indicates the presence of an object is computed, given the accuracy of fit and number of probable false matches. Object matches that pass all these tests are supposed to be identified as correct with high confidence.</p><p>The output of the SIFT classifier provides a list of training images sorted by relevance. To get the matching among training images and classification labels, the relevance of the top-ranked training image for each classification label is selected as the relevance for such label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Although we initially planned different experiments changing the image downscaling and the object acceptance thresholds, we finally submitted just one run to be evaluated due to lack of time when carrying out the experiments.</p><p>For the same reason, we had to discard our initial idea to build three different specific classifiers based on acquisition type.</p><p>Apart from the image itself and the taxon name in the training set, no use of any other metadata information was made.</p><p>The primary metric used by the organizers to evaluate the submitted runs is a classification rate on the 1st species returned for each test image. Each test image is attributed with a score of 1 if the 1st returned species is correct and 0 if it is wrong. An average score is then computed on all test images. As a simple mean will introduce some bias due to the different number of images of the same individual plant and the number of pictures provided by each contributor to the Pl@ntLeaves dataset, the final metric is defined as an average classification score S: <ref type="bibr" coords="4,368.76,248.56,11.71,8.96" target="#b0">(1)</ref> where U is the number of users (who have at least one image in the test data), Pu is the number of individual plants observed by the u-th user, Nu,p is the number of pictures taken from the p-th plant observed by the u-th user and Su,p,n is classification score (1 or 0) for the n-th picture taken from the p-th plant observed by the u-th user. An average classification score S is computed separately for each type (scan, scan-like or photo) to isolate and evaluate its impact.</p><p>The results achieved in our experiment are shown in Table <ref type="table" coords="4,373.44,354.16,3.70,8.96">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. Results (by classification score).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Scans Scanlike Photos Mean daedalus_run1 0.043 0.025 0.055 0.041</p><p>In general, those figures are very low and results are a bit disappointing. However, an interesting point shown in the table is that the top values are achieved for natural photos. As a preliminary interpretation, we think that this may be because of the fact that SIFT keypoints strongly rely on contrast changes in images (such as colour gradients or edges), and natural pictures represent more realistic conditions.</p><p>Furthermore, another possible explanation may be the fact that the training and test dataset are not evenly balanced among the three acquisition types and not even between them, as shown in Table <ref type="table" coords="4,262.20,533.92,3.76,8.96" target="#tab_0">2</ref>. Our conclusion is that we should have built three different classifiers, one for each type of image. A detailed analysis considering more than the 1st result is presented in Table <ref type="table" coords="4,463.20,659.68,3.76,8.96" target="#tab_1">3</ref>. This table shows, for each classification label (tree species), the number of test images where the label was returned (independently of its position in the result list) and the average position of that label in the result list.     Our classifier was able to find the valid label for 649 test images (45.1% of the age was identified for the Fraxinus ornus and 8 groups.</p><p>because of the low performance for like images. However our results for natural photos , as shown in Figure <ref type="figure" coords="6,462.96,448.84,3.76,8.96" target="#fig_4">3</ref>. This reinforces the idea that SIFT keypoints may be a valuable strategy for natural</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>Conclusions and Future Work</p><p>Despite the poor overall classification score, the main preliminary conclusion that can be drawn is that SIFT keypoints seem to work better for natural photos rather than scan and scan-like images, and our experiment has been able to outperform the best experiment by other groups in this type.</p><p>For future participations, we will definitely build specific classifiers for each image type. Moreover, we will try other alternatives to SIFT that are less demanding to compute and may handle colour images, such as SURF keypoints.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,216.96,398.85,173.01,8.10;2,126.36,239.40,345.72,156.00"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Example of one picture in the dataset.</figDesc><graphic coords="2,126.36,239.40,345.72,156.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,213.36,401.49,25.47,8.10"><head>Finally, Figure 2 shows tFigure</head><label></label><figDesc>Figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,196.80,676.29,25.47,8.10;6,162.36,494.16,283.44,172.80"><head>Figure</head><label></label><figDesc>Figure</figDesc><graphic coords="6,162.36,494.16,283.44,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,213.36,401.49,179.97,8.10"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overall results (by classification score).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,196.80,676.29,213.33,8.10;6,162.36,494.16,283.44,172.80"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Results for photographs (by classification score).</figDesc><graphic coords="6,162.36,494.16,283.44,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,179.04,569.73,237.10,67.35"><head>Table 2 .</head><label>2</label><figDesc>Distribution of image types in training and test datasets</figDesc><table coords="4,179.04,589.96,237.10,47.12"><row><cell>Type</cell><cell>Training</cell><cell>Test</cell><cell>Difference</cell></row><row><cell>Scans</cell><cell>58.1%</cell><cell>51.7%</cell><cell>-6.4%</cell></row><row><cell>Scan-like</cell><cell>17.1%</cell><cell>14.7%</cell><cell>-2.4%</cell></row><row><cell>Photos</cell><cell>24.8%</cell><cell>33.6%</cell><cell>+8.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,180.84,185.97,233.58,489.03"><head>Table 3 .</head><label>3</label><figDesc>Detailed analysis by classification label.</figDesc><table coords="5,180.84,204.64,233.58,470.36"><row><cell>Tree species</cell><cell>Average Position</cell><cell>Identified Images</cell></row><row><cell>Acer campestre</cell><cell>5.86</cell><cell>28</cell></row><row><cell>Acer monspessulanum</cell><cell>2.59</cell><cell>27</cell></row><row><cell>Acer negundo</cell><cell>9.26</cell><cell>19</cell></row><row><cell>Acer platanoides</cell><cell>12.70</cell><cell>10</cell></row><row><cell>Aesculus hippocastanum</cell><cell>17.00</cell><cell>1</cell></row><row><cell>Arbutus unedo</cell><cell>12.88</cell><cell>16</cell></row><row><cell>Betula pendula</cell><cell>13.33</cell><cell>3</cell></row><row><cell>Broussonetia papyrifera</cell><cell>8.80</cell><cell>45</cell></row><row><cell>Castanea sativa</cell><cell>5.00</cell><cell>1</cell></row><row><cell>Celtis australis</cell><cell>17.14</cell><cell>7</cell></row><row><cell>Cercis siliquastrum</cell><cell>8.95</cell><cell>38</cell></row><row><cell>Corylus avellana</cell><cell>20.00</cell><cell>7</cell></row><row><cell>Cotinus coggygria</cell><cell>4.29</cell><cell>28</cell></row><row><cell>Crataegus monogyna</cell><cell>12.46</cell><cell>41</cell></row><row><cell>Diospyros kaki</cell><cell>12.00</cell><cell>2</cell></row><row><cell>Eriobotrya japonica</cell><cell>6.80</cell><cell>10</cell></row><row><cell>Ficus carica</cell><cell>14.71</cell><cell>7</cell></row><row><cell>Fraxinus angustifolia</cell><cell>19.00</cell><cell>1</cell></row><row><cell>Ginkgo biloba</cell><cell>7.24</cell><cell>50</cell></row><row><cell>Ilex aquifolium</cell><cell>16.15</cell><cell>13</cell></row><row><cell>Juglans nigra</cell><cell>16.00</cell><cell>1</cell></row><row><cell>Juglans regia</cell><cell>2.00</cell><cell>1</cell></row><row><cell>Laurus nobilis</cell><cell>7.63</cell><cell>19</cell></row><row><cell>Nerium oleander</cell><cell>2.40</cell><cell>10</cell></row><row><cell>Olea europaea</cell><cell>2.59</cell><cell>32</cell></row><row><cell>Paliurus spina-christi</cell><cell>6.86</cell><cell>7</cell></row><row><cell>Pistacia lentiscus</cell><cell>13.40</cell><cell>15</cell></row><row><cell>Pistacia terebinthus</cell><cell>22.00</cell><cell>1</cell></row><row><cell>Pittosporum tobira</cell><cell>3.20</cell><cell>25</cell></row><row><cell>Platanus x</cell><cell>5.40</cell><cell>5</cell></row><row><cell>Punica granatum</cell><cell>6.75</cell><cell>4</cell></row><row><cell>Quercus coccifera</cell><cell>4.00</cell><cell>1</cell></row><row><cell>Quercus ilex</cell><cell>12.18</cell><cell>45</cell></row><row><cell>Quercus pubescens</cell><cell>18.20</cell><cell>5</cell></row><row><cell>Rhamnus alaternus</cell><cell>7.24</cell><cell>50</cell></row><row><cell>Robinia pseudoacacia</cell><cell>10.50</cell><cell>2</cell></row><row><cell>Syringa vulgaris</cell><cell>2.85</cell><cell>20</cell></row><row><cell>Viburnum tinus</cell><cell>16.23</cell><cell>47</cell></row><row><cell>Vitex agnus-castus</cell><cell>9.60</cell><cell>5</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been partially supported by several Spanish research projects: <rs type="projectName">MA2VICMR</rs>: Improving the access, analysis and visibility of the multilingual and multimedia information in web for the <rs type="projectName">Region of Madrid</rs> (<rs type="grantNumber">S2009/TIC-1542</rs>), <rs type="projectName">MULTIMEDICA: Multilingual Information Extraction in Health domain</rs> and application to scientific and informative documents (<rs type="grantNumber">TIN2010-20644-C03-01</rs>) and <rs type="projectName">BUSCAMEDIA: Towards a semantic adaptation of multi-network-multiterminal digital media</rs> (<rs type="grantNumber">CEN-20091026</rs>). Authors would like to thank all partners for their knowledge and support.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_a4Rg9Fx">
					<orgName type="project" subtype="full">MA2VICMR</orgName>
				</org>
				<org type="funded-project" xml:id="_S5nrC2F">
					<idno type="grant-number">S2009/TIC-1542</idno>
					<orgName type="project" subtype="full">Region of Madrid</orgName>
				</org>
				<org type="funded-project" xml:id="_vUS7WgG">
					<idno type="grant-number">TIN2010-20644-C03-01</idno>
					<orgName type="project" subtype="full">MULTIMEDICA: Multilingual Information Extraction in Health domain</orgName>
				</org>
				<org type="funded-project" xml:id="_Ze2EtXu">
					<idno type="grant-number">CEN-20091026</idno>
					<orgName type="project" subtype="full">BUSCAMEDIA: Towards a semantic adaptation of multi-network-multiterminal digital media</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,128.56,456.16,342.04,8.96;7,136.20,468.16,334.53,8.96;7,136.20,480.16,334.53,8.96;7,136.20,492.16,145.53,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,165.48,480.16,189.09,8.96">The CLEF 2011 plant image classification task</title>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><forename type="middle">;</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><forename type="middle">;</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><forename type="middle">;</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Nozha</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-François;</forename><surname>Daniel; Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philippe</forename><forename type="middle">;</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elise</forename><forename type="middle">;</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marie</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,361.20,480.16,105.61,8.96">CLEF 2011 working notes</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.56,504.16,303.88,8.96" xml:id="b1">
	<monogr>
		<ptr target="http://www.plantnet-project.org/" />
		<title level="m" coord="7,136.20,504.16,69.29,8.96">Pl@ntNet Project</title>
		<imprint>
			<date type="published" when="2011-08">August 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.56,516.16,341.93,8.96;7,136.20,528.16,57.33,8.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,198.36,516.16,114.45,8.96">The French Botany Network</title>
		<author>
			<persName coords=""><forename type="first">Tela</forename><surname>Botanica</surname></persName>
		</author>
		<ptr target="http://www.tela-botanica.org/" />
		<imprint>
			<date type="published" when="2011-08">August 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.56,540.16,342.04,8.96;7,136.20,552.16,334.63,8.96;7,136.20,564.16,47.61,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,222.12,540.16,244.70,8.96">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,136.20,552.16,262.96,8.96">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.56,576.16,342.04,8.96;7,136.20,588.16,268.17,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,216.36,576.16,250.11,8.96">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,136.20,588.16,165.76,8.96">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.56,600.16,342.08,8.96;7,136.20,612.16,88.17,8.96" xml:id="b5">
	<monogr>
		<ptr target="http://www.cs.ubc.ca/~lowe/keypoints/" />
		<title level="m" coord="7,136.20,600.16,169.63,8.96">Demo Software: SIFT Keypoint Detector</title>
		<imprint>
			<date type="published" when="2011-08">August 2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
