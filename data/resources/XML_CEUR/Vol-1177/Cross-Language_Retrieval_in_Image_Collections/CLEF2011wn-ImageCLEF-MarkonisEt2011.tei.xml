<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.68,115.82,329.90,12.93">The medGIFT Group in ImageCLEFmed 2011</title>
				<funder ref="#_MtRd6Qv">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_2ud38D9 #_TePfD3X #_wbBHqDJ">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,145.80,153.31,82.56,9.96"><forename type="first">Dimitrios</forename><surname>Markonis</surname></persName>
							<email>dimitrios.markonis@hevs.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,236.28,153.31,44.48,9.96"><forename type="first">Ivan</forename><surname>Eggel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.48,153.31,105.42,9.96"><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.52,153.31,68.10,9.96"><forename type="first">Henning</forename><surname>MÃ¼ller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.68,115.82,329.90,12.93">The medGIFT Group in ImageCLEFmed 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1A273DF09E6B6A29DC6A0A602A0879C1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article presents the participation of the medGIFT group in ImageCLEFmed 2011. Since 2004, the group has participated in the medical image retrieval tasks of ImageCLEF each year. The main goal is to provide a baseline by using the same technology each year, and to search for further improvements in retrieval quality. There are three types of tasks for ImageCLEFmed 2011: modality classification, image-based retrieval and case-based retrieval. The medGIFT group participated in all three tasks. For the image-based and case-based retrieval tasks, two existing retrieval engines were used: the GNU Image Finding Tool (GIFT) for visual retrieval and Apache Lucene for text. For the modality classification, a purely visual approach was used with GIFT for the visual retrieval and a kNN (k-Nearest Neighbors) classifier for the classification. Results show that the best text runs outperform the best visual runs by a factor of 10 in terms of mean average precision. Baselines provided by Apache Lucene and GIFT are ranked above the average among text runs and visual runs respectively in image-based retrieval. In the casebased retrieval task the Lucene baseline is the second best automatic run for text retrieval, and our mixed and visual runs are the best overall. For modality classification, GIFT and the kNN-based approach perform slightly better than the average of the visual approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF is the cross-language image retrieval track<ref type="foot" coords="1,375.72,518.00,3.97,4.84" target="#foot_0">1</ref> of the Cross Language Evaluation Forum (CLEF). ImageCLEFmed is part of ImageCLEF focusing on medical images <ref type="bibr" coords="1,203.40,542.71,10.56,9.96" target="#b3">[4,</ref><ref type="bibr" coords="1,215.64,542.71,7.04,9.96" target="#b4">5]</ref>. The medGIFT<ref type="foot" coords="1,292.68,541.88,3.97,4.84" target="#foot_1">2</ref> research group has participated in Image-CLEFmed using the same technology as baselines since 2004. Additional modifications of the basic techniques were attempted to improve results. Visual and textual baseline runs have been made available to other participants of Image-CLEFmed. The visual baseline is based on GIFT <ref type="foot" coords="1,352.32,589.76,3.97,4.84" target="#foot_2">3</ref> (GNU Image Finding Tool, <ref type="bibr" coords="1,134.76,602.47,10.82,9.96" target="#b5">[6]</ref>) whereas Lucene 4 was used for text retrieval. This year, the bag-of-visual-words approach <ref type="bibr" coords="2,353.88,118.39,10.56,9.96" target="#b0">[1]</ref> was also used using local descriptors also called visual words. This widely used method is applied as follows: a training set of images is chosen and a number of local descriptors (in the case of SIFT, Scale Invariant Feature Transform, 128-dimensional vectors) are extracted from each image of this set. The descriptors are then clustered using a clustering method (such as k-means) and the centroids of the clusters are used as visual words. Based on this the visual vocabulary -the set of all the visual words -is created. Local features are then also extracted from each image in a database. The images are finally indexed as histograms of the visual word occurrences (bags-of-visual-words) by assigning the nearest visual word to each feature vector. When an image is queried, a similarity measure is used to compare the query image histogram and the database images histograms, providing a similarity score. In order to include spatial information to this representation, several approaches have been proposed <ref type="bibr" coords="2,307.68,273.79,10.56,9.96" target="#b1">[2,</ref><ref type="bibr" coords="2,319.92,273.79,7.04,9.96" target="#b2">3]</ref>, improving the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>This section describes the basic techniques that we used for retrieval in Image-CLEFmed 2011.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Retrieval Tools Reused</head><p>This section details the existing retrieval tools that were reused for text and visual retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Retrieval</head><p>The text retrieval approach in 2011 is based on Lucene using standard settings. 4 text runs were submitted, 2 for case-based retrieval and 2 for image-based retrieval. For case-and image-based retrieval, captions and full text were used.</p><p>The full text approach used all texts as obtained in the data set. Links, metadata, scripts and style information were removed and only the remaining text was indexed. For image captions, an XML file containing captions of all the images was indexed. No specific terminologies such as MeSH (Medical Subject Headings) were used.</p><p>Visual Retrieval GIFT is a visual retrieval engine based on color and texture information <ref type="bibr" coords="2,189.60,571.87,9.99,9.96" target="#b5">[6]</ref>. Colors are compared in a color histogram using a simple histogram intersection. Texture information is described by applying Gabor filters and quantizing the responses into 5 strengths. This different from the previous years' use of 10 strengths because of the size of this year's data set that can cause problems for GIFT. The image is rescaled to 256x256 and partitioned into fixed regions to extract features both global and local features. GIFT uses a standard tf/idf (term frequency/inverse document frequency) strategy for feature weighting. It also allows image-based queries with multiple input images. GIFT has been used for the ImageCLEFmed tasks since 2004. Each year the default setting has been used to provide a baseline. For classification, GIFT has been used to produce the distance (similarity) value followed by a nearest neighbor (1NN) classification.</p><p>For the description of the images when using visual words, we used the SIFT implementation in the fiji<ref type="foot" coords="3,242.88,178.28,3.97,4.84" target="#foot_4">5</ref> image processing package. In order to create the visual vocabulary, our implementation of the density-based clustering algorithm DEN-CLUE [7] was used. The reason for this choice are the features and the nature of the dataset that needs to be clustered. The data set to be clustered is largescale (1000 training images produce approximately 2'500'000 descriptors) and high dimensional (SIFT descriptors are 128-dimensional). The DENCLUE algorithm is highly efficient for clustering large-scale datasets, can detect arbitrarily shaped clusters and handles outliners and noise well. Moreover, opposed to other density-based clustering algorithms it performs well for high-dimensional data. However, when using a density-based clustering algorithm care has to be taken for data sets containing clusters of different densities. To deal with this, the parameter Î¾ that controls the significance of the candidate cluster in respect to its density was set to zero.</p><p>In order to create a pipeline for easy component-based evaluation for this method the outputs of every intermediate step were stored in CSV files and mySQL tables. These use a large amount of storage resources but speed up the procedure of tuning and evaluating components of the method once the ground truth is available. Due to the characteristics of this architecture it was possible to use only vocabularies with a small number of visual words ( 100) and a n Ã n partition was used with maximum n = 2.</p><p>The third submitted approach combines the modality classification and the image retrieval tasks. Using the GIFT assignment of modalities the histograms of visual words where indexed in mySQL tables based on their classes. In this indirect manner, the approaches of GIFT and bag-of-words were combined as well. The visual word histogram of the query image was first compared to the indexed histograms of the training set using a histogram intersection. The classes of the 5 nearest neighbors were acquired. Then, the same histogram was compared again but only with the indexed histograms in the tables of these classes. The 1000 nearest images were acquired as the results for the topic images. For topics that contained more than one query image the combSUM technique was used as is explained in the next section.</p><p>Fusion Techniques In 2009, the ImageCLEF@ICPR fusion task was organized to compare fusion techniques using the best ImageCLEFmed visual and textual results <ref type="bibr" coords="3,166.56,597.67,9.99,9.96" target="#b7">[8]</ref>. Studies such as <ref type="bibr" coords="3,256.56,597.67,10.56,9.96" target="#b8">[9]</ref> show that combSUM (1) and combMNZ(2) proposed by <ref type="bibr" coords="3,175.20,609.55,15.60,9.96" target="#b9">[10]</ref> in 1994 are robust fusion strategies. With the data from the Image-CLEF@ICPR fusion task, combMNZ performed slightly better than combSUM, the difference was small and not statistically significant. In general, rank-based fusion worked better than score-based fusion.</p><formula xml:id="formula_0" coords="4,256.44,136.34,224.18,29.43">S combSUM (i) = N k k=1 S k (i)<label>(1)</label></formula><formula xml:id="formula_1" coords="4,236.16,176.11,244.47,11.56">S combMNZ (i) = F (i) * S combSUM (i)<label>(2)</label></formula><p>where F (i) is the frequency of image i being returned by one input system with a non-zero score, and S(i) is the score assigned to image i. In ImageCLEFmed2011, the fusion approach using scored-based combSUM was used in three cases:</p><p>fusing textual and visual runs to produce mixed runs; -fusing results from various images which belong to the same topic for the bag-of-visual-word approaches, (GIFT handles queries with several images automatically); -fusing GIFT and bag-of-visual-word approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Collection</head><p>230'089 medical images were available for ImageCLEFmed 2011. Among them 1'000 images with modality labels were used as training data and another 1'000 images were selected as test data for the modality classification. Details about the setup and collections of the ImageCLEFmed tasks can be found in the overview paper <ref type="bibr" coords="4,162.72,389.95,14.69,9.96" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>This section describes our results for the three medical tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modality Classification</head><p>One run was submitted to the modality classification task using GIFT. For runs of various natures (textual, visual, mixed) the best accuracy and average accuracy are shown in Table <ref type="table" coords="4,237.60,514.63,3.90,9.96" target="#tab_0">1</ref>. It can be observed that GIFT, using 1NN classification performed worse than the average accuracy. This was expected, as neither k for the kNN was optimal, nor the optimal GIFT feature configuration was used, due to the dataset size. Results also show that visual runs achieve performance close to the mixed runs showing the importance of visual characteristics in modality classification. The analysis of text results are not absolutely reliable though, as only two exclusively textual runs were submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image-based Retrieval</head><p>In total 8 runs were submitted to the image-based retrieval task by the medGIFT group. Using the GIFT baseline and the 2 textual baselines, 2 mixed runs were produced using the combSUM approach. One run was the fusion of GIFT and the 2-step approach described in Section 2.1. Results are shown in Table <ref type="table" coords="5,472.80,255.31,3.90,9.96" target="#tab_1">2</ref>.</p><p>Mean average precision (MAP), binary preference (Bpref), and early precision (P10, P30) are shown as measures. For the full text retrieval, the score of a text was extended to all images of this text, for the caption-based retrieval it was extended to all images of this caption. In terms of mean average precision (MAP), the best textual run (0.2172) outperforms the best visual run (0.0338) by a factor of 7, which shows a big performance gap between the two approaches. However, it is significantly smaller than the gap in ImageCLEF 2010. The average score of all textual runs is 0.1644, whereas the average score of all visual retrieval runs is 0.0146. The performance of the baseline produced by Apache Lucene based on image caption information (HES-SO-VS CAPTIONS) is slightly above the average. On the other hand, GIFT performed surprisingly well, considering the non-optimal configuration and age of the tool. The bagof-visual-words approaches did not demonstrate good results, most likely due to the lack of parameter tuning and lack of using training data. For the 2-step approach the initial results were not as good as the initial results, but with parameter tuning, already better results could be obtained. However, using the component-based architecture that was developed, further research will be easier to perform.</p><p>Merging of textual runs with visual runs reduces the performance of the textual runs, which is again due to the non-optimal technique using scores and not ranks. The two mixed runs submitted by the medGIFT group are based on a simple merging approach and are punished by the large performance gap between textual and visual runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case-Based Retrieval</head><p>The medGIFT group submitted four visual runs, one textual run and one mixed run for the case-based retrieval task. The visual runs were obtained by processing a case-based fusion of the results of querying all images of a case using the combSUM strategy. Text runs were performed using the full text and for the caption-based retrieval the results of all captions of a text were combined using combSUM. Based on visual and textual runs, mixed runs were produced by using the combSUM strategy. Table <ref type="table" coords="6,429.24,315.19,4.98,9.96" target="#tab_2">3</ref> shows the MedGIFT runs and if our run was not the best also the performance of the best run. Best performance in terms of MAP (0.1297) was obtained by purely textual retrieval. The Lucene baseline (fulltext) is the second best run (0.1293) among all automatic runs and the difference to the best runs is statistically not significant. MedGIFT was the only lab that submitted purely visual runs and even though the best result (0.0204 by GIFT) is lower than the best textual run, the difference is not as bad as for the image-based task. The mixed run of GIFT and the Lucene fulltext achieved the best results (0.0754) in the mixed runs. This run also has the best P5 so very early precision of all all runs but on the other hand its P10 is already lower than the best textual run for P10, which is also a run of the medGIFT group. This combination decreased the performance of the corresponding textual run for MAP, so there is still a potential in improving the current systems by not using a direct fusion but rather a reordering of the results.</p><p>Based on the results of the medGIFT participation several lessons can be learned, often similar or at least in line with previous years. The baseline run of Lucene using captions performed better in the image-based task while the fulltextbased approach showed good results in case-based retrieval task. In general, the baseline of GIFT performs well in image-based and case-based retrieval although on a lower level than the text retrieval approaches. The same cannot be said for the modality classification but this was probably due to the poor classification rule that was extremely simple without any use of training data. For visual classification several very good and optimized systems exist that reach a much better performance. As the datasets grow larger, aspects of system scalability such as the trade-off of memory usage, speed and quality have to be taken into account for future content-based image retrieval systems.</p><p>Concerning the bag-of-visual-words runs, further testing and work is required to fully exploit the advantages of the methods used. While larger vocabularies and finer partitions may improve the result, a better classifier can enhance the 2-step approach, which already delivered better results than the approach presented in this text.</p><p>Finally, we can see that the majority of the mixed runs decreased the performance compared to the textual runs when combined. This indicates that special care needs to be taken for the fusion of unbalanced runs in terms of performance, as the textual and visual runs are obtaining very different performance. In the based, rank-based measure have shown to be better than score-based approaches and this was mistakenly not taken into account.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,177.00,545.50,261.40,74.00"><head>Table 1 .</head><label>1</label><figDesc>Results of the runs for the modality classification task.</figDesc><table coords="4,203.52,565.90,208.14,53.60"><row><cell>run</cell><cell cols="2">best accuracy average accuracy</cell></row><row><cell>mixed runs</cell><cell>0.8691</cell><cell>0.7188</cell></row><row><cell>textual runs</cell><cell>0.7041</cell><cell>0.5903</cell></row><row><cell>visual runs</cell><cell>0.8359</cell><cell>0.6878</cell></row><row><cell>GIFT 1NN</cell><cell>0.6220</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.76,335.38,345.75,151.04"><head>Table 2 .</head><label>2</label><figDesc>Results of the medGIFT runs and the best runs for the image-based topics.</figDesc><table coords="5,139.80,355.66,335.74,130.76"><row><cell>run</cell><cell cols="2">run type MAP P10 P30 Rprec Bpref num rel ret</cell></row><row><cell>best mixed run</cell><cell>Manual 0.2372 0.3933 0.3550 0.2881 0.2738</cell><cell>1597</cell></row><row><cell>mixed captions ib</cell><cell>Automatic 0.1176 0.2800 0.2100 0.1575 0.1614</cell><cell>705</cell></row><row><cell>mixed full ib</cell><cell>Automatic 0.0857 0.2900 0.2700 0.1300 0.1308</cell><cell>830</cell></row><row><cell>best visual run</cell><cell>Automatic 0.0338 0.1500 0.1317 0.0625 0.0717</cell><cell>717</cell></row><row><cell>gift visual ib</cell><cell>Automatic 0.0274 0.1467 0.1367 0.0581 0.0807</cell><cell>731</cell></row><row><cell>visual ib</cell><cell>Automatic 0.0252 0.1267 0.1200 0.0554 0.0752</cell><cell>709</cell></row><row><cell>bovw visual ib</cell><cell>Automatic 0.0126 0.0867 0.0800 0.0315 0.0437</cell><cell>324</cell></row><row><cell>bovw s2 visual ib</cell><cell>Automatic 0.0076 0.0900 0.0650 0.0182 0.0279</cell><cell>213</cell></row><row><cell>best textual run</cell><cell>Automatic 0.2172 0.3467 0.3017 0.2369 0.2402</cell><cell>1471</cell></row><row><cell>image-based captions</cell><cell>Automatic 0.1742 0.3000 0.2683 0.2096 0.2179</cell><cell>1261</cell></row><row><cell>image-based fulltext</cell><cell>Automatic 0.0921 0.2167 0.2150 0.1264 0.1506</cell><cell>1211</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,137.76,376.53,339.76,85.28"><head>Table 3 .</head><label>3</label><figDesc>Results of the medGIFT runs and the best runs for the case-based topics.</figDesc><table coords="6,139.80,396.93,335.74,64.88"><row><cell>run</cell><cell cols="2">run type MAP P10 P20 Rprec Bpref num rel ret</cell></row><row><cell cols="2">mixed GIFTLucene full Automatic 0.0754 0.1667 0.1556 0.1227 0.0958</cell><cell>121</cell></row><row><cell>best textual run</cell><cell>Automatic 0.1297 0.1889 0.1500 0.1588 0.1212</cell><cell>144</cell></row><row><cell>case based fulltext</cell><cell>Automatic 0.1293 0.2000 0.1444 0.1509 0.1122</cell><cell>141</cell></row><row><cell>case based captions</cell><cell>Automatic 0.0437 0.1111 0.0833 0.0816 0.0540</cell><cell>90</cell></row><row><cell>gift visual</cell><cell>Automatic 0.0204 0.0444 0.0333 0.0336 0.0292</cell><cell>45</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.72,624.52,117.03,7.62"><p>http://www.imageclef.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,144.72,635.56,126.39,7.62"><p>http://www.hevs.ch/medgift/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,144.72,646.48,154.59,7.62"><p>http://www.gnu.org/software/gift/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="1,144.72,657.40,117.03,7.62"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,144.72,657.40,159.39,7.62"><p>http://fiji.sc/wiki/index.php/Fiji</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgments</head><p>The research leading to these results has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Seventh Framework Programme</rs> under grant agreement <rs type="grantNumber">257528</rs> (<rs type="grantNumber">KHRES-MOI</rs>), <rs type="grantNumber">249008</rs> (Chorus+) and <rs type="grantNumber">258191</rs> (Promise).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MtRd6Qv">
					<idno type="grant-number">257528</idno>
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
				<org type="funding" xml:id="_2ud38D9">
					<idno type="grant-number">KHRES-MOI</idno>
				</org>
				<org type="funding" xml:id="_TePfD3X">
					<idno type="grant-number">249008</idno>
				</org>
				<org type="funding" xml:id="_wbBHqDJ">
					<idno type="grant-number">258191</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.88,536.14,337.67,8.96;7,151.56,547.18,328.99,8.96;7,151.56,558.10,329.04,8.96;7,151.56,569.02,101.68,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,251.40,536.14,229.15,8.96;7,151.56,547.18,52.04,8.96">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,230.64,547.18,249.91,8.96;7,151.56,558.10,159.21,8.96">Proceedings of the Ninth IEEE International Conference on Computer Vision -Volume 2. ICCV &apos;03</title>
		<meeting>the Ninth IEEE International Conference on Computer Vision -Volume 2. ICCV &apos;03<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.88,579.82,337.67,8.96;7,151.56,590.73,328.84,8.96;7,151.56,601.66,328.87,8.96;7,151.56,612.70,328.90,8.96;7,151.56,623.61,41.08,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,308.76,579.82,171.79,8.96;7,151.56,590.73,202.64,8.96">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,381.12,590.73,99.28,8.96;7,151.56,601.66,328.87,8.96;7,203.52,612.70,41.25,8.96">Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
	<note>CVPR &apos;06</note>
</biblStruct>

<biblStruct coords="7,142.88,634.41,337.67,8.96;7,151.56,645.33,328.99,8.96;7,151.56,656.26,215.20,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,297.36,634.41,183.19,8.96;7,151.56,645.33,82.03,8.96">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,255.36,645.33,225.19,8.96;7,151.56,656.26,46.01,8.96">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">June 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.88,119.14,337.64,8.96;8,151.56,130.18,328.84,8.96;8,151.56,141.09,328.99,8.96;8,151.56,152.01,329.06,8.96;8,151.56,163.06,294.88,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,311.40,119.14,169.12,8.96;8,151.56,130.18,100.76,8.96">The CLEF cross-language image retrieval track (ImageCLEF) 2004</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,268.32,141.09,212.23,8.96;8,151.56,152.01,222.42,8.96">Multilingual Information Access for Text, Speech and Images: Result of the fifth CLEF evaluation campaign</title>
		<title level="s" coord="8,450.24,152.01,30.38,8.96;8,151.56,163.06,135.41,8.96">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.88,173.97,337.64,8.96;8,151.56,184.89,328.95,8.96;8,151.56,195.94,329.08,8.96;8,151.56,206.85,71.02,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,267.60,184.89,212.92,8.96;8,151.56,195.94,19.23,8.96">Overview of the CLEF 2010 medical image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bakke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Kahn</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,198.00,195.94,278.00,8.96">Working Notes of CLEF 2010 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2010-09">September 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.88,217.77,337.62,8.96;8,151.56,228.81,328.99,8.96;8,151.56,239.73,328.90,8.96;8,151.56,250.65,113.56,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,354.96,217.77,125.54,8.96;8,151.56,228.81,168.08,8.96">Content-based query of image databases: inspirations from text retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,448.28,228.81,32.28,8.96;8,151.56,239.73,328.90,8.96">Selected Papers from The 11th Scandinavian Conference on Image Analysis SCIA &apos;99)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1193" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,268.68,250.65,127.48,8.96;8,139.32,261.69,341.20,8.96;8,151.56,272.61,328.96,8.96;8,151.56,283.53,196.84,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,264.36,261.69,216.16,8.96;8,151.56,272.61,82.93,8.96">An efficient approach to clustering in large multimedia databases with noise</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">K</forename><surname>Ersboll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eds</forename><forename type="middle">7</forename><surname>Hinneburg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,258.84,272.61,221.68,8.96;8,151.56,283.53,25.93,8.96">Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">5865</biblScope>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.88,294.57,337.73,8.96;8,151.56,305.49,328.87,8.96;8,151.56,316.41,329.06,8.96;8,151.56,327.33,328.99,8.96;8,151.56,338.37,22.60,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,291.00,294.57,189.60,8.96;8,151.56,305.49,273.01,8.96">The ImageCLEF medical retrieval task at icpr 2010 -information fusion to combine viusal and textual information</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,445.32,305.49,35.11,8.96;8,151.56,316.41,291.77,8.96">Proceedings of the International Conference on Pattern Recognition (ICPR 2010)</title>
		<title level="s" coord="8,450.24,316.41,30.38,8.96;8,151.56,327.33,137.85,8.96">Lecture Notes in Computer Science (LNCS</title>
		<meeting>the International Conference on Pattern Recognition (ICPR 2010)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010-08">August 2010</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="8,142.88,349.29,337.64,8.96;8,151.56,360.21,328.96,8.96;8,151.56,371.25,269.14,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,317.76,349.29,162.76,8.96;8,151.56,360.21,107.96,8.96">Information fusion for combining visual and textual image retrieval</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,281.16,360.21,199.36,8.96;8,151.56,371.25,32.58,8.96">International Conference on Pattern Recognition, ICPR&apos;10</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.54,382.17,338.09,8.96;8,151.56,393.09,110.56,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,254.28,382.17,137.14,8.96">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,419.28,382.17,61.36,8.96;8,151.56,393.09,42.47,8.96">Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.54,404.13,338.09,8.96;8,151.56,415.05,329.08,8.96;8,151.56,425.97,329.17,8.96;8,151.56,437.01,21.82,8.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,205.92,415.05,255.67,8.96">The CLEF 2011 medical image retrieval and classification tasks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,151.56,425.97,274.40,8.96">Working Notes of CLEF 2011 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2011-09">September 2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
