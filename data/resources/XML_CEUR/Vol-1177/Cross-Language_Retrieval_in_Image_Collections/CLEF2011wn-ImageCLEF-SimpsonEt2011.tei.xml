<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.28,115.96,330.79,12.62;1,149.74,133.89,315.87,12.62;1,158.95,151.82,297.89,12.62">Text-and Content-based Approaches to Image Modality Classification and Retrieval for the ImageCLEF 2011 Medical Retrieval Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,155.57,189.66,99.69,8.74"><roleName>Md</roleName><forename type="first">Matthew</forename><surname>Simpson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications U.S. National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.57,189.66,85.30,8.74"><forename type="first">Mahmudur</forename><surname>Rahman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications U.S. National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.83,189.66,70.54,8.74"><forename type="first">Srinivas</forename><surname>Phadnis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications U.S. National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,431.42,189.66,28.37,8.74;1,149.13,201.61,46.34,8.74"><forename type="first">Emilia</forename><surname>Apostolova</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications U.S. National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,203.43,201.61,98.68,8.74"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications U.S. National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.75,201.61,62.89,8.74"><forename type="first">Sameer</forename><surname>Antani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications U.S. National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,400.99,201.61,64.86,8.74"><forename type="first">George</forename><surname>Thoma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications U.S. National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.28,115.96,330.79,12.62;1,149.74,133.89,315.87,12.62;1,158.95,151.82,297.89,12.62">Text-and Content-based Approaches to Image Modality Classification and Retrieval for the ImageCLEF 2011 Medical Retrieval Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4CBC15EBC21B8E8E1200E1B34C613DF8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Retrieval</term>
					<term>Case-based Retrieval</term>
					<term>Image Modality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the participation of the Communications Engineering Branch (CEB), a division of the Lister Hill National Center for Biomedical Communications, in the ImageCLEF 2011 medical retrieval track. Our methods encompass a variety of techniques relating to text-and content-based image retrieval. Our textual approaches primarily utilize the Unified Medical Language System (UMLS) synonymy to identify concepts in topic descriptions and image-related text, and our visual approaches utilize similarity metrics based on computed "visual concepts" and low-level image features. We also explore mixed approaches that utilize a combination of textual and visual features. In this article we present an overview of the application of our methods to the modality classification, ad-hoc image retrieval, and case-based image retrieval tasks, and we describe our submitted runs and results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This article describes the participation of the Communications Engineering Branch (CEB), a division of the Lister Hill National Center for Biomedical Communications, in the ImageCLEF 2011 medical retrieval track.</p><p>The medical retrieval track <ref type="bibr" coords="1,279.56,524.44,10.73,8.74" target="#b8">[9]</ref> of ImgeCLEF 2011 consists of an image modality classification task and two retrieval tasks. For the modality classification task, the goal is to classify a given set of medical images according to eighteen modalities (e.g., CT or Histopathology) taken from five classes (e.g., Radiology or Microscopy). In the first retrieval task, a set of ad-hoc information requests is given, and the goal is to retrieve the most relevant images for each topic. Finally, in the second retrieval task, a set of case-based information requests is given, and the goal is to retrieve the most relevant articles describing similar cases.</p><p>In the following sections, we describe the textual and visual features that comprise our image and case representations (Sections 2-3) and our methods for the modality classification (Section 4) and medical retrieval tasks (Sections 5-6). Our textual approaches primarily utilize the Unified Medical Language System (UMLS) <ref type="bibr" coords="2,210.92,118.99,15.81,8.74" target="#b10">[11]</ref> synonymy to identify concepts in topic descriptions and image-related text, and our visual approaches rely on similarity metrics based on computed "visual concepts" and other low-level visual features. We also explore mixed approaches for the modality classification and retrieval tasks that utilize a combination of textual and visual features.</p><p>In Section 7 we describe our submitted runs, and in Section 8 we present our results. For the modality classification task, our best submission achieved a classification accuracy of 74% and was ranked within the submissions from the top three groups. For the retrieval tasks, our results were lower than expected yet reveal new insights which we anticipate will improve future work. For the modality classification and image retrieval tasks, our best results were obtained using mixed approaches, indicating the importance of both textual and visual features for these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Image Representation</head><p>Images contained in biomedical articles can be represented using both textual and visual features. Textual features can include text from an article that pertains to an image, such as image captions and "mentions" (snippets of text within the body of an article that discuss an image), and visual features can include information derived from the content of an image, such as shape, color and texture. We describe the features we use in representing images below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Textual Features</head><p>We represent each image in the ImageCLEF 2011 medical collection as a structured document of image-related text. Our representation includes the title, abstract, and MeSH terms<ref type="foot" coords="2,212.44,441.44,3.97,6.12" target="#foot_0">1</ref> of the article in which the image appears as well as the image's caption and mentions. Additionally, we identify within image captions textual Regions of Interest (ROIs). A textual ROI is a noun phrase describing the content of an interesting region of an image and is identified within a caption by a pointer. For example, in the caption "MR image reveals hypointense indeterminate nodule (arrow)," the word arrow points to the ROI containing a hypointense indeterminate nodule.</p><p>The above structured documents may be indexed and searched with a traditional search engine or the underlying term vectors may be exposed and added to a mixed image representation that includes the visual features described in Section 2.2. For the latter approach, the terms in a structured document field D j (e.g., caption) are commonly represented as an N -dimensional vector</p><formula xml:id="formula_0" coords="2,245.94,592.42,234.65,14.73">f term j = [w j1 , w j2 , • • • , w jN ] T<label>(1)</label></formula><p>where w j k denotes the tf-idf weight of term t k in document field D j , and N is the size of the vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Features</head><p>In addition to the above textual features, we also represent the visual content of images using various low-level global image features and a derived feature intended to capture the high-level semantic content of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-level Global Features</head><p>We represent the spatial structure and global shape and edge features of images with the Color Layout Descriptor (CLD) and Edge Histogram Descriptor (EHD) of MPEG-7 <ref type="bibr" coords="3,341.98,214.36,9.88,8.74" target="#b1">[2]</ref>. We extract the CLD feature as a vector f cld and the EHD feature as f ehd . Additionally, we extract the Color and Edge Directivity Descriptor (CEDD) <ref type="bibr" coords="3,320.31,238.27,10.65,8.74" target="#b2">[3]</ref> as f cedd and the Fuzzy Color and Texture Histogram (FCTH) <ref type="bibr" coords="3,258.36,250.23,10.44,8.74" target="#b3">[4]</ref> as f fcth using the Lucene image retrieval (LIRE) library. <ref type="foot" coords="3,165.50,260.61,3.97,6.12" target="#foot_1">2</ref> Both CEDD and FCTH incorporate color and texture information into single histograms that are suitable for image indexing and retrieval.</p><p>Concept Feature In a heterogeneous medical image collection, it is possible to identify specific local patches in images that are perceptually or semantically distinguishable, such as homogeneous texture patterns in gray-level radiological images or differential color and texture structures in microscopic pathology images. The variation in the local patches can be effectively modeled as "visual concepts" <ref type="bibr" coords="3,179.82,362.67,15.36,8.74" target="#b11">[12]</ref> using supervised machine learning-based classification techniques.</p><p>For the generation of these concepts, we utilize a multi-class Support Vector Machine (SVM) composed of several binary classifiers organized using the oneagainst-one strategy <ref type="bibr" coords="3,230.85,398.53,10.16,8.74" target="#b6">[7]</ref>. To train the SVMs, we manually assign a set of L visual concepts C = {c 1 , • • • , c i , • • • , c L } to the color and texture features of each fixed-size patch contained in an image. For a single image, the input to the training process is a set of color and texture feature vectors for all fixed-size patches along with their manually assigned concept labels. We generate the concept feature for each image I j in the collection by first partitioning I j into l patches as {x 1j , • • • , x kj , • • • , x lj }, where each x kj ∈ d is a combined color and texture feature vector. Then, for each x kj , we determine its concept label by the prediction of the multi-class SVM. Thus, in contrast to the low-level features described above, the concept feature represents an image as a set of high-level "visual concepts." Based on this encoding scheme, we represent an image I j as a vector of concepts</p><formula xml:id="formula_1" coords="3,233.88,539.78,246.71,15.05">f concept j = [w 1j , • • • , w ij , • • • w Lj ] T<label>(2)</label></formula><p>where each w ij denotes the tf-idf weight of concept c i in Image I j .</p><p>Clustered Features In an attempt to avoid the online computational complexity required to calculate visual similarity (described in Section 5.2) using the above features, we create an index of image similarity based on the clustering of feature vectors. For each visual feature described above, we cluster the vectors assigned to all images into k = d log |I| clusters using the k-means++ <ref type="bibr" coords="3,469.87,637.56,10.73,8.74" target="#b0">[1]</ref> algorithm, where d is the number of attributes in each vector and |I| is the total number of images in the collection. We then assign each cluster a unique "word" and represent each image as a sequence of these words. For example, using only MPEG-7 features for simplicity, an image might be represented as the sequence "cld:k1 ehd:k2" if the image's CLD feature was among the vectors in the first CLD cluster and its EHD feature was among the vectors in the second EHD cluster. The resulting textual interpretation of an image's visual features may then be indexed and searched using a traditional search engine or added to a mixed image representation that includes the textual features described in Section 2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Case Representation</head><p>We represent a full-text article as the combination of the textual and visual features of each image appearing in the article. Thus, each article representation consists of an article's title, abstract, and MeSH terms as well as the caption, mention, textual ROIs, and clustered visual features of each contained image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Modality Classification Task</head><p>Owing to their empirical success, we utilize multi-class SVMs for classifying images into eighteen medical image modalities based on their textual and visual features. We compose multi-class SVMs using the one-against-one strategy <ref type="bibr" coords="4,469.93,392.18,10.66,8.74" target="#b6">[7]</ref> for combining the pairwise classifications of each binary SVM. Figure <ref type="figure" coords="4,180.69,416.40,4.90,8.74" target="#fig_0">1</ref> describes our textual, visual, and mixed approaches to the modality classification task. Our visual and textual image features (with the text-based features represented as term vectors) can be used individually to produce singlemode classifications, or they may be combined to produce multimodal predictions. For the mixed approaches, the features may be combined into a single feature vector or they may be used independently, with the separate predictions being "fused" to form a single classifier. We fuse the output of multiple classifiers with the popular "Sum" classifier combination technique <ref type="bibr" coords="4,362.97,500.08,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="4,374.78,500.08,12.73,8.74" target="#b9">10]</ref> of Bayes' theorem.</p><p>We utilize the above approach for both flat and hierarchical modality classification. For the former, the system classifies an image's modality as one of eighteen medical image modalities. For the latter, the system first classifies the image's modality as belonging to one of five high-level modality classes (i.e., Radiology, Microscopy, Photograph, Graphic, or Other), and then it classifies the image's modality as one of the original eighteen, given its predicted high-level class. The hierarchical approach requires the training of a single high-level classifier and multiple class-specific classifiers, and an appropriate set of example images must be constructed to train each classifier.</p><p>Due to the small number of training examples for several modality classes, we created an extended set of training images from the collection. We accomplished this task by first performing textual image searches, using particular modalities as queries, then by manually inspecting and labeling the retrieved results. In this section we describe our textual, visual and mixed approaches to the ad-hoc image retrieval task. Descriptions of the submitted runs that utilize these methods are presented in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Textual Approaches</head><p>To allow for efficient retrieval and to compare their relative performance, we index the textual image representations described in Section 2.1 with the Essie <ref type="bibr" coords="5,470.29,526.27,10.31,8.74" target="#b7">[8]</ref> and Lucene/SOLR<ref type="foot" coords="5,218.46,536.65,3.97,6.12" target="#foot_2">3</ref> search engines. Essie is a search engine developed by the U.S. National Library of Medicine and is particularly well-suited for the medical retrieval task due to its ability to automatically expand query terms using the UMLS synonymy. Lucene/SOLR is a popular search engine developed by the Apache Software Foundation that employs the well-known vector space model of information retrieval and tf-idf term weighting. Both Essie and Lucene/SOLR provide the ability to weight term occurrences according the location in a document in which they occur. For example, we weight term occurrences in image captions higher than those in article abstracts.</p><p>We organize each topic description into the well-formed clinical question (i.e., PICO <ref type="foot" coords="6,160.59,129.37,3.97,6.12" target="#foot_3">4</ref> ) framework following the method described by Demner-Fushman and Lin <ref type="bibr" coords="6,151.48,142.90,9.76,8.74" target="#b4">[5]</ref>. Extractors identify UMLS concepts related to problems, interventions, age, anatomy, drugs, and image modality. When used as part of an Essie query, each extracted concept is automatically expanded along the synonymy relationships in the UMLS. For approaches that make use of the Lucene/SOLR search engine, we first expand the extracted concepts using Essie's built-in synonymy and then replace the extracted concepts with their expansions.</p><p>To construct a query for each topic, we create and combine several boolean expressions derived from the extracted concepts. First, we create an expression by combining the concepts using the AND operator (meaning all of the concepts are required to occur in an image's textual representation), and then we produce additional expressions by allowing an increasing number of the extracted concepts to be optional. Finally, we combine these expressions using the OR operator giving significantly more weight to expressions containing a fewer number of optional concepts. Additionally, we often include the verbatim topic description as a component of a query, but we give minimal weight to this expression compared to those containing the extracted concepts. We use the resulting queries to search the Essie and Lucene/SOLR indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visual Approaches</head><p>Our visual approaches to image retrieval are based on retrieving images that appear visually similar to the given topic images. The similarity between a query image I q and a target image I j , based on the visual features described in Section 2.2, is defined by</p><formula xml:id="formula_2" coords="6,235.39,430.35,245.21,22.37">Sim(I q , I j ) = F α F Sim F (I q , I j )<label>(3)</label></formula><p>where F ∈ {Concept, EHD, CLD, CEDD, FCTH}, α F are feature weights, and Sim F is Euclidean distance. In the above similary matching function, the feature weights are determined based on the cross validation accuracies of the featurespecific SVMs trained for the modality classification task. The weights are normalized to 0 ≤ α F ≤ 1 and α F = 1. In order to avoid the online computation of the above similarity metric, we may utilize clustered visual features, also described in Section 2.2, to retrieve visually similar images. To allow for efficient retrieval, we index the textual interpretations of the images' clustered visual features using the Essie and Lucene/SOLR search engines. Again, we utilize both search engines in order to compare their relative performance. Retrieval is performed by first extracting a query image's visual features, then by determining the features' cluster membership, and finally by combining the unique "words" assigned to the clusters containing the features in order to form a textual query. For a given topic, we combine the textual interpretations of all features for all sample images using the OR operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Mixed Approaches</head><p>Our mixed approaches to image retrieval combine our textual and visual approaches through a process of filtering and re-ranking or by issuing multimodal queries. For the filtering approach, we first filter the image collection by the two most probable modalities of the query images, as indicated by our modality classifier. We then query the remaining images according to a textual approach. For the re-ranking approach, we first query the image collection using a textual approach, and we then re-rank the retrieved images according to their visual similarity with the query images, as indicated by the above similarity metric. Finally, for approaches involving multimodal queries, we utilize Essie and Lucene/SOLR to index images using both their textual features and the textual interpretation of their clustered image features. We construct multimodal queries by combining a query produced by a textual approach with that produced by the visual approach described above that utilizes clustered image features. We join the textual and visual components of the query with the OR operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Case-Based Retrieval Task</head><p>Our method for performing case-based retrieval is analogous to our approach for ad-hoc image retrieval. Here, we index the case representations described in Section 3 using the Essie and Lucene/SOLR search engines (for performance comparison). We generate textual and mixed queries appropriate for both search engines according to the approaches described in Sections 5.1 and 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Submitted Runs</head><p>In this section we describe each of our submitted runs for the modality classification, ad-hoc image retrieval, and case-based image retrieval tasks. Each run is identified by its trec_eval run ID and followed by a submission mode (textual, visual or mixed) and type (automatic, manual or feedback).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Modality Classification Task</head><p>We submitted the following 10 runs for the modality classification task: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Ad-hoc Image Retrieval Task</head><p>We submitted the following 10 runs for the ad-hoc image retrieval task:</p><p>1. iti-essie-baseline+expanded-concepts (textual, automatic): Textual search using the Essie search engine. Each image is represented by its textual features, and queries combine the verbatim topic description with extracted concepts and image modalities. 2. iti-lucene-baseline+expanded-concepts (textual, automatic): Textual search using the Lucene/SOLR search engine. Each image is represented as in Run 1, and queries combine the verbatim topic description with extracted concepts and image modalities that are then expanded along synonymy relationships in the UMLS. 3. iti-lucene-image (visual, automatic): Visual search using the Lucene/SOLR search engine. Each image is represented using the textual interpretation of its clustered visual features, and queries combine the visual "words" of each of the sample topic images. 4. image fusion category weight filter (visual, automatic): Similarity matching over images filtered according to modality. Each image is represented as a subset of visual features (Concept, CLD, and EHD), and similarity scores for each feature are linearly combined and weighted according to modality class. 5. image fusion category weight filter merge (visual, automatic): Similarity matching like Run 4, but each image is scored as the sum of its similarity with each of the sample topic images.</p><p>6. image fusion category weight merge (visual, automatic): Similarity matching like Run 5, but the image set is not filtered by modality prior to retrieval. 7. iti-lucene-baseline+expanded-concepts+image (mixed, automatic): Mixed search using the Lucene/SOLR search engine. Image representations and queries combine the textual and visual features of Run 2 and Run 3. 8. iti-essie-baseline+expanded-concepts+image (mixed, automatic): Mixed search using the Essie search engine. Image representations and queries combine the textual and visual features of Run 1 and Run 3. 9. Multimodal Rerank Merge (mixed, automatic): Re-ranking of Run 1 according to Run 6. 10. Multimodal Rerank Filter Merge (mixed, automatic): Re-ranking of Run 1 according to Run 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Case-based Retrieval Task</head><p>We submitted the following 10 runs for the case-based retrieval task:</p><p>1. iti-essie-manual (textual, manual): Textual search using the Essie search engine. Articles are represented by their textual features, and queries were manually generated by a medical doctor with expertise in biomedical informatics. 2. iti-essie-frames (textual, automatic): Textual search using the Essie search engine. Articles are represented by their textual features, and queries combine concepts from automatically generated PICO summary frames. 3. iti-lucene-frames (textual, automatic): Textual search using the Lucene/SOLR search engine. Articles are represented by their textual features, and queries combine concepts from automatically generated PICO summary frames that are then expanded along synonymy relationships in the UMLS. 4. iti-lucene-baseline (textual, automatic): Textual search with the Lucene/SOLR search engine. Articles are represented by their textual features, and queries are the verbatim topic descriptions. 5. iti-lucene-expanded-concepts (textual, automatic): Textual search using the Lucene/SOLR search engine. Articles are represented by their textual features, and queries combine extracted concepts and image modalities that are then expanded along synonymy relationships in the UMLS. 6. iti-lucene-baseline+expanded-concepts (textual, automatic): Textual search like Run 5, but queries also include verbatim topic descriptions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Results</head><p>Table <ref type="table" coords="10,160.70,548.52,4.88,8.74" target="#tab_1">1</ref> presents the classification accuracy of our submitted runs for the modality classification task. image text test result multilevel, a mixed approach, achieved the highest accuracy (74%) of our submitted runs and was ranked within the submissions from the top three groups. This result validates our hierarchical classification approach and, as in our previous experience with image modality classification <ref type="bibr" coords="10,195.69,608.30,14.90,8.74" target="#b13">[14]</ref>, underscores the benefits of combining textual and visual features. Surprisingly, the use of an extended set of training images did not improve classification accuracy. achieved the highest MAP (0.14) among our submitted mixed runs, iti-lucene-baseline+expanded-concepts achieved the highest MAP (0.13) among our submitted textual runs, and iti-lucene-image achieved the highest MAP (0.02) among our submitted visual runs. Although our retrieval results are lower than expected given our previous experience <ref type="bibr" coords="11,265.26,343.55,15.28,8.74" target="#b12">[13,</ref><ref type="bibr" coords="11,281.84,343.55,11.46,8.74" target="#b13">14]</ref>, they demonstrate the utility of combining both textual and visual features. In particular, the use of clustered visual features, which can be indexed and searched with a traditional text-based search engine, not only resulted in our best visual approach but, when used in combination with our best textual approach, produced our best overall submission. Finally, Table <ref type="table" coords="11,213.05,403.85,4.99,8.74" target="#tab_4">3</ref> presents the MAP of our submitted runs for the case-based retrieval task. iti-essie-manual achieved the highest MAP (0.09) among our submitted textual runs, and iti-lucene-baseline+expanded-concepts+image achieved the highest MAP (0.03) among our submitted mixed runs. iti-lucene-baseline, a textual approach, achieved the highest MAP (0.08) among our submitted automatic runs. Similar to our results for the image retrieval task, our case-based retrieval results are lower than expected given our previous experience. The relatively low MAP for most ImageCLEF 2011 case-based submissions may be due, in part, to the existence in the collection of only a small number of case reports, clinical trials, or other types of documents relevant for case-based topics. Surprisingly, our submissions that utilize extracted concepts and image modalities achieved a lower MAP than our textual baseline, which used the verbatim topic descriptions as queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>This article describes the methods and results of the Communications Engineering Branch, a division of the Lister Hill National Center for Biomedical Communications, for the ImageCLEF 2011 medical retrieval track. We submitted ten runs each for the modality classification task and the ad-hoc image and case-based retrieval tasks. For the modality classification task, our best submission, a mixed approach, achieved a classification accuracy of 74% and was ranked within the submissions from the top three groups. For the retrieval tasks, our results were lower than expected but reveal the mixed approaches involving clustered visual features to be promising methods for combing textual and visual image features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,168.21,367.30,278.93,7.89;5,134.77,115.83,345.83,236.70"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Process flow diagram for our modality classification approach</figDesc><graphic coords="5,134.77,115.83,345.83,236.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,138.97,560.42,343.00,104.43"><head></head><label></label><figDesc>image text test result ext (mixed, automatic): SVM classification like Run 3 but derived from an extended set of training images. 5. image text test result sum (mixed, automatic): Classifier combination using the "Sum" method of Bayes' theorem. Each image is represented as a group of vectors for visual and textual features that are individually classified using SVMs derived from the original set of training images. 6. image text test result sum ext (mixed, automatic): Classifier combination like Run 5 but using SVMs derived from an extended set of training images. 7. image text test result CV (mixed, feedback): Linear classifier combination weighting classifiers according to their normalized cross-validation accuracies. Each image is represented as a group of vectors for visual and textual features that are individually classified using SVMs derived from the original set of training images. 8. image text test result CV ext (mixed, automatic): Classifier combination like Run 7 but using SVMs derived from an extended set of training images. 9. image text test result multilevel (mixed, automatic): Hierarchical SVM classification derived from the original set of training images. Each image is represented as a single vector of visual and textual features that is first classified into a top-level modality class and is then further classified using a class-specific SVM. 10. image text test result multilevel ext (mixed, automatic): SVM classification like Run 9 but using SVMs derived from an extended set of training images.</figDesc><table coords="7,138.97,560.42,343.00,104.43"><row><cell>4.</cell></row><row><cell>1. image test result original (visual, automatic): SVM classification derived</cell></row><row><cell>from the original set of training images. Each image is represented as a single</cell></row><row><cell>vector of visual features.</cell></row><row><cell>2. image test result ext (visual, automatic): SVM classification like Run 1 but</cell></row><row><cell>derived from an extended set of training images.</cell></row><row><cell>3. image text test result original (mixed, automatic): SVM classification derived</cell></row><row><cell>from the original set of training images. Each image is represented as a single</cell></row><row><cell>vector containing visual features and a subset of textual features (article title,</cell></row><row><cell>MeSH terms, caption, and mention).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,138.97,549.39,344.61,115.47"><head>Table 1 .</head><label>1</label><figDesc><ref type="bibr" coords="9,138.97,549.39,3.87,8.74" target="#b6">7</ref>. iti-lucene-baseline+expanded-concepts+cases (textual, automatic): Textual search like Run 6, but articles are boosted if their MeSH terms are indicative of case studies or clinical trials. 8. iti-lucene-expanded-concepts+image (mixed, automatic): Mixed search using the Lucene/SOLR search engine. Articles are represented by their textual features and the textual interpretation of the clustered visual features of each contained image. Queries are as in Run 5 but also include the visual "words" of each of the sample topic images. 9. iti-lucene-baseline+expanded-concepts+image (mixed, automatic): Mixed search like Run 8, but queries also include verbatim topic descriptions. Accuracy results for the modality classification task.</figDesc><table coords="10,142.74,123.13,329.89,288.94"><row><cell>ID</cell><cell>Mode</cell><cell>Type</cell><cell cols="2">Accuracy (%)</cell></row><row><cell>image text test result multilevel</cell><cell>Mixed</cell><cell cols="2">Automatic</cell><cell>74.12</cell></row><row><cell>image text test result sum ext</cell><cell>Mixed</cell><cell cols="2">Automatic</cell><cell>60.25</cell></row><row><cell>image text test result CV</cell><cell>Mixed</cell><cell cols="2">Feedback</cell><cell>59.66</cell></row><row><cell>image text test result multilevel ext</cell><cell>Mixed</cell><cell cols="2">Automatic</cell><cell>59.17</cell></row><row><cell>image text test result sum</cell><cell>Mixed</cell><cell cols="2">Automatic</cell><cell>59.17</cell></row><row><cell>image text test result CV ext</cell><cell>Mixed</cell><cell cols="2">Automatic</cell><cell>58.20</cell></row><row><cell>image text test result original</cell><cell>Mixed</cell><cell cols="2">Automatic</cell><cell>58.20</cell></row><row><cell>image test result original</cell><cell>Visual</cell><cell cols="2">Automatic</cell><cell>57.12</cell></row><row><cell>image text test result ext</cell><cell>Mixed</cell><cell cols="2">Automatic</cell><cell>54.39</cell></row><row><cell>image test result ext</cell><cell>Visual</cell><cell cols="2">Automatic</cell><cell>48.53</cell></row><row><cell>ID</cell><cell></cell><cell>Mode</cell><cell>Type</cell><cell>MAP</cell></row><row><cell cols="2">iti-lucene-baseline+expanded-concepts+image</cell><cell>Mixed</cell><cell>Automatic</cell><cell>0.1356</cell></row><row><cell>iti-lucene-baseline+expanded-concepts</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>0.1255</cell></row><row><cell>iti-essie-baseline+expanded-concepts</cell><cell></cell><cell>Textual</cell><cell>Automatic</cell><cell>0.0966</cell></row><row><cell>Multimodal Rerank Filter Merge</cell><cell></cell><cell>Mixed</cell><cell>Automatic</cell><cell>0.0910</cell></row><row><cell>Multimodal Rerank Merge</cell><cell></cell><cell>Mixed</cell><cell>Automatic</cell><cell>0.0903</cell></row><row><cell cols="2">iti-essie-baseline+expanded-concepts+image</cell><cell>Mixed</cell><cell>Automatic</cell><cell>0.0843</cell></row><row><cell>iti-lucene-image</cell><cell></cell><cell>Visual</cell><cell>Automatic</cell><cell>0.0245</cell></row><row><cell>image fusion category weight filter</cell><cell></cell><cell>Visual</cell><cell>Automatic</cell><cell>0.0221</cell></row><row><cell>image fusion category weight filter merge</cell><cell></cell><cell>Visual</cell><cell>Automatic</cell><cell>0.0201</cell></row><row><cell>image fusion category weight merge</cell><cell></cell><cell>Visual</cell><cell>Automatic</cell><cell>0.0193</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,133.99,419.20,347.52,75.83"><head>Table 2 .</head><label>2</label><figDesc>Retrieval results for the ad-hoc image retrieval task</figDesc><table /><note coords="10,133.99,462.38,347.52,8.74;10,151.70,474.34,328.89,8.74;10,151.70,486.29,198.73,8.74"><p>10. iti-lucene-baseline+expanded-concepts+image+cases (mixed, automatic): Mixed search like Run 9, but articles are boosted if their MeSH terms are indicative of case studies or clinical trials.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,134.77,644.16,345.83,20.69"><head>Table 2</head><label>2</label><figDesc>presents the Mean Average Precision (MAP) of our submitted runs for the ad-hoc image retrieval task. iti-lucene-baseline+expanded-concepts+image</figDesc><table coords="11,138.75,123.13,12.08,7.89"><row><cell>ID</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,187.99,252.74,239.38,7.89"><head>Table 3 .</head><label>3</label><figDesc>Retrieval results for the case-based retrieval task.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,645.84,335.87,7.86;2,144.73,656.80,102.22,7.86"><p>MeSH is a controlled vocabulary created by U.S. National Library of Medicine to index biomedical articles.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,657.44,183.59,7.47"><p>http://freshmeat.net/projects/lirecbir/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,144.73,657.44,117.68,7.47"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,144.73,645.84,335.87,7.86;6,144.73,656.80,330.63,7.86"><p>PICO is a mnemonic for structuring clinical questions in evidence-based practice and represents Patient/Population/Problem, Intervention, Comparison, and Outcome.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,210.33,339.18,7.86;12,151.52,221.29,330.86,7.86;12,151.52,232.25,132.47,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,263.03,210.33,181.91,7.86">k-means++: The advantages of careful seeding</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,464.93,210.33,17.20,7.86;12,151.52,221.29,326.64,7.86">Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">07</biblScope>
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,243.21,337.64,7.86;12,151.19,254.17,330.48,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,300.61,243.21,148.49,7.86">Overview of the MPEG-7 standard</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Puri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,458.01,243.21,22.58,7.86;12,151.19,254.17,240.23,7.86">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="688" to="695" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,265.13,338.92,7.86;12,151.18,276.09,330.69,7.86;12,151.19,287.05,329.59,7.86;12,151.18,298.01,330.95,7.86;12,151.18,308.96,127.94,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,301.35,265.13,180.53,7.86;12,151.18,276.09,207.10,7.86">CEDD: Color and edge directivity descriptor: A compact descriptor for image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,232.45,287.05,248.33,7.86;12,151.18,298.01,57.20,7.86">Proceedings of the 6th International Conference on Computer Vision Systems</title>
		<title level="s" coord="12,214.85,298.01,135.17,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gasteratos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vincze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</editor>
		<meeting>the 6th International Conference on Computer Vision Systems<address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5008</biblScope>
			<biblScope unit="page" from="312" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,319.92,337.98,7.86;12,151.52,330.88,329.07,7.86;12,151.05,341.84,330.62,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,301.08,319.92,179.86,7.86;12,151.52,330.88,170.14,7.86">FCTH: Fuzzy color and texture histogram: A low level feature for accurate image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,340.44,330.88,140.15,7.86;12,151.05,341.84,250.69,7.86">Proceedings of the 9th International Workshop on Image Analysis for Multimedia Interactive Services</title>
		<meeting>the 9th International Workshop on Image Analysis for Multimedia Interactive Services</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,352.80,337.64,7.86;12,151.52,363.76,320.13,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,275.68,352.80,204.92,7.86;12,151.52,363.76,100.54,7.86">Answering clinical questions with knowledge-based and statistical techniques</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,259.19,363.76,106.72,7.86">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="103" />
			<date type="published" when="2007-03">Mar 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,374.72,337.64,7.86;12,151.52,385.68,45.69,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="12,306.32,374.72,87.62,7.86">Pattern Classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>John Wiley &amp; Sons Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,396.64,337.63,7.86;12,151.52,407.59,127.58,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,266.86,396.64,144.07,7.86">Classification by pairwise coupling</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,419.47,396.64,61.12,7.86;12,151.52,407.59,36.96,7.86">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="451" to="471" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,418.55,339.17,7.86;12,151.52,429.51,329.07,7.86;12,151.18,440.47,132.19,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,343.64,418.55,138.49,7.86;12,151.52,429.51,139.78,7.86">Essie: A concept-based search engine for structured biomedical text</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">C</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Loane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,298.14,429.51,182.45,7.86;12,151.18,440.47,46.17,7.86">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="263" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,451.43,338.91,7.86;12,151.19,462.39,330.33,7.86;12,151.05,473.35,88.64,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,151.19,462.39,261.53,7.86">The CLEF 2011 medical image retrieval and classification tasks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,434.29,462.39,47.22,7.86;12,151.05,473.35,59.97,7.86">CLEF 2011 Working Notes</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,484.31,337.98,7.86;12,151.19,495.27,229.74,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,352.11,484.31,99.02,7.86">On combining classifiers</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,458.01,484.31,22.58,7.86;12,151.19,495.27,134.51,7.86">IEEE Transactions on Pattern Analysis</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="2329" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,506.22,339.77,7.86;12,151.52,517.18,236.61,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,329.16,506.22,148.87,7.86">The unified medical language system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,151.52,517.18,145.99,7.86">Methods of Information in Medicine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="281" to="291" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,528.14,337.98,7.86;12,151.52,539.10,329.07,7.86;12,151.52,550.06,309.48,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,318.03,528.14,162.56,7.86;12,151.52,539.10,205.20,7.86">A medical image retrieval framework in correlation enhanced visual concept feature space</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,379.67,539.10,100.92,7.86;12,151.52,550.06,280.82,7.86">Proceedings of the 22nd IEEE International Symposium on Computer-Based Medical Systems</title>
		<meeting>the 22nd IEEE International Symposium on Computer-Based Medical Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,561.02,339.26,7.86;12,151.19,571.98,329.63,7.86;12,151.52,582.94,119.11,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="12,151.19,571.98,329.63,7.86;12,151.52,582.94,90.43,7.86">Text-and content-based approaches to image retrieval for the ImageCLEF 2009 medical retrieval track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,593.90,339.25,7.86;12,151.52,604.85,329.07,7.86;12,151.52,615.81,222.67,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="12,166.85,604.85,313.75,7.86;12,151.52,615.81,194.00,7.86">Text-and content-based approaches to image modality detection and retrieval for the ImageCLEF 2010 medical retrieval track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Thoma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
