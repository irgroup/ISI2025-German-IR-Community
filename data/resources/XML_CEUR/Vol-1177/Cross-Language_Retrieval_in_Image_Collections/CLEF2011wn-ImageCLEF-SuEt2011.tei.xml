<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,172.17,115.96,271.03,12.62;1,134.77,133.89,345.82,12.62">Semantic Contexts and Fisher Vectors for the ImageCLEF 2011 Photo Annotation Task</title>
				<funder>
					<orgName type="full">French State agency</orgName>
				</funder>
				<funder ref="#_TeufRNd">
					<orgName type="full">OSEO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,252.98,171.56,26.57,8.74"><forename type="first">Yu</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">GREYC</orgName>
								<orgName type="institution">University of Caen</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.24,171.56,60.14,8.74"><forename type="first">Frédéric</forename><surname>Jurie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">GREYC</orgName>
								<orgName type="institution">University of Caen</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,172.17,115.96,271.03,12.62;1,134.77,133.89,345.82,12.62">Semantic Contexts and Fisher Vectors for the ImageCLEF 2011 Photo Annotation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9A50BC9F06BB165C6940C0BDB28049EB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image classification</term>
					<term>Photo annotation</term>
					<term>Bag-of-Words model</term>
					<term>Semantic context</term>
					<term>Fisher Vectors</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of UNI-CAEN/GREYC to the ImageCLEF 2011 photo annotation task. The proposed approach uses visual image features and binary annotations of concepts only. In this approach, the annotations are predicted by SVM classifiers trained separately for each concept. The classifiers take Bag-of-Words histograms and fisher vectors representations as inputs, both being combined at the decision level. Furthermore, contextual information is also embedded into the Bag-of-Words histograms to enhance their performance. The experimental results show that the combination of Bag-of-Words histograms and Fisher vectors brings significant performance increase (e.g. 4% for Mean Average Precision). Furthermore, the results of our best-run rank in top 3 for both concept and image level evaluations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The aim of the ImageCLEF 2011 photo annotation task is to automatically assign to each image a set of concepts taken from a list of 99 possible pre-defined visual concepts. In this task, the participants are given 8000 training images associated with the corresponding 99 binary labels, each of which corresponds to a visual concept, as well as the photo tagging ontology, EXIF data and Flickr user tags. In the test phase, the participants are requested to give to each test image the labels of all the visual concepts describing the image. The evaluation of performance is done at concept and image levels. For the former, Mean Average Precision (MAP) is computed for each concept. For the latter, F-Measure (F-ex) and the Semantic R-Precision (SR-Precision) are computed for each image. For more details on this task, please refer to <ref type="bibr" coords="1,288.35,584.39,10.52,8.74" target="#b6">[7]</ref> .</p><p>In our participation, we did not use photo tagging ontology, EXIF data and Flickr user tags. Our results are only based on visual image features. Specifically, we extracted different types of local features (e.g. SIFT) from images and then adopted Bag-of-Words (BoW) model to aggregate local features into a global image descriptor. Our participation is mainly inspired by the work of Su and Jurie <ref type="bibr" coords="1,195.30,656.12,14.61,8.74" target="#b10">[11]</ref>, which proposed to embed some contextual information into the BoW model. In addition, some improvements over <ref type="bibr" coords="2,380.88,118.99,15.50,8.74" target="#b10">[11]</ref> are also proposed. Indeed, Fisher Vectors (FV) have been reported to give good performance on both object recognition and image retrieval tasks <ref type="bibr" coords="2,346.03,142.90,9.96,8.74" target="#b8">[9]</ref>. Thus, we also computed FV from images and combined them with the context-embedded BoW histograms at decision level, i.e, training classifiers for Fisher Vectors and context-embedded BoW histograms separately and combining classifiers by averaging their outputs. As to photo annotation, the above process is performed for each concept independently and the averaged classifier outputs are used as the confidences of concept occurrence.</p><p>The organization of this paper is as follows: In section 2, we describe local features used in our method. Then, we explain how to combine BoW model with both semantic contexts (section 3) and FV (section 4). Experimental evaluation is given in section 5, followed by a conclusion in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Visual Features</head><p>In our method, 6 kinds of visual features are extracted from each image, which are introduced in the following paragraph. Before feature computation, the images are scaled to be at most 300 × 300 pixels, with their original aspect ratios maintained. Except for LAB features which encode color information, color images are first transformed to grayscale.</p><p>SIFT Vector quantized SIFT descriptors <ref type="bibr" coords="2,331.46,386.87,10.52,8.74" target="#b5">[6]</ref> are computed for 5000 image patches with randomly selected positions and scales (with scales from 16 to 64 pixels), and are quantized to 1024 k-means centers.</p><p>HOG HOG descriptors <ref type="bibr" coords="2,247.24,428.71,10.52,8.74" target="#b2">[3]</ref> are densely extracted on a regular grid at step of 8 pixels. On each node of the grid a 31 dimensional descriptor is computed and then 2 × 2 neighboring descriptors are concatenated to form a descriptor of 124 dimensions. HOG features are finally vector quantized to 256 k-means centers.</p><p>Textons Texton descriptors <ref type="bibr" coords="2,270.07,482.51,15.50,8.74" target="#b11">[12]</ref> are generated by computing the output of 36 Gabor filters with different scales and orientations for each pixel, and then quantized to 256 k-means centers.</p><p>SSIM Self-similarity descriptors <ref type="bibr" coords="2,282.77,524.35,15.50,8.74" target="#b9">[10]</ref> are computed on a regular grid at step of five pixels. Each feature is obtained by computing the correlation map of a patch of 5 × 5 in a window with radius of 40 pixels, then quantizing it in 3 radial bins and 10 angular bins, obtaining 30 dimensional descriptor vectors. Self-similarity features are finally quantized to 256 k-means centers.</p><p>LAB LAB descriptors <ref type="bibr" coords="2,241.69,590.10,10.52,8.74" target="#b3">[4]</ref> are computed for each pixel and then quantized to 128 k-means centers.</p><p>Canny Canny edge descriptors <ref type="bibr" coords="2,277.53,619.99,10.52,8.74" target="#b0">[1]</ref> are computed for each pixel and then quantized to 8 orientation bins.</p><p>Finally, concatenating all BoW histograms gives a 1928-dimensional feature vector which can describe an image or a image region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Image Representation by Embedding Semantic Contexts into BoW Model</head><p>In this section, we first review how do we define the semantic contexts and embed them into BoW model as introduced in <ref type="bibr" coords="3,353.36,167.86,14.61,8.74" target="#b10">[11]</ref>. Then we introduce our improvements over this method. Fig. <ref type="figure" coords="3,154.40,505.15,4.13,7.89">1</ref>. Grouped semantic contexts and some illustrative training images <ref type="bibr" coords="3,443.35,505.18,13.52,7.86" target="#b10">[11]</ref>. The values between brackets are the number of semantic contexts within corresponding groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic Context</head><p>In <ref type="bibr" coords="3,146.37,596.34,14.61,8.74" target="#b10">[11]</ref>, 110 semantic contexts are defined by hand with the intention of providing abundant semantic information for image description. (see Fig. <ref type="figure" coords="3,418.71,608.30,3.87,8.74">1</ref>). Two types of contexts are distinguished: global contexts including global scenes and local contexts including local scenes, colors, shapes, materials and objects.</p><p>For each semantic context, we learn a SVM classifier with linear kernel (hereafter called as context classifiers). For the global contexts, the classifiers are learned on whole images described by BoW histograms. For the local contexts, the classifiers are learned on some randomly sampled image regions described again by BoW histograms. The training images are automatically downloaded from Google image search by using the name of context as query. After the manual annotation, about 400 relevant images are reserved for each context. They are used as positive images for the corresponding context while images from the other contexts are considered as negatives.</p><p>In test phase, images (for global contexts) or regions (for local contexts) are input to context classifiers and a sigmoid function is used to transform the original decision values to probabilities (refer to <ref type="bibr" coords="4,346.47,227.22,10.30,8.74" target="#b1">[2]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Embedding Semantic Contexts into BoW model</head><p>Assume that, for an image I, a set of local features f i , i = 1, . . . , N are extracted from it, where N is the number of local features. The BoW model consists of V visual words v j , j = 1, . . . , V . The traditional BoW feature for v j measures the occurrence probability of v j on image I, say p(v j |I). In practice, p(v j |I) is usually computed by:</p><formula xml:id="formula_0" coords="4,253.10,343.72,227.50,30.32">p(v j |I) = 1 N N i=1 δ(f i , v j ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1" coords="4,226.13,393.89,254.46,27.82">δ(f i , v j ) = 1 if j = arg min j=1,...,V d(f i , v j ) 0 else (2)</formula><p>and d is a distance function (e.g., the L2 norm). Marginalizing p(v j |I) over different local contexts gives:</p><formula xml:id="formula_2" coords="4,240.09,468.33,240.50,30.55">p(v j |I) = C k=1 p(v j |c k , I)p(c k |I),<label>(3)</label></formula><p>where c k is the k-th context, C is the number of local contexts (75 in our case), p(v j |c k , I) is the context-specific occurrence probability of v j on image I, p(c k |I) is the occurrence probability of context c k on image I.</p><p>On the other hand, the second term of Eq. 3, which gives the distribution of different contexts on image I, can also provide rich information to describe the image, as shown by <ref type="bibr" coords="4,238.20,571.81,14.61,8.74" target="#b12">[13]</ref>. For example, knowing an image is composed of one third of sky, one third of sea and one third of beach, brings a lot of information regarding the content of this image. At the end, images are eventually represented by multiple context-specific BoW histograms, i.e., p(v j |c k , I) and a vector of context-occurring probabilities, i.e., p(c k |I).</p><p>In <ref type="bibr" coords="4,162.65,632.21,14.61,8.74" target="#b10">[11]</ref>, p(v j |c k , I) is constructed by modeling the probabilistic distribution of context c k on image I which is estimated by dividing image I into a set of regions I p and predicting the occurrence probabilities of c k for each region (by using context classifiers). By denoting I p (f i ) the set of image regions which cover the local feature f i , we define:</p><formula xml:id="formula_3" coords="5,221.44,150.49,259.15,30.32">p(v j |c k , I) = 1 N N i=1 δ(f i , v j )p(c k |I p (f i )),<label>(4)</label></formula><p>where p(c k |I p (f i ) can be considered as the weight of local feature f i . In practice, where C is the number of all contexts (110 in our case) and C is the number of local contexts (75 in our case). We call this image descriptor as semantic features.</p><formula xml:id="formula_4" coords="5,134.77,202.88,106.54,9.65">p(c k |I p (f i )) is computed</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Our improvements over [11]</head><p>The above subsection reviewed the process of constructing context-specific BoW histograms introduced in <ref type="bibr" coords="5,250.28,411.91,14.61,8.74" target="#b10">[11]</ref>. For our participation to the ImageCLEF 2011 photo annotation task, some improvements over this method are proposed. First, we learn a specific vocabulary for each semantic context rather than use a uniform vocabulary for all contexts as in <ref type="bibr" coords="5,302.65,447.78,14.61,8.74" target="#b10">[11]</ref>. Second, instead of selecting a single context for each visual word as in <ref type="bibr" coords="5,291.75,459.73,14.61,8.74" target="#b10">[11]</ref>, we train a classifier for each contextspecific BoW histogram and then combine all the classifiers. Detailed implementation of these two improvements are given in the following paragraph.</p><p>In the traditional vocabulary learning process, local features extracted from a set of images are randomly (or uniformly) sampled and then vector quantized to get visual words. Differently, when learning our context-specific vocabulary, the sampling of local features is based on the distribution of this context on images. Specifically, more local features are sampled at the image regions with higher context-occurring probabilities (brighter image regions in Fig. <ref type="figure" coords="5,432.47,555.38,3.87,8.74">2</ref>). In practice, this process is implemented by assigning each local feature f i a probability p(c k |I p (f i )) (defined in section 3.2) and sampling local features based on their probabilities, which is formulated as follows.</p><formula xml:id="formula_5" coords="5,243.27,611.35,237.32,20.69">s(f i ) = 1 if p(c k |I p (f i ) ≥ r i 0 else<label>(5)</label></formula><p>where s(f i ) indicates whether the local feature f i is selected or not and r i are random numbers which are uniformly sampled between 0 and 1.</p><p>After sampling local features for each context, k-means is used to build multiple context-specific vocabularies. An image is then represented by multiple context-specific BoW histograms. The construction of context-specific BoW histogram is the same as that in 3.2 (see Eq.4)</p><p>Concatenating all the context-specific BoW histograms leads to a very high dimensional feature vector (in our case 1928 × 75=144,600D). Thus, we train a classifier for each context-specific BoW histogram and combine the classifiers by averaging their outputs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chi2 SVM semantic features</head><p>Fig. <ref type="figure" coords="6,154.40,498.18,4.13,7.89">2</ref>. Combination of BoW model and semantic contexts. For an image, multiple saliency maps are generated by both context classifiers and SPM channels, with which multiple BoW histograms are constructed by weighting local features according to saliency maps. After that, multiple classifiers are learned, each of which corresponds to a BoW histogram. In addition, the occurrence probabilities of semantic contexts (also referred as semantic features) are predicted for the image, for which a classifier is learned. Finally, all the classifiers are combined by averaging their outputs.</p><p>Recall that the way we embed contextual information into BoW model is based on weighting local features (see Eq.4). It is similar to the well-known spatial pyramid matching (SPM) <ref type="bibr" coords="6,280.13,632.21,10.52,8.74" target="#b4">[5]</ref> which divides an image into grids and build a histogram for each grid. This process can be also considered as weighting local features: for certain grid, the weights of the local features within it is set to 1 and the weights of other local features are set to 0. Although less flexible than context-based weights, the binary weights in SPM are more stable which is also favorable. Thus, we also train classifiers for BoW histograms of SPM channels. In our method, a three level pyramid, 1 × 1, 2 × 2, 3 × 1 (totally 8 channels) is used. It is worthwhile to point out that, different from traditional SPM, we learn a specific vocabulary for each SPM grid based on local features within this grid.</p><p>Finally, we train a classifier for the semantic features and combine it with the classifiers for context-specific BoW histograms and SPM channels by averaging their outputs. For both BoW histograms and semantic features, classifiers are learned by SVM with chi-square kernel. The whole process is illustrated in Fig. <ref type="figure" coords="7,472.33,226.59,4.13,8.74">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Image Representation by Fisher Vectors</head><p>Similar to the BoW model, Fisher Vectors <ref type="bibr" coords="7,331.02,282.09,10.52,8.74" target="#b7">[8]</ref> can also be used to aggregate local features into a global descriptor which is called Fisher Vectors (FV). FV can be considered as an extension of BoW histograms. They encodes how the parameters of the model should be changed to represent the image, rather than only consider the number of occurrences of each visual word as in BoW model. In our participation, we adopted the improved FV as introduced in <ref type="bibr" coords="7,431.54,341.87,10.52,8.74" target="#b8">[9]</ref> which is shown to outperform BoW histogram on some large-scale image retrieval tasks.</p><p>For an image, we computed the FV for each kind of local features except Canny for which no actual visual word exist. As in <ref type="bibr" coords="7,366.15,377.74,9.96,8.74" target="#b8">[9]</ref>, a three level pyramid, 1 × 1, 2 × 2, 3 × 1 (8 channels in total) is used to enhance the performance of fisher vector.</p><p>For SIFT and HOG descriptors, PCA is used to reduce the dimension of descriptors to 64. For SIFT descriptors, a 64-centroid Gaussian mixture model (GMM) is computed to construct fisher vector whose dimensionality is therefore 64 × 64 × 8 × 2 = 65, 536. For HOG, Texton, SSIM and LAB descriptors, 64-centroid GMMs are learned therefore the dimensionalities of fisher vector for these descriptors are 16,384, 9,216, 7,680 and 768 respectively. Then we train a classifier (SVM with linear kernel) for each fisher vector and combine all classifiers by averaging their outputs. The whole process is illustrated in Fig. <ref type="figure" coords="7,442.12,497.29,4.13,8.74">3</ref>. Please note that the semantic contexts are not used for this representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ImageCLEF Evaluation</head><p>In our participation, we submitted four runs to the photo annotation task. In this section, we describe these runs and compare their performances with other visual-only runs. are used to describe images. As illustrated in Fig. <ref type="figure" coords="8,354.07,376.03,4.13,8.74">2</ref>, we trained separated classifiers (SVMs with chi-square kernel) for both context-specific BoW histograms and semantic features and then combine them by averaging their outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Description of Our Runs</head><p>Run 2: BoW+FisherKernel In this run, we combined all the classifiers in run 1 and classifiers for fisher vectors of different features (refer to Fig. <ref type="figure" coords="8,447.53,429.83,4.09,8.74">3</ref>). The combination is performed by averaging the outputs of all classifiers.</p><p>Run 3: SVMOutput In this run, the confidences of all 99 concepts obtained from run 2 are used as a new image descriptor. A classifier (SVM with chi-square kernel) is learned on this descriptor and used to give the confidences of concepts. By doing so, we hope to benefit from the correlation of different concepts.</p><p>Run 4: BoW+FisherKernel+SVMOuput In this run, we averaged the confidences obtained from run 1, 2 and 3.</p><p>In our participation, we used the implementation of LIBSVM <ref type="bibr" coords="8,408.69,543.40,10.52,8.74" target="#b1">[2]</ref> to learn SVM classifier. The value of the SVM parameter C and the normalization factor γ of chi-square kernel are determined by fivefold cross-validation. As to the image regions used for learning local context classifiers and generating saliency maps, on each image we sampled 100 regions with random positions and scales (with scales from 20% to 40% of the image size).</p><p>For concept level evaluation, the classifier outputs are used as confidences directly. For image level evaluation, the real valued confidences are binarized by a threshold which is determined by fivefold cross-validation for each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results of Our Runs</head><p>The performances (MAP, F-ex and SR-Precision) of our runs are listed in Table <ref type="table" coords="9,134.77,149.90,3.87,8.74" target="#tab_1">1</ref>. It can be concluded that the performance of context-specific BoW histograms is significantly enhanced by combining them with fisher vectors. It is worthwhile to point out that, according to our experiments on training data, the performance of fisher vectors alone is comparable to that of context-specific BoW histograms. Another conclusion drawn from Table <ref type="table" coords="9,305.45,197.72,4.98,8.74" target="#tab_1">1</ref> is that using classifier outputs as new features does not bring any improvement. Thus we need to design more powerful methods to utilize the correlation of concepts. For more detailed result, Fig. <ref type="figure" coords="9,276.99,376.64,4.40,8.74" target="#fig_3">4</ref> gives the MAPs of 99 concepts obtained from Run 2. For some concepts, the MAPs are very low, e.g. less than 10%. The reason is either that the concept is hard to predict (e.g. abstract) or that the number of training samples is quite small (e.g. only 12 images are annotated with skateboard).  Finally, we compare our best run (Run 2: BoW+FisherKernel) with the best runs (visual-only) of several competitors. It can be seen from Table <ref type="table" coords="9,424.02,644.16,4.98,8.74" target="#tab_3">2</ref> that no run gave the best result for both concept and image level evaluation. For concept level evaluation (MAP as performance measure), TUBFI scores performed best, while for image level evaluation (F-ex and SR-Precision as performance measures), ISIS runpa-UvA-coreA performed best. Our best run ranks in the second place for both MAP and F-ex and the third place for SR-Precision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In our participation to the ImageCLEF photo annotation task, multiple visual features were used for representing the images. We embedded contextual information into the traditional Bag-of-Words model and further combined it with fisher vector which has been shown to have good performance on image classification and retrieval tasks. The evaluation results showed that the performance of the Bag-of-Words model can be significantly enhanced by combining it with semantic contexts and fisher vector. Our best run gave 38.2%, 59.2% and 72.5% for MAP and F-ex and SR-Precision respectively, while the best results of visualonly runs are 38.8%, 61.2% and 73.4% respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,245.08,202.88,235.51,8.74;5,134.77,214.84,57.94,9.65;5,149.71,226.79,330.88,9.65;5,134.77,238.75,345.82,9.65;5,134.77,250.70,345.83,9.65;5,134.77,262.66,345.83,8.74;5,134.77,274.61,345.83,8.74;5,134.77,286.57,345.83,8.74;5,134.77,298.52,161.28,8.74;5,206.60,319.28,196.58,9.65"><head></head><label></label><figDesc>by averaging the outputs of the context classifier (for c k ) on I p (f i ).As to p(c k |I), it can be easily computed by averaging the outputs of the context classifiers (for c k ) on all image regions in I p . This process is similar to the computation of p(c k |I p (f i )) in previous subsection. In addition, we also represent image I by the occurrence probabilities of global contexts. These probabilities are computed by running the corresponding context classifiers on the whole image. Finally, an image is represented by concatenating the occurrence probabilities of both global and local contexts, i.e., (p(c 1 |I), . . . , p(c C |I), p(c C+1 |I), . . . , p(c C |I)),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,134.77,265.31,345.83,7.89;8,134.77,276.29,345.83,7.86;8,134.77,287.25,345.82,7.86;8,134.77,298.21,134.49,7.86"><head>Run 1 :Fig. 3 .</head><label>13</label><figDesc>Fig. 3. Image representation and classification based on fisher vectors of multiple types of local features. The values in brackets are the dimensionalities of corresponding Fisher Vectors. After training a classifier for each Fisher Vectors, multiple classifiers are combined by averaging their outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,187.05,599.46,238.18,7.89"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The MAPs of all 99 concepts obtained from Run 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,175.78,211.01,267.01,263.24"><head>global scene (35) green + shape (7) triangle material (14) metal color (8) face motorbike object (30)</head><label></label><figDesc></figDesc><table coords="3,176.72,211.01,266.07,263.24"><row><cell></cell><cell>train station</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>lab zoo gym jail store casino church harbor</cell></row><row><cell></cell><cell></cell><cell></cell><cell>kitchen highway cemetery bathroom</cell></row><row><cell></cell><cell></cell><cell></cell><cell>classroom industrial restaurant courtroom</cell></row><row><cell></cell><cell>bedroom</cell><cell>+</cell><cell>supermarket library tunnel suburb theater laundry office tennis_court</cell></row><row><cell></cell><cell></cell><cell></cell><cell>dining_room swimming_pool hospital_room</cell></row><row><cell></cell><cell></cell><cell></cell><cell>shopping_mall living_room conference_room</cell></row><row><cell></cell><cell></cell><cell></cell><cell>parking_lot indoor/outdoor city/landscape</cell></row><row><cell>local</cell><cell>sky</cell><cell></cell></row><row><cell>scene</cell><cell></cell><cell>+</cell><cell>building coast desert forest grass lake mountain ocean river road snow soil street wall tree</cell></row><row><cell>(16)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>red black blue gray orange white yellow</cell></row><row><cell></cell><cell></cell><cell cols="2">+ box cylinder circle cone oval pyramid</cell></row><row><cell></cell><cell></cell><cell>+</cell><cell>ceramics cloth feather glass hairless leather paper plastic rubber stone water wood fur</cell></row><row><cell></cell><cell></cell><cell></cell><cell>animal_flipper animal_head animal_wing</cell></row><row><cell></cell><cell></cell><cell></cell><cell>door window hand hooves screen wheel</cell></row><row><cell></cell><cell></cell><cell>+</cell><cell>airplane bicycle bird boat bottle bus car cat chair cow diningtable dog horse person</cell></row><row><cell></cell><cell></cell><cell></cell><cell>pottedplant sheep sofa train tv/monitor</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,134.77,254.51,345.83,86.76"><head>Table 1 .</head><label>1</label><figDesc>MAP, F-ex and SR-Precision of our runs. For each measure, values in bold indicate the best performance of 4 runs.</figDesc><table coords="9,158.07,254.51,296.14,52.10"><row><cell>Runs</cell><cell>MAP</cell><cell>F-ex</cell><cell>SR-Precision</cell></row><row><cell>MultiFeat Chi2SVM</cell><cell>34.2</cell><cell>56.0</cell><cell>69.4</cell></row><row><cell>BoW+FisherKernel</cell><cell>38.2</cell><cell>60.0</cell><cell>72.7</cell></row><row><cell>SVMOutput</cell><cell>34.5</cell><cell>49.1</cell><cell>65.1</cell></row><row><cell>BoW+FisherKernel+SVMOuput</cell><cell>38.2</cell><cell>59.2</cell><cell>72.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,134.77,187.91,345.83,109.08"><head>Table 2 .</head><label>2</label><figDesc>MAP, F-ex and SR-Precision of our runs. For each measure, values in bold indicate the best performance of 4 runs. Our best run ranks in the second place for both MAP and F-ex and the third place for SR-Precision.</figDesc><table coords="10,173.92,187.91,264.44,63.46"><row><cell>Runs</cell><cell>MAP</cell><cell>F-ex</cell><cell>SR-Precision</cell></row><row><cell>BPACAD bpacad avg cns</cell><cell>36.7</cell><cell>56.8</cell><cell>72.9</cell></row><row><cell>ISIS runpa-UvA-coreA</cell><cell>37.5</cell><cell>61.2</cell><cell>73.4</cell></row><row><cell>LIRIS 4visual model 4</cell><cell>35.5</cell><cell>53.9</cell><cell>72.5</cell></row><row><cell>TUBFI scores</cell><cell>38.8</cell><cell>55.2</cell><cell>62.1</cell></row><row><cell>Our best run</cell><cell>38.2</cell><cell>59.2</cell><cell>72.5</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was partly realized under the <rs type="programName">Quaero Programme</rs>, funded by <rs type="funder">OSEO</rs>, <rs type="funder">French State agency</rs> for innovation.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TeufRNd">
					<orgName type="program" subtype="full">Quaero Programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,591.35,337.63,7.86;10,151.52,602.31,258.55,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,198.29,591.35,185.50,7.86">A computational approach to edge detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,392.24,591.35,88.35,7.86;10,151.52,602.31,177.14,7.86">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct coords="10,142.96,613.12,337.64,7.86;10,151.52,624.08,329.07,7.86;10,151.52,635.04,213.34,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,252.32,613.12,198.13,7.86">LIBSVM: A library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm" />
	</analytic>
	<monogr>
		<title level="j" coord="10,458.84,613.12,21.75,7.86;10,151.52,624.08,207.83,7.86">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,645.84,337.63,7.86;10,151.52,656.80,55.29,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,242.26,645.84,219.47,7.86">Histograms of oriented gradients for human detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,656.80,26.62,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,119.67,325.22,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,200.31,119.67,141.36,7.86">Photoelectric color difference meter</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,349.02,119.67,23.92,7.86">JOSA</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="985" to="993" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,130.63,337.64,7.86;11,151.52,141.59,273.33,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,307.33,130.63,173.27,7.86;11,151.52,141.59,197.07,7.86">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,369.56,141.59,26.62,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,152.55,337.64,7.86;11,151.52,163.51,192.34,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,193.30,152.55,227.75,7.86">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,428.34,152.55,52.25,7.86;11,151.52,163.51,106.33,7.86">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,174.47,337.63,7.86;11,151.52,185.43,221.58,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,298.91,174.47,181.67,7.86;11,151.52,185.43,80.87,7.86">The clef 2011 photo annotation and conceptbased retrieval tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liebetrau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,239.28,185.43,105.15,7.86">CLEF 2011 working notes</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,196.39,337.63,7.86;11,151.52,207.34,105.73,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,258.05,196.39,222.53,7.86;11,151.52,207.34,29.60,7.86">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,201.97,207.34,26.62,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,218.30,337.63,7.86;11,151.52,229.26,106.08,7.86" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<title level="m" coord="11,314.38,218.30,166.21,7.86;11,151.52,229.26,77.41,7.86">Improving the fisher kernel for large-scale image classification</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,240.22,337.98,7.86;11,151.52,251.18,69.37,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,255.12,240.22,221.55,7.86">Matching local self-similarities across images and videos</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,165.60,251.18,26.61,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,262.14,337.98,7.86;11,151.52,273.10,25.60,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,227.90,262.14,205.29,7.86">Visual word disambiguation by semantic contexts</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,457.04,262.14,23.55,7.86">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,284.06,337.97,7.86;11,151.52,295.02,308.05,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,266.23,284.06,214.36,7.86;11,151.52,295.02,51.15,7.86">A statistical approach to texture classification from single images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,209.99,295.02,168.17,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="81" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,305.98,337.97,7.86;11,151.52,316.93,299.79,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,237.95,305.98,242.64,7.86;11,151.52,316.93,32.07,7.86">Semantic modeling of natural scenes for content-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,190.22,316.93,170.48,7.86">International Journal on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="157" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
