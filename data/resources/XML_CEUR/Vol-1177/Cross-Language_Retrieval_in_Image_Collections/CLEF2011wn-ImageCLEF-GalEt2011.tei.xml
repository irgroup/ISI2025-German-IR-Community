<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.40,115.96,312.55,12.62;1,254.32,133.89,106.72,12.62">Multi-disciplinary modality classification for medical images</title>
				<funder>
					<orgName type="full">Marie Curie Initial Training Networks</orgName>
					<orgName type="abbreviated">ITN</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,180.06,171.56,47.14,8.74"><forename type="first">Viktor</forename><surname>Gál</surname></persName>
							<email>viktor.gal@ugent.be</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Applied Mathematics and Computer Science</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.09,171.56,37.98,8.74"><forename type="first">Illés</forename><surname>Solt</surname></persName>
							<email>solt@tmit.bme.hu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Telecommunications and Media Informatics</orgName>
								<orgName type="institution">Budapest University of Technology and Economics</orgName>
								<address>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.62,171.56,55.69,8.74"><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
							<email>tom.gedeon@anu.edu.au</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.87,171.56,71.95,8.74"><forename type="first">Mike</forename><surname>Nachtegael</surname></persName>
							<email>mike.nachtegael@ugent.be</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Applied Mathematics and Computer Science</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.40,115.96,312.55,12.62;1,254.32,133.89,106.72,12.62">Multi-disciplinary modality classification for medical images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0A4A75B702A6BF816E08A91A4D7F4DFF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image classification</term>
					<term>image feature extraction</term>
					<term>image modality</term>
					<term>text mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modality is a key facet in medical image retrieval, as a user is likely interested in only one of e.g. radiology images, flowcharts, and pathology photos. While assessing image modality is trivial for humans, reliable automatic methods are required to deal with large un-annotated image bases, such as figures taken from the millions of scientific publications. We present a multi-disciplinary approach to tackle the classification problem by combining image features, meta-data, textual and referential information. Our system achieved an accuracy of 96.86 % in cross-validation on the ImageCLEF 2011 training dataset having 18 imbalanced modality classes, and an accuracy of 90.2 % on the Image-CLEF 2010 dataset having 8 well-balanced modality classes. We evaluate the importance of the individual feature sets in detail, and provide an error analysis pointing at weaknesses of our method and obstacles in the classification task. For the benefit of the image classification community, we make the results of our feature extraction methods publicly available at http://categorizer.tmit.bme.hu/ ~illes/imageclef2011modality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Imaging modality is an important aspect of the image for medical retrieval <ref type="bibr" coords="1,455.62,572.43,9.96,8.74" target="#b5">[6]</ref>. In user-studies, clinicians have indicated that modality is one of the most important filters that they would like to be able to limit their search by. However, this modality is typically extracted from the caption and is often not correct or present. Studies have shown that the modality can be extracted from the image itself using visual features <ref type="bibr" coords="1,249.65,632.21,15.50,8.74" target="#b12">[13,</ref><ref type="bibr" coords="1,266.81,632.21,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="1,281.20,632.21,7.01,8.74" target="#b6">7]</ref>. Therefore, In this paper, we propose to use both visual and textual features for medical image representation, and combine the different features using normalised kernel function in SVM.</p><p>The proposed algorithm is evaluated in the context of the ImageCLEF 2011 Modality Classification task <ref type="bibr" coords="2,254.22,130.95,11.75,8.74" target="#b8">[9]</ref>, which uses a dataset of 988+1024 images taken from PubMed articles.</p><p>The rest of this paper is organised as follows. In Section 1, we describe in detail our experimental setting. In Section 3, we present and compare different runs we submitted. We discuss the submitted runs and the results in Section 4 and we conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this section, we describe in detail our experimental setting. The ImageCLEF 2011 Modality Classification task used split-validation measuring the accuracy of the systems. On the training dataset, we performed stratified 10-fold cross-validation to evaluate feature sets and classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Evaluation setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature extraction</head><p>Caption text Figures in scientific publications often have descriptive captions that provide information on the modality of the image. "Contrast-enhanced axial computed tomographic scan", "HRCT showing extensive areas of consolidation with air bronchogram" are examples of captions of images assigned to the 'CT' modality class. However, the caption may be missing or may not hint at the modality, e.g. "E. coli that satisfy the similarity threshold values." As the examples suggest, the linguistic constructs expressing modality can have a high variation. Considering these remarks, we extract binary features from caption texts as follows. We define a set of regular expressions to be matched against the caption text, a match results in a value of 1. Regular expressions were initially created for each word having a high information gain for any of the modality classes and were later manually refined to capture linguistic variations (e.g. f?MRI?) and multi-word phrases (e.g. error bars?).</p><p>MeSH terms Scientific articles indexed by Medline/PubMed are tagged with MeSH terms (medical subject headings) by field experts. MeSH terms can be seen as a thesaurus for the life sciences containing entries like 'Human', 'Liver Neoplasms' and 'Magnetic Resonance Imaging', entries can be further qualified by e.g. 'methods', 'pathology'. We hypothesise that the article's MeSH terms and its figures' modality are correlated, and hence define features corresponding to individual MeSH terms and qualifiers. A unique identifier for the article (e.g. PMID or DOI) is required to retrieve its MeSH annotations, however, such identifiers can be absent. As the number of MeSH terms, qualifiers and their combinations far exceeds the number of modality labels, we perform feature selection by keeping only those that are present for at least a predefined number of articles in the training set.</p><p>Colour histogram Using colour histograms in content-based image retrieval system has been successfully applied in the past, for a detailed review see <ref type="bibr" coords="3,433.99,475.47,14.61,8.74" target="#b15">[16]</ref>. Based on these studies we have chosen to use HSV colour-space based histogram, and quantised the hue and the saturation to three and the value to four levels.</p><p>Based on this we defined f hist feature vector, where each element of the vector represents the normalised number of pixels in a given histogram bin.</p><p>Mean of pixels Through manually supervised error analysis on the training set, we identified that the images in Graphic 1st-level group are mainly having a white background. Hence, we have defined a simple feature f mean = I j , that represents the mean value of the pixels in an image. By simply thresholding these values one could identify the images that belong to the Graphic group with a very high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Axis recognition</head><p>The previously mentioned mean of pixels method gave a strong support for recognising images in the Graphic top-level group, but as it consists of two sub-groups, Graphs and Drawing, thus a new feature was required to differentiate the images belonging to one or the other category. By manually observing the images in these two categories one can easily point out the main difference by using a simple edge detector: the images belonging to the Graphs category are mainly consisting of horizontal and vertical lines (i.e. the x-y axis of a graph), whereas the images in Drawing category are mostly diagrams, where the orientation of the lines is random.</p><p>Based on this idea we have defined the following feature. Let L Ij be the set of all the detected lines and GL Ij be the number of good lines in an arbitrary image I j , where a given line is a good line if it's orientation is horizontal or vertical and it is within a given margin of the picture's border. The latter condition is for not to count the borders of an image as good lines.</p><p>Using these two sets we defined a feature</p><formula xml:id="formula_0" coords="4,264.92,271.92,215.67,23.83">f lines (I j ) = |GL Ij | |L Ij |<label>(1)</label></formula><p>In order to detect the lines and their orientation in an image we used a simple Hough transform <ref type="bibr" coords="4,212.36,320.19,9.96,8.74" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skin detection</head><p>The images in the Dermatology category was one of the most difficult recognise. As not only it was the least represented category in the whole training set, i.e. there are only seven examples (see Table <ref type="table" coords="4,393.30,369.33,4.43,8.74" target="#tab_0">1</ref>) for this category, but the images in this set are simple photographs (of various skin abnormalities) thus they have very similar characteristics to the general photo labeled images. Hence, most of the previously defined features failed to distinguish the images in Dermatology set from the others.</p><p>Using a simple skin detector algorithm <ref type="bibr" coords="4,315.03,429.37,13.15,8.74" target="#b1">[2]</ref> we defined a new feature f skin (I j ) for and image</p><formula xml:id="formula_1" coords="4,198.17,441.29,282.42,23.79">I j f skin (I j ) = SD(I j )<label>(2)</label></formula><p>where the function SD(•) calculates the skin-segmented binary image of an input image, and I k -as previously defined-is the mean value of image I k .</p><p>Meta-data We determine whether an image post-processing software was used by analysing meta-data stored in JPEG files' EXIF section. For this, we analyze the 'Comment' field, to find mentions of commonly used image manipulation software (e.g. Adobe Photoshop, MS Paint). We also extract from the EXIF whether the image is stored as gray-scale only.</p><p>Radiopaedia Radiopaedia (http://radiopaedia.org) is a community wiki for radiology images and patient cases. Images are tagged by users with the body system (e.g. Heart, Musculoskeletal) depicted, but unfortunately for us, not with the type of radiology method used to create the image. Leveraging the mutual information between body systems and radiology methods, we derived features for modality classification by taking the output probabilities of a classifier trained to predict body systems shown in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bag of visual-words</head><p>The state-of-the-art content based image retrieval systems has been significantly improved by the introduction of SIFT <ref type="bibr" coords="5,389.09,130.95,19.16,8.74" target="#b10">[11]</ref> features and the bag-of-words image representation <ref type="bibr" coords="5,287.44,142.90,15.50,8.74" target="#b11">[12,</ref><ref type="bibr" coords="5,304.60,142.90,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="5,314.01,142.90,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="5,323.41,142.90,11.62,8.74" target="#b13">14]</ref>.</p><p>The bag-of-visual-words image representation is based on the bag of words (BoW) model in natural language processing (NLP). BoW in NLP is a popular method for representing documents In this model a document is simply represented by the number of different words that are in the document. The idea behind this is, that documents on the same topic have similar words with similar number of occurrences in them (see LDA <ref type="bibr" coords="5,307.30,217.20,14.32,8.74" target="#b0">[1]</ref>).</p><p>In case of and image, the basic idea of bag-of-words model is that a set of local image patches is sampled using some method-e.g. densely or using a key-point detector-and a vector of visual descriptors is evaluated on each patch independently. In this paper we used the well known SIFT descriptor on each patch. The SIFT descriptor computes a gradient orientation histogram within the support region. For each of eight orientation planes, the gradient image is sampled over a four y four grid of locations, hence resulting in a 128-dimensional feature vector for each region. In order to make the descriptor less sensitive to small changes in the position of the support region and put more emphasis on the gradients that are near the centre of the region a Gaussian window function is used to assign a weight to the magnitude of each sample point.</p><p>After acquiring these SIFT features for all the images in the dataset, the final step is to convert vector represented patches to "codewords" (analogy to words in text documents), which also produces a "codebook" (analogy to a word dictionary). A codeword can be considered as a representative of several similar patches. In our case we performed k-means clustering over all the vectors. Codewords are then defined as the centres of the learned clusters. Thus, each patch in an image is mapped to a certain codeword through the clustering process and the image can be represented by the histogram of the codewords.</p><p>In our bag-of-visual-words model we used the the tf-idf weighting <ref type="bibr" coords="5,427.03,463.98,17.54,8.74" target="#b14">[15]</ref> scheme, that has proven to be a very successful approach for image retrieval. The tf part of the weighting scheme represents the number of features described by a given visual word. The frequency of visual word in the image provides useful information about repeated structures and textures. While, the idf part captures the informativeness of visual words-visual words that appear in many different images are less informative than those that appear rarely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other systems</head><p>The challenge organisers generously supplied participants with predictions of their in-house system. This classification was automatic for the test set, but confusingly enough, the ground truth labels were used for the train set. In order to exploit this valuable resource, we used it as an input to our classifier by introducing artificial smoothing to avoid overfitting on this particular otherwise noise free indicator variable. Also note that while split evaluation is sound in this setting, the cross-validation evaluation of those two runs is flawed (being over-optimistic) due to information leakage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classification</head><p>Based on the numerical and binary features of the images obtained through feature extraction, we perform vector space classification to predict modality classes of unseen images. Among the classification algorithms available in Weka <ref type="bibr" coords="6,452.67,162.30,9.96,8.74" target="#b4">[5]</ref>, we found the support vector machine SMO to have the best standalone performance over the full feature space in cross-validation on ImageCLEF 2011 training dataset. We used SMO with default settings for the rest of the experiments unless stated otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>In this section, we provide the final results of the five submitted runs for the modality classification tasks. Table <ref type="table" coords="6,288.00,276.82,4.98,8.74" target="#tab_2">2</ref> shows both the correctly classified percentage for the different features set compositions. Comparing the result of our best submitted run and the best submitted run of the modality classification task, one can see that there is very small (0.88%) difference between the two runs.</p><p>The performance of the runs broken down for the individual classes is show in Table <ref type="table" coords="6,173.78,336.59,4.98,8.74" target="#tab_3">3</ref> and in Figure <ref type="figure" coords="6,244.52,336.59,3.87,8.74" target="#fig_0">1</ref>.</p><p>Table <ref type="table" coords="6,163.32,379.28,3.87,8.74" target="#tab_2">2</ref>: Results of the runs for the medical modality classification task. For the reference we have included the best performing run of the competition. The figures in parenthesis are the result of information leakage that only appears in the cross-validation setting, see Section 2.2 for details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>As can be seen on Figure <ref type="figure" coords="6,245.22,584.39,3.87,8.74" target="#fig_0">1</ref>, the systems performs well on higher support classes, while performance drops to zero for some more rare classes. This behaviour is tolerated by the challenge main evaluation metric accuracy, in contrast to a more pessimistic evaluation like F-measure.   <ref type="table" coords="7,178.26,645.79,3.87,8.74" target="#tab_3">3</ref>.</p><p>Using MeSH and Radiopaedia features gained us about one percent in accuracy.</p><p>The in-house modality classifier of the challenge organisers proved to be superior in predicting the 'Dermatology' class (Table <ref type="table" coords="8,361.35,154.86,3.87,8.74" target="#tab_3">3</ref>, however, its inferior performance on higher support classes prevented it from being benefitial in combination (Table <ref type="table" coords="8,197.03,178.77,3.87,8.74" target="#tab_2">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Other experiments</head><p>Motivated by the grouping of modality labels by the challenge organisers, we experimented with hierarchical classification. In particular, we applied a hierarchical greedily ascending classifier scheme wrapping the baseline classifier. In this scheme, classification is first performed on the hierarchies uppermost level (here groups), then the most probable hierarchy node is selected where classification continues recursively. For hierarchical classification, cross-validation results were inferior to those obtained from the baseline (flat) classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed to extract different visual and textual features for medical image representation, and fusion the different extracted visual feature and textual feature for modality classification. To extract visual features from the images, we used some state-of-art methods like bag-of-visual words and some standard ones like colour histogram and introduced some heuristic representations of the images specialised for the ImageCLEF2011 medical modality classification task.</p><p>With the suggested feature extraction algorithms in this paper and the SVM classifier we have achieved to 2nd place on the ImageCLEF2011 medical image modality classification task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,134.77,621.88,345.83,8.74;7,134.77,633.84,345.82,8.74;7,134.77,645.79,51.25,8.74"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Modality class distribution and best run performance. Modality classes are sorted by support in descending order. For the names of modality classes, see Table 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,157.48,320.46,300.41,282.56"><head>Table 1 :</head><label>1</label><figDesc>Modality labels at ImageCLEF 2011 and their distribution</figDesc><table coords="2,157.48,341.96,300.41,261.06"><row><cell></cell><cell></cell><cell>Modality label</cell><cell cols="2">Training</cell></row><row><cell>Group</cell><cell>Code</cell><cell>Description</cell><cell>#</cell><cell>%</cell></row><row><cell>Radiology</cell><cell>AN</cell><cell>angiography</cell><cell>11</cell><cell>1.1</cell></row><row><cell></cell><cell>CT</cell><cell>computed tomography</cell><cell>70</cell><cell>7.1</cell></row><row><cell></cell><cell>MR</cell><cell>magnetic resonance imaging</cell><cell>17</cell><cell>1.7</cell></row><row><cell></cell><cell>US</cell><cell>ultrasound</cell><cell>30</cell><cell>3.0</cell></row><row><cell></cell><cell>XR</cell><cell>X-ray</cell><cell>59</cell><cell>6.0</cell></row><row><cell>Microscopy</cell><cell>FL</cell><cell>fluorescence</cell><cell>44</cell><cell>4.5</cell></row><row><cell></cell><cell>EM</cell><cell>electronmicroscopy</cell><cell>16</cell><cell>1.6</cell></row><row><cell></cell><cell>GL</cell><cell>gel</cell><cell>50</cell><cell>5.1</cell></row><row><cell></cell><cell>HX</cell><cell>histopathology</cell><cell>208</cell><cell>21.1</cell></row><row><cell>Photograph</cell><cell>PX</cell><cell>general photo</cell><cell>165</cell><cell>16.7</cell></row><row><cell></cell><cell>GR</cell><cell>gross pathology</cell><cell>43</cell><cell>4.4</cell></row><row><cell></cell><cell>EN</cell><cell>endoscopic imaging</cell><cell>10</cell><cell>1.0</cell></row><row><cell></cell><cell>RN</cell><cell>retinograph</cell><cell>5</cell><cell>0.5</cell></row><row><cell></cell><cell>DM</cell><cell>dermatology</cell><cell>7</cell><cell>0.7</cell></row><row><cell>Graphic</cell><cell>GX</cell><cell>graphs</cell><cell>161</cell><cell>16.3</cell></row><row><cell></cell><cell>DR</cell><cell>drawing</cell><cell>43</cell><cell>4.4</cell></row><row><cell>Other</cell><cell>3D</cell><cell>3D reconstruction</cell><cell>32</cell><cell>3.2</cell></row><row><cell></cell><cell>CM</cell><cell>compound figure (&gt; 1 type of image)</cell><cell>17</cell><cell>1.7</cell></row><row><cell>Total</cell><cell></cell><cell>18</cell><cell cols="2">988 100.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,620.25,345.83,44.60"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="6,363.50,620.25,117.10,8.74"><row><cell>shows, which features have</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,134.77,134.10,345.82,475.86"><head>Table 3 :</head><label>3</label><figDesc>Correctly classified images per category for the submitted runs. For each modality class, the result of the best performing run is typeset in bold.</figDesc><table coords="7,162.89,167.56,289.57,442.40"><row><cell></cell><cell></cell><cell cols="2">Ratio (%)</cell><cell>Run</cell></row><row><cell></cell><cell>Modality class</cell><cell cols="2">train test #1 #2 #3 #4 #5</cell></row><row><cell cols="2">3D : 3D render</cell><cell cols="2">3.2 4.4 66.7 71.1 73.3 66.7 57.8</cell></row><row><cell cols="2">AN: Angiography</cell><cell cols="2">1.1 0.9 88.9 77.8 77.8 66.7 88.8</cell></row><row><cell cols="2">CM: Compound figure</cell><cell cols="2">1.7 2.0</cell><cell>0.0 5.0 5.0 5.0 5.0</cell></row><row><cell cols="4">CT : Computed tomography 7.1 8.1 98.8 97.6 95.2 91.6 89.2</cell></row><row><cell cols="2">DM: Dermatology</cell><cell cols="2">0.7 1.5</cell><cell>0.0 0.0 0.0 6.7 13.3</cell></row><row><cell cols="2">DR: Drawing</cell><cell cols="2">4.4 7.2 68.9 66.2 70.3 27.0 24.3</cell></row><row><cell cols="2">EM: Electronmicroscope</cell><cell cols="2">1.6 1.8 55.6 55.6 55.6 55.6 55.6</cell></row><row><cell cols="2">EN : Endoscope</cell><cell cols="2">1.0 1.1 36.4 36.4 27.3 36.4 27.3</cell></row><row><cell cols="2">FL : Fluorescence</cell><cell cols="2">4.5 2.7 96.4 96.4 100 100 100</cell></row><row><cell cols="2">GL : Gel</cell><cell cols="2">5.1 4.9 98.0 98.0 100 82.0 80.0</cell></row><row><cell cols="2">GR: Gross pathology</cell><cell cols="2">4.4 3.1 46.9 40.6 34.4 34.4 34.4</cell></row><row><cell cols="2">GX: Graphics</cell><cell cols="2">16.3 16.8 97.1 96.5 94.8 97.1 96.5</cell></row><row><cell cols="2">HX: Histopathology</cell><cell cols="2">21.1 19.0 99.0 99.0 99.0 95.4 95.9</cell></row><row><cell cols="2">MR: MRI</cell><cell cols="2">1.7 2.0 65.0 70.0 75.0 60.0 50.0</cell></row><row><cell cols="2">PX : Photo</cell><cell cols="2">16.7 13.8 91.5 90.1 88.7 73.8 66.7</cell></row><row><cell cols="2">RN: Retiongraph</cell><cell cols="2">0.5 0.3 66.7 66.7 66.7 0.0 33.3</cell></row><row><cell cols="2">US : Ultrasound</cell><cell cols="2">3.0 4.0 95.1 95.1 90.2 85.4 78.0</cell></row><row><cell cols="2">XR: X-ray</cell><cell cols="2">6.0 6.5 92.5 94.0 94.0 82.1 71.6 Sheet2</cell></row><row><cell></cell><cell cols="3">Modality class distribution and system performance</cell></row><row><cell></cell><cell>100%</cell><cell></cell></row><row><cell></cell><cell>90%</cell><cell></cell></row><row><cell></cell><cell>80%</cell><cell></cell></row><row><cell></cell><cell>70%</cell><cell></cell></row><row><cell>Ratio / Accuracy</cell><cell>30% 40% 50% 60%</cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell></cell></row><row><cell></cell><cell cols="3">HX PX GX CT XR GL FL DR GR 3D US CM MR EM AN EN DM RN</cell></row><row><cell></cell><cell></cell><cell cols="2">Modality class</cell></row><row><cell></cell><cell cols="2">Train % Test %</cell><cell>Run #1 accuracy</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p><rs type="person">Viktor Gál</rs> was supported by <rs type="funder">Marie Curie Initial Training Networks (ITN)</rs> Ref.</p></div>
<div><head n="238819">(FP7-PEOPLE-ITN-2008).</head></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,580.48,337.64,7.86;8,151.52,591.44,189.94,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,373.29,580.48,103.51,7.86">Latent dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,151.52,591.44,83.60,7.86">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,602.20,337.64,7.86;8,151.52,613.16,329.07,7.86;8,151.52,624.12,77.82,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,256.08,602.20,224.51,7.86;8,151.52,613.16,46.83,7.86">Face segmentation using skin-color map in videophone applications</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K N</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,207.14,613.16,172.49,7.86">Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="551" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,634.88,337.63,7.86;8,151.52,645.84,329.07,7.86;8,151.52,656.80,320.35,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,337.39,634.88,143.20,7.86;8,151.52,645.84,242.52,7.86">Total Recall: Automatic query expansion with a generative feature model for object retrieval</title>
		<author>
			<persName coords=""><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,414.94,645.84,65.66,7.86;8,151.52,656.80,184.22,7.86">2007 IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007-10">October 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,119.67,337.64,7.86;9,151.52,130.63,144.75,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,196.08,119.67,280.69,7.86">Use of the Hough transformation to detect lines and curves in pictures</title>
		<author>
			<persName coords=""><surname>Duda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,151.52,130.63,114.68,7.86">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,141.59,337.63,7.86;9,151.52,152.55,329.07,7.86;9,151.52,163.51,128.74,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,241.56,152.55,189.60,7.86">The WEKA data mining software: an update</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,443.77,152.55,36.82,7.86;9,151.52,163.51,48.42,7.86">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,174.47,337.64,7.86;9,151.52,185.43,329.07,7.86;9,151.52,196.39,329.07,7.86;9,151.52,207.34,82.42,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,226.66,185.43,253.93,7.86;9,151.52,196.39,92.40,7.86">Advancing Biomedical Image Retrieval: Development and Analysis of a Test Collection</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>William R Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianji</forename><surname>Jeffery R Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,250.73,196.39,225.80,7.86">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="488" to="496" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,218.30,337.63,7.86;9,151.52,229.26,77.10,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,186.88,218.30,151.43,7.86">Image retrieval using color and shape</title>
		<author>
			<persName coords=""><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,347.90,218.30,78.55,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1233" to="1244" />
			<date type="published" when="1996-08">August 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,240.22,337.64,7.86;9,151.52,251.18,329.07,7.86;9,151.52,262.14,234.80,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,323.10,240.22,157.49,7.86;9,151.52,251.18,138.18,7.86">A contextual dissimilarity measure for accurate and efficient image search</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Harzallah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,308.67,251.18,167.85,7.86;9,176.45,262.14,137.49,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR &apos;07)</note>
</biblStruct>

<biblStruct coords="9,142.96,273.10,337.64,7.86;9,151.52,284.06,329.07,7.86;9,151.52,295.02,329.07,7.86;9,151.52,305.98,75.08,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,353.45,284.06,127.15,7.86;9,151.52,295.02,127.80,7.86">The CLEF 2011 medical image retrieval and classification tasks</title>
		<author>
			<persName coords=""><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,299.83,295.02,104.81,7.86">CLEF 2011 working notes</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,316.93,337.98,7.86;9,151.52,327.89,329.07,7.86;9,151.52,338.85,329.07,7.86;9,151.52,349.81,314.38,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,299.20,316.93,181.39,7.86;9,151.52,327.89,194.62,7.86">A New Content-Based Image Retrieval Approach Based on Pattern Orientation Histogram</title>
		<author>
			<persName coords=""><forename type="first">Abolfazl</forename><surname>Lakdashti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Moin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,242.40,338.85,238.19,7.86;9,151.52,349.81,23.77,7.86">Computer Vision/Computer Graphics Collaboration Techniques</title>
		<editor>
			<persName><forename type="first">André</forename><surname>Gagalowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wilfried</forename><surname>Philips</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin / Heidelberg, Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="587" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,360.77,337.98,7.86;9,151.52,371.73,329.07,7.86;9,151.52,382.69,318.57,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,217.56,360.77,211.24,7.86">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,447.41,360.77,33.18,7.86;9,151.52,371.73,241.03,7.86;9,151.52,382.69,37.37,7.86">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1150</biblScope>
		</imprint>
	</monogr>
	<note>ICCV &apos;99</note>
</biblStruct>

<biblStruct coords="9,142.62,393.65,337.98,7.86;9,151.52,404.61,329.07,7.86;9,151.52,415.56,108.74,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,261.36,393.65,178.40,7.86">Scalable Recognition with a Vocabulary Tree</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,458.48,393.65,22.11,7.86;9,151.52,404.61,152.22,7.86;9,334.42,404.61,146.17,7.86;9,151.52,415.56,8.29,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="2161" to="2168" />
		</imprint>
	</monogr>
	<note>IEEE Computer Society Conference on</note>
</biblStruct>

<biblStruct coords="9,142.62,426.52,337.97,7.86;9,151.52,437.48,329.07,7.86;9,151.52,448.44,20.99,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,317.45,426.52,163.14,7.86;9,151.52,437.48,77.16,7.86">Photobook: Content-based manipulation of image databases</title>
		<author>
			<persName coords=""><forename type="first">R W</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Picard</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,240.82,437.48,172.92,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,459.40,337.98,7.86;9,151.52,470.36,329.07,7.86;9,151.52,481.32,308.98,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,395.92,459.40,84.68,7.86;9,151.52,470.36,265.13,7.86">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,434.71,470.36,45.88,7.86;9,151.52,481.32,280.88,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,492.28,337.98,7.86;9,151.52,503.24,329.07,7.86;9,151.52,514.19,284.03,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,298.17,492.28,182.43,7.86;9,151.52,503.24,108.53,7.86">Video Google: A Text Retrieval Approach to Object Matching in Videos</title>
		<author>
			<persName coords=""><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,281.29,503.24,199.31,7.86;9,151.52,514.19,80.99,7.86">9th IEEE International Conference on Computer Vision (ICCV 2003)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,525.15,337.98,7.86;9,151.52,536.11,127.78,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,213.67,525.15,201.81,7.86">A survey of content-based image retrieval systems</title>
		<author>
			<persName coords=""><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,424.61,525.15,55.99,7.86;9,151.52,536.11,100.16,7.86">Content-based image and video retrieval</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
