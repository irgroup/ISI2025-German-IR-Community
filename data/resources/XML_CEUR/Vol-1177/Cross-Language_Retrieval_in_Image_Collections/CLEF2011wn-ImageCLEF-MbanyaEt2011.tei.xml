<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.81,115.96,317.74,12.62;1,256.01,133.89,103.33,12.62">Sample Selection, Category Specific Features and Reasoning</title>
				<funder ref="#_yPH6KCm">
					<orgName type="full">German Federal Ministry of Economics and Technology</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,134.77,171.56,67.25,8.74"><forename type="first">Eugene</forename><surname>Mbanya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fraunhofer Institute for Telecommunications</orgName>
								<orgName type="institution">Heinrich Hertz Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.69,171.56,68.48,8.74"><forename type="first">Sebastian</forename><surname>Gerke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fraunhofer Institute for Telecommunications</orgName>
								<orgName type="institution">Heinrich Hertz Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.19,171.56,83.94,8.74"><forename type="first">Christian</forename><surname>Hentschel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fraunhofer Institute for Telecommunications</orgName>
								<orgName type="institution">Heinrich Hertz Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,398.25,171.56,82.34,8.74"><forename type="first">Patrick</forename><surname>Ndjiki-Nya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fraunhofer Institute for Telecommunications</orgName>
								<orgName type="institution">Heinrich Hertz Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.81,115.96,317.74,12.62;1,256.01,133.89,103.33,12.62">Sample Selection, Category Specific Features and Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BDE03B34D83F97828C28C2F3DBF5988E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present our approach to the 2011 ImageClef PhotoAnnotation task, which is based on the well known bag-of-words model. We investigated an approach for selecting the most informative training samples per concept for classification and the impact of fusing the OpponentSIFT feature with the GIST feature which calculates global image statistics, on scene-based concepts. We also incorporated a post-classification processing step, which refined classification results based on rules of inference and exclusion between concepts. The different approaches provided classification gains when compared to the standard bag-of-words model using only the OpponentSIFT feature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageClef Photo Annotation Task is an annual competition, which draws interest from the Computer Vision research community, with the aim of addressing the problem of efficient annotation of large scale image collections. The task achieves this by inviting competition from different research institutions to provide solutions for the automatic classification of photos taken from the Flickr 1 community into different categories.</p><p>The 2010 ImageClef PhotoAnnotation Task <ref type="bibr" coords="1,341.45,482.23,10.52,8.74" target="#b5">[6]</ref> focused on providing annotations to a testset of 10000 images using 93 concepts. These concepts were mostly object-based e.g. dog, cat, scene-based e.g. landscape, beach, event-based e.g. work, travel, quality-based e.g. blurry, overexposed or representation-based e.g. portrait, art. Our experiments <ref type="bibr" coords="1,267.63,530.05,10.52,8.74" target="#b4">[5]</ref> in last year's task were based on the standard bag-of-words model for image classification. We performed a feature fusion of the OpponentSIFT <ref type="bibr" coords="1,220.83,553.96,10.52,8.74" target="#b8">[9]</ref> local feature and the no-reference objective image sharpness measure <ref type="bibr" coords="1,194.01,565.92,9.96,8.74" target="#b2">[3]</ref>, which showed classification gains with the most gains occuring among the quality-based concepts. We also performed a post-classification step, which was based on the observation that many of the categories could be seen as being related to one other, thereby enabling the inference or exclusion of other categories.</p><p>In this year's task, the list of concepts was extended to 99 including sentimentbased concepts e.g. sad and happy. Our experiments in this year's task built on our approaches of last year. We performed a feature fusion of the OpponentSIFT descriptor and the GIST descriptor, with the target being to improve the classification performance of scene-based concepts, since the GIST descriptor has successfully been applied to scene classification and recognition <ref type="bibr" coords="2,419.55,154.86,9.96,8.74" target="#b7">[8]</ref>. Computation of the descriptor involves accumulating image statistics over the entire scene rather than over local regions. We also evaluated an approach to select the most informative training samples from the training set per category to train the classifiers, which resulted in qualitative as well as runtime performance gains. We will refer to this approach as Smart Sampling (SS) in the rest of this paper. Finally we used optimized versions of our post-classification processing algorithms of last year's task in this year's detection system.</p><p>In the following, section 2 describes our concept detection system in more detail and outlines the differences to our system of last year while section 3 summarizes this paper by giving some results and providing an outlook into future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Concept Detection System</head><p>Our concept detection system for this year's task was an optimized version of last year's system, which integrated a fusion of the OpponentSIFT descriptor and the GIST descriptor, the SS approach for effcient training sample selection per category and an optimized post-classification processing step. For a detailed description of last year's system architecture, including details of the OpponentSIFT feature refer to <ref type="bibr" coords="2,272.80,414.69,9.96,8.74" target="#b4">[5]</ref>.</p><p>The spatial envelope model was initially introduced in <ref type="bibr" coords="2,380.93,427.81,10.52,8.74" target="#b7">[8]</ref> as a low-dimensional representation of a scene and successfully applied for k-nearest-neighbor scene classification. The idea was to represent the dominant spatial structure of a scene rather than applying segmentation or any kind of processing of individual objects or regions. The authors propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. The spatial envelop model was later termed the GIST descriptor following Friedman's <ref type="bibr" coords="2,303.40,511.50,10.52,8.74" target="#b3">[4]</ref> definition of a scene gist; an abstract representation of the scene that spontaneously activates memory representations of scene categories (a city, a mountain, etc.), which is essentially what is captured by the spatial envelope model <ref type="bibr" coords="2,268.86,547.36,9.96,8.74" target="#b6">[7]</ref>. Extraction of GIST is performed by filtering the image by a bank of Gabor filters. The image is split into a 4 × 4 regular grid and the Gabor filter responses are averaged over each block.</p><p>We use the implementation presented in <ref type="bibr" coords="2,327.20,584.39,10.52,8.74" target="#b1">[2]</ref> which takes a squared gray-level image of fixed size as input. All images are rescaled initially to 256×256 irrespective of their aspect ratios. We obtain a GIST feature vector of 512 dimensions per image, which is significantly less than the bag of keypoints vectors at even the smallest grid resolution. Moreover, the GIST features can be computed much more efficiently as there are no codebook and histogram computation steps involved. Fusion of the GIST descriptor and the OpponentSIFT descriptor was done analogously to our fusion of the OpponentSIFT descriptor and the noreference objective image sharpness measure in last year's system.</p><p>We further applied a Smart Sampling optimization step to all classifiers. Smart Sampling selects the most informative training images from the training set in order to train a classifier faster and more efficiently <ref type="bibr" coords="3,388.61,167.02,9.96,8.74" target="#b0">[1]</ref>. This occurs in an iterative process, whereby the training set is divided into a number of subsets and the classifier is trained using one of the subsets and used to classify another subset. For every further iteration, the training set is composed of the previous iteration's training images and classified images having a classifier confidence of |c| &lt; 1, and with classification performed on a new subset. Through this process, we select training samples from the whole training set which lie in close proximity to the separating hyperplanes of the classifier for each category. This led to an increase in runtime and qualitative performance.</p><p>Finally, we performed further tests to optimize the Exclusion and Inference rules, which we used in the post-classification processing step of last year's system, while adapting them to the newly added categories of this year's task. Changes in the category list of this year, led to the need for modifications of the post-classification processing algorithms. Last year it was possible to identify groups of categories, with each image being able to belong to only one category in each group. With the exclusion of categories such as No Visual Season this year, the same groups as in last year could no longer be identified. Consequently, the following equation which was assumed to hold for all groups of excluding categories was invalidated.</p><formula xml:id="formula_0" coords="3,283.95,407.18,196.64,20.06">p∈P C p = I<label>(1)</label></formula><p>The equation was modified to</p><formula xml:id="formula_1" coords="3,283.95,460.91,196.64,20.06">p∈P C p ⊆ I<label>(2)</label></formula><p>which further leads to different update rules for confidences. Rather than</p><formula xml:id="formula_2" coords="3,199.23,512.06,281.37,20.69">c (i, p) = c(i, p) if c(i, p) &gt; c(i, q) ∀q ∈ P \ p 0 else<label>(3)</label></formula><p>where the maximum confidence for a category is maintained and all other confidences in a group of excluding categories are set to 0, the following update rule was used.</p><formula xml:id="formula_3" coords="3,146.18,598.16,334.41,20.69">c (i, p) = c(i, p) if c(i, p) &gt; c(i, q) and c(i, q) &gt; 0.73105 ∀q ∈ P \ p 0 else<label>(4)</label></formula><p>The treshold 0.73105 was chosen because it is the value of the sigmoid function applied to 1.0, i.e. the exclusion rule is only used if the maximum category confidence for a given image is outside the tube around the separating hyperplane.</p><p>For category inference, the system from last years submission was maintained. However, the rule confidence threshold (i.e. the threshold that toggles if a rule is used) was tuned. Last year a threshold of 0.99 was used, meaning that a rule was only used if its confidence in the training set was at least 0.99. We optimized this threshold to maximize the Example-based F-Measure. The maximum gain was reached using a value of 0.63 for this threshold. The update rule was modified analogously to the exclusion update rule given before, by applying rules only if the confidence of the rules' left-hand side category confidence was above 0.73105 (i.e. outside of the tube around the separating hyperplane).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Summary</head><p>We submitted 5 different runs. All runs use the OpponentSIFT histograms as baseline. The first run (OpSIFT ) uses the OpponentSIFT feature alone. Another run (OpSIFT+Excl+Inf+SS ) uses the Smart Sampling optimization and applies category inference and exclusion as a post-classification processing step. A third run (OpSIFT+Gist) does no post-classification processing but uses the GIST feature as an additional feature. The fourth run (OpSIFT+SS ) uses the Smart Sampling optimization step in addition to the OpponentSIFT feature. A final fifth run (OpSIFT+SS+Inf ) uses the OpponentSIFT features with the Smart Sampling optimization step and applies the inference rule on the classification results.</p><p>Three different evaluation measures were computed. For evaluating the classification performance per concept the Mean Average Precision (MAP) was used. The evaluation per example was performed using the example-based F-Measure (F-Ex) and the Semantic R-Precision (SR-Precision).</p><p>Table <ref type="table" coords="4,177.36,438.11,4.98,8.74" target="#tab_0">1</ref> shows the average scores achieved for each measure. For all evaluation measures, all runs with extensions to the baseline OpponentSIFT method resulted in performance gains. In order to observe the influence of the GIST feature on scene-based concepts, we compared the MAP for those concepts out of the overall 99 concepts we considered as scene-based concepts ('Building Sights', 'Citylife', 'Landscape/ Nature','Indoor','Outdoor','Mountains', 'Sunset/ Sunrise', 'Park/ Garden', 'Beach/ Holidays') with their baseline (using only the OpponenSIFT descriptor) values. This is depicted in table 2. We observe that using the GIST feature together with the OpponenSIFT feature yields a gain of 0.00807 compared to the baseline alone. Table <ref type="table" coords="5,177.43,337.70,4.98,8.74" target="#tab_2">3</ref> shows the detailed average precision for each category individually. For those categories where the results differ, the best performing run is highlighted in the table. In terms of MAP per category, the exclusion of categories often performed worse than the other runs. For categories especially, where the average precision was already low, the exclusion rule worsened the results. We attributed this to a lack of reliability of the SVM confidence outputs. The SVM classifier of bad performing categories had a very small output range, e.g. values ranging from 0.94 to 0.96. In such cases, the number of support vectors used for the categories was usually near the total number of training samples. Using these categories for inference or exclusion of other categories yielded a significant decrease in performance, propagating the error introduced by one categorie's classifier to other categories. In the future, a check for the reliability of classifier outputs should be performed to avoid such error propagation. This also holds for the category inference post-processing rule, which also suffers from this problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,494.63,345.82,77.76"><head>Table 1 .</head><label>1</label><figDesc>Average evaluation scores for all submitted runs. Highlighted values show the run, which obtained the best score for a specific evaluation measure.</figDesc><table coords="4,213.46,494.63,188.44,46.44"><row><cell>-Configuration</cell><cell cols="2">MAP Avg. F-Ex SR-Precision</cell></row><row><cell>OpSIFT</cell><cell>0.325111 0.579904</cell><cell>0.71262264</cell></row><row><cell>OpSIFT+GIST</cell><cell>0.335234 0.588042</cell><cell>0.71764547</cell></row><row><cell>OpSIFT+SS</cell><cell>0.326483 0.580804</cell><cell>0.71251965</cell></row><row><cell cols="2">OpSIFT+Excl+Inf+SS 0.325981 0.580729</cell><cell>0.71252900</cell></row><row><cell>OpSIFT+SS+Inf</cell><cell>0.325981 0.580729</cell><cell>0.71252900</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,173.52,345.83,126.38"><head>Table 2 .</head><label>2</label><figDesc>Average Precision for categories where the GIST feature has been used. Best scores are highlighted.</figDesc><table coords="5,224.72,173.52,165.92,95.06"><row><cell></cell><cell cols="2">OpSIFT OpSIFT+GIST</cell></row><row><cell>Park/Garden</cell><cell>0.381190</cell><cell>0.401956</cell></row><row><cell>Sunset/Sunrise</cell><cell>0.722446</cell><cell>0.716504</cell></row><row><cell>Mountains</cell><cell>0.496681</cell><cell>0.489736</cell></row><row><cell>Outdoor</cell><cell>0.875141</cell><cell>0.881579</cell></row><row><cell>Indoor</cell><cell>0.573047</cell><cell>0.581117</cell></row><row><cell>Landscape/Nature</cell><cell>0.769700</cell><cell>0.778197</cell></row><row><cell>Citylife</cell><cell>0.502739</cell><cell>0.515869</cell></row><row><cell>Building Sights</cell><cell>0.518642</cell><cell>0.545948</cell></row><row><cell>Beach/Holidays</cell><cell>0.369467</cell><cell>0.370691</cell></row><row><cell cols="2">Mean Average Precision 0.578772</cell><cell>0.586844</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,137.71,536.87,337.07,126.14"><head>Table 3 :</head><label>3</label><figDesc>Average Precision per category. Scores are only highlighted, when any of the extensions provided a performance increase or decrease.</figDesc><table coords="5,137.71,536.87,337.07,126.14"><row><cell>Category</cell><cell>OpSIFT</cell><cell cols="4">OpSIFT+SS OpSIFT+SS+I+E OpSIFT+GIST OpSIFT+SS+I</cell></row><row><cell>Partylife</cell><cell>0.234868</cell><cell>0.244337</cell><cell>0.244337</cell><cell>0.232401</cell><cell>0.244337</cell></row><row><cell>Family Friends</cell><cell>0.476125</cell><cell>0.477097</cell><cell>0.477097</cell><cell>0.494923</cell><cell>0.477097</cell></row><row><cell>Beach Holidays</cell><cell>0.369467</cell><cell>0.357638</cell><cell>0.357638</cell><cell>0.370691</cell><cell>0.357638</cell></row><row><cell>Building Sights</cell><cell>0.518642</cell><cell>0.525117</cell><cell>0.525117</cell><cell>0.545948</cell><cell>0.525117</cell></row><row><cell>Snow</cell><cell>0.127158</cell><cell>0.135231</cell><cell>0.135231</cell><cell>0.147183</cell><cell>0.135231</cell></row><row><cell>Citylife</cell><cell>0.502739</cell><cell>0.504007</cell><cell>0.504007</cell><cell>0.515869</cell><cell>0.504007</cell></row><row><cell>Landscape Nature</cell><cell>0.7697</cell><cell>0.767596</cell><cell>0.767596</cell><cell>0.778197</cell><cell>0.767596</cell></row><row><cell>Sports</cell><cell>0.141025</cell><cell>0.141975</cell><cell>0.141975</cell><cell>0.147944</cell><cell>0.141975</cell></row><row><cell>Desert</cell><cell>0.03989</cell><cell>0.039747</cell><cell>0.039747</cell><cell>0.04092</cell><cell>0.039747</cell></row><row><cell>Spring</cell><cell>0.119698</cell><cell>0.157181</cell><cell>0.157181</cell><cell>0.155588</cell><cell>0.157181</cell></row><row><cell>Summer</cell><cell>0.226242</cell><cell>0.22794</cell><cell>0.22794</cell><cell>0.283984</cell><cell>0.22794</cell></row><row><cell>Autumn</cell><cell>0.293937</cell><cell>0.300913</cell><cell>0.300913</cell><cell>0.31722</cell><cell>0.300913</cell></row><row><cell>Winter</cell><cell>0.181803</cell><cell>0.203416</cell><cell>0.203416</cell><cell>0.188093</cell><cell>0.203416</cell></row><row><cell>Indoor</cell><cell>0.573047</cell><cell>0.575148</cell><cell>0.575148</cell><cell>0.581117</cell><cell>0.575148</cell></row><row><cell>Outdoor</cell><cell>0.875141</cell><cell>0.873909</cell><cell>0.873742</cell><cell>0.881579</cell><cell>0.873742</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements This work was supported in part by the <rs type="funder">German Federal Ministry of Economics and Technology</rs> under the project <rs type="projectName">THESEUS</rs> (<rs type="grantNumber">01MQ07018</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_yPH6KCm">
					<idno type="grant-number">01MQ07018</idno>
					<orgName type="project" subtype="full">THESEUS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,388.22,342.25,7.86;7,146.91,399.18,333.67,7.86;7,146.91,410.14,45.06,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,409.25,388.22,71.35,7.86;7,146.91,399.18,139.27,7.86">Fast kernel classifiers with online and active learning</title>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seyda</forename><surname>Ertekin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,292.87,399.18,151.05,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1579" to="1619" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,420.62,342.25,7.86;7,146.91,431.58,333.67,7.86;7,146.91,442.54,333.67,7.86;7,146.91,453.50,20.99,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,185.25,431.58,231.45,7.86">Evaluation of gist descriptors for web-scale image search</title>
		<author>
			<persName coords=""><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Herv</forename><surname>Jgou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harsimrat</forename><surname>Sandhawalia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurent</forename><surname>Amsaleg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,426.96,431.58,53.63,7.86;7,146.91,442.54,298.72,7.86">Proceeding of the ACM International Conference on Image and Video Retrieval CIVR 09</title>
		<meeting>eeding of the ACM International Conference on Image and Video Retrieval CIVR 09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,463.99,342.25,7.86;7,146.91,474.95,333.68,7.86;7,146.91,485.91,124.84,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,252.07,463.99,228.53,7.86;7,146.91,474.95,250.43,7.86">A No-Reference Objective Image Sharpness Metric Based on the Notion of Just Noticeable Blur (JNB). Image Processing</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ferzli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,404.15,474.95,76.44,7.86;7,146.91,485.91,8.29,7.86">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="717" to="728" />
			<date type="published" when="2009-04">April 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,496.39,342.24,7.86;7,146.91,507.35,333.68,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,201.40,496.39,279.19,7.86;7,146.91,507.35,62.23,7.86">Framing pictures: the role of knowledge in automatized encoding and memory for gist</title>
		<author>
			<persName coords=""><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,215.77,507.35,171.53,7.86">Journal of experimental psychology General</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="316" to="355" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,517.84,342.24,7.86;7,146.91,528.80,333.68,7.86;7,146.91,539.76,140.42,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,281.63,528.80,198.96,7.86;7,146.91,539.76,111.29,7.86">Augmenting bag-of-words -category specific features and concept reasoning</title>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Mbanya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Hentschel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Nrnberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Ndjiki-Nya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,550.24,342.24,7.86;7,146.91,561.20,333.68,7.86;7,146.91,572.16,79.48,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,258.70,550.24,221.89,7.86;7,146.91,561.20,183.09,7.86">New Strategies for Image Annotation: Overview of the Photo Annotation Task at ImageCLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Huiskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,353.58,561.20,122.74,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,582.65,257.07,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,184.79,582.65,66.12,7.86">Gist of the scene</title>
		<author>
			<persName coords=""><surname>Oliva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Elsevier</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="251" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,593.13,342.24,7.86;7,146.91,604.09,333.67,7.86;7,146.91,615.05,20.99,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,298.31,593.13,182.28,7.86;7,146.91,604.09,152.83,7.86">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName coords=""><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,310.66,604.09,93.60,7.86">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,625.54,342.24,7.86;7,146.91,636.50,333.67,7.86;7,146.91,647.45,290.28,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,359.35,625.54,121.24,7.86;7,146.91,636.50,126.33,7.86">A comparison of color features for visual concept classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,283.10,636.50,197.49,7.86;7,146.91,647.45,221.56,7.86">Proceedings of the 2008 international conference on Content-based image and video retrieval -CIVR &apos;08</title>
		<meeting>the 2008 international conference on Content-based image and video retrieval -CIVR &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">141</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
