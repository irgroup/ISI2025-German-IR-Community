<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.82,150.75,316.59,12.64;1,284.21,166.83,28.01,12.64">UNED-UV at Medical Retrieval Task of ImageCLEF 2011</title>
				<funder ref="#_Yrp3rN6">
					<orgName type="full">Spanish Government</orgName>
				</funder>
				<funder ref="#_eHzQy7T">
					<orgName type="full">Regional Government of Madrid under Research Network MA2VIRMR</orgName>
				</funder>
				<funder ref="#_TnevjEb">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,156.98,182.08,61.49,9.94"><forename type="first">A</forename><surname>Castellanos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidad Nacional de Educación a Distancia</orgName>
								<orgName type="institution" key="instit2">UNED</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,226.85,182.08,55.03,9.94"><forename type="first">X</forename><surname>Benavent</surname></persName>
							<email>xaro.benavent@uv.es</email>
							<affiliation key="aff1">
								<orgName type="institution">Universitat de València</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.33,182.08,48.65,9.94"><forename type="first">J</forename><surname>Benavent</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universitat de València</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.47,182.08,88.08,9.94"><forename type="first">Ana</forename><surname>García-Serrano</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidad Nacional de Educación a Distancia</orgName>
								<orgName type="institution" key="instit2">UNED</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.82,150.75,316.59,12.64;1,284.21,166.83,28.01,12.64">UNED-UV at Medical Retrieval Task of ImageCLEF 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">21F13132E7488DB7B5D21C84440AFDE9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Query Expansion</term>
					<term>Textual-based Retrieval</term>
					<term>Content-based Retrieval</term>
					<term>Merging</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The main goal of this paper it is to present our experiments in ImageCLEF 2011 Campaign (Medical Retrieval Task). This edition we use textual and visual information, based on the assumption that the textual module better captures the meaning of a topic. So that, the TBIR module works firstly and acts as a filter, and the CBIR system reorder the textual result list. We also investigate if query expansion with image terms or with modality classification could be a way to improve base queries This paper is profiting on the work done in previous years on ImageCLEF (Wikipedia Retrieval Task). In this edition we submitted a total of ten runs (4 textual and 6 mixed). Textual ones have better results (being two of them 2 nd and 6 th within their category). Mixed runs are about the middle of the results, although have demonstrated that can improve the only textual results. With our results we have proved that query expansion with term concerning image type of the query is a promising way to further research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The main goal of this paper it is to present our experiments in ImageCLEF 2011 Campaign (Medical Image retrieval task) <ref type="bibr" coords="1,292.85,536.51,10.69,8.96" target="#b0">[1]</ref>. In this working note, we rather focus on our participation in two sub-tasks of the Medical Retrieval Tasks (Image Modality Classification and Ad-hoc Image Retrieval). This ImageCLEF edition our group presents a way of working using the information of the Content Based Image Retrieval (CBIR) system and the information of the Textual Based Image Retrieval (TBIR) system. We use the work done in this regards in previous editions of ImageCLEF <ref type="bibr" coords="1,267.77,625.94,10.60,8.96" target="#b1">[2]</ref>, based on the assumption that the conceptual meaning of a topic is initially better captured by the text module itself than by the visual module, so our merging method gives greater weight.</p><p>In this field numerous approaches have been researched in previous works <ref type="bibr" coords="2,426.19,149.70,11.69,8.96" target="#b2">[3]</ref>  <ref type="bibr" coords="2,440.59,149.70,10.69,8.96" target="#b3">[4]</ref>, our view raises a different type of merging, the TBIR system works firstly over the whole database working as a filter, and then the Valencia University CBIR system reorders the filtered textual result list, taking into account the visual features.</p><p>Concerning the TBIR subsystem, we have worked on two different approaches. The first of them pretended to check if the query expansion with term relatives to image type could achieve a significant improvement of the results obtained. In past years of ImageCLEF, different methods of query expansion have been posed. The bulk of them were focused on using external medical sources like MESH <ref type="bibr" coords="2,398.35,265.53,11.69,8.96" target="#b4">[5]</ref> or UMLS <ref type="bibr" coords="2,456.55,265.53,10.71,8.96" target="#b5">[6]</ref>, also has been raised the expansion of queries with another external sources like Wikipedia <ref type="bibr" coords="2,172.22,292.05,10.69,8.96" target="#b6">[7]</ref>. Our aim is to prove another type of query expansion that can complement the studies presented above.</p><p>The second approach was focused on investigating how vary the results returned by the queries adding the information relating to the classification of the images provided by ImageCLEF.</p><p>The CBIR subsystem is quite similar to the system used in previous works of ImageCLEF <ref type="bibr" coords="2,177.50,391.41,10.69,8.96" target="#b1">[2]</ref>. The CBIR subsystem uses its own low-level features or the CEDD features <ref type="bibr" coords="2,159.74,404.63,15.42,8.96" target="#b10">[11]</ref>, depending on the experiment in order to test the influence of the lowlevel features in the final result. Two different algorithms have also been used: logistic regression relevance feedback algorithm and an automatic algorithm with the Tanimoto distance.</p><p>A more detailed presentation of the system, the submitted experiments, and the obtained results is included in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>The global system includes two main subsystems: the TBIR and the CBIR (Fig. <ref type="figure" coords="2,456.03,537.95,7.33,8.96" target="#fig_0">1.</ref>). The TBIR subsystem is responsible for preprocessing and indexing the textual information both of the images, textual annotations of images and queries.</p><p>The TBIR subsystem acts over the whole images of the database as a filter. Only the images returned by TBIR module are sending to the CBIR system. In a second step, the CBIR system works over the set of filtered images reordering this list taking into account the visual information of the image and the score given by TBIR module to each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text-based Index and Retrieval</head><p>This module (TBIR) is in charge of the textual image retrieval using the text associated with each image in the collection.</p><p>Previously, to be able to work with the collection, this was preprocessed. Later, it has been carried out the indexing of the images and their subsequent search and retrieval of images for each query through Solr1, a search platform from Lucene2 project. The result of this process is an image list that is ranked according to their similarity with the corresponding query, in accordance only with the textual information. Below, is explained in more detail each of the different stages performed by TBIR module:   Construct Queries File. Based on original query file, four query files are constructed in order to address the two approaches we want to investigate: query expansion and inclusion of modality classification. Each of these files corresponds to one of the four runs explained below</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MERGE</head><p>Preprocess. Textual information is preprocessed in different ways in order to improve search and retrieval. The order in which transformations are applied is as follows: 1) special characters deletion: characters with no statistical meaning, like punctuation marks or blanks, are eliminated; 2) stopwords detection: deletion of semantic empty words in English language, 3) stemming: reduction of word to their base form and 4) convert all words to lower case.</p><p>Indexing. The indexing is done automatically by Solr, this requires to configure which fields must have the index and create a handler that establish how Solr have to read the xml file input and add their information to the index. The indexing itself is made by Lucene. The time of indexing, about 7 minutes, is relatively short considering the size of the collection.</p><p>Searching. This process is started manually, running each query by a Solr interface.</p><p>The results, returned in xml format, are transformed to the trec-eval format, in order to merge these textual results with visual results and check the results using the UV tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Content-Based Information and Visual Retrieval</head><p>The VISION-Team at the Computer Science Department of the University of Valencia has its own CBIR system, and that has been used in previous ImageCLEF editions (Photo-retrieval task <ref type="bibr" coords="4,245.21,560.54,10.56,8.96" target="#b7">[8]</ref>). Last edition, the focus of the work was the testing of three different visual algorithms applied to the results retrieved by the textual module: the automatic, the relevance feedback and the query expansion, obtaining the best results with the relevance feedback algorithm. Therefore, this edition we have used the relevance feedback algorithm and our work focus on testing the behavior of our own low-level features with the low-level features given by the organization (the CEDD algorithm described in (1)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extraction of low level features:</head><p>The first step at the Visual Retrieval system is extracting these features for all the images on the database as for each of the cluster query topic images for each question. Instead of using the low-level features provided by the organization, we have used our own features:</p><p> Color information: Color information has been extracted calculating both local and global histograms of the images using 10x3 bins on the HS color system. Local histograms have been calculated dividing the images in four fragments of the same size. A bidimensional HS histogram with 10x3 bins is computed for each patch. Therefore, a feature vector of 222 components represents the color information of the image.  Texture information: Two types of texture features are computed: The granulometric distribution function, using the coefficients that result of fitting the distribution function with a B-spline basis. And, the Spatial Size Distribution. We have used two different versions of it by using as the structuring elements for the morphological operation that get size both a horizontal and a vertical segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic algorithm.</head><p>This is the most common algorithm in a CBIR system. Each image in the database has an associated low level feature vector. Concretely, we have used for this algorithm the low level features given by the organization (CEDD).</p><p>The next step is to calculate the similarity measurement between the feature vectors of each image on the database and the N query images. The distance metric applied in our experiments is the Tanimoto As we have N query images, we will obtain N visual result lists, one for each query image in the topic. These N result lists are merged by using an average OWA operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relevance feedback algorithm based on logistic regression.</head><p>This algorithm works differently to the two previous ones. Therefore, we will explain the concept of relevance feedback and the adjustments made to get a good performance of the algorithm for the proposed tasks <ref type="bibr" coords="5,336.31,669.26,10.69,8.96" target="#b8">[9]</ref>. Relevance feedback is a term used to describe the actions performed by an user to interactively improve the results of a query by reformulating it. An initial query formulated by a user may not fully capture his/her wishes. Users then typically change the query manually and re-execute the search until they are satisfied. By using relevance feedback, the system learns a new query that better captures the user's need for information. The user enters his/her preferences every iteration through the selection of relevant and non-relevant images.</p><p>We will explain the way the logistic regression relevance feedback algorithm works.</p><p>Let us consider the (random) variable Y giving the user evaluation where Y=1 means that the image is positively evaluated and Y=0 means a negative evaluation. Each image in the database has been previously described by using low-level features in such a way that the j-th image has the k-dimensional feature vector xj associated. Our data will consist of (xj, yj), with j=1,…,n, where n is the total number of images, xj is the feature vector and yj the user evaluation (1=positive and 0=negative). The image feature vector x is known for any image and we intend to predict the associated value of Y. In this work, we have used a logistic regression where P(Y=1|x) i.e. the probability that Y=1 (the user evaluates the image positively) given the feature vector x, is related with the systematic part of the model (a linear combination of the feature vector) by means of the logit function. For a binary response variable Y and p explanatory variables X1,…,Xp, the model for π</p><formula xml:id="formula_0" coords="6,124.82,384.47,345.69,22.27">(x)=P(Y=1|x) at values x=(x1,…,xp) of predictors is logit[π(x)]=α+β1x1+…+βpxp, where logit[π(x)]=ln(π(x)/(1-π(x))).</formula><p>The model parameters are obtained by maximizing the likelihood function given by:</p><formula xml:id="formula_1" coords="6,231.27,451.79,239.43,19.37">    n y i y i i i x x l 1 )] ( 1 [ ) ( ) (   <label>(1)</label></formula><p>The maximum likelihood estimators (MLE) of the parameter vector β are calculated by using an iterative method.</p><p>We have a major difficulty when having to adjust a global regression model in which we take the whole set of variables into account, because the number of selected images (the number of positive plus negative images) is typically smaller than the number of characteristics. In this case, the regression model adjusted has as many parameters as the number of data and many relevant variables could be not considered. In order to solve this problem, our proposal is to adjust different smaller regression models: each model considers only a subset of variables consisting of semantically related characteristics of the image. Consequently, each sub-model will associate a different relevance probability to a given image x, and we face the question of how to combine them in order to rank the database according to the user's preferences. This problem has been solved by means of an ordered averaged weighted operator (OWA) <ref type="bibr" coords="6,193.70,668.90,16.72,8.96" target="#b9">[10]</ref> In our case, we have adapted the manual relevance feedback to an automatic performance. The examples and the counter-examples (positive and negative images) are automatically selected for each topic. The examples are the query images of the topic plus N images taken from the first positions of the textual result list. The counter-examples are the M latest positions of the textual result list. The relevance feedback algorithm is executed once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Merging Results</head><p>We use a merging process for combining textual and visual results in order to try to improve them. To perform this merging process we use TBIR system like filter over the whole collection, only the results retrieved by TBIR system are passed through the CBIR system.</p><p>The CBIR system search taking into account visual characteristics of the images contained in each query. Based on the score of the results retrieved by this search, the images returned by TBIR system are reordered in three different ways (taking into account only score of the visual results, taking into account both visual and textual results and by combining textual and visual through an OWA fusion method).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We have participated in ad-hoc image-based retrieval task of Medical Image Retrieval Task 2011. Finally, we submitted 10 runs: 4 textual and 6 mixed (textual and visual).</p><p>This runs is shown in Table <ref type="table" coords="7,242.57,468.21,4.14,9.94" target="#tab_1">1</ref>.</p><p>With this runs we intend to study how the two approaches raised for this work affect to the results. For the first approach, query expansion about image terms, we present four different types of queries for textual retrieval that are explained in more detail below. Besides this in two of this runs (UNED-UV_05 and UNED-UV_06), we cover the other method, including modality classification. Hoping to improve results, we present another six runs merging textual retrieval with content-based retrieval</p><p>To do this we present 4 textual runs, based on 4 different types of queries explained in previous sections, UNED-UV_02 like baseline, UNED-UV_03 using query expansion and UNED-UV_05 and UNED-UV_06 including modality classification. Original query: photographs of benign or malignant skin lesions Expanded query: benign OR malignant skin lesions UNED_UV_03_TXT_AUTO_EN: Instead of deleting image terms, these are replaced by modality classification terms, whenever possible (x-ray = XR).</p><p>Original query: all x-ray images containing one or more fractures Expanded query: all (XR OR "x-ray") AND (one or more fractures)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNED_UV_05_TXT_AUTO_EN:</head><p>In this RUN we use the classification provided by ImageCLEF. It's intended to demonstrate that including information concerning to the classification of images significantly improves the results returned.</p><p>For this run, were added to each query another query against tag field (included in collection expansion) with the value of classification modality expected for the original query. The original query is weighted with 0.3 and query against tag field is weighted with 0.7. With these weights we want to avoid bias of the results, caused by medical terms.</p><p>Original query: chest CT images with emphysema. Expanded query: (tags:CT)^0.7 OR (chest CT AND emphysema)^0. <ref type="formula" coords="8,432.90,547.43,4.76,8.96">3</ref>UNED_UV_06_TXT_AUTO_EN: This RUN is similar to previous. We also included classification of images and expansion of the original queries with another query against tag field, but in this case, instead of aggregate this second query, it's used for filtering the results returned by the original query.</p><p>Original query: chest CT images with emphysema. Expanded query: chest CT AND emphysema results  tags:CT  filtered results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNED_UV_11_TXTIMG_AUTO_EN:</head><p>In this RUN we used the results returned by UNED_UV_05_TXT_AUTO_EN as input data and are reordered taking into account the weights established by CBIR module.</p><p>UNED_UV_12_TXTIMG_AUTO_EN: Same as above RUN, but using UNED_UV_06_TXT_AUTO_EN instead of UNED_UV_05_TXT_AUTO_EN.</p><p>UNED_UV_17_TXTIMG_AUTO_EN: UNED_UV_05_TXT_AUTO_EN results are used like input and are reordered based on the products of weights of TBIR and CBIR modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNED_UV_18_TXTIMG_AUTO_EN:</head><p>Same as above but using UNED_UV_06_TXT_AUTO_EN results like input.</p><p>UNED_UV_23_TXTIMG_AUTO_EN: UNED_UV_05_TXT_AUTO_EN results are used like input and are reordered according to an OWA with a weight of 0.3 for CBIR weighting and 0.7 for TBIR weighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNED_UV_24_TXTIMG_AUTO_EN:</head><p>Same as above but using UNED_UV_06_TXT_AUTO_EN results like input.  With the textual experiments we aimed to analyze two approaches this year. First of all we wanted investigate if query expansion, with terms relatives to the image type, improves the retrieved results. As it can be viewed in the table, runs in this sense (Run2_Txt and Run3_Txt) provide excellent results. The other approach that we pretend investigate was the inclusion of the images classification provided by the organization. In this regard we submitted two runs (Run6_Txt and Run5_Txt). The results of these runs are not good as we expected, worsen the results of the baseline runs.</p><p>In order to improve the textual results, we submitted 6 mixed runs, combining textual and visual retrieval. As is evident by the results, mixed results slightly improve textual results (especially using an OWA like fusion method). Since we expected that better results was provided by Run6_Txt and Run5_Txt, we based our 6 experiments on these, so mixed results are not as good as could be if we had used Run2_Txt or Run3_Txt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks and Future Work</head><p>In this year we have participated by first time in Medical Image Retrieval Task and yet our runs have had very satisfactory results. Our bests results are for the textual modality, two of our runs are in the 2nd and 6th position respectively in the textual category. In addition, most of our runs are between first 50% in their category.</p><p>Concerning to our approaches, we have demonstrated that query expansion with terms relatives to image concepts improves in a significantly way the retrieved results and show much more effective than the query expansion with modality classification of the images.</p><p>Can be gathered from our mixed runs is that the combination and textual and visual results can improve results, although in a slightly way. In this regard we pretend to continue investigating in order to refine the process of fusion textual and visual results.</p><p>Future work pretends to go beyond in the work of query expansion conducted in this work, including external sources like MESH <ref type="bibr" coords="11,329.95,377.73,11.69,8.96" target="#b4">[5]</ref> that have demonstrated that significantly improves the results for this task, as well as further research in the mixed (textual and visual) retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,251.81,521.44,91.85,9.00;3,199.32,404.28,127.08,82.92"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. -System Overview</figDesc><graphic coords="3,199.32,404.28,127.08,82.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,431.59,355.78,5.89,6.26;3,431.95,401.04,16.66,6.26;3,430.75,410.64,19.07,6.26;3,430.75,373.66,9.35,6.26;3,432.55,383.02,5.89,6.26;3,365.71,441.72,13.54,6.26;3,355.99,451.08,32.77,6.26;3,210.53,412.30,20.08,7.24;3,262.37,463.32,53.35,6.26;3,262.13,472.68,53.69,6.26;3,214.13,443.04,20.82,6.26;3,209.93,452.40,29.12,6.26;3,274.37,426.36,29.35,6.26;3,260.81,435.84,56.38,6.26;3,327.67,357.27,19.96,5.40;3,364.75,320.38,15.95,6.26;3,283.13,339.94,30.28,6.26;3,323.81,312.63,29.01,5.40;3,213.05,310.90,34.86,6.26;3,212.21,320.26,36.69,6.26;3,212.69,371.84,19.72,7.24;3,207.41,342.22,46.37,6.26;3,223.85,351.82,13.50,6.26;3,363.19,365.86,19.06,6.26;3,141.14,473.93,25.88,5.40;3,145.34,481.97,17.57,5.40;3,141.14,357.63,25.88,5.40;3,142.22,365.49,24.06,4.54;3,136.82,311.79,34.63,5.40;3,144.86,319.83,18.64,5.40;3,144.26,418.56,19.72,6.26;3,149.54,401.93,10.90,5.40;3,146.42,434.93,17.21,5.40;4,124.82,149.70,345.56,8.96;4,124.82,163.02,345.34,8.96;4,124.82,176.22,345.23,8.96;4,124.82,189.42,184.73,8.96"><head></head><label></label><figDesc>In order to investigate how the image classification affects the results of queries, it has decided to include an additional field (called tags) in the xml file descriptors of the images. This field stores the modality of the image established by the classification provided by ImageCLEF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,124.82,631.46,345.71,8.96;7,124.82,644.66,345.74,8.96;7,124.82,657.86,345.76,8.96;7,124.82,671.06,345.90,8.96;7,124.82,684.26,345.93,8.96;8,124.82,149.70,345.36,8.96;8,124.82,163.02,345.63,8.96;8,124.82,176.22,334.54,8.96;8,124.82,207.54,12.41,8.96;8,153.14,207.54,158.84,8.96;8,124.82,228.66,345.72,8.96;8,124.82,241.79,345.26,9.06;8,124.82,254.99,345.39,9.05;8,124.82,268.29,345.37,8.96;8,124.82,281.51,108.56,9.06"><head></head><label></label><figDesc>The mixed runs are based on UNED-UV_05 and UNED-UV_06 (which it expected would have better results). The results obtained with UNED-UV_05 it have been passed through CBIR module in three different ways: 1) reordering the results taking into account only the weight given by CBIR module (Pi) [UNED-UV_11], 2) reordering based on the product of textual and visual weights (Pt*Pi) [UNED-UV_17] and 3) reordering combining textual and visual results by an OWA weighted with 0.7 and 0.3 respectively [UNED-UV_23]. For the UNED-UV_06, following the same structure we obtained runs [UNED-UV_12], [UNED-UV_18] and [UNED-UV_24] Baseline Run. Terms concerning to the image type (image, photograph …) are deleted in order to focus on medical terms included in each query. It's expected to improve results by reducing the noise introduced by these terms. We do not remove terms that refer directly to a type of modality classification (CT, XR …).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,124.82,409.84,345.86,278.34"><head>Table 1 .</head><label>1</label><figDesc>-Submitted textual and mixed experiments</figDesc><table coords="9,124.82,428.41,345.86,259.77"><row><cell></cell><cell></cell><cell></cell><cell>CBIR</cell><cell>TBIR</cell></row><row><cell>Run</cell><cell cols="2">Modality Image</cell><cell>Merge</cell><cell>Method</cell></row><row><cell>UNED_UV_02_TXT_AUTO_EN</cell><cell>Text</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Baseline</cell></row><row><cell>UNED_UV_03_TXT_AUTO_EN</cell><cell>Text</cell><cell>-</cell><cell>-</cell><cell>Query Expansion</cell></row><row><cell>UNED_UV_05_TXT_AUTO_EN</cell><cell>Text</cell><cell>-</cell><cell>-</cell><cell>Include Modality</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Classification -Aggregation</cell></row><row><cell>UNED_UV_06_TXT_AUTO_EN</cell><cell>Text</cell><cell>-</cell><cell>-</cell><cell>Include Modality</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Classification -Filtering</cell></row><row><cell>UNED_UV_11_TXTIMG_AUTO_EN</cell><cell>Mixed</cell><cell>LR_RF</cell><cell>Pi</cell><cell>-</cell></row><row><cell>UNED_UV_12_TXTIMG_AUTO_EN</cell><cell>Mixed</cell><cell>LR_RF</cell><cell>Pi</cell><cell>-</cell></row><row><cell>UNED_UV_17_TXTIMG_AUTO_EN</cell><cell>Mixed</cell><cell>LR_RF</cell><cell>Pt*Pi</cell><cell>-</cell></row><row><cell>UNED_UV_18_TXTIMG_AUTO_EN</cell><cell>Mixed</cell><cell>LR_RF</cell><cell>Pt*Pi</cell><cell>-</cell></row><row><cell>UNED_UV_23_TXTIMG_AUTO_EN</cell><cell>Mixed</cell><cell>LR_RF</cell><cell>OWA(Orness(0.3))</cell><cell>-</cell></row><row><cell>UNED_UV_24_TXTIMG_AUTO_EN</cell><cell>Mixed</cell><cell>LR_RF</cell><cell>OWA(Orness(0.3))</cell><cell>-</cell></row><row><cell>4 Results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Our results for the submitted experiments are shown in Table 2. As reflected in the</cell></row><row><cell cols="5">table, concerning only mixed experiments, our best results are those based on Run6</cell></row><row><cell cols="5">(Run6_TxtImgOwaOr03, at the 19th position and Run6_TxtImg_PtPi at the 20th</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,124.82,187.91,345.63,224.42"><head>Table 2 -</head><label>2</label><figDesc>Results for the submitted experiments</figDesc><table coords="10,124.82,207.31,345.63,205.02"><row><cell>Pos</cell><cell>Run</cell><cell>Mode</cell><cell>Map</cell><cell>P@10</cell><cell>P@20</cell><cell>R-prec</cell><cell>B-prec.</cell><cell>Num_rel_ret</cell></row><row><cell>19</cell><cell>Run6_TxtImg_OwaOr03</cell><cell>Mixed</cell><cell>0.1346</cell><cell>0.3467</cell><cell>0.2867</cell><cell>0.1666</cell><cell>0.1604</cell><cell>565</cell></row><row><cell>20</cell><cell>Run6_TxtImg_PtPi</cell><cell>Mixed</cell><cell>0.1311</cell><cell>0.3333</cell><cell>0.2817</cell><cell>0.1651</cell><cell>0.1557</cell><cell>565</cell></row><row><cell>21</cell><cell>Run5_TxtImg_OwaOr03</cell><cell>Mixed</cell><cell>0.1299</cell><cell>0.3200</cell><cell>0.2567</cell><cell>0.1613</cell><cell>0.1641</cell><cell>710</cell></row><row><cell>23</cell><cell>Run5_TxtImg_PtPi</cell><cell>Mixed</cell><cell>0.1176</cell><cell>0.2800</cell><cell>0.2100</cell><cell>0.1575</cell><cell>0.1614</cell><cell>705</cell></row><row><cell>30</cell><cell>Run6_TxtImg_Pi</cell><cell>Mixed</cell><cell>0.0891</cell><cell>0.2400</cell><cell>0.1933</cell><cell>0.1206</cell><cell>0.1288</cell><cell>508</cell></row><row><cell>33</cell><cell>Run5_TxtImg_Pi</cell><cell>Mixed</cell><cell>0.0699</cell><cell>0.1667</cell><cell>0.1517</cell><cell>0.1017</cell><cell>0.1394</cell><cell>755</cell></row><row><cell>2</cell><cell>Run2_Txt</cell><cell>Textual</cell><cell>0.2158</cell><cell>0.3533</cell><cell>0.3383</cell><cell>0.2470</cell><cell>0.2514</cell><cell>1383</cell></row><row><cell>6</cell><cell>Run3_Txt</cell><cell>Textual</cell><cell>0.2125</cell><cell>0.3867</cell><cell>0.3317</cell><cell>0.2468</cell><cell>0.2430</cell><cell>1138</cell></row><row><cell>53</cell><cell>Run6_Txt</cell><cell>Textual</cell><cell>0.1309</cell><cell>0.3433</cell><cell>0.2733</cell><cell>0.1652</cell><cell>0.1597</cell><cell>564</cell></row><row><cell>55</cell><cell>Run5_Txt</cell><cell>Textual</cell><cell>0.1270</cell><cell>0.3100</cell><cell>0.2517</cell><cell>0.1565</cell><cell>0.1622</cell><cell>651</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work has been partially supported for <rs type="funder">Regional Government of Madrid under Research Network MA2VIRMR</rs> (<rs type="grantNumber">S2009/TIC-1542</rs>), for <rs type="funder">Spanish Government</rs> by project <rs type="projectName">BUSCAMEDIA</rs> (<rs type="grantNumber">CEN-20091026</rs>) and by project <rs type="grantNumber">MCYT TEC2009-12980</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_eHzQy7T">
					<idno type="grant-number">S2009/TIC-1542</idno>
				</org>
				<org type="funded-project" xml:id="_Yrp3rN6">
					<idno type="grant-number">CEN-20091026</idno>
					<orgName type="project" subtype="full">BUSCAMEDIA</orgName>
				</org>
				<org type="funding" xml:id="_TnevjEb">
					<idno type="grant-number">MCYT TEC2009-12980</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,128.22,524.56,343.61,8.10;11,136.22,535.60,335.86,8.10;11,136.22,546.52,254.95,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,257.27,535.60,214.81,8.10;11,136.22,546.52,17.03,8.10">The CLEF 2011 medical image retrieval and classification tasks</title>
		<author>
			<persName coords=""><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,162.35,546.52,97.14,8.10">CLEF 2011 working notes</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.22,560.56,343.66,8.10;11,136.22,571.63,335.71,8.10;11,136.22,582.55,269.64,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,216.17,560.56,255.71,8.10;11,136.22,571.63,81.30,8.10">Experiences at ImageCLEF 2010 using CBIR and TBIR mixing information aproaches</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benavent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,225.89,571.63,246.04,8.10;11,136.22,582.55,190.89,8.10">Conference on Multilingual and Multimodal Information Access Evaluation CLEF 2010. Working Notes for the CLEF</title>
		<meeting><address><addrLine>Padua (Italy)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.22,596.59,343.83,8.10;11,136.22,607.63,236.88,8.10" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,308.09,596.59,163.96,8.10;11,136.22,607.63,210.09,8.10">Using Ontology Dimensions and Negative Expansion to solve Precise Queries in CLEF Medical Task</title>
		<author>
			<persName coords=""><forename type="first">Jean-Pierre And</forename><surname>Chevallet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joo-Hwee</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.22,621.55,343.89,8.10;11,136.22,632.59,335.86,8.10;11,136.22,643.63,288.76,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,425.47,621.55,46.64,8.10;11,136.22,632.59,332.64,8.10">Methods for combining content-based and textual-based approaches in medical image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Mouna</forename><surname>Torjmen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Pinel-Sauvagnat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohand</forename><forename type="middle">S</forename><surname>Boughanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,136.22,643.63,262.07,8.10">Evaluating Systems for Multilingual and Multimodal Information Access</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.22,657.55,343.98,8.10;11,136.22,668.59,173.86,8.10" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,333.55,657.55,138.65,8.10;11,136.22,668.59,128.66,8.10">Improving Retrieval Using External Annotations: OHSU at ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kalpathy-Cramer</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jayashree</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.22,149.99,344.06,8.10;12,136.22,161.03,335.76,8.10;12,136.22,172.07,335.66,8.10;12,136.22,182.99,20.35,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,136.22,161.03,335.76,8.10;12,136.22,172.07,89.63,8.10">MIRACLE at ImageCLEFmed 2007: Merging Textual and Visual Strategies to Improve Medical Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Villena-Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lana</forename><forename type="middle">-</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sara</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">González-Cristobal</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><forename type="middle">S</forename><surname>Carlos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,232.61,172.07,235.76,8.10">Advances in Multilingual and Multimodal Information Retrieval</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.22,196.94,343.77,8.19;12,136.22,208.07,335.95,8.10;12,136.22,218.99,335.84,8.10;12,136.22,230.03,123.81,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,241.01,196.94,230.99,8.18;12,136.22,208.07,248.63,8.10">XRCE&apos;s Participation in Wikipedia Retrieval, Medical Image Modality Classification Ad-hoc Retrieval Tasks of ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,416.35,208.07,55.82,8.10;12,136.22,218.99,335.84,8.10;12,136.22,230.03,45.37,8.10">Conference on Multilingual and Multimodal Information Access Evaluation CLEF 2010. Working Notes for the CLEF</title>
		<meeting><address><addrLine>Padua (Italy)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.22,244.10,343.93,8.10;12,136.22,255.02,297.84,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,242.06,244.10,140.87,8.10">MIRACLE (FI) at ImageCLEFphoto</title>
		<author>
			<persName coords=""><forename type="first">Ruben</forename><surname>Granados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,415.27,244.10,56.88,8.10;12,136.22,255.02,217.05,8.10">Cross-Languaje Evaluation Forum CLEF 2009</title>
		<meeting><address><addrLine>Corfu(Grecia)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note>Working Notes for the CLEF</note>
</biblStruct>

<biblStruct coords="12,128.23,269.06,343.66,8.10;12,136.22,280.10,157.30,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,190.82,269.06,249.70,8.10">Applying logistic regression to relevance feedback in image retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,446.40,269.06,25.48,8.10;12,136.22,280.10,42.40,8.10">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2621" to="2632" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.39,294.02,339.52,8.10;12,136.22,305.06,315.13,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,181.82,294.02,290.09,8.10;12,136.22,305.06,24.96,8.10">On ordered weighted averaging aggregation operators in multi criteria decision making</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Yaguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,167.66,305.06,177.72,8.10">IEEE Transactions Systems Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="183" to="190" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.39,319.01,339.39,8.19;12,136.22,329.93,335.57,8.18;12,136.22,341.06,335.65,8.10;12,136.22,352.10,212.70,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,413.19,319.01,58.59,8.18;12,136.22,329.93,328.00,8.18">Accurate Image Retrieval based on Compact Composite Descriptors and Relevance Feedback Information</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zagoris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papamarkos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,136.22,341.06,287.02,8.10">International Journal of Pattern Recognition and Artificial Intelligence (IJPRAI)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2010-02">February, 2010</date>
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
