<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,169.12,115.96,277.12,12.62;1,199.10,133.89,217.17,12.62">The CLEF 2011 Photo Annotation and Concept-based Retrieval Tasks</title>
				<funder ref="#_GuKrktR">
					<orgName type="full">Ministry of Economics</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,192.53,171.97,64.67,8.74"><forename type="first">Stefanie</forename><surname>Nowak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Technology (IDMT)</orgName>
								<orgName type="institution">Fraunhofer Institute for Digital Media</orgName>
								<address>
									<addrLine>Ehrenbergstr. 31</addrLine>
									<postCode>98693</postCode>
									<settlement>Ilmenau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.86,171.97,60.63,8.74"><forename type="first">Karolin</forename><surname>Nagel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Technology (IDMT)</orgName>
								<orgName type="institution">Fraunhofer Institute for Digital Media</orgName>
								<address>
									<addrLine>Ehrenbergstr. 31</addrLine>
									<postCode>98693</postCode>
									<settlement>Ilmenau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,349.19,171.97,73.64,8.74"><forename type="first">Judith</forename><surname>Liebetrau</surname></persName>
							<email>judith.liebetrau@idmt.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="department">Technology (IDMT)</orgName>
								<orgName type="institution">Fraunhofer Institute for Digital Media</orgName>
								<address>
									<addrLine>Ehrenbergstr. 31</addrLine>
									<postCode>98693</postCode>
									<settlement>Ilmenau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,169.12,115.96,277.12,12.62;1,199.10,133.89,217.17,12.62">The CLEF 2011 Photo Annotation and Concept-based Retrieval Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DFFF92344BD26D8FC0C81D33BFB0421B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ImageCLEF 2011 Photo Annotation and Concept-based Retrieval Tasks pose the challenge of an automated annotation of Flickr images with 99 visual concepts and the retrieval of images based on query topics. The participants were provided with a training set of 8,000 images including annotations, EXIF data, and Flickr user tags. The annotation challenge was performed on 10,000 images, while the retrieval challenge considered 200,000 images. Both tasks differentiate among approaches that consider solely visual information, approaches that rely only on textual information in form of image metadata and user tags, and multi-modal approaches that combine both information sources. The relevance assessments were acquired with a crowdsourcing approach and the evaluation followed two evaluation paradigms: per concept and per example. In total, 18 research teams participated in the annotation challenge with 79 submissions. The concept-based retrieval task was tackled by 4 teams that submitted a total of 31 runs. Summarizing the results, the annotation task could be solved with a MiAP of 0.443 in the multimodal configuration, with a MiAP of 0.388 in the visual configuration, and with a MiAP of 0.346 in the textual configuration. The conceptbased retrieval task was solved best with a MAP of 0.164 using multimodal information and a manual intervention in the query formulation. The best completely automated approach achieved 0.085 MAP and uses solely textual information. Results indicate that while the annotation task shows promising results, the concept-based retrieval task is much harder to solve, especially for specific information needs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the increasing amount of digital information on the Web and on personal computers, the need for systems that are capable of automated indexing, searching, and organising multimedia documents incessantly grows. Automated systems have to retrieve information with high precision in order to be accepted by industry and end-users. Often, multimedia retrieval systems are evaluated on different test collections with different performance measures, which makes the comparison of retrieval performance impossible and limits the benefit of the approaches. Benchmarking campaigns counteract these tendencies and establish an objective comparison among the performance of different approaches by posing challenging tasks and by distributing test collections, topics, and measures. This paper presents an overview of the ImageCLEF 2011 Photo Annotation and Concept-based Retrieval Tasks. The two tasks aim at the automated detection of visual concepts in consumer photos and the retrieval of photos based on a certain topic. Section 2 introduces the tasks and the evaluation methodology. Following, Section 3 discusses the visual concepts and query topics which simulate the user's information need in image search. Then, Section 4 describes the test collection and the relevance assessment process. Section 5 summarizes the approaches of the participants. Following, the results for the annotation task and the concept-based retrieval task are presented and discussed in Section 6 and Section 7, respectively. Finally, Section 8 summarizes and concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>The Photo Annotation and Concept-based Retrieval Tasks pose an image analysis challenge which consists of two sub tasks. The annotation task aims at the automated annotation of consumer photos with multiple concepts. It is similar to the visual concept detection and annotation task (VCDT) as it was posed in the last years <ref type="bibr" coords="2,209.47,346.18,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,221.64,346.18,7.01,8.74" target="#b1">2]</ref>. This year, the participants are asked to annotate a test set of 10,000 Flickr images with 99 visual concepts. To solve this task, an annotated training set of 8,000 images is provided. The evaluation considers a fully assessed test collection to compare the approaches of the participants. The second challenge poses a concept-based retrieval task. The participants are asked to retrieve (up to) the 1,000 most relevant images in ranked order for a given topic out of a test collection of 200,000 images. In total 40 topics, each consisting of a logical connection of concepts from the annotation task, are provided. Concept detectors may be trained on the training set of the annotation task (8,000 images annotated with 99 visual concepts). The assessment incorporates a pooling strategy with crowdsourced relevance assessments. Both tasks can be solved by following three different approaches:</p><p>1. Automatic annotation with visual information only ("V") 2. Automatic annotation based on Flickr user tags and image metadata ("T") 3. Multi-modal approaches that consider visual information and/or Flickr user tags and/or EXIF information ("M")</p><p>The participants can choose one task or participate in both. Both tasks make use of a subset of the MIR Flickr 1 Million image dataset <ref type="bibr" coords="2,393.58,554.46,9.96,8.74" target="#b2">[3]</ref>. The MIR Flickr collection supplies all original tag data provided by the Flickr users (further denoted as Flickr user tags). These Flickr user tags are made available for the textual and multi-modal approaches of both subtasks. For most of the photos, the EXIF data is included and may be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Evaluation Objectives</head><p>The main evaluation objectives of the two tasks in 2011 lie in the exploitation of different knowledge sources, the benefit of annotation approaches as part of the retrieval process, and the automated prediction of subjective concepts such as sentiments. Moreover, participants need to deal with an unbalanced amount of data per concept, a varying number of labels per image, the diversity of image content per concept, and the different qualities of image metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ontology</head><p>The novel sentiment concepts are included in the Photo Tagging ontology <ref type="bibr" coords="3,458.83,200.40,10.52,8.74" target="#b3">[4]</ref> of the last years. The hierarchy allows making assumptions about the assignment of concepts to documents. Additionally, other relationships between concepts determine possible label assignments. The ontology restricts, for instance, the simultaneous assignment of some concepts (disjoint items) or defines that one concept postulates the presence of other concepts. The ontology allows the participants to incorporate semantic knowledge in their annotation algorithms, and to make assumptions about probable concept combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Measures</head><p>In the annotation task, the evaluation sticks to the concept-based and examplebased evaluation paradigm. For the concept-based evaluation, the Mean interpolated Average Precision (MiAP) is utilized, while the example-based evaluation applies the example-based F-Measure (F-Ex). Additionally, we introduce a novel performance measure called Semantic R-Precision (SR-Precision) which is based upon the example-based R-Precision, but incorporates the Flickr Tag Similarity (FTS) <ref type="bibr" coords="3,182.13,401.36,10.52,8.74" target="#b4">[5]</ref> to determine the semantic relatedness of visual concepts in the case of misclassifications. R-Precision calculates Precision at perfect Recall in an example-based evaluation scenario. The SR-Precision variant assigns misclassification costs based on the semantic relatedness among misclassified concepts. The semantic relatedness is derived from the FTS measure. In contrast to the Ontology Score with Flickr Context Similarity (OS-FCS) which was used in 2010 <ref type="bibr" coords="3,158.70,473.09,9.96,8.74" target="#b1">[2]</ref>, the SR-Precision is able to incorporate ranked predictions instead of forcing the systems to provide binary decisions. However, this measure requires a normalization of classifier scores over different classifier outputs to deliver meaningful results. This requirement was not explicitly posed to the participants and therefore algorithms might not be optimally parameterized for this measure.</p><p>The concept-based retrieval task evaluates performance on a test collection with incomplete relevance judgments. All submissions of the participants are pooled by using a pool depth of 100 documents per topic and run. Finally, the runs are evaluated with the Mean uninterpolated Average Precision (MAP), Precision@10 (P@10), Precision@20 (P@20), Precision@100 (P@100), and conceptbased R-Precision (R-Prec) with the trec eval 8.1 program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Incorporation of user needs in the evaluation</head><p>Topics and visual concepts are strongly related to user needs and define the use cases of a system. While concepts are modality-independent (i.e., the event "birthday" might be detectable in the visual modality (birthday cake, people celebrating) as well as in the auditory modality (people singing a birthday song)), visual concepts are solely described by the visual content of a photo and are therefore language independent. This section introduces the visual concepts that are applied in the annotation task and the derivation of query topics for the retrieval task based on these concepts, query logs, and related work. Please note that the process of collecting images and defining visual concepts is different from related work. While usually the concept lexicon exists before images are collected, in the case of the ImageCLEF VCDT test collection, this process is decoupled and the images have been collected first. This approach is much closer to reality and poses new challenges, as objects are not necessarily centred in the image and the distribution of images per concept varies considerably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definition of visual concepts</head><p>The test collection for the annotation task contains manual annotations for 99 visual concepts. These concepts describe the scene (indoor, outdoor, landscape...), depicted objects (car, animal, person...), the representation of image content (portrait, graffiti, art...), events (travel, work...), or quality issues (overexposed, underexposed, blurry...). This year, a special focus is laid on the detection of sentiment concepts. All in all, 49 concepts of the 53 concepts used in 2009 <ref type="bibr" coords="4,447.13,357.67,10.52,8.74" target="#b0">[1]</ref> were utilized again. The concept Canvas as well as the concepts No Visual Season, No Visual Place, and No Visual Time were discarded in this year's challenge. The 41 concepts which were added in 2010 are all reused. In 2011, nine novel sentiment concepts were added to the test collection. For the definition of sentiments, we follow the approach of Russell <ref type="bibr" coords="4,309.58,417.44,9.96,8.74" target="#b5">[6]</ref>, who defines an emotional space with two dimensions (arousal and valence) on which emotional adjectives/sentiments can be placed. Valence spans from the negative pole "misery" to the positive pole "pleasure" on the x-dimension, while arousal spans from "passive" to "active" in the y-dimension. In this model, adjectives are grouped into eight affect concepts in circular order. The model was slightly adapted and an additional concept funny was included. The eight sentiments are structured according to their degrees in the circle as proposed by Russel. Partly, the wording is changed as to better fit the sentiments to describe images (e.g., an image cannot be excited or astonished, but it may look exciting to a human being). Starting with happy at 0 • , the circle is further composed of funny (about 30 • ), euphoric (70 • ), active (90 • ), scary (150 • ), unpleasant (180 • ), melancholic (210 • ), inactive, (270 • ), and calm/comforting (330 • ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Definition of topics for the concept-based retrieval task</head><p>Based on the visual concepts, 40 topics for the concept-based retrieval task were constructed. We conceived that each topic contains a different number of relevant images, and that the topics comprise a range of difficulty levels. For the definition of relevant topics, we followed two approaches: First, we adapted topics from the ImageCLEF Wikipedia retrieval task <ref type="bibr" coords="4,380.17,656.12,9.96,8.74" target="#b6">[7]</ref>, <ref type="bibr" coords="4,397.76,656.12,9.96,8.74" target="#b7">[8]</ref>, as these topics Table <ref type="table" coords="5,162.97,147.78,3.87,8.74">1</ref>: Topics of the concept-based retrieval task. Topics, labelled with WR and an abbreviated year, are taken or adapted from the ImageCLEF Wikipedia retrieval tasks. were designed based on web-query logs and because they range from simple to semantic (hence highly difficult) topics as described in <ref type="bibr" coords="6,377.59,130.95,9.96,8.74" target="#b8">[9]</ref>. A total of 17 topics were directly applicable to our test data. Second, we examined interesting queries for the test collection. Based on the output for each query, it was decided if the chosen topic comprises an adequate occurrence in the test collection. The 40 resulting topics and their source are shown in Table <ref type="table" coords="6,372.91,178.77,3.87,8.74">1</ref>. Sample images of the dataset were taken for clarification and provided as examples for the topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ground Truth Acquisition</head><p>The relevance assessments for the annotation task and the concept-based retrieval task were acquired with a crowdsourcing approach using Amazon Mechanical Turk<ref type="foot" coords="6,173.49,267.30,3.97,6.12" target="#foot_0">1</ref> (MTurk). MTurk is an online marketplace which distributes mini-jobs to an undefined crowd of people. At MTurk these mini-jobs are called HITs (Human Intelligence Tasks). The workers at MTurk, called turkers, can choose the HITs they would like to perform and submit the results to MTurk. The requester of the work collects the results from MTurk and approves or rejects the work of the turkers. Experiences with MTurk from ImageCLEF 2010 show the applicability of crowdsourcing for ground truth acquisition of image labels. This year, additional quality assurance mechanisms were incorporated to reduce the impact of spammers on the annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Design of the annotation HIT template</head><p>The assessment of the sentiment concepts was performed by asking the turkers what sentiments an image conveys. The HIT template includes a definition of sentiments, synonymous sentiments, and example images (see Figure <ref type="figure" coords="6,447.30,436.69,3.87,8.74" target="#fig_0">1</ref>). The definitions are derived from WordNet 3.0<ref type="foot" coords="6,309.77,447.07,3.97,6.12" target="#foot_1">2</ref> and the Free Dictionary<ref type="foot" coords="6,418.80,447.07,3.97,6.12" target="#foot_2">3</ref> . Each survey comprises ten images. The image is depicted on the left, while on the right the adapted circumplex model of Russel (see Section 3.1) is visualised, as illustrated on the example of one image in Figure <ref type="figure" coords="6,311.51,484.51,3.87,8.74">2</ref>. The option no sentiment should be chosen if no sentiment fits to the image. After selecting this checkbox, the turkers were asked to give a mandatory reason why no sentiment fits. We included this question in the survey to prevent turkers from clicking at this checkbox without thinking about the task. For all other sentiments, several choices could be selected at the same time. Additionally, the turkers were asked which reason let them decide for a sentiment: the motif or the overall impression of the image. They could choose on a five-point scale with the scales "motif" -"mostly motif" -"both equally" -"mostly overall impression" -"overall impression".</p><p>The HIT template includes an automated verification procedure. For all ten images that belong to one HIT, it is verified that the survey is completely filled out before the submission of the task works. In the case of missing answers, the turkers see the corresponding questions marked in red. This procedure ensures that it is not too easy to answer randomly and submit spam, and it helps reducing our work to filter out incomplete answers that need to be republished. While it does not assure that all random annotators are excluded (as turkers still can randomly answer each question), this at least assures that it also costs some amount of work to cheat compared to the time that is needed to answer honestly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Assessment statistics of the annotation task</head><p>The ground truth was acquired in different annotation batches. The pretest included 400 images of the training set arranged in surveys of ten images per HIT. Each HIT was annotated three times by a total of 22 turkers in an average an-Fig. <ref type="figure" coords="8,222.29,341.12,3.87,8.74">2</ref>: Example HIT for a complete annotation. notation time of 3 minutes and 12 seconds and paid with 0.05$. The purpose of the pretest was to understand if the template design and the task were understandable and if the turkers were able to solve the task. Results show that in about 50% of the images the turkers are agreeing on the sentiment (or choosing neighbouring sentiments) while in the other 50%, they chose opposite sentiments (like happy and melancholic). The rest of the training set of the Photo Annotation Task was annotated in altogether 4,225 HITs. Each HIT contained nine photos of the training set and one photo of the pretest as gold standard. The gold standard was built by a majority vote of the pretest images excluding the no sentiment concept and randomly placed into each HIT survey. Each HIT was annotated five times and rewarded with 0.07$. On average, they were completed in 2 minutes and 36 seconds by a total of 258 distinct turkers. The test set was assessed in 5,560 HITs which each included nine images and one gold standard image of the training set. Each image of the test set was annotated five times. The HITs of the test set were divided into two batches (in order not to pose too many HITs at the same time) and annotated by 156 distinct turkers. Each HIT was rewarded with 0.07$, again. For the first batch of 2,745 HITs, each HIT was annotated on average in 2 minutes and 8 seconds, while the 2,815 HITs of the second batch were annotated in average in 1 minute and 44 seconds.</p><p>The verification of the work of the turkers is difficult, as the task of sentiment annotation is very subjective. Therefore, we followed several strategies on how to compare the annotations. The verification of the HITs of the training and test set with the gold standard images lead to a direct acceptance of 3,204 and Fig. <ref type="figure" coords="9,213.99,334.60,3.87,8.74">3</ref>: Instructions in the HIT template for topic 4.</p><p>4,358 HITs, respectively, allowing a deviation of at most 90 • on the affect circle. For HITs that did not pass the gold standard test, we compared the results of the HIT to the four answers of other turkers for the same HIT. For all images of the HIT, the deviation to the annotations of the other HITs was computed and the HITs were accepted when the deviation was equal or less to 90 • on the affect circle per image. A total of seven out of the 10 images had to fit. With this procedure all remaining HITs could be accepted.</p><p>The final construction of the ground truth considers the majority vote for each image. In the case that no clear answer was given, we decided to discard any sentiment information for that image. In total, about 15% of the training set images and 14% of the test set images have no sentiment information. Interestingly, the no sentiment option was rarely chosen by the turkers. For none of the images of the training set and only for one image of the test set a majority of people decided for this concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Design of the topic HIT templates</head><p>In the relevance assessment of the concept-based retrieval task, the turkers were asked to mark all relevant images on the HIT template for a given topic. Each HIT template includes a definition of the topic and example images (see Figure <ref type="figure" coords="9,152.38,620.25,3.87,8.74">3</ref>). A HIT contains 22 images plus two gold standards images, which were used as a means of reliability control for the assessments. For each topic, we selected one image that fits the definition and one image that is not relevant for the given topic. Special attention was taken in the design of the irrelevant Fig. <ref type="figure" coords="10,153.45,298.53,3.87,8.74">4</ref>: Sample images that are not in the scope of the topic fish in the water (top) and images that are covered by the topic (bottom). images per topic. Instead of using images that are clearly out of the scope of the topic, images that match the meaning of the topic quite close, but not exactly, were chosen; see Figure <ref type="figure" coords="10,240.91,380.11,4.98,8.74">4</ref> for sample images of the topic fish in the water. The gold images were placed randomly in the HIT templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Assessment statistics of the retrieval task</head><p>The number of HITs per topic is dependent on the total number of distinct images that were retrieved by the runs of the participants. Each HIT contained 24 images and was assessed by three turkers; so in total 7,868 HITs were processed. Accepted HITs were paid with 0.03$. Each topic was processed by at least five (topic 29) and at most 41 (topic 12) distinct turkers. The average grading time varies per topic between 31 seconds (topic 25) and 1 minute and 19 seconds (topic 15). To increase the reliability of the relevance judgments, the results were subjected to a post screening procedure. The assessments of a turker were rejected if the gold standard images were not marked correctly. These HITs were published again until for all HITs three reliable results were available. In the next step, the assessments of the three turkers per HIT were compared with each other. As the task of selecting relevant images for a topic is a subjective task, its verification is difficult. In an additional step, we visualized the images that were assessed as relevant and estimated the number of false assignments and missing assignments. Depending on these results, the number of votes that were necessary to define an image as relevant were chosen for each topic. For most topics, a majority vote from at least two of the three assessors was necessary. A minority vote was used for only five topics, while all assessors had to agree on relevance for three topics.</p><p>Altogether, 48 groups registered for the challenge. 42 groups signed the license agreement and were provided with the test collections. For the annotation task, 18 groups submitted results in altogether 79 runs. The number of runs was restricted to a maximum of 5 runs per group. In total, there were 46 submissions using only visual information, 8 submissions using only textual information, and 25 submissions utilising multi-modal approaches. For the retrieval task, 4 groups submitted results in a total of 31 runs. The maximum number of runs per group was set to 10. The submissions include 14 visual runs, 7 textual runs, and 10 multi-modal runs. The runs can be subdivided into 16 runs that retrieved all images in a completely automated fashion and 15 runs that included a manual intervention in the query generation step or relevance feedback. All participants that submitted to the retrieval task also took the challenge in the annotation task. The teams and their approaches are briefly introduced in the following:</p><p>BPACAD <ref type="bibr" coords="11,202.98,297.43,16.80,8.77" target="#b9">[10]</ref>: The team of the Computer and Automation Research Institute of the Hungarian Academy of Science submitted one textual, two visual and three multi-modal runs to the annotation task. Their approach is based on a kernel weighting procedure using visual Fisher kernels and a Flickr-tag based Jensen-Shannon divergence based kernel. Classification uses a linear SVM trained for each concept separately.</p><p>BUFFALO: The team of the University at Buffalo, New York, USA submitted five visual runs for the annotation task. They follow two approaches: the first considers a local linear coordinate method to learn concepts with a regression method. The second uses a combination of GIST and colour features and classifies the images by a neural network.</p><p>CAEN <ref type="bibr" coords="11,186.27,428.94,16.80,8.77" target="#b10">[11]</ref>: The group of University of Caen, France participated with four visual runs in the annotation task. The proposed approach uses visual image features, such as SIFT, HOG, Texton, LAB, SSIM, and Canny, and aggregated them by a Bag-of-Words (BoW) model into a global histogram. Fisher Vectors and contextual information were used as enhancement of the BoW-models. The classification considers SVM models trained for each concept separately.</p><p>CEALIST <ref type="bibr" coords="11,203.47,500.67,16.80,8.77" target="#b11">[12]</ref>: The team from the Laboratory of Vision and Content Engineering, France submitted one textual, one visual, and three multi-modal runs to the annotation task. The textual descriptor is based on semantic similarity between tags and visual concepts. Two distances were used: one based on the Wordnet ontology and one based on social networks. The visual component considers various local and global features, such as Fisher vectors as well as colour and edge features. Late fusion was used to combine visual and textual modalities.</p><p>DBIS: The team of the Technical University of Cottbus, Germany submitted five runs in the visual configuration to the annotation task. They use various features and investigate the influence of several parameters in clustering on the annotation performance.</p><p>HHI <ref type="bibr" coords="11,176.99,632.18,16.80,8.77" target="#b12">[13]</ref>: The team of Fraunhofer HHI, Berlin, Germany submitted five visual runs to the annotation task. Their approach is based on the BoW model. A feature fusion of the opponent SIFT descriptor and the GIST descriptor was done in order to improve the classification performance of scene-based concepts. HHI investigates a sampling of informative images in the training procedure, which resulted in qualitative as well as runtime performance gains. A post-classification processing step is incorporated, which refines classification results based on rules of inference and exclusion between concepts.</p><p>IDMT <ref type="bibr" coords="12,185.24,180.59,16.80,8.77" target="#b13">[14]</ref>: The group of Fraunhofer IDMT, Ilmenau, Germany submitted one textual and four multi-modal runs to the annotation task. Their approach focuses on the fusion of multi-modal information and the exploitation of Flickr user tags. As visual features, they employ RGB-SIFT features in a codebook approach and classify the images with a one-against-all strategy using a SVM with RBF kernel.</p><p>ISIS <ref type="bibr" coords="12,176.49,254.17,16.80,8.77" target="#b14">[15]</ref>: The Intelligent Systems Lab of the University of Amsterdam, The Netherlands participated with five runs in the annotation task (3V, 2M) and ten runs (10V) in the retrieval task. All runs of the annotation task use several colour SIFT features with Harris-Laplace and dense sampling, and apply the SVM classifier. The multi-modal runs further include binary vectors for the most frequent Flickr user tags. In the retrieval task, three runs are computed completely automated and seven include a manual intervention by following two approaches. In the fully automated runs, a combination of the provided positive example images and random irrelevant images were used to train the concept detector. For the human topic mapping, a human reads the topic and then selects the relevant concept(s). The probability scores of these concepts are then combined using either summation or multiplication. In the human topic inspection approach, relevance feedback was used to improve results.</p><p>LAPI <ref type="bibr" coords="12,181.19,411.44,16.80,8.77" target="#b15">[16]</ref>: The group of Laboratorul de Analiza si Prelucrarea Imaginilor, Universitatea Politehnica Bucuresti, Bucharest, Romania submitted two runs using a visual-only approach. They combine colour and structural features and adopt a Linear Discriminant Analysis for classification. Post-processing considers joint probabilities of concept occurrences in the training set for label elimination.</p><p>LIRIS <ref type="bibr" coords="12,183.40,473.06,16.80,8.77" target="#b16">[17]</ref>: The group of Université de Lyon, CNRS, France participated in the annotation task with two textual, one visual, and two multi-modal runs. They consider two textual descriptors: one is based on a semantic distance between the text and an emotional dictionary, the other one contains the valence and arousal meanings by making use of the Affective Norms for English Words dataset. In the visual approaches, different visual features including colour, texture, shape, and high level aesthetic features are applied. Performance is compared using different fusion strategies as well as Adaboost and SVM classifiers.</p><p>MEIJI <ref type="bibr" coords="12,187.81,570.55,16.80,8.77" target="#b17">[18]</ref>: The group of Meiji University, Kanagawa, Japan submitted five runs (2V, 1T, 2M) to the annotation task and ten completely automated runs (2V, 2T, 6M) to the retrieval task. Their approach is based on visual word co-occurrence using the BoW model and global colour features as well as textual features derived by tf-idf weigths of Flickr user tags. Classification is performed by an adaptation of the so-called confabulation model.</p><p>MLKD <ref type="bibr" coords="12,190.76,644.13,16.81,8.77" target="#b18">[19]</ref>: The Machine Learning and Knowledge Discovery group of the Aristotle University of Thessaloniki, Greece participated in the annotation task with five runs (1V, 1T, 3M) and in the retrieval task with two automated and eight semi-automated runs (2V, 4T, 4M). They approach the photo annotation challenge with multi-label learning algorithms based on Random Forests as base classifier. The visual features consider seven local descriptors with two sampling strategies. The textual models are based on a Boolean BoW representation including word stemming, stop words removal, and feature selection. The multi-modal approach considers a hierarchical late-fusion of the modalities. For the concept-based retrieval task two approaches were used: one based on the concept relevance scores in a manual configuration and one automated approach which is based solely on the sample images using textual information.</p><p>MRIM <ref type="bibr" coords="13,188.17,243.13,16.80,8.77" target="#b19">[20]</ref>: The team of Grenoble University, France submitted four runs (3V, 1M) to the annotation task. Classification considers multiple SVM classifiers with RBF kernel. In the visual runs, several global and local colour and texture descriptors are applied and dimension reduction techniques are investigated. The multi-modal run additionally considers Flickr user tags as simple textual features in a late fusion of SVM classifier scores.</p><p>MUFIN <ref type="bibr" coords="13,193.21,319.47,16.80,8.77" target="#b20">[21]</ref>: The Faculty of Informatics, Masaryk University, Brno, Czech Republic participated with four multi-modal runs in the annotation task. Their approach is based on a free-text annotation system that assigns arbitrary words to web images by visual and textual neighbour searching. For the textual search, the EXIF data and image descriptions were used, while the visual search considers different MPEG-7 descriptors. The search considers the Profimedia dataset to find the nearest neighbours and transfers its annotations to the ImageCLEF test collection including a removal of stopwords and names. The resulting words were transformed into the fixed set of 99 visual concepts of the annotation task with the help of WordNet and the provided ontology.</p><p>NII <ref type="bibr" coords="13,171.90,443.63,16.80,8.77" target="#b21">[22]</ref>: The team of the National Institute of Informatics, Tokyo, Japan participated with five visual runs in the annotation task. Their models are using global and local features. As for global features, colour moments, colour histogram, edge orientation histogram, and local binary patterns are applied. As for local features, keypoint detectors such as Harris Laplace, Hessian Laplace, Harris Affine, and Dense Sampling are used to extract SIFT-descriptors. Classification is performed with a SVM classifier.</p><p>REGIMVID <ref type="bibr" coords="13,217.53,531.93,16.80,8.77" target="#b22">[23]</ref>: The research group on Intelligent Machines, University of Sfax, Tunisia submitted one textual run to the annotation task and one textual, automated run to the retrieval task. Their approach focuses on the exploration of Flickr tags to extract contextual relationships of tag relations. Therefore, two types of contextual graphs are modeled: an inter-concepts graph and a concept-tags graph.</p><p>TUBFI <ref type="bibr" coords="13,190.20,608.27,16.80,8.77" target="#b23">[24]</ref>: The joint submission of the Machine Learning Group, Berlin Insitute of Technology and Fraunhofer FIRST Berlin, Germany consists of four visual and one multi-modal run to the annotation task. Classification considers non-sparse multiple kernel learning and multi-task learning. Different extensions of the BoW models with respect to sampling strategies and BoW mappings were proposed. The multi-modal run further considers frequent Flickr user tags based on a soft mapping for textual BoWs and Markov random walks over tags. UNIKLU: The team of the Institute of Information Technology, Alpen-Adria University, Klagenfurt, Austria participated with four visual runs in the annotation task. They made use of the LIRE framework and applied several features such as SIFT, SURF, MSER, CEDD, FCTH, and colour histograms and classified the images with a linear SVM. Two of the runs incorporate an automated post-processing of the classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Annotation Task: Results</head><p>This section illustrates the results for the annotation subtask. First, the overall results of all teams are presented, independent of the configuration. In the following subsections the results per configuration are highlighted. The results for all runs can be found at the Photo Annotation Task website <ref type="foot" coords="14,398.00,610.78,3.97,6.12" target="#foot_3">4</ref> .</p><p>The task was solved best with a MiAP of 0.443 (TUBFI), followed by a MiAP of 0.437 (LIRIS) as illustrated in Table <ref type="table" coords="14,311.87,636.41,3.87,8.74" target="#tab_1">2</ref>. Both runs make use of multi-modal Table <ref type="table" coords="15,161.53,127.36,3.87,8.74">3</ref>: Summary of the results for the evaluation per example. The table shows the F-Ex, SR-Precision, and run configuration for the best run per group.</p><p>Team Rank F-Ex Config.</p><p>Team Rank SR-Prec. Config. information. Table <ref type="table" coords="15,217.78,397.70,4.98,8.74">3</ref> depicts the overall rankings for the results of the evaluation per example. The best results in terms of F-Ex are achieved in a multi-modal configuration with 0.622 (ISIS), followed by 0.600 F-Ex (CAEN) which makes use of a visual configuration. In terms of SR-Precision, the best run scores with 0.742 SR-Precision (ISIS) in a multi-modal run, followed by 0.729 SR-Precision (BPACAD) in a visual run.</p><formula xml:id="formula_0" coords="15,191.06,175.78,16.89,7.86">ISIS</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results for the visual configuration</head><p>Table <ref type="table" coords="15,163.36,503.00,4.98,8.74">4</ref> shows the results of the best run of each group that participated in the visual configuration evaluated with all three evaluation measures. The best results in the visual configuration are achieved by the TUBFI team in terms of MiAP, closely followed by the team CAEN (0.388 vs 0.382 MiAP). In the example-based evaluation, ISIS scored best for both measures followed by CAEN (F-Ex) and BPACAD (SR-Precision).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results for the textual configuration</head><p>The results for the textual runs are presented in Table <ref type="table" coords="15,381.53,608.30,3.87,8.74">5</ref>. The best run scores with 0.346 MiAP (BPACAD), followed by 0.326 MiAP (IDMT, MLKD). In the example-based evaluation the best run scores with 0.525 F-Ex (IDMT) and 0.677 SR-Precision (IDMT) followed by 0.506 F-Ex (MLKD) and 0.676 SR-Precision (CEALIST, LIRIS).</p><p>Table <ref type="table" coords="16,163.22,127.36,3.87,8.74">4</ref>: Summary of the results for the evaluation per concept in the visual configuration. The table shows the best run per group.</p><p>Team Rank MiAP Team Rank F-Ex Team Rank SR-Prec. </p><formula xml:id="formula_1" coords="16,161.37,175.78,29.43,7.86">TUBFI</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results for the multi-modal configuration</head><p>Table <ref type="table" coords="16,161.78,385.85,4.98,8.74" target="#tab_4">6</ref> depicts the results for the best multi-modal configuration of each group. As already stated, the run of TUBFI achieves the best overall results in terms of MiAP. In the example-based evaluation, ISIS scores best overall with 0.622 F-Ex and 0.742 SR-Precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Comparison of achievements with different information sources</head><p>Last year only two runs considered the textual configuration. In contrast, this year eight textual runs were submitted by seven teams. This allows for a more reliable analysis of the performance of textual runs in image annotation. The Table <ref type="table" coords="16,162.80,536.82,3.87,8.74">5</ref>: Summary of the results for the evaluation per concept in the textual configuration. The table shows the best run per group.</p><p>Team Rank MiAP Team Rank F-Ex Team Rank SR-Prec. performance of textual runs is close to the results that can be achieved in the visual configuration. The best visual run achieves a MiAP of 0.388 in contrast to the best textual run, which scores with a MiAP of 0.346. The difference of 4.2% is rather small, especially when considering that not for all images EXIF data and Flickr user tags exist. In the example-based evaluation, the difference between visual and textual runs is more significant. Visual runs score better by about 9% in terms of F-Ex and about 6% in terms of SR-Precision. Results in the multimodal configuration outperform classification with single modality information in the visual configuration by 5.5% and the textual configuration by about 10% in terms of MiAP. For the example-based measures F-Ex and SR-Precision, differences are very small with 1% for the visual configuration. Comparing the multi-modal to the textual configuration, differences are significant and lie by about 10% and 6.5% for F-Ex and SR-Precision, respectively.</p><formula xml:id="formula_2" coords="16,148.98,585.24,39.29,7.86">BPACAD</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Annotation performance per concept</head><p>In Table <ref type="table" coords="17,177.64,500.70,3.87,8.74">7</ref>, the results for each concept are summarized independent of the configuration. On average, the concepts could be detected with a MiAP of 0.48 considering the best run per concept out of all configurations. In general, 79 concepts were best detected with a multi-modal approach, 17 concepts were detected best with a visual approach, and 3 concepts were detected best by a textual approach. High performance is achieved for the concepts Neutral-Illumination, No-Persons, No-Blur, and Outdoor. Following, the concepts Sky, Day, Clouds, and Plants were annotated with high scores. The worst annotation quality was achieved for the concept abstract followed by the concepts work, graffiti, technical, old-person, and boring.</p><p>In the evaluation in 2010, a great difference in prediction quality among the concepts from 2009 and the ones newly introduced in 2010 of 0.57 MiAP and 0.37 MiAP could be seen. This difference is still present in this evaluation cycle. The concepts from 2009 (number 1-49) could be detected with a MiAP Table <ref type="table" coords="18,164.33,169.20,3.87,8.74">7</ref>: This table presents the best annotation performance per concept, achieved by any team in any configuration, in terms of iAP. It lists the concept name, the iAP score, the team, and the configuration of the run. of 0.57 considering the best prediction for each concept out of all runs, while the 2010 concepts (numbers 50-90) improved minimally to a MiAP of 0.38. The new sentiment concepts (numbers 91-99) can be detected with a MiAP of 0.39. Although these are arguably very subjective concepts, the detection algorithms are capable of identifying a strong trend of sentiments correctly. Especially, the sentiments calm and inactive could be detected very well, while the sentiments scary and euphoric were annotated worst. However, one has to note that the 2010 concepts occur on average in 7.9% of the training set images, while the 2009 and 2011 concepts are visible in 18% and 14% of the training set, respectively. Therefore, the algorithms had more example images to learn the sentiments in comparison to the more object-based concepts introduced in 2010.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Concept-based Retrieval Task: Results</head><p>In the following, the results of the concept-based retrieval task are presented and discussed. The participation of four teams was lower with respect to the annotation task. Despite, 31 runs have been submitted in all configurations, consisting of 10 multi-modal, 7 textual, and 14 visual runs. Approximately half of the systems used a completely automated processing.</p><p>Table <ref type="table" coords="19,178.04,367.94,4.98,8.74">8</ref> depicts all runs and indicates the configuration and the degree of automation of each run. Results are sorted in terms of MAP. The MAP value ranges from 0.1640 to 0.0013. Overall, the task was solved best with a MAP of 0.164 by the MLKD group, who used a multi-modal configuration with manual query formulations. It can clearly be seen that the approaches using manual processing achieve better results than the automated versions. The best performing automated run achieves a MAP of 0.0849 (MLKD). The multi-modal and textual configurations of MLKD work significantly better than their visual ones. MLKD provides also the best MAP value for a textual configuration, which is only 0.0094 points lower than the overall best run. The best working visual configuration, also with manual query formulation, was submitted by ISIS, achieving a MAP of 0.0997. MEIJI provided solely automated runs, for which the multimodel approaches outperform the textual and visual runs. REGIMVID provided one automated, text-based configuration, which achieves a MAP value of 0.0042.</p><p>Table <ref type="table" coords="19,178.73,536.57,4.98,8.74">9</ref> presents the best run per team in the three configurations. If a team submitted an automated and a manual run in the same configuration, the results for both runs are illustrated. The direct comparison of automated and manual runs per team shows that the manual runs work best, independent from the configuration (textual, visual, multi-modal). This is mostly apparent in the big difference -nearly factor two -of the textual configuration of MLKD. The results of the two approaches submitted by ISIS support this interpretation. The manually processed run outperforms the automated approach with a MAP of 0.0997 vs. 0.0430. The performance difference between the two multi-modal configurations submitted by MKLD and MEIJI can partially be explained by the different degree of automation. MEIJI uses a fully automated system resulting Table <ref type="table" coords="20,449.63,143.85,4.37,7.75">8</ref>: The table shows the results for all runs sorted by MAP. Automated runs are abbreviated with "A", while manual query formulations are abbreviated with "M" in the column Automation.  <ref type="table" coords="21,161.47,127.36,3.87,8.74">9</ref>: This table presents the best results per team in the single configurations differentiated by the degree of automation. In the case a team submitted an automated and a manual run in a particular configuration, both runs are listed. Results are sorted in terms of MAP.</p><p>Team MAP P@10 P@20 P@100 R- The boxplots of the MAP scores of all approaches per topic in Figure <ref type="figure" coords="21,475.61,428.97,4.98,8.74" target="#fig_2">5</ref> allow for a more detailed examination. Most topics show a wide variation among the obtained MAP values, which indicates the differences in performance per topic. This can be seen, e.g., for the topics 33 (cars and motion blur ) and 28 (fireworks). For topic 33, the lowest MAP scores are 0. The highest value (0.5218) for this topic is achieved by ISIS with an automated, visual configuration. MAP scores for topic 28 are in the range of 0 to 0.423. A closer look reveals that only for two topics (33 and 5) MAP values over 0.5 are reached. This is most surprising in the case of topic 5 (riders on horse), due to the consistent low MAP values of the other approaches. The six runs performing significantly better than the rest are provided by MLKD and use textual and multi-modal information. The figure also shows that the MAP scores for topic 18 (on the far right) are homogeneously low. It can be concluded that all approaches had great difficulties identifying relevant images for female old person. The topics 13, 21 and 26 were also hard to identify (MAP close to 0) for most of the approaches, but better performing outliers clearly exist. For topic 13 (female person(s) doing sports) some approaches of MLKD reach values above 0.14. The same observation, with values close to 0.1, applies to topic 21 (scary dog(s)). The most striking effect can be observed for houses in mountains (26): Only two ISIS approaches were able to achieve significantly higher MAP values than 0. The best sentiment topic  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>The ImageCLEF 2011 Photo Annotation and Concept-based Retrieval Tasks posed two image analysis challenges that could be solved with three general configurations: textual-based analysis, visual-based analysis, and multi-modal analysis. The aim of the annotation task was to automatically annotate images with 99 concepts in a multi-label scenario. The task attracted a considerable number of international teams with a final participation of 18 teams that submitted a total of 79 runs. The results show that the annotation task could be solved reasonably well, with the best multi-modal run achieving a MiAP of 0.443 in the multi-modal configuration, a MiAP of 0.388 in the visual configuration, and a MiAP of 0.346 in the textual configuration. For the evaluation per example, the best multi-modal run achieves 0.62 F-Ex, the best visual run scores with 0.61 F-Ex, and the best textual run with 0.53 F-Ex. All in all, the multi-modal approaches got the best scores for 79 out of 99 concepts, followed by 17 concepts that could be detected best with the visual approach and 3 that won with a textual approach. In general, the multi-modal approaches outperformed visual and textual configurations for nearly all performance measures of the teams that submitted results for more than one configuration. The concept-based retrieval task asked participants to retrieve the most relevant images given certain topics. The topics were constructed based on user needs and query logs, and consist of a Boolean connection of several visual concepts. In total, 4 teams participated in this novel challenge and submitted 31 runs. 10 runs belong to the multi-modal configuration, 14 runs were submitted in the visual configuration, and 7 runs are based on textual information. The best multi-model configuration obtained a MAP value of 0.164, the textual configuration scored best with 0.1546 MAP, and the best visual run achieves a score of 0.0997 MAP. The task was solved by 16 completely automated approaches and 14 runs which include manual intervention. It was observed that most manually processed runs work best, independent from the configuration (textual, visual, or multi-modal). They achieve MAP values in the range of 0.164 and 0.0295, whereas the automated solutions range between scores of 0.0849 and 0.0013 MAP. A closer examination showed that all approaches had great difficulties to identify relevant images for the topic female old person. Also the topics 5, 13, 21, and 26 were hard to identify as well, but here some approaches were able to reach higher MAP values. Especially, the topic riders on horse shows very strong outliers with high MAP values above 0.5. The obtained MAP scores of the remaining topics vary widely which points to a large variation in the difficulty level of topics. Some configurations are able to achieve MAP values higher than 0.5 for individual topics. Considering the topic female old person, all runs show nearly the same low performance. This seems to be an extremely critical topic for concept-based image retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,175.34,477.31,264.68,8.74;7,138.22,115.83,338.91,349.95"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Definition of sentiment concepts in the HIT template.</figDesc><graphic coords="7,138.22,115.83,338.91,349.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="20,415.54,342.05,6.14,16.71;20,415.54,367.61,6.14,21.35;20,415.54,397.80,6.14,22.14;20,415.54,428.80,6.14,22.14;20,415.54,459.79,6.14,26.66;20,415.54,495.29,6.14,27.73;20,415.54,528.83,6.14,47.25;20,415.54,578.87,6.14,54.30;20,402.57,202.80,6.12,24.19;20,402.57,229.84,6.12,51.62;20,402.57,287.19,6.12,17.17;20,402.57,307.22,6.12,23.16;20,402.57,333.24,6.12,28.55;20,402.57,367.23,6.12,22.11;20,402.57,397.82,6.12,22.11;20,402.57,428.81,6.12,22.11;20,402.57,462.05,6.12,22.11;20,402.57,498.09,6.12,22.11;20,402.57,548.87,6.12,7.17;20,402.57,602.43,6.12,7.17;20,394.60,209.82,6.12,24.19;20,394.60,236.86,6.12,51.62;20,394.60,294.21,6.12,17.17;20,394.60,314.24,6.12,16.15;20,394.60,333.24,6.12,28.55;20,394.60,367.23,6.12,22.11;20,394.60,397.82,6.12,22.11;20,394.60,428.81,6.12,22.11;20,394.60,462.05,6.12,22.11;20,394.60,498.09,6.12,22.11;20,394.60,548.87,6.12,7.17;20,394.60,603.17,6.12,5.69;20,386.63,202.80,6.12,24.19;20,386.63,229.84,6.12,51.62;20,386.63,287.19,6.12,17.17;20,386.63,307.22,6.12,23.16;20,386.63,333.24,6.12,28.55;20,386.63,367.23,6.12,22.11;20,386.63,397.82,6.12,22.11;20,386.63,428.81,6.12,22.11;20,386.63,462.05,6.12,22.11;20,386.63,498.09,6.12,22.11;20,386.63,548.87,6.12,7.17;20,386.63,602.43,6.12,7.17;20,378.66,206.77,6.12,24.19;20,378.66,233.81,6.12,51.62;20,378.66,291.16,6.12,17.17;20,378.66,311.19,6.12,23.16;20,378.66,337.21,6.12,24.58;20,378.66,367.23,6.12,22.11;20,378.66,397.82,6.12,22.11;20,378.66,428.81,6.12,22.11;20,378.66,462.06,6.12,22.11;20,378.66,498.09,6.12,22.11;20,378.66,548.87,6.12,7.17;20,378.66,602.43,6.12,7.17;20,370.69,213.79,6.12,24.19;20,370.69,240.84,6.12,51.62;20,370.69,298.18,6.12,17.17;20,370.69,318.21,6.12,16.15;20,370.69,337.21,6.12,24.58;20,370.69,367.23,6.12,22.11;20,370.69,397.82,6.12,22.11;20,370.69,428.81,6.12,22.11;20,370.69,462.06,6.12,22.11;20,370.69,498.09,6.12,22.11;20,370.69,548.87,6.12,7.17;20,370.69,603.17,6.12,5.69;20,362.72,202.80,6.12,24.19;20,362.72,229.84,6.12,51.62;20,362.72,287.19,6.12,21.14;20,362.72,311.19,6.12,23.16;20,362.72,337.21,6.12,24.58;20,362.72,367.23,6.12,22.11;20,362.72,397.82,6.12,22.11;20,362.72,428.81,6.12,22.11;20,362.72,462.06,6.12,22.11;20,362.72,498.09,6.12,22.11;20,362.72,548.87,6.12,7.17;20,362.72,602.43,6.12,7.17;20,354.75,159.05,6.12,14.56;20,354.75,176.46,6.12,51.62;20,354.75,233.81,6.12,127.99;20,354.75,367.23,6.12,22.11;20,354.75,397.82,6.12,22.11;20,354.75,428.81,6.12,22.11;20,354.75,462.06,6.12,22.11;20,354.75,498.10,6.12,22.11;20,354.75,548.87,6.12,7.17;20,354.75,603.08,6.12,5.88;20,346.78,171.76,6.12,14.56;20,346.78,189.17,6.12,51.62;20,346.78,246.52,6.12,115.28;20,346.78,367.23,6.12,22.11;20,346.78,397.82,6.12,22.11;20,346.78,428.81,6.12,22.11;20,346.78,462.06,6.12,22.11;20,346.78,498.10,6.12,22.11;20,346.78,548.87,6.12,7.17;20,346.78,603.08,6.12,5.88;20,338.81,185.17,6.12,14.56;20,338.81,202.59,6.12,51.62;20,338.81,259.93,6.12,101.86;20,338.81,367.23,6.12,22.11;20,338.81,397.82,6.12,22.11;20,338.81,428.81,6.12,22.11;20,338.81,462.05,6.12,22.11;20,338.81,498.09,6.12,22.11;20,338.81,548.87,6.12,7.17;20,338.81,603.08,6.12,5.88;20,330.84,172.23,6.12,14.56;20,330.84,189.64,6.12,51.62;20,330.84,246.99,6.12,114.81;20,330.84,367.23,6.12,22.11;20,330.84,397.82,6.12,22.11;20,330.84,428.81,6.12,22.11;20,330.84,462.06,6.12,22.11;20,330.84,498.10,6.12,22.11;20,330.84,548.87,6.12,7.17;20,330.84,603.08,6.12,5.88;20,322.87,197.88,6.12,14.56;20,322.87,215.30,6.12,51.62;20,322.87,272.64,6.12,89.15;20,322.87,367.23,6.12,22.11;20,322.87,397.82,6.12,22.11;20,322.87,428.81,6.12,22.11;20,322.87,462.05,6.12,22.11;20,322.87,498.09,6.12,22.11;20,322.87,548.87,6.12,7.17;20,322.87,603.08,6.12,5.88;20,314.90,145.59,6.12,24.19;20,314.90,172.64,6.12,51.62;20,314.90,229.98,6.12,17.17;20,314.90,250.01,6.12,16.15;20,314.90,269.01,6.12,61.37;20,314.90,333.24,6.12,28.55;20,314.90,367.23,6.12,22.11;20,314.90,397.82,6.12,22.11;20,314.90,428.81,6.12,22.11;20,314.90,462.05,6.12,22.11;20,314.90,498.09,6.12,22.11;20,314.90,549.51,6.12,5.88;20,314.90,603.18,6.12,5.69;20,306.93,171.11,6.12,14.56;20,306.93,188.53,6.12,51.62;20,306.93,245.87,6.12,115.92;20,306.93,367.23,6.12,22.11;20,306.93,397.82,6.12,22.11;20,306.93,428.82,6.12,22.11;20,306.93,462.06,6.12,22.11;20,306.93,498.09,6.12,22.11;20,306.93,548.87,6.12,7.17;20,306.93,603.08,6.12,5.88;20,298.95,149.56,6.12,24.19;20,298.95,176.61,6.12,51.62;20,298.95,233.95,6.12,17.17;20,298.95,253.98,6.12,16.15;20,298.95,272.99,6.12,61.37;20,298.95,337.21,6.12,24.58;20,298.95,367.23,6.12,22.11;20,298.95,397.82,6.12,22.11;20,298.95,428.81,6.12,22.11;20,298.95,462.06,6.12,22.11;20,298.95,498.09,6.12,22.11;20,298.95,549.51,6.12,5.88;20,298.95,603.18,6.12,5.69;20,290.98,188.65,6.12,14.56;20,290.98,206.07,6.12,51.62;20,290.98,263.41,6.12,98.38;20,290.98,367.23,6.12,22.11;20,290.98,397.82,6.12,22.11;20,290.98,428.81,6.12,22.11;20,290.98,462.06,6.12,22.11;20,290.98,498.09,6.12,22.11;20,290.98,548.87,6.12,7.17;20,290.98,603.08,6.12,5.88;20,283.01,228.07,6.12,22.35;20,283.01,253.27,6.12,51.62;20,283.01,310.62,6.12,51.17;20,283.01,367.23,6.12,22.11;20,283.01,397.82,6.12,22.11;20,283.01,428.82,6.12,22.11;20,283.01,462.06,6.12,22.11;20,283.01,498.09,6.12,22.11;20,283.01,549.52,6.12,5.88;20,283.01,602.44,6.12,7.17;20,275.04,202.71,6.12,14.56;20,275.04,220.13,6.12,51.62;20,275.04,277.47,6.12,84.32;20,275.04,367.23,6.12,22.11;20,275.04,397.82,6.12,22.11;20,275.04,428.81,6.12,22.11;20,275.04,462.06,6.12,22.11;20,275.04,498.10,6.12,22.11;20,275.04,549.51,6.12,5.88;20,275.04,603.08,6.12,5.88;20,267.07,227.66,6.12,22.35;20,267.07,252.87,6.12,51.62;20,267.07,310.21,6.12,51.58;20,267.07,367.23,6.12,22.11;20,267.07,397.82,6.12,22.11;20,267.07,428.81,6.12,22.11;20,267.07,462.06,6.12,22.11;20,267.07,498.09,6.12,22.11;20,267.07,549.51,6.12,5.88;20,267.07,602.44,6.12,7.17;20,259.10,233.54,6.12,22.35;20,259.10,258.75,6.12,51.62;20,259.10,316.09,6.12,45.70;20,259.10,367.23,6.12,22.11;20,259.10,397.82,6.12,22.11;20,259.10,428.81,6.12,22.11;20,259.10,462.06,6.12,22.11;20,259.10,498.09,6.12,22.11;20,259.10,549.51,6.12,5.88;20,259.10,602.44,6.12,7.17;20,251.13,206.68,6.12,14.56;20,251.13,224.10,6.12,51.62;20,251.13,281.44,6.12,80.35;20,251.13,367.23,6.12,22.11;20,251.13,397.82,6.12,22.11;20,251.13,428.81,6.12,22.11;20,251.13,462.06,6.12,22.11;20,251.13,498.10,6.12,22.11;20,251.13,549.51,6.12,5.88;20,251.13,603.08,6.12,5.88;20,243.16,204.04,6.12,24.19;20,243.16,231.09,6.12,51.62;20,243.16,288.43,6.12,17.17;20,243.16,308.46,6.12,21.92;20,243.16,333.24,6.12,28.55;20,243.16,367.23,6.12,22.11;20,243.16,397.82,6.12,22.11;20,243.16,428.81,6.12,22.11;20,243.16,462.05,6.12,22.11;20,243.16,498.09,6.12,22.11;20,243.16,548.87,6.12,7.17;20,243.16,603.08,6.12,5.88;20,235.19,230.00,6.12,22.35;20,235.19,255.20,6.12,51.62;20,235.19,312.55,6.12,49.25;20,235.19,367.23,6.12,22.11;20,235.19,397.82,6.12,22.11;20,235.19,428.81,6.12,22.11;20,235.19,462.06,6.12,22.11;20,235.19,498.09,6.12,22.11;20,235.19,549.51,6.12,5.88;20,235.19,602.44,6.12,7.17;20,227.22,229.59,6.12,22.35;20,227.22,254.80,6.12,51.62;20,227.22,312.14,6.12,49.66;20,227.22,367.23,6.12,22.11;20,227.22,397.82,6.12,22.11;20,227.22,428.81,6.12,22.11;20,227.22,462.06,6.12,22.11;20,227.22,498.10,6.12,22.11;20,227.22,549.51,6.12,5.88;20,227.22,602.43,6.12,7.17;20,219.25,235.47,6.12,22.35;20,219.25,260.68,6.12,51.62;20,219.25,318.02,6.12,43.77;20,219.25,367.23,6.12,22.11;20,219.25,397.82,6.12,22.11;20,219.25,428.81,6.12,22.11;20,219.25,462.05,6.12,22.11;20,219.25,498.09,6.12,22.11;20,219.25,549.51,6.12,5.88;20,219.25,602.43,6.12,7.17;20,211.28,208.01,6.12,24.19;20,211.28,235.06,6.12,51.62;20,211.28,292.40,6.12,17.17;20,211.28,312.43,6.12,21.92;20,211.28,337.21,6.12,24.58;20,211.28,367.23,6.12,22.11;20,211.28,397.82,6.12,22.11;20,211.28,428.81,6.12,22.11;20,211.28,462.06,6.12,22.11;20,211.28,498.09,6.12,22.11;20,211.28,548.87,6.12,7.17;20,211.28,603.08,6.12,5.88;20,203.31,241.35,6.12,22.35;20,203.31,266.56,6.12,51.62;20,203.31,323.90,6.12,37.89;20,203.31,367.23,6.12,22.11;20,203.31,397.82,6.12,22.11;20,203.31,428.81,6.12,22.11;20,203.31,462.05,6.12,22.11;20,203.31,498.09,6.12,22.11;20,203.31,549.51,6.12,5.88;20,203.31,603.18,6.12,5.69;20,195.34,239.42,6.12,22.35;20,195.34,264.63,6.12,51.62;20,195.34,321.97,6.12,39.82;20,195.34,367.23,6.12,22.11;20,195.34,397.82,6.12,22.11;20,195.34,428.81,6.12,22.11;20,195.34,462.06,6.12,22.11;20,195.34,498.09,6.12,22.11;20,195.34,549.51,6.12,5.88;20,195.34,603.18,6.12,5.69;20,187.37,206.68,6.12,14.56;20,187.37,224.10,6.12,51.62;20,187.37,281.44,6.12,80.35;20,187.37,367.23,6.12,22.11;20,187.37,397.82,6.12,22.11;20,187.37,428.81,6.12,22.11;20,187.37,462.06,6.12,22.11;20,187.37,498.10,6.12,22.11;20,187.37,549.51,6.12,5.88;20,187.37,603.08,6.12,5.88;20,179.40,179.08,6.12,42.11;20,179.40,224.05,6.12,51.62;20,179.40,281.39,6.12,80.40;20,179.40,367.23,6.12,22.11;20,179.40,397.82,6.12,22.11;20,179.40,428.81,6.12,22.11;20,179.40,462.06,6.12,22.11;20,179.40,498.10,6.12,22.11;20,179.40,549.51,6.12,5.88;20,179.40,603.17,6.12,5.69;20,171.43,239.23,6.12,22.35;20,171.43,264.43,6.12,51.62;20,171.43,321.78,6.12,40.02;20,171.43,367.23,6.12,22.11;20,171.43,397.82,6.12,22.11;20,171.43,428.81,6.12,22.11;20,171.43,462.06,6.12,22.11;20,171.43,498.09,6.12,22.11;20,171.43,549.52,6.12,5.88;20,171.43,603.08,6.12,5.88;20,163.46,241.16,6.12,22.35;20,163.46,266.36,6.12,51.62;20,163.46,323.71,6.12,38.09;20,163.46,367.23,6.12,22.11;20,163.46,397.82,6.12,22.11;20,163.46,428.81,6.12,22.11;20,163.46,462.05,6.12,22.11;20,163.46,498.09,6.12,22.11;20,163.46,549.51,6.12,5.88;20,163.46,603.08,6.12,5.88;21,134.77,127.36,24.08,8.74"><head></head><label></label><figDesc>Run MAP P@10 P@20 P@100 R-Prec Automation Configuration MLKD 1307692157334 Sub4 Multi1 1000.txt 0.1640 0.3900 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="22,212.08,381.54,191.20,8.74"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Comparison of MAP scores per topic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="8,152.06,115.84,311.24,213.76"><head></head><label></label><figDesc></figDesc><graphic coords="8,152.06,115.84,311.24,213.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,138.22,115.83,338.91,207.24"><head></head><label></label><figDesc></figDesc><graphic coords="9,138.22,115.83,338.91,207.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="14,134.77,127.36,345.83,277.85"><head>Table 2 :</head><label>2</label><figDesc>Summary of the results for the evaluation per concept. The table shows the MiAP for the best run per group and the averaged MiAP for all runs for each group and indicates the configuration of the run. The teams are sorted by the rank of their best run.</figDesc><table coords="14,173.35,183.71,268.66,221.51"><row><cell></cell><cell></cell><cell>BEST RUN</cell><cell cols="2">AVERAGE RUNS</cell></row><row><cell cols="5">Team Runs MiAP Rank Config. MiAP Rank Config.</cell></row><row><cell>TUBFI</cell><cell>5</cell><cell>1 0.443</cell><cell>M 11.00 0.394</cell><cell>V+M</cell></row><row><cell>LIRIS</cell><cell>5</cell><cell>2 0.437</cell><cell cols="2">M 26.00 0.372 V+T+M</cell></row><row><cell>BPACAD</cell><cell>5</cell><cell>3 0.436</cell><cell cols="2">M 13.40 0.401 V+T+M</cell></row><row><cell>ISIS</cell><cell>5</cell><cell>5 0.433</cell><cell>M 14.00 0.391</cell><cell>V+M</cell></row><row><cell>MLKD</cell><cell>5</cell><cell>9 0.402</cell><cell cols="2">M 31.00 0.349 V+T+M</cell></row><row><cell>CEALIST</cell><cell>5</cell><cell>11 0.384</cell><cell cols="2">M 32.80 0.339 V+T+M</cell></row><row><cell>CAEN</cell><cell>4</cell><cell>14 0.382</cell><cell>V 23.50 0.363</cell><cell>V</cell></row><row><cell>MRIM</cell><cell>4</cell><cell>16 0.377</cell><cell>M 43.75 0.289</cell><cell>V+M</cell></row><row><cell>IDMT</cell><cell>5</cell><cell>20 0.371</cell><cell>M 28.00 0.354</cell><cell>T+M</cell></row><row><cell>NII</cell><cell>5</cell><cell>34 0.337</cell><cell>V 42.80 0.321</cell><cell>V</cell></row><row><cell>HHI</cell><cell>5</cell><cell>36 0.335</cell><cell>V 40.00 0.328</cell><cell>V</cell></row><row><cell>MEIJI</cell><cell>5</cell><cell>50 0.304</cell><cell cols="2">T 63.20 0.254 V+T+M</cell></row><row><cell>MUFIN</cell><cell>4</cell><cell>52 0.299</cell><cell>M 53.75 0.296</cell><cell>M</cell></row><row><cell>BUFFALO</cell><cell>5</cell><cell>60 0.249</cell><cell>V 62.60 0.236</cell><cell>V</cell></row><row><cell>DBIS</cell><cell>5</cell><cell>63 0.230</cell><cell>V 66.40 0.218</cell><cell>V</cell></row><row><cell>UNIKLU</cell><cell>4</cell><cell>70 0.207</cell><cell>V 71.50 0.206</cell><cell>V</cell></row><row><cell>REGIMVID</cell><cell>1</cell><cell>74 0.204</cell><cell>T 74.00 0.204</cell><cell>T</cell></row><row><cell>LAPI</cell><cell>2</cell><cell>77 0.177</cell><cell>V 77.50 0.177</cell><cell>V</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="16,138.94,585.24,337.47,73.62"><head>Table 6 :</head><label>6</label><figDesc>Summary of the results for the evaluation per concept in the multimodal configuration. The table shows the best run per group.</figDesc><table coords="16,138.94,585.24,337.47,73.62"><row><cell></cell><cell>1 0.346</cell><cell>IDMT</cell><cell>1 0.525</cell><cell>IDMT</cell><cell>1</cell><cell>0.677</cell></row><row><cell>IDMT</cell><cell>2 0.326</cell><cell>MLKD</cell><cell cols="2">2 0.506 CEALIST</cell><cell>2</cell><cell>0.676</cell></row><row><cell>MLKD</cell><cell cols="2">3 0.326 BPACAD</cell><cell>3 0.502</cell><cell>LIRIS</cell><cell>3</cell><cell>0.676</cell></row><row><cell>LIRIS</cell><cell cols="2">4 0.321 CEALIST</cell><cell>4 0.479</cell><cell>MLKD</cell><cell>5</cell><cell>0.653</cell></row><row><cell>MEIJI</cell><cell>6 0.304</cell><cell>MEIJI</cell><cell cols="2">5 0.459 BPACAD</cell><cell>6</cell><cell>0.635</cell></row><row><cell>CEALIST</cell><cell>7 0.292</cell><cell>LIRIS</cell><cell>6 0.432</cell><cell>MEIJI</cell><cell>7</cell><cell>0.491</cell></row><row><cell>REGIMVID</cell><cell cols="2">8 0.204 REGIMVID</cell><cell cols="2">8 0.141 REGIMVID</cell><cell>8</cell><cell>0.396</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="22,142.09,122.45,312.43,244.95"><head></head><label></label><figDesc>MAPs, sorted by descending Median of single Topics</figDesc><table coords="22,142.09,168.14,312.43,199.26"><row><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2 MAP scores 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>35</cell><cell>30</cell><cell>27</cell><cell>1</cell><cell>12</cell><cell>9</cell><cell>33</cell><cell>6</cell><cell>34</cell><cell>15</cell><cell>all</cell><cell>40</cell><cell>32</cell><cell>14</cell><cell>4</cell><cell>19</cell><cell>8</cell><cell>17</cell><cell>7</cell><cell>37</cell><cell>2</cell><cell>24</cell><cell>20</cell><cell>11</cell><cell>36</cell><cell>25</cell><cell>38</cell><cell>31</cell><cell>29</cell><cell>16</cell><cell>10</cell><cell>5</cell><cell>39</cell><cell>13</cell><cell>21</cell><cell>23</cell><cell>28</cell><cell>26</cell><cell>22</cell><cell>3</cell><cell>18</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">No. of Topic</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,144.73,635.53,61.20,7.47"><p>www.mturk.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.73,645.84,299.23,8.12"><p>http://wordnetweb.princeton.edu/perl/webwn, last accessed 20.07.2011</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,144.73,656.80,256.87,8.12"><p>http://www.thefreedictionary.com/, last accessed 20.07.2011</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="14,144.73,657.44,164.76,7.47"><p>http://www.imageclef.org/2011/photo</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank the <rs type="institution">CLEF</rs> campaign for supporting the ImageCLEF initiative. This work was partly supported by grant <rs type="grantNumber">01MQ07017</rs> of the <rs type="programName">German research program THESEUS</rs> funded by the <rs type="funder">Ministry of Economics</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GuKrktR">
					<idno type="grant-number">01MQ07017</idno>
					<orgName type="program" subtype="full">German research program THESEUS</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept iAP</head><p>Team Config. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="23,142.96,557.72,337.64,7.86;23,151.52,568.68,329.07,7.86;23,151.52,579.64,329.07,7.86;23,151.52,590.60,329.07,7.86;23,151.52,601.56,329.07,7.86;23,151.52,612.52,232.78,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="23,250.13,557.72,230.46,7.86;23,151.52,568.68,125.10,7.86">Overview of the CLEF 2009 Large-Scale Visual Concept Detection and Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,363.38,579.64,117.21,7.86;23,151.52,590.60,329.07,7.86;23,151.52,601.56,221.55,7.86">Multilingual Information Access Evaluation Vol. II Multimedia Experiments: Proceedings of the 10th Workshop of the Cross-Language Evaluation Forum (CLEF 2009)</title>
		<title level="s" coord="23,151.52,612.52,141.41,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="23,142.96,623.70,337.64,7.86;23,151.52,634.63,318.41,7.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="23,257.63,623.70,222.96,7.86;23,151.52,634.66,160.72,7.86">New strategies for image annotation: Overview of the photo annotation task at imageclef 2010</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Huiskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,320.52,634.63,120.74,7.89">Working notes of CLEF 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.96,645.84,337.64,7.86;23,151.52,656.80,329.07,7.86;24,151.52,119.67,329.07,7.86;24,151.52,130.63,129.77,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="23,287.34,645.84,193.25,7.86;23,151.52,656.80,173.54,7.86">New trends and ideas in visual concept detection: The mir flickr retrieval evaluation initiative</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,346.33,656.80,134.26,7.86;24,151.52,119.67,278.55,7.86">MIR &apos;10: Proceedings of the 2010 ACM International Conference on Multimedia Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.96,141.60,337.64,7.86;24,151.52,152.56,329.07,7.86;24,151.52,163.52,329.07,7.86;24,151.52,174.48,244.81,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="24,248.29,141.60,232.30,7.86;24,151.52,152.56,35.99,7.86">A Consumer Photo Tagging Ontology: Concepts and Annotations</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,212.50,152.56,268.09,7.86;24,151.52,163.52,329.07,7.86;24,151.52,174.48,125.12,7.86">THESEUS/ImageCLEF Pre-Workshop 2009, Co-located with the Cross-Language Evaluation Forum (CLEF) Workshop and 13th European Conference on Digital Libraries ECDL</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.96,185.44,337.64,7.86;24,151.52,196.40,329.07,7.86;24,151.52,207.36,249.90,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="24,285.74,185.44,194.85,7.86;24,151.52,196.40,164.92,7.86">Semantic context transfer across heterogeneous sources for domain adaptive video search</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,339.28,196.40,141.31,7.86;24,151.52,207.36,156.63,7.86">Proceedings of the seventeen ACM international conference on Multimedia</title>
		<meeting>the seventeen ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.96,218.33,337.64,7.86;24,151.52,229.26,105.96,7.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="24,201.13,218.33,117.25,7.86">A circumplex model of affect</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,327.17,218.33,153.43,7.86;24,151.52,229.29,30.20,7.86">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1161</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.96,240.25,337.64,7.86;24,151.52,251.21,127.68,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="24,251.48,240.25,190.61,7.86">Overview of the wikipediamm task at imageclef</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,151.52,251.21,93.56,7.86">Working notes of CLEF</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.96,262.18,337.63,7.86;24,151.52,273.14,329.07,7.86;24,151.52,284.10,79.69,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="24,305.88,262.18,174.71,7.86;24,151.52,273.14,104.69,7.86">Overview of the Wikipedia Image Retrieval Task at ImageCLEF 2011</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,281.85,273.14,121.40,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.96,295.06,337.64,7.86;24,151.52,306.02,230.19,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="24,309.12,295.06,171.47,7.86;24,151.52,306.02,80.15,7.86">Overview of the Wikipedia Retrieval Task at ImageCLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,254.03,306.02,93.56,7.86">Working notes of CLEF</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.62,316.99,337.97,7.86;24,151.52,327.95,239.42,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="24,311.20,316.99,114.49,7.86">SZTAKI @ ImageCLEF 2011</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Daróczy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pethes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Benczúr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,446.53,316.99,34.06,7.86;24,151.52,327.95,80.97,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.62,338.92,337.98,7.86;24,151.52,349.88,329.07,7.86;24,151.52,360.83,98.64,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="24,232.96,338.92,247.63,7.86;24,151.52,349.88,119.11,7.86">Semantic Contexts and Fisher Vectors for the ImageCLEF 2011 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,298.71,349.88,123.72,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.62,371.80,337.98,7.86;24,151.52,382.76,329.07,7.86;24,151.52,393.72,79.69,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="24,263.15,371.80,217.44,7.86;24,151.52,382.76,104.09,7.86">CEA LISTs participation to Visual Concept Detection Task of ImageCLEF 2011</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Znaidia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Le Borgne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,281.50,382.76,121.63,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.62,404.69,337.97,7.86;24,151.52,415.65,329.07,7.86;24,151.52,426.60,98.64,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="24,370.19,404.69,110.40,7.86;24,151.52,415.65,128.81,7.86">Sample Selection, Category Specific Features and Reasoning</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mbanya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hentschel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ndjiki-Nya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,304.14,415.65,119.39,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.62,437.57,337.97,7.86;24,151.52,448.53,329.07,7.86;24,151.52,459.49,98.64,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="24,346.32,437.57,134.27,7.86;24,151.52,448.53,138.91,7.86">The Fraunhofer IDMT at Image-CLEF 2011 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Kühhirt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,310.44,448.53,114.34,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.62,470.46,337.98,7.86;24,151.52,481.41,329.07,7.86;24,151.52,492.37,98.64,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="24,290.69,470.46,189.90,7.86;24,151.52,481.41,131.37,7.86">The University of Amsterdam&apos;s Concept Detection System at ImageCLEF 2011</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,305.46,481.41,118.32,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.62,503.34,337.98,7.86;24,151.52,514.30,329.07,7.86;24,151.52,525.26,25.60,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="24,252.49,503.34,228.11,7.86;24,151.52,514.30,62.02,7.86">Testing a Method for Statistical Image Classification in Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rasche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vertan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,234.80,514.30,116.91,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.62,536.23,337.97,7.86;24,151.52,547.18,329.07,7.86;24,151.52,558.14,98.64,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="24,378.61,536.23,101.98,7.86;24,151.52,547.18,137.86,7.86">LIRIS-Imagine at Image-CLEF 2011 Photo Annotation task</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandréa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,309.45,547.18,115.14,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.62,569.11,337.98,7.86;24,151.52,580.07,329.07,7.86;24,151.52,591.03,213.52,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="24,315.06,569.11,165.53,7.86;24,151.52,580.07,245.36,7.86">Annotation and Retrieval System Using Confabulation Model for ImageCLEF2011 Photo Annotation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Izawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Motohashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,420.38,580.07,60.21,7.86;24,151.52,591.03,55.06,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.62,602.00,337.97,7.86;24,151.52,612.95,329.07,7.86;24,151.52,623.91,290.64,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="24,416.34,602.00,64.25,7.86;24,151.52,612.95,324.87,7.86">MLKD&apos;s Participation at the CLEF 2011 Photo Annotation and Concept-Based Retrieval Tasks</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sechidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,165.60,623.91,118.10,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.62,634.88,337.98,7.86;24,151.52,645.84,329.07,7.86;24,151.52,656.80,98.64,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="24,350.51,634.88,130.09,7.86;24,151.52,645.84,134.74,7.86">LIG-MRIM at Image Photo Annotation task in ImageCLEF 2011</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Albatal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Safadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Quénot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,307.63,645.84,116.59,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,119.67,337.98,7.86;25,151.52,130.63,326.87,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="25,310.99,119.67,169.61,7.86;25,151.52,130.63,32.15,7.86">MUFIN at ImageCLEF 2011: Success or Failure?</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Budikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,201.84,130.63,118.10,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,141.59,337.98,7.86;25,151.52,152.55,276.56,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="25,235.36,141.59,225.57,7.86">NII, Japan at ImageCLEF 2011 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,151.52,152.55,118.11,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,163.51,337.98,7.86;25,151.52,174.47,329.07,7.86;25,151.52,185.43,322.65,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="25,312.83,163.51,167.76,7.86;25,151.52,174.47,329.07,7.86;25,151.52,185.43,24.44,7.86">REGIMvid at ImageCLEF2011: Integrating Contextual Information to Enhance Photo Annotation and Concept-based Retrieval</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Amel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Benammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,197.61,185.43,118.11,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,196.39,337.97,7.86;25,151.52,207.34,329.07,7.86;25,151.52,218.30,276.56,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="25,311.06,196.39,169.53,7.86;25,151.52,207.34,309.41,7.86">The joint submission of the TU Berlin and Fraunhofer FIRST (TUBFI) to the ImageCLEF2011 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,151.52,218.30,118.11,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
