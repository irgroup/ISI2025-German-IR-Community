<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.99,116.50,335.39,13.46;1,253.28,134.44,108.80,13.46">Overview of the Wikipedia Image Retrieval Task at ImageCLEF 2011</title>
				<funder ref="#_Tsh9Pt4 #_wSXu2f5">
					<orgName type="full">EU</orgName>
				</funder>
				<funder>
					<orgName type="full">Swiss National Fund</orgName>
					<orgName type="abbreviated">SNF</orgName>
				</funder>
				<funder ref="#_NXatqdk">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,177.08,172.41,80.28,9.58"><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
							<email>theodora.tsikrika@acm.org</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.68,172.41,70.81,9.58"><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
							<email>adrian.popescu@cea.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CEA</orgName>
								<orgName type="institution" key="instit2">LIST, Vision &amp; Content Engineering Laboratory</orgName>
								<address>
									<postCode>92263</postCode>
									<settlement>Fontenay aux Roses</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.81,172.41,53.00,9.58"><forename type="first">Jana</forename><surname>Kludas</surname></persName>
							<email>jana.kludas@unige.ch</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">CUI</orgName>
								<orgName type="institution" key="instit2">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.99,116.50,335.39,13.46;1,253.28,134.44,108.80,13.46">Overview of the Wikipedia Image Retrieval Task at ImageCLEF 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B72AE019F22E42945B0A515CD086074E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ImageCLEF's Wikipedia Image Retrieval task provides a testbed for the system-oriented evaluation of multimedia and multilingual information retrieval from a collection of Wikipedia images. The aim is to investigate retrieval approaches in the context of a large and heterogeneous collection of images (similar to those encountered on the Web) that are searched for by users with diverse information needs. This paper presents an overview of the resources, topics, and assessments of the Wikipedia Image Retrieval task at ImageCLEF 2011, summarizes the retrieval approaches employed by the participating groups, and provides an analysis of the main evaluation results.</p><p>The ImageCLEF 2010 Wikipedia collection was used for the second time in 2011. It consists of 237,434 Wikipedia images, their user-provided annotations, the Wikipedia articles that contain these images, and low-level visual features of these images. The collection was built to cover similar topics in English, German, and French and it is based on the September 2009 Wikipedia dumps. Images are annotated in none, one or several languages and, wherever possible, the annotation language is given in the metadata file. The articles in which these images appear were extracted from the Wikipedia dumps and are provided as such. The collection is described in more detail in <ref type="bibr" coords="2,359.55,245.24,16.60,9.58" target="#b10">[11]</ref> and an example image with its associated metadata is given in Figure <ref type="figure" coords="2,345.95,257.19,3.74,9.58">1</ref>. A first set of image features were extracted using MM, CEA LIST's image indexing tool [7] and include both local (bags of visual words) and global features (texture, color and edges). An alternative set of global features, extracted with the MMRetrieval tool [13], was kindly provided by the Information Retrieval group at the Democritus University of Thrace, Greece (DUTH group). Fig. 1: Wikipedia image+metadata example from the ImageCLEF 2010 Wikipedia image collection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Wikipedia Image Retrieval task is an ad-hoc image retrieval task. The evaluation scenario is thereby similar to the classic TREC ad-hoc retrieval task: simulation of the situation in which a system knows the set of documents to be searched, but cannot anticipate the particular topic that will be investigated (i.e., topics are not known to the system in advance). Given a multimedia query that consists of a title in three different languages and a few example images describing a user's information need, the aim is to find as many relevant images as possible from a collection of Wikipedia images. Similarly to past years, participants are encouraged to develop approaches that combine the relevance of different media types and of multilingual textual resources into a single ranked list of results. A number of resources that support participants towards this research direction were provided this year.</p><p>The paper is organized as follows. First, we introduce the task's resources: the Wikipedia image collection and additional resources, the topics, and the assessments (Sections 2-4). Section 5 presents the approaches employed by the participating groups and Section 6 summarizes their main results. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topic Format</head><p>These multimedia queries consist of a multilingual textual part, the query title, and a visual part made of several example images. The narrative of the query is only used during the assessment phase. &lt;title&gt; query by keywords, one for each language: English, French, German &lt;image&gt; query by image content (four or five) &lt;narrative&gt; description of query in which an unambiguous definition of relevance and irrelevance is given &lt;title&gt; The topic &lt;title xml:lang="en"&gt; has a language attribute that marks the English (en), French (fr) and German (de) topic title. It simulates a user who does not have (or want to use) example images or other visual constraints. The query expressed in the topic &lt;title&gt; is therefore a text-only query. This profile is likely to fit most users searching digital libraries or the Web.</p><p>Upon discovering that a text-only query does not produce many relevant hits, a user might decide to add visual hints and formulate a multimedia query.</p><p>&lt;image&gt; The visual hints are example images, which express the narrative of the topic.</p><p>&lt;narrative&gt; A clear and precise description of the information need is required in order to unambiguously determine whether or not a given document fulfils the given information need. In a test collection this description is known as the narrative. It is the only true and accurate interpretation of a user's needs. Precise recording of the narrative is important for scientific repeatability -there must exist, somewhere, a definitive description of what is and is not relevant to the user.</p><p>Textual terms and visual examples can be used in any combination in order to produce results. It is up to the systems how to use, combine or ignore this information; the relevance of a result does not directly depend on these constraints, but it is decided by manual assessments based on the &lt;narrative&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topic Development</head><p>The 50 topics in the ImageCLEF 2011 Wikipedia Image Retrieval task (see Table <ref type="table" coords="3,151.11,589.08,3.60,9.58" target="#tab_0">1</ref>), created by the organizers of the task, aim to cover diverse information needs and to have a variable degree of difficulty. They were chosen after a statistical analysis of a large scale image query log kindly provided by Exalead so as to cover a wide variety of topics commonly searched on the Web. Candidate topics were run through the Cross Modal Search Engine 4 (CMSE developed by the University of Geneva) in order to get an indication of the number of relevant images in top results for visual, textual and multimodal candidate queries. The topics range from simple, and thus relatively easy (e.g., "brown bear"), to semantic, and hence highly difficult (e.g., "model train scenery"), with a balanced distribution of the two types of topics. One difference with 2010 <ref type="bibr" coords="4,439.21,166.84,16.60,9.58" target="#b10">[11]</ref> is the higher number of topics with named entities (and particularly known person names and products) proposed this year. This change is motivated by the results of the log analysis which confirmed that a lot of named entities are used in Web queries. Semantic topics typically have a complex set of constraints, need world knowledge, and/or contain ambiguous terms, so they are expected to be challenging for current state-of-the-art retrieval algorithms. We encouraged the participants to use multimodal and multilingual approaches since they are more appropriate for dealing with semantic information needs.</p><p>Image examples were selected from Flickr, after ensuring that they were uploaded under Creative Commons licenses. Each topic has four or five image examples, chosen so as to illustrate, to the extent possible, the visual diversity of the topic. Compared to 2010 <ref type="bibr" coords="4,275.95,310.31,15.27,9.58" target="#b10">[11]</ref>, a larger number of images was provided per topic in order to have an improved visual characterization of the topics and thus to encourage multimodal approaches. Query image examples and their low-level features were also provided with the collection in order to ensure repeatability of the experiments. On average, the 50 topics contain 4.84 images and 3.1 words in their English formulation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Assessments</head><p>The Wikipedia Image Retrieval task is an image retrieval task, where an image is either relevant or not (binary relevance). We adopted TREC-style pooling of the retrieved images with a pool depth of 100, resulting in pool sizes of between 764 and 2327 images with a mean of 1467 and median of 1440.</p><p>The relevance assessments were performed with a crowdsourcing approach using CrowdFlower (http://crowdflower.com/), a general-purpose platform for managing crowdsourcing tasks and ensuring high-quality responses. CrowdFlower enables the processing of large amounts of data in a short period of time by breaking a repetitive "job" into many "assignments", each consisting of small numbers of "units", and distributing them to many "workers" simultaneously. In our case, a job corresponded to performing the relevance assessments of the pooled images for a single topic, each unit was the image to be assessed, whereas each assignment consisted of assessing the relevance for a set of five units (i.e., images) for a single topic. The assessments were carried out by Amazon Mechanical Turk (http://www.mturk.com) workers based in the UK and the USA and each assignment was rewarded with 0.04$.</p><p>For each assignment, each worker was provided with instructions in English for the English version of the topic, as shown in Figure <ref type="figure" coords="7,417.10,315.14,4.98,9.58" target="#fig_0">2</ref> for topic 76, followed by five units to be assessed for that topic, each similar to the one shown in Figure <ref type="figure" coords="7,208.77,339.05,3.74,9.58" target="#fig_1">3</ref>. To prevent spammers and thus obtain accurate results, each assignment contained one "gold standard" image among the five images, i.e., an image already correctly labelled by the organizers. These gold standard data were used for estimating the workers' accuracy: if a worker's accuracy dropped below a threshold (70%), his/her assessments were excluded. For each topic, the gold standard data were created as follows. First, the images in the pool of depth 5 for that topic which could be unambiguously marked as relevant or non-relevant were assessed. This subset was selected so as to ensure that at least some relevant images were included in the gold standard set. If at the end of this round of assessment, the gold standard set contained less than 6% of the total images to be assessed for that topic, then further images from the original pool of depth 100 were randomly selected and assessed until the 6% limit was reached.</p><p>Each image was assessed by three workers with the final assessment obtained through a majority vote.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participants</head><p>A total of 11 groups submitted 110 runs. The participation has slightly reduced compared to last year both in terms of number of participants <ref type="bibr" coords="8,406.68,286.64,39.17,9.58">(11 vs. 13</ref>) and of submitted runs (110 vs. 127). Nine participating groups out of 11 are located in Europe, one comes from Turkey and another one from Tunisia.  Table <ref type="table" coords="8,174.99,620.28,4.98,9.58" target="#tab_2">2</ref> gives an overview of the types of the submitted runs. Similarly to last year, more multimodal (text/visual) than text-only runs were submitted. Table 3 presents the combinations of annotation and topic languages used by participants in their textual and multimodal runs. The majority of submitted runs are multilingual in at least one of the two aspects. Most teams used both multilingual queries and multilingual annotations in order to maximize retrieval performance and the best results presented in the next section (see Tables <ref type="table" coords="9,456.45,142.93,4.98,9.58" target="#tab_4">4</ref> and<ref type="table" coords="9,134.77,154.89,4.15,9.58" target="#tab_5">5</ref>) validate this approach. Although runs that implicate English only queries are by far more frequent than runs implicating German and French only, some participants also submitted the latter type of runs. A short description of the participants' approaches follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CEA LIST (9 runs -5 single + 4 CEA-XRCE) [5]</head><p>Their approach is mainly based on query expansion with Wikipedia. Given a topic, related concepts are retrieved from Wikipedia and used to expand the initial query. Then results are re-ranked using query models extracted from Flickr. They also used visual concepts (face/no face; indoor/outdoor) to characterize topics in terms of presence of these concepts in the image examples and to re-rank the results accordingly. Some of the runs submitted by CEA LIST (noted CEA-XRCE) were created using a late fusion of results with visual results produced by XRCE. DBISForMaT (12 runs) <ref type="bibr" coords="9,240.76,313.06,16.60,9.34" target="#b13">[14]</ref> They introduced a retrieval model based on the polyrepresentation of documents which assumes that different modalities of a document can be combined in a structured manner to reflect a user's information need. Global image features were extracted using LIRE, a CBIR engine built on top of LUCENE. As it is underlined in <ref type="bibr" coords="9,384.18,360.80,15.27,9.58" target="#b13">[14]</ref>, although promising, their results are hampered by the use of a naive textual representation of the documents. DEMIR (6 runs) [3] They used the Terrier IR platform to test a large number of classical weighting schemes (BM25, TF-IDF, PL2 etc.) over a bag-of-words representation of the collection for text retrieval. They also performed a comparison of the visual descriptors provided by DUTH and report that the best purely visual results are obtained using the CEDD descriptor. Their multimodal runs are based on a late fusion approach and results show that merging modalities achieves small improvements compared to the textual results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUTH (19 runs) [1]</head><p>The group has further developed its MMRetrieval engine that they introduced in 2010. It includes a flexible indexing of text and visual modalities as well as different fusion strategies (score combination and score normalization). This year, they introduced an estimation of query difficulty whose combination with score combination gave the best results. The group also kindly provided a set of low-level features which were used by a large number of participants. ReDCAD (4 runs) <ref type="bibr" coords="9,218.04,573.42,11.62,9.34" target="#b1">[2]</ref> They focused on text retrieval and tested the use of the metadata related to the images as well as of the larger textual context of the images. LUCENE was used for both indexing and retrieving documents.</p><p>Taking into account the textual context of the image is more effective than the use of the metadata only and a combination of the two provides a small additional improvement of results. SINAI (6 runs) <ref type="bibr" coords="9,204.21,644.27,11.62,9.34" target="#b7">[8]</ref> The group submitted only textual runs and focused on an automatic translation of image descriptions from French and German to En-glish. All their runs work with English queries only. Different linear combinations of image captions and descriptions were tested and they also combined results from Lemur and LUCENE retrieval engines. The combination of the two achieved the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SZTAKI (10 runs) [6]</head><p>The team used a retrieval system based on Okapi BM25 and also added synonyms from WordNet to expand the initial queries. Light Fisher vectors were used to represent low-level image features and then used to re-rank the top results obtained with purely textual retrieval. This late fusion procedure resulted in a slight degradation of performance compared to the textual run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UAIC (6 runs) [4]</head><p>For textual retrieval, they used the standard LUCENE search engine library and expanded some of the queries using WordNet synonyms.</p><p>The visual search was performed using the Color and Edge Directionality Descriptor (CEDD) provided by the DUTH team. A linear combination of text and image results was performed which gave the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNED (20 runs) [9]</head><p>They performed textual retrieval with a combination of IDRA, their in-house retrieval tool, and LUCENE and experimented with different settings (such as named entity recognition or use of Wikipedia articles). For multilingual textual runs, UNED tested early and late fusion strategies and the results show that the latter approach gives better results. Content based retrieval based on the CEDD features provided by DUTH was applied to the textual results. UNED tested both early and late fusion approached to obtain merged runs. Their fusion approaches were effective and the best results were obtained with a logistic regression feedback algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNTESU (7 runs) [12]</head><p>They applied Salient Semantic Analysis in order to expand queries with semantically similar terms from Wikipedia. A picturability measure was defined in order to boost the weight of terms which are associated to the initial topic in Flickr annotations. French and German annotations in the collection were translated to English and only English topics were used for the experiments. The best results were obtained with a combination of terms from the initial query and of expanded terms found using Lavrenko's relevance model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XRCE (11 runs -4 single + 7 XRCE-CEA) [5]</head><p>For text retrieval, they implemented an information-based model and a lexical entailment IR model. Image content was described using spatial pyramids of Fisher Vectors and local RGB statistics. Late Semantic Combination (LSC) was exploited to combine results from text and image modalities. They showed that, although text retrieval largely outperforms pure visual retrieval, an appropriate combination of the two modalities results in a significant improvement over each modality considered independently. A part of the runs submitted by XRCE (noted XRCE-CEA) were created using a LSC approach which combined their text and visual runs as well as textual runs proposed by CEA LIST.</p><p>Tables <ref type="table" coords="11,165.78,154.13,4.98,9.58" target="#tab_4">4</ref> and<ref type="table" coords="11,194.51,154.13,4.98,9.58" target="#tab_5">5</ref> present the evaluation results for the 15 best performing runs and the best performing run for each participant, respectively, ranked by Mean Average Precision (MAP). Similarly to 2010, the best runs were multimodal and multilingual. The best MAP performance (0.388) was reported for a run which combines XRCE and CEA LIST runs. The best textual run, ranked 11th, was also a combination of results from XRCE and CEA LIST and had a MAP of 0.3141. The results in Table <ref type="table" coords="11,224.73,225.86,4.98,9.58" target="#tab_5">5</ref> show that the best submitted runs were multimodal for eight out of nine participating groups that submitted such runs. The complete list of results can be found at the ImageCLEF website<ref type="foot" coords="11,447.91,625.94,3.49,6.71" target="#foot_1">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Performance per modality for all topics</head><p>Here, we analyze the evaluation results using only the top 90% of the runs to exclude noisy and buggy results. Table <ref type="table" coords="12,310.97,150.91,4.98,9.58" target="#tab_6">6</ref> shows the average performance and standard deviation with respect to each modality. On average, the multimodal runs have better performance than textual ones with respect to all examined evaluation metrics (MAP, Precision at 20, and precision after R (= number of relevant) documents retrieved). This is in contrast with results reported in previous years when textual runs had better performances on average. This shift can be explained with changes in the resources as well as the approaches this year, i.e., increased number of visual examples in the queries, improved visual features and more appropriate fusion techniques used by the participants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance per topic and per modality</head><p>To analyze the average difficulty of the topics, we classify the topics based on the AP values per topic averaged over all runs as follows: easy: M AP &gt; 0.3 medium: 0.2 &lt; M AP &lt;= 0.3 hard: 0.1 &lt; M AP &lt;= 0.2 very hard: M AP &lt; 0.1.</p><p>Table <ref type="table" coords="12,176.64,512.69,4.98,9.58" target="#tab_7">7</ref> presents the top 10 topics per class (i.e., easy, medium, hard, and very hard), together with the total number of topics per class. Out of 50 topics, 23 fall in the hard or very hard classes. This was actually intended during the topic development process, because we opted for highly semantic topics that are challenging for current retrieval approaches. 7 topics were very hard to solve(M AP &lt; 0.10). The topic set includes only 17 easy topics ( such as "illustrations of Alice's adventures in Wonderland", "Sagrada Familia in Barcelona", "colored Volkswagen beetles", "KISS live"). Similarly to last year, a large number of the topics in the easy and medium classes include a reference to a named entity and, consequently, are easily retrieved with simple textual approaches. As for very hard topics, they often contain general terms ("cat", "house", "train" or "bird"), which have a difficult semantic interpretation or high concept variation and are, hence, very hard to solve.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Visuality of topics</head><p>We also analyzed the performance of runs that use only text (TXT) versus runs that use both text and visual resources (MIX). Figure <ref type="figure" coords="14,370.29,166.27,4.98,9.58" target="#fig_2">4</ref> shows the average performance on each topic for all, text-only and text-visual runs. The multimodal runs outperform the textual ones in 42 out of the 50 topics and the textual runs outperform mixed runs in 8 cases. This indicates that most of the topics benefit from a multimodal approach.</p><p>The "visuality" of topics can be deduced from the performance of text-only and text-visual approaches that were presented in the last section. We consider that, if for a topic the text-visual approaches improve significantly the MAP over all runs (i.e., by diff(M AP ) &gt;= 0.01), then we could consider that to be a visual topic. In the same way, we can define topics as textual, if the text-only approaches improve significantly the MAP over all runs of a topic. Based on this analysis, 38 of the topics can be characterized as visual and 7 as textual. The remaining 5 topics, where no clear improvements are observed, are considered to be neutral. Compared to 2010, when there were more textual than visual topics, the distribution of topics in visual vs. textual changed significantly. As with the aggregate run performances, this change is most probably a result of the increased number of query images, the improved low-level image indexing as well as the better fusion techniques proposed this year. Table <ref type="table" coords="14,176.25,387.60,4.98,9.58" target="#tab_8">8</ref> presents the topics in each group, as well as some statistics on the topic, their relevant documents, and their distribution over the classes that indicate their difficulty. Given that there are only few textual and neutral topics, it is difficult to provide a robust analysis of the characteristics of the topics of each type. The number of words per topic is larger for neutral queries than for textual and visual ones. The average number of relevant documents is significantly smaller for textual topics compared to the other two classes whereas the average MAP is bigger for neutral topics.</p><p>The distribution of the textual, visual and neutral topics over the classes expressing their difficulty shows that the visual and textual topics are more likely to fall into the hard/very hard class than the neutral ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Effect of Query Expansion and Relevance Feedback</head><p>Finally, we analyze the effect of the application of query expansion (QE), relevance feedback (FB) techniques as well as of their combination (FBQE). Similarly to the analysis in the previous section, we consider the techniques to be useful for a topic, if they improved significantly the MAP over all runs. Table <ref type="table" coords="15,475.61,272.53,4.98,9.58" target="#tab_9">9</ref> presents the best performing topics for these techniques and some statistics. Query expansion is useful only for 3 topics and relevance feedback for 10. Interestingly, a combination of query expansion and of relevance feedback is effective for a much larger number of topics (33 out of 50). Expansion and feedback tend to be more useful for topics that are either hard or very hard compared to easy or medium topics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>For the second time this year, a multimodal and multilingual approach performed best in the Wikipedia Image Retrieval task. The majority of runs focused either on a combination of topic languages or on English queries only, only a few runs were submitted for German and French queries only. Multilingual runs perform clearly better than monolingual ones due to the distribution of the information over the different languages.</p><p>It is encouraging to see that more than half of the submitted runs were multimodal and that the best submitted runs were multimodal for eight out of nine participating groups that submitted such runs. Many of the participants in the Wikipedia Image Retrieval Task have participated in the past and thus have been able to improve their multimodal retrieval approaches continuously. For the first time this year, there was a cooperation of two of the participating groups for testing late fusion of their results which is an interesting development.</p><p>A further analysis of the results showed that most topics (42 out of 50) were significantly better solved with multimodal approaches. This is not only due to the improvement of the fusion approaches mentioned above, but also due to an increased number of query images compared to the last years and improved visual features. Finally, we found that expansion and feedback techniques tend to be more useful for topics that are either hard or very hard compared to easy or medium topics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,148.88,591.98,317.60,9.58;6,134.77,176.42,345.83,400.41"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Instructions to workers for performing the relevance assessments.</figDesc><graphic coords="6,134.77,176.42,345.83,400.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,238.90,652.56,137.56,9.58;7,134.77,435.24,345.83,202.17"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: An image to be assessed.</figDesc><graphic coords="7,134.77,435.24,345.83,202.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="13,155.98,627.86,303.40,9.58;13,134.77,345.56,345.82,267.16"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Average topic performance over all, text-only, and mixed runs.</figDesc><graphic coords="13,134.77,345.56,345.82,267.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,136.56,352.29,345.82,169.80"><head></head><label></label><figDesc></figDesc><graphic coords="2,136.56,352.29,345.82,169.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,133.52,393.22,345.83,271.80"><head>Table 1 :</head><label>1</label><figDesc>Topics for the ImageCLEF 2011 Wikipedia Image Retrieval task: IDs, titles, the number of image examples providing additional visual information, and the number of relevant images in the collection.</figDesc><table coords="4,148.69,439.52,317.97,225.49"><row><cell>ID</cell><cell>Topic title</cell><cell></cell><cell cols="2"># image examples # relevant images</cell></row><row><cell cols="4">71 colored Volkswagen beetles 5</cell><cell>50</cell></row><row><cell cols="2">72 skeleton of dinosaur</cell><cell></cell><cell>5</cell><cell>116</cell></row><row><cell cols="2">73 graffiti street art on walls</cell><cell></cell><cell>5</cell><cell>95</cell></row><row><cell cols="2">74 white ballet dress</cell><cell></cell><cell>5</cell><cell>49</cell></row><row><cell cols="2">75 flock of sheep</cell><cell></cell><cell>5</cell><cell>34</cell></row><row><cell cols="2">76 playing cards</cell><cell></cell><cell>5</cell><cell>47</cell></row><row><cell cols="2">77 cola bottles or cans</cell><cell></cell><cell>5</cell><cell>24</cell></row><row><cell cols="2">78 kissing couple</cell><cell></cell><cell>5</cell><cell>33</cell></row><row><cell cols="2">79 heart shaped</cell><cell></cell><cell>5</cell><cell>34</cell></row><row><cell cols="2">80 wolf close up</cell><cell></cell><cell>4</cell><cell>25</cell></row><row><cell cols="2">81 golf player on green</cell><cell></cell><cell>5</cell><cell>22</cell></row><row><cell cols="2">82 model train scenery</cell><cell></cell><cell>5</cell><cell>40</cell></row><row><cell cols="2">83 red or black mini cooper</cell><cell></cell><cell>5</cell><cell>10</cell></row><row><cell>84 Sagrada</cell><cell>Familia</cell><cell>in</cell><cell>5</cell><cell>7</cell></row><row><cell cols="2">Barcelona</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">85 Beijing bird nest</cell><cell></cell><cell>5</cell><cell>12</cell></row><row><cell cols="2">86 KISS live</cell><cell></cell><cell>5</cell><cell>11</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Continued on next page</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,148.69,117.51,315.03,464.60"><head>Table 1 -continued from previous page ID Topic title # image examples # relevant images 87</head><label>1</label><figDesc></figDesc><table coords="5,148.69,142.14,247.88,439.97"><row><cell>boxing match</cell><cell>5</cell><cell>45</cell></row><row><cell cols="2">88 portrait of Segolene Royal 5</cell><cell>10</cell></row><row><cell>89 Elvis Presley</cell><cell>4</cell><cell>7</cell></row><row><cell>90 gondola in Venice</cell><cell>5</cell><cell>62</cell></row><row><cell>91 freestyle jumps with bmx or</cell><cell>5</cell><cell>18</cell></row><row><cell>motor bike</cell><cell></cell><cell></cell></row><row><cell>92 air race</cell><cell>5</cell><cell>12</cell></row><row><cell>93 cable car</cell><cell>5</cell><cell>47</cell></row><row><cell>94 roller coaster wide shot</cell><cell>5</cell><cell>155</cell></row><row><cell>95 photo of real butterflies</cell><cell>5</cell><cell>112</cell></row><row><cell>96 shake hands</cell><cell>5</cell><cell>77</cell></row><row><cell>97 round cakes</cell><cell>5</cell><cell>43</cell></row><row><cell>98 illustrations of Alice's ad-</cell><cell>4</cell><cell>21</cell></row><row><cell>ventures in Wonderland</cell><cell></cell><cell></cell></row><row><cell>99 drawings of skeletons</cell><cell>5</cell><cell>95</cell></row><row><cell>100 brown bear</cell><cell>5</cell><cell>46</cell></row><row><cell>101 fountain with jet of water in</cell><cell>5</cell><cell>141</cell></row><row><cell>daylight</cell><cell></cell><cell></cell></row><row><cell>102 black cat</cell><cell>5</cell><cell>20</cell></row><row><cell>103 dragon relief or sculpture</cell><cell>5</cell><cell>41</cell></row><row><cell>104 portrait of Che Guevara</cell><cell>4</cell><cell>13</cell></row><row><cell>105 chinese characters</cell><cell>5</cell><cell>316</cell></row><row><cell>106 family tree</cell><cell>5</cell><cell>76</cell></row><row><cell>107 sunflower close up</cell><cell>5</cell><cell>13</cell></row><row><cell>108 carnival in Rio</cell><cell>5</cell><cell>37</cell></row><row><cell>109 snowshoe hiking</cell><cell>4</cell><cell>12</cell></row><row><cell>110 male color portrait</cell><cell>5</cell><cell>596</cell></row><row><cell>111 two euro coins</cell><cell>5</cell><cell>58</cell></row><row><cell>112 yellow flames</cell><cell>5</cell><cell>92</cell></row><row><cell>113 map of Europe</cell><cell>5</cell><cell>267</cell></row><row><cell>114 diver underwater</cell><cell>5</cell><cell>33</cell></row><row><cell>115 flying bird</cell><cell>5</cell><cell>115</cell></row><row><cell>116 houses in mountains</cell><cell>5</cell><cell>105</cell></row><row><cell>117 red roses</cell><cell>4</cell><cell>27</cell></row><row><cell>118 flag of UK</cell><cell>4</cell><cell>12</cell></row><row><cell>119 satellite image of desert</cell><cell>4</cell><cell>93</cell></row><row><cell>120 bar codes</cell><cell>4</cell><cell>14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,218.34,342.47,178.68,98.47"><head>Table 2 :</head><label>2</label><figDesc>Types of the 110 submitted runs.</figDesc><table coords="8,244.15,365.84,127.06,75.10"><row><cell>Run type</cell><cell># runs</cell></row><row><cell>Text (TXT)</cell><cell>51</cell></row><row><cell>Visual (IMG)</cell><cell>2</cell></row><row><cell>Text/Visual (TXTIMG)</cell><cell>57</cell></row><row><cell>Query Expansion (QE)</cell><cell>16</cell></row><row><cell>Relevance Feedback (RF)</cell><cell>15</cell></row><row><cell>QE &amp; RF</cell><cell>12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.77,485.19,345.82,110.82"><head>Table 3 :</head><label>3</label><figDesc>Annotation and topic language combinations in the textual and multimodal runs.</figDesc><table coords="8,201.93,520.50,211.50,75.50"><row><cell></cell><cell cols="2">Annotation language (AL)</cell><cell></cell></row><row><cell cols="3">Topic Language (TL) EN DE FR EN+DE+FR</cell><cell></cell></row><row><cell>EN</cell><cell>37 1 0</cell><cell>0</cell><cell>38</cell></row><row><cell>DE</cell><cell>0 2 0</cell><cell>0</cell><cell>2</cell></row><row><cell>FR</cell><cell>0 0 3</cell><cell>0</cell><cell>3</cell></row><row><cell>EN+DE+FR</cell><cell>0 0 0</cell><cell>65</cell><cell>65</cell></row><row><cell></cell><cell>37 3 3</cell><cell>65</cell><cell>108</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,147.20,276.14,320.95,149.21"><head>Table 4 :</head><label>4</label><figDesc>Results for the top 15 runs.</figDesc><table coords="11,147.20,298.70,320.95,126.66"><row><cell cols="3">Rank Participant Run</cell><cell cols="3">Modality FB/QE AL</cell><cell>TL</cell><cell>MAP P@10 P@20 R-prec.</cell></row><row><cell>1</cell><cell cols="2">XRCE-CEA SFLAXvis</cell><cell>Mix</cell><cell cols="3">FBQE ENFRDE ENFRDE 0.3880 0.6320 0.5100 0.4162</cell></row><row><cell>2</cell><cell cols="2">XRCE-CEA AXmixFVSFL</cell><cell>Mix</cell><cell cols="3">FBQE ENFRDE ENFRDE 0.3869 0.6240 0.5030 0.4174</cell></row><row><cell>3</cell><cell cols="3">XRCE-CEA SPLAXmixFVSFL Mix</cell><cell cols="3">FBQE ENFRDE ENFRDE 0.3848 0.6200 0.4990 0.4174</cell></row><row><cell>4</cell><cell cols="3">XRCE-CEA XTInn10AXmix Mix</cell><cell cols="3">FBQE ENFRDE ENFRDE 0.3560 0.5340 0.4710 0.3835</cell></row><row><cell>5</cell><cell>XRCE</cell><cell>AXFVSFL</cell><cell>Mix</cell><cell>QE</cell><cell cols="2">ENFRDE ENFRDE 0.3557 0.5940 0.4870 0.4051</cell></row><row><cell>6</cell><cell>XRCE</cell><cell>SPLAXFVSFL</cell><cell>Mix</cell><cell cols="3">FBQE ENFRDE ENFRDE 0.3556 0.5780 0.4840 0.4006</cell></row><row><cell>7</cell><cell cols="2">XRCE-CEA SFLAXvis</cell><cell>Mix</cell><cell cols="3">FBQE ENFRDE ENFRDE 0.3471 0.5740 0.4450 0.3756</cell></row><row><cell>8</cell><cell>UNED</cell><cell>UNEDUV18</cell><cell>Mix</cell><cell>FB</cell><cell cols="2">ENFRDE ENFRDE 0.3405 0.5420 0.4500 0.3752</cell></row><row><cell>9</cell><cell>UNED</cell><cell>UNEDUV20</cell><cell>Mix</cell><cell>FB</cell><cell cols="2">ENFRDE ENFRDE 0.3367 0.5460 0.4410 0.3673</cell></row><row><cell>10</cell><cell>UNED</cell><cell>UNED-UV19</cell><cell>Mix</cell><cell>FB</cell><cell cols="2">ENFRDE ENFRDE 0.3233 0.5400 0.4230 0.3586</cell></row><row><cell>11</cell><cell cols="2">XRCE-CEA SPLAXmix</cell><cell>Txt</cell><cell cols="3">FBQE ENFRDE ENFRDE 0.3141 0.5160 0.4270 0.3504</cell></row><row><cell>12</cell><cell cols="2">XRCE-CEA AXmix</cell><cell>Txt</cell><cell>QE</cell><cell cols="2">ENFRDE ENFRDE 0.3130 0.5300 0.4250 0.3560</cell></row><row><cell>13</cell><cell cols="2">CEA LIST mixFVSFL</cell><cell>Mix</cell><cell>QE</cell><cell cols="2">ENFRDE ENFRDE 0.3075 0.5420 0.4210 0.3486</cell></row><row><cell>14</cell><cell>UNED</cell><cell>UNEDUV8</cell><cell>Txt</cell><cell cols="3">NOFB ENFRDE ENFRDE 0.3044 0.5060 0.4040 0.3435</cell></row><row><cell>15</cell><cell>UNED</cell><cell>UNEDUV14</cell><cell>Mix</cell><cell>FB</cell><cell cols="2">ENFRDE ENFRDE 0.3006 0.5200 0.4030 0.3379</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,147.19,479.84,320.97,117.33"><head>Table 5 :</head><label>5</label><figDesc>Best performing run for each participant.</figDesc><table coords="11,147.19,502.40,320.97,94.78"><row><cell cols="3">Rank Participant Run</cell><cell cols="3">Modality FB/QE AL</cell><cell>TL</cell><cell>MAP P@10 P@20 R-prec.</cell></row><row><cell>1</cell><cell cols="2">XRCE-CEA SFLAXvis</cell><cell>Mix</cell><cell cols="4">FBQE ENFRDE ENFRDE 0.3880 0.6320 0.5100 0.4162</cell></row><row><cell>8</cell><cell>UNED</cell><cell>UNEDUV18</cell><cell>Mix</cell><cell>FB</cell><cell cols="3">ENFRDE ENFRDE 0.3405 0.5420 0.4500 0.3752</cell></row><row><cell>13</cell><cell cols="2">CEA-XRCE mixFVSFL</cell><cell>Mix</cell><cell>QE</cell><cell cols="3">ENFRDE ENFRDE 0.3075 0.5420 0.4210 0.3486</cell></row><row><cell>18</cell><cell>DUTH</cell><cell>QDSumw60</cell><cell>Mix</cell><cell cols="4">NOFB ENFRDE ENFRDE 0.2886 0.4860 0.3870 0.3401</cell></row><row><cell>23</cell><cell>UNTESU</cell><cell>BLRF</cell><cell>Txt</cell><cell>FB</cell><cell>EN</cell><cell>EN</cell><cell>0.2866 0.4220 0.3650 0.3276</cell></row><row><cell>53</cell><cell>DEMIR</cell><cell>Mix2</cell><cell>Mix</cell><cell cols="4">NOFB ENFRDE ENFRDE 0.2432 0.4520 0.3420 0.3001</cell></row><row><cell>61</cell><cell>ReDCAD</cell><cell>redcad02tx</cell><cell>Txt</cell><cell cols="4">NOFB ENFRDE ENFRDE 0.2306 0.3700 0.3060 0.2862</cell></row><row><cell>64</cell><cell cols="3">DBISForMaT COMBINEDSW Mix</cell><cell cols="2">NOFB EN</cell><cell>EN</cell><cell>0.2195 0.4180 0.3630 0.2827</cell></row><row><cell>65</cell><cell>SZTAKI</cell><cell>txtjencolimg</cell><cell>Mix</cell><cell cols="4">FBQE ENFRDE ENFRDE 0.2167 0.4700 0.3690 0.2762</cell></row><row><cell>74</cell><cell>SINAI</cell><cell>lemurlucene</cell><cell>Txt</cell><cell>FB</cell><cell>EN</cell><cell>EN</cell><cell>0.2068 0.4020 0.3380 0.2587</cell></row><row><cell>94</cell><cell>UAIC2011</cell><cell>lucenecedd</cell><cell>Mix</cell><cell cols="4">NOFB ENFRDE ENFRDE 0.1665 0.4080 0.3090 0.2313</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="12,165.43,280.40,284.50,76.55"><head>Table 6 :</head><label>6</label><figDesc>Results per modality over all topics.</figDesc><table coords="12,165.43,303.77,284.50,53.19"><row><cell>Modality</cell><cell>MAP Mean SD Mean SD Mean SD P@20 R-prec.</cell></row><row><cell cols="2">All top 90% runs (100 runs) 0.2492 0.056 0.3597 0.0577 0.2989 0.0498</cell></row><row><cell cols="2">Mix in top 90% runs (50 runs) 0.2795 0.0498 0.3940 0.0494 0.3289 0.0414</cell></row><row><cell cols="2">Txt in top 90% runs (50 runs) 0.2189 0.0445 0.3254 0.0432 0.2689 0.0381</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="13,134.77,142.13,345.82,145.23"><head>Table 7 :</head><label>7</label><figDesc>Topics classified based on their difficulty (up to 10 topics per class) -the total number of topics per class is given in the table header.</figDesc><table coords="13,137.67,176.70,339.63,110.66"><row><cell>easy (17 topics)</cell><cell>medium (12 topics)</cell><cell>hard (14 topics)</cell><cell>very hard (7 topics)</cell></row><row><cell>98 illustrations of Alice's</cell><cell>108 carnival in Rio</cell><cell>99 drawings of skeletons</cell><cell>82 model train scenery</cell></row><row><cell>adventures in Wonderland</cell><cell></cell><cell></cell><cell></cell></row><row><cell>88 portrait of S. Royal</cell><cell>92 air race</cell><cell>117 red roses</cell><cell>87 boxing match</cell></row><row><cell>84 Sagrada Familia in</cell><cell>120 bar codes</cell><cell>101 fountain with jet of wa-</cell><cell>115 flying bird</cell></row><row><cell>Barcelona</cell><cell></cell><cell>ter in daylight</cell><cell></cell></row><row><cell>89 Elvis Presley</cell><cell>91 freestyle jumps with</cell><cell>93 cable car</cell><cell>118 flag of UK</cell></row><row><cell></cell><cell>bmx or motor bike</cell><cell></cell><cell></cell></row><row><cell cols="3">94 roller coaster wide shot 104 portrait of Che Guevara 80 wolf close up</cell><cell>102 black cat</cell></row><row><cell>96 shake hands</cell><cell>114 diver underwater</cell><cell>112 yellow flames</cell><cell>110 male color portrait</cell></row><row><cell>71 colored VW Beetles</cell><cell cols="2">83 red or black mini cooper 90 gondola in Venice</cell><cell>116 houses in mountains</cell></row><row><cell>86 KISS live</cell><cell>79 heart shaped</cell><cell>78 kissing couple</cell><cell></cell></row><row><cell>107 sunflower close up</cell><cell cols="2">73 graffiti street art on walls 95 photos of real butterflies</cell><cell></cell></row><row><cell>77 cola bottles or cans</cell><cell>106 family tree</cell><cell>119 satellite image of desert</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="14,134.77,475.43,345.82,186.67"><head>Table 8 :</head><label>8</label><figDesc>Best performing topics for textual and text-visual runs relative to the average over all runs (up to 10 topics per type).</figDesc><table coords="14,136.16,509.99,343.04,152.11"><row><cell></cell><cell>textual (7 topics)</cell><cell>visual (38 topics)</cell><cell>neutral (5 topics)</cell></row><row><cell>Topics</cell><cell>79 heart shaped</cell><cell>120 bar codes</cell><cell>98 illustrations of Alice's adven-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>tures in Wonderland</cell></row><row><cell></cell><cell>96 shake hands</cell><cell>94 roller coaster wide shot</cell><cell>75 flock of sheep</cell></row><row><cell></cell><cell>118 flag of UK</cell><cell>114 diver underwater</cell><cell>83 red or black mini cooper</cell></row><row><cell></cell><cell cols="2">103 dragon relief of sculpture 89 Elvis Presley</cell><cell>76 playing cards</cell></row><row><cell></cell><cell>74 white ballet dress</cell><cell>92 air race</cell><cell>113 map of Europe</cell></row><row><cell></cell><cell>77 cola bottles or cans</cell><cell>72 skeleton of dinosaur</cell><cell></cell></row><row><cell></cell><cell>78 kissing couple</cell><cell>93 cable car</cell><cell></cell></row><row><cell></cell><cell></cell><cell>108 carnival in Rio</cell><cell></cell></row><row><cell></cell><cell>106 family tree</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>85 Beijing bird nest</cell><cell></cell></row><row><cell cols="2">#words/topic 2.857</cell><cell>3.026</cell><cell>3.8</cell></row><row><cell>#reldocs</cell><cell>38.57</cell><cell>73.44</cell><cell>75.8</cell></row><row><cell>MAP</cell><cell>0.238</cell><cell>0.244</cell><cell>0.369</cell></row><row><cell>easy</cell><cell>2</cell><cell>11</cell><cell>4</cell></row><row><cell>medium</cell><cell>1</cell><cell>10</cell><cell>1</cell></row><row><cell>hard</cell><cell>3</cell><cell>11</cell><cell>0</cell></row><row><cell>very hard</cell><cell>1</cell><cell>6</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="15,134.77,377.14,345.82,190.26"><head>Table 9 :</head><label>9</label><figDesc>Best performing topics for query expansion (QE) and feedback (FB) runs relative to the average over all runs. Only the top 10 topics that benefit from query expansion are presented here.</figDesc><table coords="15,142.66,423.67,330.03,143.74"><row><cell></cell><cell>QE (3 topics)</cell><cell>FB (10 topics)</cell><cell>FBQE (33 topics)</cell></row><row><cell>Topics</cell><cell>118 flag of UK</cell><cell>99 drawings of skeletons</cell><cell>80 wolf close up</cell></row><row><cell></cell><cell>80 wolf close up</cell><cell>79 heart shaped</cell><cell>76 playing cards</cell></row><row><cell></cell><cell cols="3">90 gondola in Venice 95 photo of real butterflies 74 white ballet dress</cell></row><row><cell></cell><cell></cell><cell>93 cable car</cell><cell>97 round cakes</cell></row><row><cell></cell><cell></cell><cell>115 flying bird</cell><cell>91 freestyle jumps with bmx or motor bike</cell></row><row><cell></cell><cell></cell><cell>78 kissing couple</cell><cell>120 bar codes</cell></row><row><cell></cell><cell></cell><cell>74 white ballet dress</cell><cell>114 diver underwater</cell></row><row><cell></cell><cell></cell><cell>92 air race</cell><cell>96 shake hands</cell></row><row><cell></cell><cell></cell><cell cols="2">73 graffiti street art on walls 101 fountain with jet of water in daylight</cell></row><row><cell></cell><cell></cell><cell>82 model train scenery</cell><cell>94 roller coaster wide shot</cell></row><row><cell cols="2">#words/topic 3</cell><cell>2.8</cell><cell>3.273</cell></row><row><cell>#reldocs</cell><cell>33</cell><cell>63.2</cell><cell>79.848</cell></row><row><cell>avg. MAP</cell><cell>0.156</cell><cell>0.202</cell><cell>0.2153</cell></row><row><cell>easy</cell><cell>0</cell><cell>0</cell><cell>8</cell></row><row><cell>medium</cell><cell>0</cell><cell>3</cell><cell>8</cell></row><row><cell>hard</cell><cell>2</cell><cell>5</cell><cell>12</cell></row><row><cell>very hard</cell><cell>1</cell><cell>2</cell><cell>5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="3,144.73,657.93,156.01,6.31"><p>http://dolphin.unige.ch/cmse/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="11,144.73,657.93,237.22,6.31"><p>http://www.imageclef.org/2011/wikimm-results</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p><rs type="person">Theodora Tsikrika</rs> was supported by the <rs type="funder">EU</rs> in the context of Promise (contract no. <rs type="grantNumber">258191</rs>) and <rs type="projectName">Chorus+</rs> (contract no. <rs type="grantNumber">249008</rs>) FP7 projects. <rs type="person">Adrian Popescu</rs> was supported by the <rs type="funder">French ANR (Agence Nationale de la Recherche)</rs> via the <rs type="projectName">Georama</rs> project (<rs type="grantNumber">ANR-08-CORD-009</rs>). <rs type="person">Jana Kludas</rs> was funded by the <rs type="funder">Swiss National Fund (SNF)</rs>.</p><p>The authors would also like to thank the <rs type="institution">Information Retrieval group at the Democritus University of Thrace, Greece (DUTH group)</rs> for sharing visual features with all other task participants.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Tsh9Pt4">
					<idno type="grant-number">258191</idno>
					<orgName type="project" subtype="full">Chorus+</orgName>
				</org>
				<org type="funding" xml:id="_wSXu2f5">
					<idno type="grant-number">249008</idno>
				</org>
				<org type="funded-project" xml:id="_NXatqdk">
					<idno type="grant-number">ANR-08-CORD-009</idno>
					<orgName type="project" subtype="full">Georama</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="16,142.61,515.01,337.98,8.63;16,150.95,525.97,229.27,8.63" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="16,443.18,515.01,37.41,8.63;16,150.95,525.97,146.84,8.63">DUTH at ImageCLEF 2011 Wikipedia Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Avi</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Konstantinos</forename><surname>Zagoris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Savvas</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,536.80,337.98,8.63;16,150.95,547.75,329.64,8.63;16,150.95,558.71,146.96,8.63" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="16,437.33,536.80,43.26,8.63;16,150.95,547.75,329.64,8.63;16,150.95,558.71,45.92,8.63">Evaluating some contextual factors for image retrieval: ReDCAD participation at ImageCLE-FWikipedia</title>
		<author>
			<persName coords=""><forename type="first">Hatem</forename><surname>Awadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mouna</forename><surname>Torjmen Khemakhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maher</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jemaa</forename></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,569.54,337.98,8.63;16,150.95,580.50,329.64,8.63;16,150.95,591.46,256.38,8.63" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="16,267.45,580.50,213.14,8.63;16,150.95,591.46,173.95,8.63">DEMIR at ImageCLEFwiki 2011: Evaluating Different Weighting Schemes in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Tolga</forename><surname>Berber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><forename type="middle">Hosseinzadeh</forename><surname>Vahid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Okan</forename><surname>Ozturkmenoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roghaiyeh</forename><surname>Gachpaz Hamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adil</forename><surname>Alpkocak</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,602.29,337.98,8.63;16,150.95,613.25,247.86,8.63" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="16,398.21,602.29,82.38,8.63;16,150.95,613.25,165.20,8.63">UAIC&apos;s participation at Wikipedia Retrieval @ ImageCLEF 2011</title>
		<author>
			<persName coords=""><forename type="first">Emanuela</forename><surname>Boros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandru-Lucian</forename><surname>Ginsca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,624.08,337.98,8.63;16,150.95,635.04,302.36,8.63" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="16,390.45,624.08,90.14,8.63;16,150.95,635.04,201.32,8.63">XRCE and CEA LIST&apos;s Participation at Wikipedia Retrieval of ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stphane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,645.84,337.98,8.65;16,150.95,656.82,75.40,8.63" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="16,368.57,645.87,89.81,8.63">SZTAKI @ ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">Andrs</forename><forename type="middle">A</forename><surname>Blint Dar czy, R bert Pethes</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bencz r</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,119.70,337.98,8.63;17,150.95,130.66,329.64,8.63;17,150.95,141.62,329.64,8.63;17,150.95,152.46,329.64,8.74;17,150.95,163.42,214.48,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,311.35,141.62,169.25,8.63;17,150.95,152.58,81.11,8.63">MM: modular architecture for multimedia information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Bertrand</forename><surname>Delezoide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Le</forename><surname>Herv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Romaric</forename><surname>Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gal</forename><surname>Besanc on</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>De Chalendar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Faza</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Gara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meriama</forename><surname>Hde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Laib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre-Alain</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nasredine</forename><surname>Moellic</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Semmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,248.26,152.46,232.34,8.55;17,150.95,163.42,127.15,8.55">Proceedings of the 8th International Workshop on Content-Based Multimedia Indexing (CBMI 2010)</title>
		<meeting>the 8th International Workshop on Content-Based Multimedia Indexing (CBMI 2010)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="136" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,172.44,337.98,10.68;17,150.95,185.43,329.64,8.65;17,150.95,196.41,205.40,8.63" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="17,291.76,185.45,188.83,8.63;17,150.95,196.41,122.55,8.63">SINAI at ImageCLEF Wikipedia Retrieval task 2011: testing combined systems</title>
		<author>
			<persName coords=""><forename type="first">Miguel</forename><surname>ngel Garca-Cumbreras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Carlos Daz-Galiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Alfonso Ure a-L pez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javier</forename><surname>Arias-Buenda</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,207.37,337.98,8.63;17,150.95,218.33,329.64,8.63;17,150.95,229.29,123.34,8.63" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="17,186.60,218.33,293.99,8.63;17,150.95,229.29,22.30,8.63">Multimodal information approaches for the Wikipedia collection at Image-CLEF</title>
		<author>
			<persName coords=""><forename type="first">Ruben</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joan</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xaro</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Esther</forename><surname>De Ves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><surname>Garca-Serrano</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.24,240.13,338.35,8.74;17,150.95,251.21,20.17,8.63" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Vivien</forename><surname>Petras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<title level="m" coord="17,381.35,240.13,95.71,8.55">CLEF 2011 working notes</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.24,262.17,338.35,8.63;17,150.95,273.01,289.81,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,372.32,262.17,108.27,8.63;17,150.95,273.13,120.07,8.63">Overview of the wikipedia retrieval task at imageclef 2010</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jana</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,288.77,273.01,125.55,8.55">Working notes of ImageCLEF 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.24,284.08,338.35,8.63;17,150.95,295.04,329.64,8.63;17,150.95,306.00,17.18,8.63" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="17,376.24,284.08,104.35,8.63;17,150.95,295.04,268.24,8.63">UNT at ImageCLEF 2011: Relevance Models and Salient Semantic Analysis for Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Miguel</forename><forename type="middle">E</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chee</forename><forename type="middle">Wee</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samer</forename><surname>Hassan</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.24,316.96,338.35,8.63;17,150.95,327.80,329.64,8.74;17,150.95,338.76,329.64,8.74;17,150.95,349.84,166.93,8.63" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="17,416.08,316.96,64.51,8.63;17,150.95,327.92,208.53,8.63">Chatzichristofis. www.mmretrieval.net: a multimodal search engine</title>
		<author>
			<persName coords=""><forename type="first">Konstantinos</forename><surname>Zagoris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Avi</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Savvas</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,386.27,327.80,94.32,8.55;17,150.95,338.76,297.97,8.74">Proceedings of the Third International Conference on SImilarity Search and APplications, SISAP &apos;10</title>
		<meeting>the Third International Conference on SImilarity Search and APplications, SISAP &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="117" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.24,360.77,338.35,8.65;17,150.95,371.76,180.35,8.63" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="17,306.56,360.80,174.03,8.63;17,150.95,371.76,79.31,8.63">BTU DBIS&apos; Multimodal Wikipedia Retrieval Runs at ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Zellh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">fer</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>ttcher</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
