<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,119.67,100.02,363.66,12.62;1,196.34,117.95,210.31,12.62">XRCE and CEA LIST&apos;s Participation at Wikipedia Retrieval of ImageCLEF 2011</title>
				<funder ref="#_aHg55dZ">
					<orgName type="full">French ANR (Agence Nationale de la Recherche)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,154.72,155.87,70.74,8.74"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.62,155.87,85.24,8.74"><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">LIG</orgName>
								<orgName type="institution">Univ. Grenoble I</orgName>
								<address>
									<addrLine>BP 53</addrLine>
									<postCode>-38041</postCode>
									<settlement>Grenoble cedex 9, Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,371.36,155.87,69.14,8.74"><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
							<email>adrian.popescu@cea.fr</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">CEA</orgName>
								<orgName type="institution" key="instit2">LIST, Vision &amp; Content Engineering Laboratory</orgName>
								<address>
									<postCode>92263</postCode>
									<settlement>Fontenay aux Roses</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,119.67,100.02,363.66,12.62;1,196.34,117.95,210.31,12.62">XRCE and CEA LIST&apos;s Participation at Wikipedia Retrieval of ImageCLEF 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">18E8409CA3C8D779999A33D0765CEF14</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-modal Information Retrieval</term>
					<term>Wikipedia Retrieval</term>
					<term>Fisher Vector</term>
					<term>Lexical Entailment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this document, we first recall briefly our baseline methods both for text and image retrieval and describe our information fusion strategy, before giving specific details concerning our submitted runs.</p><p>As text retrieval, XRCE used either and Information-Based IR model <ref type="bibr" coords="1,420.92,324.69,10.52,8.74" target="#b3">[4]</ref> or a Lexical Entailment IR model based on statistical translation IR model <ref type="bibr" coords="1,394.55,336.64,9.97,8.74" target="#b4">[5]</ref>. Alternatively, we also used an approach from CEA List that models the queries using on one hand socially related Flickr tags and on the other hand Wikipedia concepts introduced in [13]. The combination of these runs have shown that the approaches were rather complementary.</p><p>As image representation, we used spatial pyramid of Fisher Vectors built on local orientation histograms and local RGB statistics. The dot product was used to define the similarity between two images and to combine the color and texture based ranking we used simple score averaging.</p><p>Finally, to combine visual and textual information, we used a so called the Late Semantic Combination (LSC) method <ref type="bibr" coords="1,288.91,480.61,9.96,8.74" target="#b2">[3]</ref>, where first the text expert is used to retrieved semantically relevant documents, and than the visual and textual scores are averaged to rank these documents. This strategy allowed us to significantly improve over mono-modal retrieval performances. Using the late fusion of the best text expert from XRCE and from CEA and combining with our Fisher Vector based image run with LSC leaded to a MAP of 37% (best score obtained in the Challenge).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Wikipedia Retrieval task consisted of multilingual and multimedia retrieval <ref type="bibr" coords="1,443.62,652.74,14.62,8.74" target="#b13">[14]</ref>. The collection contains images with their captions extracted from Wikipedia in different languages namely French, English and German. In addition, participants were provided with the original Wikipedia pages in wikitext format. The aim was to retrieve as many relevant images as possible from the aforementioned collection, given a textual query translated in the three different languages and one or several query images.</p><p>Each team submitted different types of runs (see Table <ref type="table" coords="1,363.22,724.72,4.43,8.74" target="#tab_0">1</ref>) : mono-media runs (text) and multimedia (mixed) runs with different fusion approaches. As the table shows, we also submitted common runs for which we applied the semantic late filtering or late combination of XRCE textual runs with the textual from CEA List.</p><p>As the results show, these textual runs were complemantary to our runs that used no external resources and hence their combination allowed for further boosting the performance of mono and multi-modal retrieval.</p><p>Our image representation based on Fisher Vectors is briefly recalled in section 4. To combine visual and textual information, we used a semantic filtered late combination method described in section 5. Finally, we give specific details about the submitted run in section 6 and conclude in 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">XRCE Text Based IR Models</head><p>We start from a traditional bag-of-words representation of pre-processed texts where pre-processing includes tokenization, lemmatization, and standard stopword removal. However, in some cases lemmatization might lead to a loss of information. Therefore before building the bag-of-words representation we concatenated a lemmatized version of the document with the original document. We build an index for the image captions and one for the surrounding paragraph containing the images (as last year). For all runs, we average the score obtained on the captions and paragraph index.</p><p>We used basically two textual models, the Smoothed Power Law (SPL) Information-Based Model <ref type="bibr" coords="2,120.45,349.42,10.51,8.74" target="#b3">[4]</ref> and the Lexical Entailment (AX) IR Model <ref type="bibr" coords="2,327.31,349.42,9.96,8.74" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Information Based IR Model (SPL)</head><p>Information models draw their inspiration from a long-standing hypothesis in IR, namely the fact that the difference in the behaviors of a word at the document and collection levels brings information on the significance of the word for the document. This hypothesis has been exploited in the 2-Poisson mixture model, in the notion of eliteness in BM25, and more recently in DFR models. In particular, several researchers, Harter <ref type="bibr" coords="2,302.55,455.71,10.51,8.74" target="#b5">[6]</ref> being one of the first ones, have observed that the distribution of significant, "specialty" words in a document deviates from the distribution of "functional" words. The more the distribution of a word in a document deviates from its average distribution in the collection, the more likely is this word significant for the document considered. This can be easily captured in terms of information:</p><formula xml:id="formula_0" coords="2,190.59,527.18,322.40,8.74">Info(x) = -log P (X = x|λ) = Informative Content (1)</formula><p>If a word behaves in the document as expected in the collection, then it has a high probability P (X = x|λ) of occurrence in the document, according to the collection distribution, and the information it brings to the document, -log P (X = x|λ), is small. On the contrary, if it has a low probability of occurrence in the document, according to the collection distribution, then the amount of information it conveys is greater. In a nutshell, information can be understood as a deviation from an average behavior.</p><p>Overall, the general idea of the information-based family is the following:</p><p>1. Due to different document length, discrete term frequencies (x d w ) are renormalized into continuous values t d w = t(x d w , l d ) 2. For each term w , we assume that the renormalized values t d w follow a probability distribution P on the corpus. Formally, T w ∼ P (.|λ w ). 3. Queries and documents are compared through a measure of surprise, or a mean of information of the form RSV (q, d) = w∈q -q w log P (T w &gt; t d w |λ w ) So, information models are specified by two main components: a function which normalizes term frequencies across documents, and a probability distribution modeling the normalized term frequencies. Information is the key ingredient of such models since information measures the significance of a word in a document. We choosed for our runs the Smoothed Power model proposed in <ref type="bibr" coords="3,395.73,150.87,9.96,8.74" target="#b3">[4]</ref>. This model is specified in two steps: the Divergence from Randomness (DRF) normalization of terms frequencies and the Smooth Power Law (SPL) distributions:</p><p>-DFR Normalization with parameter c:</p><formula xml:id="formula_1" coords="3,96.23,190.32,278.23,27.67">t d w = x d w log(1 + c avg l l d ) -T f w ∼ SPL(λ w = Nw N )</formula><p>where avg l is the mean document length, l d the document length, c a parameter N is the number of documents in the collection and N w the number of document containing word w. The retrieval function is then:</p><formula xml:id="formula_2" coords="3,212.06,256.78,300.94,38.85">RSV (q, d) = w∈q∩d -x q w log( λ t d w t d w +1 w -λ w 1 -λ w ) (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Lexical Entailment based IR Models (AX)</head><p>Berger and Lafferty <ref type="bibr" coords="3,179.20,330.84,10.52,8.74" target="#b1">[2]</ref> addressed the problem of information retrieval as a statistical translation problem with the well-known noisy channel model. This model can be viewed as a probabilistic version of the generalized vector space model. The analogy with the noisy channel is the following one: To generate a query word, a word is first generated from a document and this word then gets "corrupted" into a query word. The key mechanism of this model is the probability P (v|u) that term u is "translated" by term v. These probabilities enable us to address a vocabulary mismatch, and some kinds of semantic enrichments. The problem now lies in the estimation of such probability models.</p><p>We refer here to a previous work <ref type="bibr" coords="3,252.98,426.48,10.51,8.74" target="#b4">[5]</ref> on lexical entailment models to estimate the probability that one term entails another. It can be understood as a probabilistic term similarity or as a unigram language model associated to a word (rather than to a document or a query). Let u be a term in the corpus, then lexical entailment models compute a probability distribution over terms v of the corpus P (v|u). These probabilities can be used in information retrieval models to enrich queries and/or documents and to give a similar effect to use of a semantic thesaurus. However, lexical entailment is purely automatic, as statistical relationships are only extracted from the considered corpus. In practice, a sparse representation of P (v|u) is adopted, where we restrict v to be one of the N max terms that are the closest to u using an Information Gain metric. We computed the probabilistic term similarity on the paragraph collections because we believed paragraphs could capture a better semantical context as opposed to captions. and retain the top 10 words for each word in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CEA LIST Text and Visual Prototype Based Retrieval</head><p>The main idea behind in CEA LIST's approach is to model the queries using on one hand socially related Flickr tags and on the other hand Wikipedia concepts and to combine them. Here we provide only a quick description of Flickr and Wikipedia models which are similar to those introduced in <ref type="bibr" coords="3,131.56,644.34,14.61,8.74" target="#b12">[13]</ref>. The main novelty introduced this year is the creation of a visual topic prototype with visual concepts. Each result image is then characterized in the visual prototype space and the results which match the prototype are ranked higher using a late fusion approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Flickr Query Modeling</head><p>Term relations extracted from Flickr are defined within a photographic tagging language. We use this data source to define an adaptation of the TF-IDF model to the social space. Given a query Q, we define its social relatedness to term T using:</p><formula xml:id="formula_3" coords="4,197.74,103.05,207.52,8.74">SocRel(T |Q) = users(Q, T ) * 1/log(pre(T )) (1)</formula><p>where users(Q, T ) is the number of distinct users which associate tag T to query Q among the top 20,000 results returned by the Flickr API for the query Q; and where pre(T ) is the number of distinct users from a prefetched subset of 30,000 Flickr users that have tagged photos with tag T .</p><p>In this new social weighting scheme, term frequency and document counts from the classical IR formulas are replaced with user counts, which prevents the final relatedness score from being biased by heavy contributions from a reduced number of users. The computation of users(Q, T ) from 20,000 top results is destined to keep computation time low, while accounting for different query relevant contexts. pre(T ) is precomputed from all the tags submitted by a random subset of 120,000 Flickr users.</p><p>Related terms are computed from preprocessed queries. This processing involves removing photographic terms (which can have a negative impact on queries with few results) as well as prepositions and articles from the queries. Both prepositions and photographic terms are kept in precompiled lists, extracted from Wikipedia: the "List of English prepositions" page, lists of photographic terms exist on the "Category:Film techniques" and "Category:Photographic processes" pages. In the following sections, we will use the preprocessed forms of the queries. For instance, skeleton of dinosaur becomes skeleton dinosaur. For this query, the most related Flickr terms are: bones, museum, trex, fossil, natural history museum and tyrannosaurus rex.</p><p>Before using Wikipedia, we create an enriched version of the query (Q E ) by selectively stemming orginal query terms. A Flickr related term is retained as a variant if its edit distance to one stemmed term from the initial query is smaller than three or if the stemmed form is a prefix of the related term, so skeleton dinosaur becomes (skeleton:skeletons) (dinosaur:dinosaurs). Words in the initial query and the top related Flickr term have weights of 1 while terms starting from x = 2 have weights which are normalized with SocRel(T |Q 1 ). This weighting expresses the fact that the importance of a term in the Flickr model decreases with its rank among the socially related terms. . We create a Flickr query model for each language in an identical manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Wikipedia Query Modeling</head><p>When the terms in a query (or a part of a query) are categorical in nature, results that correspond to their subtypes or to other semantically related concepts are ignored in absence of query expansion. For instance, tyrannosaurus or allosaurus are valid subtypes of dinosaur, which is a part of the topic skeleton of dinosaur. Images tagged with these related concepts are potentially relevant for skeleton of dinosaur but they would not be returned when querying with the initial terms. Query expansion is particularly useful in cases when initial queries return a small number of results or for languages that are seldom used in the annotations of the images. Since image queries cover a broad range of concepts and are expressed in different languages, a generic, detailed and multilingual data source is needed to enable an efficient expansion and we consider Wikipedia to be an appropriate data source for the extraction of semantically related concepts.</p><p>We express the semantic relatedness between a query and a Wikipedia article as a combination of two scores. We first measure the overlap between the words in the query and the words in the category section and in the first sentence of encyclopedic articles and then compute the dot product between the query and a vectorial representation of the entire article content. Priority is given to the first score because categorical and definitional information have a priviledged role in defining semantic relatedness. For English, queries are run through WordNet and synonyms are added to the query terms that are unambiguous in WordNet, to avoid introducing noise from polysemous word senses. Overlap is normalized by the number of words in a query and its values vary from 0 (no terms in common between the query and the article's categories) and 1 (all terms in common). Due to the fact that queries usually contain a small number of words, the overlap scores offer a coarse expression of semantic relatedness and a large number of articles will share the same scores. In our system, given the query golf player on court, an article categorized under with golf and player is always better ranked than an article categorized only under player.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Text Retrieval</head><p>The collection is preprocessed in order to represent textual annotations using a TF-IDF formalism. Wikipedia and Flickr models have different roles in the retrieval system. Related concepts from the encyclopedia are used for semantic query expansion, whereas the set of related Flickr tags is exploited for result ranking. We first retrieve all documents in the collection that are annotated with at least one word from the initial query or with one of the related Wikipedia concepts and give these documents a coarse ranking score which is based on the number of terms from the initial query the document is related to. To break the many ties that result from the use of the coarse score, we compute a fine-grained score as the cosine similarity between the document representation and the Flickr query model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Visual Prototype Based Retrieval</head><p>Each 2011 Wikipedia Retrieval topic is provided with four or five example images. We extract visual concepts from these positive examples in order to extract a visual prototype of the query. Extracted visual concepts include one or several of the following: presence of a face, indoor/outdoor scene, black &amp; white vs. color image; photograph/clipart/map content. For instance, the prototype of portrait of Che Guevara includes the face and photograph while a the prototype of golf player on green will include outdoor, photograph, color. Each image in the collection is preprocessed in order to extract the visual concepts described above. We then compare each image in the textual ranking to the query prototype and increase its visual score each time for each matching visual concept. We take a simple approach a give the same score to each different visual concept we extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">XRCE Fisher Vector based Image Representation</head><p>As for the image representation, we used an improved version <ref type="bibr" coords="5,368.18,429.66,15.49,8.74" target="#b11">[12,</ref><ref type="bibr" coords="5,385.33,429.66,12.73,8.74" target="#b10">11]</ref> of the Fisher Vector <ref type="bibr" coords="5,494.73,429.66,14.62,8.74" target="#b9">[10]</ref>. The Fisher Vector can be understood as an extension of the bag-of-visual-words (BOV) representation. Instead of characterizing an image with the number of occurrences of each visual word, it characterizes the image with the gradient vector derived from a generative probabilistic model. The gradient of the log-likelihood describes the contribution of the parameters to the generation process.</p><p>Assuming that the local descriptors I = {x t , x t ∈ R D , t = 1 . . . T } of an image I are generated independently by Gaussian mixture model (GMM) u λ (x) = M i=1 w i N (x|mu i , Σ i ), I can be described by the following gradient vector (see also <ref type="bibr" coords="5,316.07,526.61,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="5,328.25,526.61,11.63,8.74" target="#b9">10]</ref>):</p><formula xml:id="formula_4" coords="5,245.88,547.32,267.13,30.20">G I λ = 1 T T t=1 ∇ λ log u λ (x t )<label>(3)</label></formula><p>where λ = {w i , µ i , Σ i , i = 1 . . . M } are the parameters of the GMM. A natural kernel on these gradients is the Fisher Kernel <ref type="bibr" coords="5,222.67,600.21,9.96,8.74" target="#b6">[7]</ref>:</p><formula xml:id="formula_5" coords="5,157.06,621.19,351.70,13.38">K(I, J) = G I λ F -1 λ G J λ , F λ = E x∼u λ [∇ λ log u λ (x)∇ λ log u λ (x) ] . (<label>4</label></formula><formula xml:id="formula_6" coords="5,508.76,623.41,4.24,8.74">)</formula><p>where F λ is the Fisher information matrix. As it is symmetric and positive definite, F -1 λ has a Cholesky decomposition F -1 λ = L λ L λ and K(I, J) can be rewritten as a dot-product between normalized vectors G λ with: G I λ = L λ G I λ . We will refer to G I λ as the Fisher Vector (FV) of the image I.</p><p>In the case of diagonal covariance matrices Σ i (we denote by σ 2 i the corresponding variance vectors), closed form formulas can be derived for <ref type="bibr" coords="5,134.33,720.06,14.76,8.74" target="#b10">[11]</ref>). As we do not consider G I The Fisher Vector is further normalized with Power (α = 0.5) and L2 normalization as suggested in <ref type="bibr" coords="6,132.14,115.01,15.50,8.74" target="#b10">[11]</ref> and the dot product is used as similarity between the Fisher Vectors. We also used in some cases the spatial pyramid <ref type="bibr" coords="6,245.05,126.96,10.52,8.74" target="#b7">[8]</ref> to take into account the rough geometry of a scene. The main idea is to repeatedly subdivide the image and represent each layout as a concatenation of the representations (in our case Fisher Vectors) of individual sub-images. As we used three spatial layouts (1 × 1, 2 × 2, and 1 × 3), we obtained 3 image representations of respectively N , 4N and 3N dimensions.</p><formula xml:id="formula_7" coords="5,90.00,702.99,423.00,25.80">G I w d i , G I µ d i , G I σ d i , for i = 1 . . . M , d = 1 . . . D (see details in</formula><p>As low level features we used our usual (see for example <ref type="bibr" coords="6,349.38,187.04,10.79,8.74" target="#b0">[1]</ref>) SIFT-like Orientation Histograms (ORH) and local color statistics (COL), i.e. local color means and standard deviations in the R,G and B channels, both extracted on regular multi-scale grids and reduced to 50 or 64 dimensional with Principal Component Analysis (PCA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The Late Semantic Combination Fusion Method</head><p>There has been many research works addressing text/image information fusion. The method we mainly used in our runs was the one we described in <ref type="bibr" coords="6,328.13,293.62,9.96,8.74" target="#b2">[3]</ref>. The intuition behind this technique is that since different media (here text and image) are semantically expressed at different levels, we should not combine them independently as most of information fusion techniques so far do, but on the contrary, we should consider the underlying complementarities that exist between these media. In the case of text/image data fusion, as the results in most ImageClef Task shows <ref type="bibr" coords="6,479.41,341.44,9.96,8.74" target="#b8">[9]</ref>, text based search is more efficient than visual based since it is more difficult to extract the semantics of an image compared to a text. However, basic late fusion approaches showed that combination of visual and textual information can outperform the only text based search. This shows that the two media are often complementary to each other despite the differences between monomedia performances.</p><p>In <ref type="bibr" coords="6,116.62,413.47,9.97,8.74" target="#b2">[3]</ref>, we have shown that the late fusion can be improved by simply adding a semantic filtering step before score combination. This filtering step has the role of enforcing the visual system to search among the set of retrieved objects by the text expert. In this way we impose that images visually similar to the query images share a common semantic (given by the textual query). While this filtering step is the basis of the image reranking methods, we have shown in <ref type="bibr" coords="6,435.46,461.29,10.52,8.74" target="#b2">[3]</ref> that remaining at this level is unsufficient. Indeed, when the visual system has low performance, the image reranking allows to significantly out-perform it however its performance is generally poorer than using only the text alone. Therefore, what we proposed was to combine image reranking with late fusion in order to overcome their respective weaknesses. Note that the strength of image reranking is to realign the visual system to search in a relevant subset with respect to the semantic viewpoint, while the strength of late fusion relies on a well performing text expert.</p><p>Hence, the Late Semantic Combination (LSC) combination works as follows. First, we define a semantic filter for the image scores according to the textual expert:</p><formula xml:id="formula_8" coords="6,229.04,581.35,283.96,23.08">SF (q, d) = 1 if d ∈ KNN t (q) 0 otherwise<label>(5)</label></formula><p>where KNN t (q) denotes the set of the K most similar objects to q according to the textual similarities. Hence, this will give us a reduced list (K) for which we need to compute the image similarities.</p><p>After normalization (all scores are transformed to have values between 0 and 1), the semantically filtered image scores are combined with the text ones:</p><formula xml:id="formula_9" coords="6,186.27,690.23,326.73,9.68">s LSC (q, d) = α t N(s t (q, d)) + α v (N(SF (q, d)s v (q, d)))<label>(6)</label></formula><p>where N is an operator normalizing score between 0 and 1, α t = α and α v = 1 -α are positive weights that sum to 1. Note that the similarity for all documents d that are not in KNN t (q) are set to 0.</p><p>Similarly to 2010, we evaluated different mono-media and multimedia runs on the Wikipedia corpus using the new set of queries. The MAP and P10 performances of these runs are shown in Table <ref type="table" coords="7,117.40,147.84,3.88,8.74" target="#tab_0">1</ref>. Some of the runs were submitted separately by each of the two participants while others were the result of a combination of text experts from XRCE and CEA LIST. The latter runs were obtained either using the late semantic filtering where CEA LIST Runs were used as text expert or first we combined their run with our text expert before using the LSC method.</p><p>In addition, in Table <ref type="table" coords="7,199.54,207.61,4.98,8.74" target="#tab_0">1</ref> we show also the performance of our visual run (not submitted). The performance of these visual runs were again poor, as averaging the Fisher Vectors of the set of image queries was not sufficient to capture the underlying semantics of the query. However, as in 2010, we can observe that in spite of the low performance of the visual expert, the Semantic Filtered Late was able to take advantage of the complementarity of the media types to improve over the pure text based approach. In which follows, we give details on each run individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Text based runs:</head><p>-T1: XRCE Text based retrieval with the Lexical Entailment (AX) based IR Models (section 2.2). -T2: XRCE Text based retrieval with the the Smoothed Power Law (SPL) Information-Based Model (section 2.1). -T3: CEA LIST multilingual textual run (section 3.3).</p><p>-T4: Late fusion between T1 and T2.</p><p>-T5: Late fusion between T1 and T3.</p><p>-T6: Late fusion between T4 and T3.</p><p>We can see from the Table <ref type="table" coords="7,212.06,700.81,4.98,8.74" target="#tab_0">1</ref> that on this dataset AX works better than SPL. While their combination does not seem to help, when we further combine them with T3 (T6) we obtain better performance than just combining T1 with T3 (T5). In all cases, combining XRCE runs with the CEA run (T3) leaded to an absolute improvement of 4% in MAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Image based run.</head><p>Our image run (I1) was based on similarity between Fisher Vector based signatures as described in section 4. We built 4 image signatures for each image corresponding to the two different low level features (ORH and COL) and for each using either a global FV or a spatial pyramid (1x1 2x2 1x3). The 4 FVs were used independently to rank the Wikipedia images using the dot product as similarity measure and the 4 scores were simply averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Text and image based runs.</head><p>The combination of visual and textual runs were done using the Late Semantic Combination combination described in section 5. The image scores were all the same, correponding to I1 described above, so only the text expert changed from one run to other. Hence:</p><p>-M2: used T5, a late fusion between AX and the XRCE text run from CEA.</p><p>-M3: used T6, a late fusion between AX, SPL and the text run from CEA.</p><p>-M4: used T1 corresponding to the AX model. -M5: used T4 corresponding to the late fusion between AX and SPL.</p><p>-M7: used T3, CEA multilingual textual run.</p><p>The three remaining runs were based on M8, a linear combination of textual results and visual prototype based results from CEA (see details in section 3.4). Hence, we had:</p><p>-M6: a late fusion between T4 and M8.</p><p>-M1: a late semantic combination where we used M6 as "the semantic filter" and also we averages the visual scores with the normalized scores of the run M7. Hence, while M7 was not purely text based, it had the role of the "text expert" in the LCS approach.</p><p>As a conclusion, we can again see that the CEA and XRCE runs were complementary with a gain of absolute improvement of 3% in MAP. While adding the visual characterization of the topics to pure text retrieval helps (T2 compared to M6), when we added the visual expert the gain was almost negligible (M1 compared with M2). This is interesting as it reinforces some of our observations we made in <ref type="bibr" coords="8,199.73,475.50,10.52,8.74" target="#b0">[1]</ref> experimenting with the IAPR TC-12 photographic collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This year XRCE participated again with success in the Wikipedia Retrieval Task showing again that despite the fact that pure visual based retrieval led to poor results, when we appropriately combined them with our text ranking we are able to outperform the mono-modal ones. This was shown with various text expert, including those from the CEA LIST.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,268.79,724.90,9.64,7.16;5,274.55,729.57,2.66,4.37;5,283.30,720.06,196.20,8.74;5,480.09,718.48,3.53,6.12;5,479.50,724.92,4.73,6.12;5,488.59,720.06,24.40,8.74;5,90.00,735.55,147.70,8.74;5,238.29,733.97,3.53,6.12;5,237.70,740.39,8.53,7.16;5,242.54,745.06,2.66,4.37;5,250.55,735.55,25.30,8.74;5,276.44,733.97,3.53,6.12;5,275.84,740.39,8.55,7.16;5,280.46,745.06,2.66,4.37;5,288.71,735.55,175.49,8.74"><head>w d i (</head><label>i</label><figDesc>the derivatives according to the weights), G I λ is the concatenation of the derivatives G therefore N = 2M D-dimensional.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,90.00,297.37,423.00,226.17"><head>Table 1 .</head><label>1</label><figDesc>Wikipedia retrieval: overview of the performances of our different runs. Top table deals with monomedia runs and bottom table with multimedia runs.</figDesc><table coords="7,110.90,327.38,381.20,196.16"><row><cell></cell><cell>ID RUN</cell><cell>MAP P@10</cell></row><row><cell></cell><cell cols="2">T6 XRCE CEA TXT RUN SPLAX ENFRDE 0.3141 0.516</cell></row><row><cell></cell><cell>T5 XRCE CEA TXT RUN AX ENFRDE</cell><cell>0.3130 0.530</cell></row><row><cell></cell><cell>T1 XRCE TXT RUN AX</cell><cell>0.2780 0.470</cell></row><row><cell></cell><cell>T4 XRCE TXT RUN SPLAX</cell><cell>0.2769 0.464</cell></row><row><cell></cell><cell>T3 CEA enfrde all</cell><cell>0.2591 0.466</cell></row><row><cell></cell><cell>T2 XRCE TXT RUN SPL</cell><cell>0.2432 0.422</cell></row><row><cell></cell><cell>I1 XRCE VIS FV</cell><cell>0.0271 0.0860</cell></row><row><cell cols="2">Txt Run ID RUN</cell><cell>MAP P@10 Rel. Improv.</cell></row><row><cell></cell><cell>M1 XRCE CEA MULTI RUN SFL AX viscon</cell><cell>0.3880 0.632</cell></row><row><cell>T5</cell><cell>M2 XRCE CEA MULTI RUN AX ENFRDE FV SFL</cell><cell>0.3869 0.624 +23.6 %</cell></row><row><cell>T6</cell><cell cols="2">M3 XRCE CEA MULTI RUN SPLAX ENFRDE FV SFL 0.3848 0.620 +22.5 %</cell></row><row><cell>T1</cell><cell>M4 XRCE MULTI RUN AX FV SFL</cell><cell>0.3557 0.594 +27.9 %</cell></row><row><cell>T4</cell><cell>M5 XRCE MULTI RUN SPLAX FV SFL</cell><cell>0.3556 0.578 +28.4 %</cell></row><row><cell></cell><cell>M6 XRCE CEA MULTI RUN SPLAX VISCON</cell><cell>0.3471 0.574</cell></row><row><cell>T3</cell><cell>M7 CEA XRCE RUN ENFRDE FV SFL</cell><cell>0.3075 0.54</cell></row><row><cell></cell><cell>M8 CEA viscon 1.07</cell><cell>0.2703 0.480</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like also to acknowledge <rs type="person">Florent Perronnin</rs> and <rs type="person">Jorge Sánchez</rs> for the efficient implementation of the Fisher Vectors we used in our experiments.</p><p><rs type="person">Adrian Popescu</rs> was supported by the <rs type="funder">French ANR (Agence Nationale de la Recherche)</rs> via the <rs type="projectName">Periplus</rs> project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_aHg55dZ">
					<orgName type="project" subtype="full">Periplus</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,98.19,715.44,414.80,7.86;8,106.76,726.40,406.24,7.86;8,106.76,737.36,234.06,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,382.18,715.44,130.81,7.86;8,106.76,726.40,294.23,7.86">Leveraging image, text and crossmedia similarities for diversity-focused multimedia retrieval, chapter 3.4</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ah-Pine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J-M</forename><surname>Renders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,445.52,726.40,67.48,7.86;8,106.76,737.36,90.85,7.86">The Information Retrieval Series of etal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,98.19,103.73,414.81,7.86;9,106.76,114.69,406.24,7.86;9,106.76,125.65,58.87,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,246.79,103.73,185.38,7.86">Information retrieval as statistical translation</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,467.12,103.73,45.88,7.86;9,106.76,114.69,376.77,7.86">Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,98.19,136.61,414.81,7.86;9,106.76,147.57,406.24,7.86;9,106.76,158.53,59.25,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,358.86,136.61,154.14,7.86;9,106.76,147.57,165.45,7.86">Semantic combination of textual and visual information in multimedia retrieval</title>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Ah-Pine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,289.98,147.57,223.01,7.86;9,106.76,158.53,30.16,7.86">ACM International Conference on Multimedia Retrieval (ICMR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,98.19,169.49,414.80,7.86;9,106.76,180.45,406.24,7.86;9,106.76,191.41,239.57,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,269.10,169.49,158.51,7.86">Information-based models for ad hoc ir</title>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,447.14,169.49,65.86,7.86;9,106.76,180.45,406.24,7.86;9,106.76,191.41,31.94,7.86">SIGIR &apos;10: Proceeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,98.19,200.10,414.81,10.13;9,106.76,213.32,406.25,7.86;9,106.76,224.28,155.69,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,325.08,202.36,172.24,7.86">Lexical entailment for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,106.76,213.32,365.95,7.86">Advances in Information Retrieval, 28th European Conference on IR Research, ECIR 2006</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-12">April 10-12. 2006</date>
			<biblScope unit="page" from="217" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,98.19,235.24,414.80,7.86;9,106.76,246.20,168.74,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,167.14,235.24,232.73,7.86">A probabilistic approach to automatic keyword indexing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Harter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,411.28,235.24,101.71,7.86;9,106.76,246.20,125.81,7.86">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,98.19,257.16,414.81,7.86;9,106.76,268.12,208.74,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,229.33,257.16,227.43,7.86">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,476.05,257.16,36.95,7.86;9,106.76,268.12,169.42,7.86">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,98.19,279.08,414.82,7.86;9,106.76,290.04,194.20,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,265.37,279.08,247.63,7.86;9,106.76,290.04,121.76,7.86">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,247.95,290.04,23.15,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,98.19,300.99,414.81,7.86;9,106.76,311.95,406.24,7.86;9,106.76,322.91,74.74,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,358.32,300.99,154.68,7.86;9,106.76,311.95,129.01,7.86">ImageCLEF-Experimental Evaluation in Visual Information Retrieval</title>
	</analytic>
	<monogr>
		<title level="s" coord="9,276.66,311.95,134.34,7.86">The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Th</forename><surname>Deselaers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.85,333.87,415.15,7.86;9,106.76,344.83,20.99,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,221.30,333.87,244.08,7.86">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,484.06,333.87,23.15,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.85,355.79,415.15,7.86;9,106.76,366.75,64.52,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,260.26,355.79,248.13,7.86">Large-scale image categorization with explicit data embedding</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,118.27,366.75,23.15,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.85,377.71,415.15,7.86;9,106.76,388.67,172.82,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,379.94,377.71,133.06,7.86;9,106.76,388.67,100.36,7.86">Large-scale image retrieval with compressed fisher vectors</title>
		<author>
			<persName coords=""><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Poirier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,226.57,388.67,23.15,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.85,399.62,415.15,7.86;9,106.76,410.58,207.36,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,280.18,399.62,138.47,7.86">Social media driven image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,436.22,399.62,76.77,7.86;9,106.76,410.58,178.26,7.86">ACM International Conference on Multimedia Retrieval (ICMR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.85,421.54,415.15,7.86;9,106.76,432.50,348.63,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,336.78,421.54,176.22,7.86;9,106.76,432.50,56.99,7.86">Overview of the wikipedia retrieval task at imageclef 2011</title>
		<author>
			<persName coords=""><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jana</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,183.55,432.50,119.47,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
