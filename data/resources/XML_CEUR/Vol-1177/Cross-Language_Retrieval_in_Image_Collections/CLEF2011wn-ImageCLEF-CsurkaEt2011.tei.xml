<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,125.37,100.02,352.26,12.62;1,99.95,117.95,403.10,12.62;1,285.36,135.88,32.28,12.62">XRCE&apos;s Participation at Medical Image Modality Classification and Ad-hoc Retrieval Tasks of ImageCLEF 2011</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,156.66,174.50,70.74,8.74"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.97,174.50,85.24,8.74"><forename type="first">St√©phane</forename><surname>Clinchant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">LIG</orgName>
								<orgName type="institution">Univ. Grenoble I</orgName>
								<address>
									<addrLine>BP 53</addrLine>
									<postCode>-38041</postCode>
									<settlement>Grenoble cedex 9, Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.70,174.50,81.85,8.74"><forename type="first">Guillaume</forename><surname>Jacquet</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,125.37,100.02,352.26,12.62;1,99.95,117.95,403.10,12.62;1,285.36,135.88,32.28,12.62">XRCE&apos;s Participation at Medical Image Modality Classification and Ad-hoc Retrieval Tasks of ImageCLEF 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">16DE91BF995CE304CDC1B10FEF1C19DA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-modal Information Retrieval</term>
					<term>Medical Image Modality Classification</term>
					<term>Ad-hoc Retrieval</term>
					<term>Semi-supervised learning</term>
					<term>Fisher Vector</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The aim of this document is to describe our methods used in the Medical Image Modality Classification and Ad-hoc Image Retrieval Tasks of ImageClef 2011.</p><p>The main novelty in medical image modality classification this year was, that there were more classes (18 modalities) organized in a hierarchy and for some categories only few annotated examples were available. Therefore, our strategy in image categorization was to use a semi-supervised approach. In our experiments, we investigated mono-modal (text and image) and mixed modality based classification. The image classification was based on Fisher Vectors built on SIFT-like local orientation histograms and local color statistics. For text representation we used a binarized bag-ofwords representation where each element indicated whether the term appeared in the image caption or not. In the case of multi-modal classification, we simply averaged the text and image classification scores.</p><p>For the ad-hoc retrieval task, we used the image captions for text retrieval and Fisher Vectors for visual similarity and modality detection. Our text runs were based on a late fusion of different state of the art text experts and the Lexical Entailment model. This Lexical Entailement model used the last year articles to compute similarities between terms and rank first at the previous challenge.</p><p>Concerning the submitted runs, we realized that we forgot by inadvertance 3 , to submit our best run from last year [3]. We did not submit either improvement over this run, which was proposed in <ref type="bibr" coords="1,239.99,545.46,9.96,8.74" target="#b5">[6]</ref>. Overall, this explain the medium performance of our submitted runs. In this document, we show that our system from last year and its improvements would have achieve top performance. We have not tuned the parameter of this system for this year task, we have just evaluated the runs we did not submit !.</p><p>Finally, we experimented with different fusion strategies of our textual expert, visual expert and image modality classification scores, which gives consistent results to last year results and to our analysis presented in <ref type="bibr" coords="1,314.67,642.05,9.96,8.74" target="#b5">[6]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This year the medical retrieval task of ImageCLEF 2011 uses a subset of PubMed Central containing 231,000 images <ref type="bibr" coords="2,192.70,139.41,9.97,8.74" target="#b8">[9]</ref>. As it was indicated by the clinicians that modality is one of the most important filters to limit their search, a first subtask of the Medical Challenge was the Medical Image Modality Classification. Participants were therefore provided a training set of 1K images that have been classified into one out of 18 modalities organized in a hierarchy (see Figure <ref type="figure" coords="2,488.15,175.28,3.88,8.74" target="#fig_0">1</ref>). The main novelty in medical image modality classification this year was, that we had more classes and less annotated data. Furthermore, for some of the categories only few annotated examples were available. Therefore, our main stategy in image categorization was to first automatically augment the training set. We basically used two main approaches, a semi-supervised learning approach and an approach based on an CBIR retrieval scenario (see section 2).</p><p>In both cases, we experimented with mono-modal and multi-modal strategies. In the case of visual modality, we used as image representation Fisher Vectors built on SIFT-like local orientation histograms and local color statistics (see for details <ref type="bibr" coords="2,314.12,512.43,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="2,331.28,512.43,7.01,8.74" target="#b6">7]</ref>, while text (in our case image captions) were represented by a binarized bag-of-words representation, where each element indicated whether the term appeared in the image caption or not. In the case of multi-modal classification, we simply averaged the text and image classification scores.</p><p>Concerning the ad-hoc medical image retrieval task, the only information we used this year was image captions and visual representation. Our text expert was based on a late fusion four textual model built on image captions: a Dirichlet Smoothed language model (DIR), two Power Law Information-Based Model <ref type="bibr" coords="2,224.45,596.21,10.52,8.74" target="#b3">[4]</ref> (SPL and LGD) and the Lexical Entailment IR Model <ref type="bibr" coords="2,474.21,596.21,10.52,8.74" target="#b4">[5]</ref> (AX). The only model that used other information than the provided image captions was the Lexical Entailment, as it used the last years articles to compute similarities between terms. This text expert was combined with our Fisher Vector based visual model, with modality class predictions or both using different fusion strategies (see section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Medical Image Modality Classification Task</head><p>In our experiments we investigated both mono-modal and mixed modality based classification. Concerning the pure visual-based classifiers, we used Fisher Vector <ref type="bibr" coords="2,399.10,712.77,15.50,8.74" target="#b9">[10]</ref> representation of the images as decribed also in <ref type="bibr" coords="2,207.83,724.72,9.97,8.74" target="#b6">[7]</ref>. Note that in the case of medical images we used only a single FV per image and per feature without using the spatial pyramid. The low level features we used were similar to the features used in our Wikipedia runs <ref type="bibr" coords="3,312.59,103.05,9.96,8.74" target="#b6">[7]</ref>, i.e. SIFT-like local orientation histograms (ORH) and local RGB statistics.</p><p>Note, that in the medical corpus a large amount of images were 1 channel gray-scale image.To be able to compute the COL features for this images, we first transformed them into 3 channel images where the R, G, B channels were simply made equal each with the luminance channel of the gray scale image. This allowed us to obtain low level features of the same size for grayscale and color RGB images, and hence, to build a common COL visual vocabulary.</p><p>Concerning our text representation (TXT), we used a binarized bag-of-words representation, where each element indicated whether the term appeared in this document or not (in our case image caption). Similarly to the Fisher Vectors, we further normalized this vector with Power Norm (Œ± = 0.5) and L2 normalization.</p><p>To train the classifiers, we used our own implementation of the Sparse Logistic Regression (SLR) <ref type="bibr" coords="3,120.57,246.60,9.97,8.74" target="#b7">[8]</ref>, (i.e. logistic regression with a Laplacian prior). We trained a classifier per class (oneversus-all) and per feature type (ORH, COL or TXT). Finally, we used the late fusion to combine them where the scores were simply averaged.</p><p>However, when we tested this system on the training data, using a 5 fold cross-validation scheme, the results we obtained were rather poor (see Table <ref type="table" coords="3,354.93,294.46,3.88,8.74" target="#tab_0">1</ref>. Analyzing our results per class, we realized that the low performance might be due to the fact that for some of the categories only few annotated examples were available. Therefore, we decided to automatically increase the training set. We basically used two main approach for this a semisupervised learning approaches and a visual retrieval based approach and the image collection from the medical retrieval task.</p><p>The main idea of the semi-supervised learning approaches was to use the modality classifiers trained on the provided training data to rank the 231K images of the collection based on the classification score. Hence for each modality, the corresponding top K documents were considered as most probably correctly classified, labeled with the given modality and added to the training set.</p><p>In the case of the second scenario, we first built an image query with a random set of images labeled with the given modality. The images in the collection were ranked based on their average similarity (dot product between Fisher Vectors) to the query image set. The top K documents ranked as most similar to the query set were added to the training set labeled with the given modality.</p><p>Finally, the modality classifiers were retrained with the increased training using different feature types and combined as described above. We submitted different runs (detailed below) using either only visual information or both visual and textual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual only based runs:</head><p>Table <ref type="table" coords="3,117.40,668.78,4.98,8.74" target="#tab_1">2</ref> describes the results of the following "visual only" based runs:</p><p>-V1: For this run we used the semi-supervised approach with the visual classifier using COL+ORH features in both steps (training with the original set and training with the increased training set). After the first step we added the top 25 images for each modality classifier. -V2 and V3: For this run we used the visual retrieval to increase the training set. To build our queries, we used the labeled images from two of the 5 folds used in our first experiments (one for each run). The top 20 images for each query were added to the training set and a COL+ORH visual categorization system trained.</p><p>Note that both stategies leaded to similar performances. The choice of k might be non-optimal and a better strategy would be to learn a different k for each modality based on some confidence of the classification scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mixed modality based runs:</head><p>Table <ref type="table" coords="4,131.08,347.74,4.13,7.89">3</ref>. Overview of our mixed modality runs at the Medical Image Modality Classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RUN Modality ACC M1 XRCE all MIX semiLM</head><p>Mixed 0.8691 M2 XRCE Testset MIX semiL50 Mixed 0.8642 M3 XRCE Testset MIX semiL25 Mixed 0.8593 Table <ref type="table" coords="4,132.34,459.06,4.98,8.74">3</ref> describes the results of the following "mixed modality" runs:</p><p>-M1: For this run we used the semi-supervised approach with the multi-modal classifier (COL+ORH+TXT) in both steps (training with the original set and training with the increased training set). After the first step we added the top 50 images for each modality classifier. -M2 and M3: For this run we used the semi-supervised approach with the visual classifier (COL+ORH) in the first step (same as for V1) and the multi-modal classifier (COL+ORH+TXT) in the final step. As their names show, for M2 we added the top 50 and in the case of M3 we added the top 25 images for each classifier.</p><p>As the table <ref type="table" coords="4,148.94,577.13,4.98,8.74">3</ref> shows, all the "mixed modality" runs outperformed the "visual only" runs with about 2-3% in accuracy (see for example V1 and M3, where the enriched training set was the same). Finally, comparing with the results in table 1, we can see that both strategy to increase the training set was extremely useful leading to an absolute 25% over the baseline using only the annotated training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Medical Ad-hoc Image Retrieval Task</head><p>Concerning the ad-hoc medical image retrieval task, the only information we used this year was image captions (not the full articles) and visual representation. Using these information, we built several mono-modal and multi-modal systems. In addition, we also integrated the modality classification scores with these systems. In which follows, we give further details on these methods and the corresponding runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visual only retrieval system</head><p>As visual representation, we used exactly the same representation as in the modality classification, namely Fisher Vectors with ORH and COL features. As we use the dot product as similarity measure, the sum of similarities (used in our case) between the FVs of the corresponding features is equivalent to the dot product between image signatures built as a concatenation of the FV ORH with FV COL. As the Table <ref type="table" coords="5,217.65,172.93,4.98,8.74">4</ref> shows, adding the color information slightly improve the retrieval results, however as expected the visual similarity alone is not sufficient to handle the semantic queries.</p><p>Table <ref type="table" coords="5,118.08,229.69,4.13,7.89">4</ref>. Medical ad-hoc retrieval: using visual only information (not submitted). MAP and P10 are shows using percentages.</p><p>Model MAP P@10 V1 ORH 1.18 7.00 V2 ORH+COL 1.81 11.00</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text based retrieval systems</head><p>Concerning the text representation, we used only the image captions that were first pre-processed including tokenization, lemmatization, and standard stopword removal. As in some cases lemmatization might lead to a loss of information, when we constructed the dictionary, we kept for each term its lemmatized and non lemmatized version. Then, each caption was transformed first into a bag-of-words representation on which we built basically four textual models. These models ( summerized in Table <ref type="table" coords="5,167.27,431.76,4.43,8.74" target="#tab_2">5</ref>) were Diriclet Smoothed standard language model (similar to the techniques used in our past participation in other tasks of ImageCLEF <ref type="bibr" coords="5,345.12,443.71,10.30,8.74" target="#b0">[1]</ref>), two Power Law Information-Based Model <ref type="bibr" coords="5,120.45,455.67,10.51,8.74" target="#b3">[4]</ref> (LGD and SPL) and finally the Lexical Entailment IR Model <ref type="bibr" coords="5,405.42,455.67,9.96,8.74" target="#b4">[5]</ref>. In the first model (DIR), we used with standard Language Model representation the Dirichlet smoothing that gives the following retrieval model <ref type="bibr" coords="5,312.39,599.82,14.62,8.74" target="#b10">[11]</ref>:</p><formula xml:id="formula_0" coords="5,172.43,629.32,101.82,20.73">RSV (q, d) = w‚ààQ,x dw &gt;0</formula><p>x q w log(1 +</p><formula xml:id="formula_1" coords="5,324.81,621.00,188.19,24.80">x d w ¬µp(w|C) ) + l q log ¬µ l d + ¬µ (1)</formula><p>where x d w is the number of occurrences of word w in document d, l d is the length of d in tokens after lemmatization, ¬µ the smoothing parameter and p(w|C) is the corpus language model.</p><p>The Log-logistic model (LGD) has the same steps as the Smoothed Power Law model (SPL) (described in <ref type="bibr" coords="5,147.53,700.14,10.30,8.74" target="#b6">[7]</ref>). The only difference is that Relevance Score Vector in the Ranking Model becomes (see details in <ref type="bibr" coords="5,153.20,712.10,10.30,8.74" target="#b3">[4]</ref>):</p><formula xml:id="formula_2" coords="5,207.68,725.25,305.32,22.21">RSV (q, d) = w‚ààq‚à©d x q w -log P (T f w &gt; t d w ) (2)</formula><p>Our last model, the Lexical Entailment based IR Models (AX) is also described in <ref type="bibr" coords="6,480.89,103.05,9.97,8.74" target="#b6">[7]</ref>. For this model, we need to compute the probabilistic term similarities between terms. However, using only the image captions, gives a context rather poor for the words. As we didn't processed the full articles from this year, we used the processed articles from last year's medical corpus.</p><p>Finally, expecting to bring complementary information, we averaged (with equal weighting) several models to get a single text expert. However, as the Table <ref type="table" coords="6,382.38,163.09,3.88,8.74" target="#tab_3">6</ref>, our expectation was rather wrong, and instead we bringed more noise than useful information. Comparing the four models shows that without any external information, using only the image captions is unsufficient. However, using the AX model alone (not submitted<ref type="foot" coords="6,302.72,197.38,3.97,6.12" target="#foot_0">4</ref> ) gives better performance than all other runs, including the ones submitted to the challenge by other teams. Note that only T6 and T7 (corresponding to the runs XRCE RUN TXTax dir spl respectively XRCE RUN TXT noMOD) were submitted (results in red). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-modal retrieval systems</head><p>As multi-modal retrieval system, we used the Late Semantic Combination (LSC) proposed in <ref type="bibr" coords="6,499.71,473.49,9.97,8.74" target="#b1">[2]</ref>, but also described in <ref type="bibr" coords="6,179.71,485.45,9.97,8.74" target="#b6">[7]</ref>. The main idea of this late fusion is that first we use the text expert to select the top N = 1000 semantically relevant documents, and then we average their textual and visual scores. Results for different text experts using equal weighting (w T = w V = 0.5) and unbalanced weighting between scores images and score texts (w T = 0.9 and w V = 0.1) are shown in Table <ref type="table" coords="6,90.00,533.27,3.88,8.74" target="#tab_4">7</ref>. Note that we submitted only M6 (corresponding to XRCE RUN MIX SFL noMOD ax dir spl) and M7 (corresponding to XRCE RUN MIX SFL noMOD ax dir spl lgd) with equal weigthing (results in red in the table ).</p><p>We can see first that using LSC with equal weighting leaded to a decrease of the MAP but increased in most cases the P10 value. On the contrary, giving a much important weighting for the text scores compared to the image similarities (w T = 0.9 and w V = 0.1), we are able in some cases to improve over our text results such as the model AX. As we obtain similar performances or below using the late fusion, we can say that for this task the image similarities did not really help to improve the retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-modal retrieval systems with image modality prediction</head><p>In this section we show the performance of our retrieval system when we combined them with the modality prediction. Therefore, for each topic each image and the query text was individually clssified by our mono-modal modality classifier, and we retained the modality (or modalities) we obtained. Note that in the case of topics where any type of images were allowed, we also considered only modalities obtained by this automatic model. Hence our model was sub-optimal. Then, we experimented with two strategies. In the first case (FILT), for each document in the dataset that corresponded to the selected modality of the topic we boosted its retrieval score (multiplied by 2), while all other scores were retained unchanged. Hence if a score was high and with the desired modality, it was significantly increased, while if the retrieval score was low, the modality classifier had smaller effect on it.</p><p>In the second case (Mscore), for each document we added to the retrieval score the classification score of the query modality. When several modalities were retrained, we used the maximum of all thoses scores. Results for both strategies applied to different text runs are shown in Table <ref type="table" coords="7,485.22,385.49,3.88,8.74" target="#tab_5">8</ref>. Unfortunately, we submitted only our equal weighted mixed runs M6 and M7 combined with the modality classifier instead of our text runs, shown in table <ref type="table" coords="7,362.49,607.32,3.88,8.74" target="#tab_6">9</ref>. While these results confirm that the modality classification helps, their performance is significantly worse that the performances obtained with TM4 and TM5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this document, we describe the methods we used in Medical Image Modality and Medical Ad-hoc Retrieval Tasks at ImageClef 2011. We have shown that while for some classes only few examples were available, using a semi-supervised approach to increase the training data can lead to very significant improvement of the results. With this method our method came again as best performing in the challenge. Concerning the ad-hoc retrieval task, our strategy to average several text models instead of using only the Lexical Entailment IR Model <ref type="bibr" coords="8,430.30,210.46,10.52,8.74" target="#b4">[5]</ref> (AX) leaded to a text expert that had a medium performance. Further combining this with a equally weighted Late semantic combination leaded to a retrieval system that performed even poorer than our text expert. While the combination of this expert allowed to increase slightly the performance of the system, it remained far behind the best performing systems in the challenge. However, after testing our not submitted runs we realized that our AX text model alone performed better that the best performing text expert in the challenge, and when we further combine it with the modality classifier, the system out-performs the best submitted run.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,233.51,395.21,135.97,7.89;2,195.75,207.45,211.50,163.03"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Medical image modalities.</figDesc><graphic coords="2,195.75,207.45,211.50,163.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,326.56,423.00,51.48"><head>Table 1 .</head><label>1</label><figDesc>Modality classification experiment using the provided training data with a 5 fold cross-validation scheme. The accuracy results (average of the diagonal of the confusion matrix) are in %.</figDesc><table coords="3,159.22,358.82,284.56,19.22"><row><cell cols="5">Features ORH COL TXT ORH+COL ORH+TXT ORH+COL+TXT</cell></row><row><cell>ACC</cell><cell>57.02 55.73 49.23</cell><cell>62.2</cell><cell>61.5</cell><cell>64.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,111.89,99.97,379.22,71.90"><head>Table 2 .</head><label>2</label><figDesc>Overview of our visual only runs at the Medical Image Modality Classification task.</figDesc><table coords="4,193.59,130.73,215.83,41.14"><row><cell>RUN</cell><cell>Modality ACC</cell></row><row><cell>V1 XRCE all VIS semiL25</cell><cell>Visual 0.8359</cell></row><row><cell cols="2">V2 XRCE Testset VIS semi20 CBIR Visual 0.8349</cell></row><row><cell>V3 XRCE all VIS semi20 CBIR</cell><cell>Visual 0.8339</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,177.35,488.57,248.29,71.15"><head>Table 5 .</head><label>5</label><figDesc>Notations for the Runs Name</figDesc><table coords="5,177.35,507.63,248.29,52.10"><row><cell cols="2">Notations Descriptions</cell></row><row><cell>DIR</cell><cell>Dirichlet Smoothed Language Model [11]</cell></row><row><cell cols="2">LGD Log-logistic Information-Based Model [4]</cell></row><row><cell>SPL</cell><cell>Smoothed Power Law Information-Based Model [4]</cell></row><row><cell>AX</cell><cell>Lexical Entailment based IR Model [5]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,90.00,267.40,422.99,128.09"><head>Table 6 .</head><label>6</label><figDesc>Medical ad-hoc retrieval: overview of the performances of using different text models. Finally for comparison, we added the best textual run in the challenge.</figDesc><table coords="6,220.92,299.16,159.39,96.33"><row><cell>Model</cell><cell>MAP P@10</cell></row><row><cell>T1 DIR</cell><cell>16.62 27.00</cell></row><row><cell>T2 LGD</cell><cell>16.67 30.33</cell></row><row><cell>T3 SPL</cell><cell>17.25 29.00</cell></row><row><cell>T4 AX</cell><cell>22.95 38.67</cell></row><row><cell>T5 AX+DIR</cell><cell>21.86 36.67</cell></row><row><cell>T6 AX+DIR+SPL</cell><cell>18.70 32.33</cell></row><row><cell cols="2">T7 AX+DIR+SPL+LGD 18.02 31.00</cell></row><row><cell>best TXT run</cell><cell>21.72 34.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,90.00,99.97,423.00,139.45"><head>Table 7 .</head><label>7</label><figDesc>Medical ad-hoc retrieval: overview of multi-modal runs using the Late Semantic Combination method without modality predictions.</figDesc><table coords="7,187.24,131.73,228.52,107.69"><row><cell>LCS Model /(wT , wV )</cell><cell>(0.5, 0.5)</cell><cell>(0.9, 0.1)</cell></row><row><cell>MIX TXT expert</cell><cell>MAP P@10</cell><cell>MAP P@10</cell></row><row><cell>M1 DIR</cell><cell>15.22 33.67</cell><cell>16.44 27.33</cell></row><row><cell>M2 LGD</cell><cell>13.32 34.33</cell><cell>16.99 29.33</cell></row><row><cell>M3 SPL</cell><cell>13.44 32.67</cell><cell>16.48 29.33</cell></row><row><cell>M4 AX</cell><cell>16.98 37.67</cell><cell>23.28 37.67</cell></row><row><cell>M5 AX+DIR</cell><cell>17.30 36.33</cell><cell>21.87 37.67</cell></row><row><cell>M6 AX+DIR+SPL</cell><cell>14.72 34.33</cell><cell>18.82 32.00</cell></row><row><cell>M7 AX+DIR+SPL+LGD</cell><cell>14.29 33.67</cell><cell>18.10 30.33</cell></row><row><cell>best MIXED run</cell><cell>23.72 39.33</cell><cell>23.72 39.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,90.00,417.35,422.99,139.65"><head>Table 8 .</head><label>8</label><figDesc>Medical ad-hoc retrieval: overview of multi-modal runs using text based expert and modality predictions using different combination strategies.</figDesc><table coords="7,179.18,449.11,244.65,107.89"><row><cell></cell><cell>Model /strategy</cell><cell>Filt</cell><cell>Mscore</cell></row><row><cell cols="2">Ti+Mod TXT expert</cell><cell>MAP P@10</cell><cell>MAP P@10</cell></row><row><cell>TM1</cell><cell>DIR</cell><cell>20.27 38.67</cell><cell>18.48 36.67</cell></row><row><cell>TM2</cell><cell>LGD</cell><cell>19.29 35.67</cell><cell>18.92 34.33</cell></row><row><cell>TM3</cell><cell>SPL</cell><cell>19.77 36.67</cell><cell>19.27 34.00</cell></row><row><cell>TM4</cell><cell>AX</cell><cell>24.15 40.33</cell><cell>21.31 37.67</cell></row><row><cell>TM5</cell><cell>AX+DIR</cell><cell>24.54 41.00</cell><cell>21.55 38.67</cell></row><row><cell>TM6</cell><cell>AX+DIR+SPL</cell><cell>21.10 38.33</cell><cell>20.78 36.00</cell></row><row><cell>TM7</cell><cell>AX+DIR+SPL+LGD</cell><cell>20.54 37.33</cell><cell>20.12 40.00</cell></row><row><cell></cell><cell>best MIXED run</cell><cell>23.72 39.33</cell><cell>23.72 39.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,90.00,99.97,423.00,73.29"><head>Table 9 .</head><label>9</label><figDesc>Medical ad-hoc retrieval: overview of multi-modal runs using the Late Semantic Combination method with modality predictions.</figDesc><table coords="8,155.02,131.73,292.97,41.54"><row><cell></cell><cell>Model /strategy</cell><cell>NoMod</cell><cell>Filt</cell><cell>Mscore</cell></row><row><cell cols="2">Mi+Mod TXT expert</cell><cell>MAP P@10</cell><cell>MAP P@10</cell><cell>MAP P@10</cell></row><row><cell>MM6</cell><cell>AX+DIR+SPL</cell><cell>14.72 34.33</cell><cell>15.20 36.33</cell><cell>16.43 38.00</cell></row><row><cell>MM7</cell><cell>AX+DIR+SPL+LGD</cell><cell>14.29 33.67</cell><cell>15.12 36.67</cell><cell>15.45 38.00</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="6,99.96,726.40,413.04,7.86;6,99.96,737.36,272.61,7.86"><p>As explained in our abstract, we realized that we forgot by inadvertance (and because we participated in several ImageCLEF Task), to submit our best run from last year</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like also to acknowledge <rs type="person">Florent Perronnin</rs> and <rs type="person">Jorge S√°nchez</rs> for the efficient implementation of the Fisher Vector computation and the of the Sparse Logistic Regression (SLR) we used in our experiments.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="8,98.19,421.00,414.82,7.86;8,106.76,431.96,406.24,7.86;8,106.76,442.92,244.45,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,403.03,421.00,109.98,7.86;8,106.76,431.96,261.55,7.86">Leveraging image, text and cross-media similarities for diversity-focused multimedia retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ah-Pine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J-M</forename><surname>Renders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,406.88,431.96,106.11,7.86;8,106.76,442.92,22.00,7.86">The Information Retrieval Series</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,98.19,453.74,414.81,7.86;8,106.76,464.70,406.24,7.86;8,106.76,475.66,59.25,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,358.86,453.74,154.14,7.86;8,106.76,464.70,165.45,7.86">Semantic combination of textual and visual information in multimedia retrieval</title>
		<author>
			<persName coords=""><forename type="first">St√©phane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Ah-Pine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,289.98,464.70,223.01,7.86;8,106.76,475.66,30.16,7.86">ACM International Conference on Multimedia Retrieval (ICMR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,98.19,486.48,414.81,7.86;8,106.76,497.44,406.24,7.86;8,106.76,508.40,21.09,7.86;8,106.76,519.22,406.24,7.86;8,106.76,530.18,20.99,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,248.17,497.44,264.83,7.86;8,106.76,508.40,21.09,7.86;8,106.76,519.22,202.84,7.86">Xrce&apos;s participation in wikipedia retrieval, medical image modality classi cation and ad-hoc retrieval tasks of imageclef 2010</title>
		<author>
			<persName coords=""><forename type="first">St√©phane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Ah-Pine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Jacquet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keyvan</forename><surname>Minoukadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,329.57,519.22,119.63,7.86">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,98.19,541.00,414.80,7.86;8,106.76,551.96,406.24,7.86;8,106.76,562.92,239.57,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,269.10,541.00,158.51,7.86">Information-based models for ad hoc ir</title>
		<author>
			<persName coords=""><forename type="first">St√©phane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,447.14,541.00,65.86,7.86;8,106.76,551.96,406.24,7.86;8,106.76,562.92,31.94,7.86">SIGIR &apos;10: Proceeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,98.19,571.47,414.81,10.13;8,106.76,584.70,406.25,7.86;8,106.76,595.66,155.69,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,325.08,573.74,172.24,7.86">Lexical entailment for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">St√©phane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">√âric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,106.76,584.70,365.95,7.86">Advances in Information Retrieval, 28th European Conference on IR Research, ECIR 2006</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-12">April 10-12. 2006</date>
			<biblScope unit="page" from="217" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,98.19,606.48,414.81,7.86;8,106.76,617.44,349.68,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,362.43,606.48,150.56,7.86;8,106.76,617.44,49.98,7.86">Medical image modality classification and retrieval</title>
		<author>
			<persName coords=""><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">St√©phane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Jacquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,175.91,617.44,252.39,7.86">International Workshop on Content-based Multimedia Indexing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,98.19,628.26,414.82,7.86;8,106.76,639.22,387.56,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,367.21,628.26,145.79,7.86;8,106.76,639.22,95.92,7.86">Xrces participation at wikipedia retrieval of imageclef 2011</title>
		<author>
			<persName coords=""><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">St√©phane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Jacquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,222.48,639.22,119.47,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,98.19,650.04,414.81,7.86;8,106.76,661.00,171.95,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,273.14,650.04,239.86,7.86;8,106.76,661.00,85.93,7.86">Sparse multinomial logistic regression: Fast algorithms and generalization bounds</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Hartemink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,201.28,661.00,21.39,7.86">PAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,98.19,671.82,414.81,7.86;8,106.76,682.78,373.27,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,377.62,671.82,135.37,7.86;8,106.76,682.78,81.98,7.86">Overview of the clef 2011 medical image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,208.19,682.78,119.47,7.86">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,97.85,693.60,415.15,7.86;8,106.76,704.56,20.99,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,221.30,693.60,244.08,7.86">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,484.06,693.60,23.15,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,97.85,715.38,415.15,7.86;8,106.76,726.34,316.53,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,207.38,715.38,305.62,7.86;8,106.76,726.34,82.00,7.86">A study of smoothing methods for language models applied to ad hoc to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,207.93,726.34,96.34,7.86">Proceedings of SIGIR&apos;01</title>
		<meeting>SIGIR&apos;01</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
