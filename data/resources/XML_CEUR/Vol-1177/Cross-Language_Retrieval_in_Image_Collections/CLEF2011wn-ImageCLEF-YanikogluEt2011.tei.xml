<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.33,116.95,290.71,12.62;1,222.79,134.89,169.78,12.62">Sabanci-Okan System at ImageClef 2011: Plant identification task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,183.43,172.63,74.08,8.74"><forename type="first">Berrin</forename><surname>Yanikoglu</surname></persName>
							<email>berrin@sabanciuniv.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Sabanci University</orgName>
								<address>
									<postCode>34956</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.07,172.63,69.36,8.74"><forename type="first">Erchan</forename><surname>Aptoula</surname></persName>
							<email>erchan.aptoula@okan.edu.tr</email>
							<affiliation key="aff1">
								<orgName type="institution">Okan University</orgName>
								<address>
									<postCode>34959</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.35,172.63,60.10,8.74"><forename type="first">Caglar</forename><surname>Tirkaz</surname></persName>
							<email>caglart@sabanciuniv.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Sabanci University</orgName>
								<address>
									<postCode>34956</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,162.33,116.95,290.71,12.62;1,222.79,134.89,169.78,12.62">Sabanci-Okan System at ImageClef 2011: Plant identification task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A98CD0313F0C4154CB06BBB998F41760</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Plant identification</term>
					<term>mathematical morphology</term>
					<term>morphological covariance</term>
					<term>Fourier descriptors</term>
					<term>Support Vector machines</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our participation in the plant identification task of ImageClef 2011. Our approach employs a variety of texture, shape as well as color descriptors. Due to the morphometric properties of plants, mathematical morphology has been advocated as the main methodology for texture characterization, supported by a multitude of contour-based shape and color features. We submitted a single run, where the focus has been almost exclusively on scan and scan-like images, due primarily to lack of time. Moreover, special care has been taken to obtain a fully automatic system, operating only on image data. While our photo results are low, we consider our submission successful, since besides being our first attempt, our accuracy is the highest when considering the average of the scan and scan-like results, upon which we had concentrated our efforts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The plant identification task in ImageCLEF 2011 consisted of labelling images of plants that were captured by different means (scans, scan-like photos called pseudo-scans and unrestricted photos). The details of the recognition task are described in <ref type="bibr" coords="1,188.97,537.49,9.96,8.74" target="#b5">[6]</ref>. A content-based image retrieval (CBIR) system for plants would be very useful for plant enthusiasts or botanists who would like to learn more about a plant they encounter. The goal of the competition was to benchmark state-of-the-art in this open problem where there are very few systems for identifying unconstrained whole or partial plant images <ref type="bibr" coords="1,358.38,585.31,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="1,370.56,585.31,11.62,8.74" target="#b12">13]</ref>. The existing research in this area is concentrated on isolated leaf identification <ref type="bibr" coords="1,385.43,597.26,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="1,397.61,597.26,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="1,407.03,597.26,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="1,421.41,597.26,12.73,8.74" target="#b11">12,</ref><ref type="bibr" coords="1,435.80,597.26,12.73,8.74" target="#b14">15,</ref><ref type="bibr" coords="1,450.20,597.26,11.62,8.74" target="#b13">14]</ref>.</p><p>Content-based plant identification problem faces many challenges such as color, illumination, size variations that are also common in other CBIR problems, as well as some specific problems such as the variations in the composition of the leaves that makes the plant shape variable. In addition, one can see that color is less identifying in the plant retrieval problem compared to many other retrieval problems, since most plants have green tones as their main color with subtle differences. In the rare cases that color is discriminative for a certain plant, that is when the plant has an unusual color, then it may be the case that the leaves of that plant may also have other colors, due to individual plant or seasonal variations (e.g. Gingko, Eurasian smoketree), as shown in Fig. <ref type="figure" coords="2,409.76,282.60,3.87,8.74" target="#fig_0">1</ref>. Another issue with color in plant identification is due to the challenges posed by the color of the flowers: a flowering plant should be matched despite differences in flower colors.</p><p>While shape is quite discriminative in identifying isolated leafs, it is not as useful in identifying full plant images, since the global shape of a plant is affected from its leaf composition <ref type="bibr" coords="2,283.71,354.33,9.96,8.74" target="#b8">[9]</ref>. In that regard, isolated leaf identification can be said to be a simpler problem compared to the unconstrained images of full or partial plants. One method to address this problem could be to extract an individual leaf image by segmenting the overall plant image. Texture on the other hand seems to be a more robust and useful feature category for plant identification, and is widely used in plant identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overall architecture</head><p>Upon examination of the training images that were categorized according to the capture type, we observed that the scanned and scan-like categories were similar in difficulty and both seemed significantly easier compared to the photo category that included a larger variation in scale. Due to shortage of time, we decided to concentrate our efforts on the scan categories, while the photo category was tackled during the last week -which was insufficient for such a difficult problem.</p><p>The final system is designed as two separate sub-systems, one for scan and scan-like images and another one for photos. Since the meta-data included the acquisition type, an input image is automatically sent to the correct subsystem. The acquisition method was the only meta-data used in the overall system.</p><p>Based on our previous work on plant identification <ref type="bibr" coords="2,374.78,585.38,9.96,8.74" target="#b8">[9]</ref>, we had some experience with the usefulness of different feature groups. For handling photographs, which was the problem addressed in our previous work, we found that global shape descriptors and many of the descriptors considered in the present work, would not be useful if the photo consisted of an overlapping set of leaves, rather than a single leaf. On the other hand for scan and scan-like categories, all three main feature categories are useful: color, texture and shape.</p><p>After experimenting with a large number of descriptors, we selected a 115dimensional feature vector for the scan/scan-like sub-system and its 91-dimensional subset for the photo category. The features used in our system are explained in Section 3. For training the system, we trained a classifier combination using Support Vector Machines (SVMs), as explained in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature extraction</head><p>As we are dealing with objects characterized mainly by their morphometric properties, whenever possible we attributed special preference to using morphological solutions. Since mathematical morphology, a nonlinear image processing framework, excels at shape based image analysis as well as at exploiting the spatial relationships of pixels.</p><p>An additional motivation in this regard has been to test our recently conceived morphological texture descriptors <ref type="bibr" coords="3,313.00,301.59,10.52,8.74" target="#b0">[1]</ref> in the context of a real-world application. Although a rich variety of content descriptors has been investigated, we present in this section primarily those that were included in the final system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Texture Features</head><p>As far as scan and scan-like images are concerned, one can easily remark that we are dealing with relatively low scale variations, that can be easily countered with some form of normalization, while plant alignment is also not a major issue. Consequently, scale invariance set aside, from a texture description point of view, we require descriptors possessing a) a high discriminatory potential, b) illumination invariance, and unless we apply some form of angle normalization, then also c) rotation invariance.</p><p>When it comes to photos however, global texture characterization methods are bound to fail, since besides requiring all kinds of invariances, the background varies extremely in terms of complexity, thus presenting a considerable challenge. Hence, in order to apply any global morphological texture operators, a successful segmentation isolating the plant is necessary.</p><p>A set of novel morphological grayscale texture descriptors, possessing the aforementioned qualities has been recently introduced <ref type="bibr" coords="3,376.10,537.56,9.96,8.74" target="#b0">[1]</ref>, leading to the highest classification scores among grayscale approaches, with a variety of texture benchmark collections, including Outex, CUReT and ALOT. They have been formulated as extensions for morphological covariance, that equip it with rotation and illumination invariance. Among them, we focused particularly on circular covariance histograms (CCH) and rotation invariant points (RIT). In summary, these two features achieve rotation invariance straightforwardly by replacing the point pairs of standard morphological covariance, with a circular structuring element (SE), possessing its center. Although any isotropic SE would suffice, this particular shape has the advantage of preserving the principle of covariance, consisting of comparing pixels at various distances.</p><p>As to illumination invariance, they take advantage of the complete lattice foundation of mathematical morphology. More precisely, morphological operators operate on pixel extrema, and not on their linear combinations. In other words, even if in a set of pixels the overall intensity levels change, as long as the relative order of pixels with respect to their intensity remains the same, the morphological operator under consideration, be it erosion, dilation or a combination thereof, will be unaffected, and will still pick as extremum the same pixel, albeit with a modified intensity value. That is why, conversely to granulometries and covariance, where a Lebesgue measure is used in order to quantify the morphological series, CCH and RIT rely on using directly the characteristic scale of each pixel. While the entire input image is described by means of its histogram of characteristic scales.</p><p>As to the difference of RIT from CCH, it is computed similarly, with the exception of first decomposing the circular SE into anti-diametrical point triplets. Thus there is an additional step of computing a label image, by means of a pixel based fusion. In particular, a rotation invariant measure is used to this end (e. g. minimum, maximum), upon all the intermediately filtered images by point triplets of various orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Color Features</head><p>Since the previously chosen texture descriptors have not yet been extended to color data, it was decided to employ the parallel color texture description strategy, where color is described independently from texture. Among the investigated methods we can mention multi-resolution histograms based on morphological scale-spaces <ref type="bibr" coords="4,189.64,436.81,9.96,8.74" target="#b1">[2]</ref>, in both polar and perceptual color spaces, non-uniformly subquantized saturation weighted LSH histograms, as well as color invariants and color moments. Yet, following an experimental evaluation of these color descriptors, only color moments have been included in the final system.</p><p>To explain, a color image corresponds to a function I defining RGB triplets for image positions (x, y) : I : (x, y) → (R(x, y), G(x, y), B(x, y)). By regarding RGB triplets as data points coming from a distribution, it is possible to define moments. Mindru et al. <ref type="bibr" coords="4,241.11,521.94,15.50,8.74" target="#b10">[11]</ref> have defined generalized color moments M abc pq :</p><formula xml:id="formula_0" coords="4,191.45,552.18,289.15,12.69">M abc pq = x p y q [I R (x, y)] a [I G (x, y)] b [I B (x, y)] c dxdy<label>(1)</label></formula><p>M abc pq is referred to as a generalized color moment of order p+q and degree a+b+c. This descriptor uses all generalized color moments up to the second degree and the first order, which leads to nine possible combinations for the degree: M 100 pq , M 010 pq , M 001 pq , M 200 pq , M 110 pq , M 020 pq , M 011 pq , M 002 pq and M 101 pq . These are combined with three possible combinations for the order, M abc 00 , M abc 10 and M abc 01 , which makes a 27-dimensional feature vector, possessing additionally shift-invariance, if the average is subtracted from all input channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Shape Features</head><p>Undoubtedly, shape plays a major role in plant identification and a plethora of shape descriptors, usually categorized as region-and contour-based, are available. In our case we employed a variety of shape descriptors from both categories.</p><p>Fourier descriptors We used the Fourier descriptors that are widely used to describe shape boundaries, as the main shape feature in our system. The Fourier Transform coefficients of a discrete signal f (t) of length N is defined as:</p><formula xml:id="formula_1" coords="5,202.44,227.52,278.16,30.20">C k = 1 N N -1 t=0 f (t)e -j2πtk/N k = 0, 1..., N -1<label>(2)</label></formula><p>In our case, f (t) is the 8-directional chaincode of the plant, N is the number of points in the chaincode, and C k is the k-th Fourier coefficient.</p><p>The coefficients computed on the chaincode is invariant to translation since the chaincode is invariant to translation. Rotation invariance is achieved by using only the magnitude of the coefficients and ignoring the phase information. Scale invariance is achieved by dividing all the coefficients by the magnitude of the DC component. We used the first 50 coefficients to obtain a fixed-length feature and to eliminate the noise in the leaf contour.</p><p>Width length/volume factor: These two descriptors are slight variations of the leaf width factor (LWF) introduced by Hossain and Amin <ref type="bibr" coords="5,412.76,384.92,9.96,8.74" target="#b7">[8]</ref>. Specifically, given an isolated leaf image (f ), their method consists in dividing it into n strips, perpendicular to its major axis (Fig. <ref type="figure" coords="5,293.24,408.83,3.87,8.74">2</ref>). For the final n-dimensional feature, they compute the length of each strip (l i ), divided by the length of the entire leaf (l):</p><formula xml:id="formula_2" coords="5,261.80,444.70,218.79,11.14">LW F n = {l i /l} 1≤i≤n<label>(3)</label></formula><p>Fig. <ref type="figure" coords="5,242.37,546.97,4.13,7.89">2</ref>. Example of leaf decomposition <ref type="bibr" coords="5,380.34,547.00,9.22,7.86" target="#b7">[8]</ref>.</p><p>We derived two new features from this. The Width length factor normalizes the lengths of each strip by the maximum width of the leaf. This is necessary as we normalize the length of each leaf into a fixed size during preprocessing, leaving the width as variable. The second derived feature is obtained by integrating into LWF the grayscale variations of each strip (f i ), thus obtaining Width volume factor (WVF). Specifically, we employ the ratio of volumes (i. e. sum of pixel values) instead of lengths:</p><formula xml:id="formula_3" coords="5,235.87,665.17,244.73,11.15">W V F n = {Vol(f i )/ Vol(f )} 1≤i≤n<label>(4)</label></formula><p>Convexity: This mono-dimensional feature aims to describe the overall contour smoothness of its binary input, which is assumed to be consisting of a single connected component. To explain, after isolating and binarizing the plant image, we compute its convex hull (CH) and then trivially derive its convexity:</p><formula xml:id="formula_4" coords="6,218.00,179.38,262.59,22.31">Convexity(f ) = Area(CH(f )) -Area(f ) Area(f )<label>(5)</label></formula><p>Basic shape statistics: This descriptor (BSS) on the other hand, operates on the contour profile of its binary input. Specifically, we start by computing the center of mass of a given binary plant image (f ), assumed to be consisting of a single connected component. Then we obtain its morphological internal gradient, computed by means of 3 × 3 square SE:</p><formula xml:id="formula_5" coords="6,271.92,291.86,208.67,9.65">g i (f ) = f -ε(f )<label>(6)</label></formula><p>Next, we calculate the Euclidean distances from the aforementioned center to each of the border pixels, which leads to a discrete series S(f ); in case of rotation of the input image, S(f ) is only shifted horizontally. Thus the final feature is obtained by means of 4 simple statistical measures on S:</p><formula xml:id="formula_6" coords="6,182.09,375.78,298.50,8.74">BSS(f ) = {max(S(f )), min(S(f )), med(S(f )), var(S(f ))}<label>(7)</label></formula><p>using its maximum (max), minimum (min), median (med) and variance (var). And since they are horizontally translation invariant w.r.t S(f ), they lead to a simple yet effective and rotation invariant description.</p><p>Border Covariance: Similarly to basic shape statistics, border covariance (BK) also operates on the contour profile S(f ) of its binary input, under the same assumptions. This time however, instead of computing simple statistical measures, we aim to capture contour regularity. To this end we employ morphological covariance, along with a horizontal pair of points. In other words, we treat the contour profile as a mono-dimensional texture. We modified the standard morphological covariance operator so as to employ openings and closings instead of erosions, in order to capture respectively both bright details on dark background, as well as dark details on bright background:</p><formula xml:id="formula_7" coords="6,237.89,574.44,238.46,25.46">BK n T (f ) = Vol(T P2,v (S(f )) Vol(S(f )) v (<label>8</label></formula><formula xml:id="formula_8" coords="6,476.35,581.78,4.24,8.74">)</formula><p>where T denotes the morphological operator (either an opening γ or a closing φ), Vol the sum of pixel values of f and P 2,v a pair of points separated by a vector v. Moreover, it should be noted that since we use a horizontal pair of points (i. e. translation invariant w.r.t the contour profile), the resulting feature is thus rotation invariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scan and Scan-like Images</head><p>Mainly due to the collaboration of two universities, we trained two separate classifiers using two different sets of features. For the first classifier (Classifier 1 ), we used a 67-dimensional shape feature consisting of the 50 Fourier descriptors, the width length factor, eccentricity and solidity. For the second classifier (Classifier 2 ), we used a 115-dimensional feature consisting of all the contour, texture and shape features described in Section 3, excluding the Fourier descriptors.</p><p>For both classifiers, we used an SVM using the radial basis function kernel. The outputs of these classifiers are distances of the test instance to each plant class. We then trained a third classifier to learn how to combine these two classifiers at score level. Hence, the feature vector used in training the combiner is of length 2 × K, where K is the number of classes in the problem.</p><p>In the final stage of classification the most probable 5 classes are selected and a multi-class SVM is trained specifically for those classes for disambiguation. This was done because we found it beneficial to train classifiers that would learn to distinguish similar classes (e.g. different kinds of maples which are very similar amongs themselves, compared to the other plants). While the original idea was to train one such classifier according to the number of lobes in the leaves, the difficulty in assessing this information and the remaining complexity of this task led us to train a new classifier on the fly, using only the training instances from the 5 most probable classes and all of the 182 (=115+67) features. We use the outcome of this stage as the final classification decision. Cross-validation accuracies obtained for each classifier using the training data set are summarized in Table <ref type="table" coords="7,173.78,429.94,3.87,8.74" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Photographs</head><p>As far as photos are concerned, due to time constraints, neither their feature extraction nor their classification received the attention they deserved. Since shape features were not used, we trained only a single SVM classifier, using a 91-dimensional feature vector. For this classifier, default parameters (cost = 25, first degree polynomial kernel) of the Weka SVM software <ref type="bibr" coords="7,386.78,645.16,10.52,8.74" target="#b6">[7]</ref> and the Sequential Minimal Optimization (SMO) algorithm were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental design</head><p>In this section we describe the implementational choices that have been made, as well as the experiments that have been carried out, while designing &amp; optimizing our plant identification system. The majority of experimentations has been realized using Weka (v.3.6.4) <ref type="bibr" coords="8,284.22,179.77,9.96,8.74" target="#b6">[7]</ref>. In all stages of our experiments and in the results given in throughout this paper, we used cross-validation on the training data and measured the average accuracy obtained across images, rather than the average user-based accuracy used in the competition.</p><p>For practical reasons, we chose to first optimize our descriptor combination, then the preprocessing scheme, followed finally by the classification step. Given their visual similarity, we handled scan and scan-like images identically, while treating photos separately. Special care has been taken to obtain a fully automatic system.</p><p>For the sake of simplicity, and in order to minimize the number of variables, initial feature selection experiments have been carried out with a nearest neighbour (1NN) classifier along with the χ 2 -distance, while subsequently we switched to Support Vector Machines (SVM). Whenever necessary, conversion to grayscale has been realized using the weighted combination of RGB channels, as 0.299 × R + 0.587 × G + 0.114 × B, while binarization has been achieved using Otsu's threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Feature Selection</head><p>Preliminary experimentation with various features has been realized by handling shape, color and texture descriptors separately, in an effort to determine the most suitable among them for the problem under consideration. After this step, we experimented with their combination and parameterization.</p><p>Scan and scan-like images: At this stage a relatively simple preprocessing step was realized, consisting of first extracting the bounding rectangle of the plant, followed by scale normalization resulting in a fixed height of 600 pixels. This was applied to all 71 classes containing a total of 3066 scan and scan-like images. Then a series of cross-validation experiments took place, in an effort to determine the most suitable descriptors for distinguishing among these classes. The results are shown in Table <ref type="table" coords="8,272.80,537.89,3.87,8.74" target="#tab_1">2</ref>, along with their arguments.</p><p>The accuracy scores have been obtained by dividing the available data randomly into train (1444 samples) and test (1622 samples) sets, using the aforementioned classification settings. Interestingly, one can observe that texture exhibits the highest discriminatory potential, followed by color and shape.</p><p>In addition to measuring the individual discriminatory potential of each feature, we experimented with many of their combinations. The resulting scores are given in Table <ref type="table" coords="8,202.18,621.57,3.87,8.74" target="#tab_2">3</ref>, where we present the classification accuracies obtained with various combinations of feature sets. Photographs: Conversely to scan and scan-like images, the main challenge presented by photos lies in isolating the plant from its often very complicated background. Due mainly to lack of time, we hardly had any chance of constructing an optimized feature set for this image category, as done for the other images. Instead, we transferred almost directly our descriptor choices for scan and scanlike data, with no additional preprocessing whatsoever. Nonetheless, one of the very few experiments with photos that has been carried out, consisted of simply testing the combination of the features given in Table <ref type="table" coords="9,365.25,455.62,3.87,8.74" target="#tab_1">2</ref>, with the end of adapting them to the new content.</p><p>In particular, we joined the scan and scan-like images with photos, thus obtaining a total of 3996 samples, and divided them equally and randomly into training and test sets. The classification accuracies are provided in Table <ref type="table" coords="9,449.11,503.44,3.87,8.74" target="#tab_3">4</ref>. Considering the background complexity of photos, contour-based shape descriptors suffered a significant performance loss; which was expected, since they rely heavily on correct border extraction. Border covariance in particular has been unable to contribute any longer, so it has been removed from the set of descriptors used to characterize photos. Consequently the length of the feature vector used with photos is 115 -24 = 91. In summary, the addition of photographs has decreased the overall classification performance considerably, with shape descriptors being affected the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Preprocessing</head><p>Having determined a set of features for describing the plant collection, we focused on optimizing the preprocessing stage, in order to further improve performance. Besides the already applied scale normalization, we considered illumination normalization as well, through histogram equalization as proposed in Ref. <ref type="bibr" coords="10,441.69,251.57,9.96,8.74" target="#b4">[5]</ref>. However this did not contribute in any substantial way, probably due to the fact that our primary descriptors are already illumination invariant. Moreover, the removal of the leaf petiole has also been tackled, with the end of obtaining a more accurate plant border for contour-based shape descriptors, but unfortunately its effect has been negligible, probably due to the partial success of the removal procedure. Furthermore, although most of our operators are rotation invariant, WVF and color moments are not. That is why, we chose to apply an additional step, that would align the plant vertically along its major axis. However, the small gains were hindered by mistakes in the angle estimate; thus this normalization did not improve our classification rates. Consequently, as far as scan and scanlike images are concerned their preprocessing consists of extracting the bounding rectangle of a given plant image, followed by its scale normalization to a fixed height of 600 pixels.</p><p>Photographs on the other hand, given their content variation, should benefit from a background removal step. Due to time constraints, we experimented with relatively simple and automatic hue based background removal; but it did not contribute significantly to classification performance, and therefore were not included in the final system's operation. In short, photographs were not preprocessed in any way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Classifier Optimization</head><p>Having determined the descriptors and preprocessing operations for each subproblem (scan and photo), we optimized the parameters of the classifiers. For the two base classifiers used in scan/scan-like categories, the cost and spread parameters (C and γ) of the SVM are learned using 5-fold cross validation and grid search. As for the combiner, we first divided the training set into half. Then, we trained the two classifiers with their optimum parameters on the first half and used the second half to produce distances. In the next step, we found the optimum parameters for the combiner using 5-fold cross validation and grid search on the parameters using the produced distances. Furthermore, as we analyzed the errors of the system on training data, we decided to add a final classifier which is trained with only the closely scoring (top-5 classes) classes' instances, as explained in Section 4. For the single classifier used in photos, we used the default SVM parameters due to shortness of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and discussion</head><p>According to the official results (Table <ref type="table" coords="11,307.90,194.17,4.43,8.74" target="#tab_4">5</ref>) our run achieved 6 th place in overall classification, 2 nd place with scan type images, 6 th place with scan-like images and 16 th place with photographs. However, we did achieve the best score when considering the average of the scan and scan-like results, upon which we had concentrated our efforts.</p><p>Nevertheless, although the placements came as no surprise, the scores on the other hand have been lower than our expectations, especially when compared with the values obtained during our cross-validation tests with the training data (Tables <ref type="table" coords="11,169.65,290.45,4.98,8.74" target="#tab_2">3</ref> and<ref type="table" coords="11,196.67,290.45,3.87,8.74" target="#tab_3">4</ref>). As a matter of fact, there is a very significant drop of approximately 40% overall. This important difference may be due to several factors, one of which is overfitting of the classifiers. Our descriptor optimization stage may have very well led to excessively powerful features capable of distinguishing the training data quite effectively, yet when faced with the test dataset, containing distinct plants of the same genus, the same features failed to generalize their performance. Furthermore, the scoring function is no longer the average accuracy across images, but instead the average accuracy obtained per user; and of course there is always the possibility of implementation errors.</p><p>In conclusion, given that this is our first participation, we consider our attempt satisfactory and even successful, in the sense that we accomplished our main goal; which consisted of identifying effectively the plants in scan and scanlike images. All the same, our score is far from being perfect. Future work in this category will include testing non-morphological descriptors, in an attempt to harness the advantages of both non-linear and linear image analysis methodologies. As far as photographs are concerned, our main focus will be on the preprocessing stage, with the end goal of isolating the plant effectively from its background, so as to be able to employ the same optimized features as with the other image types. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,207.31,203.09,200.74,7.89;2,169.35,116.83,276.67,71.48"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Color variations of "Eurasian Smoketree".</figDesc><graphic coords="2,169.35,116.83,276.67,71.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.77,460.02,345.83,83.85"><head>Table 1 .</head><label>1</label><figDesc>Cross-validation accuracies of each classifier used for the scan and scan-like categories.</figDesc><table coords="7,163.63,491.77,22.02,7.86"><row><cell>Stage</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,134.77,116.91,345.82,117.13"><head>Table 2 .</head><label>2</label><figDesc>The descriptors used for scan and scan-like images along with their accuracies.</figDesc><table coords="9,165.92,137.71,279.76,96.33"><row><cell>ID</cell><cell>Descriptor</cell><cell>Applied in</cell><cell cols="2">Size Accuracy (%)</cell></row><row><cell>1</cell><cell>CCH γ,12,#1 + CCH φ,12,#1</cell><cell cols="2">Grayscale 12+12</cell><cell>72.32</cell></row><row><cell>2</cell><cell>RITγ,12,min + RIT φ,12,min</cell><cell cols="2">Grayscale 12+12</cell><cell>53.45</cell></row><row><cell>3</cell><cell>W V F11</cell><cell>Grayscale</cell><cell>11</cell><cell>44.94</cell></row><row><cell>4</cell><cell>Convexity</cell><cell>Binary</cell><cell>1</cell><cell>7.89</cell></row><row><cell>5</cell><cell>BSS</cell><cell>Binary</cell><cell>4</cell><cell>37.30</cell></row><row><cell>6</cell><cell>BK n γ + BK n φ</cell><cell>Binary</cell><cell>12+12</cell><cell>21.33</cell></row><row><cell>7</cell><cell>Color moments</cell><cell>RGB</cell><cell>27</cell><cell>49.94</cell></row><row><cell></cell><cell>Total</cell><cell></cell><cell>115</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.77,249.04,345.83,104.03"><head>Table 3 .</head><label>3</label><figDesc>Accuracies (%) for the combinations of the descriptors given in Table2, for scan and scan-like data.</figDesc><table coords="9,219.25,279.05,169.29,74.02"><row><cell>Descriptors</cell><cell>1NN</cell><cell>SVM/SMO</cell></row><row><cell>1,3</cell><cell>79.59</cell><cell>80.27</cell></row><row><cell>1,2,3</cell><cell>85.02</cell><cell>86.93</cell></row><row><cell>1,2,3,4</cell><cell>85.45</cell><cell>87.11</cell></row><row><cell>1,2,3,4,5</cell><cell>87.36</cell><cell>89.15</cell></row><row><cell>1,2,3,4,5,6</cell><cell>88.59</cell><cell>90.20</cell></row><row><cell>1,2,3,4,5,6,7</cell><cell>88.41</cell><cell>90.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,134.77,117.41,345.83,105.77"><head>Table 4 .</head><label>4</label><figDesc>Cross-validation accuracies (%) for the combinations of the descriptors of Table2with all images, using 1NN and SMO classifiers.</figDesc><table coords="10,223.38,149.16,154.33,74.02"><row><cell>Descriptors</cell><cell>1NN</cell><cell>SMO</cell></row><row><cell>1,3</cell><cell>70.23</cell><cell>71.03</cell></row><row><cell>1,2,3</cell><cell>74.62</cell><cell>74.98</cell></row><row><cell>1,2,3,4</cell><cell>75.59</cell><cell>76.77</cell></row><row><cell>1,2,3,4,5</cell><cell>77.02</cell><cell>78.67</cell></row><row><cell>1,2,3,4,5,6</cell><cell>75.01</cell><cell>76.44</cell></row><row><cell>1,2,3,4,5,7</cell><cell>79.57</cell><cell>81.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,134.77,116.91,345.82,149.61"><head>Table 5 .</head><label>5</label><figDesc>Partial plant retrieval results showing the top-performing systems ordered according to mean performances.</figDesc><table coords="12,143.26,148.67,324.37,117.85"><row><cell>Run</cell><cell cols="5">Scan Scan-like Photo Mean Scan &amp; Scan-like</cell></row><row><cell>IFSC USP run2</cell><cell>0.562</cell><cell>0.402</cell><cell cols="2">0.523 0.496</cell><cell>0.482</cell></row><row><cell cols="2">inria imedia plantnet run1 0.685</cell><cell>0.464</cell><cell cols="2">0.197 0.449</cell><cell>0.574</cell></row><row><cell>LIRIS run3</cell><cell>0.546</cell><cell>0.513</cell><cell cols="2">0.251 0.437</cell><cell>0.529</cell></row><row><cell>LIRIS run1</cell><cell>0.539</cell><cell>0.543</cell><cell cols="2">0.208 0.430</cell><cell>0.541</cell></row><row><cell>IFSC USP run1</cell><cell>0.411</cell><cell>0.430</cell><cell cols="2">0.503 0.448</cell><cell>0.420</cell></row><row><cell>Sabanci Okan-run1</cell><cell>0.682</cell><cell>0.476</cell><cell cols="2">0.053 0.404</cell><cell>0.579</cell></row><row><cell>LIRIS run2</cell><cell>0.530</cell><cell>0.508</cell><cell cols="2">0.169 0.403</cell><cell>0.519</cell></row><row><cell>LIRIS run4</cell><cell>0.537</cell><cell>0.538</cell><cell cols="2">0.121 0.399</cell><cell>0.537</cell></row><row><cell cols="2">inria imedia plantnet run2 0.477</cell><cell>0.554</cell><cell cols="2">0.090 0.374</cell><cell>0.515</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,568.19,337.63,7.86;11,151.52,579.15,44.55,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,202.30,568.19,143.67,7.86">Extending morphological covariance</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,353.89,568.19,78.06,7.86">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
	<note>Submitted</note>
</biblStruct>

<biblStruct coords="11,142.96,590.75,337.63,7.86;11,151.52,601.71,329.07,7.86;11,151.52,612.67,64.53,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,263.37,590.75,217.22,7.86;11,151.52,601.71,60.41,7.86">Morphological description of color images for contentbased retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lefèvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,227.59,601.71,171.04,7.86">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2505" to="2517" />
			<date type="published" when="2009-11">November 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,624.27,337.64,7.86;11,151.52,635.23,317.94,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,287.07,624.27,193.52,7.86;11,151.52,635.23,114.08,7.86">Shape classification using complex network and multi-scale fractal dimension</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,274.00,635.23,109.27,7.86">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="51" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,646.84,337.63,7.86;11,151.52,657.79,291.65,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,375.83,646.84,104.76,7.86;11,151.52,657.79,86.44,7.86">Fractal dimension applied to plant identification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">O</forename><surname>Plotze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Falvo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,245.72,657.79,83.38,7.86">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2722" to="2733" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,296.44,337.63,7.86;12,151.52,307.40,329.07,7.86;12,151.52,318.36,60.33,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,392.36,296.44,88.23,7.86;12,151.52,307.40,178.72,7.86">Illuminant and device invariant colour using histogram equalisation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hordley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,337.63,307.40,77.83,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="190" />
			<date type="published" when="2005-02">February 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,329.31,337.63,7.86;12,151.52,340.27,329.07,7.86;12,151.52,351.23,269.14,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,300.03,340.27,176.71,7.86">The clef 2011 plant image classification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,163.04,351.23,104.06,7.86">CLEF 2011 working notes</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,362.19,337.64,7.86;12,151.52,373.15,329.07,7.86;12,151.52,384.11,43.13,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,464.72,362.19,15.88,7.86;12,151.52,373.15,155.20,7.86">The weka data mining software: An update</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,315.76,373.15,133.97,7.86">SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009-06">June 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,395.07,337.64,7.86;12,151.52,406.03,329.07,7.86;12,151.52,416.99,166.35,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,271.08,395.07,192.37,7.86">Leaf shape identification based plant biometrics</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,406.03,276.30,7.86">International Conference on Computer and Information Technology</title>
		<meeting><address><addrLine>Dhaka, Bangladesh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-12">December 2010</date>
			<biblScope unit="page" from="458" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,427.94,337.64,7.86;12,151.52,438.90,278.55,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,319.47,427.94,161.12,7.86;12,151.52,438.90,80.28,7.86">Plant image retrieval using color, shape and texture features</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kebapci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Unal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,239.68,438.90,90.76,7.86">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2010-04">April 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,449.86,337.98,7.86;12,151.52,460.82,329.07,7.86;12,151.52,471.78,230.75,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,329.16,449.86,151.44,7.86;12,151.52,460.82,171.98,7.86">Multiple classification of plant leaves based on gabor transform and lbp operator</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Man</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,342.72,460.82,137.87,7.86;12,151.52,471.78,66.85,7.86">International Conference on Intelligent Computing</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="432" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,482.74,337.97,7.86;12,151.52,493.70,329.07,7.86;12,151.52,504.66,213.90,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,389.05,482.74,91.53,7.86;12,151.52,493.70,227.11,7.86">Moment invariants for recognition under changing viewpoint and illumination</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Mindru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Moons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,390.98,493.70,89.61,7.86;12,151.52,504.66,84.22,7.86">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="3" to="27" />
			<date type="published" when="2004-06">April-June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,515.62,337.98,7.86;12,151.52,526.57,254.64,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,278.91,515.62,128.85,7.86">Shape based leaf image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,415.39,515.62,65.20,7.86;12,151.52,526.57,152.20,7.86">IEE Proceedings -Vision, Image and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="43" />
			<date type="published" when="2003-02">Feb 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,537.53,337.98,7.86;12,151.52,548.49,329.07,7.86;12,151.52,559.45,329.07,7.86;12,151.52,570.41,143.78,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,458.70,537.53,21.89,7.86;12,151.52,548.49,329.07,7.86;12,151.52,559.45,75.98,7.86">Plant species identification, size, and enumeration using machine vision techniques on near-binary images</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Woebbecke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Von Bargen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Mortensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,251.48,559.45,141.13,7.86">Optics in Agriculture and Forestry</title>
		<meeting><address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">1836</biblScope>
			<biblScope unit="page" from="208" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,581.37,337.97,7.86;12,151.52,592.33,221.36,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,368.74,581.37,111.84,7.86;12,151.52,592.33,91.35,7.86">Shape-based image retrieval in botanical collections</title>
		<author>
			<persName coords=""><forename type="first">Itheri</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Hervé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nozha</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,262.15,592.33,17.73,7.86">PCM</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="357" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,603.29,337.97,7.86;12,151.52,614.25,329.07,7.86;12,151.52,625.20,79.86,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,312.12,603.29,168.47,7.86;12,151.52,614.25,212.34,7.86">Identification of idealized leaf types using simple dimensionless shape factors by image analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yonekawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,371.94,614.25,103.12,7.86">Transactions of the ASAE</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1525" to="2533" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
