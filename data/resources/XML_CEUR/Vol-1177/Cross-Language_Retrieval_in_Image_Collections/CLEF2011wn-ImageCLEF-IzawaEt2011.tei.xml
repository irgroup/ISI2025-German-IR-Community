<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.18,151.67,326.32,12.54;1,159.98,169.07,275.41,12.54">Annotation and Retrieval System Using Confabulation Model for ImageCLEF2011 Photo Annotation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,198.38,207.32,41.00,9.05"><forename type="first">Ryo</forename><surname>Izawa</surname></persName>
							<email>rizawa27@cs.meiji.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
								<address>
									<addrLine>1-1-1 Higashimita, Tama-ku, Kawasaki-shi</addrLine>
									<postCode>214-8571</postCode>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.32,207.32,67.54,9.05"><forename type="first">Naoki</forename><surname>Motohashi</surname></persName>
							<email>motohashi@cs.meiji.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
								<address>
									<addrLine>1-1-1 Higashimita, Tama-ku, Kawasaki-shi</addrLine>
									<postCode>214-8571</postCode>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.93,207.32,70.15,9.05"><forename type="first">Tomohiro</forename><surname>Takagi</surname></persName>
							<email>takagi@cs.meiji.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
								<address>
									<addrLine>1-1-1 Higashimita, Tama-ku, Kawasaki-shi</addrLine>
									<postCode>214-8571</postCode>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.18,151.67,326.32,12.54;1,159.98,169.07,275.41,12.54">Annotation and Retrieval System Using Confabulation Model for ImageCLEF2011 Photo Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BE686BBDC65476DA8B2AE2A2AD3CFB1E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bag of Visual Words</term>
					<term>Confabulation Model</term>
					<term>ImageCLEF</term>
					<term>Flickr User Tag</term>
					<term>Annotation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe systems we developed and submitted to ImageCLEF2011. These systems are applied to the confabulation model which is based on the human brain structure and are used for image recognition and retrieval. Simply put, these systems involve the co-occurrence of visual words. The visual words approach has recently become a focus of attention in the field of image recognition. We propose a new approach that differs from the ordinary technique and evaluate its effectiveness by participating in this workshop.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The annotation task <ref type="bibr" coords="1,215.51,465.37,11.69,9.05" target="#b0">[1]</ref> at ImageCLEF2011 is a task designed to evaluate the accuracy of "automatic image annotation." In this task, 99 concepts such as "Dog," "Sea," and "flower" are provided to the participants. The concept based retrieval task <ref type="bibr" coords="1,124.82,499.93,10.69,9.05" target="#b0">[1]</ref>, which is extended from the annotation task, involves retrieving images that are relevant to the provided topics (40 topics). Participation in this task is easy because we can use the resources of the annotation task. The MIR Flickr 1 million image dataset <ref type="bibr" coords="1,155.42,534.37,11.69,9.05" target="#b1">[2]</ref> is used as a dataset for these tasks. In addition, we can use the Flickr User Tags attached to each image. These tasks are carried out by using three approaches: "Visual Information only," "Flickr User Tags only," and a "Multi-modal" approach. Then we evaluated which approach was most effective.</p><p>There is currently a need for image retrieval technology to search for target images from a large number of images. Thus, a lot of research is being done on image recognition systems. In particular, generic object recognition systems, in which a general object is recognized by searching for its name, have been studied closely for the last several decades. However, such systems have not yet reached a level of practical use. The main reason for this is that there is a semantic gap between the image features and recognition.</p><p>Recently, the Bag of Visual Words <ref type="bibr" coords="1,283.97,672.28,11.58,9.05" target="#b2">[3]</ref> technique has been very popular in image recognition and retrieval. In this technique, the key points extracted by Scale Invariant Feature Transform (SIFT) <ref type="bibr" coords="2,234.89,149.48,11.69,9.05" target="#b3">[4]</ref> are quantized to patterns that are called visual words, and an image is represented as a frequency histogram of these words. This method performs well when the objective is to simply represent an image. Therefore, many techniques using this approach have been proposed.</p><p>We propose a mixed approach that combines the Bag of Visual Words technique with the Confabulation model. The Confabulation model is based on the structure of the human brain. Although recent techniques have used machine learning techniques such as the Support Vector Machine (SVM), several problems may arise. For example, the computational cost may be large if the amount of training data is excessive; additionally, the classifier that is used needs to be constructed to identify many categories. We attempt to resolve these problems by using a recognition approach that is familiar to human beings. Although the Confabulation model was applied to natural language processing in <ref type="bibr" coords="2,220.73,287.39,11.69,9.05" target="#b4">[5]</ref> [6] <ref type="bibr" coords="2,250.49,287.39,11.69,9.05" target="#b6">[7]</ref> [8], our goal is to evaluate the accuracy when it is applied to image processing.</p><p>This paper is organized as follows: Section 2 describes the Confabulation model, and section 3 explains the idea extended from it. Section 4 details our submitted systems, and section 5 explains their results. Finally, a conclusion is given in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Confabulation Model</head><p>In this section, we describe the confabulation model that forms the foundation of our study and explain how we apply this model to generic object recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Confabulation Model</head><p>Hecht-Nielsen advocated the theory, based on brain science, that human recognition is caused by the co-occurrence of multiple attributes. He named this model the Confabulation model and conducted an experiment to predict which word would appear next in a consecutive list of words (Fig. <ref type="figure" coords="2,314.65,500.65,3.36,9.05">1</ref>.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Overview of confabulation model</head><p>Here, we explain the co-occurrence of multiple attributes by offering a concrete example. For instance, humans can identify an apple by the co-occurrence of senses: the color is red, the shape is round, and the surface is smooth. Therefore, if an unknown object has these attributes, humans can determine that it is an apple.</p><p>Repeating the same experience makes it easier for them to identify the object. Hecht-Nielsen called this confidence of prediction cogency, and it is defined by (1).</p><p>(</p><p>"P" is a backward probability based on a physiological experiment of the brain. A target word can be predicted by multiplying each cogency, and when the cogency of each attribute α, β, γ, and δ is high, the cogency of ε becomes high by calculating (2).</p><p>(</p><formula xml:id="formula_1" coords="3,463.05,294.33,7.72,8.96">)<label>2</label></formula><p>We apply this to visual words in this system, and the system recognizes an object by these co-occurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Recognition technique using confabulation model</head><p>The Bag of Visual Words technique makes it possible to recognize the object by using training in the similarity of visual words. We focus on this feature and assume that images in the same category have common features, and that the category can be identified by the co-occurrence of these features. For example, in the category "city," it is highly possible that the associated images contain both "road" and "building." In other words, if a query has the features of these objects, a "city" may show up in this image. We constructed a training model using visual words such as those in Fig. <ref type="figure" coords="3,448.72,469.81,4.98,9.05" target="#fig_0">2</ref> and use it to recognize categories. </p><formula xml:id="formula_2" coords="3,157.95,202.78,259.97,103.72">) | ( ) , ( Y X P Y X cogency  ) | ( ) | ( ) | ( ) | ( ) , (           P P P P conf  3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent Confabulation Model</head><p>We propose a recurrent confabulation model as an extension of the Confabulation model described in the previous section. This model is based on the mechanism of human learning and recognition. When a human tries to understand an unknown situation, he thinks recurrently, starting from the most extreme condition that he can think of. If there is no correspondence between what the person sees and information he already knows, it seems that he gradually relaxes the conditions to attempt to understand the situation. For example, take the case of a person seeing a green apple for the first time. Few thoughts are necessary to recognize this object. The person identifies the object as a green apple because it corresponds closely to the known characteristics of previously experienced apples, except that the color is green. In other words, a human chooses "the thing which seems most likely to be the answer" that corresponds to known information (e.g. apples are red, round, and sweet) that excludes only color information as an answer. That is to say, a human repeats the process of relaxing the conditions if a solution is not found in the most extreme condition and looks for a solution again. Humans naturally judge that something that seems most likely to be the answer is the answer.</p><p>The recurrent confabulation model was made by applying this idea to linguistic processing by computer. This model uses both the recognition results under strict and more relaxed conditions. However, because there may only be a few retrieval results under strict conditions, the results obtained in more relaxed conditions, in which the number of combinations of attributes is reduced, compensate for that.</p><p>In this case, for example, even though there are only 10 recognition results using color, shape, and taste, the system can make up for this lack by using the results from the relaxed condition, e.g., using just shape and taste, if there are more than 10 recognition results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">System Description</head><p>We describe our systems in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Annotation Task</head><p>We applied the confabulation model to the system submitted to the annotation task. We collected training images of every concept in the dataset and tried to represent the concepts by extracting common features from their images.</p><p>We used the features of Bag of Visual Words and color. This is because in recent years image recognition using Bag of Visual Words has reportedly had high accuracy, and we thought that color was an important feature depending on the concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Flickr User Tag Approach</head><p>We describe the approach using Flickr user tags before explaining the visual features approach.</p><p>The overall process is shown in Fig. <ref type="figure" coords="5,281.74,194.96,3.76,9.05">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3. Schematic of annotation system using Flickr user tags</head><p>We perform a morphological analysis of all the user tags of the training images. Next, a TFIDF score is given to each tag. Then, the system stores the correspondence information for the tags and concepts in a casebase. When using a test image, we perform morphological analysis similarly, and the system matches the tags attached to the test image to the tags of the casebase.</p><p>However, this matching is probably vague because the system checks a match by using one word.</p><p>In the "apple" example, when we presume that the object is an apple, using combined information such as "red and round" makes it easier to judge whether the object is an apple than by using a single piece of information such as "red" or "round".</p><p>Therefore, we establish combinations between words to reduce vagueness. A matching score is calculated as shown in (3).</p><p>(3) Here, C is a combination, w is a tag in the combination, and hit is a tag in the corresponding concept. Finally, the system sorts these scores for the concepts and outputs them as results.</p><formula xml:id="formula_3" coords="5,185.93,593.50,187.05,34.08">                    query i c c w hit TFIDF TFIDF Score j j i w</formula><p>In addition, queries are generated by changing the number of combinations (b of aCb) from b=1 to 3. We output the results using these queries and apply the recurrent confabulation model to them (Fig. <ref type="figure" coords="6,263.98,172.52,3.34,9.05" target="#fig_1">4</ref>.). First, we output results for different numbers of combinations. Then, concepts that are annotated for both a small and large number of combinations are excluded from the set of results with a small number of combinations. As a result, there are no concepts included that appear in annotation results for many combinations. ii.</p><p>Next, we take the top M% of the high-ranked annotation results of N combinations. We use Eq. ( <ref type="formula" coords="6,267.80,521.89,3.87,9.05">4</ref>) to convert the score so that the best results for the number of combinations N-1 are ranked highest and the others are integrated with the top results. In other words, we integrate the score of the i rank for the number of combinations = N-1 with the number of combinations = N. We decided to set M = 30% from the experiment; however, it seemed that the optimum value of M will change depending on the corpus. <ref type="bibr" coords="6,459.07,613.81,11.75,9.05" target="#b3">(4)</ref> iii.</p><p>We use the annotation results integrated up to the number of combinations = N and the result of the number of combinations = N+1 and follow the procedure in ii. This is recursively repeated.</p><formula xml:id="formula_4" coords="6,211.73,600.89,176.71,31.82">        top i M i score score score score N N N N 1 1    </formula><p>Therefore, the high-ranked results do not include much noise. The low-ranked results are vague; however, we can expect them to include many candidate concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Bag of Visual Words Approach</head><p>i.</p><p>Construction of visual words Constructing visual words is an important factor when we represent all images using a histogram. Although many techniques for this have been proposed over the years, the most effective technique to reveal how visual words are constructed has yet to be revealed. In our study, we had to process many images in a short period (about one month), so we only used 1,000 randomly selected images from 8,000 training images. We constructed 500 visual words by clustering SIFT features extracted from these images. We clustered the features using the common k-means clustering algorithm.</p><p>ii.</p><p>The representation of image using visual words Our goal was to recognize test images by using the Confabulation model. We show the detailed process we used to achieve this (see Fig. <ref type="figure" coords="7,338.27,354.95,4.17,9.05" target="#fig_3">5</ref>) in this section.</p><p>First, the training images are represented as a histogram just as in step i. Then, the visual words that are used are those obtained in step i.</p><p>Second, each visual word is weighted using TFIDF. TFIDF is a popular technique in the field of text retrieval and is used for weighting the words in a text. We used 8,000 training images. In short, the maximum document frequency (DF) is 8,000. We assume that 50 visual words of an image that have high weighted scores have a strong relation to that image, and we keep these words for the third process.</p><p>Third, we calculate the occurrence probability of each visual word in each concept. As shown in Fig. <ref type="figure" coords="7,197.98,458.29,3.76,9.05" target="#fig_3">5</ref>, for instance, a visual word representing "ear" occurs three times in three images of the concept "horse." Thus, its probability is 1 (3/3).</p><p>(5)</p><p>The occurrence probability of each visual word is calculated by <ref type="bibr" coords="7,399.24,538.81,10.74,9.05" target="#b4">(5)</ref>. Here, "num" represents the number of images of each concept t, and "df" shows the number of images having i-th visual words. However, this probability cannot deal with rare cases. For instance, when comparing one image with 100 images, the importance of a visual word differs between 1 of 1 time probability and 100 of 100 times. These probabilities concurrently become 1. However, the former is obviously a rare case, and its degree of confidence is probably low. On the other hand, it is highly possible that the visual word in the latter case is important because it comes out 100 times in 100 images. To deal with this problem, we decided to multiply the logarithm of the "num" by the occurrence probability as a weight and calculate the score using <ref type="bibr" coords="7,383.95,642.16,10.68,9.05" target="#b5">(6)</ref>. The rare case can be removed by using this equation, and the occurrence probability of visual words appearing in many images can be increased.  iii.</p><p>Recognition Here, we show how to calculate the similarity between the test image and each concept. Fig. <ref type="figure" coords="8,179.14,528.85,4.98,9.05" target="#fig_4">6</ref> represents the process flow.</p><p>First, a test image is transformed into a visual word histogram, and each word is weighted by TFIDF the same way as in the training step. Visual words contained by many training images are obviously noise and are not suitable for use with recognition. Therefore, by using TFIDF, the weights of these visual words can be decreased.</p><p>Second, 18 visual words having a high score are used for matching. This number was obtained experimentally. Then, the matching takes place by using the recurrent idea described in chapter. 3. In other words, the number of visual words used varies from 18, 15, and 7. The parameter of M in 4.1.1 is set as 20.The matching equation is shown as <ref type="bibr" coords="8,164.15,632.32,10.66,9.05" target="#b6">(7)</ref>. ( <ref type="formula" coords="8,462.99,661.36,3.92,9.05">7</ref>)</p><formula xml:id="formula_5" coords="8,204.23,159.73,177.58,524.99">) (num num df ) P(t|vw t t i i log      18 1 ) ( ) ( i i P concept sim</formula><p>Here, "i" represents the index of 18 visual words, and P(i) shows the occurrence probability of the i-th visual word in the concept. As a result, the top 20 concepts having high similarity are annotated to a test image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Color Approach</head><p>i. Representation of images using color feature The idea for this is basically the same as visual words. The color features representing a concept are trained as one pattern. We use RGB as the color feature in this task. However, each color (for example, R) has 256 tones; thus, the histogram has about 16 million dimensions (256×256×256). To avoid an expensive calculation cost, we reduce each color from 256 tones to 4. Thus, the histogram can be represented in 64 dimensions.</p><p>With the color feature, we cannot weight each color (here, each dimension) by TFIDF. Hence, we use the top 10 colors out of the 64 that have high occurrence frequency. The training patterns are created using the same procedure as for visual words.</p><p>ii.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recognition for use with color feature</head><p>The recognition process is the same as for visual words. However, the weighting cannot be carried out for the color feature (as described above). Thus, 15 color features having high frequency are used, and color features are not applied in the recurrent approach. This number is also determined experimentally. The matching equation is shown as <ref type="bibr" coords="10,210.19,206.96,10.67,9.05" target="#b7">(8)</ref>. <ref type="bibr" coords="10,459.07,247.40,11.75,9.05" target="#b7">(8)</ref> In this case, P(i) is not probability but occurrence frequency. This score is usually a few thousand. Thus, we transform it into a logarithm because an overflow occurs if P is multiplied many times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Integration</head><p>We also construct the integration system using many feature values (In short, tag, visual words, color feature). The training and recognition are conducted by using cooccurrences of each annotation result, which is based on Confabulation. This can be simply implemented because each similarity of the annotation result is just multiplied respectively. When this method is applied, the multiplied result may be zero if there is a feature that has zero similarity. Thus, we need to make improvements in order to resolve this problem in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Concept Based Retrieval Task</head><p>The overall flow of the concept based retrieval task is shown in Fig. <ref type="figure" coords="10,399.17,491.89,3.76,9.05" target="#fig_5">7</ref>. First, all 200 thousand images are annotated to concepts by using the annotation task system.</p><p>Second, the case base is constructed by arranging the similarity of these images in descending order (see Fig. <ref type="figure" coords="10,245.93,537.85,16.11,9.05" target="#fig_5">7(B)</ref>). Only 5,000 of the 200 thousand images are memorized because only 1,000 retrieval results are needed.</p><p>The topic images are also annotated to concepts respectively. The matching is conducted between the concept of the test image and the one in the casebase. We use 20 concepts from the annotation result of the test image (see Fig. <ref type="figure" coords="10,386.71,583.81,16.43,9.05" target="#fig_5">7(A)</ref>).</p><p>Here, we describe a retrieval example. It is assumed that the result of the test image has two concepts ("cat" and "Day"), and Img0111 is retrieved. The similarity of the concept "cat" of Img0111 and the test image is 0.8 and 0.5 respectively, and that of "Day" is 0.4 and 0.7, respectively. Then, the similarity of the retrieved Img0111 is 0.68 (0.5×0.8 + 0.7×0.4). In fact, the topic has several images, and these also retrieve the relevant images in the case base. As a topic result, the final similarity for Img0111 is the maximum in the result of each topic image. In short, if the value of topic image 1 is 0.87, and it is 0.98 for topic image 2, the final similarity of Img0111</p><formula xml:id="formula_6" coords="10,272.04,230.73,33.29,41.42">   15 1</formula><p>)) ( ( ) ( i e i P Log concept sim becomes 0.98. This process is used because the annotation result may be unstable. Even if several images are contained in the same topic, the appearances of these images may be different. Thus, a difference regarding annotated concepts or similarity occurs. Then, concepts such as "dog" that have only a few training images may not have a high ranking in the annotation result of all images in one topic, but concepts such as "Day" may be at the top. Thus, in all topics, the images relevant to "Day" are retrieved. This is a negative effect from the logarithm used to calculate the occurrence probability. By using this approach, we can avoid this problem. Finally, 1,000 images having high similarity are retrieved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submissions and Results</head><p>This section explains our submitted systems and the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Annotation Task</head><p>We submitted the following five systems to the annotation task.</p><p>i. meiji_tag_r Method using only Flickr user tags ii.</p><p>meiji_vw_r Method using only visual words iii.</p><p>meiji_vw_color_r Method using co-occurrence of visual words and color iv.</p><p>meiji_vw_color_tag_r Method using co-occurrence of visual words, color, and tags v.</p><p>meiji_vw_tag_r -vw_r Method using co-occurrence of visual words and tags However, there were about 1,000 images that did not have any tags. Therefore, if their co-occurrence is used as described in 4.1.4, we cannot annotate concepts to these images. To make up for this, this system reflects the result using only visual words in the images that have few annotations.</p><p>The results of the annotation task are listed in Table <ref type="table" coords="12,334.93,275.99,3.76,9.05" target="#tab_0">1</ref>. The best execution result in each evaluation measure was "meiji_tag_r" in MAP and SR-Precision and "meiji_vw_tag_r-vw_r" in F-ex. The result using only tags was best, and visual information reduced the precision. Because a tag is a word, its meaning is clear. Thus, the high-ranked concepts using tags had good reliability. (For example, let us consider an image annotated to the concept "sea." In visual words, a visual word representing "sea" is vague; however, in tags, "ocean" is close enough to "sea" to be correct.) Nevertheless, although the precision was low for visual information when the system integrated each feature by using co-occurrence, we integrated them by equivalent weight. It is possible that the vagueness of the visual words reduced the correct highranking concepts obtained using tags. However, when we used color information and only visual words, precision was not good either. It is therefore necessary to continue studying techniques that apply the Confabulation model to image processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Concept Based Retrieval Task</head><p>We submitted the following 10 systems to the concept based retrieval task.</p><p>i. meijiTr Method using only Flickr user tags. ii. meijiVr Method using only visual words iii.</p><p>meijiVTr Method using co-occurrence of visual words and tags iv.</p><p>meijiVCTr Method using co-occurrence of visual words, color, and tags v.</p><p>meijiVTVr Method using co-occurrence of visual words and tags The additional process of this system is the same as "meiji_vw_tag_r -vw_r" which was submitted to the annotation task. vi -x. The files in which the end of i -v was changed from "r" to "n"</p><p>The execution results in which the recurrent confabulation model was not implemented. These were submitted to confirm the effectiveness of this model.</p><p>The results of the concept based retrieval task are listed in Table <ref type="table" coords="13,384.88,298.91,3.76,9.05" target="#tab_1">2</ref>. In our experiment, we knew that precision improves when text is processed by using the recurrent confabulation model. Therefore, one of our purposes was to investigate how effective the model was when applied to visual information. First, we compared a result using the recurrent confabulation model to a result in which the model was not used and found that the precision of the results using the model decreased except for "meijiTr". Because the meanings of visual words are vague, there is a lot of noise when the number of combinations is small. Thus, the noise increased when different numbers of combinations were integrated. It will be necessary to derive the appropriate number of combinations and the proper generation method in the future.</p><p>Second, precision improved using the co-occurrence of plural attributes. Because we constructed a casebase from a large collection of 200,000 images, the system probably learned sufficiently and precision improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We developed systems for image recognition and retrieval based on the Confabulation model. It is difficult to demonstrate at this time exactly how effective our system was due to the low precision of the system using the Confabulation model. However, we confirmed that integrating plural attributes was effective, although it is necessary to improve the method of co-occurrence of features. We were also able to evaluate the recurrent confabulation model.</p><p>The points that need to be improved were clarified from these results and we can make use of them in a future study. We think if we can imitate the function of the brain when a human recognizes an image, it may be possible for the system to recognize it faster and with better precision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,124.82,608.59,183.61,8.18;3,124.80,502.10,252.25,104.40"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Learning model using Confabulation model</figDesc><graphic coords="3,124.80,502.10,252.25,104.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,124.82,418.43,194.48,9.05;6,124.80,204.85,360.60,211.45"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Overview of recurrent confabulation model</figDesc><graphic coords="6,124.80,204.85,360.60,211.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,321.57,525.16,2.82,8.12;7,314.28,503.31,2.82,8.12;7,262.43,513.10,2.82,8.12;7,292.48,516.89,29.71,13.95;7,300.15,495.00,13.41,13.95;7,268.83,504.80,5.81,13.95;7,218.59,504.65,68.65,14.11;8,459.07,172.52,11.69,9.05;8,124.82,206.96,345.85,9.05;8,124.82,218.48,152.31,9.05"><head></head><label></label><figDesc>above process, a training pattern such as that in Fig. 5 is constructed. Finally, 99 training patterns are made.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,124.82,472.39,274.48,8.18;8,124.80,250.80,353.65,219.60"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Process of producing the characteristic visual words of each concept.</figDesc><graphic coords="8,124.80,250.80,353.65,219.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,124.82,455.59,345.71,8.18;9,124.82,465.91,345.78,8.18;9,124.82,476.23,345.85,8.18;9,124.82,486.55,150.57,8.18;9,124.80,204.85,360.70,248.75"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Recognition process. We construct 99 training patterns for each concept. Each pattern is kept as one case in the casebase. Matching takes place by using 18, 15, and 7 visual words having a high TFIDF weight for the test image. As a result, the top 20 concepts having high similarity are annotated to this test image.</figDesc><graphic coords="9,124.80,204.85,360.70,248.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="11,124.82,504.67,195.47,8.18;11,124.80,273.70,353.65,229.00"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Detailed system of concept based retrieval task</figDesc><graphic coords="11,124.80,273.70,353.65,229.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,134.75,215.90,345.60,200.00"><head></head><label></label><figDesc></figDesc><graphic coords="5,134.75,215.90,345.60,200.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="12,124.82,298.91,338.08,111.30"><head>Table 1 .</head><label>1</label><figDesc>Results of Annotation Task</figDesc><table coords="12,144.74,314.01,318.16,96.20"><row><cell>Run</cell><cell>MAP</cell><cell>F-ex</cell><cell>SR-Precision</cell></row><row><cell>meiji_vw_r</cell><cell>0.188589</cell><cell>0.471923</cell><cell>0.43197626</cell></row><row><cell>meiji_tag_r</cell><cell>0.303823</cell><cell>0.458523</cell><cell>0.49061212</cell></row><row><cell>meiji_vw_color_r</cell><cell>0.20408</cell><cell>0.451822</cell><cell>0.4520852</cell></row><row><cell>meiji_vw_color_tag_r</cell><cell>0.287871</cell><cell>0.423201</cell><cell>0.4799387</cell></row><row><cell>meiji_vw_tag_r -vw_r</cell><cell>0.287695</cell><cell>0.495162</cell><cell>0.46919692</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="13,124.82,321.95,332.40,198.80"><head>Table 2 .</head><label>2</label><figDesc>Results of concept based retrieval task</figDesc><table coords="13,137.18,336.93,320.04,183.82"><row><cell>Run</cell><cell>MAP</cell><cell>P@10</cell><cell>P@20</cell><cell>P@100</cell><cell>R-Prec</cell></row><row><cell>meijiVn</cell><cell>0.0017</cell><cell>0.015</cell><cell>0.015</cell><cell>0.0197</cell><cell>0.0151</cell></row><row><cell>meijiVr</cell><cell>0.0013</cell><cell>0.0125</cell><cell>0.0125</cell><cell>0.0185</cell><cell>0.0122</cell></row><row><cell>meijiTn</cell><cell>0.0213</cell><cell>0.0675</cell><cell>0.0862</cell><cell>0.0865</cell><cell>0.0648</cell></row><row><cell>meijiTr</cell><cell>0.0227</cell><cell>0.09</cell><cell>0.0962</cell><cell>0.0865</cell><cell>0.0628</cell></row><row><cell>meijiVTn</cell><cell>0.0408</cell><cell>0.175</cell><cell>0.1513</cell><cell>0.1432</cell><cell>0.1053</cell></row><row><cell>meijiVTr</cell><cell>0.0325</cell><cell>0.1425</cell><cell>0.125</cell><cell>0.114</cell><cell>0.0867</cell></row><row><cell>meijiVCTn</cell><cell>0.0444</cell><cell>0.1625</cell><cell>0.165</cell><cell>0.1465</cell><cell>0.1053</cell></row><row><cell>meijiVCTr</cell><cell>0.0333</cell><cell>0.13</cell><cell>0.12</cell><cell>0.113</cell><cell>0.0824</cell></row><row><cell>meijiVTVn</cell><cell>0.042</cell><cell>0.1725</cell><cell>0.1437</cell><cell>0.1417</cell><cell>0.1061</cell></row><row><cell>meijiVTVr</cell><cell>0.0327</cell><cell>0.1275</cell><cell>0.1138</cell><cell>0.1135</cell><cell>0.0847</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="14,128.23,345.89,281.03,8.18" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Imageclefphotoannotation</surname></persName>
		</author>
		<ptr target="http://www.imageclef.org/2011/Photo" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,128.23,356.21,242.39,8.18" xml:id="b1">
	<monogr>
		<ptr target="http://press.liacs.nl/mirflickr/" />
		<title level="m" coord="14,136.22,356.21,123.90,8.18">MIR Flickr 1million image dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,128.23,366.53,342.18,8.18;14,136.22,376.85,315.86,8.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,287.98,366.53,167.34,8.18">Visual categorization with bags of keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,136.22,376.85,250.35,8.18">Proc. of ECCV Workshop on Statistical Learning in Computer Vision</title>
		<meeting>of ECCV Workshop on Statistical Learning in Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="59" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,128.23,387.17,342.48,8.18;14,136.22,397.61,225.14,8.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,182.88,387.17,231.86,8.18">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,424.91,387.17,45.79,8.18;14,136.22,397.61,99.22,8.18">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,128.23,407.93,342.50,8.18;14,136.22,418.25,108.84,8.18" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="14,208.01,407.93,106.90,8.18">A Theory of Cerebral Cortex</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hecht-Nielsen</surname></persName>
		</author>
		<idno>#0401</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>UCSD Institute for Neural Computation</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="14,128.23,428.57,342.53,8.18;14,136.22,438.91,108.84,8.18" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="14,208.01,428.57,106.90,8.18">A Theory of Cerebral Cortex</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hecht-Nielsen</surname></persName>
		</author>
		<idno>#0404</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>UCSD Institute for Neural Computation</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="14,128.23,449.23,327.46,8.18" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,205.85,449.23,76.26,8.18">Cogent confabulation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hecht-Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,288.08,449.23,59.69,8.18">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="111" to="115" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,128.23,459.67,342.35,8.18;14,136.22,469.99,111.72,8.18" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="14,220.37,459.67,79.37,8.18">Confabulation theory</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hecht-Nielsen</surname></persName>
		</author>
		<idno>#0501</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>UCSD Institute for Neural Computation</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
