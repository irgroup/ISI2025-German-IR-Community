<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.40,115.76,336.55,12.93;1,246.43,133.69,122.50,12.93">LIG-MRIM at Image Photo Annotation task in ImageCLEF 2011</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,147.91,171.53,59.06,9.19"><forename type="first">Rami</forename><surname>Albatal</surname></persName>
							<email>rami.albatal@imag.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">LIG-UPMF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,217.55,171.53,59.62,9.19"><forename type="first">Bahjat</forename><surname>Safadi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">LIG-UJF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.75,171.53,69.87,9.19"><forename type="first">Georges</forename><surname>Quénot</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">LIG</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>385 av. de la bibliothèque</addrLine>
									<postCode>38041</postCode>
									<settlement>Grenoble Cedex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.57,171.53,75.40,9.19"><forename type="first">Philippe</forename><surname>Mulhem</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">LIG</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>385 av. de la bibliothèque</addrLine>
									<postCode>38041</postCode>
									<settlement>Grenoble Cedex</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.40,115.76,336.55,12.93;1,246.43,133.69,122.50,12.93">LIG-MRIM at Image Photo Annotation task in ImageCLEF 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D04DC2F646AD6DC6ECB1748080968FCD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe in this paper the different approaches tested for the Photo Annotation task for CLEF 2011. We experimented state of the art techniques, by proposing late fusions of several classifiers trained on several features extracted from the images. The classifiers are SVMs and the late fusion is a simple addition of classification probabilities coming from the SVMs. The results obtained place our runs in the middle of the pack, with our best visual-based MAP at 0.337 We also integrated of Flickr human annotations, leading to a large increase of the MAP with a value of 0.377.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper aims at describing the proposal and results of the LIG-MRIM research group at the Photo Annotation task for CLEF 2011. The proposal of the group focused mainly on applying a late fusion on multiple learners based on SVM. We also experimented some processes to reduce the feature space dimensions, and we made use of a simple integration with Flickr tags. The findings according to the official evaluations confirm that: late fusion of multiple features lead to good result, that dimension reduction on few features is an interesting direction to focus on, and that a simple integration of human assigned tags improves results. The corpus <ref type="bibr" coords="1,210.65,524.38,9.72,8.97" target="#b3">[4]</ref> for this year is composed of a training set of 8,000 images and the test set is 10,000 images large. The image annotation is a multilabel classification process, where the 99 labels go from image elements (like Flowers), to feelings generated by the images (like scary). The images are possibly associated with EXIF data, as well as with Flickr tags provided by human. The main evaluation is MAP-based, and we focus here only on this measure to evaluate our runs. The outline of this paper is the following. In section 2, we begin describe the visual feature extracted and their representation. Section 3 presents the processing applied on Flickr tags. In section 4, we focus on the classification applied on the extraction, as on the fusion processed between the different learners results. In section 5, we list our results, and we conclude in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Extraction and representation of visual features</head><p>We focus here the feature extracted, as well some PCA-based dimension reduction on some features. The features considered cover most the common feature we find the in literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Simple features</head><p>The features that were extracted are color-based as well as texture based. Some features are extracted globally from the whole image, and others are extracted from image regions, before being aggregated to represent one image. In the following, we give an identifier for each feature before explaining the extracted feature. Such identifiers will be reused in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global features</head><p>-h3d 64: normalized RGB Histogram. Such color-based histogram is 64 dimensions large, using a simple 4 x 4 x 4 subsampling respectively the R, G and B colors components; -gab 40: normalized Gabor transform <ref type="bibr" coords="2,330.31,346.85,9.21,8.97" target="#b1">[2]</ref>. For this texture-based feature, we select 8 orientations at 5 scales, leading to a 40 dimensions space for these histograms; -hg 104: this feature results in a simple concatenation of the two representations above (h3d64 and gab40), generating a 104 dimensions space for the histograms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local features</head><p>The local features extracted are SIFT-like. They are extracted for regions of the images, resulting from dense sampling of harris-laplace region of interest detection. Each of these features are represented as bag of visual word, similarly to <ref type="bibr" coords="2,347.46,461.18,9.21,8.97" target="#b0">[1]</ref>; the visual vocabulary is generated using a Kmeans algorithm on a sample of the features extracted from the training set; -opp sift har 1000 and opp sift har 4000: opponent sift features with Harris-Laplace region of interest detector, generated using Koen Van de Sande's software <ref type="bibr" coords="2,262.85,515.33,9.21,8.97" target="#b4">[5]</ref>. Two representations are considered: one of 1000 and one of 4000 dimensions. -opp sift dense 1000: features similar to above, except that the regions or obtained by dense sampling every 8 pixels of the images. The bag of word representation generates 1000 dimensional histograms; -rgSift har 4000: rgSIFT features are extracted based on regions obtained by the Harris-Laplace detector. The same tool than above is used to generate the 1000 dimensions histograms; -rgSift dense 4000: the rgSIFT are extracted with dense sampling selection. The size of the histograms is 4000 dimensions; All the features described above are based on 1 nearest neighbor assignment for the generation of the bag of visual words histograms. As described in <ref type="bibr" coords="2,215.60,645.24,9.21,8.97" target="#b5">[6]</ref>, softer assignments may be used. We experiment those on opponent sift features:</p><p>-opp sift har unc 1000 and opp sift har unc 4000: opponent sift features extracted from region generated by Harris Laplace detectors, with soft assignment. The space dimensions are respectively here 1000 and 4000; -opp sift dense unc 1000: opponent sift features extracted from dense sampling, with soft assignment. The space dimensions are 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dimension reduction on features representations</head><p>As shown by <ref type="bibr" coords="3,222.00,216.22,9.21,8.97" target="#b2">[3]</ref>, some space dimension reduction do not necessarily degrades the results, and has a large advantage during the learning phase.</p><p>That is why we applied PCA-based dimension reduction on some of the large spaces defined in the previous subsection. First, to modify the values in histograms bin we apply a power law normalization, similar to <ref type="bibr" coords="3,174.16,271.01,9.21,8.97" target="#b2">[3]</ref>, so that the normalized value vnorm for each bin of the histograms is: vnorm = v α , with v the initial value of the bin, and α a float number depending on the collection. On the normalized histograms, we reduce the dimensions to a fixed number by using Principal Component Analysis (PCA). The resulting features are generated using the same a power law normalization with α = 0.500 or α = 0.450 (according to the pw in the identifier) and PCA reduction to 400 dimensions, leading to: -rgsift har 4000 pw0.500p400: from rgsift har 4000; -rgsift dense 4000 pw0.500p400: from rgsift har 4000; -opp sift har 1000 pw0.450 p400 and opp sift har 4000 pw0.450 p400: from respectively opp sift har 1000 and opp sift har 4000; -opp sift dense 1000 pw0.450 p400: from opp sift dense 1000; -opp sift har unc 1000 pw0.450 p400: from opp sift har unc 1000; -opp sift dense unc 1000 pw0.450 p400: from opp sift dense unc 1000; For the "low dimensional" features h3D64, gab40 and hg104, similar techniques lead to h3d 64 pw0.250 32, gab 40 pw0.500 20 and hg 104 pw0.375 54, when considering reducing the dimensions by a half.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Extraction and representation of Flickr tags</head><p>As Image annotation collection is an excerpt from Flickr, the human generated tags are available. We know that such manually input tags are not always easy to process (typos, jokes, etc.), but we propose a simple way to handle some of them. First, for each image, we split the tags into words, and we apply a Porter stemmer in a way to group similar words into classes. In a second step, if one stemmed tag equals one of the 99 stemmed labels, then the label is selected for the image. The resulting representation is a 99 dimensions binary vector, with 1 if the label describes the image and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual only</head><p>All the classification processes on the visual features use Multiple-SVM classifiers based on Radial Basis Funcion (RBF) kernels, since it was Fig. <ref type="figure" coords="4,217.93,337.28,4.12,8.08">1</ref>. Global classification process for visual features proved to be a good solution for data imbalance problems. Such problems occur for many labels in the collection under consideration here. So, for each label, we get positive and negative samples that are used as input for the learning of the Support Vectors. During the classification, the image representation is input to the SVM using each model, and a binary classification is processed. We assume here that each classifier outputs a probability of classification in [0,1]. The final score for each label is then the average of each individual score from each classifier of the label, as shown in figure <ref type="figure" coords="4,369.46,478.19,3.58,8.97">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visual + Flickr tags</head><p>For integrating Flickr tags and visual elements, we also use a late fusion approach. In this case the visual classification result for each label is fused using a max with the label value for the image according to the Flickr tags processing described earlier. The overall process is described in figure <ref type="figure" coords="4,199.49,569.68,3.58,8.97" target="#fig_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Validation set Results</head><p>We present the MAP results obtained on a validation set. Our training set is composed of 2/3rd of the official training set, generated randomly with a post processing ensuring a similar distribution of the tags that on the official training set. This last point is important, especially for the The table 1 presents the results obtained feature by feature for each visual feature listed in section 2.1. This table shows that all the SIFTbased features with hard assignment behave consistently, with MAP values between 0.246 and 0.258. The soft assignment opp sift har unc 1000 outperforms slightly the hard assignments, but only marginally. We notice also that the hg 104 features behave surprisingly well compared to SIFT-like features. The table 2 focuses on the results obtained when considering the dimension reduction process depicted in part 2.2. In the last column of this table, we list the percentage of increase compared to the original (i.e., not reduced) features. This table shows that the reduction of dimension proposed always outperforms the original features. This result is especially visible with the opponent sift features with strict assignment. In any cases, the dimension reduction seems effective for harris laplace features, and less for dense sampling-based features. For the "low dimansional" features, we notice also a large imrpovement with one one half reduction, leading to very good results for hg 104 pw0.375 54, which has onlyt 54 dimensions. The last table, 3 of this section deals with the results obtained after fusing the results, according to the explanations of section 4. We chose three fusions, which correspond to the configuration of the official run submitted: The conclusions drawn from this table is that the fusion always outperforms each f its components (such result is well known in the community).</p><p>We see here that Flickr tags integration, even is the processing is quite straightforward, leads to an important increase of the results.</p><p>These three configurations are the ones used for the official submissions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Official Results</head><p>We present here the official MAP results obtained from our runs in 4.</p><p>This table shows also in the last column the rank obtained in comparable lists (i.e., list of visual results for msvm and msvw two desc, and list of multi-modal results for msvm tags). The results obtained place our best visual run, msvm with a MAP of 0.336, in the first tier of the list, and above the average and the median values respectively of 0.289 and 0.323. For the multimodal run, msvm tags with a MAP of 0.378, the rank is above the middle, and also above the average and the median values of respectively 0.370 and 0.371 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper presented the worjk of the LIG-MRIM team for the Photo Annotation task for CLEF 2011. We used a large set of 20 features, with or without strict bin assignment, dimension reductions, with and without integrating Flickr tags. The results obtained place our run in the first tier for the visual runs, and in the first half for the multimedia runs.</p><p>In the future, we will focus on dimension reductions to find out what reductions are useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aknowledgements</head><p>This work was partly supported by: a) the Quaero Programme, funded by OSEO, French State agency for innovation and b) the Région Rhones Alpes (projet LIMA).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,179.91,330.03,255.53,8.97;5,180.15,116.17,254.80,199.60"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Global classification process for visual + Flickr features</figDesc><graphic coords="5,180.15,116.17,254.80,199.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,180.95,116.16,253.20,206.40"><head></head><label></label><figDesc></figDesc><graphic coords="4,180.95,116.16,253.20,206.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,219.19,115.33,176.96,144.76"><head>Table 1 .</head><label>1</label><figDesc>MAP results on the validation set</figDesc><table coords="6,247.70,137.36,119.96,122.74"><row><cell>Descriptor identifier</cell><cell>MAP</cell></row><row><cell>h3d 64</cell><cell>0.186</cell></row><row><cell>gab 40</cell><cell>0.213</cell></row><row><cell>hg 104</cell><cell>0.243</cell></row><row><cell>opp sift har 1000</cell><cell>0.252</cell></row><row><cell>opp sift har 4000</cell><cell>0.253</cell></row><row><cell>opp sift dense 1000</cell><cell>0.255</cell></row><row><cell>rgSift har 4000</cell><cell>0.246</cell></row><row><cell>rgSift dense 4000</cell><cell>0.258</cell></row><row><cell cols="2">opp sift har unc 1000 0.262</cell></row><row><cell cols="2">opp sift dense unc 1000 0.255</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,147.53,298.22,320.30,301.47"><head>Table 2 .</head><label>2</label><figDesc>MAP results on the validation set for reduced feature representations</figDesc><table coords="6,163.66,321.98,294.18,277.71"><row><cell>Descriptor identifier</cell><cell>MAP (increase vs. no reduction)</cell></row><row><cell>opp sift har 1000 pw0.450 p400</cell><cell>0.273 (+ 8.33%))</cell></row><row><cell>opp sift har 4000 pw0.450 p400</cell><cell>0.282 (+ 11.46%)</cell></row><row><cell>opp sift dense 1000 pw0.450 p400</cell><cell>0.267 (+ 4.71%)</cell></row><row><cell>rgsift har 4000 pw0.500p400</cell><cell>0.264 (+ 7.3%)</cell></row><row><cell>rgsift dense 4000 pw0.500p400</cell><cell>0.270 (+ 4.7%)</cell></row><row><cell>opp sift har unc 1000 pw0.450 p400</cell><cell>0.280 (+ 6.9%)</cell></row><row><cell>opp sift dense unc 1000 pw0.450 p400</cell><cell>0.267 (+ 1.9%)</cell></row><row><cell>h3d 64 pw0.250 32</cell><cell>0.211 (+ 13.44%))</cell></row><row><cell>gab 40 pw0.500 20</cell><cell>0.215 (+ 0.94%))</cell></row><row><cell>hg 104 pw0.375 54</cell><cell>0.259 (+ 6.58%))</cell></row><row><cell cols="2">-msvm: the late fusion of all the 20 visual features considered earlier</cell></row><row><cell>in the paper;</cell><cell></cell></row><row><cell cols="2">-msvm tags: the late fusion of the visual scores and the Flickr tags</cell></row><row><cell>scores;</cell><cell></cell></row><row><cell cols="2">-msvw two desc: the late fusion of the two best features according the</cell></row><row><cell cols="2">table 2, but considering two different kinds of regions for the features</cell></row><row><cell cols="2">(i.e. one Harris-Laplace based, and one dense sampling based) to en-</cell></row><row><cell cols="2">sure variability in the fused results: opp sift har unc 1000pw0.450p400</cell></row><row><cell cols="2">and opp sift dense 1000pw0.450p400.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,161.69,115.33,291.97,67.25"><head>Table 3 .</head><label>3</label><figDesc>MAP results on the validation set for three late fusions Descriptor identifier MAP (increase vs. best visual feature in the fusion)</figDesc><table coords="7,161.69,151.70,222.76,30.88"><row><cell>msvm</cell><cell>0.314 (+ 11.35%)</cell></row><row><cell>msvm tags</cell><cell>0.357 (+ 26.60%)</cell></row><row><cell>msvw two desc</cell><cell>0.297 (+ 6.07%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,172.07,365.94,271.20,67.25"><head>Table 4 .</head><label>4</label><figDesc>Official MAP results for the submitted MRIM runsDescriptor identifier Official Id MAP rank (in comparable list)</figDesc><table coords="7,172.07,402.30,232.38,30.89"><row><cell>msvm</cell><cell>1308318230664 0.336</cell><cell>15/46</cell></row><row><cell>msvm tags</cell><cell>1308226825708 0.378</cell><cell>11/25</cell></row><row><cell>msvw two desc</cell><cell>1308318529187 0.324</cell><cell>23/46</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,166.69,141.99,285.55,8.97;8,175.26,152.94,277.00,8.97;8,175.26,163.91,226.90,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,426.85,141.99,25.39,8.97;8,175.26,152.94,149.00,8.97">Visual categorization with bags of keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,356.43,153.25,95.83,8.26;8,175.26,164.21,147.87,8.26">Workshop on Statistical Learning in Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,166.69,174.87,285.55,8.97;8,175.26,185.82,276.97,8.97;8,175.26,196.78,102.70,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,314.20,174.87,138.04,8.97;8,175.26,185.82,92.86,8.97">Texture features for browsing and retrieval of image data</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,279.50,186.13,169.41,8.26">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="837" to="842" />
			<date type="published" when="1996-08">August 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,166.69,207.75,285.56,8.97;8,175.26,218.70,276.96,8.97;8,175.26,229.66,276.97,8.97;8,175.26,240.61,20.99,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,434.18,207.75,18.07,8.97;8,175.26,218.70,276.96,8.97">Lear and xrces participation to visual concept detection task -imageclef</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,212.56,229.97,182.15,8.26">Working Notes for the CLEF 2010 Workshop</title>
		<imprint>
			<date type="published" when="2010-09">2010. sep 2010</date>
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,166.69,251.58,285.54,8.97;8,175.26,262.54,276.98,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,328.35,251.58,123.88,8.97;8,175.26,262.54,130.32,8.97">The clef 2011 photo annotation and concept-based retrieval tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liebetrau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,323.03,262.84,101.99,8.26">CLEF 2011 working notes</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,166.69,273.49,285.53,8.97;8,175.26,284.45,277.00,8.97;8,175.26,295.41,276.98,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,408.85,273.49,43.37,8.97;8,175.26,284.45,193.34,8.97">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,375.76,284.76,76.49,8.26;8,175.26,295.72,180.06,8.26">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,166.69,306.37,285.54,8.97;8,175.26,317.33,276.98,8.97;8,175.26,328.29,204.56,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,204.78,317.33,89.26,8.97">Visual word ambiguity</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,302.63,317.63,149.60,8.26;8,175.26,328.60,117.89,8.26">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1271" to="1283" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
