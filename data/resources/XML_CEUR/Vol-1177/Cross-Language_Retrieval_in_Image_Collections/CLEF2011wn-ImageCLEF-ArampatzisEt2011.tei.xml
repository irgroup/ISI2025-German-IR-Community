<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,159.55,115.90,296.26,12.90">DUTH at ImageCLEF 2011 Wikipedia Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,167.87,153.95,60.71,8.64"><forename type="first">Avi</forename><surname>Arampatzis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Democritus University of Thrace</orgName>
								<address>
									<postCode>67100</postCode>
									<settlement>Xanthi</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,235.48,153.95,83.53,8.64"><forename type="first">Konstantinos</forename><surname>Zagoris</surname></persName>
							<email>kzagoris@ee.duth.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Democritus University of Thrace</orgName>
								<address>
									<postCode>67100</postCode>
									<settlement>Xanthi</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,342.50,153.95,104.99,8.64"><forename type="first">Savvas</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Democritus University of Thrace</orgName>
								<address>
									<postCode>67100</postCode>
									<settlement>Xanthi</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,159.55,115.90,296.26,12.90">DUTH at ImageCLEF 2011 Wikipedia Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F9DBFE07405AB47F7F153F798912E528</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As digital information is increasingly becoming multimodal, the days of single-language text-only retrieval are numbered. Take as an example Wikipedia where a single topic may be covered in several languages and include non-textual media such as image, audio, and video. Moreover, non-textual media may be annotated with text in several languages in a variety of metadata fields such as object caption, description, comment, and filename. Current search engines usually focus on limited numbers of modalities at a time, e.g. English text queries on English text or maybe on textual annotations of other media as well, not making use of all information available. Final rankings are usually results of fusion of individual modalities, a task which is tricky at best especially when noisy modalities are involved.</p><p>In this paper we present the experiments performed by Democritus University of Thrace (DUTH), Greece, in the context of our participation to the ImageCLEF 2011 Wikipedia Retrieval task. <ref type="foot" coords="1,236.53,404.29,3.49,6.05" target="#foot_0">1</ref> The ImageCLEF 2011 Wikipedia collection is the same as in 2010. It has image as its primary medium, consisting of 237, 434 items, associated with noisy and incomplete user-supplied textual annotations and the Wikipedia articles containing the images. Associated annotations are written in any combination of English, German, French, or any other unidentified language. This year there are 50 new test topics, each one consisting of a textual and a visual part: three title fields (one per language-English, German, French), and 4 or 5 example images. The exact details of the setting of the task, e.g., research objectives, collection etc., are provided at the task's webpage.</p><p>We kept building upon and improving the experimental multimodal search engine we introduced last year, www.mmretrieval.net (Fig. <ref type="figure" coords="1,367.04,525.51,3.74,8.64" target="#fig_0">1</ref>). The engine allows multiple image and multilingual queries in a single search and makes use of the total available information in a multimodal collection. All modalities are indexed separately and searched in parallel, and results can be fused with different methods. The engine demonstrates the feasibility of the proposed architecture and methods, and furthermore enables a visual inspection of the results beyond the standard TREC-style evaluation. Using the engine, we experimented with different score normalization and combination methods for fusing results. We eliminated the least effective methods based on our last year's participation to ImageCLEF <ref type="bibr" coords="1,249.60,621.15,11.62,8.64" target="#b0">[1]</ref> and improved upon whatever worked best.</p><p>The rest of the paper is organized as follows. In Section 2 we describe the MMretrieval engine, give the details on how the Wikipedia collection is indexed and a brief overview of the search methods that the engine provides. In Section 3 we describe in more detail the fusion methods we experimented with and justify their use. A comparative evaluation of the methods is provided in Section 4; we used the 2010 topics for tuning. Experiments with the 2011 topics are summarized in Section 5. Conclusions are drawn in Section 6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">www.MMRetrieval.net: A Multimodal Search Engine</head><p>During last year's ImageCLEF Wikipedia Retrieval, we introduced an experimental search engine for multilingual and multimedia information, employing a holistic web interface and enabling the use of highly distributed indices <ref type="bibr" coords="3,369.65,164.97,10.58,8.64" target="#b7">[8]</ref>. Modalities are searched in parallel, and results can be fused via several selectable methods. This year, we built upon the same engine eliminating the least effective methods and trying to improve whatever worked best last year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexing</head><p>To index images, we employ the family of descriptors known as Compact Composite Descriptors (CCDs). CCDs consist of more than one visual features in a compact vector, and each descriptor is intended for a specific type of image. We index with two descriptors from the family, which we consider them as capturing orthogonal information content, i.e., the Joint Composite Descriptor (JCD) <ref type="bibr" coords="3,330.24,294.14,11.62,8.64" target="#b2">[3]</ref> and the recently proposed Spatial Color Distribution (SpCD) <ref type="bibr" coords="3,243.47,306.10,10.58,8.64" target="#b3">[4]</ref>. JCD is developed for color natural images, while SpCD is considered suitable for colored graphics and artifficially generated images. Thus, we have 2 image indices.</p><p>The collection of images at hand, i.e. the ImageCLEF 2010/2011 Wikipedia collection, comes with XML metadata consisting of a description, a comment, and multiple captions, per language (English, German, and French). Each caption is linked to the wikipedia article where the image appears in. Additionally, a raw comment is supplied which may contain some of the per-language comments and any other comment in an unidentified language. Any of the above fields may be empty or noisy. Furthermore, a name field is supplied per image containing its filename. We do not use the supplied &lt;license&gt; field.</p><p>For text indexing and retrieval, we employ the Lemur Toolkit V4.11 and Indri V2.11 with the tf.idf retrieval model. <ref type="foot" coords="3,256.77,447.89,3.49,6.05" target="#foot_1">2</ref> In order to have clean global (DF) and local statistics (TF, document length), we split the metadata and articles per language and index them separately. Thus, we have 4 indices: one per language which includes metadata and articles together but allows limiting searches in either of them, plus one for the unidentified language metadata including the name field (which can be in any language). For English text, we enable Krovetz stemming; no stemming is done for other languages in the current version of the system. We also Krovetz-stem the unidentified language metadata, assuming that most of it is probably English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Searching</head><p>The web application is developed in the C#/.NET Framework 4.0 and requires a fairly modern browser as the underlying technologies which are employed for the interface are HTML, CSS and JavaScript (AJAX). Fig. <ref type="figure" coords="3,318.93,602.64,4.15,8.64" target="#fig_1">2</ref> illustrates an overview of the architecture. The user provides image and text queries through the web interface which are dispatched in parallel to the associated databases. Retrieval results are obtained from each of the databases, fused into a single listing, and presented to the user. Users can supply no, single, or multiple query images in a single search, resulting in 2 * i active image modalities, where i is the number of query images. Similarly, users can supply no text query or queries in any combination of the 3 languages, resulting in 3 * l active text modalities, where l is the number query languages. Each supplied query results to 3 modalities: it is run against the corresponding language metadata, articles, as well as, the unidentified language metadata. The current alpha version assumes that the user provides multilingual queries for a single search, while operationally query translation may be done automatically.</p><p>The results from each modality are fused by one of the supported methods. Fusion consists of two components: score normalization and combination. In CombSUM, the user may select a weigh factor W ∈ [0, 100] which determines the percentage contribution of the image modalities against the textual ones.</p><p>For efficiency reasons, only the top-2500 results are retrieved from each modality. If a modality returns less than 2500 items, all non-returned items are assigned zero scores for the modality. When a modality returns 2500 items, all non-occurring items in the top-2500 are assigned half the score of the 2500th item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fusion</head><p>Let i = 1, 2, . . . be the index running over example images, and j running over the visual descriptors (only two in our setup), i.e. j ∈ {1, 2}. Let DESC ji be the score of a collection image against the ith example image for the jth descriptor.</p><p>Let l ∈ {1, 2, 3} be the index running over provided natural languages (or example text queries, i.e. three in our setup), and m ∈ {1, 2, 3} running over the textual data streams per language (we consider three: metadata, articles, and undefined language metadata). Let TEXT ml be the score of a collection item against the text query in the lth language for the mth text stream.</p><p>Fusion consists of two successive steps: score normalization and score combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Score Combination</head><p>CombSUM</p><formula xml:id="formula_0" coords="5,206.19,242.21,274.40,26.88">s = (1 -w) 1 ji j,i DESC ji + w 1 ml m,l TEXT ml<label>(1)</label></formula><p>The parameter w controls the relative contribution of the two media; for w = 1 retrieval is based only on text while for w = 0 is based only on image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CombDUTH</head><p>Image Modalities Assuming that the descriptors capture orthogonal information, we add their scores per example image. Then, to take into account all example images, the natural combination is to assign to each collection image the maximum similarity seen from its comparisons to all example images; this can be interpreted as looking for images similar to any of the example images. Summarizing, the score s for a collection image against the topic is defined as:</p><formula xml:id="formula_1" coords="5,254.79,428.96,225.80,33.53">s = max i   j DESC ji  <label>(2)</label></formula><p>Text Modalities Assuming that the text streams capture orthogonal information, we add their scores per language. Then, to take into account all the languages, the natural combination is to assign to each collection item the maximum similarity seen from its comparisons to all text queries; this can be interpreted as looking for items in any of the languages. Summarizing, the score s for a collection image against the topic is defined as:</p><formula xml:id="formula_2" coords="5,253.30,559.56,227.29,19.61">s = max l m TEXT ml<label>(3)</label></formula><p>Combining Media Incorporating text, again as an orthogonal modality, we add its contribution. Summarizing, the score s for a collection image against the topic is defined as:</p><formula xml:id="formula_3" coords="5,170.87,627.06,309.72,33.53">s = (1 -w) max i   1 j j DESC ji   + w max l 1 m m TEXT ml (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Score Normalization</head><p>MinMax For the text modalities, we apply MinMax in different 'flavours':</p><p>-Per Modality. This is the standard MinMax taking the maximum score seen per ranked-list. -Per Modality Type. We take the maximum score seen across ranked-lists of the same modality type. For example, to MinMax a ranked-list coming from English metadata, we take the maximum score seen across the ranked-lists of English, French, and German metadata, produced by the queries in the corresponding languages.</p><p>-Per Index Language. We take the maximum score seen across all ranked-lists from modalities coming from the same index. For example, to MinMax a ranked-list coming from English metadata, we take the maximum score seen across the rankedlists of English metadata and English articles. -Per Query Language. We take the maximum score seen across all ranked-lists produced by the same query language. For example, to MinMax a ranked-list coming from English metadata, we take the maximum score seen across the ranked-lists produced by English metadata, English articles, and undefined language metadata, using the same English query.</p><p>The minimum score is always 0 for tf.idf. Given that image modalities produce scores in [0, 100] (using the Tanimoto coefficient for similarity matching), we do not apply any MinMax normalization to image scores.</p><p>Query Difficulty Inverse document frequency (IDF) is a widely used and robust term weighting function capturing term specificity <ref type="bibr" coords="6,319.26,456.94,10.58,8.64" target="#b6">[7]</ref>. Analogously, query specificity (QS) or query IDF can be seen as a measure of the discriminative power of a query over a collection of documents. A query's IDF is a log estimate of the inverse probability that a random document from a collection of N documents would contain all query terms, assuming that terms occur independently. QS is a good pre-retrieval predictor for query performance <ref type="bibr" coords="6,187.60,516.72,10.58,8.64" target="#b5">[6]</ref>. For a query with k terms 1, . . . k, QS is defined as</p><formula xml:id="formula_4" coords="6,232.52,539.38,248.08,30.32">QS k = log k i=1 N df i = k i=1 log N df i<label>(5)</label></formula><p>where df i is the document frequency (DF), i.e. the number of collection documents in which the term i occurs.</p><p>In the Query Difficulty (QD) normalization, we divide all scores per modality by QS, using the df statistics corresponding to the modality. This will promote the scores of 'easy' modalities and demote the scores of 'difficult' modalities for the query.</p><p>For image modalities, we do a similar normalization as defined in the above equation, except that the k terms are replaced by each descriptor's bins. the runs reported so far have a better MAP and bpref than last year's best automatic run submitted to ImageCLEF, and a slightly lower but comparable initial precision. <ref type="foot" coords="9,476.61,129.60,3.49,6.05" target="#foot_2">3</ref>Nevertheless, a visual inspection of our results reveals that with CompDUTH we are retrieving un-judged items which are sometimes relevant, a fact that most of the times does not seem to get picked up by bpref. Table <ref type="table" coords="9,174.06,397.34,9.96,8.64" target="#tab_0">11</ref> summarizes a selection of our official runs with the 2011 topics. Note that we submitted more runs employing pseudo-relevance feedback methods for the image modalities which we do not include or analyse here; their performance was comparable to the ones included in the table.</p><p>In score combination, the simplest method of linearly combining evidence (Comp-SUM) is once more found to be robust, irrespective of the normalization method. However, CompDUTH is very competitive with similar performance. In score normalization, query difficulty normalization (QD) gives the best effectiveness in both MAP and initial precision when scores are combined with CompSUM.</p><p>The current experiment points to that the choice of normalization method is more important than the combination method. MinMax achieves the best results for w = 0.6 or 0.7, i.e. retrieval based on 60-70% text, while with QD the contribution of text can be reduced to 40-60% improving overall effectiveness. It seems that with a better score normalization across modalities or media, we can use more of content-based image retrieval in a multimodal mix.</p><p>We reported our experiences and research conducted in the context of our participation to the controlled experiment of the ImageCLEF 2010 Wikipedia Retrieval task. As second-time participants, we improved upon and extended our experimental search engine, http://www.mmretrieval.net, which combines multilingual and multiimage search via a holistic web interface and employs highly distributed indices. Modalities are search in parallel, and results can be fused via several methods.</p><p>All in all, we are modestly satisfied with our results. Although our best MAP run ranked our system as the second-best among the other participants' systems (excluding all relevance feedback and query expansions runs), we believe that the content-based image retrieval part of the problem has a large room for improvement. A promising direction may be using new image modalities such as those based on the bag-of-visualwords paradigm and other similar approaches. Furthermore, we consider score normalization and combination important problems; while effective methods exist in traditional text retrieval, those problems are not trivial in multimedia setups.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,218.99,629.23,177.38,8.12;2,177.99,226.12,259.35,388.37"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The www.MMRetrieval.net search engine.</figDesc><graphic coords="2,177.99,226.12,259.35,388.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,254.45,348.14,104.21,8.12;4,203.93,115.84,207.50,217.57"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. System's architecture.</figDesc><graphic coords="4,203.93,115.84,207.50,217.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,134.77,197.25,305.40,164.41"><head>Table 11 .</head><label>11</label><figDesc>Results with the 2011 topics, sorted on MAP.</figDesc><table coords="9,134.77,197.25,305.40,153.95"><row><cell cols="2">5 Experiments with the 2011 Topics</cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell>w Details</cell><cell>MAP P10</cell><cell>P20 bpref</cell></row><row><cell>QD + CompSUM</cell><cell>0.6</cell><cell cols="2">0.2886 0.4860 0.3870 0.2905</cell></row><row><cell>QD + CompDUTH</cell><cell>0.5</cell><cell cols="2">0.2871 0.4620 0.3870 0.2885</cell></row><row><cell>QD + CompSUM</cell><cell>0.4</cell><cell cols="2">0.2866 0.5120 0.4190 0.3014</cell></row><row><cell cols="4">MinMax + CompDUTH 0.7 PerIndexLang 0.2840 0.4580 0.3990 0.2775</cell></row><row><cell cols="4">MinMax + CompSUM 0.7 PerIndexLang 0.2818 0.4840 0.3990 0.2945</cell></row><row><cell cols="4">MinMax + CompDUTH 0.6 PerIndexLang 0.2786 0.4640 0.4110 0.2815</cell></row><row><cell cols="4">MinMax + CompDUTH 0.8 PerIndexLang 0.2751 0.4360 0.3730 0.2677</cell></row><row><cell cols="4">MinMax + CompSUM 0.8 PerModality 0.2717 0.4380 0.3740 0.2728</cell></row><row><cell>QD + CompDUTH</cell><cell>0.3</cell><cell cols="2">0.2605 0.4840 0.4090 0.2768</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.93,209.81,6.31"><p>http://www.imageclef.org/2011/Wikipedia</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,657.93,145.26,6.31"><p>http://www.lemurproject.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="9,144.73,646.13,335.86,7.77;9,144.73,657.08,193.39,7.77"><p>Last year's best MAP, P10, P20, and bpref were 0.2765, 0.6114, 0.5407, and 0.3137, respectively; they were all achieved by the XRCE group<ref type="bibr" coords="9,325.42,657.08,9.52,7.77" target="#b4">[5]</ref>.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>w MAP P10 P20 bpref 0.5 0.1712 0.5329 0.4757 0.2273 0.6 0.2283 0.5771 0.5164 0.2825 0.7 0.2741 0.5743 0.5221 0.3258 0.8 0.3004 0.5543 0.4971 0.3442 0.9 0.2940 0.5186 0.4629 0.3335 Table <ref type="table" coords="7,163.36,182.98,3.36,8.06">1</ref>. MinMax per modality + CompSUM w MAP P10 P20 bpref 0.6 0.1837 0.5300 0.4807 0.2411 0.7 0.2360 0.5700 0.5100 0.2935 0.8 0.2712 0.5586 0.4879 0.3228 0.9 0.2773 0.5257 0.4557 0.3187 1.0 0.2461 0.4871 0.4121 0.2871 Tables 1, 2, 3, and 4 summarize the MinMax+CompSUM results. Best early precision is achieved by per-index-language MinMax at w = 0.7, while the best effectiveness in terms of MAP and all other measures is achieved by per-modality MinMax at w = 0.8. Per-modality-type is the weakest MinMax normalization, while per-query-language is competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MinMax+CompDUTH</head><p>w MAP P10 P20 bpref 0.4 0.1781 0.5157 0.4636 0.2322 0.5 0.2427 0.5471 0.4900 0.2921 0.6 0.2789 0.5457 0.4864 0.3207 0.7 0.2937 0.5329 0.4679 0.3304 0.8 0.2903 0.5029 0.4421 0.3238 Table <ref type="table" coords="7,160.63,526.99,3.36,8.06">5</ref>. MinMax per modality + CompDUTH w MAP P10 P20 bpref 0.5 0.1920 0.5129 0.4486 0.2465 0.6 0.2382 0.5171 0.4593 0.2875 0.7 0.2622 0.4886 0.4521 0.3085 0.8 0.2646 0.4543 0.4136 0.3061 0.9 0.2520 0.4271 0.3879 0.2900 Table <ref type="table" coords="7,334.96,526.99,3.36,8.06">6</ref>. MinMax per mod. type + CompDUTH Tables 5, 6, 7, and 8 summarize the MinMax+CompDUTH results. Per-modalitytype is the weakest MinMax normalization, followed by per-query-language. Best early precision is achieved by per-modality (best P10) at w = 0.5 and per-index-language (best P20) at w = 0.6. Per-modality at w = 0.7 achieves the best MAP, while perindex-language achieves the best bpref at w = 0.7. Although per-index-language has lower MAP than per-modality, its MAP comparable to per-modality; moreover, perindex-language achieves a higher bpref which signals that we may be retrieving unjudged relevant items. All in all, we conclude that per-index-language is the strongest MinMax normalization.</p><p>w MAP P10 P20 bpref 0.5 0.2237 0.5414 0.4764 0.2738 0.6 0.2724 0.5429 0.4950 0.3167 0.7 0.2922 0.5343 0.4750 0.3325 0.8 0.2911 0.5186 0.4593 0.3259 0.9 0.2772 0.4900 0.4371 0.3113 Table <ref type="table" coords="8,157.71,182.98,3.36,8.06">7</ref>. MinMax per index lang. + CompDUTH w MAP P10 P20 bpref 0.5 0.1702 0.4971 0.4429 0.2161 0.6 0.2283 0.5400 0.4736 0.2783 0.7 0.2624 0.5143 0.4657 0.3122 0.8 0.2711 0.5029 0.4514 0.3177 0.9 0.2596 0.4614 0.4193 0.3012 Table <ref type="table" coords="8,333.94,182.98,3.36,8.06">8</ref>. MinMax per query lang. + CompDUTH</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overall Comparison of CompSUM, CompDUTH, and MinMax Types</head><p>Overall, best early precision is achieved by per-index-language MinMax with Comp-SUM at w = 0.7, and all other measures are optimized by per-modality MinMax with CompSUM at w = 0.8. However, since the 2011 topic set consists of 4 or 5 example images per topic, CompDUTH may show larger effectiveness differences than these on the 2010 topic set; consequently, we will retain CompDUTH runs with 2011 topic set, using per-index-language MinMax and w = 0.6, 0.7, 0.8. All these will result to 5 runs in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">QD Normalization</head><p>w MAP P10 P20 bpref 0.3 0.2090 0.5557 0.4986 0.2598 0.4 0.2562 0.5914 0.5243 0.3087 0.5 0.2807 0.5657 0.5129 0.3296 0.6 0.2928 0.5471 0.4971 0.3383 0.7 0.2907 0.5286 0.4714 0.3344 Table <ref type="table" coords="8,196.49,442.13,3.36,8.06">9</ref>. QD + CompSUM w MAP P10 P20 bpref 0.2 0.1913 0.5200 0.4507 0.2427 0.3 0.2595 0.5671 0.4957 0.3034 0.4 0.2851 0.5586 0.4779 0.3252 0.5 0.2901 0.5371 0.4707 0.3287 0.6 0.2864 0.5100 0.4507 0.3236 Table <ref type="table" coords="8,367.83,442.13,7.47,8.06">10</ref>. QD + CompDUTH</p><p>Tables 9 and 10 summarize the QD normalization results with both combination methods. In early precision, the QD normalization works much better with CompSUM than with CompDUTH. The best CompSUM results are achieved for w = 0.4; this run has also the best P10 we have reported so far. In all other measures, although CompSUM is slightly better than CompDUTH, their effectiveness is comparable.</p><p>In comparison to the MinMax normalizations, the QD normalization achieves the best initial precision results (when CompSUM is used for combination), and comparable effectiveness to the best MinMax normalization in all other measures.</p><p>In summary, we will retain QD+CompSUM at w = 0.4 and QD+CompDUTH at w = 0.3 and 0.5; thus, we will have 3 QD runs in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Summary</head><p>While we have experimented with radically different normalization and combination methods, our results have not shown a large variance. This suggests that we are 'pushing' at the effectiveness ceiling of the 2010 dataset. It is worth noting that most of</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.13,354.08,342.46,7.77;10,146.47,365.04,194.29,7.77" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,331.79,354.08,148.80,7.77;10,146.47,365.04,109.95,7.77">Multimedia search with noisy modalities: Fusion and multistage retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zagoris</surname></persName>
		</author>
		<editor>Braschler et al.</editor>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.13,376.00,342.46,7.77;10,146.47,386.96,183.26,7.77" xml:id="b1">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="10,313.49,376.00,124.47,7.77">CLEF 2010 LABs and Workshops</title>
		<title level="s" coord="10,445.23,376.00,35.36,7.77;10,146.47,386.96,22.30,7.77">Notebook Papers</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Pianta</surname></persName>
		</editor>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">September 2010. 2010</date>
			<biblScope unit="page" from="22" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.13,397.92,342.46,7.77;10,146.47,408.88,315.45,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,321.60,397.92,158.99,7.77;10,146.47,408.88,194.96,7.77">Selection of the proper compact composite descriptor for improving content-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,359.05,408.88,29.65,7.77">SPPRA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="134" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.13,419.84,342.46,7.77;10,146.47,430.80,334.12,7.77;10,146.47,441.75,251.37,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,314.05,419.84,166.54,7.77;10,146.47,430.80,334.12,7.77;10,146.47,441.75,28.50,7.77">SpCD -Spatial Color Distribution Descriptor -A fuzzy rule-based compact composite descriptor appropriate for hand drawn color sketches retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,192.59,441.75,75.57,7.77">Proceedings ICAART</title>
		<meeting>ICAART</meeting>
		<imprint>
			<publisher>INSTICC Press</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="58" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.13,452.71,342.47,7.77;10,146.47,463.67,334.12,7.77;10,146.47,474.63,200.50,7.77" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,146.47,463.67,334.12,7.77;10,146.47,474.63,96.91,7.77">Xrce&apos;s participation in wikipedia retrieval, medical image modality classification and ad-hoc retrieval tasks of imageclef</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ah-Pine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jacquet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Minoukadeh</surname></persName>
		</author>
		<editor>Braschler et al.</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.13,485.59,342.46,7.77;10,146.47,496.55,82.08,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,314.29,485.59,106.73,7.77">Predicting query performance</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,440.80,485.59,21.38,7.77">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.13,507.51,342.47,7.77;10,146.47,518.47,334.12,8.87;10,146.47,530.27,43.04,6.31" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,210.86,507.51,266.57,7.77">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName coords=""><forename type="first">Spärck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename></persName>
		</author>
		<ptr target="http://www.soi.city.ac.uk/˜ser/idf.html" />
	</analytic>
	<monogr>
		<title level="j" coord="10,146.47,518.47,95.29,7.77">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.13,540.38,342.46,7.77;10,146.47,551.34,189.16,7.77" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,345.97,540.38,134.62,7.77;10,146.47,551.34,47.55,7.77">www.mmretrieval.net: a multimodal search engine</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zagoris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<editor>SISAP.</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="117" to="118" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
