<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.25,114.78,318.80,15.06;1,278.37,132.72,58.62,15.06;1,214.86,150.64,185.60,15.06;1,207.71,168.58,198.02,15.06">Evaluating some contextual factors for image retrieval ReDCAD participation at ImageCLEFWikipedia 2011</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,158.21,207.75,96.30,10.46"><roleName>Mouna</roleName><forename type="first">Hatem</forename><surname>Awadi</surname></persName>
							<email>awadi.hatem@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Applied Mathematics</orgName>
								<orgName type="laboratory">Research unit on Development and Control of Distributed Applications (ReDCAD)</orgName>
								<orgName type="institution" key="instit1">National School of Engineers of Sfax</orgName>
								<orgName type="institution" key="instit2">University of Sfax</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.83,207.75,91.44,10.46"><forename type="first">Torjmen</forename><surname>Khemakhem</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Applied Mathematics</orgName>
								<orgName type="laboratory">Research unit on Development and Control of Distributed Applications (ReDCAD)</orgName>
								<orgName type="institution" key="instit1">National School of Engineers of Sfax</orgName>
								<orgName type="institution" key="instit2">University of Sfax</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.69,207.75,79.46,10.46"><forename type="first">Maher</forename><forename type="middle">Ben</forename><surname>Jemaa</surname></persName>
							<email>maher.benjemaa@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Applied Mathematics</orgName>
								<orgName type="laboratory">Research unit on Development and Control of Distributed Applications (ReDCAD)</orgName>
								<orgName type="institution" key="instit1">National School of Engineers of Sfax</orgName>
								<orgName type="institution" key="instit2">University of Sfax</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.25,114.78,318.80,15.06;1,278.37,132.72,58.62,15.06;1,214.86,150.64,185.60,15.06;1,207.71,168.58,198.02,15.06">Evaluating some contextual factors for image retrieval ReDCAD participation at ImageCLEFWikipedia 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E4B8DE243A97968466B23E8543391EFC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>context-based image retrieval</term>
					<term>textual content</term>
					<term>metadata</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our participation in the ImageCLEF Wikipedia retrieval task aims to study the efficiency of using two contextual factors in image retrieval: metadata which contains specific information about images, and textual content which contains general information about images. For this aim, the Lucene library is used for indexing and searching. We propose also to combine both factors using two different methods: one based on simple linear function and one based on scores comparison. In addition, a comparison between monolingual and multilingual image retrieval using queries in a single language (English) and queries in different language is done. Results show that the use of textual content is more useful then the use of metadata and the combination of both factors further improves results. In addition, the use of all provided languages exceeds over the use of only English langage.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is our first participation in imageClef Wikipedia retrieval task. We aim in this work to evaluate the impact of some contextual factors in image retrieval using the Wikipedia collection. More precisely, we used the Lucene Search engine <ref type="bibr" coords="1,476.12,572.95,3.97,7.32" target="#b0">1</ref> to calculate an image relevance score for each contextual factor: textual content and metadata. The difference between these two factors is that the metadata contains the description of the image, so the most specific information. On the contrary, the textual content can be the same for two or more images, so it contains general information about the image.</p><p>Consequently, comparing the use of specific information and general information to represent images returns to compare the use of metadata and textual content to compute the image score.</p><p>To well evaluate these two factors, a combination between them should be done. We propose so to use either a classical linear combination, or a comparisonbased approach.</p><p>In addition, we would like to study the impact of the monolingual and multilingual image retrieval by using queries in a single language (English) and queries in different language. When using an English query, only the English documents are used, and when using a query in different languages (English, French and Dutch), the three translations of the query will be used in a single query, and the whole collection (English and/or French and/or Dutch documents) will be used.</p><p>The remainder of this paper is organized as follows. Section 2 describes our retrieval model. Section 3 details the runs and discuss the results. Finally, a conclusion and future work are done in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Textual model</head><p>To calculate a relevance score for images using the metadata or the textual content, we used the Lucene library for indexing and searching.</p><p>Let q be a query, t a term of q , d j a given document and im i,j is an image belonging to the document d j . The relevance score of an image equals the relevance score of the document containing the image. It is calculated according to the following formula provided by the Lucene search engine [1]:</p><formula xml:id="formula_0" coords="2,158.77,463.62,321.82,29.39">S(im i,j , q) = coord(q, d j ) * queryN orm(q) * (tf (t ∈ d j ) * idf (t) 2 * boost(t.f ield ∈ d j ) * lengthN orm(t.f ield ∈ d j ))<label>(1)</label></formula><p>where :</p><p>-coord(q, d): Coordination factor, based on the number of query terms the document contains. The coordination factor gives a boost to documents that contain more of the search terms than other documents. -queryN orm(q): Normalization value for a query, given the sum of the squared weights of each of the query terms. -tf (t ∈ d j ): Term frequency factor for the term t in the document d i : how many times the term t occurs in the document. -idf (t): Inverse document frequency of the term: a measure of how "unique" the term is. Very common terms have a low idf ; very rare terms have a high idf .</p><p>-boost(t.f ield ∈ d): Field and document boost, as set during indexing. It can be used to statically boost certain fields and certain documents over others.</p><p>-lengthN orm(t.f ield ∈ d): Normalization value of a field, given the number of terms within the field. This value is computed during indexing and stored in the index norms. Shorter fields (fewer tokens) get a bigger boost from this factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Combination model</head><p>In order to combine both scores of textual content and metadata, we propose to use either a simple linear combination or a comparison-based combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear combination</head><p>To calculate a linear combination score of each image S LC (im), we used the following equation:</p><formula xml:id="formula_1" coords="3,207.40,288.12,273.19,11.36">S LC (im) = α × S txt (im) + (1 -α) × S md (im) (2)</formula><p>where S txt (im) is the image score according to the textual content and S md (im) is image score according to the metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison-based combination</head><p>Since both scores of metadata (specific information) and textual content (generic information) are computed by the same equation and the same way, we propose here to use for each image only the best factor, which gives the best representation for the image. To achieve this idea, we propose to use the maximum score of both factors after normalization, as follows:</p><formula xml:id="formula_2" coords="3,227.21,423.88,253.37,11.35">S CC (im) = M ax(S txt (im), S md (im))<label>(3)</label></formula><p>Where S CC (im) is the final score of the image after combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Textual model results</head><p>In this section, we present the results obtained by our approach when using the metadata and textual content. Table <ref type="table" coords="3,298.04,525.36,4.98,10.46" target="#tab_0">1</ref> presents our official runs. -redcad01tx: This run used only the English version of textual content and queries. In fact, a score was calculated for each document using the formula 1, and then, this score was attributed to all images in this document. -redcad02tx: This run used all provided textual content and all query languages. More precisely, we have concatenated the queries provided in three language in a single query, and then performed the search in the textual content in different languages. -redcad01md: This run used only the English version of the metadata and the queries. All extracted fields are used with the same importance i.e. the boost factor in formula 1 was fixed to 1. -redcad02md: This run used the metadata with all queries language as in the case of redcad02tx approach. All fields are used with the same importance.</p><p>According to the different metrics, results show that the use of a query composed of three languages (redcad02tx and redcad02md) is more efficient than the use of a single language query (redcad01tx and redcad01md). This result is expected given that the images are described or belonging on documents in different languages. In fact, it is possible that some relevant images are described in a language and the query is in another language, then the image cannot be retrieved.</p><p>By comparing the use of textual content and the metadata, we note that results obtained by using textual content outperform results obtained by using the metadata. A possible reason is that some relevant images have a short or no associated metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Combination model results</head><p>In our additional runs, we combined the best results obtained by using the textual content and the metadata. In these experiments, the whole collection and only one query composed of the three queries were used.</p><p>Table <ref type="table" coords="4,177.10,475.26,4.98,10.46" target="#tab_1">2</ref> presents obtained results. Our official runs are in grayed boxes.</p><p>According to the different metrics, the combination of both scores of textual content and metadata improves the retrieval accuracy. More precisely, best results are obtained with α ∈ [0.4..0.6]. Consequently, we can conclude that both factors are complementary and support the image retrieval.</p><p>Concerning the comparison-based combination, table 2 presents obtained results. We note here that using a comparison-based combination improves slightly results of a classical linear combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>Thanks to previous experiments, we can conclude that using generic information (contextual content) is more significant than using specific information (metadata) about images. However, combining both factors improves results, so they are both important to determine image relevance. In fact, both factors are complementary. To compare fairly our runs to official ones in the Wikipedia retrieval, we take into account only official runs using textual information without relevance feedback or query expansion. Table <ref type="table" coords="5,291.79,388.57,4.98,10.46" target="#tab_3">4</ref> shows official results ranked by MAP: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and future work</head><p>In this paper, we addressed the context-based image retrieval approach. We studied and evaluated the impact of using two contextual factors: textual content and metadata in image retrieval. Results in the Wikipedia Clef 2011 collection showed that the textual content which contains general information about images is more significant then the metadata which contains specific information, but the combination of both factors is encouraged and consequently the two factors are complementary.</p><p>In addition, comparing the use of monolingual and multilingual retrieval according to the query language, we can conclude that the use of a query composed of different language is more interesting than the use of queries in a single language.</p><p>In future work, we plan to extract and use semantics of image contextual factors in image retrieval and to add a content based image retrieval in our system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,201.70,556.10,211.95,74.42"><head>Table 1 .</head><label>1</label><figDesc>Impact of textual content and metadata</figDesc><table coords="3,201.70,576.88,211.95,53.64"><row><cell>Runs</cell><cell>MAP P10</cell><cell>P20 Rprec bpref</cell></row><row><cell cols="3">redcad01tx 0.1335 0.3160 0.2760 0.2181 0.1735</cell></row><row><cell cols="3">redcad02tx 0.2306 0.3700 0.3060 0.2862 0.2326</cell></row><row><cell cols="3">redcad01md 0.1578 0.3140 0.2420 0.2052 0.1730</cell></row><row><cell cols="3">redcad02md 0.1896 0.3400 0.2920 0.2386 0.2016</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,162.71,115.62,286.76,149.39"><head>Table 2 .</head><label>2</label><figDesc>Classical linear combination of textual content and metadata</figDesc><table coords="5,191.01,134.65,233.32,130.35"><row><cell>Runs</cell><cell>α MAP P10</cell><cell>P20 Rprec bpref</cell></row><row><cell cols="3">redcad02md 0 0.1896 0.3400 0.2920 0.2386 0.2016</cell></row><row><cell cols="3">redcadComb1 0.1 0.2092 0.3660 0.2960 0.2609 0.2177</cell></row><row><cell cols="3">redcadComb2 0.2 0.2244 0.3880 0.3140 0.2823 0.2379</cell></row><row><cell cols="3">redcadComb3 0.3 0.2355 0.4220 0.3210 0.2953 0.2520</cell></row><row><cell cols="3">redcadComb4 0.4 0.2412 0.4380 0.3280 0.2965 0.2571</cell></row><row><cell cols="3">redcadComb5 0.5 0.2468 0.4200 0.3300 0.2978 0.2584</cell></row><row><cell cols="3">redcadComb6 0.6 0.2467 0.4160 0.3380 0.2940 0.2579</cell></row><row><cell cols="3">redcadComb7 0.7 0.2416 0.4180 0.3340 0.2888 0.2531</cell></row><row><cell cols="3">redcadComb8 0.8 0.2355 0.4100 0.3260 0.2951 0.2511</cell></row><row><cell cols="3">redcadComb9 0.9 0.2318 0.4060 0.3240 0.2929 0.2498</cell></row><row><cell>redcad02txt</cell><cell cols="2">1 0.2306 0.3700 0.3060 0.2862 0.2326</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,157.58,287.31,300.07,41.54"><head>Table 3 .</head><label>3</label><figDesc>Comparison-based combination of textual context and metadata</figDesc><table coords="5,211.37,308.09,192.61,20.76"><row><cell>Run</cell><cell>MAP P10 P20 Rprec bpref</cell></row><row><cell cols="2">redcadCBC 0.2537 0.3920 0.3310 0.3008 0.2494</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,134.77,420.15,345.83,166.68"><head>Table 4 .</head><label>4</label><figDesc>Results of approaches using only simple textual informationAccording to this table, we have obtained third rank, but if we include our best run which is obtained by comparison-based combination (MAP=0.2537), our rank becomes the second.</figDesc><table coords="5,251.80,440.93,111.74,75.56"><row><cell cols="3">Rank Participant MAP</cell></row><row><cell>1</cell><cell>UNED</cell><cell>0.3044</cell></row><row><cell>2</cell><cell>DEMIR</cell><cell>0.2432</cell></row><row><cell>3</cell><cell cols="2">ReDCAD 0.2306</cell></row><row><cell>4</cell><cell>SZTAKI</cell><cell>0.2167</cell></row><row><cell>5</cell><cell cols="2">DBISForMaT 0.2096</cell></row><row><cell>6</cell><cell>SINAI</cell><cell>0.2068</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,656.48,103.92,9.41"><p>http://lucene.apache.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,293.71,342.10,9.41;6,146.91,304.66,147.72,9.41" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Otis</surname></persName>
		</author>
		<title level="m" coord="6,272.54,293.71,114.32,9.41">Lucene in Action 2nd Edition</title>
		<imprint>
			<publisher>Manning Publications Co, United States of America</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
