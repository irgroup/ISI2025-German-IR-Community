<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.34,115.96,338.69,12.62;1,139.52,133.89,336.32,12.62">MLKD&apos;s Participation at the CLEF 2011 Photo Annotation and Concept-Based Retrieval Tasks</title>
				<funder>
					<orgName type="full">PetaMedia Network of Excellence)</orgName>
				</funder>
				<funder ref="#_PpYfbjz">
					<orgName type="full">EU FP7</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,187.57,171.57,134.30,8.74"><forename type="first">Eleftherios</forename><surname>Spyromitros-Xioufis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept of Informatics Aristotle</orgName>
								<orgName type="institution">University of Thessaloniki</orgName>
								<address>
									<postCode>54124</postCode>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,329.65,171.57,93.97,8.74"><forename type="first">Konstantinos</forename><surname>Sechidis</surname></persName>
							<email>sechidis@csd.auth.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept of Informatics Aristotle</orgName>
								<orgName type="institution">University of Thessaloniki</orgName>
								<address>
									<postCode>54124</postCode>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,212.14,183.53,89.53,8.74"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept of Informatics Aristotle</orgName>
								<orgName type="institution">University of Thessaloniki</orgName>
								<address>
									<postCode>54124</postCode>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,329.49,183.53,73.72,8.74"><forename type="first">Ioannis</forename><surname>Vlahavas</surname></persName>
							<email>vlahavas@csd.auth.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept of Informatics Aristotle</orgName>
								<orgName type="institution">University of Thessaloniki</orgName>
								<address>
									<postCode>54124</postCode>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.34,115.96,338.69,12.62;1,139.52,133.89,336.32,12.62">MLKD&apos;s Participation at the CLEF 2011 Photo Annotation and Concept-Based Retrieval Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6BB279D40B26182EA98A6348744D8821</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We participated both in the photo annotation and conceptbased retrieval tasks of CLEF 2011. For the annotation task we developed visual, textual and multi-modal approaches using multi-label learning algorithms from the Mulan open source library. For the visual model we employed the ColorDescriptor software to extract visual features from the images using 7 descriptors and 2 detectors. For each combination of descriptor and detector a multi-label model is built using the Binary Relevance approach coupled with Random Forests as the base classifier. For the textual models we used the boolean bag-of-words representation, and applied stemming, stop words removal, and feature selection using the chi-squared-max method. The multi-label learning algorithm that yielded the best results in this case was Ensemble of Classifier Chains using Random Forests as base classifier. Our multi-modal approach was based on a hierarchical late-fusion scheme. For the concept based retrieval task we developed two different approaches. The first one is based on the concept relevance scores produced by the system we developed for the annotation task. It is a manual approach, because for each topic we manually selected the relevant topics and manually set the strength of their contribution to the final ranking produced by a general formula that combines topic relevance scores. The second approach is based solely on the sample images provided for each query and is therefore fully automated. In this approach only the textual information was used in a query-by-example framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF is the cross-language image retrieval track run annually since 2003 as part of the Cross Language Evaluation Forum (CLEF) <ref type="foot" coords="1,396.67,587.54,3.97,6.12" target="#foot_0">1</ref> . This paper documents the participation of the Machine Learning and Knowledge Discovery (MLKD) group of the Department of Informatics of the Aristotle University of Thessaloniki at the photo annotation task (also called visual concept detection and annotation task) of ImageCLEF 2011. This year, the photo annotation task consisted of two subtasks. An annotation task, similar to that of ImageCLEF 2010, and a new concept-based retrieval task. Data for both tasks come from the MIRFLICKR-1M image dataset <ref type="bibr" coords="2,467.30,142.90,9.96,8.74" target="#b0">[1]</ref>, which apart from the image files contains Flickr user tags and Exchangeable Image File Format (Exif) information. More information about the exact setup of the data can be found in <ref type="bibr" coords="2,257.36,178.77,9.96,8.74" target="#b3">[4]</ref>.</p><p>In the annotation task, participants are asked to annotate a test set of 10,000 images with 99 visual concepts. An annotated training set of 8,000 images is provided. This multi-label learning task <ref type="bibr" coords="2,309.76,214.88,10.52,8.74" target="#b7">[8]</ref> can be solved in three different ways according to the type of information used for learning: 1) visual (the image files), 2) textual (Flickr user tags), 3) multi-modal (visual and textual information). We developed visual, textual and multi-modal approaches for this task using multi-label learning algorithms from the Mulan open source library <ref type="bibr" coords="2,434.75,262.70,9.96,8.74" target="#b8">[9]</ref>. In this task, the relative performance of our textual models was quite good, but that of our visual models was bad (our group does not have expertise on computer vision), leading to an average multi-modal (and overall) performance.</p><p>In the concept-based retrieval task, participants were given 40 topics consisting of logical connections between the 99 concepts of the photo annotation task, such as "find all images that depict a small group of persons in a landscape scenery showing trees and a river on a sunny day", along with 2 to 5 examples images of each topic from the training set of the annotation task. Participants were asked to submit (up to) the 1,000 most relevant photos for each topic in ranked order from a set of 200,000 unannotated images. This task can be solved by manual construction of the query out of the narrative of the topics, followed by automatic retrieval of images, or by a fully automated process. We developed a manual approach that exploits the multi-label models trained in the annotation task and a fully automated query-by-example approach based on the tags of the example images. In this task, both our manual and automated approaches ranked 1st in all evaluation measures by a large margin.</p><p>The rest of this paper is organized as follows. Sections 2 and 3 describe our approaches to the annotation task and concept-based retrieval task respectively. Section 4 presents the results of our runs for both tasks. Section 5 concludes our work and poses future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Annotation Task</head><p>This section presents the visual, textual and multi-modal approaches that we developed for the automatic photo annotation task. There were two (eventually three) evaluation measures to consider for this task: a) mean interpolated average precision (MIAP), b) example-based F-measure (F-ex), c) semantic R-precision (SR-Precision). In order to optimize a learning approach based on each of the initial two evaluation measures and type of information, six models should be built. However, there were only five runs allowed for this task. We therefore decided to perform model selection based on the widely-used mean average precision (MAP) measure for all types of information. In particular, MAP was estimated using an internal 3-fold cross-validation on the 8,000 training images. Our multimodal approach was submitted in three different variations to reach the total number of five submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Automatic Annotation with Visual Information</head><p>We here describe the approach that we followed in order to learn multi-label models using the visual information of the images. The flowchart of this approach is shown in Fig. <ref type="figure" coords="3,206.96,221.54,3.87,8.74">1</ref>.</p><formula xml:id="formula_0" coords="3,180.95,254.09,279.09,24.37">i i x ( | ) j i p c x j </formula><p>Fig. <ref type="figure" coords="3,215.65,312.41,4.13,7.89">1</ref>. Automatic annotation using visual information.</p><p>As our group does not have expertise in computer vision, we largely followed the color-descriptor extraction approach described in <ref type="bibr" coords="3,401.10,359.14,10.96,8.74" target="#b5">[6,</ref><ref type="bibr" coords="3,412.06,359.14,7.31,8.74" target="#b6">7]</ref> and used the accompanying software tool<ref type="foot" coords="3,255.51,369.52,3.97,6.12" target="#foot_1">2</ref> for extracting visual features from the images.</p><p>Harris-Laplace and Dense Sampling were used as point detection strategies. Furthermore seven different descriptors were used: SIFT, HSV-SIFT, HueSIFT, OpponentSIFT, C-SIFT, rgSIFT and RGB-SIFT. For each one of the 14 combinations of point detection strategy and descriptor, a different codebook was created in order to obtain a fixed length representation for all images. This is also known as the bag-of-words approach. The k-means clustering algorithm was applied to 250,000 randomly sampled points from the training set, with the codebook size (k) fixed to 4096 words. Finally, we employed hard assignment of points to clusters.</p><p>Using these 4,096-dimensional vector representations along with the ground truth annotations given for the training images we built 14 multi-label training datasets. After experimenting with various multi-label learning algorithms we found that the simple Binary Relevance (BR) approach coupled with Random Forests as the base classifier (number of trees = 150, number of features = 40) yielded the best results.</p><p>In order to deal with the imbalance in the number of positive and negative examples of each label we used instance weighting. The weight of the examples of the minority class was set to (min+maj)/min and the weight of the examples of the majority class was set to (min + maj)/maj, where min is the number of examples of the minority class and maj the number of examples of the majority class. We also experimented with sub-sampling, but the results were worse than instance weighting.</p><p>Our approach concludes with a late fusion scheme that averages the output of the 14 different multi-label models that we built.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Automatic Annotation with Flickr User Tags</head><p>We here describe the approach that we followed in order to learn multi-label models using the tags assigned to images by Flickr users. The flowchart of this approach is shown in Fig. <ref type="figure" coords="4,249.89,203.75,3.87,8.74" target="#fig_0">2</ref>.  An initial vocabulary was constructed by taking the union of the tag sets of all images in the training set. We then applied stemming to this vocabulary and removed stop words. This led to a vocabulary of approximately 27000 stems. The use of stemming improved the results, despite that some of the tags were not in the English language and that we used an English stemmer. We further applied feature selection in order to remove irrelevant or redundant features and improve efficiency. In particular, we used the χ 2 max criterion <ref type="bibr" coords="4,391.42,415.94,10.52,8.74" target="#b2">[3]</ref> to score the stems and selected the top 4000 stems, after experimenting with a variety of sizes (500, 1000, 2000, 3000, 4000, 5000, 6000 and 7000).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stemmer</head><p>The multi-label learning algorithm that was found to yield the best results in this case was Ensemble of Classifier Chains (ECC) <ref type="bibr" coords="4,370.63,463.76,10.52,8.74" target="#b4">[5]</ref> using Random Forests as base classifier. ECC was run with 15 classifier chains and Random Forests with 10 decision trees, while all other parameters were left to their default value. The approach that we followed to deal with class imbalance in the case of visual information (see the previous subsection), was followed in this case too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Automatic Annotation with a Multi-Modal Approach</head><p>Our multi-modal approach is based on a late fusion scheme that combines the output of the 14 visual models and the single textual model. The combination is not an average of these 15 models, because in that case the visual models would dominate the final scores. Instead, we follow a hierarchical combination scheme. We separately average the 7 visual models of each point estimator and then combine the output of the textual model, the Harris-Laplace average and the Dense Sampling average, as depicted in Fig. <ref type="figure" coords="4,307.69,632.21,3.87,8.74">3</ref>. The motivation for this scheme was the three different views of the images that existed in the data (Harris-Laplace, Dense Sampling, user tags) as explained in the following two paragraphs. </p><formula xml:id="formula_1" coords="5,187.24,159.69,185.33,209.73">j i p c x j  ( | ) j i p c x j  ( | ) j i p c x j  Fig. 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Automatic annotation with a multi-modal approach</head><p>We can discern two main categories of concepts in photo annotation: objects and scenes. For objects, Harris-Laplace performs better because it ignores the homogeneous areas, while for scenes, Dense Sampling performs better <ref type="bibr" coords="5,434.77,424.07,9.96,8.74" target="#b5">[6]</ref>. For example, two of the concepts where Dense Sampling achieves much higher Average Precision (AP) from Harris-Laplace are Night and Macro, which are abstract, while the inverse is happening in concepts Fish and Ship, which correspond to things (organisms, objects) of particular shape.</p><p>Furthermore, we observe that the visual approach performs better in concepts, such as Sky, which for some reason (e.g. lack of user interest for retrieval by this concept) do not get tagged. On the other hand the textual approach performs much better when it has to predict concepts, such as Horse, Insect, Dog and Baby that typically get tagged by users. Table <ref type="table" coords="5,391.03,534.12,4.98,8.74" target="#tab_2">1</ref> shows the average precision for 10 concepts, half of which suit much better the textual models and half the visual models.</p><p>Two variations of this scheme were developed, differing in how the output of the three different views is combined. The first one, named Multi-Modal-Avg, used an averaging operator, similarly to the one used at the lower levels of the hierarchy. The second one, named Multi-Modal-MaxAP, used an arbitrator function to select the best one out of the three outputs for each concept, according to internal evaluation results in terms of average precision. Our third multi-modal submission, named Multi-Modal-MaxAP-RGBSIFT, was a preliminary version of Multi-Modal-MaxAP, where only the RGBSIFT descriptor was used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Thresholding</head><p>The multi-label learners used in this work provide us with a confidence score for each concept. This is fine for an evaluation with MIAP and SR-Precision, but does not suffice for an evaluation with example-based F-measure, which requires a bipartition of the concepts into relevant and irrelevant ones. This is a typical issue in multi-label learning, which is dealt with a thresholding process <ref type="bibr" coords="6,445.93,314.64,9.96,8.74" target="#b1">[2]</ref>.</p><p>We used the thresholding method described in <ref type="bibr" coords="6,356.82,327.10,9.96,8.74" target="#b4">[5]</ref>, which applies a common threshold across all concepts and provides a close approximation of the label cardinality (LC) of the training set to the predictions made on the test set. The threshold is calculated using the following formula:</p><formula xml:id="formula_2" coords="6,179.54,388.90,301.05,9.96">t = argmin {t∈0.00,0.05,...,1.00} |LC(D train ) -LC(H t (D test ))|<label>(1)</label></formula><p>where D train is the training set and H t is a classifier which has made predictions on a test set D test under threshold t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Concept-Based Retrieval Task</head><p>We developed two different approaches for the concept-based retrieval task. The first one is based on the concept relevance scores produced by the system we developed for the annotation task. It is a manual approach, because for each topic we manually selected the relevant topics and manually set the strength of their contribution to the final ranking produced by a general formula that combines topic relevance scores. The second one is based solely on the sample images provided for each query and is therefore fully automated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Manual Approach</head><p>Let I = 1, . . . , 200, 000 be the collection of images, Q = 1, . . . , 40 the set of topics and C = 1, . . . , 99 the set of concepts. We first apply our automated image annotation system to each image i ∈ I and obtain a corresponding 99dimensional vector S i = [s 1 i , s 2 i , ..., s 99 i ] with the relevance scores of this image to each one of the 99 concepts. For efficiency reasons, we used simplified versions of our visual approach, taking into account only models produced with the RGB-SIFT descriptor, which has been found in the past to provide better results compared to other single color descriptors <ref type="bibr" coords="7,321.04,142.90,9.96,8.74" target="#b6">[7]</ref>.</p><p>Then, based on the description of each of the 40 queries, we manually select a number of concepts that we consider related to the query, either positively or negatively. Formally, for topic q ∈ Q let P q ⊆ C denote the set of concepts that are positively related to q and N q ⊆ C the set of concepts that are negatively related to q, P q ∩ N q = ∅. For each concept c in P q ∪ N q , we further define a real valued parameter m c q ≥ 1 denoting the strength of relevance of concept c to q. The larger this value, the stronger the influence of concept c to the final relevance score. For each topic q and image i, the scores of the relevant concepts are combined using <ref type="bibr" coords="7,222.02,251.90,11.62,8.74" target="#b1">(2)</ref>.</p><formula xml:id="formula_3" coords="7,237.20,277.69,243.39,23.78">S q,i = c∈Pq (s c i ) m c q c∈Nq (1 -s c i ) m c q (2)</formula><p>Finally, for each topic, we arrange the images in descending order according to the overall relevance score and we retrieve a fixed number of images (in our submissions we retrieved 250 and 1,000 images).</p><p>Note that for each topic, the selection of related concepts and the setting of values for the m c q parameters was done using a trial-and-error approach involving careful visual examination of the top 10 retrieved images, as well as more relaxed visual examination of the top 100 retrieved images. Two examples of topics and corresponding combination of scores follow.</p><p>Topic 5: rider on horse. Here we like to find photos of riders on a horse. So no sculptures or paintings are relevant. The rider and horse can be also only in parts on the photo. It is important that the person is riding a horse and not standing next to it. Based on the description of this topic and experimentation, we concluded that concepts 75 (Horse) and 8 (Sports) are positively related (rider on horse), while concept 63 (Visual Arts) is negatively related (no sculptures or paintings). We therefore set P 5 = {8, 75}, N 5 = {63}. All concepts were set to equal strength for this topic: m 8,5 = m 63,5 = m 75,5 = 1.</p><p>Topic 24: funny baby. We like to find photos of babies looking funny. The baby should be in the main focus of the photo and be the reason why the photo looks funny. Photos presenting funny things that are not related to the baby are not relevant. Based on the description of this topic and experimentation, we concluded that concepts 86 (Baby), 92 (Funny) and 32 (Portrait) are positively related. We therefore set P 24 = {32, 86, 92}, N 24 = ∅. Based on experimentation the concept Funny was given twice the strength of the other concepts, we set m 32,24 = m 86,24 = 1 and m 92,24 = 2.</p><p>For some topics, instead of explicitly using the score of a group of interrelated concepts we considered introducing a virtual concept with score equal to the maximum of this group of concepts. This slight adaptation of the general rule of (2), enhances its representation capabilities. The following example clarifies this adaptation.</p><p>Topic 32: underexposed photos of animals. We like to find photos of animals that are underexposed. Photos with normal illumination are not relevant. The animal(s) should be more or less in the main focus of the image. Based on the description of this topic and experimentation, we concluded that concepts 44 (Animals), 34 (Underexposed), 72 (Dog), 73 (Cat), 74 (Bird), 75 (Horse), 76 (Fish) and 77 (Insect) are positively related, while concept 35 (Neutral Illumination) is negatively related. The six last specific animal concepts were grouped into a virtual concept, say concept 1001, with score, the maximum of the scores of these six concepts. We then set P 32 = {34, 44, 1001}, N 32 = {35} and m 34,32 = m 44,32 = m 1001,32 = m 35,32 = 1.</p><p>Figure <ref type="figure" coords="8,182.07,238.55,4.98,8.74">4</ref> shows the top 10 retrieved images for topics 5, 24 and 32, along with the Precision@10 for these topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Automated Approach</head><p>Apart from the narrative description, each topic of the concept-based retrieval task was accompanied by a set of 2 to 5 images from the training set which could be considered relevant for the topic. Using these examples images as queries we developed a Query by Example approach to find the most relevant images in the retrieval set. The representation followed the bag-of-words model and was based on the Flickr user tags assigned to each image.</p><p>To generate the feature vectors, we applied the same method as the one used for the annotation task. Thus, each image was represented as a 4000-dimensional feature vector where each feature corresponds to a tag from the training set which was selected by the feature selection method. A value of 1/0 denotes the presence/absence of the tag in the tags accompanying an image.</p><p>To measure the similarity between the vectors representing two images we used the Jaccard similarity coefficient which is defined as the total number of attributes where two vectors A and B both have a value of 1 divided by the the total number of attributes where either A or B have a value of 1.</p><p>Since more than one images where given as examples for each topic, we added their feature vectors in order to form a single query vector. This approach was found to work well in comparison to other approaches, such as taking only one of the example images as query or measuring the similarity between a retrieval image and each example image separately and then returning the images from the retrieval set with the largest similarity score to any of the queries. We attribute this to the fact that by adding the feature vectors, a better representation of the topic of interest was created which could not be possible if only one image (with possibly noisy or very few tags) was considered.</p><p>As in the manual approach, we submitted two runs, one returning the 250 and one the 1000 most similar images from the retrieval set (in descending similarity order).</p><p>Figure <ref type="figure" coords="8,181.01,616.44,4.98,8.74">5</ref> shows the top 10 retrieved images, along with the Precision@10 for the following topics:</p><p>-Topic 10: single person playing a musical instrument. We like to find pictures (no paintings) of a person playing a musical instrument. The person can be on stage, off stage, inside or outside, sitting or standing, but should be alone on the photo. It is enough if not the whole person or instrument is shown as long as the person and the instrument are clearly recognizable. -Topic 12: snowy winter landscaper. We like to find pictures (photos or drawings) of white winter landscapes with trees. The landscape should not contain human-made objects e.g. houses, cars and persons. Only snow on the top of a mountain is not relevant, the landscape has to be fully covered in (at least light) snow. -Topic 30: cute toys arranged to a still-life. We like to find photos of toys arranged to a still-life. These toys should look cute in the arrangement. Simple photos of a collection of toys e.g. in a shop are not relevant.</p><p>We see that the 10 retrieved images for topic 30 are better than those of topics 12 and 10. This can be explained by noticing that topic 12 is a difficult one, while the tags of the example images for topic 10 are not very descriptive/informative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We here briefly present our results, as well as our relative performance compared to other groups and submissions. Results for all groups, as well as more details on the data setup and evaluation measures can be found in <ref type="bibr" coords="9,396.48,372.78,9.96,8.74" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Annotation Task</head><p>The official results of our runs are illustrated in Table <ref type="table" coords="9,387.42,429.72,3.87,8.74" target="#tab_3">2</ref>. We notice that in terms of MIAP, the textual model is slightly better than the visual, while for the other two measures, the visual model is much better than the textual. Among the multi-modal variations, we notice that averaging works better than arbitrating, and as expected using all descriptors is better than using just the RGB-SIFT one. In addition, we notice that the multi-modal approach significantly improves over the MIAP of the visual and textual approaches, while it slightly decreases/increases the performance of the visual model in the two examplebased measures. This may partly be due to the fact that we performed model selection based on MAP. Table <ref type="table" coords="10,177.99,118.99,4.98,8.74" target="#tab_4">3</ref> shows the rank of our best result compared to the best results of other groups and compared to all submissions. We did quite good in terms of textual information, but quite bad in terms of visual information, leading to an overall average performance. Lack of computer vision expertise in our group may be a reason for not being able to get results out of the visual information. Among the three evaluation measures, we notice that overall we did better in terms of MIAP, slightly worse in terms of F-measure, and even worse in terms of SR-Precision. The fact that model selection was performed based on MAP definitely played a role for this result. In this task, participating systems were evaluated using the following measures: Mean Average Precision (MAP), Precision@10, Precision@20, Precision@100 and R-Precision.</p><p>The official results of our runs are illustrated in Table <ref type="table" coords="10,401.70,464.02,3.87,8.74" target="#tab_5">4</ref>. We first notice that the first 5 runs, which retrieved 1000 images, lead to better results in terms of MAP and R-Precision compared to the last 5 runs, which retrieved 250 images. Obviously, in terms of Precision@10, Precision@20 and Precision@100, the results are equal. Among the manual runs, we notice that the visual models perform quite bad. We hypothesize that a lot of concepts that favor textual rather than visual models, as discussed in Sect. 2, appear in most of the topics. The textual and multi-modal models perform best, with the Multi-Modal-Avg model having the best result in 3 out of the 5 measures.</p><p>The automated approach performs slightly better than the visual model of the manual approach, but still much worse than the textual and multi-modal manual approaches. As expected, the knowledge that is provided by a human can clearly lead to better results compared to a fully automated process. However, this is not true across all topics, as can be seen in Table <ref type="table" coords="10,383.23,620.25,3.87,8.74" target="#tab_6">5</ref>, which compares the results of the best automated and manual approach for each individual topic. We can see there that the automated approach performs better on 9 topics, while the manual on 31. Table <ref type="table" coords="11,176.20,632.21,4.98,8.74">6</ref> shows the rank of our best result compared to the best results of other groups and compared to all submissions. Both our manual and our automated approach ranked 1st in all evaluation measures. Table <ref type="table" coords="12,164.77,115.91,4.13,7.89">6</ref>. Rank of our best result compared to the best results of other teams and compared to all submissions in the annotation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Configurations</head><p>Team Rank Submission Rank MAP P@10 P@20 P@100 R-Prec MAP P@10 P@20 P@100 R-Prec Our participation to the very interesting photo annotation and concept-based retrieval tasks of CLEF 2011, led to a couple of interesting conclusions. First of all, we found out that we need the collaboration of a computer vision/image processing group to achieve better results. In terms of multi-label learning algorithms, we noticed that binary approaches worked quite well, especially when coupled with the strong Random Forests algorithm and class imbalance issues are taken into account. We also reached to the conclusion, that we should have performed model selection separately for each evaluation measure. We therefore suggest that in future versions of the annotation task, the allowed number of submissions should be equal to the number of evaluation measures multiplied by the number of information types, so that there is space in the official results for models with all kinds of information.</p><p>There is a lot of room for improvements in the future, both in the annotation and the very interesting concept-based retrieval task. In terms of textual information, we intend to investigate the translation of non-English tags. We would also like to investigate other hierarchical late fusion schemes, such as an additional averaging step for the two different visual modalities (Harris-Laplace, Dense Sampling) and more advanced arbitration techniques. Other thresholding approaches for obtaining bipartitions is another interesting direction for future study. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,201.42,310.66,212.52,7.89"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Automatic annotation using Flickr user tags.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="14,134.77,676.20,345.83,7.89;14,134.77,687.18,182.33,7.86"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig.4. Retrieved images for topics 5, 24 and 32 using manual retrieval. Images come from the MIRFLICKR-1M image dataset<ref type="bibr" coords="14,304.81,687.18,9.22,7.86" target="#b0">[1]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,115.91,345.83,99.32"><head>Table 1 .</head><label>1</label><figDesc>Average precision for 10 concepts, half of which suit much better the textual models and half the visual models.</figDesc><table coords="6,196.99,146.69,218.31,68.54"><row><cell cols="3">Concept Textual Visual Concept Textual Visual</cell></row><row><cell>Airplane 0.6942 0.0946</cell><cell>Trees</cell><cell>0.3004 0.5501</cell></row><row><cell>Horse 0.5477 0.0541</cell><cell>Clouds</cell><cell>0.4744 0.6949</cell></row><row><cell>Bird 0.5260 0.1275</cell><cell>Sky</cell><cell>0.6021 0.7945</cell></row><row><cell cols="3">Insect 0.5087 0.1241 Overexposed 0.0183 0.1937</cell></row><row><cell cols="3">Dog 0.6190 0.2406 Big Group 0.1510 0.3245</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,170.36,568.30,274.64,92.34"><head>Table 2 .</head><label>2</label><figDesc>Official results of the MLKD team in the annotation task.</figDesc><table coords="9,175.10,588.13,262.08,72.52"><row><cell>Run Name</cell><cell cols="2">MIAP F-measure SR-Precision</cell></row><row><cell>Textual</cell><cell>0.3256 0.5061</cell><cell>0.6527</cell></row><row><cell>Visual</cell><cell>0.3114 0.5595</cell><cell>0.6981</cell></row><row><cell>Multi-Modal-Avg</cell><cell>0.4016 0.5588</cell><cell>0.6982</cell></row><row><cell cols="2">Multi-Modal-MaxAP-RGBSIFT 0.3489 0.5094</cell><cell>0.6687</cell></row><row><cell>Multi-Modal-MaxAP</cell><cell>0.3589 0.5165</cell><cell>0.6709</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,134.77,247.75,345.82,164.35"><head>Table 3 .</head><label>3</label><figDesc>Rank of our best result compared to the best results of other teams and compared to all submissions in the annotation task.</figDesc><table coords="10,134.77,280.28,311.21,131.83"><row><cell>Approach</cell><cell cols="5">Team Rank MIAP F-Measure SR-Prec MIAP F-Measure SR-Prec Submission Rank</cell></row><row><cell>Visual</cell><cell cols="2">9th/15 5th/15</cell><cell cols="3">9th/15 25th/46 12th/46 17th/46</cell></row><row><cell>Textual</cell><cell>3rd/7</cell><cell>2nd/7</cell><cell>3rd/7 3rd/8</cell><cell>2nd/8</cell><cell>4th/8</cell></row><row><cell cols="3">Multi-modal 5th/10 5th/10</cell><cell cols="3">7th/10 9th/25 7th/25 15th/25</cell></row><row><cell>All</cell><cell cols="5">5th/18 7th/18 10th/18 9th/79 19th/79 31st/79</cell></row><row><cell cols="4">4.2 Concept-Based Retrieval Task</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,136.56,115.91,347.66,156.35"><head>Table 4 .</head><label>4</label><figDesc>Official results of the MLKD team in the concept-based retrieval task.</figDesc><table coords="11,136.56,137.48,347.66,134.79"><row><cell>Run Name</cell><cell>MAP P@10 P@20 P@100 R-Prec</cell></row><row><cell>Manual-Visual-RGBSIFT-1000</cell><cell>0.0361 0.1525 0.1375 0.1080 0.0883</cell></row><row><cell>Automated-Textual-1000</cell><cell>0.0849 0.3000 0.2800 0.2188 0.1530</cell></row><row><cell>Manual-Textual-1000</cell><cell>0.1546 0.4100 0.3838 0.3102 0.2366</cell></row><row><cell cols="2">Manual-Multi-Modal-Avg-RGBSIFT-1000 0.1640 0.3900 0.3700 0.3180 0.2467</cell></row><row><cell cols="2">Manual-Multi-Modal-MaxAP-RGBSIFT-1000 0.1533 0.4175 0.3725 0.2980 0.2332</cell></row><row><cell>Manual-Visual-RGBSIFT-250</cell><cell>0.0295 0.1525 0.1375 0.1080 0.0863</cell></row><row><cell>Automated-Textual-250</cell><cell>0.0708 0.3000 0.2800 0.2188 0.1486</cell></row><row><cell>Manual-Textual-250</cell><cell>0.1328 0.4100 0.3838 0.3102 0.2298</cell></row><row><cell>Manual-Multi-Modal-Avg-RGBSIFT-250</cell><cell>0.1346 0.3900 0.3700 0.3180 0.2397</cell></row><row><cell cols="2">Manual-Multi-Modal-MaxAP-RGBSIFT-250 0.1312 0.4175 0.3725 0.2980 0.2260</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,134.77,304.34,344.85,283.38"><head>Table 5 .</head><label>5</label><figDesc>Comparison of AP for each topic between automated and manual approach</figDesc><table coords="11,199.19,325.91,213.90,261.81"><row><cell cols="6">Topic Automated Manual Topic Automated Manual</cell></row><row><cell>1</cell><cell>0.235</cell><cell>0.2201</cell><cell>21</cell><cell cols="2">0.0799 0.0312</cell></row><row><cell>2</cell><cell cols="3">0.0294 0.1518 22</cell><cell>0</cell><cell>0.1018</cell></row><row><cell>3</cell><cell cols="2">0.0893 0.0613</cell><cell>23</cell><cell cols="2">0.0405 0.0617</cell></row><row><cell>4</cell><cell>0.257</cell><cell cols="2">0.3701 24</cell><cell cols="2">0.0231 0.1226</cell></row><row><cell>5</cell><cell cols="3">0.0011 0.5478 25</cell><cell>0.009</cell><cell>0.1691</cell></row><row><cell>6</cell><cell>0.12</cell><cell cols="2">0.3574 26</cell><cell cols="2">0.0027 0.0056</cell></row><row><cell>7</cell><cell cols="3">0.0142 0.2164 27</cell><cell cols="2">0.0477 0.1311</cell></row><row><cell>8</cell><cell cols="3">0.0864 0.0879 28</cell><cell cols="2">0.0123 0.1315</cell></row><row><cell>9</cell><cell cols="3">0.0001 0.1143 29</cell><cell>0.0232</cell><cell>0.118</cell></row><row><cell>10</cell><cell cols="3">0.1618 0.2528 30</cell><cell>0.135</cell><cell>0.0378</cell></row><row><cell>11</cell><cell cols="3">0.1393 0.3133 31</cell><cell cols="2">0.0794 0.1535</cell></row><row><cell>12</cell><cell cols="3">0.0519 0.0734 32</cell><cell cols="2">0.0221 0.1135</cell></row><row><cell>13</cell><cell cols="3">0.0275 0.1516 33</cell><cell>0.0343</cell><cell>0.434</cell></row><row><cell>14</cell><cell cols="3">0.0087 0.0968 34</cell><cell cols="2">0.4464 0.4341</cell></row><row><cell>15</cell><cell cols="3">0.0455 0.3327 35</cell><cell cols="2">0.3065 0.3685</cell></row><row><cell>16</cell><cell cols="3">0.0711 0.0715 36</cell><cell cols="2">0.0001 0.1426</cell></row><row><cell>17</cell><cell cols="3">0.0349 0.0401 37</cell><cell cols="2">0.2232 0.0207</cell></row><row><cell>18</cell><cell cols="3">0.0011 0.0044 38</cell><cell cols="2">0.2431 0.1477</cell></row><row><cell>19</cell><cell cols="3">0.0379 0.0691 39</cell><cell cols="2">0.0226 0.0153</cell></row><row><cell>20</cell><cell cols="2">0.1837 0.1168</cell><cell>40</cell><cell cols="2">0.0508 0.1703</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">MAP 0.0849 0.1640</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.44,137.01,7.47"><p>http://www.clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,656.80,208.16,8.12"><p>Available from http://www.colordescriptors.com</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to acknowledge the student travel support from <rs type="funder">EU FP7</rs> under grant agreement no <rs type="grantNumber">216444</rs> (<rs type="funder">PetaMedia Network of Excellence)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_PpYfbjz">
					<idno type="grant-number">216444</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,138.35,623.92,342.24,7.86;12,146.91,634.88,333.68,7.86;12,146.91,645.84,333.68,7.86;12,146.91,656.80,177.92,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,314.42,623.92,166.17,7.86;12,146.91,634.88,216.04,7.86">New trends and ideas in visual concept detection: The mir flickr retrieval evaluation initiative</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,383.35,634.88,97.24,7.86;12,146.91,645.84,313.80,7.86">MIR &apos;10: Proceedings of the 2010 ACM International Conference on Multimedia Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,119.67,342.24,7.86;13,146.91,130.63,333.68,7.86;13,146.91,141.59,186.67,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,368.74,119.67,111.85,7.86;13,146.91,130.63,166.91,7.86">Obtaining bipartitions from score vectors for multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sakkas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,320.95,130.63,159.64,7.86;13,146.91,141.59,112.43,7.86">Tools with Artificial Intelligence, IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,152.55,342.24,7.86;13,146.91,163.51,275.00,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,323.44,152.55,157.15,7.86;13,146.91,163.51,109.87,7.86">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,263.77,163.51,83.91,7.86">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,174.47,342.24,7.86;13,146.91,185.43,250.24,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,296.61,174.47,183.98,7.86;13,146.91,185.43,80.87,7.86">The clef 2011 photo annotation and conceptbased retrieval tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liebetrau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,248.74,185.43,119.75,7.86">Working Notes of CLEF 2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,196.39,342.24,7.86;13,146.91,207.34,333.68,7.86;13,146.91,218.30,104.45,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,352.05,196.39,128.54,7.86;13,146.91,207.34,49.79,7.86">Classifier chains for multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,219.53,207.34,261.07,7.86">Proc. 20th European Conference on Machine Learning (ECML</title>
		<meeting>20th European Conference on Machine Learning (ECML</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="254" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,229.26,342.24,7.86;13,146.91,240.22,333.68,7.86;13,146.91,251.18,210.08,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,294.08,229.26,186.51,7.86;13,146.91,240.22,153.58,7.86">University of Amsterdam at the Visual Concept Detection and Annotation Tasks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,308.75,240.22,133.94,7.86;13,146.91,251.18,46.42,7.86">The Information Retrieval Series</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="343" to="358" />
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>ImageCLEF</note>
</biblStruct>

<biblStruct coords="13,138.35,262.14,342.24,7.86;13,146.91,273.10,333.67,7.86;13,146.91,284.06,145.41,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,353.55,262.14,127.05,7.86;13,146.91,273.10,112.85,7.86">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,266.67,273.10,213.92,7.86;13,146.91,284.06,45.57,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,295.02,342.24,7.86;13,146.91,305.98,333.68,7.86;13,146.91,316.93,140.84,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,314.13,295.02,94.39,7.86">Mining multi-label data</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Katakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,219.32,305.98,200.93,7.86">Data Mining and Knowledge Discovery Handbook</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Maimon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="667" to="685" />
		</imprint>
	</monogr>
	<note>2nd edn.</note>
</biblStruct>

<biblStruct coords="13,138.35,327.89,342.24,7.86;13,146.91,338.85,333.68,7.86;13,146.91,349.81,102.78,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,420.33,327.89,60.26,7.86;13,146.91,338.85,121.39,7.86">Mulan: A java library for multi-label learning</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vilcek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,275.16,338.85,190.67,7.86">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2411" to="2414" />
			<date type="published" when="2011-07-12">July 12 2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
