<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.20,115.96,332.95,12.62;1,241.98,133.89,131.39,12.62">University of Padua at MusiCLEF 2011: Music Identification Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,183.75,171.86,86.90,8.74"><forename type="first">Emanuele</forename><forename type="middle">Di</forename><surname>Buccio</surname></persName>
							<email>emanuele.dibuccio@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.54,171.86,80.26,8.74"><forename type="first">Nicola</forename><surname>Montecchio</surname></persName>
							<email>nicola.montecchio@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,381.49,171.86,50.12,8.74"><forename type="first">Nicola</forename><surname>Orio</surname></persName>
							<email>nicola.orio@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.20,115.96,332.95,12.62;1,241.98,133.89,131.39,12.62">University of Padua at MusiCLEF 2011: Music Identification Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7D3B7486DBCC198D45C055AC803EA143</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>System Research Group of the University of Padua to the Music Identification task of the MusiCLEF Laboratory in CLEF 2011. The system under evaluation is FALCON, an open-source engine for content-based cover song identification written in Java that applies classic techniques derived from textual Information Retrieval to music identification. The obtained results show how such approach yields satisfying results using little computational resources.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic identification of music documents has become an essential component of many popular web services. Audio fingerprinting techniques are widely used in order to identify copies of a recording, which can differ from the original because of data compression, A/D conversion or environmental noises <ref type="bibr" coords="1,432.64,419.98,9.96,8.74" target="#b0">[1]</ref>. In most major video sharing websites, the background music of user generated videos is automatically identified in order to either remove the video (for copyright issues) or suggest where to purchase the original music (advertising) <ref type="foot" coords="1,400.85,454.27,3.97,6.12" target="#foot_0">1</ref> .</p><p>Audio fingerprint approaches aim at the identification of a particular performance/recording rather than work : even alternate takes of a composition are considered different items instead of being regarded as different instances of the same piece. This assumption imposes significant restrictions to the range of possible differences between copies of the same recording, allowing audio fingerprinting techniques to be particularly efficient.</p><p>Aside from audio fingerprinting techniques, past research on music identification has focused, for obvious commercial reasons, on popular music; the research field is commonly referred to as cover song identification, a more general name for this research field being version identification <ref type="bibr" coords="1,356.61,576.00,9.96,8.74" target="#b1">[2]</ref>. The problem however is also of interest for other genres: in classical music there is especially a vast number of interpretations of the same work and version identification technology can be beneficial for many music libraries and archives that aim at the preservation and dissemination of classical music. For a comprehensive review of previous approaches to the version identification problem the reader can refer to <ref type="bibr" coords="1,438.51,635.78,9.96,8.74" target="#b1">[2]</ref>, where the author provides a review based on a functional block decomposition of previously proposed systems.</p><p>The experience gained in the development of text search engines shows that in most cases simple and efficient techniques can be generally employed in various retrieval tasks, with results that are often comparable with more complex and less efficient approaches. Following this idea, we developed a music identification engine named FALCON<ref type="foot" coords="2,239.97,189.15,3.97,6.12" target="#foot_1">2</ref> . The fact that our software is implemented on top of Apache Lucene <ref type="foot" coords="2,200.21,201.11,3.97,6.12" target="#foot_2">3</ref> , an open source text search engine, substantiates the claim that the problem of music identification can be modelled as a more general retrieval task. In this paper we study its adaptability to classical music, making use of the dataset prepared for the Music Identification task of the MusiCLEF 2011 benchmarking activity. The collection is constituted by circa 7 thousand recordings for which the MusiCLEF Laboratory organizers provided precomputed descriptors. Testing FALCON on this test collection has allowed us to investigate the scalability of our proposed approach, thus extending previous investigations carried out on popular music <ref type="bibr" coords="2,231.15,298.32,9.96,8.74" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Our approach is based on a two-level bag-of-features hierarchy; the input descriptors (chroma features) are transformed into hashes which are subsequently grouped into a "set of hash sets" representation for each recording. The cardinality of set intersection is adopted as a similarity measure. The methodology, described in detail in <ref type="bibr" coords="2,229.77,401.95,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="2,240.29,401.95,7.01,8.74" target="#b3">4]</ref>, can be summarized as follows.</p><p>audio content analysis -this step consists in: (i) chroma feature extraction from audio waveforms, (ii) key-finding -the most probable keys of a recording are estimated in order to preserve transposition invariance -and (iii) hashing of the transposed chroma features into a sequence of integer values which form the output of this phase. indexing -each sequence of hashes is segmented into a set of possible overlapping segments. Hashes are interpreted as terms in a textual document and segments as passages constituted by sets of terms. This representation can be easily stored in an inverted index: the set of all the distinct hashes appearing in the sequences obtained from the recordings in the collection constitutes the index vocabulary; each item in the posting list is associated to an hash and retains information about the frequency of occurrence of the hash in a specific segment. querying -the similarity between a query Q and a recording D is computed as</p><formula xml:id="formula_0" coords="2,196.08,604.72,284.51,26.88">S(Q, D) = |Q| q∈Q max d∈D t∈q∩d min hf(t, d) |d| , hf(t, q) |q|<label>(1)</label></formula><p>where q and d denote two segments of a query and a collection recording, respectively of length (number of hashes) |q|, |d|; similarly, |Q|, |D| denote the number of distinct segments for Q, D and hf(t, d) denotes the frequency of the hash t in the segment d. Formula 1 can be interpreted as follows: the first step (inner summation) computes local similarity between segments as the (normalized) number of terms they have in common; the second step aggregates the contributions of all the query segments, by computing the geometric mean of the best local similarities.</p><p>The procedure is repeated multiple times according to the most probable keys detected in the audio analysis step; for each recording in the collection, the highest similarity value is preserved.</p><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Test Collection and Setup</head><p>The test collection comprises 6679 recordings of classical music works, which add up to more than 572 hours of music. Of these recordings, 2,671 are associated to works that are represented at least twice in the data base, forming 945 cover sets <ref type="foot" coords="3,151.62,372.21,3.97,6.12" target="#foot_3">4</ref> . The audio descriptors were extracted using the MIRToolbox package <ref type="bibr" coords="3,467.31,373.79,9.96,8.74" target="#b4">[5]</ref>. More details on the collection, and in general on the MusiClef campaign, are available in <ref type="bibr" coords="3,187.34,397.70,9.96,8.74" target="#b5">[6]</ref>.</p><p>All experiments were repeated twice, one time using a key-finding algorithm to preserve independency to transposition, and one time without such strategy. The other parameters were set according to the values reported in <ref type="bibr" coords="3,435.29,435.04,9.96,8.74" target="#b2">[3]</ref>, except the segment overlap which was set to 50% the length of each segment (15s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Identification Accuracy and Computational Load</head><p>In FALCON, the trade-off between accuracy and speed of retrieval privileges the latter, as the architecture was designed with large collections in mind. It is therefore important to measure the computational resources required by the software. All experiments were run on a machine with a 3.4 GHz dual-core processor (4 logical cores), 24 GB of RAM and a 7200 RPM hard disk.</p><p>Table <ref type="table" coords="3,176.19,572.85,4.98,8.74" target="#tab_0">1</ref> shows the results of our experimentation. As can be seen, the software can index about 3.5 hours of music per second and is able to perform a typical query in under 3 seconds. The average query time is different because the keyfinding algorithm repeats a query multiple times (in this case 3) in parallel; each thread is completely independent of the others. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Concluding Remarks</head><p>This paper reports our contribution to the Music Identification task of the MusiCLEF Laboratory in CLEF2011. Our experimental results show how a simple approach based on text retrieval techniques can yield satisfying results using little computational resources. These results suggest that the methodology implemented in FALCON can be adopted as the first of a two-step methodology. The identification performed by FALCON would aim at providing candidate versions of the query recording at the top k rank positions. Then identification could be refined by means of more sophisticated but also more resource consuming music alignment techniques, that when exploited in isolation and on the entire collection are hardly scalable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,161.70,127.36,291.96,86.97"><head>Table 1 :</head><label>1</label><figDesc>Accuracy and timing for 667 queries over 6679 recordings.</figDesc><table coords="4,178.72,145.87,254.85,68.46"><row><cell></cell><cell>no transposition</cell><cell>with transposition</cell></row><row><cell>total indexing time</cell><cell>163s</cell><cell>166s</cell></row><row><cell>average query time</cell><cell>1.79s</cell><cell>2.32s</cell></row><row><cell>MAP</cell><cell>0.6754</cell><cell>0.6826</cell></row><row><cell>MRR</cell><cell>0.7617</cell><cell>0.7771</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,656.80,273.92,8.12"><p>See, for instance, http://www.youtube.com/t/copyright_my_video</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,646.48,141.22,7.47"><p>http://ims.dei.unipd.it/falcon</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.73,657.44,117.68,7.47"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,144.73,645.84,335.86,7.86;3,144.73,656.80,58.94,7.86"><p>The term "cover set" is commonly used to define a set of recordings of the same piece of music.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,138.35,434.20,342.24,7.86;4,146.91,445.13,295.16,7.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,343.96,434.20,132.82,7.86">A review of audio fingerprinting</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Batlle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kalker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Haitsma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,146.91,445.16,173.93,7.86">Journal of VLSI Signal Processing Systems</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="271" to="284" />
			<date type="published" when="2005-11">November 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,456.12,342.24,7.86;4,146.91,467.07,310.47,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="4,188.48,456.12,292.11,7.86;4,146.91,467.07,71.51,7.86">Identification of versions of the same musical composition by processing audio descriptions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<pubPlace>Barcelona</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universitat Pompeu Fabra</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="4,138.35,478.03,342.24,7.86;4,146.91,488.99,333.68,7.86;4,146.91,499.95,119.54,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,313.31,478.03,148.04,7.86">A scalable cover identification engine</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Di Buccio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Montecchio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Orio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,146.91,488.99,267.05,7.86">Proceedings of the international conference on Multimedia. MM &apos;10</title>
		<meeting>the international conference on Multimedia. MM &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1143" to="1146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,510.91,342.24,7.86;4,146.91,521.87,333.68,7.86;4,146.91,532.83,183.58,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,235.04,510.91,245.55,7.86;4,146.91,521.87,76.98,7.86">A music identification system based on chroma indexing and statistical modeling</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Miotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Orio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,246.75,521.87,233.85,7.86;4,146.91,532.83,85.63,7.86">Proceedings of the 9th International Conference on Music Information Retrieval</title>
		<meeting>the 9th International Conference on Music Information Retrieval</meeting>
		<imprint>
			<publisher>ISMIR</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,543.79,342.24,7.86;4,146.91,554.75,333.68,7.86;4,146.91,565.70,102.18,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,264.47,543.79,216.12,7.86;4,146.91,554.75,20.48,7.86">A matlab toolbox for musical feature extraction from audio</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Lartillot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Toiviainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,187.24,554.75,289.65,7.86">Proceedings of the 10th International Conference on Digital Audio Effects</title>
		<meeting>the 10th International Conference on Digital Audio Effects<address><addrLine>Bordeaux, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,576.66,342.24,7.86;4,146.91,587.62,333.68,7.86;4,146.91,598.58,333.68,7.86;4,146.91,609.54,55.67,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,444.70,576.66,35.89,7.86;4,146.91,587.62,262.00,7.86">Musiclef: A benchmark activity in multimodal music information retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Orio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rizo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Lartillot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Miotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Montecchio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,432.52,587.62,48.08,7.86;4,146.91,598.58,329.49,7.86">Proceedings of the 12th International Conference on Music Information Retrieval (to appear)</title>
		<meeting>the 12th International Conference on Music Information Retrieval (to appear)</meeting>
		<imprint>
			<publisher>ISMIR</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
