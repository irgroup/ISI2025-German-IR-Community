<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.80,140.92,300.33,13.79">Music Identification using Chroma Features</title>
				<funder ref="#_8EzTusj">
					<orgName type="full">Sector Operational Program for Human Resources Development</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,224.76,179.67,49.15,8.37"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<addrLine>General Berthelot, 16</addrLine>
									<postCode>700483</postCode>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,279.84,179.67,45.76,8.37"><forename type="first">Andrei</forename><surname>Rusu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<addrLine>General Berthelot, 16</addrLine>
									<postCode>700483</postCode>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,332.28,179.67,65.03,8.37"><forename type="first">Alexandra</forename><surname>Leahu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<addrLine>General Berthelot, 16</addrLine>
									<postCode>700483</postCode>
									<settlement>Iasi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.80,140.92,300.33,13.79">Music Identification using Chroma Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AF7F786AEBE30D50C823C23F4003313F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper some specific issues related to Music Information Retrieval (MIR) are presented. First part is dedicated to introductive notions from this field in the field, and second part is dedicated to giving details about the system we built for MusiCLEF 2011. An important aspect related to our work is related to searching metrics able to allow us to identify an audio recording in a database with existing songs. Our system uses chroma features associated to a song and apply on it many types of metrics. Some of these metrics wants to find more accurate song of which part of that fragment belong to and other metrics are used to enable us to do this as quickly as possible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the last years, we have seen an explosion in terms of the number of existing audio files on the Internet (music files, video files, audio streaming, etc.). Music Information Retrieval (MIR) is devoted to the study of methodologies for automatic music access. Today, although MIR usually focuses on content-based approaches, music search facilities in commercial systems are still based on simple keyword matching between music tags, with additional exploitation of user profiling and collaborative filtering approaches <ref type="foot" coords="1,269.52,458.60,3.04,5.45" target="#foot_0">1</ref> .</p><p>As the organizers say, MusicCLEF 2011 aims at promoting the development of new methodologies that allow us direct access to audio files, combined with the use of knowledge from the database associated with audio files. For that, organizers allowed access to information automatically extracted from audio files, and additional they allowed access to contextual information provided by users who have had access to their tags or comments and reviews associated with them.</p><p>In edition 2011, the organizers propose two tasks: Content and Context-based Music Retrieval and Music Identification 2 . For this exercise they offer to participants a test collection of about 10,000 songs stored in MP3 format for both tasks. First task, Content and Context-based Music Retrieval is a categorization task and it aims to combine information automatically extracted about the content with information generated by user. The second task, Music Identification is closed to what MIR meaning, and it has aim to automatically identify an audio recording and to cluster a group of recordings in the same group.</p><p>In the following chapters we present the basic notions used in this paper and the details about our group approach in an attempt to build a system for the second related to Music Identification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Basic notions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sound</head><p>The first definition of sound from the American Heritage Dictionary of the English language is <ref type="bibr" coords="2,188.04,245.43,10.14,8.37" target="#b0">[1]</ref>:</p><p>Vibrations transmitted through an elastic solid or a liquid or gas, with frequencies in the approximate range of 20 to 20,000 hertz, capable of being detected by human organs of hearing. The propagation of sound<ref type="foot" coords="2,246.84,286.67,3.04,5.47" target="#foot_2">3</ref> is a sequence of waves of pressure which propagates through compressible media such as air or water or even solids. During their propagation, waves can be reflected, refracted, or attenuated by the medium. All media have three properties which affect the behavior of sound propagation:</p><p>1. A relationship between density and pressure determines the speed of sound within the medium. 2. The motion of the medium itself (e.g., wind): if the medium is moving, the sound is further transported. 3. The viscosity of the medium determines the rate at which sound is attenuated.</p><p>For many media, such as air or water, attenuation due to viscosity is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pitch</head><p>Pitch <ref type="foot" coords="2,173.76,427.34,3.04,5.33" target="#foot_3">4</ref> is an auditory perceptual property that allows the ordering of sounds on a frequency-related scale <ref type="bibr" coords="2,232.92,440.19,9.94,8.37" target="#b1">[2]</ref>. Pitches are compared as "higher" and "lower" in the sense associated with musical melodies <ref type="bibr" coords="2,272.88,450.99,10.03,8.37" target="#b2">[3]</ref>, which require "sound whose frequency is clear and stable enough to be heard as not noise" <ref type="bibr" coords="2,313.44,461.79,9.94,8.37" target="#b3">[4]</ref>. Pitch is one of the auditory attribute of musical tones, along with duration, loudness, and timbre <ref type="bibr" coords="2,368.28,472.59,9.94,8.37" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Octave</head><p>In music, an octave <ref type="foot" coords="2,231.00,503.06,3.04,5.33" target="#foot_4">5</ref> is the interval between one musical pitch and another with half or double its frequency. The octave relationship is a natural phenomenon that has been referred to as the "basic miracle of music," the use of which is "common in most musical systems." <ref type="bibr" coords="2,213.36,537.51,10.91,8.37" target="#b5">[6]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pitch class</head><p>In music, a pitch class <ref type="foot" coords="2,241.20,567.98,3.04,5.33" target="#foot_5">6</ref> is a set of all pitches that are a whole number of octaves apart, e.g., the pitch class C consists of the Cs in all octaves. "The pitch class C stands for all possible Cs, in whatever octave position." <ref type="bibr" coords="3,330.12,140.79,10.91,8.37" target="#b6">[7]</ref> Psychologists refer to the quality of a pitch as its "chroma".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chroma</head><p>A chroma <ref type="foot" coords="3,192.48,182.06,3.04,5.33" target="#foot_6">7</ref> is an attribute of pitches. Similar, a "pitch class" is a set of all pitches sharing the same chroma. The concept behind chroma is that octaves play a basic role in music perception and composition <ref type="bibr" coords="3,287.04,205.71,9.94,8.37" target="#b7">[8]</ref>. Chroma features have been already used in music retrieval applications <ref type="bibr" coords="3,261.96,216.51,10.03,8.37" target="#b8">[9]</ref>. Thus, the application of chroma features to identification task has been already been proposed for classical <ref type="bibr" coords="3,387.00,227.31,15.59,8.37" target="#b9">[10]</ref> and for pop <ref type="bibr" coords="3,452.52,227.31,15.71,8.37" target="#b10">[11]</ref> music.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Metrics</head><p>For the MusiCLEF 2011, the organizers provide a set of songs which are described by Chroma vectors. In order to extract chroma from mp3 songs, they used the MIRToolbox <ref type="foot" coords="3,192.84,319.04,3.04,5.45" target="#foot_7">8</ref> . Chroma features are considered as pointers to the recordings they belong to, playing the same role of words in textual documents. The information on the time position of chroma features is used to directly access to relevant audio excerpts.</p><p>Chroma features consist of a twelve-element vector with each dimension representing the intensity associated with a particular semitone, regardless of octave. The vector has one single integer value and it is obtained through a hashing function.</p><p>For each song we have a file that contains many chroma vectors that describes the respective mp3 song. We also have a set of fragments of songs that are also described by these vectors.</p><p>An example of this type of vector is: We can see this vector as a point in a n-dimensional space (with n equal to 12). From this point of view we decide to use n-dimensional metrics in order to calculate distance between chroma associated to songs from our database (Alicante fonoteca in our case).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Euclidian Metrics</head><p>First used metric was Euclidean<ref type="foot" coords="3,268.32,563.96,3.04,5.45" target="#foot_8">9</ref> distance. In a n-dimensional space if we have two points ‫‬ = ‫(‬ ଵ , ‫‬ ଶ , … , ‫‬ ) and ‫ݍ‬ = ‫ݍ(‬ ଵ , ‫ݍ‬ ଶ , … , ‫ݍ‬ ) then we can define the Euclidean distance between p and q the following value:</p><formula xml:id="formula_0" coords="4,180.36,140.46,250.50,12.80">‫,(݀‬ ‫)ݍ‬ = ‫,ݍ(݀‬ ‫)‬ = ඥ(‫‬ ଵ -‫ݍ‬ ଵ ) ଶ + ‫(‬ ଶ -‫ݍ‬ ଶ ) ଶ + ⋯ ‫(+‬ -‫ݍ‬ ) ଶ</formula><p>In this case p represents a chroma vector from a song from Alicante fonoteca database (more precisely p represent a line from a file associated to a song), and q represents a chroma vector for a fragment that need to be identified.</p><p>Based on this Euclidian distance we define a global Euclidean distance, like a sum of Euclidean distances corresponding to all consecutive lines from file associated to the song that must be identified and consecutive lines from a song from our database. The sequence of lines, from our song, from database, which has the global Euclidian distance with minimum value, given to us the minimum global Euclidean distance.</p><p>In order to classify a song, we need to calculate all possible minimum global Euclidean distances between it and songs from our database and in the end to select the song with minimum global Euclidian distance. In the end we decide that this song is similar with the song from database with shortest minimum global Euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relative Euclidian Metrics</head><p>This metric is based on Euclidian metrics, but it aims to eliminate similar differences which exist on different octaves in two chromas. Thus, in a n-dimensional space if we have two points ‫‬ = ‫(‬ ଵ , ‫‬ ଶ , … , ‫‬ ) and ‫ݍ‬ = ‫ݍ(‬ ଵ , ‫ݍ‬ ଶ , … , ‫ݍ‬ ) then we define the Relative Euclidean distance between p and q the following value:</p><formula xml:id="formula_1" coords="4,143.64,401.62,324.14,12.99">݀ ‫,(‬ ‫)ݍ‬ = ݀ ‫,ݍ(‬ ‫)‬ = ඥ(‫‬ ଵ -‫ݍ‬ ଵ ) ଶ + ⋯ ‫(+‬ -‫ݍ‬ ) ଶ -(݊ -1) × min ୧ ‫({‬ -‫ݍ‬ ) ଶ }</formula><p>In comparison with above metric, this metric comes to cover cases when a song is written in other gamma. Obviously it is possible that this metric does not always work well, but we want that after tests on training data, to find a way to automatically identify cases where it would be preferable to use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Scalability Metrics</head><p>Because it is possible to have very many songs in our database is possible like above metrics not be effective, in terms of scalability. From this reason we try to find a way to reduce the search space, making an obvious reduction in quality of final solutions. For this case instead to consider all lines from a file with chromas associated to a song, we consider the following values for all octaves from these files: minimum, average and maximum value. In this way instead to consider around 30,000 lines for a song from our database and instead to consider around 5,000 lines for a song that must be identified, we consider only three lines of values for both types of files. Thus for two songs s 1 and s 2 there are two metrics available based on Euclidian metrics from above:</p><formula xml:id="formula_2" coords="4,180.36,627.58,250.84,13.11">݀ ௦ ‫ݏ(‬ ଵ , ‫ݏ‬ ଶ ) = ‫ݏ(݀‬ ଵ , ‫ݏ‬ ଶ ) + ‫ݏ(݀‬ ଵ௩ , ‫ݏ‬ ଶ௩ ) + ‫ݏ(݀‬ ଵ௫ , ‫ݏ‬ ଶ௫ ) and ݀ ௦ ‫ݏ(‬ ଵ , ‫ݏ‬ ଶ ) = ݀ ‫ݏ(‬ ଵ , ‫ݏ‬ ଶ ) + ݀ ‫ݏ(‬ ଵ௩ , ‫ݏ‬ ଶ௩ ) + ݀ ‫ݏ(‬ ଵ௫ , ‫ݏ‬ ଶ௫ )</formula><p>After we calculate all values for scalability metrics between a song that must be identified and all songs from our database, we decide to classify this song using the minimum value ݀ ௦ or ݀ ௦ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Combined Metrics</head><p>These metrics start from scalability metrics, but instead to keep only the minimum value, we keep first 10 lowest values. On these 10 lowest values we apply corresponding Euclidian or Relative Euclidian metrics and we identify the most suitable file. In this way we perform a faster identification using the scalability metrics from 3.3, and then using the basic metrics from 3.1 and 3.2 we refine the search and in the end we obtain a better solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Another idea that we had was to use clustering algorithms in order to identify similarities between songs, and similarities between different parts of the same song. Thus we first tried to accomplish that using some Java machine learning libraries that are available (such as Java-ML<ref type="foot" coords="5,263.40,401.00,6.04,5.45" target="#foot_9">10</ref> ), we soon observed that much faster results can be computed in MATLAB <ref type="foot" coords="5,231.96,411.80,6.16,5.45" target="#foot_10">11</ref> .</p><p>So far, we used the k-means clustering method with the Euclidean distance measure (and also implemented some efficient methods to compute the global Euclidean distance between n points), and we partitioned each set of chroma vectors into ten clusters. The result is a vector representing each song and containing the cluster indices of each point. Also, in addition to that, we made use of a vector containing the within-cluster sums of point-to-centroid distances. Of course, this raw data would be of no use unless processed in the right way, and since the results described so far were computed for individual songs, is natural that for finding similarities between different songs we have to use this data, but apply metaclustering algorithms on it. Also, we are currently working on a more precise classifier which will enable us the make even more accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Instead of conclusions</head><p>Until now, our current work was related to find metrics that allow us to identify a song and to classify it accordingly to chroma features associated to it. The results obtained until now are promising, but in the next period we must use the training data and to identify thresholds, that will allow us to say that a song is in our database or not.</p><p>Another big problem until now was related to scalability of our work, and from this reason we try to find suitable metrics that can be applied in real time. This problem will give us another future direction in our work related to combination of Euclidian metrics with scalability metrics.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,148.32,635.56,38.05,7.56"><p>MusiCLEF</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2011" xml:id="foot_1" coords="1,207.36,635.56,221.83,7.56;1,143.28,643.47,2.81,5.04;1,148.32,645.28,265.27,7.56"><p>Overview: http://ims.dei.unipd.it/websites/MusiCLEF/index.html 2 MusiCLEF 2011 Tasks: http://ims.dei.unipd.it/websites/MusiCLEF/tasks.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,148.32,616.12,240.33,7.56"><p>The propagation of sound: http://www.jhu.edu/virtlab/ray/acoustic.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,148.32,625.84,191.42,7.56"><p>Pitch: http://en.wikipedia.org/wiki/Pitch_%28music%29</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,148.32,635.56,149.19,7.56"><p>Octave: http://en.wikipedia.org/wiki/Octave</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="2,148.32,645.28,175.24,7.56"><p>Pitch class: http://en.wikipedia.org/wiki/Pitch_class</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="3,148.32,625.84,166.48,7.56"><p>Chroma: http://en.wikipedia.org/wiki/Pitch_class</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="3,148.32,635.56,319.70,7.56"><p>MIRToolbox: https://www.jyu.fi/hum/laitokset/musiikki/en/research/coe/materials/mirtoolbox</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="3,148.32,645.28,230.67,7.56"><p>Euclidean distance: http://en.wikipedia.org/wiki/Euclidean_distance</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="5,151.20,635.56,214.27,7.56"><p>Java Machine Learning Library: http://java-ml.sourceforge.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="5,151.20,645.28,230.83,7.56"><p>MATLAB: http://www.mathworks.com/products/matlab/index.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The research presented in this paper was funded by the <rs type="funder">Sector Operational Program for Human Resources Development</rs> through the project "<rs type="projectName">Development of the innovation capacity and increasing of the research impact through post</rs><rs type="programName">-doctoral programs</rs>" <rs type="grantNumber">POSDRU/89/1.5/S/49944</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_8EzTusj">
					<idno type="grant-number">POSDRU/89/1.5/S/49944</idno>
					<orgName type="project" subtype="full">Development of the innovation capacity and increasing of the research impact through post</orgName>
					<orgName type="program" subtype="full">-doctoral programs</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,146.37,331.84,321.89,7.56;6,160.20,341.56,308.11,7.56;6,160.20,351.28,22.49,7.56" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,160.20,331.84,267.41,7.56">The American Heritage Dictionary of the English Language, Fourth Edition</title>
		<imprint>
			<date type="published" when="2000-06-25">2000. June 25, 2008. May 20, (2010</date>
			<publisher>Houghton Mifflin Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,146.38,361.12,321.94,7.56;6,160.20,370.84,103.13,7.56" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,241.56,361.12,171.58,7.56">Signal processing methods for music transcription</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Klapuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Davy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,146.38,380.56,301.52,7.56" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,195.48,380.56,123.06,7.56">Pitch: Neural Coding and Perception</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Plack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,146.38,390.16,321.81,7.56;6,160.20,400.00,128.45,7.56" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,234.72,390.16,115.35,7.56">The Harvard Dictionary of Music</title>
		<editor>Don Michael, R.</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Harvard University Press</publisher>
			<biblScope unit="page">499</biblScope>
		</imprint>
	</monogr>
	<note>4 ed</note>
</biblStruct>

<biblStruct coords="6,146.38,409.72,321.89,7.56;6,160.20,419.44,308.07,7.56;6,160.20,429.16,215.09,7.56" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,318.48,409.72,149.78,7.56;6,160.20,419.44,50.40,7.56">The Perception of Family and Register in Musical Tones</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gaudrain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Walters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,447.00,419.44,21.27,7.56;6,160.20,429.16,34.65,7.56">Music Perception</title>
		<editor>
			<persName><forename type="first">Mari Riess</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Richard</forename><forename type="middle">R</forename><surname>Fay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Arthur</forename><forename type="middle">N</forename><surname>Popper</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="37" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,146.38,439.00,321.93,7.56;6,160.20,448.60,98.93,7.56" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,202.80,439.00,229.66,7.56">Perspectives in Music Theory: An Historical-Analytical Approach</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cooper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,146.38,458.32,321.89,7.56;6,160.20,468.04,177.65,7.56" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Whitall</surname></persName>
		</author>
		<title level="m" coord="6,203.64,458.32,141.93,7.56">The Cambridge Introduction to Serialism</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page">276</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,146.38,477.88,321.89,7.56;6,160.20,487.60,252.41,7.56" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,302.52,477.88,165.74,7.56;6,160.20,487.60,103.96,7.56">Audio Thumbnailing of Popular Music Using Chroma-based Representations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Bartschand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">H</forename><surname>Wakefield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,269.88,487.60,114.13,7.56">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,146.38,497.20,321.89,7.56;6,160.20,506.92,308.06,7.56;6,160.20,516.76,103.37,7.56" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,239.52,497.20,228.74,7.56;6,160.20,506.92,71.78,7.56">A Music Identification System Based on Chroma Indexing and Statistical Modeling</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Miotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Orio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,243.36,506.92,224.90,7.56;6,160.20,516.76,29.66,7.56">International Symposium/Conference on Music Information Retrieval</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,150.21,526.48,317.98,7.56;6,160.20,536.20,308.11,7.56;6,160.20,545.92,69.53,7.56" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,303.60,526.48,164.59,7.56;6,160.20,536.20,27.16,7.56">Audio Matching Via Chroma-based Statistical Features</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Kurthand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Clausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,194.76,536.20,270.23,7.56">Proceedings of the International Conference of Music Information Retrieval</title>
		<meeting>the International Conference of Music Information Retrieval<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,150.21,555.64,317.98,7.56;6,160.20,565.36,307.99,7.56;6,160.20,575.08,145.25,7.56" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,315.24,555.64,152.95,7.56;6,160.20,565.36,70.07,7.56">Polyphonic Audio Matching and Alignment for Music Retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Dannenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,238.20,565.36,229.99,7.56;6,160.20,575.08,116.98,7.56">Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</title>
		<meeting>the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
