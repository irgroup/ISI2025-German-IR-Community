<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,186.24,115.83,242.85,12.88;1,155.16,133.83,304.93,12.88;1,223.44,153.61,168.58,10.74">Multilingual Vandalism Detection using Language-Independent &amp; Ex Post Facto Evidence Notebook for PAN at CLEF 2011</title>
				<funder ref="#_zNPknyh">
					<orgName type="full">ONR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,245.16,189.83,66.01,8.91"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>West</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer and Information Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,330.72,189.83,39.58,8.91"><forename type="first">Insup</forename><surname>Lee</surname></persName>
							<email>lee@cis.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer and Information Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,186.24,115.83,242.85,12.88;1,155.16,133.83,304.93,12.88;1,223.44,153.61,168.58,10.74">Multilingual Vandalism Detection using Language-Independent &amp; Ex Post Facto Evidence Notebook for PAN at CLEF 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">92BF0B859650B1E761A226E2C0DA7348</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is much literature on Wikipedia vandalism detection. However, this writing addresses two facets given little treatment to date. First, prior efforts emphasize zero-delay detection, classifying edits the moment they are made. If classification can be delayed (e.g., compiling offline distributions), it is possible to leverage ex post facto evidence. This work describes/evaluates several features of this type, which we find to be overwhelmingly strong vandalism indicators.</p><p>Second, English Wikipedia has been the primary test-bed for research. Yet, Wikipedia has 200+ language editions and use of localized features impairs portability. This work implements an extensive set of language-independent indicators and evaluates them using three corpora (German, English, Spanish). The work then extends to include language-specific signals. Quantifying their performance benefit, we find that such features can moderately increase classifier accuracy, but significant effort and language fluency are required to capture this utility.</p><p>Aside from these novel aspects, this effort also broadly addresses the task, implementing 65 total features. Evaluation produces 0.840 PR-AUC on the zerodelay task and 0.906 PR-AUC with ex post facto evidence (averaging languages). Performance matches the state-of-the-art (English), sets novel baselines (German, Spanish), and is validated by a first-place finish over the 2011 PAN-CLEF test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unconstructive or ill-intentioned edits (i.e., vandalism) on Wikipedia erode the encyclopedia's reputation and waste the utility of those who must locate/remove the damage. Moreover, while Wikipedia is the focus of this work, these are issues that affect all wiki environments and collaborative software <ref type="bibr" coords="1,297.84,548.62,10.60,8.91" target="#b8">[9]</ref>. Classifiers capable of detecting vandalism can mitigate these issues by autonomously undoing poor edits or prioritizing human efforts in locating them. Numerous proposals have addressed this need, as well surveyed in <ref type="bibr" coords="1,146.16,584.50,10.92,8.91" target="#b1">[2,</ref><ref type="bibr" coords="1,157.08,584.50,7.28,8.91" target="#b5">6,</ref><ref type="bibr" coords="1,164.37,584.50,7.28,8.91" target="#b8">9]</ref>. These techniques span multiple domains, including natural language processing (NLP), reputation algorithms, and metadata analysis. Recently, our own prior work <ref type="bibr" coords="1,157.92,608.38,11.60,8.91" target="#b1">[2]</ref> combined the leading approaches from these domains to establish a new performance baseline; our technique herein borrows heavily from that effort.</p><p>The 2011 edition of the PAN-CLEF vandalism detection competition, however, has slightly redefined the task relative to the 2010 competition <ref type="bibr" coords="1,371.52,644.26,11.60,8.91" target="#b5">[6]</ref> and the bulk of existing anti-vandalism research. In particular, two differences have motivated novel analysis and feature development. First, the prior edition permitted only zero-delay features: an edit simultaneously committed and evaluated at time t n can only leverage information from time t ≤ t n . However, if evaluation can be delayed until time t n+m , it is possible to use ex post facto evidence from the t n &lt; t ≤ t n+m interval to aid predictive efforts. While such features are not relevant for "gate-keeping," they still have applications. For example, the presence of vandalism would severely undermine static content distributions like the Wikipedia 1.0 project<ref type="foot" coords="2,309.84,189.21,3.49,6.23" target="#foot_0">1</ref> , which targets educational settings. This work describes/evaluates several ex post facto features and finds them to be very strong vandalism predictors.</p><p>The second redefinition is that this year's corpus contains edits from three languages: German, English, and Spanish. Prior research, however, has been conducted almost exclusively in English, and the 2010 PAN-CLEF winning approach heavily utilized English-specific dictionaries <ref type="bibr" coords="2,276.84,262.55,10.79,8.91" target="#b5">[6,</ref><ref type="bibr" coords="2,287.63,262.55,7.19,8.91" target="#b7">8]</ref>. Such techniques do not lend themselves to portability across Wikipedia's 200+ language editions, motivating the use of languageindependent features. While these are capable of covering much of the problem space, we find the addition of language-specific features still moderately improves classifier performance. Orthogonal to the issue of portability, we also use the multiple corpora to examine the consistency of feature performance across language versions.</p><p>While discussion concentrates on these novel aspects, we also implement a breadth of features (65 in total). Performance measures, as detailed in Sec. 3.2, vary based on language and task. The complete feature set produces cross-validation results consistent with the state-of-the-art for English <ref type="bibr" coords="2,278.52,370.07,11.60,8.91" target="#b1">[2]</ref> and establishes novel performance benchmarks for Spanish and German (PR-AUC=0.91, weighing languages equally). Though performance varied considerably over the label-withheld PAN-CLEF 2011 test set, our approach took first-place in the associated competition, reinforcing its status as the most accurate known approach to vandalism classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Feature Set</head><p>This section describes the features implemented. Discussion begins with a core featureset that is both zero-delay and language independent (Sec. 2.1). Then, two extensions to that set are handled: ex post facto (Sec. 2.2) and language-specific (Sec. 2.3). Any feature which cannot be calculated directly from the provided corpus utilizes the Wikipedia API <ref type="foot" coords="2,150.84,507.33,3.49,6.23" target="#foot_1">2</ref> . Readers should consult cited works to learn about the algorithms and parameters of complex features (i.e., reputations and lower-order classifiers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Zero-Delay, Language-Independent Features</head><p>Tab. 1 presents features that are: (1) zero-delay and (2) language-independent. Note that features utilizing standardized language localization are included in this category (e.g., "User Talk" in English, is "Benutzer Diskussion" in German).</p><p>Nearly all of these features have been described in prior work <ref type="bibr" coords="2,396.48,593.99,10.79,8.91" target="#b1">[2,</ref><ref type="bibr" coords="2,407.27,593.99,7.19,8.91" target="#b5">6]</ref>, so their discussion is abbreviated here. Even so, these signals are fundamental to our overall approach, given that a single implementation is portable across all language versions. This is precisely why an extensive quantity of these features have been encoded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FEATURE DESCRIPTION</head><p>USR_IS_IP Whether the editor is anonymous/IP, or a registered editor USR_IS_BOT Whether the editor has the "bot" flag (i.e., non-human user) USR_AGE Time, in seconds, since the editor's first ever edit USR_BLK_BEFORE Whether the editor has been blocked at any point in the past USR_PG_SIZE Size, in bytes, of the editor's "user talk" page USR_PG_WARNS Quantity of vandalism warnings on editor's "user talk" (EN only)</p><p>USR_EDITS_ * Editor's revisions in last, t ∈ {hour, day, week, month, ever} USR_EDITS_DENSE Normalizing USR_EDITS_EVER by USR_AGE USR_REP Editor reputation capturing vandalism tendencies <ref type="bibr" coords="3,409.44,218.82,14.99,8.02" target="#b9">[10]</ref> (EN only) USR_COUNTRY_REP Reputation for editor's geo-located country of origin <ref type="bibr" coords="3,421.56,229.86,14.99,8.02" target="#b9">[10]</ref> (EN only)</p><p>USR_HAS_RB Whether the editor has ever been caught vandalizing <ref type="bibr" coords="3,421.20,240.78,14.99,8.02" target="#b9">[10]</ref> (EN only) USR_LAST_RB Time, in seconds, since editor last vandalized <ref type="bibr" coords="3,395.64,251.70,14.99,8.02" target="#b9">[10]</ref>  LANG_MARKUP Measure of the addition/removal of wiki syntax/markup Table <ref type="table" coords="3,158.04,529.12,3.34,8.05">1</ref>. Zero-delay, language-independent features. Some features are not calculated for all languages. These are not fundamental limitations, rather, the source APIs are yet to extend support (but trivially could). See Sec. 2.3 for discussion regarding features of the "LANG_ * " form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Leveraging Ex Post Facto Evidence</head><p>More novel is the utilization of ex post facto data in the classification task. To the best of our knowledge, only the WikiTrust system of Adler et al. <ref type="bibr" coords="3,400.44,620.27,11.42,8.91" target="#b0">[1,</ref><ref type="bibr" coords="3,411.86,620.27,7.62,8.91" target="#b1">2]</ref>  No doubt, the strongest of these features is the WikiTrust score (WIKITRUST). This captures the notion of reputation-weighted content-persistence: text that survives is trustworthy, especially when the subsequent editors have good reputations. The Wik-iTrust values we obtain are from a lower-order classifier, encompassing ≈70 data points.</p><p>However, it may be possible to improve upon or supplement the WikiTrust score. First, WikiTrust is computationally intense, having to track word-level histories. Second, content is sometimes removed or re-authored for reasons other than malicious intent. Third, WikiTrust is not presently enabled for all languages. This motivated our creation of feature HASH_REVERT, a more efficient and coarse-grained measure. The hash-code is computed for the article version prior-to, and immediately-after, the edit under inspection (scope is expanded if the editor makes multiple consecutive edits). If the hashes match it indicates an identity revert, the wholesale removal of the editor's contributions, which is highly indicative of vandalism.</p><p>Another novel feature, USR_PG_SZ_DELT, captures that poor contributors are often notified/warned of their transgressions on their "talk page". Informal analysis suggested that German and Spanish versions lack the standardized warning system that English employs <ref type="bibr" coords="4,204.24,478.55,10.60,8.91" target="#b2">[3]</ref>. Thus, a generic "size change" feature was implemented to detect such talk page contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">On Language-Driven Features</head><p>When talking about language features, realize that is possible to produce languagedriven features that are not language-specific (i.e., generic properties). Examples include our features of the form LANG_ * , as found at the bottom of Tab. 1. These measures are certainly applicable to the languages used herein (German, English, Spanish) and analogues likely exist in many languages. However, these properties are unlikely to be universal in nature. In particular, different character sets (e.g., Hindi, Chinese, Japanese) might prove problematic, but this is ultimately outside the authors' range of expertise. It should be noted that languages similar to those under evaluation (i.e., use of Latin characters, letter casing, space-delimited words, and Arabic numerals) represent a significant portion of Wikipedia's article space <ref type="foot" coords="4,327.60,636.33,3.49,6.23" target="#foot_2">3</ref> .  <ref type="table" coords="5,158.64,182.32,3.34,8.05">3</ref>. Features requiring natural-language customization. Each feature is implemented independently, per-language. Spanish and German edits are also processed by the English versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LANG</head><p>While generic language features are portable, they lack the intuition of languagespecific ones. After all, profanity and slang have little place in encyclopedic content. Not only are such measures intuitive, they are effective, as the 2010 PAN-CLEF winning approach of Velasco <ref type="bibr" coords="5,242.88,271.31,11.60,8.91" target="#b7">[8]</ref> used multiple dictionaries (profanity, sexual terms, biased words, etc.). This is disheartening as such features: (1) lack portability, (2) can be evaded with obfuscation, (3) require time-consuming implementation by fluent speakers, and (4) tend to be computationally expensive. Velasco, however, did not include many of the language-independent features we present in Tab. 1. Thus, as <ref type="bibr" coords="5,447.84,319.07,11.60,8.91" target="#b1">[2]</ref> suggested, language-independent features might overlap and render language-specific ones less critical. We extend that analysis here and do so across multiple natural languages.</p><p>Unfortunately, Velasco's dictionaries are not open source and the German and Spanish equivalents must be implemented. Not NLP experts ourselves, we intend only to create proof-of-concept and non-exhaustive language-specific features, as per Tab. 3. This also allows us to perform cost-benefit analysis (i.e., the coverage of dictionaries vs. the performance improvement) and motivates our decision to encode three different approaches to compiling the offensive word lists ("offensive" here is just the combination of all undesirable text categories):</p><p>-SPANISH (ES): We re-purposed a scoring list designed for Spanish Wikipedia use <ref type="foot" coords="5,474.12,451.89,3.49,6.23" target="#foot_3">4</ref> .</p><p>The list contains 800+ manually constructed regexps of extensive complexity (capturing intra-word permutations of diacritics, case, repeated letters, etc.). Manual inspection removed regexps not specific to offensive terminology. -ENGLISH (EN): A generic list of 1300+ offensive words (not regexps) is utilized <ref type="foot" coords="5,474.24,499.77,3.49,6.23" target="#foot_4">5</ref> .</p><p>The list is not Wikipedia-specific, but does enumerate conjugated verb forms. -GERMAN (DE): Unable to locate a dictionary of sufficient breadth, we decided to examine the feasibility of a programmatic approach. We took the union of informal profanity lists and ran a stemming algorithm to produce roots which could be searched for as embedded (i.e., non word-boundary delimited) regexp matches.</p><p>The text added and removed by an edit is scanned for word/regexp matches. The number of matches are quantified (+1 for additions, -1 for removals) and these form the {DE,EN,ES}_OFFEND features. The first-person "pronoun" features are straightforward and intend to capture bias in authorship and possible non-neutral points-of-view. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>This section describes and evaluates the machine-learning model built atop our feature set. We begin by describing our choice of classification algorithm (Sec. 3.1). Then, this model is used to evaluate feature effectiveness over the labeled training set, paying particular attention to novel subsets (Sec. 3.2). Finally, we summarize performance over the PAN-CLEF 2011 competition test set (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classification Model</head><p>The Weka <ref type="bibr" coords="6,179.64,532.55,11.60,8.91" target="#b3">[4]</ref> implementation of the alternating decision tree algorithm (ADTree) is used for scoring/classification. This method was chosen because it: (1) produces humanreadable models, (2) handles missing features (API failures, missing data, etc.), and</p><p>(3) supports enumerated features (our strategy has many booleans). ADTrees have one parameter of interest: the quantity of "boosting iterations" (i.e., tree-depth). German and Spanish classifiers utilize 18 iterations and English uses 30, quantities arrived at via cross-validation (the English training corpus <ref type="bibr" coords="6,328.80,604.31,11.60,8.91" target="#b4">[5]</ref> is 32× the size of the other two).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Set Evaluation</head><p>All results are produced via 10-fold cross-validation over the training corpus <ref type="bibr" coords="6,447.96,649.55,10.60,8.91" target="#b4">[5]</ref>. The labels of the test corpus were withheld for the competition, as discussed in Sec. 3.3.</p><formula xml:id="formula_0" coords="7,136.20,117.28,330.46,123.36">(a) (b) # GERMAN ENGLISH SPANISH 1 WT_NO_DELAY WT_NO_DELAY USR_EDITS_MONTH 2 USR_EDITS_EVER USR_EDITS_MONTH USR_EDITS_WEEK 3 USR_IS_IP USR_EDITS_WEEK USR_EDITS_EVER 4 USR_EDITS_MONTH USR_EDITS_EVER USR_IS_IP 5 USR_EDITS_WEEK USR_COUNTRY_REP ES_OFFEND_IMPACT 1 NEXT_COMM_VAND (F) WIKITRUST (F) NEXT_COMM_VAND (F) 2 WIKITRUST (F) WT_DELAY_DELT (F) NEXT_TIME_AHEAD (F) 3 WT_NO_DELAY WT_NO_DELAY HASH_REVERT (F) 4 HASH_REVERT (F) HASH_REVERT (F) USR_PG_SZ_DELT (F) 5 NEXT_USR_IP (F) NEXT_COMM_VAND (F) USR_EDITS_MONTH</formula><p>Table <ref type="table" coords="7,175.44,250.84,3.34,8.05">5</ref>. Extending Tab. 4 for all language corpora. Portion (a) permits only zero-delay features, while portion (b) also includes ex post facto signals, as indicated by "(F)".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Core Features and Cross-Language Consistency:</head><p>We begin with the "core" set of features (Tab. 1). Though these have been described in the past, their cross-language evaluation is novel. Although space considerations prevent showing the full featureranking for all languages (Tab. 5a), they are remarkably similar to those presented for English (Tab. 4, ignoring "(F)" entries), especially when binned by the info-gain metric.</p><p>That is, a feature tends to be equally effective no matter the language of evaluation.</p><p>It is unsurprising that the zero-delay WikiTrust feature (WT_NO_DELAY) is the topperforming feature where available (English, German) -it is a lower-order classifier that wraps many data points. Beyond that, user participation statistics and registration status are also dominant. Generic language features tend to perform moderately (not all edits add content), with article-driven signals tending towards the bottom of the rankings.</p><p>While the feature ranking is not unexpected, the cross-language consistency has stronger implications. It is a sociologically interesting observation that misbehavior is characterized similarly across language and cultural boundaries. More technically, it suggests the creation of language-independent classifiers might be feasible, eliminating the need for new corpora to be amassed for each new Wikipedia edition.</p><p>Ex Post Facto Inclusion: As Tab. 5b demonstrates, the inclusion of ex post facto features dramatically modifies the list of "best features," with 4 of the top 5 being of this type for all languages. Such signals also positively affect overall performance, varying between 3.6% (English) and 13.6% (Spanish) PR-AUC increase (see Tab. 6). While these improvements are not overwhelming, it should be emphasized that the highaccuracy of zero-delay approaches decreases the possible margin for improvement.</p><p>These ex post facto features are redundant, however, all trying to capture the same notion: "was the edit reverted?" (particularly WIKITRUST, NEXT_COMM_VAND, and HASH_REVERT). While all are features of exemplary performance, they vary in efficiency and robustness. For example, WikiTrust employs a complex but secure algorithm that mines reputation from implicit Wikipedia actions. In contrast, NEXT_COMM_VAND parses explicit summaries for keywords, which while simple, could easily be gamed. The degree to which secure features are required is not immediately apparent. Vandals are typically poorly incentivized <ref type="bibr" coords="7,268.68,661.55,11.60,8.91" target="#b6">[7]</ref> and therefore may not evade crude protections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GERMAN ENGLISH SPANISH METRIC RND ZD ALL RND ZD ALL RND ZD ALL</head><p>PR-AUC 0.302 0.878 0.930 0.074 0.773 0.801 0.310 0.868 0.986 ROC-AUC 0.500 0.958 0.981 0.500 0.963 0.968 0.500 0.946 0.993 Table <ref type="table" coords="8,158.16,171.28,3.34,8.05">6</ref>. Area-under-curve (AUC) measurements for feature sets over training data. This is done for precision-recall (PR) and receiver-operating characteristic (ROC) curves. Feature sets include a control classifier (random, RND), zero-delay (ZD), and including ex post facto data (ALL). Cost vs. Benefits of Language-Specific Signals: As Tab. 7 shows, the performance benefit of language-specific features varies dramatically. They prove most helpful when targeting zero-delay detection, and the extensiveness and expertise involved in creating the "offensive word list" correlates with performance gains. Recall from Sec. 2.3 that our German approach was quite crude (a stemming algorithm over informal profanity lists). Such attempts did not translate positively, adding only noise to the classifier. At the other extreme, a third-party, Wikipedia-customized, and complex set of regularexpressions was able to increase zero-delay PR-AUC by nearly 8% in the Spanish case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LANG</head><p>Where infrastructure already exists for these purposes, it can and should be reutilized (as we did for English and Spanish). Where it does not, it would seem casual attempts should be avoided. More broadly, it seems wise to investigate autonomous (and language-independent) means to produce robust dictionaries (e.g., n-grams).</p><p>Cumulative Performance: A broader viewer of classifier performance is presented numerically in Tab. 6 and visualized in Fig. <ref type="figure" coords="8,312.12,504.23,3.77,8.91" target="#fig_1">1</ref>. One interesting observation is the varying performance between languages. English, despite having the most enabled features, and 32× more training examples, is classified much poorer than Spanish and German. At current, we have two hypotheses why this is the case. First, English has a tool called the "Edit Filter" which prevents trivial vandalism from being saved <ref type="foot" coords="8,406.44,550.41,3.49,6.23" target="#foot_5">6</ref> (and becoming a corpus member). We are unaware of any German/Spanish equivalent, meaning obvious vandalism (i.e., "low-hanging fruit") would be corpus members in those cases. Second, vandalism tagging is a subjective process. The labeling of the English corpus was done via Amazon Mechanical Turk <ref type="bibr" coords="8,277.32,599.87,11.60,8.91" target="#b4">[5]</ref> (utilizing random persons), whereas the smaller German/Spanish versions involved Wikipedia researchers. The latter group is likely to be more consistent in upholding the standards of the Wikipedia community, and such agreement is particularly important for features like NEXT_COMM_VAND.  Regardless, English-language performance (the only known baseline) is comparable to the state-of-the-art. That benchmark was set in our prior work <ref type="bibr" coords="9,391.92,438.83,10.60,8.91" target="#b1">[2]</ref>, which this writing re-implements with slight modifications. It should be emphasized that it was not our intention to best that prior work, rather, we sought to use the expanded PAN-CLEF 2011 rules/corpora to analyze novel portions of the problem space.</p><p>Finally, it is interesting to produce the most effective feature subsets for each language (Tab. 8). Unlike Tab. 5, this list considers feature correlation and overlap; displaying the features weighted most heavily in the actual ADTree models. These orderings are quite unique compared to Tabs. 4 &amp; 5, and greater analysis is needed to determine what correlations give rise to these rule chains. For instance, English feature LANG_MARKUP ranked 25th in info-gain, yet was the 3rd highest ranking in subset form. Results like these imply a large degree of overlap between features, suggesting that small (and therefore, efficient) feature sets/trees can produce accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Test Set Performance</head><p>When applied to the label-withheld test set, our model won the 2011 PAN-CLEF competition. The PR-AUCs (EN= 0.706, EN= 0.822, ES= 0.489) show a slight performance increase for English, but a dramatic drop for German/Spanish relative to crossvalidation over training data (Tab. 6). When the test corpus labels are revealed, they should be inspected to see if some type of systematic bias gave rise to this discrepancy.</p><p>Our novel research directions in this paper were motivated by changes in the 2011 PAN-CLEF competition with respect to both the 2010 edition and the bulk of existing Wikipedia vandalism research. First, the competition permitted features to leverage evidence after the edits were made. We identified multiple metrics of this type, which were extremely effective, and whose implementation made clear the trade-off between feature efficiency and robustness.</p><p>Second, the competition spanned three natural languages. For language-independent features (i.e., metadata) this was the first non-English evaluation of such signals, though relative order was found to be surprisingly consistent across languages. Multiple languages, however, imply costly localization for language-specific features (e.g., profanity lists), forcing examination of their effectiveness. Including these atop an extensive set of language-independent features, we find that minor-to-moderate contributions are still possible, and the degree of improvement correlates with the localization's complexity.</p><p>We hope that this work continues to promote and improve the autonomous detection of vandalism. Such progress frees editors of monitoring roles and allows them to better contribute to a growing body of collaborative knowledge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,199.64,117.28,131.55,8.05;5,167.28,131.22,283.89,8.02;5,178.08,142.14,264.94,9.29;5,162.00,153.06,280.00,8.02;5,172.68,164.10,275.62,9.29;5,134.76,182.32,21.14,8.05"><head></head><label></label><figDesc>-SPEC. FEAT. DESCRIPTION {DE,EN,ES}_OFFEND Quantity of offensive terms added/removed by edit * _OFFEND_IMPACT Normalizing * _OFFEND by ART_SIZE_DELT {DE,EN,ES}_PRONOUN Quantity of 1st-person pronouns added/removed * _PRONOUN_IMPACT Normalizing * _PRONOUN by ART_SIZE_DELT Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,215.16,222.76,184.88,8.16"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Precision-recall curves over training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,134.76,620.27,346.02,44.79"><head>Table 2 .</head><label>2</label><figDesc>has previously described features of this type. Tab. 2 lists the ex post facto signals implemented in our approach, which includes our own novel contributions (the first 4 features), as well as those proposed and calculated by Adler et al. (the remainder). USR_BLK_EVER Whether the editor has ever been blocked on the wiki USR_PG_SZ_DELT Size change of "user talk" page between edit time and +1 hour ART_DIVERSITY Percentage of recent revisions (±10 edits) made by editor HASH_REVERT Whether article content hash-codes indicate edit was reverted WIKITRUST WikiTrust [1] score with ex-post-facto evidence (DE, EN only) WT_DELAY_DELT Difference in WIKITRUST and WT_NO_DELAY (DE, EN only) NEXT_TIME_AHEAD Time, in seconds, until article was next revised NEXT_USR_IP Whether the next editor of the article is an IP/anonymous editor NEXT_USR_SAME Whether the next article editor is same as current editor NEXT_COMM_VAND Whether the next "comment" indicates vandalism removal Ex-post-facto features: Leveraging evidence after edit save, but before evaluation.</figDesc><table /><note coords="4,162.36,117.28,137.27,8.05"><p>EX POST FEAT. DESCRIPTION</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.76,117.26,345.88,270.38"><head>Table 4 .</head><label>4</label><figDesc>Kullback</figDesc><table coords="6,136.20,117.26,340.72,230.18"><row><cell>ENGLISH FEATURE</cell><cell>#</cell><cell>. . . FEATURE . . .</cell><cell>#</cell><cell>. . . FEATURE . . .</cell><cell>#</cell></row><row><cell>WIKITRUST (F)</cell><cell>1</cell><cell cols="2">ART_SIZE_DELT 21</cell><cell cols="2">USR_LAST_RB 41</cell></row><row><cell>WT_DELAY_DELT (F)</cell><cell>2</cell><cell cols="2">USR_PG_SIZE 22</cell><cell cols="2">COMM_HAS_SEC 42</cell></row><row><cell>WT_NO_DELAY</cell><cell>3</cell><cell cols="4">ART_REP 23 ART_CHURN_CHARS 43</cell></row><row><cell>HASH_REVERT (F)</cell><cell>4</cell><cell cols="2">USR_PG_WARNS 24</cell><cell cols="2">COMM_IND_VAND 44</cell></row><row><cell>NEXT_COMM_VAND (F)</cell><cell>5</cell><cell cols="2">LANG_MARKUP 25</cell><cell cols="2">ART_CHURN_BLKS 45</cell></row><row><cell>USR_EDITS_MONTH</cell><cell>6</cell><cell cols="2">LANG_LONG_TOK 26</cell><cell cols="2">ART_EDITS_WEEK 46</cell></row><row><cell>USR_EDITS_WEEK</cell><cell>7</cell><cell cols="2">LANG_UCASE 27</cell><cell cols="2">ART_SIZE 47</cell></row><row><cell>USR_EDITS_EVER</cell><cell cols="3">8 EN_PRONOUN_IMPCT 28</cell><cell cols="2">ART_EDITS_DAY 48</cell></row><row><cell>USR_COUNTRY_REP</cell><cell>9</cell><cell cols="2">ART_EDITS_TOTAL 29</cell><cell cols="2">TIME_DOW 49</cell></row><row><cell cols="2">USR_EDITS_DENSE 10</cell><cell cols="2">USR_REP 30</cell><cell cols="2">ART_EDITS_HOUR 50</cell></row><row><cell cols="2">USR_IS_IP 11</cell><cell cols="4">ART_AGE 31 NEXT_USR_SAME (F) 51</cell></row><row><cell cols="2">USR_EDITS_DAY 12</cell><cell cols="2">LANG_ALPHA 32</cell><cell cols="2">USR_HAS_RB 52</cell></row><row><cell cols="2">USR_PG_SZ_DELT (F) 13</cell><cell cols="2">LANG_MARKUP 33</cell><cell cols="2">PREV_USR_IP 53</cell></row><row><cell cols="2">NEXT_TIME_AHEAD (F) 14</cell><cell cols="2">EN_PRONOUN 34</cell><cell cols="2">USR_BLK_EVER (F) 54</cell></row><row><cell cols="2">USR_AGE 15</cell><cell cols="2">ART_EDITS_DENSE 35</cell><cell cols="2">USR_BLK_BEFORE 55</cell></row><row><cell cols="4">COMM_LEN_NO_SEC 16 ART_DIVERSITY (F) 36</cell><cell cols="2">USR_IS_BOT 56</cell></row><row><cell cols="2">EN_OFFEND_IMPACT 17</cell><cell cols="2">LANG_CHAR_REP 37</cell><cell cols="2">NEXT_USR_IP (F) 57</cell></row><row><cell cols="2">USR_EDITS_HOUR 18</cell><cell cols="2">PREV_USR_SAME 38</cell><cell cols="2">TIME_TOD 58</cell></row><row><cell cols="2">EN_OFFEND 19</cell><cell cols="2">PREV_TIME_AGO 39</cell><cell></cell><cell></cell></row><row><cell cols="2">COMM_LEN 20</cell><cell cols="2">ART_EDITS_MONTH 40</cell><cell></cell><cell></cell></row></table><note coords="6,199.73,357.78,280.91,8.02;6,134.76,368.70,345.76,8.02;6,134.76,379.62,314.00,8.02"><p>-Leibler divergence (i.e., information-gain) ranking for English features. Ex post facto signals are indicated by "(F)" (but ranking is independent, so a zero-delay list would have the same relative ordering). Foreign language features are not included for brevity.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.76,220.48,345.86,84.12"><head>Table 7 .</head><label>7</label><figDesc>Measuring the impact of language-specific features (Tab. 3). Feature sets are evaluated with (W) and without (WO) the inclusion of language-specific signals. Otherwise, acronyms are as defined as in Tab. 6. PR-AUC is the singular metric used in this comparison.</figDesc><table coords="8,157.92,220.48,297.33,43.80"><row><cell></cell><cell cols="6">ZD-WO ZD-W DIFF% ALL-WO ALL-W DIFF%</cell></row><row><cell>(PR-AUC) DE</cell><cell>0.881</cell><cell>0.878</cell><cell>-0.34%</cell><cell>0.930</cell><cell cols="2">0.930 ±0.00%</cell></row><row><cell>(PR-AUC) EN</cell><cell>0.737</cell><cell cols="2">0.773 +4.89%</cell><cell>0.776</cell><cell cols="2">0.801 +3.22%</cell></row><row><cell>(PR-AUC) ES</cell><cell>0.805</cell><cell cols="2">0.868 +7.83%</cell><cell>0.988</cell><cell>0.986</cell><cell>-0.20%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,150.84,259.26,328.40,138.70"><head>Table 8 .</head><label>8</label><figDesc>Top feature subsets of size n = 5, calculated using greedy step-wise analysis.</figDesc><table coords="9,161.28,259.26,305.39,109.42"><row><cell>1 WT_NO_DELAY</cell><cell cols="2">EN_OFFEND_IMPACT ES_OFFEND_IMPACT</cell></row><row><cell>2 USR_EDITS_MONTH</cell><cell>USR_PG_WARNS</cell><cell>USR_IS_IP</cell></row><row><cell>3 ART_CHURN_CHARS</cell><cell>WT_NO_DELAY</cell><cell>TIME_TOD</cell></row><row><cell>4 USR_PG_SIZE</cell><cell>USR_EDITS_MONTH</cell><cell>LANG_UCASE</cell></row><row><cell>5 ART_SIZE_DELT</cell><cell>LANG_UCASE</cell><cell>PREV_USR_IP</cell></row><row><cell cols="2">1 NEXT_COMM_VAND (F) WIKITRUST (F)</cell><cell>NEXT_COMM_VAND (F)</cell></row><row><cell>2 USR_IS_IP</cell><cell cols="2">NEXT_COMM_VAND (F) USR_EDITS_WEEK</cell></row><row><cell>3 LANG_UCASE</cell><cell>LANG_MARKUP</cell><cell>NEXT_TIME_AHEAD (F)</cell></row><row><cell>4 LANG_ALPHA</cell><cell>USR_COUNTRY_REP</cell><cell>PREV_TIME_AGO</cell></row><row><cell>5 ART_CHURN_CHARS</cell><cell>LANG_LONG_TOK</cell><cell>LANG_LONG_TOK</cell></row></table><note coords="9,150.84,389.80,273.56,8.16"><p>Portion (a) permits only zero-delay features; (b) includes ex post facto ones.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.72,647.70,176.38,5.31"><p>http://en.wikipedia.org/wiki/Wikipedia:1.0</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.72,658.86,138.58,5.31"><p>http://en.wikipedia.org/w/api.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.72,658.86,281.02,5.31"><p>http://meta.wikimedia.org/wiki/List_of_Wikipedias_by_language_group</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,144.72,647.70,281.02,5.31"><p>http://es.wikipedia.org/wiki/Usuario:AVBOT/Lista_del_bien_y_del_mal</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,144.72,658.86,167.98,5.31"><p>http://www.cs.cmu.edu/~biglou/resources/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="8,144.72,658.86,209.98,5.31"><p>http://en.wikipedia.org/wiki/Wikipedia:Edit_Filter</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements: This research was supported in part by <rs type="funder">ONR</rs> <rs type="grantNumber">MURI N00014-07-1-0907</rs>.</p><p>The authors recognize those colleagues whose techniques were a component in the described approach: <rs type="person">B. Thomas Adler</rs>, <rs type="person">Luca de Alfaro</rs>, <rs type="person">Santiago Mola-Velasco</rs>, <rs type="person">Sampath Kannan</rs>, <rs type="person">Ian Pye</rs>, and <rs type="person">Paolo Rosso</rs> (see [2]). <rs type="person">Andreas Haeberlen</rs> is thanked for his German language assistance. <rs type="person">Martin Potthast</rs> is acknowledged for his continued dedication to the vandalism detection task.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zNPknyh">
					<idno type="grant-number">MURI N00014-07-1-0907</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.54,434.70,309.95,8.02;10,150.96,445.62,302.27,8.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,246.24,434.70,190.17,8.02">A content-driven reputation system for the Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>De Alfaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,150.96,445.62,257.54,8.02">WWW&apos;07: Proc. of the 16th International World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.54,455.94,330.16,8.02;10,150.96,466.98,323.68,8.02;10,150.96,477.90,306.59,8.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,396.60,455.94,76.10,8.02;10,150.96,466.98,262.01,8.02">Wikipedia vandalism detection: Combining natural language, metadata, and reputation features</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>De Alfaro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Mola-Velasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,430.80,466.98,43.84,8.02;10,150.96,477.90,225.90,8.02">CICLing&apos;11 (Comp. Linguistics and Intelligent Text Processing) and LNCS</title>
		<imprint>
			<date type="published" when="2011-02">February 2011</date>
			<biblScope unit="volume">6609</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.54,488.22,313.72,8.02;10,150.96,499.14,328.55,8.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,237.72,488.22,218.54,8.02;10,150.96,499.14,22.08,8.02">The work of sustaining order in Wikipedia: The banning of a vandal</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ribes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,191.28,499.14,262.24,8.02">CSCW&apos;10: Proc. of the Conf. on Computer Supported Cooperative Work</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.54,509.46,324.73,8.02;10,150.96,520.50,251.51,8.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,424.08,509.46,43.19,8.02;10,150.96,520.50,115.77,8.02">The WEKA data mining software: An update</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,272.88,520.50,81.85,8.02">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.54,530.82,321.04,8.02;10,150.96,541.74,302.14,8.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,199.32,530.82,164.25,8.02">Crowdsourcing a Wikipedia vandalism corpus</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,381.84,530.82,81.74,8.02;10,150.96,541.74,224.98,8.02">SIGIR&apos;10: Proc. of the 33rd International ACM SIG Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="189" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.54,552.06,310.74,8.02;10,150.96,562.98,293.39,8.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,278.28,552.06,175.00,8.02;10,150.96,562.98,109.98,8.02">Overview of the 1st International competition on Wikipedia vandalism detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Holfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,279.00,562.98,139.21,8.02">PAN-CLEF 2010 Labs and Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.54,573.30,304.30,8.02;10,150.96,584.34,266.51,8.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,413.76,573.30,33.08,8.02;10,150.96,584.34,157.77,8.02">Creating, destroying, and restoring value in Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Priedhorsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Panciera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Terveen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,327.24,584.34,53.85,8.02">ACM GROUP&apos;</title>
		<imprint>
			<biblScope unit="volume">07</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.54,594.66,336.61,8.02;10,150.96,605.58,262.67,8.02" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,215.76,594.66,263.39,8.02;10,150.96,605.58,64.96,8.02">Wikipedia vandalism detection through machine learning: Feature review and new proposals</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M M</forename><surname>Velasco</surname></persName>
		</author>
		<idno>PAN at CLEF 2010</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Lab Report for</note>
</biblStruct>

<biblStruct coords="10,142.54,615.90,304.62,8.02;10,150.96,626.82,323.75,8.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,353.40,615.90,93.76,8.02;10,150.96,626.82,41.95,8.02">Trust in collaborative web applications</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,198.72,626.82,249.95,8.02">Future Generation Comp. Sys. section on Trusting Software Behavior</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.18,637.14,309.72,8.02;10,150.96,648.18,329.02,8.02" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,267.00,637.14,184.89,8.02;10,150.96,648.18,102.45,8.02">Detecting Wikipedia vandalism via spatio-temporal analysis of revision metadata</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,271.80,648.18,182.20,8.02">EUROSEC&apos;10: European Wkshp. on Sys. Security</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
