<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.51,115.90,336.35,12.90;1,223.43,135.75,168.50,10.75">Vote/Veto Meta-Classifier for Authorship Identification Notebook for PAN at CLEF 2011</title>
				<funder>
					<orgName type="full">Austrian Research Promotion Agency FFG</orgName>
				</funder>
				<funder>
					<orgName type="full">Austrian Ministry of Economics and Labor</orgName>
				</funder>
				<funder ref="#_rJ4GnmE">
					<orgName type="full">Austrian Ministry of Transport, Innovation and Technology</orgName>
				</funder>
				<funder>
					<orgName type="full">State of Styria</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,157.50,172.15,50.94,8.64"><forename type="first">Roman</forename><surname>Kern</surname></persName>
							<email>rkern@tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Knowledge Management</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,217.88,172.15,61.16,8.64"><forename type="first">Christin</forename><surname>Seifert</surname></persName>
							<email>christin.seifert@tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Knowledge Management</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.49,172.15,59.48,8.64"><forename type="first">Mario</forename><surname>Zechner</surname></persName>
							<email>mzechner@know-center.at</email>
							<affiliation key="aff1">
								<orgName type="department">Know-Center GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,374.30,172.15,72.76,8.64"><forename type="first">Michael</forename><surname>Granitzer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Knowledge Management</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Know-Center GmbH</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.51,115.90,336.35,12.90;1,223.43,135.75,168.50,10.75">Vote/Veto Meta-Classifier for Authorship Identification Notebook for PAN at CLEF 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F334FBBBFD28CEF6CF77DB72CEC510E9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the PAN 2011 authorship identification challenge we have developed a system based on a meta-classifier which selectively uses the results of multiple base classifiers. In addition we also performed feature engineering based on the given domain of e-mails. We present our system as well as results on the evaluation dataset. Our system performed second and third best in the authorship attribution task on the large data sets, and ranked middle for the small data set in the attribution task and in the verification task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The PAN 2011 challenge tackled the problem of authorship identification <ref type="bibr" coords="1,433.74,393.97,11.38,8.64" target="#b4">[5,</ref><ref type="bibr" coords="1,445.12,393.97,11.38,8.64" target="#b11">12]</ref>. Two different task were given for this domain. The first task was an authorship attribution task. The goal of this task has been to identify the author of a previously unseen text from a set of candidate authors. The second task was authorship verification. The goal of authorship verification is to verify whether a text of unknown authorship was written by a specific (given) author.</p><p>We focused on the authorship attribution task and did not directly address the authorship verification problem setting. Instead, we applied the pipeline developed in the authorship attribution task to the verification task. The source code is open source and available for download <ref type="foot" coords="1,225.92,499.90,3.49,6.05" target="#foot_0">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Authorship Attribution System Overview</head><p>Our authorship identification systems consists of two main stages. In the first stage we analyze each author's documents and generate multiple sets of features for each document. In the second stage we train multiple classification models for each author based on these feature sets. We then combine these models to assign author names to unseen documents. The system was trained on the PAN authorship attribution development dataset. This dataset exclusively consists of e-mails, which allows us to employ some assumptions. The following list gives an overview of the stages and the sections describing the stages in detail:</p><p>1. Document analysis and feature set generation (a) Preprocessing and feature generation, see section 2.1 (b) Calculating additional statistics from external resources, see section 2.2 (c) Weighting feature values, see section 2.3 (d) Creating feature spaces, see section 2.4 2. Classifier training, see section 2.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing and Feature Generation</head><p>The task of the preprocessing step is to analyze the e-mail content (plain text) and to store the result alongside the document. Out of these so called annotations, the features will be generated in consecutive processing stages. The preprocessing pipeline consists of several components which are executed in a fixed sequence. The pipeline is individually applied to each document. Each annotation class will briefly be covered in the following section.</p><p>Text Line and Block Annotations: The first pre-processing of the text splits the content of each document into lines. Line endings are identified via the newline character. Multiple consecutive, non-empty lines are merged into text blocks. The text block annotations try to capture the layout of an e-mail message. All succeeding pre-processing steps operate on individual text-blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natural Language Annotations:</head><p>We employ the open-source Java library OpenNLP <ref type="foot" coords="2,476.61,373.92,3.49,6.05" target="#foot_1">4</ref>and the available maximum entropy models to split the text blocks into tokens and sentences. For each token we generated alternative representations. At first the characters of the token are normalized to lower-case. Next, the tokens are stemmed by applying the Snowball stemmer <ref type="foot" coords="2,223.74,421.74,3.49,6.05" target="#foot_2">5</ref> . Finally all tokens are scanned for stop-words. Stop-words are detected by checking tokens against a set of predefined stop words from the Snowball stemmer project. Additionally we mark tokens with more than 50% non-letter characters as stop-words. Finally we annotate the part-of-speech tag for each token, again using the respective OpenNLP classifier and model for the English language.</p><p>Slang-Word Annotations Based on the intuition that e-mails may contain an Internet specific vocabulary we have developed an annotator tailored towards this kind of writing style. This annotator is based on gazetteer-lists <ref type="foot" coords="2,340.97,515.87,3.49,6.05" target="#foot_3">6</ref> of three different kinds of words expected to be found within a document written in the "Internet" writing style:</p><p>-Smilies: A list of common smilies <ref type="foot" coords="2,288.70,545.85,3.49,6.05" target="#foot_4">7</ref> .</p><p>-Internet Slang: A list of words that are regularly used within online conversations <ref type="foot" coords="2,474.12,557.42,3.49,6.05" target="#foot_5">8</ref> .</p><p>-Swear-Words: A list of swear words has been collected similar to the other two gazetteer lists <ref type="foot" coords="2,206.16,580.96,3.49,6.05" target="#foot_6">9</ref> .</p><p>The usefulness of such annotations depends on the presence of such words in the document set. Manual inspection of a sample of the documents reveals that smilies and other Internet specific terminology is hardly used by the authors of the training data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grammar Annotations</head><p>The final set of annotations have been developed motivated by the intuition that the writing style of people varies in the grammatical complexity of the sentences. For example some people prefer short sentences and a simple grammatical structure, while others may prefer to construct complex sentences with many interjections. Grammatical relations have been used in the past to exploit the grammatical structure of sentences <ref type="bibr" coords="3,226.49,233.01,10.58,8.64" target="#b7">[8]</ref>. For our authorship identification system we employed the Stanford Parser <ref type="bibr" coords="3,199.22,244.96,11.62,8.64" target="#b8">[9]</ref> to produce the sentence parse tree, the sentence phrases, as well as to identify the typed grammatical relationships <ref type="bibr" coords="3,322.25,256.92,15.27,8.64" target="#b9">[10]</ref>. These informations are then added as annotations to the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Calculating Statistics from External Resources</head><p>For many term weighting approaches the commonness of a word plays an important part. As it is not clear whether the training set is large enough to derive reliable statistics, we incorporated an external resource to generate this information: We parsed the Open American National Corpus (OANC) 10 to gain global statistics of the usage of words. This corpus consists of many different kinds of documents and writing styles. Many of the contained documents are rather long. Therefore we employed a text segmentation algorithm <ref type="bibr" coords="3,176.00,402.61,11.62,8.64" target="#b5">[6]</ref> to split long document into smaller, coherent parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feature Value Weighting</head><p>We developed multiple normalization and weighting strategies to allow a customizable representation of the frequency based document features. Finding the best suited feature weighting function for a given feature-space is an important aspect of feature engineering. This is especially true for information retrieval applications, but also applies on supervised machine learning. Depending on the feature space different strategies may lead to better performance of the classifier.</p><p>Binary Feature Value A numeric feature value is transformed into a binary value based on a given threshold. In our current system configuration the input to this normalization step is a term-count and the threshold is 1.</p><p>Locally Weighted Feature Value A more general approach than the binary feature value normalization is the application of an unary operation to a numeric feature value. In the current version we apply the sqrt(x) function on the feature value x (e.g. term-count).</p><p>Externally Weighted Feature Value For the authorship identification we adapted a weighting scheme which is rooted on the well-known BM-25 retrieval model <ref type="bibr" coords="4,417.94,131.27,18.45,8.64" target="#b10">[11]</ref> -although we replaced the length normalization as the data set is expected to be vastly different to the OANC corpus. Additionally to the commonness of a term we also integrated the dispersion of its occurrences, which has proven to be beneficial to capture the semantics of short text documents <ref type="bibr" coords="4,230.91,179.09,10.58,8.64" target="#b6">[7]</ref>.</p><p>For a term x, the equation of the global weighting function incorporates the feature value tf x , the number of documents N in the OANC corpus, the number of documents the term occurs in df x , the length of the current document and the dispersion of the term DP (x):</p><formula xml:id="formula_0" coords="4,181.57,243.08,299.02,23.22">w ext = tf x * log(N -df x + 0.5) df x + 0.5 * 1 √ length * DP (x) -0.3<label>(1)</label></formula><p>Globally Weighted Feature Value Another weighting strategy uses the training documents to calculate the document frequency of features. In contrast to the external weighting scheme, the term dispersion is not integrated into the weighting function.</p><p>The equation for the global weighting incorporates the number of training document N and the number of documents the term occurs in.</p><formula xml:id="formula_1" coords="4,204.81,337.16,275.78,23.23">w global = tf x * log(N -df x + 0.5) df x + 0.5 * 1 √ length<label>(2)</label></formula><p>Purity Weighted Feature Value The final weighting strategy is a derivative of the global weighting function. Instead of using the individual documents of the training set, we concatenate all documents from each author into one single document. Thus, for each author there is one document which contains all terms that have been used by this author. The weight calculation is identical to the global weighting scheme. As the weighting will generate the highest values if a term is only used by a single author, we refer to this weighting scheme as a measure of purity. In the equation the term |A| denotes the number of authors and af x denotes the number of authors who have used the term at least once.</p><formula xml:id="formula_2" coords="4,201.07,473.53,279.52,23.23">w purity = tf x * log(|A| -af x + 0.5) af x + 0.5 * 1 √ length<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Feature Spaces</head><p>Our system produces two kinds of feature spaces. The first three feature-spaces (basic statistics, token statistics and grammar statistics) represent statistical properties of the documents. The range of the values for each of the features depends on the statistic property. Each of the remaining feature spaces represent a vector space model, where each feature corresponds to a single term. We conducted a series of evaluations based on the training set to find the best suited weighting strategy for each of these featurespaces. In the following construction of the feature spaces is described in detail.</p><p>Basic Statistics Feature Space The first feature space captures basic statistics of the layout and organization of the documents. Additionally these feature spaces also carry information of the sentences and token annotations. Table <ref type="table" coords="4,375.47,644.48,4.98,8.64" target="#tab_0">1</ref> lists the features of this feature space. Token Statistics Feature Space The token and the part-of-speech (POS) annotations build the base for the token statistics feature space. For this feature space only a restricted set of word classes, namely adjectives, adverbs, verbs and nouns, are evaluated.</p><p>For each POS tag a feature is generated and its value reflects the relative number of times this tag has been detected in relation to all tokens. Furthermore the average length of all tokens for each tag is collected. Finally a set of features encodes the histogram of token length. Table <ref type="table" coords="5,213.56,656.44,4.98,8.64">2</ref> gives an overview of the features of this feature-space.</p><p>Table <ref type="table" coords="6,205.41,115.83,3.36,8.06">2</ref>. Overview of the features of the token statistics feature-space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>token-length</head><p>Average number of characters of each token token-&lt;POS&gt; Relative frequency of each POS tag token-length-&lt;POS&gt; Average number of characters of each token for each POS tag token-length-[01-20] Relative number of tokens for the range between 1 and 20 characters Grammar Statistics Feature-Space From the grammatical annotations a set of features have been generated. This grammar statistics feature space captures the difference in the richness of grammatical constructions of different authors. Furthermore this feature space is motivated by the assumption that individual writing styles differ in the complexity of sentences. Table <ref type="table" coords="6,260.26,266.09,4.98,8.64">3</ref> summarizes these features.</p><p>Slang-Word Feature Space The slang-word annotations are transformed into an own feature space. The values of the features are incremented for each occurrence of a slang word (Internet slang, smiley or swear word).</p><p>Pronoun Feature Space Based on the hypothesis that different people prefer different pronouns, we constructed a specific feature space. As with the slang-word features, each occurrence of a pronoun is counted. Finally for each document this feature-space contains the frequency of all used pronouns.</p><p>Stop Word Feature Space All tokens marked as stop-word are counted and collected. They form the stop-word feature space. In contrast to the previous two feature spaces, the occurrence counters are not directly used. Instead the binary feature value transformation is applied. Thus this feature space only captures whether a document contains a specific stop word, or not.</p><p>Pure Unigrams Feature Space All tokens, which have not been marked as stop-word or punctuation, are used to create the pure unigram feature space. Again the raw occurrence counter for each token are further processed. For this feature-space the purity based feature weighting strategy is applied. Therefore the values of this feature-space are in inverse proportion to the number of author who use a specific word.</p><p>Bigrams Feature Space Some authors may have the tendency to reuse specific phrases, a property which is not captured by the previously introduced feature spaces. Therefore we created the bigram feature space (two consecutive terms represent a single feature). For this feature-space we apply the local feature weighting strategy.</p><p>Intro-Outro Feature-Space It can be observed that some people have the tendency to reuse the same phrase to start and end their e-mails. In our authorship identification system we have developed a simple heuristic to generate a feature-space for these phrases. For each document the first and last text-block is inspected. If any of these text-blocks consists of less than 3 lines and less than 50 characters per line, its terms are added to the feature-space. Finally these features are weighted using the external feature weighting strategy.</p><p>Table <ref type="table" coords="7,158.54,115.83,3.36,8.06">3</ref>. Feature generated out of the grammatical annotations. The values for the phrase types (&lt;TYPE&gt;) and relation types (&lt;REL&gt;) are directly taken from the parser component. sentence-tree-depth Average depth of the sentence parse tree for all sentences phrase-count Average number of phrases as detected by the parser phrase-&lt;TYPE&gt;-ratio Relative number of a specific phrase type (noun-phrase, verb-phrase, ...) relation-&lt;REL&gt;-ratio Relative number of a specific grammatical dependency type Term Feature Space The final feature-space of our system is build using all tokens of a document. For each token its occurrence counter is tracked. Finally the feature values are processing using the weighting scheme which incorporates the commonness of word via an external resource.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Classification Algorithms</head><p>We have integrated the open-source machine learning library WEKA <ref type="bibr" coords="7,406.80,312.88,11.62,8.64" target="#b3">[4]</ref> into our system for the base classifiers. We have developed a meta classifier, which combines the results of the underlying base classifiers.</p><p>Base Classifiers We use different classification algorithms for each feature space. The configuration which has been used to produce the final result makes use of two different base classifiers. For all three statistical feature-spaces we applied Bagging <ref type="bibr" coords="7,448.60,383.04,11.62,8.64" target="#b0">[1]</ref> with Random Forests <ref type="bibr" coords="7,201.54,395.00,11.62,8.64" target="#b1">[2]</ref> as the base classifier. For the remaining feature-spaces, we used the LibLinear classifier <ref type="bibr" coords="7,215.60,406.95,11.62,8.64" target="#b2">[3]</ref> setting the type to SVMTYPE_L2_LR in order to get posterior probabilities of the classification results. For all classifiers, we used the default parameter settings of the library, and did not conduct any detailed analysis of the influence of the parameters on the classifiers' performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta Classifier</head><p>The meta classifier combines the result of all base classifiers based on their performance on the training set, as well as on the posterior probabilities. Each of the base classifiers is trained using all documents from the training set, and 10-fold cross-validation is performed. For each of the classes (authors in this case) contained in the training set, the precision and recall on the cross-validated training set are recorded. If the precision of a class exceeds a pre-defined threshold -t p -the base classifier is allowed to vote for this class. For all classes where the recall is higher than another threshold -t r -the classifier is permitted to vote against a class (veto). Additionally each base classifier has a weight w c controlling its impact on the final classification. Further, for unseen documents, the posterior probabilities of the base classifiers are also evaluated to asses whether a base classifier's result will be included in the final prediction. If the probability for an author is higher than p p and the classifier is allowed to vote for this author, the probability is multiplied by w c and added to the authors score. If the probability is lower than p r (again assuming the classify may veto against this author), the probability times the weight is subtracted from the authors score.</p><p>The classifier, which operates on the term feature space, is treated differently. The classification results of this classifier are always used without any additional weighting or filtering. Thus all predicted probabilities of this classifier will be added to the scores of the authors. Finally, the author with the highest score is considered to be the most probable author. In the evaluation section we report the performance of our meta-classifier using the parameters: w c = 0.9, t p = 0.5, p p = 0.3, t r = 0.3, p r = 0.01. We conducted several experiments to find the parameters values that provide the best performance. During these evaluations we found that individual settings for each base classifier produced the optimal results. But finally we decided to use a single set of parameters for all classifiers as our experiments were conducted only on a single test corpus (LargeTrain) and we want to (i) avoid any over-fitting problems, and (ii) keep an already complex system as simple as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>For brevity we only report the results of the evaluation achieved on the data-set labeled as "LargeTrain". Table <ref type="table" coords="8,228.03,501.01,4.98,8.64" target="#tab_1">4</ref> shows the number of votes an vetos of the different base classifiers. Table <ref type="table" coords="8,187.89,512.96,4.98,8.64" target="#tab_2">5</ref> summarizes the vote and veto performance of the base classifiers on the validation data set.</p><p>Further our results were evaluated by the PAN team on unseen evaluation data sets. The PAN author identification challenge consisted of two tasks: authorship attribution and authorship verification. Altogether 17 runs were submitted by 12 different groups. We focused on the authorship identification task and used the resulting system (as outlined in the previous section) for the verification task as well. We present the macro and micro precision, recall and F1-meassure of our system on the four authorship attribution datasets and the three authorship verification datasets in Table <ref type="table" coords="8,383.67,608.61,3.74,8.64">6</ref>.</p><p>Our system performed second and third best in the authorship attribution task on the large data sets, and ranked middle for the small data set in the attribution task and in the verification task. On the author attribution datasets we compared very favorable in terms of precision compared to the best ranked systems. Our macro recalls tended to be lower on average. Given that our system has not explicitly been developed for author verification, it did perform acceptably for the first two verification datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>We presented our system for authorship attribution in the context of the PAN 2011 author identification challenge. Our system is based on heavy feature engineering as well as combining multiple classifiers in a unique way. In terms of feature engineering we employed standard corpus statistics as well as features tailored towards the domain of e-mails. In addition we also integrated features determined from annotations created by a syntactical natural language parser. Given this abundant number of different feature spaces we settled on a voting strategy for classification in which we train base classifiers for each individual feature space and apply voting and vetoing as well as weighting to form the classification model of an author. These two stages proved to perform acceptably in the context of the PAN challenge, especially in the case of the large author attribution data sets. Future work will include the augmentation of our vetoing scheme with boosting and ensemble classification principles as well as applying our system to other domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,136.16,115.83,345.39,423.10"><head>Table 1 .</head><label>1</label><figDesc>Overview of the features of the basic statistics feature space.</figDesc><table coords="5,136.16,149.35,345.39,389.57"><row><cell>Feature</cell><cell>Description</cell></row><row><cell>number-of-lines</cell><cell>Number of lines</cell></row><row><cell>number-of-characters</cell><cell>Number of characters</cell></row><row><cell>number-of-tokens</cell><cell>Number of tokens</cell></row><row><cell>number-of-sentences</cell><cell>Number of sentences</cell></row><row><cell>number-of-text-blocks</cell><cell>Number of text-block annotations</cell></row><row><cell>number-of-text-lines</cell><cell>Number of lines containing more the 50% letter charac-</cell></row><row><cell></cell><cell>ters</cell></row><row><cell>number-of-shout-lines</cell><cell>Number of lines containing more than 50% upper-case</cell></row><row><cell></cell><cell>characters</cell></row><row><cell>empty-lines-ratio</cell><cell>#emptylines number-of -lines</cell></row><row><cell>text-lines-ratio</cell><cell>number-of -text-lines number-of -lines</cell></row><row><cell>mean-line-length</cell><cell>Average number of characters per line</cell></row><row><cell>mean-nonempty-line-length</cell><cell>Average number of character per non-empty line</cell></row><row><cell>max-line-length</cell><cell>Number of characters of the longest line within the doc-</cell></row><row><cell></cell><cell>ument</cell></row><row><cell>text-blocks-to-lines-ratio</cell><cell>number-of -text-blocks number-of -lines</cell></row><row><cell>{max,mean}-text-block-line-length</cell><cell>Number of lines per text-block (maximum, average)</cell></row><row><cell>{max,mean}-text-block-char-length</cell><cell>Number of characters per text-block (maximum, aver-</cell></row><row><cell></cell><cell>age)</cell></row><row><cell>{max,mean}-text-block-token-length</cell><cell>Number of tokens per text-block (maximum, average)</cell></row><row><cell cols="2">{max,mean}-text-block-sentence-length Number of sentences per text-block (maximum, aver-</cell></row><row><cell></cell><cell>age)</cell></row><row><cell>{max,mean}-tokens-in-sentence</cell><cell>Number of tokens per sentence (maximum, average)</cell></row><row><cell cols="2">{max,mean}-punctuations-in-sentence Number of punctuations per sentence, in relation to the</cell></row><row><cell></cell><cell>number of tokens (maximum, average)</cell></row><row><cell>{max,mean}-words-in-sentence</cell><cell>Number of words per sentence, in relation to the number</cell></row><row><cell></cell><cell>of tokens (maximum, average)</cell></row><row><cell>number-of-punctuations</cell><cell>Number of tokens tagged as punctuations</cell></row><row><cell>number-of-stopwords</cell><cell>Number of tokens marked as stop word</cell></row><row><cell>number-of-words</cell><cell>Number of non-punctuation tokens</cell></row><row><cell>capitalletterwords-words-ratio</cell><cell>Ratio of number of words with at least one capital letter</cell></row><row><cell></cell><cell>divided by the number of words</cell></row><row><cell>capitalletter-character-ratio</cell><cell>Ratio of capital letters divided by the total number of</cell></row><row><cell></cell><cell>characters</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,115.83,345.83,157.83"><head>Table 4 .</head><label>4</label><figDesc>Assessment of the base classifiers performance after the training phase. For each of the voting authors the precision of the base classifier exceeds a threshold. For each of the veto authors the recall of the base classifier has been higher than a pre-defined threshold.</figDesc><table coords="8,225.65,162.25,161.81,111.40"><row><cell>Classifier</cell><cell cols="2">#Authors Vote #Authors Veto</cell></row><row><cell>basic-stats</cell><cell>4</cell><cell>14</cell></row><row><cell>token-stats</cell><cell>5</cell><cell>7</cell></row><row><cell>grammar-stats</cell><cell>5</cell><cell>5</cell></row><row><cell>slang-words</cell><cell>3</cell><cell>2</cell></row><row><cell>pronoun</cell><cell>6</cell><cell>1</cell></row><row><cell>stop-words</cell><cell>4</cell><cell>10</cell></row><row><cell>intro-outro</cell><cell>25</cell><cell>11</cell></row><row><cell>pure-unigrams</cell><cell>6</cell><cell>15</cell></row><row><cell>bigrams</cell><cell>20</cell><cell>23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,151.95,115.83,311.47,134.08"><head>Table 5 .</head><label>5</label><figDesc>Vote and Veto -Performance of the base classifiers on the TrainValid data-set.</figDesc><table coords="9,181.50,138.50,250.11,111.40"><row><cell>Classifier</cell><cell cols="4">Vote Accuracy Vote Count Veto Accuracy Veto Count</cell></row><row><cell>basic-stats</cell><cell>0.958</cell><cell>5141</cell><cell>1</cell><cell>252380</cell></row><row><cell>tokens-stats</cell><cell>0.985</cell><cell>1056</cell><cell>1</cell><cell>77492</cell></row><row><cell>grammar-stats</cell><cell>0.980</cell><cell>2576</cell><cell>1</cell><cell>89085</cell></row><row><cell>slang-words</cell><cell>0.819</cell><cell>94</cell><cell>0.997</cell><cell>9277</cell></row><row><cell>pronoun</cell><cell>-</cell><cell>0</cell><cell>1</cell><cell>85</cell></row><row><cell>stop-words</cell><cell>0.532</cell><cell>1924</cell><cell>0.998</cell><cell>107544</cell></row><row><cell>intro-outro</cell><cell>0.826</cell><cell>2101</cell><cell>0.998</cell><cell>102431</cell></row><row><cell>pure-unigrams</cell><cell>0.995</cell><cell>186</cell><cell>0.999</cell><cell>35457</cell></row><row><cell>bigrams</cell><cell>0.999</cell><cell>6239</cell><cell>1</cell><cell>281442</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="1,144.73,657.93,312.03,6.31"><p>https://knowminer.at/svn/opensource/projects/pan2011/trunk</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,144.73,602.34,156.01,6.31"><p>http://maxent.sourceforge.net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="2,144.73,613.49,150.63,6.31"><p>http://snowball.tartarus.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="2,144.73,623.80,171.36,7.77"><p>All resources have been crawled on 2011-05-12</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="2,144.73,635.63,156.01,6.31"><p>http://piology.org/smiley.txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="2,144.73,646.78,204.43,6.31"><p>http://www.noslang.com/dictionary/full</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6" coords="2,144.73,657.93,193.67,6.31"><p>http://www.noswearing.com/dictionary</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>The <rs type="institution">Know-Center</rs> is funded within the <rs type="programName">Austrian COMET Program</rs> under the auspices of the <rs type="funder">Austrian Ministry of Transport, Innovation and Technology</rs>, the <rs type="funder">Austrian Ministry of Economics and Labor</rs> and by the <rs type="funder">State of Styria</rs>. COMET is managed by the <rs type="funder">Austrian Research Promotion Agency FFG</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rJ4GnmE">
					<orgName type="program" subtype="full">Austrian COMET Program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.61,646.01,278.54,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,198.77,646.01,67.26,7.77">Bagging predictors</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,271.74,646.01,48.05,7.77">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996-08">August 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,657.08,256.12,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,198.77,657.08,55.51,7.77">Random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,259.78,657.08,48.05,7.77">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001-10">October 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,397.45,326.52,7.77;10,150.95,408.41,225.62,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,366.21,397.45,102.93,7.77;10,150.95,408.41,68.45,7.77">Liblinear: A library for large linear classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,224.92,408.41,73.96,7.77">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,419.00,336.26,7.77;10,150.95,429.95,295.77,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,426.59,419.00,52.28,7.77;10,150.95,429.95,96.28,7.77">The weka data mining software: an update</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,253.22,429.95,89.43,7.77">SIGKDD Explor. Newsl</title>
		<imprint>
			<date type="published" when="2009-11">November 2009</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,440.54,323.26,7.77;10,150.95,452.34,285.13,6.31" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,185.33,440.54,78.73,7.77">Authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
		<ptr target="http://portal.acm.org/citation.cfm?id=1373450.1373451" />
	</analytic>
	<monogr>
		<title level="j" coord="10,269.58,440.54,86.22,7.77">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="233" to="334" />
			<date type="published" when="2006-12">December 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,462.08,325.07,7.77;10,150.95,473.04,322.98,7.77;10,150.95,484.00,193.10,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,238.77,462.08,228.91,7.77;10,150.95,473.04,36.90,7.77">Efficient linear text segmentation based on information retrieval techniques</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,205.99,473.04,267.94,7.77;10,150.95,484.00,115.69,7.77">MEDES &apos;09: Proceedings of the International Conference on Management of Emergent Digital EcoSystems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="167" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,494.58,331.65,7.77;10,150.95,505.54,302.62,7.77;10,150.95,516.50,319.39,7.77;10,150.95,527.46,65.82,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,238.77,494.58,235.49,7.77;10,150.95,505.54,39.04,7.77">German Encyclopedia Alignment Based on Information Retrieval Techniques</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,150.95,516.50,204.10,7.77">Research and Advanced Technology for Digital Libraries</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Rauber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Frommholz</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="315" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,538.04,327.94,7.77;10,150.95,549.00,309.55,7.77;10,150.95,559.96,76.59,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,277.51,538.04,193.04,7.77;10,150.95,549.00,160.08,7.77">KCDC: Word Sense Induction by Using Grammatical Dependencies and Sentence Phrase Structure</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Muhr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,328.99,549.00,93.46,7.77">Proceedings of SemEval-2</title>
		<meeting>SemEval-2<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,570.54,332.44,7.77;10,150.95,581.50,308.47,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,247.09,570.54,109.58,7.77">Accurate unlexicalized parsing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,362.48,570.54,112.56,7.77;10,150.95,581.50,235.25,7.77">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -ACL &apos;03</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics -ACL &apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,592.08,333.59,7.77;10,150.95,603.04,167.59,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,324.57,592.08,151.25,7.77;10,150.95,603.04,80.39,7.77">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,249.31,603.04,43.09,7.77">LREC 2006</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,613.63,334.36,7.77;10,150.95,624.58,107.82,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,251.32,613.63,60.84,7.77">Okapi at TREC-4</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,331.26,613.63,145.34,7.77;10,150.95,624.58,39.61,7.77">Proceedings of the Fourth Text Retrieval Conference</title>
		<meeting>the Fourth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="73" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,635.17,325.81,7.77;10,150.95,646.13,132.10,7.77;10,150.95,657.93,285.13,6.31" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,207.24,635.17,182.01,7.77">A survey of modern authorship attribution methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<ptr target="http://portal.acm.org/citation.cfm?id=1527090.1527102" />
	</analytic>
	<monogr>
		<title level="j" coord="10,395.57,635.17,72.48,7.77;10,150.95,646.13,31.00,7.77">J. Am. Soc. Inf. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009-03">March 2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
