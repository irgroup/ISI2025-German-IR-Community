<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,216.23,116.95,182.89,12.62">Classifying Patent Images</title>
				<funder ref="#_TucG4ED">
					<orgName type="full">Austrian Research Promotion Agency (FFG)</orgName>
				</funder>
				<funder ref="#_tHwDbMt">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.79,154.62,78.51,8.74"><forename type="first">Roland</forename><surname>Mörzinger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">JOANNEUM RESEARCH DIGITAL -Institute for Information and Communication Technologies</orgName>
								<address>
									<addrLine>Steyrergasse 17</addrLine>
									<postCode>8010</postCode>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.86,154.62,57.68,8.74"><forename type="first">Andras</forename><surname>Horti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">JOANNEUM RESEARCH DIGITAL -Institute for Information and Communication Technologies</orgName>
								<address>
									<addrLine>Steyrergasse 17</addrLine>
									<postCode>8010</postCode>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.09,154.62,74.30,8.74"><forename type="first">Georg</forename><surname>Thallinger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">JOANNEUM RESEARCH DIGITAL -Institute for Information and Communication Technologies</orgName>
								<address>
									<addrLine>Steyrergasse 17</addrLine>
									<postCode>8010</postCode>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,384.93,154.62,61.03,8.74"><forename type="first">Naeem</forename><surname>Bhatti</surname></persName>
							<email>bhatti@caa.tuwien.ac.at</email>
							<affiliation key="aff1">
								<orgName type="institution">Vienna University of Technology Institute for Computer-Aided Automation</orgName>
								<address>
									<addrLine>Favoritenstr. 9-11/183</addrLine>
									<postCode>1040</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.05,166.58,64.78,8.74"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
							<email>hanbury@ifs.tuwien.ac.at</email>
							<affiliation key="aff2">
								<orgName type="institution">Vienna University of Technology Institute for Software Technology and Interactive Systems</orgName>
								<address>
									<addrLine>Favoritenstr, 9-11/188</addrLine>
									<postCode>1040</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,216.23,116.95,182.89,12.62">Classifying Patent Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E7AA58A1ABC8ED82200081E696E9EF72</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>patent</term>
					<term>image</term>
					<term>classification</term>
					<term>technical drawings</term>
					<term>SVM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report presents the work carried out for the image classification task in the course of the CLEF-IP 2011 competition. Based on the visual content, patent images are automatically classified into several drawing types, such as abstract drawings, tables, flow chart and graphs. For that purpose, a series of SVM classifiers, multi-modal fusion schemes and a variety of content-based low-level features for black and white images were used. The overall reported performance was promising. Our best runs achieved a true positive rate of over 66% and the reported average area under curve is over 0.9.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There are many different types of images in patents, such as technical drawings, diagrams, photos, flow charts and graphs <ref type="bibr" coords="1,341.38,525.61,9.96,8.74" target="#b4">[5]</ref>. In patents, these images are linked to the text through references which usually only contain the label 'Fig. <ref type="bibr" coords="1,473.30,537.56,3.64,8.74">'</ref>, see examples in Figure <ref type="figure" coords="1,241.30,549.52,3.87,8.74" target="#fig_0">1</ref>. In many patent examination tasks, it is important to focus an analysis on a specific type of image. This information is frequently not available in the patent text. The automatic classification of the drawing type of patent images is helpful for restricting the search to relevant figures. For example, shape-based similarity search for shapes with Gaussian distribution can be automatically restricted to all images with graphs and all other image types such as abstract drawings are disregarded in the search process. Moreover, classification results from automatic content-based analysis can be used to validate text-based classification of the drawing type. This paper presents methods for automatic content-based classification of patent images into several drawing types (classes) for the image classification task (IMG CLS) in CLEF-IP 2011. The aim of the image classification task is to automatically classify the type of patent images based on their visual content. Manually classified and checked data is provided for training, and the long term aim is, based on these training data, to make it possible to reliably classify the millions of images in patents. It is required to classify images into these 9 classes: abstract drawing, chemical structure, program listing (code), gene sequence (dna), flow chart, graph, math formula, table and character (symbol). This paper describes the work done for producing the results for the image classification challenge. The rest of this paper is organized as follows: Section 2 describes the used content-based features in detail, Section 3 outlines the classification process. Results are presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Features for content-based classification</head><p>The content-based features described in this section are the basis for the image classification. All features were extracted globally for each image in the training and test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Binary Patterns (LBP)</head><p>The LBP <ref type="bibr" coords="2,329.33,645.16,10.52,8.74" target="#b2">[3]</ref>  The range of all feature vectors in the training set was linearly scaled between 0 and 1. The feature values in the test set were rescaled accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Semi-supervised classification</head><p>Our approach to patent image classification is based on training support vector machines (SVMs) since they had achieved satisfactory performance in related tasks over the past few years. The challenge of classifying a patent image into one of the 9 classes, see Section 1, was regarded as a sest of two-class problems (i.e. 'flow chart' or 'no flow chart' and 'graph' or 'no graph'), where for each problem the positive and negative examples were extracted from an existing annotated training set. The maximum of the scores yields the final classification for an image. Since for all classes there were more negative examples than positive examples, the SVM training data for a specific class was composed of all its positive annotations with an equal number of negative annotations randomly selected from the remaining classes. The training data, contains between 310 (for flow charts) and 5983 (for dna) training images per class. For better comparability we assured that the random selection produced the same annotations across different runs. In total, 36 SVMs (9 classes * 4 feature sets) were produced using the LIBSVM software package <ref type="bibr" coords="3,297.39,566.98,9.96,8.74" target="#b0">[1]</ref>. We adopted the Gaussian RBF kernel function. Due to limited time and computational power, only half of SVMs were tuned by grid search with cross-validation in order to select the best choice of the parameters C and γ.</p><p>The list below describes the specific configuration of the 8 runs produced. For runs with a single modality, only one of the previously mentioned content-based features was chosen. The other 4 runs apply various simple late fusion methods on the output of the base classifiers. Late fusion first reduces unimodal features to separately learned scores, then these scores are integrated to produce final scores for the classes <ref type="bibr" coords="4,231.13,143.90,9.96,8.74" target="#b3">[4]</ref>.</p><p>arcturus Run uses EH feature set. vega Run uses OCR feature set. alphacentauri Run uses LBP feature set. procyon Run uses BIF feature set. betelgeuse Maximum probability of the scores obtained in the runs using the feature sets OCR, EH and LBP. sirius Joint probability (product) of the scores obtained in the runs using the feature sets OCR, EH and LBP. canopus Mean probability of the scores obtained in the runs using the feature sets OCR, EH and LBP. rigel Joint probability (product) of the scores obtained in the runs using the feature sets OCR, EH and LBP. For weighting each score is multiplied by its absolute difference from the mean classification scores for the image. This puts emphasis on cases where an image has a high score for a single class rather than similar scores for all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Evaluation</head><p>In order to get a first impression of the image classification task, exemplary classification results from the test data are presented in Figure <ref type="figure" coords="4,401.11,405.83,3.87,8.74" target="#fig_3">5</ref>. For each of the 9 classes (from top to bottom row: abstract drawing, chemical, code program, gene sequence/dna, flow chart, graph, maths, table, character/symbol) the 5 images with highest classification scores are shown. The images with chemical structures, mathematical formulas and symbols are apparently easy to discriminate. On the contrast, tables and code programs seem visually similar. The last image in the row with gene sequences was incorrectly classified. The second image in the row of graphs shows an abstract drawing and was also misclassified which is possibly due to its graph like curved structure. The analysis has to deal with images with multiple figures (of the same class in our case), see last image in row with graphs. Another challenge is the variable and unknown orientation of the images. As can be seen in the row with tables and graphs, some of the images are rotated. The test data consists of 1000 unclassified images. For each of the images the type of the image was classified. Figure <ref type="figure" coords="4,333.01,561.47,4.98,8.74" target="#fig_1">2</ref> shows the performance per run and class using the Area under Curve (AUC) and True Positive Rate (TPR) measures. From the submitted 8 runs, 5 runs achieved satisfactory performance indicated by an AUC value of 0.9 an above. The best run according to the TPR measure is alphacentauri with an accuracy of 66.3% and AUC of 0.96, slightly outperforming the runs with score fusion (TPR between 62.4% and 65.3%). The good performance is obviously due to the feature LBP. Interestingly, a fusion with other features did not improve the classification results on average. As can be seen in Figure <ref type="figure" coords="4,215.94,657.11,3.87,8.74" target="#fig_1">2</ref>, the fusion with OCR and EH was only beneficial for the classes flow chart (improvement from 44.7% to 82.8%), abstract drawing (from 54.2% to 75.3%) and table (from 68.7% to 76.5%). The single modalities using OCR, EH, or BIF only did not provide satisfactory results on average. Most or even all of the characters/symbols were correctly classified even when applying runs that use features that generally came of badly.</p><p>A detailed view on the performance of the run alphacentauri is shown in Figure <ref type="figure" coords="5,151.31,596.47,3.87,8.74">3</ref>. Very promising results could be achieved for characters/symbols, chemical structures and math formulas (AUC value over 0.95). The image types abstract drawing, flow charts and graphs are difficult cases mainly due to their intra-class variance.</p><p>A confusion matrix for run alphacentauri is given in Figure <ref type="figure" coords="5,405.94,645.16,3.87,8.74">4</ref>. Actuals belong on the side of the confusion matrix and predictions are across the top. For the classifier many image types (all but chemical, maths and characters) seem to be difficult to distinguish from tables. Similarly, graphs were difficult to classify.</p><p>No or hardly any confusion was attained for the types chemical, maths and characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper presented the experiments for our participation in the CLEF-IP image classification challenge. The type of images in patents was automatically classified by using SVM classifiers and simple multi-modal fusion schemes. A variety of content-based low-level features for black and white images were used. Generally, training the SVM models and in particular the parameter tuning is a computationally expensive process and must not be neglected. The perfor- mance of the classifiers tested on 1000 patent images was promising. Our best runs achieved a true positive rate of over 66% and the reported average area under curve is over 0.9 for 4 of the 8 submitted runs. Some classes have been identified better by using only a single input feature and others by late fusion. Consequently, for different classes different features and fusion methods should be applied.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,486.06,345.82,7.89;2,134.77,497.05,195.97,7.86;2,309.21,364.21,114.12,92.14"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of four types of images found in patents: (a) abstract drawing, (b) chemical structure, (c) flow chart and (d) graph.</figDesc><graphic coords="2,309.21,364.21,114.12,92.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,477.43,345.82,7.89;5,134.77,488.41,159.07,7.86;5,154.74,116.84,305.88,345.82"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Plots showing the Area Under Curve (AUC) and True Positive Rate (TPR) per class and run. Best viewed in color.</figDesc><graphic coords="5,154.74,116.84,305.88,345.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,134.77,307.35,345.82,7.89;6,134.77,318.34,319.06,7.86;6,138.78,338.51,176.13,90.71"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Detail of ROC for run alphacentauri which uses the LBP feature. The different classification performances for the 9 classes are indicated. Best viewed in color.</figDesc><graphic coords="6,138.78,338.51,176.13,90.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,134.77,644.29,345.83,7.89;7,134.77,655.27,250.90,7.86;7,182.29,507.12,252.45,83.88"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Classification examples. For 9 different classes (rows) the 5 images with the highest classification scores are shown. Best viewed magnified.</figDesc><graphic coords="7,182.29,507.12,252.45,83.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,151.70,645.16,328.89,20.69"><head></head><label></label><figDesc>The edge histogram descriptor<ref type="bibr" coords="3,426.50,155.77,12.42,8.74" target="#b1">[2]</ref> represents the spatial distribution of five types of edges, namely four directional edges and one non-directional edge. We use a global histogram generated directly from the local edge histograms of 4x4 sub-images. The final descriptor comprises 80 features values. Optical Character Recognition (OCR) The following feature values were obtained by applying an optical character recognition toolkit 4 on the images: font size, number of of text blocks, ratio between area of text blocks and image area image size, number of 'fig' occurrences, number of 'tab' occurrences, number of digits and image orientation. Binary Image Features (BIF) For each of the images a descriptor with total</figDesc><table coords="2,151.70,645.16,328.89,20.69"><row><cell>borhood of each pixel and considers the result as a binary number. The</cell></row><row><cell>descriptor is a histogram of 8-digit binary numbers which yields 256 feature</cell></row><row><cell>values.</cell></row><row><cell>MPEG-7 Edge Histogram (EH)</cell></row><row><cell>is a simple yet very efficient tex-</cell></row><row><cell>ture operator which labels the pixels of an image by thresholding the neigh-</cell></row></table><note coords="3,151.70,287.11,328.89,8.74;3,151.70,299.07,328.89,8.74;3,151.70,311.02,269.51,8.74"><p>length of 12 was computed. It comprises the image's Euler number, mean, standard deviation, variance, skewness, kurtosis, perimeter, area, number of (4-and 8-) connected components, thinness ratio and density.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="3,144.73,658.44,155.34,7.47"><p>http://www.leadtools.com/sdk/ocr/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="8,144.73,658.44,145.93,7.47"><p>http://www.joanneum.at/?id=3922</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">Austrian Research Promotion Agency (FFG)</rs> <rs type="projectName">FIT-IT</rs> project <rs type="projectName">IMPEx 5 Image Mining for Patent EXploration</rs> (No. <rs type="grantNumber">825846</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_TucG4ED">
					<orgName type="project" subtype="full">FIT-IT</orgName>
				</org>
				<org type="funded-project" xml:id="_tHwDbMt">
					<idno type="grant-number">825846</idno>
					<orgName type="project" subtype="full">IMPEx 5 Image Mining for Patent EXploration</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,302.99,342.24,7.86;8,146.91,313.95,277.66,9.85" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,262.19,302.99,189.80,7.86">LIBSVM: a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,323.91,342.25,7.86;8,146.91,334.87,94.33,7.86" xml:id="b1">
	<monogr>
		<idno>Standard No. ISO/IEC n15938</idno>
		<title level="m" coord="8,146.91,323.91,32.97,7.86">MPEG-7</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Multimedia content description interface</note>
</biblStruct>

<biblStruct coords="8,138.35,344.83,342.24,7.86;8,146.91,355.79,310.44,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,318.27,344.83,162.32,7.86;8,146.91,355.79,193.86,7.86">A comparative study of texture measures with classification based on feature distributions</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996-01">January 1996</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="51" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,365.75,342.24,7.86;8,146.91,376.71,333.68,7.86;8,146.91,387.67,333.68,7.86;8,146.91,398.63,72.69,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,384.11,365.75,96.48,7.86;8,146.91,376.71,104.60,7.86">Early versus late fusion in semantic video analysis</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,272.22,376.71,208.38,7.86;8,146.91,387.67,179.42,7.86">MULTIMEDIA &apos;05: Proceedings of the 13th annual ACM international conference on Multimedia</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="399" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,408.59,342.24,7.86;8,146.91,419.55,333.68,7.86;8,146.91,430.51,228.17,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,217.28,419.55,263.31,7.86;8,146.91,430.51,30.77,7.86">Towards content-based patent image retrieval: A framework perspective</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moumtzidou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pianta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,185.62,430.51,104.36,7.86">World Patent Information</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="94" to="106" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
