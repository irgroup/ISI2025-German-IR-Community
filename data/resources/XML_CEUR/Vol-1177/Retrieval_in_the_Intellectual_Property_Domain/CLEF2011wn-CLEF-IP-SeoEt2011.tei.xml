<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.93,151.85,300.97,12.50;1,128.93,169.24,337.47,12.50">CLEF-IP 2011 Working Notes: Utilizing Prior Art Candidate Search Results for Refined IPC Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,214.13,207.29,68.35,9.09"><forename type="first">Hyung-Kook</forename><surname>Seo</surname></persName>
							<email>hkseo@wisenut.co.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Ottogi Center</orgName>
								<address>
									<addrLine>2F, 1009-1 Daechi-Dong, Kangnam-Gu</addrLine>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.35,207.29,56.29,9.09"><forename type="first">Kyouyeol</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ottogi Center</orgName>
								<address>
									<addrLine>2F, 1009-1 Daechi-Dong, Kangnam-Gu</addrLine>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.95,207.29,39.71,9.09"><forename type="first">Jaean</forename><surname>Lee</surname></persName>
							<email>jalee@wisenut.co.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Ottogi Center</orgName>
								<address>
									<addrLine>2F, 1009-1 Daechi-Dong, Kangnam-Gu</addrLine>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.93,151.85,300.97,12.50;1,128.93,169.24,337.47,12.50">CLEF-IP 2011 Working Notes: Utilizing Prior Art Candidate Search Results for Refined IPC Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">81A6EA75F10C31DC9F1DA421AA52B2D9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Prior Art Candidate Search</term>
					<term>IPC Classification</term>
					<term>Document Categorization</term>
					<term>KNN Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the refined IPC classification in the CLEF-IP 2011 task, we constructed classification system with KNN classification which uses PAC (Prior Art Candidate) search results as neighbors. We also slightly modified the neighborhood evaluation. We also furnished a simple PAC search system. We produced some running results both in PAC search and classification, and evaluated our system. Our test showed an improved result in the refined IPC classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.4" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.4" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.4" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.4" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.4" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.4" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.4" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.4" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.4" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.4" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.4" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our lab performed prior art candidate (PAC) search and IPC classification for the Korean Intellectual Property Office. We dealt with domestic patents in Korea, so linguistic analysis was limited to Korean. However PACs are not limited to domestic patents, and we also have interests in the PAC search of other languages, including English and Japanese.</p><p>We decided to participate in CLEF-IP to share our experience in patent domain and extend our technical coverage and experience to other patents domain like European patent corpus.</p><p>However, we have limited knowledge about European patent structure (except commonly shared fields like claims, IPCs, etc.) and lack experience in linguistic analysis of other languages (except Korean). We also were on a limited time schedules, so we focused our interests on refined IPC classification. But our approach needs PAC results, so we also implemented our PAC system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Test Collection and Topics</head><p>Like other participants in CLEF-IP 2011, we used only test the data collection which comprises extracts of the MAREC dataset by IRF. We only used this collection in the entire process for producing running result of the tasks we participated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">CLEF-IP Tasks Participated</head><p>We participated in following three tasks in CLEF-IP 2011 <ref type="bibr" coords="2,359.35,171.99,10.95,9.09" target="#b0">[1]</ref>:</p><p>1. Prior Art Candidate (PAC) Search 2. IPC Classification: up to subclass level 3. Refined IPC Classification: up to subgroup level, with given subclass value As we stated before, our ultimate interests lie in refined IPC classification, and there were small efforts to improve PAC search results. This is described in the next chapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approaches to Refined Classification</head><p>Classification of a patent up to sub-class degree is quite difficult task for model-based classification, because of sparseness of training samples in that level. So, we implemented indirect (and simple) method, that implements KNN-like classification using PAC search results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Existing Classification Approach</head><p>In patent classification, Kostar et al. <ref type="bibr" coords="2,289.05,419.82,11.73,9.09" target="#b1">[2]</ref> proposed a method using the winnow algorithm. Winnow is a mistake driven learning algorithm that computes for each category a vector of weights for separating between relevant and irrelevant patent <ref type="bibr" coords="2,456.38,442.82,10.71,9.09" target="#b2">[3]</ref>.</p><p>In this study, they obtained F-measure of around 68% (multi-categorization). This result was measured with a customized success criterion and relatively few documents. Fall et al. <ref type="bibr" coords="2,177.98,477.32,11.78,9.09" target="#b3">[4]</ref> applied various machine learning algorithms to patent classification. The machine learning algorithms were Naive Bayes(NB), SNoW, support vector machines(SVM) and k-nearest neighbor(KNN) algorithms. Here SNoW is a variation of the winnow algorithm. They investigated useful patent document fields to index, and defined three measures of categorization success. As a result, they presented the best precision of 41%(SVM), 39%(NB), 33%(NB) and 36%(SNoW) when the first 300 words are indexed at subclass level. In first three guesses KNN achieved the best precision of 62% and all categories SVM achieved the best precision of 48%. They <ref type="bibr" coords="2,462.16,557.82,11.84,9.09" target="#b4">[5]</ref> also presented a customized language-independent text classification system for categorization.</p><p>When the amount of training data increses, a model-based system has increased feature scale and time complexity. In order to reduce the feature scale, some of researchers limited the number of documents, term selection, and length of the documents. To reduce time complexity, other have attempted instance-based learning such as KNN. It first selects K samples when the similarity values are sorted in descending order, and then determines the categories of test sample with class mapping method. It makes a trade-off between effectiveness and time complexity.</p><p>W. Wang et al. <ref type="bibr" coords="2,206.93,672.85,10.73,9.09" target="#b5">[6]</ref> reported their experience in the NTCIR-7 Patent Mining Task(MT) to classify patent documents according to the IPC taxonomy. Their approach is based on the KNN algorithm using cosine and Euclid distance similarity. And T. Xiao et al. <ref type="bibr" coords="3,200.02,160.99,10.74,9.09" target="#b6">[7]</ref> described their methodology that are used KNN and re-ranking models. They achieved a mean average precision (MAP) of 48.86% when classifying according to the subgroup level. Also <ref type="bibr" coords="3,282.35,183.99,11.73,9.09" target="#b7">[8]</ref> reported result of their experiments on the automatic assignment of patent classification to research paper abstracts. The results showed the best precision of 50.62% (MAP) when using formal run data and particular query group.</p><p>Lately, Y. Cai et al. <ref type="bibr" coords="3,226.15,229.99,11.73,9.09" target="#b8">[9]</ref> presented a KNN text categorization method based on shared nearest neighbor, effectively combining the BM25 similarity calculation method and the Neighborhood Information of samples in the NTCIR-8 workshop. BM25 is a bag-of-words retrieval function, combines the word frequency and document frequency, balances the length of the document, and is a highly efficient similarity calculation method. They conducted a comparison experiment on Japanese corpus and English corpus provided by the National Institute of Informatics from 1993 to 2002, using the basic KNN and KNN based on shared nearest neighbors. Compared to KNN method, KNN+SNN method showed 72.12% precision (about 0.03) higher at subclass levels and 36.93% precision at subgroup levels on English corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Our IPC Classification Approach</head><p>As we know about KNN Classification, K nearest neighbors are K documents most similar with the given query document to be classified <ref type="bibr" coords="3,344.45,412.52,15.44,9.09" target="#b9">[10]</ref>.</p><p>So it can be easily connected with search results. That is, top K search results with query document can be directly adopted in KNN classification, and one system used this simple method, though it was not so competitive with model-based classification algorithms <ref type="bibr" coords="3,170.02,458.52,15.45,9.09">[11]</ref>.</p><p>In fact, at subgroup level we need about 70,000 categories to be trained, and most classification models suffer from sparseness of training documents and problems in system memory (for loading models) and processing time (training or classification itself). But according to our experience in Korean patent domain, KNN classification with PAC search showed quite good quality in classification of subgroup level.</p><p>So we tried to construct refined IPC classification system utilizing PAC results. (And IPC classification up to subclass level as well. In fact, we paid not so much attention in the optimization or improvement in subclass level, because of limited time)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">PAC Search Approach</head><p>We implemented a PAC search system using only selected weighted keywords which are extracted from major content fields (title of invention, abstract, description, claims).</p><p>We added two additional efforts in PAC search to improve our results. They are the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Removing Non-Content Words</head><p>A Document will be represented by a set of words which consist of content-word and functional word. After POS tagging, we try to remove functional words and stop word. While stop words are controlled by human input and cannot be automated, we can algorithmically find words which don't describe a particular document. (non-content words).</p><p>The standard probabilistic model for the distribution of a certain type of event over units of a fixed size is the Poisson distribution.</p><p>(1)</p><p>The most common model of the Poisson distribution in IR, the parameter &gt; 0 is the average number of occurrences of per document: that is, where is the collection frequency and N is the total number of documents in the collection. And we can get an approximation of DF by . As this model assumes independence between term occurrences, its estimations are good for non-content words. <ref type="bibr" coords="4,154.21,366.32,16.75,9.09" target="#b10">[12]</ref> Algorithm:</p><p>1. Calculate : collection frequency of i / N ( N is total number of document in the collection )</p><p>2. Calculate expected document frequency by Poisson distribution:</p><p>3. Get the overestimation value: expected document frequency(i)/df(i)</p><p>Parameter:</p><p>1. Document Frequency Rate: Percentage of the number of document in the collection 2. Lower overestimation criteria and upper overestimation criteria</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result:</head><p>According to parameter tests, our system (in Korean) showed best result under observed document frequency of 0.05%, lower overestimation criteria of 0.9 and upper overestimation criteria of 1.0.</p><p>In the CLEF-IP 2011, we'll fix lower overestimation criteria as 0.9, and change upper overestimation criteria from 1.0 to 1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extracting Co-Occurrence Terms</head><p>A null hypothesis is often stated by saying the parameter Θ is in a specified subset 0 of the parameter space .</p><p>(2)</p><p>Likelihood ratios are an approach to hypothesis testing. The likelihood function is is a function of the parameter θ with x held fixed at the value that was actually observed, i.e., the data. The likelihood ratio test statistic is <ref type="bibr" coords="5,412.38,172.69,16.62,9.09" target="#b12">[14]</ref> (3)</p><p>In applying the likelihood ratio test to collocation discovery, we examine the following two alternative explanations for the occurrence frequency of a bigram <ref type="bibr" coords="5,154.02,232.99,16.74,9.09" target="#b11">[13]</ref> (4)</p><p>We used the usual maximum likelihood estimates for p, p 1 and p 2 and write c 1 , c 2 , and c 12 for the number of occurrences of w 1 , w 2 and w 1 w 2 in the corpus:</p><p>(5) Now we can get likelihood ratio by assuming a binomial distribution and then following is asymptotically distributed <ref type="bibr" coords="5,458.98,344.52,11.62,9.09" target="#b5">(6)</ref> In the CLEF-IP 2011, we used confidence level of α=99.9. We tried both with cooccurrence terms and without them in the runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Setup</head><p>We implemented our system according to procedures to be explained in this chapter. We first extracted weighted keywords from each patent xml file provided in CLEF-IP 2011 corpora, combined them in several bulk files, and indexed them. For indexer and searcher, we used Lucene. We implemented a simple searcher program implemented in Java, and a final classifier program applying search results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Architecture</head><p>Our system is illustrated in the following architecture diagram. It's very similar with traditional document search system. We used our in-house English POS tagger for base English analysis.</p><p>For translating other languages like French or German into English, we used open online translation service, MyMemory <ref type="bibr" coords="6,278.85,442.72,15.42,9.09" target="#b13">[15]</ref>.</p><p>We used Lucene 3.1.0 for the base search engine, and for accessing this engine, we wrote simple java applications for indexing and searching. We also wrote an application for classification which calls the searcher application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preprocessing and Indexing</head><p>Basically, we used a nearly identical preprocessing system except this time we used English POS tagger instead of Korean one. We selected only one XML document among various versions of a same patent to guarantee uniqueness of the patent, so that there's no patent document with same application number in the entire index system. After this process, we got 1,331,182 unique patents in the EP(European Patents), and 437,987 in the WO(WIPO Patents)</p><p>During this procedure we also translated content fields with the online translation service. After some sample runs, we discovered that translating full text would consume a lot of time (and may lead to missing the CLEF-IP 2011 deadline), so we only translated the abstract and select sentences(about 2048 characters) in other content fields.</p><p>For feature extraction, we used these content fields: Title of Invention, Abstract, Description, and Claims. We also extracted some co-occurrence terms and select up to 5 terms with extracted features. We finally produced bulk files with features with and without co-occurrence terms (we call them co-terms for simplification) for indexing. And we produced two separated indices, to analyze the effect of co-terms used in the search.</p><p>We also preprocessed patents used as topics (queries). In this case, translation with full content was conducted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prior Art Candidate Search</head><p>We produced a total of 8 runs for search results. First 4 runs target index without coterms, and other 4 runs target index with co-terms. In each group, we changed upper overestimation threshold for non-content words removal from 1.0 to 1.3 (1.0, 1.1, 1.2, 1.3) resulting in 4 runs for each group. We produced 1,000 results for every patent query. The results are produced without lower threshold in the weight of search results, so in most cases, our search results per one document were almost 1,000 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classification</head><p>Because we used search results of PAC search, we also produced 8 runs for each classification task.</p><p>For KNN classification, we set K as 1,000, because we produced 1,000 search results per a query.</p><p>In fact, we have observed that combining reciprocal of search results than just counting the number of patents per category shows much better results. It's similar to adoption of weighting in the average precision <ref type="bibr" coords="7,333.99,445.52,15.34,9.09" target="#b14">[16]</ref>. We basically adopted this improved weighting scheme in the KNN classification results we got.</p><p>To verify this intuition, we also ran the base condition and compared this result with improved weighting scheme applied in the CLEF-IP 2011 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head><p>Following is a simple report on our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PAC Search Results</head><p>We simply show the result of our runs along with best runs of CLEF-IP 2011. We got slightly improved result when co-terms are applied. And the differences in upper threshold in the non-content words extraction made no special differences.</p><p>And due to multilingual issues, our result showed quite low quality. (It's partially displayed in English results that show quite narrower gaps with the top runners) We'll try to find alternatives to overcome this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">IPC Classification Results</head><p>We also got the results of our classification runs and compared them with ones from the other participant. Because we do not use model-based methods, our result showed lower result in the precision. We also didn't limit the score of classification result; if we tune the score thresholds, it's expected that we may produce much better results.  Whie simple comparison is quite dangerous, our system showed quite improved results in this track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RUN_NAME</head><p>And as we stated before, we compared the new, refined weighting scheme (which is applied in CLEF-IP 2011) with the base one. Following table shows that result. Refined scheme showed far better results than the base scheme, especially in precision. MAPs were also dramatically improved. (Note that precision and recall were micro-averaged, so they're quite different from our reported values)</p><p>Considering the result of <ref type="bibr" coords="9,244.25,628.44,10.80,9.09" target="#b8">[9]</ref>, our result is very promising, because precision in suggesting one IPC classification result showed almost the same or improved quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>We implemented a simple refined IPC classification system utilizing search results provided from PACS system. Though our PACS system showed rather lower performance than those of other labs, our refined classification results based on the search results of our system showed quite good performance, especially when the criteria for category selection is changed.</p><p>We left some challenges as future work. First, we can improve PACS search results. For example, we didn't set threshold in the score, only the maximum number of results. And we had a major problem in search results due to multilingual defects of our system. We may improve these problems at the next workshop.</p><p>Second, we can adapt model-based classification up to subclass level. In fact, it's true that model-based classification method works well up to subclass level, so our IPC classification system should use classification model like SVM does.</p><p>Finally, we may optimize weighting factor of ranked documents in the refined IPC classification. As just using reciprocal of ranks in the search results improved the refined classification, it's expected that adopting more sophisticated weighting factor in the KNN can produce improved classification results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,136.23,396.72,135.64,9.09;6,271.13,259.51,69.94,87.63"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Entire system architecture.</figDesc><graphic coords="6,271.13,259.51,69.94,87.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,124.82,624.63,331.49,8.18"><head>Table 1 .</head><label>1</label><figDesc>PAC search results compared with best runs of other CLEF-IP 2011 participants.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,124.82,382.31,333.74,194.79"><head>Table 2 .</head><label>2</label><figDesc>IPC classification results compared with other CLEF-IP 2011 participant.</figDesc><table coords="8,129.33,410.30,329.23,166.80"><row><cell>RUN_NAME</cell><cell cols="2">set_P set_recall</cell><cell>set_F_1.0</cell></row><row><cell>NIJMEGEN.RUN_ADMWCIT_CLS1</cell><cell>0.5379</cell><cell>0.8563</cell><cell>0.6168</cell></row><row><cell>NIJMEGEN.RUN_ADMW_CLS1</cell><cell>0.5436</cell><cell>0.8506</cell><cell>0.6186</cell></row><row><cell>WISENUT.WISENUT_R1_BASE_CLS1</cell><cell>0.2867</cell><cell>0.838</cell><cell>0.4021</cell></row><row><cell>WISENUT.WISENUT_R2_BASE_10_CLS1</cell><cell>0.2871</cell><cell>0.8389</cell><cell>0.4027</cell></row><row><cell>WISENUT.WISENUT_R3_BASE_20_CLS1</cell><cell>0.2869</cell><cell>0.8384</cell><cell>0.4024</cell></row><row><cell>WISENUT.WISENUT_R4_BASE_30_CLS1</cell><cell>0.2871</cell><cell>0.8387</cell><cell>0.4027</cell></row><row><cell>WISENUT.WISENUT_R5_CO_CLS1</cell><cell>0.2882</cell><cell>0.8366</cell><cell>0.4027</cell></row><row><cell>WISENUT.WISENUT_R6_CO_10_CLS1</cell><cell>0.2883</cell><cell>0.8371</cell><cell>0.4029</cell></row><row><cell>WISENUT.WISENUT_R7_CO_20_CLS1</cell><cell>0.2885</cell><cell>0.8376</cell><cell>0.4032</cell></row><row><cell>WISENUT.WISENUT_R8_CO_30_CLS1</cell><cell>0.2884</cell><cell>0.8376</cell><cell>0.4031</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,138.57,149.26,344.08,81.74"><head></head><label></label><figDesc>Finally, our refined IPC classification results are displayed in the table below. If more labs participated in this track, we may get a better perspective on our quality.</figDesc><table coords="8,138.57,149.26,344.08,81.74"><row><cell>4.3</cell><cell cols="5">Entire Language MAP SET_P SET_recall recall_5 recall_10 recall_20 Refined IPC Classification Results</cell><cell cols="4">English Only MAP SET_P SET_recall recall_5 recall_10 recall_20</cell></row><row><cell></cell><cell>CHEMNITZ.CUT_UHI_CLEFIP_BOW</cell><cell>0.0914 0.0037</cell><cell>0.4318 0.0896</cell><cell>0.1251</cell><cell>0.1635</cell><cell>0.1009 0.0049</cell><cell>0.5233 0.0956</cell><cell>0.1401</cell><cell>0.1921</cell></row><row><cell></cell><cell>HYDERABAD. TEXTRANK_IDFCITATIONALL</cell><cell>0.097 0.0036</cell><cell>0.3993 0.0932</cell><cell>0.118</cell><cell>0.1489</cell><cell>0.0943 0.0046</cell><cell>0.482 0.0897</cell><cell>0.1237</cell><cell>0.1671</cell></row><row><cell></cell><cell>WISENUT_R1_BASE_PAC</cell><cell>0.0565 0.0028</cell><cell>0.3948 0.0562</cell><cell>0.081</cell><cell>0.1125</cell><cell>0.0836 0.0036</cell><cell>0.4677 0.0812</cell><cell>0.1137</cell><cell>0.1573</cell></row><row><cell></cell><cell>WISENUT_R2_BASE_10_PAC</cell><cell>0.0566 0.0028</cell><cell>0.3949 0.0563</cell><cell>0.081</cell><cell>0.1125</cell><cell>0.0836 0.0036</cell><cell>0.468 0.0812</cell><cell>0.1137</cell><cell>0.1573</cell></row><row><cell></cell><cell>WISENUT_R3_BASE_30_PAC</cell><cell>0.0566 0.0028</cell><cell>0.3949 0.0563</cell><cell>0.0811</cell><cell>0.1126</cell><cell>0.0837 0.0036</cell><cell>0.468 0.0813</cell><cell>0.1137</cell><cell>0.1574</cell></row><row><cell></cell><cell>WISENUT_R4_BASE_30_PAC</cell><cell>0.0567 0.0028</cell><cell>0.3949 0.0565</cell><cell>0.0811</cell><cell>0.1125</cell><cell>0.0837 0.0036</cell><cell>0.468 0.0817</cell><cell>0.1138</cell><cell>0.1574</cell></row><row><cell></cell><cell>WISENUT_R5_CO_PAC</cell><cell>0.0573 0.0028</cell><cell>0.3966 0.0564</cell><cell>0.0809</cell><cell>0.1112</cell><cell>0.0841 0.0036</cell><cell>0.4656 0.0803</cell><cell>0.1126</cell><cell>0.1535</cell></row><row><cell></cell><cell>WISENUT_R6_CO_10_PAC</cell><cell>0.0573 0.0028</cell><cell>0.3966 0.0564</cell><cell>0.081</cell><cell>0.1112</cell><cell>0.0841 0.0036</cell><cell>0.4656 0.0803</cell><cell>0.1126</cell><cell>0.1535</cell></row><row><cell></cell><cell>WISENUT_R7_CO_20_PAC</cell><cell>0.0573 0.0028</cell><cell>0.3966 0.0564</cell><cell>0.081</cell><cell>0.1114</cell><cell>0.0841 0.0036</cell><cell>0.4658 0.0802</cell><cell>0.1127</cell><cell>0.1538</cell></row><row><cell></cell><cell>WISENUT_R8_CO_30_PAC</cell><cell>0.0573 0.0028</cell><cell>0.3966 0.0564</cell><cell>0.0808</cell><cell>0.1115</cell><cell>0.0841 0.0036</cell><cell>0.466 0.0803</cell><cell>0.1123</cell><cell>0.1538</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,124.82,206.79,342.24,171.41"><head>Table 3 .</head><label>3</label><figDesc>Refined IPC classification results compared with other CLEF-IP 2011 participant.</figDesc><table coords="9,129.13,234.07,337.93,144.13"><row><cell>RUN_NAME</cell><cell>set_P</cell><cell>set_recall</cell><cell>set_F_1.0</cell></row><row><cell>NIJMEGEN.RUN_WINNOW_WORDS_CLS2</cell><cell>0.0731</cell><cell>0.0622</cell><cell>0.0609</cell></row><row><cell>WISENUT.WISENUT_R1_BASE_CLS2</cell><cell>0.2928</cell><cell>0.495</cell><cell>0.3326</cell></row><row><cell>WISENUT.WISENUT_R2_BASE_10_CLS2</cell><cell>0.293</cell><cell>0.4951</cell><cell>0.3327</cell></row><row><cell>WISENUT.WISENUT_R3_BASE_20_CLS2</cell><cell>0.293</cell><cell>0.4952</cell><cell>0.3328</cell></row><row><cell>WISENUT.WISENUT_R4_BASE_30_CLS2</cell><cell>0.2928</cell><cell>0.4954</cell><cell>0.3328</cell></row><row><cell>WISENUT.WISENUT_R5_CO_CLS2</cell><cell>0.2925</cell><cell>0.494</cell><cell>0.332</cell></row><row><cell>WISENUT.WISENUT_R6_CO_10_CLS2</cell><cell>0.2926</cell><cell>0.4938</cell><cell>0.3319</cell></row><row><cell>WISENUT.WISENUT_R7_CO_20_CLS2</cell><cell>0.2925</cell><cell>0.4938</cell><cell>0.3319</cell></row><row><cell>WISENUT.WISENUT_R8_CO_30_CLS2</cell><cell>0.2926</cell><cell>0.4943</cell><cell>0.3321</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,124.82,454.11,352.46,120.29"><head>Table 4 .</head><label>4</label><figDesc>Refined IPC classification results comparison between weighting schemes.</figDesc><table coords="9,129.13,482.20,348.15,92.20"><row><cell>Classification</cell><cell>Scheme with Rank</cell><cell></cell><cell></cell><cell>Base Scheme</cell><cell></cell><cell></cell></row><row><cell>Results</cell><cell>P Recall</cell><cell>F1</cell><cell>MAP</cell><cell>P Recall</cell><cell>F1</cell><cell>MAP</cell></row><row><cell>up to 1 result</cell><cell cols="6">0.4453 0.1893 0.2657 0.2476 0.2397 0.1019 0.1430 0.1241</cell></row><row><cell>up to 5 results</cell><cell cols="6">0.2251 0.4606 0.3024 0.4028 0.1470 0.3018 0.1977 0.2202</cell></row><row><cell>up to 10 results</cell><cell cols="6">0.1473 0.5791 0.2348 0.4297 0.1064 0.4204 0.1698 0.2450</cell></row><row><cell>up to 20 results</cell><cell cols="6">0.0934 0.6869 0.1645 0.4433 0.0747 0.5480 0.1314 0.2598</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,128.20,426.41,239.85,8.18" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<title level="m" coord="10,170.33,426.41,120.92,8.18">CLEF-IP 2011: Track Guidelines</title>
		<meeting><address><addrLine>Vienna</addrLine></address></meeting>
		<imprint>
			<publisher>IRF</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.20,436.71,342.26,8.18;10,136.23,447.11,197.18,8.18" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,284.83,436.71,167.46,8.18">Classifying Patent Applications with Winnow</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H A</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Seutter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,136.23,447.11,124.69,8.18">Proceedings Benelearn Conference</title>
		<meeting>Benelearn Conference<address><addrLine>Antwerpen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.20,457.41,342.57,8.18;10,136.23,467.81,334.49,8.18;10,136.23,478.11,24.01,8.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,198.50,457.41,272.27,8.18;10,136.23,467.81,76.09,8.18">Learning Quickly when Irrelevant Attributes Abound: A new Linear-Threshold Algorithm</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Littlestone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,234.19,467.81,66.21,8.18">Machine Learning</title>
		<meeting><address><addrLine>Nethelands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="285" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.20,488.51,342.68,8.18;10,136.23,498.81,334.64,8.18;10,136.23,509.21,44.71,8.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,349.05,488.51,121.83,8.18;10,136.23,498.81,123.84,8.18">Automated Categorization in the International Patent Classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Fall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torcsvari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Benzineb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karetka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,280.45,498.81,72.86,8.18">ACM SIGIR Forum</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.20,519.51,342.47,8.18;10,136.23,529.91,334.50,8.18;10,136.23,540.21,291.94,8.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,345.95,519.51,124.72,8.18;10,136.23,529.91,239.35,8.18">Computer-Assisted Categorization of Patent Documents in the International Patent Classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Fall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Benzineb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guyot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torcsvari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fievet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,398.67,529.91,72.06,8.18;10,136.23,540.21,207.06,8.18">Proceedings of the International Chemical Information Conference (ICIC&apos;03)</title>
		<meeting>the International Chemical Information Conference (ICIC&apos;03)<address><addrLine>Nimes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.20,550.61,342.47,8.18;10,136.23,560.91,258.44,8.18" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,252.85,550.61,217.82,8.18;10,136.23,560.91,51.47,8.18">ICL at NTCIR-7: A Improved KNN Algorithm for Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,205.93,560.91,162.51,8.18">Proceedings of NTCIR-7 Workshop Meeting</title>
		<meeting>NTCIR-7 Workshop Meeting</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.20,571.31,342.56,8.18;10,136.23,581.63,334.44,8.18;10,136.23,592.04,56.20,8.18" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,389.74,571.31,81.02,8.18;10,136.23,581.63,176.38,8.18">KNN and Re-ranking Models for English Patent Mining at NTCIR-7</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,335.25,581.63,135.42,8.18;10,136.23,592.04,29.99,8.18">Proceedings of NTCIR-7 Workshop Meeting</title>
		<meeting>NTCIR-7 Workshop Meeting</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.20,602.34,342.47,8.18;10,136.23,612.73,188.62,8.18" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,227.95,602.34,226.02,8.18">Hitachi Ltd.: NTCIR-7 Patent Mining Experiments at Hitachi</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iwayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,136.23,612.73,162.40,8.18">Proceedings of NTCIR-7 Workshop Meeting</title>
		<meeting>NTCIR-7 Workshop Meeting</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.20,623.04,342.48,8.18;10,136.23,633.43,334.40,8.18;10,136.23,643.73,24.00,8.18" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,228.35,623.04,242.33,8.18;10,136.23,633.43,63.62,8.18">A KNN Research Paper Classification Method Based on Shared Nearest Neighbor</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,221.75,633.43,190.46,8.18">Proceedings of the 8th NTCIR Workshop Meeting</title>
		<meeting>the 8th NTCIR Workshop Meeting</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="336" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.32,654.13,329.00,8.18;10,124.82,664.43,345.83,8.18;10,136.23,674.84,181.78,8.18" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,230.05,664.43,240.60,8.18;10,136.23,674.84,57.44,8.18">Experiments with Citation Mining and Key-Term Extraction for Prior Art Search</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename></persName>
		</author>
		<ptr target="http://en.wikipedia.org/wiki/K" />
	</analytic>
	<monogr>
		<title level="m" coord="10,138.43,654.13,102.78,8.18;10,211.91,674.84,51.43,8.18">k-nearest neighbor algorithm</title>
		<meeting><address><addrLine>Padua</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>CLEF-IP 2010</note>
</biblStruct>

<biblStruct coords="11,132.33,149.29,338.21,8.18;11,136.23,159.59,89.68,8.18" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<title level="m" coord="11,246.22,149.29,201.25,8.18">Foundations of Statistical Natural Language Processing</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.33,169.98,338.33,8.18;11,136.23,180.29,177.53,8.18" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,196.43,169.98,254.95,8.18">Accurate Methods for the Statistics of Surprise and Coincidence</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,136.23,180.29,95.59,8.18">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.33,190.68,330.46,8.18" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<title level="m" coord="11,234.67,190.68,70.31,8.18">Statistical Inference</title>
		<imprint>
			<publisher>Duxbury Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">375</biblScope>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct coords="11,132.33,200.98,167.36,8.18" xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Mymemory</surname></persName>
		</author>
		<ptr target="http://mymemory.translated.net" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.32,211.38,329.59,8.18" xml:id="b14">
	<monogr>
		<ptr target="http://en.wikipedia.org/wiki/Information_retrieval#Average_precision" />
		<title level="m" coord="11,138.41,211.38,64.76,8.18">Average Precision</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
