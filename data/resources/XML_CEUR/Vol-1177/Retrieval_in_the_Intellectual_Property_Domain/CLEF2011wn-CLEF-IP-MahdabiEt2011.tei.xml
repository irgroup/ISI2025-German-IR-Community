<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.42,115.96,306.50,12.62;1,191.22,133.89,232.92,12.62">Report on the CLEF-IP 2011 Experiments: Exploring Patent Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,142.39,171.24,65.94,7.86"><forename type="first">Parvaz</forename><surname>Mahdabi</surname></persName>
							<email>parvaz.mahdabi@usi.chandersson@ifs.tuwien.ac.athanbury@ifs.tuwien.ac.atfabio.crestani@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.79,171.24,68.08,7.86"><forename type="first">Linda</forename><surname>Andersson</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Vienna University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.35,171.24,59.92,7.86"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Vienna University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,409.74,171.24,59.08,7.86"><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.42,115.96,306.50,12.62;1,191.22,133.89,232.92,12.62">Report on the CLEF-IP 2011 Experiments: Exploring Patent Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A2338E0995AF5F7FCF69FD7A3BC768DD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Patent Retrieval</term>
					<term>Query Generation</term>
					<term>Patent Summarization</term>
					<term>CLEF-IP track</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This technical report presents the work carried out for the Prior Art Candidate Search track of CLEF-IP 2011. In this search scenario, information need is expressed as a patent document (query topic). We compare two methods for estimating query model from the patent document to support summary-based query modeling and descriptionbased query modeling. The former approach utilizes a known text summarization technique, called "TextTiling", and is adopted for patent documents. The latter approach uses the description section of a patent document for estimating the query model. With summary-based query modeling we aspire to capture the main topic of the document as well as the most important subtopics and discard subtopics, which are only marginally discussed in the patent document. We submitted four runs for the Prior Art Candidate Search task. According to recall@1000 our best run was ranked 3 rd across 6 participants and 8 th , across all 30 submitted runs. In terms of MAP our best run achieved the 3 rd rank across participants and 4 th rank, across all runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents the participation of University of Lugano in collaboration with Vienna University of Technology in the Prior Art Candidate Search task of CLEF-IP 2011. This track has been running since 2009 and it is an important platform for comparing the retrieval performance of different patent retrieval systems and testing new ideas. However, compared to other test collections within the IR community, patent retrieval is known to be a difficult search task.</p><p>Different term weighting techniques, IR models and ranking functions developed and tested within the CLEF and TREC tracks have been reused on the patent collections, but the expected retrieval effectiveness do not occur <ref type="bibr" coords="1,446.71,596.34,9.96,8.74" target="#b1">[2]</ref>. It is shown that even for the best runs of CLEF-IP, the retrieval effectiveness is quite lower compared to other domains in Information Retrieval <ref type="bibr" coords="1,392.74,620.25,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="1,404.92,620.25,7.01,8.74" target="#b4">5]</ref>.</p><p>The goal of Prior Art Candidate Search is to find all relevant documents for a given patent (considered as query topic) <ref type="bibr" coords="1,319.68,644.16,9.96,8.74" target="#b6">[7]</ref>. We submitted four runs and used the topic set of CLEF-IP 2010 as our training data for tweaking the parameters.</p><p>In total, 6 participants submitted 30 runs for this task. Our best performing run was ranked 3 rd across participants and 4 th , across all the runs in terms of MAP. According to recall@1000 our best run was ranked 3 rd across participants and 8 th , across all the runs. This paper is organized as follows. In Section 2 we detail our summarization technique and query modeling approach. In Section 3 we describe the details of our experimental setup. In Section 4 we report the evaluation results of our submitted runs. We follow with an analysis in Section 5 and a conclusion in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>An important goal for us is to devise a summary from a patent document. Our intuition is that the patent summary will reflect the main topic as well as the subtopics of a patent document in a concise manner. We focus on generating the summary to improve our query formulation. We were motivated to do so because of the two following reasons:</p><p>Our previous work <ref type="bibr" coords="2,237.00,342.72,10.52,8.74" target="#b5">[6]</ref> showed that queries generated from the description section outperform generated queries from the claims section. Although, it is known that patent examiners use claims section for query formulation. We tried to merge the results of different sections-to exploit all the available textual information. But this merging did not shown to be helpful. In the present work we address this problem by building a summary from the patent document.</p><p>In a recent study <ref type="bibr" coords="2,226.73,415.50,10.52,8.74" target="#b7">[8]</ref> on automatic query generation from patent documents, authors experimented with US patents and found that "background summary" performs as the best field for extracting query terms. Since the background summary is not available in the European patents, we decided to create a summary which resembles the background summary.</p><p>In this section we describe the details of our approach. We first explain how to build a patent summary. We then discuss our take on query generation. Finally, we explain our citation extraction technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Patent Summarization</head><p>Our summarization technique PatTextTiling is a modification of TextTiling-a state of the art text summarization algorithm <ref type="bibr" coords="2,397.96,572.43,9.96,8.74" target="#b2">[3]</ref>. Automatic text segmentation and text summarization techniques aspire to capture documents main topic and subtopics by analyzing the pragmatic structure in terms of cohesive markers and text coherence. TextTiling divides the text into sequences with N tokens. The benefit of having a fixed number is that each sequence carries the same amount of information. For each text segment consisting of N number of sentence sequences a depth score will be produced. The depth score indicates the gap cohesion which represents a topic shift in the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query Generation</head><p>In this work, we aim to employ the knowledge embedded in IPC classes, to generate important terms and also to improve the retrieval performance. Patent documents are annotated with IPC classes which represents the different areas of technology to which a patent document pertains. We define the relevance set consisting of documents that have same IPC classes as the query topic. Each relevant document from this sample is considered as evidence towards the estimation of the relevance model. We assume documents in relevance set have equal importance. This set is more specific in contrast to what we used in our previous work <ref type="bibr" coords="3,198.80,237.55,9.96,8.74" target="#b5">[6]</ref>.</p><p>We estimate the importance of each term with a weighted log-likelihood based approach as shown in Equation <ref type="formula" coords="3,280.88,262.06,4.98,8.74" target="#formula_0">1</ref>. H(θ Q , θ Coll ) represents the cross entropy between the query and the collection and H(θ Q , θ Cluster ) represents the cross entropy between the query and the cluster.</p><formula xml:id="formula_0" coords="3,171.02,333.72,309.57,23.22">H(θ Q , θ Coll ) -H(θ Q , θ Cluster ) ∝ p(w|θ Q ) log p(w|θ Cluster ) p(w|θ Coll )<label>(1)</label></formula><p>This approach favors terms which have high similarity to the document language model θ Q and the cluster language model θ Cluster and low similarity to the collection language model θ Coll . We use maximum likelihood estimates for calculating the language models.</p><p>We have two versions of estimating the query model. First, we build a query model for the summary of the patent document. This method for query modeling is referred to as summary-based query modeling (SM). Second, we build a query from the description section of the patent. We refer to this method as descriptionbased query modeling (DM). Full details of the query generation approach can be found in <ref type="bibr" coords="3,187.90,480.78,9.96,8.74" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Citation Extraction</head><p>Making use of distinguishing events (e.g. patent application number) in unrestricted text could be considered as a form of known-item search. The knownitem search is applied as a search strategy to facilitate the extraction of key terms and synonyms that later can be used in a non-known item search. Therefore, we chose to extract the citations in the unrestricted text from all language sections and add surrounding text from the English text in the topic queries. We extracted the citations with a two-stage regular expression approach. The first step consists of capturing sentences with at least 4 digit sequences combined with and without hyphen. The next stage aimed to reduce the false positive by a set of regular expression sequences, where letter prefix was checked against a positive stop word list consisting of all accepted country codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We first perform query generation on the patent summary. We refer to this run as SM. Next, we use the same method for query generation but instead of selecting terms from the summary of query topic, we select terms from the description section of the query topic. The output of this method is our second run called DM. We filter the ranked list of both runs by excluding documents which do not have at least one IPC class in common with the query document. After that, we use the list of direct citations extracted for each query topic and we combined this list with our keyword-based run (by performing a linear combination). The output of this combination is two more runs which we refer to as Cit+SM and Cit+DM. The evaluation of our runs is presented in section 4.</p><p>In this section we explain our experimental setup and different parameter settings we used for our submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Set-Up</head><p>We index the collection with Terrier<ref type="foot" coords="4,294.61,316.52,3.97,6.12" target="#foot_0">1</ref> . Our preprocessing consists of stop-word removal and stemming using Porter stemmer. In the experiments we use the BM25 implementation of Terrier. We limit our experiments to the English subset of the collection. As explained before, we build two query models: one based on summary and one based on the description. However, for the retrieval we use full text of the documents.</p><p>Tables <ref type="table" coords="4,181.78,389.82,4.98,8.74" target="#tab_0">1</ref> and<ref type="table" coords="4,210.96,389.82,4.98,8.74" target="#tab_1">2</ref> list some statistical properties of the English subset of the CLEF-IP 2011 collection. The average number of unique terms and document length for each section are displayed for both EP and WO subsections of the collection. The last column displays the number of patent documents which were used in the calculations. We considered an additional condition while calculating the statistics. We performed our calculation for documents where the English language meta-tags are consistent with two independent language detection application (one based on stop words and one using n-gram technique). This is due to the fact that there are about 80,000 language meta-tags on section level where the meta-tags show inconsistency with the suggestion of the language detection applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameter Settings</head><p>We explain the parameter settings used for summarization technique. We used a basic TextTiling Perl module with the following parameters:</p><p>1. Number of tokens per sequence which reflects the document length 2. Sequence of window gap (default 2) 3. Smoothing round (default 2) 4. Minimum segment size (default 3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Number of segments</head><p>For parameters 1 and 2 we first tried to set these values dynamically according to each section length and the sentence length but the performance decreased. The best performance was obtained when parameter 1 was set to 100 and parameter 2 was set to 2. For the gap sequence and the smoothing round we used the default value. The minimum size was increased to 7 and the number of segments was set according to the number of paragraph meta-tags present in the text. In PatTextTiling additional binary weight was given to the abstract and specific paragraphs with citation or heading (e.g. Prior-Art, Background) in the description section-if present they were included in the final summery. For the description and claims sections the lexical cohesive gap distribution were first computed independently of each other; and once again on the selected text segments. The description section was given a more granular threshold meanwhile the claims section had a reduced granular threshold due to the fact of its stylistic repetitive writing. The threshold for description and claims were twofold: one based upon average difference in the cohesive gap and one fixed to a threshold value (claims 30 and description 20). The fixed values were added due to the fact of the diversity in the gap scores found among topic set documents. The information found in Lists and Tables were not included in the final summery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Organizers used different evaluation scores for evaluating the submitted runs. We used MAP, ndcg, P@100, P@500, recall@100, recall@500 and recall@1000 to report our retrieval performance on this task. Table <ref type="table" coords="5,383.43,632.21,4.98,8.74" target="#tab_2">3</ref> shows the results of our submitted runs on the English subset of queries which is composed of 1351 queries.</p><p>We mainly focused on the textual information for which we submitted SM and DM run. We also detected the direct citation information present in the query topic and we combined it with our first two runs. The output of this combination is two more runs denoted as Cit+SM and Cit+DM. We zoom in to our best performing run Cit+DM to see the effect of extracted citations. We only managed to extract citations for 102 query topics out of 1351 English query topics and the average number of found citation for each topic is 1. Most of the identified citations in the unrestricted text did not have EP and WO numbers. Since we ignored the patent numbers which did not exist in the collection, our citation extraction runs performed just slightly better than our runs without citation. An interesting observation was that several extracted citations were cited by more than one query topic. On average each extracted citation was cited 1.24 times.</p><p>In order to fully explore the citation extraction mechanism it has to be used in combination with an online service (e.g. Open Patent Services<ref type="foot" coords="6,397.55,423.84,3.97,6.12" target="#foot_1">2</ref> ) to map identified references to a valid patent application number.</p><p>Table <ref type="table" coords="6,177.28,449.32,4.98,8.74">4</ref> shows the evaluation scores for Cit run. This run has a comparable MAP to submitted runs but it has a poor recall. This explains why the text and citation combination is not improving their matching runs without citations, as expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4. Evaluation scores for Cit run</head><p>Method MAP ndcg P@100 P@500 recall@100 recall@500 Cit 0.07 0.1329 0.005 0.001 0.0784 0.0784</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section we performed some analysis with the aim to identify the low retrieval effectiveness of the SM run. In order to analyze this we looked into some features characterizing both the topics and the qrels. In the following analysis we used the 1348 English topics belonging to the topic set of the CLEF-IP 2010. We used the topic set of last year for performing our analysis. We considered per topic analysis and we first looked into the number of topics in SM run which have a higher Average Precision (AP) value compared to the DM run. The output of this analysis shows that SM run outperforms DM run for 618 topics. While, the DM run outperforms SM run for 628 topics. Figure <ref type="figure" coords="7,363.20,190.72,4.98,8.74" target="#fig_0">1</ref> shows the AP differences between SM and DM. For some topics SM works best while for others DM works best and it is mostly a balanced picture. Therefore, it is not easy to favor one approach against the other.</p><p>-0. We zoom into one example topic where SM performs better than DM. This example concerns the topic 1038 where the title of the document is Damping arrangements for Y25 bogies. Table <ref type="table" coords="7,292.67,515.21,4.98,8.74" target="#tab_3">5</ref> reports MAP and recall scores and Table <ref type="table" coords="7,134.77,527.17,4.98,8.74" target="#tab_4">6</ref> shows the top 10 terms for the query models constructed for topic 1038 with SM performing much better than DM. SM managed to identify all relevant documents for this query through the terms introduced by SM query model. As it is displayed in the Table <ref type="table" coords="8,431.76,305.73,4.98,8.74" target="#tab_4">6</ref> the terms piston and arrangement are only selected with the SM and not with the DM.</p><p>Next feature we examined is the non-retrievable topics of each run, i.e. the number of topics that no relevant document was retrieved by that run. The size of non-retrievable set for SM is 63 and the size of non-retrievable set for DM is 52. We calculated the overlapping between the non-retrievable set of SM and DM and we found that for 42 topics none of the runs managed to retrieve any relevant documents. We first looked into the number of relevant documents for non-retrievable topic set in the qrels. Based on our findings, this feature was not able to distinguish non-retrievable topic set from the other topics.</p><p>We then decided to check the document length of the relevant documents for non-retrievable topic set. To our surprise, this investigation showed us that 0.48 of relevant documents do not contain any English text apart from the title. As a contrast, we decided to compare this with the easy-retrievable, i.e. topics which were retrieved by both methods with an AP value over 0.9. The corresponding value for this set is 112 topics and 0.18 of retrieved relevant documents for this set contain only title section in English.</p><p>These rather contradicting facts indicate that our methods managed to retrieve relevant documents where only the title of relevant documents existed for the easy-retrievable set. One reason for the good performance of our methods despite the lack of the text, could be the extra weight given to the surrounding text of the unrestricted citation.</p><p>The lack of the text in the qrels of non-retrievable set is one of the reasons which explains the low retrieval effectiveness of our runs on this set. According to Bashir and Rauber <ref type="bibr" coords="8,231.67,595.61,10.52,8.74" target="#b0">[1]</ref> this problem can be considered as a retrievability bias.</p><p>Depending on how the similarity between a query and a document is measured, some documents maybe more or less retrievable in certain systems, up to some documents not being retrievable at all within common threshold settings. Retrieval biases are due to different factors such as the popularity of a document (e.g. increasing weight of references), length of documents and structural infor-mation such as metadata or headings. Therefore, in such scenarios one search strategy alone (e.g. keyword search) does not perform well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we presented the experiments and results of our participation in CLEF-IP 2011 Prior Art Candidate Search task. We submitted four runs to this track. For our first run we built a summary of the patent document and then we introduced a method for sampling query terms from the patent summary.</p><p>In our second run we used the description section of a patent document for sampling query terms. For our third and fourth runs, we combined the extracted citations from the topics with our first two runs. According to the evaluation results our text summarization run performed slightly lower than the run based on the description. One reason for this is that words in Lists and Tables of the query topic were not included in the patent summary. In addition, the parameter setting of the text summarization technique needs to be further explored.</p><p>In this work, we used the documents with same IPC classes as query topic to calculate the sampling distribution. In an extension to this, we can also take the citations and use them for estimating the relevancy. Moreover, a document's importance can be approximated by its relevance to the original query and this can be used as a document prior.</p><p>For our future work, we need to explore other retrieval mechanisms such as bibliographic data to address the problem of missing text. In terms of query modeling, in addition to unigrams, we need to consider n-grams to capture concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>Authors would like to thank Information Retrieval Facility (IRF) for the support of this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,218.82,453.02,177.73,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. AP differences between SM and DM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,167.33,430.81,280.68,73.39"><head>Table 1 .</head><label>1</label><figDesc>Properties of CLEF-IP 2011 collection (EP section)</figDesc><table coords="4,167.33,452.11,280.68,52.10"><row><cell cols="4">EP source Avg. document length Avg. unique terms No. Documents</cell></row><row><cell>Title</cell><cell>28</cell><cell>23</cell><cell>1,824,499</cell></row><row><cell>Abstract</cell><cell>90</cell><cell>57</cell><cell>904,277</cell></row><row><cell>Description</cell><cell>5079</cell><cell>718</cell><cell>962,686</cell></row><row><cell>Claims</cell><cell>577</cell><cell>123</cell><cell>1,151,609</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,167.33,116.41,280.68,73.39"><head>Table 2 .</head><label>2</label><figDesc>Properties of CLEF-IP 2011 collection (WO section)</figDesc><table coords="5,167.33,137.71,280.68,52.10"><row><cell cols="4">WO source Avg. document length Avg. unique terms No. Documents</cell></row><row><cell>Title</cell><cell>20</cell><cell>16</cell><cell>311,755</cell></row><row><cell>Abstract</cell><cell>91</cell><cell>57</cell><cell>223,348</cell></row><row><cell>Description</cell><cell>5632</cell><cell>916</cell><cell>182,653</cell></row><row><cell>Claims</cell><cell>904</cell><cell>147</cell><cell>182,625</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,184.17,345.83,84.35"><head>Table 3 .</head><label>3</label><figDesc>Comparison of two query estimation methods (SM and DM) and the combination with the direct citations (Cit+SM and Cit+DM)</figDesc><table coords="6,140.91,216.40,333.55,52.12"><row><cell cols="5">Method MAP ndcg P@100 P@500 recall@100 recall@500 recall@1000</cell></row><row><cell>SM</cell><cell>0.0871 0.2305 0.0206 0.0064</cell><cell>0.2789</cell><cell>0.423</cell><cell>0.5254</cell></row><row><cell>DM</cell><cell>0.088 0.2318 0.0209 0.0064</cell><cell>0.2822</cell><cell>0.4287</cell><cell>0.5261</cell></row><row><cell cols="2">Cit+SM 0.0887 0.2331 0.0207 0.0064</cell><cell>0.2808</cell><cell>0.4245</cell><cell>0.5283</cell></row><row><cell cols="2">Cit+DM 0.0896 0.2344 0.021 0.0065</cell><cell>0.2842</cell><cell>0.4303</cell><cell>0.529</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,228.35,575.11,158.66,50.98"><head>Table 5 .</head><label>5</label><figDesc>Performance on topic # 1038</figDesc><table coords="7,273.58,595.91,68.19,30.18"><row><cell>run MAP recall</cell></row><row><cell>SM 0.2599 1</cell></row><row><cell>DM 0.022 0.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,157.33,115.91,300.71,138.65"><head>Table 6 .</head><label>6</label><figDesc>Query models for topic "Damping arrangements for Y25 bogies"</figDesc><table coords="8,260.56,136.71,94.23,117.85"><row><cell>S M</cell><cell>DM</cell></row><row><cell>pedestal</cell><cell>bogie</cell></row><row><cell>piston</cell><cell>axle</cell></row><row><cell>bogie</cell><cell>pedestal</cell></row><row><cell>axle</cell><cell>box</cell></row><row><cell>box</cell><cell>spring</cell></row><row><cell cols="2">arrangement resilient</cell></row><row><cell>damp</cell><cell>damp</cell></row><row><cell>spring</cell><cell>relative</cell></row><row><cell>lenoir</cell><cell>movement</cell></row><row><cell>mount</cell><cell>wagon</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.73,656.80,74.07,7.86"><p>http://terrier.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.73,656.80,181.15,7.86"><p>http://www.epo.org/searching/free/ops.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.35,536.34,342.24,7.86;9,146.91,547.30,333.68,7.86;9,146.91,558.26,138.64,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,251.70,536.34,228.89,7.86;9,146.91,547.30,18.39,7.86">Analyzing document retrievability in patent retrieval settings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,183.26,547.30,293.05,7.86">Database and Expert Systems Applications, 20th International Conference</title>
		<meeting><address><addrLine>DEXA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="753" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,569.19,342.24,7.86;9,146.91,580.15,333.68,7.86;9,146.91,591.11,209.65,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,251.74,569.19,228.85,7.86;9,146.91,580.15,89.65,7.86">On the relationship between query characteristics and IR functions retrieval bias</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,254.54,580.15,226.05,7.86;9,146.91,591.11,59.98,7.86">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1515" to="1532" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,602.03,342.24,7.86;9,146.91,612.99,333.68,7.86;9,146.91,623.95,54.85,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,206.57,602.03,257.69,7.86">Context and structure in automated full-text information access</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<idno>number UCB/CSD- 94/836</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>UC Berkeley Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="9,138.35,634.88,342.24,7.86;9,146.91,645.84,333.67,7.86;9,146.91,656.80,142.53,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,248.36,634.88,232.23,7.86;9,146.91,645.84,73.86,7.86">Experiments with citation mining and key-term extraction for prior art search</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,228.12,645.84,252.47,7.86;9,146.91,656.80,41.83,7.86">Workshop of the Cross-Language Evaluation Forum, LABs and Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Notebook Papers</note>
</biblStruct>

<biblStruct coords="10,138.35,119.67,342.24,7.86;10,146.91,130.63,333.68,7.86;10,146.91,141.59,265.26,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,286.49,119.67,194.10,7.86;10,146.91,130.63,188.31,7.86">Applying the KISS Principle for the CLEF-IP 2010 Prior Art Candidate Patent Search Task</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Magdy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,346.02,130.63,134.57,7.86;10,146.91,141.59,164.56,7.86">Workshop of the Cross-Language Evaluation Forum, LABs and Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Notebook Papers</note>
</biblStruct>

<biblStruct coords="10,138.35,152.55,342.24,7.86;10,146.91,163.51,193.06,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,415.24,152.55,65.35,7.86;10,146.91,163.51,75.08,7.86">Building queries for prior-art search</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mahdabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Keikha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gerani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Landoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,241.52,163.51,20.14,7.86">IRFC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,174.47,342.24,7.86;10,146.91,185.43,333.67,7.86;10,146.91,196.39,92.75,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,188.11,174.47,292.48,7.86;10,146.91,185.43,18.02,7.86">CLEF-IP 2010: Retrieval Experiments in the Intellectual Property Domain</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,174.17,185.43,301.77,7.86">Workshop of the Cross-Language Evaluation Forum, LABs and Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Notebook Papers</note>
</biblStruct>

<biblStruct coords="10,138.35,207.34,342.24,7.86;10,146.91,218.30,333.68,7.86;10,146.91,229.26,84.02,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,249.60,207.34,171.03,7.86">Transforming patents into prior-art queries</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,428.00,207.34,52.60,7.86;10,146.91,218.30,329.84,7.86">International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="808" to="809" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
