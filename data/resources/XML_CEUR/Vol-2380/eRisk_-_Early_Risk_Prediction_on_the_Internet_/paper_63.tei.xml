<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,237.10,115.96,141.16,12.62;1,174.30,133.89,266.75,12.62">UDE at eRisk 2019: Early Risk Prediction on the Internet</title>
				<funder>
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG</orgName>
				</funder>
				<funder ref="#_2sQkcWD">
					<orgName type="full">German Research Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,179.19,171.56,64.68,8.74"><forename type="first">Razan</forename><surname>Masood</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Duisburg-Essen</orgName>
								<address>
									<settlement>Duisburg</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,254.42,171.56,93.63,8.74"><forename type="first">Faneva</forename><surname>Ramiandrisoa</surname></persName>
							<email>faneva.ramiandrisoa@irit.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Université de Toulouse</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.98,171.56,53.71,8.74"><forename type="first">Ahmet</forename><surname>Aker</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Duisburg-Essen</orgName>
								<address>
									<settlement>Duisburg</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,237.10,115.96,141.16,12.62;1,174.30,133.89,266.75,12.62">UDE at eRisk 2019: Early Risk Prediction on the Internet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7A291D88AA99838CC03C769DB0D2C843</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Early risk detection</term>
					<term>Anorexia</term>
					<term>Self-harm</term>
					<term>NLP</term>
					<term>LSTM</term>
					<term>SVM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our participation in CLEF eRisk workshop. eRisk 2019 is the third edition of this track 3 , which was first introduced in 2017. In the current edition, the organizers are targeting Social Media users, namely Reddit, who may be under the risk of Anorexia, self-harm, and depression. We participated in both tasks of early risk detection of Anorexia and self-harm. Our predictions are based on Natural Language Processing using supervised machine learning with Support Vector Machines (SVM) and neural networks. SVM gave the best results among our five submitted models with latency-weighted F1 of 0.58 and ERDE5 of 0.08 and ERDE50 of 0.04 for Anorexia detection task, while our more complicated neural network models did not show the desired performances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Social Media (SM) provided a more vibrant environment for communication and sharing experiences. However, different means of interactions on SM platforms induced enormous amounts of data of different variations. Feeding the data to machine learning and data mining algorithms revealed much of user personality that could not be known even by their close family members 4 .</p><p>Hence, the users' mental health state is not an exception and might be revealed and understood through the use of SM data as well.</p><p>In mental health treatment, professionals use the textual content produced by people suffering from different kinds of mental illness to analyze and help them in the treatment and diagnosis. This procedure could be costly to do on site, and it may not result in enough data. Therefore, mining social media could be a promising method to get more related content for analysis. Besides, it is not only a cheaper source but also, a reliable way to reach more people that can be at risk and in need of professional therapy <ref type="bibr" coords="2,335.93,154.86,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="2,348.10,154.86,7.01,8.74" target="#b4">5]</ref>.</p><p>eRisk 2019 introduced three collaborative tasks to deal with the early risk detection of different mental illnesses, namely Anorexia, self-harm, and depression, using textual data from related Reddit subreddits. We submitted predictions for the first two tasks. We used five models for both of the tasks, including variations of SVM and LSTM neural networks. In the following, we introduce our models and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>The data was extracted from Reddit <ref type="foot" coords="2,293.14,289.85,3.97,6.12" target="#foot_0">5</ref> . This website includes many communities to discuss certain topics and interests in what are called subreddits.</p><p>The first task is related to the early detection of signs of Anorexia. This task has two stages training and testing. Training and Test datasets of the same task in eRisk 2018 were joined to be used as the training set for eRisk 2019. The data consists of users' messages, which are separate posts with a title and a textual content.</p><p>In earlier anorexia and depression detection challenges, users' posts were divided into ten chunks each with 10% of each user posts. In 2019 anorexia challenge, the test data is provided in an item-by-item manner. The predicting model must submit a decision for the user, i.e., positive or not, upon receiving each message from their stream. A score indicating the level of Anorexia must also be submitted after receiving each writing.</p><p>The second task is related to the early detection of Signs of Self-harm. The source of the data is the same as the first task with the same test stage format. There is no training data for this task, so it is presented as an unsupervised learning task to motivate search based methods.</p><p>More information about the data is provided by <ref type="bibr" coords="2,362.66,497.95,9.96,8.74" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We first pre-processed the textual data by lemmatizing, lower casing, and then removing stop words, new-lines, and other unrelated terms or symbols related to the Reddit platform. After the cleaning process, we used different machine learning approaches to address the tasks. In our strategy, we aimed to use methods that are as simple as possible, but that led in earlier cases to competitive results <ref type="bibr" coords="2,166.08,621.75,15.50,8.74" target="#b9">[10]</ref> and then also try out more complex models that showed successes in related areas <ref type="bibr" coords="2,193.55,633.70,14.61,8.74" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Linear Support Vector Classification</head><p>For the first model, we employed a traditional machine learning (ML) model, which is an SVM with a linear kernel. For the training phase, we concatenated all the writings for each labeled user, including text and title parts. Then, we applied a term-frequency transformer and then performed feature selection using chi-squared test to select the most significant 500 terms. On the resulting feature vectors from the described pipeline, we performed a 10-fold cross validation using LinearSVC provided by Python sklearn library. We used grid search to determine the SVM model parameters. Besides, we defined a class weight to overcome the unbalanced data problem. The parameters that were used are: {penalty = "l2", dual = T rue, tol = 1e -3, C = 10, class weight = 1 : 4}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Filtered Linear Support Vector Classification</head><p>This method is a two-steps classification. The idea is based on the fact that by observing each user's posts, we found that the stream contains many posts that are not related to the mental health issue and discuss other topics like games, movies or politics. Hence, the first step is to filter out the writings that have a higher probability of being not related to the topics discussed frequently by mental health subreddit users and keep only those that are more likely to be about mental illness related issues. These most related writings pass to the next classification step to predict if the user is under risk or not. If no writings pass this filter, the user's label remains negative. In this model, we tried to narrow down the writings to get the ones that have a higher probability of having informative features on the users' mental health and examine if this will enhance the SVM performance by adding a pre-filtering phase.</p><p>To perform the first step, we manually annotated writings in the training set that are not related to mental health. Our guidelines were to filter out any post that does not discuss any of the following issues: depression, food or cooking, physical activity, medications, life experience/story, self-harming/suicidal and eating disorder experiences. These topics are selected based on literature that mentioned topics written by people with mental health issues and by manually observing positive users postings in the data <ref type="bibr" coords="3,326.63,512.53,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="3,343.80,512.53,11.62,8.74" target="#b12">13]</ref>. Two people have performed the data annotation. We first chose randomly 200 posts and annotated them. The inter-rater agreement computed using Kappa on these 200 posts is κ = 0.72.</p><p>We then extended the dataset and annotated more posts. In total, we have 660 posts. From this 660 posts, 230 (34.85%) are related to mental health, and 430 (65.15%) are not. We trained a linear SVM with similar features to the one trained for the first model described in Section 3.1, but we selected the most significant 200 terms. We trained the SVM using 10-fold cross-validation on the 660 writings. The best parameter combination, found using grid search, produced 0.68 as the mean cross-validated F1 score. These parameters are: {tol = 1e -3, C = 1, class weight = "balanced", gamma = "scale"}.</p><p>We consider this described classifier as a filter which does not allow the writing to pass to the next step if the probability of it not being related to mental health is higher than 95%. We chose this high probability to reduce the loss of true positives.</p><p>The second step classifier is responsible for the writings that pass through the previous filtering step. It is the same classifier described in Section 3.1. To train this classifier, we filtered the training data using the step-1 model and used the resulted data for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">LSTM Model</head><p>This model is based on a vanilla LSTM (Long Short Term Memory) neural network, which is a type of recurrent neural network.</p><p>Since the eRisk data is a stream of user's posts ordered by time, LSTM could be applicable for classifying users. For training this model, we did not use the full posts stream, but we used only 45 writings of the users' streams by which we took 15 writings from the beginning, another 15 from the middle and another 15 from the end of the stream. We chose the number of writings based on manual observation of the data in order to summarize the users' writing stream. The goal is to have a minimum and a representing number of writings to asses the users' risk. The features extracted from these writings are a concatenation of doc2vec features and term frequency features. We trained the model on the concatenation of all writings (including title and text) for each user.</p><p>The doc2vec model produces a 200 long vector which is a concatenation of the results of two doc2vec algorithms, namely Distributed Bag of Words and Distributed Memory with an output of 100 long vector each and trained in the same way described by <ref type="bibr" coords="4,240.48,411.61,15.50,8.74" target="#b13">[14]</ref> using Python implementation provided by gensim library. We concatenated the 200 doc2vec feature vector with a vector of term frequencies of the most significant 70 terms selected as described in Section 3.1.</p><p>We used Keras implementation of LSTM with 500 units for output, a dropout of 0.2, a dense layer of size 2 and Softmax function for the final output. We used binary cross-entropy for the loss function and Adam as an optimizer. We defined F1 metric for model evaluation.</p><p>In the test stage, the input to the trained model is given one writing at a time. Whenever new writings arrive, we concatenate it with the previous ones and use the concatenation as input to the model. When the stream of writings became longer, we started to arrange the input to concatenate only 45 writings the way we described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Global Attention Model</head><p>Attention is a mechanism used in deep learning models that have been quite popular lately, and first appeared in neural machine translation. Attention was mainly introduced to address the inefficiency of sequence-to-sequence encoding in memorizing longer sentences <ref type="bibr" coords="4,275.82,632.21,9.96,8.74" target="#b0">[1]</ref>. This mechanism allows the model to learn for each word in the target sequence which words to attend to (pay attention to) in the input sequence by learning alignment weights between that pairs of output and input words. These alignments are in turns used to calculate the final context vector for each word/ time-step. In our case, time steps are the writings in the user's stream. Accordingly, the model is supposed to learn the importance of each of the writings to predict if the owner is at risk of Anorexia.</p><p>The model's input is similar to what we described for the previous simple LSTM model in Section 3.3 but using only the doc2vec feature vectors, i.e. a doc2vec for each writing. The 45 vectors formed an input layer for another LSTM layer with 16 units and then followed by an attention layer. We use what is called global/ soft attention, as described by <ref type="bibr" coords="5,335.55,215.07,9.96,8.74" target="#b8">[9]</ref>, which is a simplification of the attention mechanism in <ref type="bibr" coords="5,241.75,227.03,9.96,8.74" target="#b0">[1]</ref>. We use a dropout of 0.1 and then normalizing the attention output with the Softmax function and predict a positive result when its probability is higher than 0.5. In the test phase, we used the same strategy described in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inner Attention Model</head><p>By manually comparing writings streams of positive labeled users and negative labeled ones, we noticed that the positive users' writings contain different kinds of topics and information that is more frequent in their feed. These topics, for example, can be related to diet, eating habits like fasting and food, and medications. Hence, the idea is to train a model that learns the importance of each writing depending on its topic.</p><p>This idea is similar to the model proposed in <ref type="bibr" coords="5,342.56,389.23,15.50,8.74" target="#b11">[12]</ref> for arguments classification. In their paper, Stab et al. use an average vector of embedding of each topic's terms to build the attention layer. These topics embeddings allowed to detect whether a sentence constitutes an argument or not by engaging the topic of the sentence in the detection model. For our case, we used doc2vec of paragraphs collected from web sources related to Eating Disorders in addition to posts from ED related r/EatingDisorders/ and r/AnorexiaNervosa/ subreddits, instead of topic terms that were used in the original paper. We used collected paragraphs from eight different web sources that contain articles about Anorexia and other eating disorders 6,7,8 . We used 15 paragraphs from the ED related websites chosen manually to be as diverse as possible to consider different aspects of the disorder. Besides, we added 15 writings from the mentioned subreddits. We selected the writings to cover different topics that users mention in these subreddits, such as body/weight information, diets, and recovering experiences.</p><p>The implemented model includes an inner-attention layer that receives both the input as doc2vev features from 45 writings as described before and the doc2vec feature of 30 paragraphs of related topics and an LSTM with 64 units with a dropout of 0.1 and a dense layer of 2 units for the output using Softmax function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task 1: Early Detection of Signs of Anorexia</head><p>We developed the five models described in the earlier sections for Task 1, namely Anorexia detection. eRisk organizers evaluate the prediction based on F1, precision, recall, and ERDE measure, which was first proposed in <ref type="bibr" coords="6,423.37,182.72,9.96,8.74" target="#b5">[6]</ref>. However, ERDE measure has some deficiencies. Hence, eRisk 2019 introduced latencyweighted F1 score measure <ref type="bibr" coords="6,251.99,206.63,9.96,8.74" target="#b7">[8]</ref>. The linear SVM model produces the best latencyweighted F1 score among our other four submitted models. See Table <ref type="table" coords="6,458.75,218.58,4.98,8.74" target="#tab_0">1</ref> for results. Whereas, other NN models performed poorly on this task. The SVM model ranked 11th between 13 teams' 54 submitted models according to the latency-weighted F1 measure. The organizers have added new ranking-based evaluation measures that are based on the submitted scores that accompanied each received writing. The goal is to rank the users according to their estimated risks <ref type="bibr" coords="6,368.83,417.79,9.96,8.74" target="#b7">[8]</ref>. The standard IR metrics P@10 and NDCG were reported after seeing 1, 100, 500 and 1000 writings. According to this evaluation metric, the filtered-SVM model performed slightly better than the single step SVM model. Again, the other NN models performed poorly according to this measure. Results are shown in Table <ref type="table" coords="6,414.51,465.61,3.87,8.74">2</ref>. Nonetheless, these measure values were high compared to other teams, but this could be a result of a relatively high recall of our models.</p><p>Table <ref type="table" coords="6,208.42,520.01,4.13,7.89">2</ref>. Ranking-based evaluation of the five models in task 1 Model 1 writing 100 writings 500 writings 1000 writings P@10 NDCG@10 NDCG@100 P@10 NDCG@10 NDCG@100 P@10 NDCG@10 NDCG@100 P@10 NDCG@10 NDCG@100 SVM . Many studies and investigations showed that different mental illnesses accompany and relate to each other. For example, people who suffer from Anorexia could be depressed or self-harming and vice versa <ref type="bibr" coords="7,363.24,118.99,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="7,375.41,118.99,7.01,8.74" target="#b1">2]</ref>. Since task 2 had no training data, we wanted to investigate if training the model on writing provided for different tasks, namely depression detection, and anorexia detection could help in detecting users at risk of self-harm. Therefore, we trained the same five classifiers described before for Anorexia detection on previously provided eRisk data for depression detection in addition to the data provided for Anorexia detection. Precisely, we used the datasets provided by eRisk 2018 <ref type="bibr" coords="7,470.07,190.72,10.52,8.74">[7]</ref> on depression and Anorexia and used the positively labeled users in each as positive self-harm cases. The performance was poor using the mentioned datasets for training according to the measures proposed by the organizers which were mentioned in the previous section. See Table <ref type="table" coords="7,327.85,238.55,3.87,8.74" target="#tab_2">3</ref>. The ranking evaluation indicated higher performance for this task as well. See Table <ref type="table" coords="7,358.65,250.50,3.87,8.74" target="#tab_3">4</ref>. Model 1 writing 100 writings 500 writings 1000 writings P@10 NDCG@10 NDCG@100 P@10 NDCG@10 NDCG@100 P@10 NDCG@10 NDCG@100 P@10 NDCG@10 NDCG@100 SVM 0 0 0.09 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper, we introduced the working notes of our participation in eRisk 2019 for the early risk detection of Anorexia and self-harm. We proposed solutions based on traditional ML with linear SVM and NNs, including LSTM and attention mechanisms. What makes this task tricky is the trade-off between deciding earlier with a low number of posts that a user is at risk of having the negative consequences of mental illnesses symptoms on the one hand, and being wrong to classify such user to be at such risk. Accordingly, by looking at the data and trying to decide whether a user is at risk by reading their posts one by one, it seemed hard to decide with much confident especially when some users who are already labeled as positive cases do not have many posts. However, introducing a score to guide the level of risk and ranking users according to their estimate of risk could be a more reliable indicator to use, as stated by <ref type="bibr" coords="8,403.81,130.95,9.96,8.74" target="#b7">[8]</ref>. SVM did not perform as it is expected, this could be due to the difference between the training and testing settings, where the training is done using the stream of the whole writing for each user, whereas the judging is associated with each writing at a time. On the other hand, what is most tricky by this kind of problem is the coding for a user's writing stream. The NN solutions we used are initially used for a stream of words that constitute a sentence that needs to be classified, but in our case, we have several paragraphs/sentences that belong to one user which should be classified. It is most likely that our input encoding did not fit with the test and evaluation strategy of the tasks. On the other hand, as mentioned before, the performance was better in terms of ranking evaluation when using more writings.</p><p>In our future work, we aim to investigate methods that suit the item-by-item test phase. Moreover, we need to investigate better encoding for the user input without losing much knowledge by encoding each post as one unit of information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,172.37,284.94,269.41,82.11"><head>Table 1 .</head><label>1</label><figDesc>Results of the five models in task 1: Anorexia detection</figDesc><table coords="6,172.37,303.99,269.41,63.06"><row><cell>Model</cell><cell cols="4">P R F1 ERDE5 ERDE50 latency-weighted F1</cell></row><row><cell>SVM</cell><cell>.51 .74 .61</cell><cell>.08</cell><cell>.04</cell><cell>.58</cell></row><row><cell cols="2">Fitered-SVM .44 .73 .55</cell><cell>.07</cell><cell>.04</cell><cell>.53</cell></row><row><cell>LSTM</cell><cell>.13 .68 .22</cell><cell>.13</cell><cell>.08</cell><cell>.19</cell></row><row><cell cols="2">Global-attention 0 0 0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Inner-attention 0 0 0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,173.26,282.10,267.63,82.11"><head>Table 3 .</head><label>3</label><figDesc>Results of the five model in task 2: Self-harm detection</figDesc><table coords="7,173.26,301.15,267.63,63.06"><row><cell>Model</cell><cell cols="4">P R F1 ERDE5 ERDE50 latency-weighted F1</cell></row><row><cell>SVM</cell><cell>.50 .07 .13</cell><cell>.12</cell><cell>.11</cell><cell>.12</cell></row><row><cell cols="2">Fitered-SVM .45 .22 .30</cell><cell>.11</cell><cell>.10</cell><cell>.29</cell></row><row><cell>LSTM</cell><cell>.18 .68 .29</cell><cell>.14</cell><cell>.10</cell><cell>.28</cell></row><row><cell cols="2">Global-attention 0 0 0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Inner-attention .06 .34 .10</cell><cell>.20</cell><cell>.20</cell><cell>.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,179.52,411.74,256.31,7.89"><head>Table 4 .</head><label>4</label><figDesc>Ranking-based evaluation of the five models in task 2</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0" coords="2,152.70,656.80,102.72,7.86"><p>https://www.reddit.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1" coords="5,152.70,623.92,313.56,7.86;5,134.77,634.88,82.33,7.86"><p>https://www.mayoclinic.org/diseases-conditions/anorexia-nervosa/symptomscauses/syc-20353591</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2" coords="5,148.55,644.07,3.65,5.24;5,152.70,645.84,173.04,7.86"><p>7 https://www.psycom.net/eating-disorders/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_3" coords="5,325.74,645.84,38.94,7.86;5,148.55,655.03,3.65,5.24;5,152.70,656.80,266.24,7.86"><p>anorexia/ 8 https://www.eatingdisorderhope.com/information/eating-disorder</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">acknowledgement</head><p>This work was funded by the <rs type="funder">Deutsche Forschungsgemeinschaft (DFG</rs>, <rs type="funder">German Research Foundation</rs>) -<rs type="grantNumber">GRK 2167</rs>, <rs type="projectName">Research Training Group User-Centred Social Media</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_2sQkcWD">
					<idno type="grant-number">GRK 2167</idno>
					<orgName type="project" subtype="full">Research Training Group User-Centred Social Media</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,436.62,337.63,7.86;8,151.52,447.58,247.18,7.86" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m" coord="8,296.12,436.62,184.47,7.86;8,151.52,447.58,85.88,7.86">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.96,458.68,337.64,7.86;8,151.52,469.64,233.34,7.86" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03538</idno>
		<title level="m" coord="8,302.19,458.68,178.40,7.86;8,151.52,469.64,67.49,7.86">Multi-task learning for mental health using social media text</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.96,480.74,337.63,7.86;8,151.52,491.70,89.48,7.86" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Coopers</surname></persName>
		</author>
		<title level="m" coord="8,204.83,480.74,275.76,7.86;8,151.52,491.70,20.37,7.86">The costs of eating disorders: Social, health and economic impacts. B-eat</title>
		<meeting><address><addrLine>Norwich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,502.80,337.63,7.86;8,151.52,513.76,329.08,7.86;8,151.52,524.72,279.86,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,331.31,502.80,149.28,7.86;8,151.52,513.76,26.22,7.86">Quantifying mental health signals in twitter</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Coppersmith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,198.75,513.76,281.85,7.86;8,151.52,524.72,205.78,7.86">Proceedings of the workshop on computational linguistics and clinical psychology: From linguistic signal to clinical reality</title>
		<meeting>the workshop on computational linguistics and clinical psychology: From linguistic signal to clinical reality</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,535.82,337.64,7.86;8,151.52,546.76,185.65,7.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,319.72,535.82,156.93,7.86">Ex-ray: Data mining and mental health</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Diederich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Al-Ajmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yellowlees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,151.52,546.78,98.81,7.86">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="923" to="928" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,557.88,337.64,7.86;8,151.52,568.84,329.07,7.86;8,151.52,579.80,329.07,7.86;8,151.52,590.76,180.75,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,286.09,557.88,194.50,7.86;8,151.52,568.84,106.63,7.86">A test collection for research on depression and language use</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-44564-93</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-44564-9" />
	</analytic>
	<monogr>
		<title level="m" coord="8,292.33,568.84,188.26,7.86;8,151.52,579.80,14.22,7.86">Conference Labs of the Evaluation Forum</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,601.86,337.64,7.86;8,151.52,612.82,178.74,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,313.54,601.86,167.06,7.86;8,151.52,612.82,178.74,7.86">Overview of erisk 2018: Early risk prediction on the internet (extended lab overview)</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,623.92,337.64,7.86;8,151.52,634.88,329.07,7.86;8,151.52,645.84,329.07,7.86;8,151.52,656.80,275.83,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,315.66,623.92,164.93,7.86;8,151.52,634.88,88.92,7.86">Overview of eRisk 2019: Early Risk Prediction on the Internet</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,260.50,634.88,220.09,7.86;8,151.52,645.84,329.07,7.86;8,151.52,656.80,16.79,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 10th International Conference of the CLEF Association, CLEF 2019</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,119.67,337.64,7.86;9,151.52,130.63,272.55,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,320.68,119.67,159.92,7.86;9,151.52,130.63,106.58,7.86">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.62,141.59,337.98,7.86;9,151.52,152.55,150.64,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,392.08,141.59,49.81,7.86">Irit at e-risk</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ramiandrisoa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Benamara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,152.55,66.27,7.86">E-Risk workshop</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,163.51,337.98,7.86;9,151.52,174.47,329.07,7.86;9,151.52,185.43,329.07,7.86;9,151.52,196.39,329.07,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,163.57,174.47,317.02,7.86;9,151.52,185.43,36.52,7.86">Beyond lda: exploring supervised topic modeling for depression-related language in twitter</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,208.03,185.43,272.56,7.86;9,151.52,196.39,250.22,7.86">Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality</title>
		<meeting>the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="99" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,207.34,337.97,7.86;9,151.52,218.30,329.07,7.86;9,151.52,229.26,25.60,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="9,285.58,207.34,195.01,7.86;9,151.52,218.30,187.99,7.86">Cross-topic argument mining from heterogeneous sources using attention-based neural networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05758</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.62,240.22,337.98,7.86;9,151.52,251.18,329.07,7.86;9,151.52,262.14,107.04,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,239.08,240.22,223.80,7.86">Social media mining to understand public mental health</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toulis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Golab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,251.18,324.90,7.86">VLDB Workshop on Data Management and Analytics for Medicine and Healthcare</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="55" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,273.10,337.98,7.86;9,151.52,284.06,329.07,7.86;9,151.52,295.02,25.60,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,314.23,273.10,166.36,7.86;9,151.52,284.06,212.85,7.86">Linguistic metadata augmented classifiers at the clef 2017 task for early detection of depression</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Trotzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,385.61,284.06,94.98,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
