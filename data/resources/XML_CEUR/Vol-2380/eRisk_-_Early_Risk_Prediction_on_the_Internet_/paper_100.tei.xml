<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,185.80,115.96,243.77,12.62;1,255.54,133.89,98.05,12.62">Early Risk Prediction by means of DeepLearning</title>
				<funder ref="#_QneRcc8">
					<orgName type="full">of the Ministry of Economy and Competitiveness -Government of Spain</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,184.96,171.56,49.26,8.74"><forename type="first">Pablo</forename><surname>Raez</surname></persName>
							<email>praez@pa.uc3m.es</email>
						</author>
						<author>
							<persName coords="1,237.55,171.56,74.42,8.74"><forename type="first">Garcia</forename><surname>Retamero</surname></persName>
						</author>
						<author>
							<persName coords="1,334.67,171.56,95.72,8.74"><forename type="first">Isabel</forename><surname>Segura Bedmar</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,185.80,115.96,243.77,12.62;1,255.54,133.89,98.05,12.62">Early Risk Prediction by means of DeepLearning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CDBF6B8FCD02B3C34ADB07A49BE4927A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Natural Language Processing</term>
					<term>Risk Prediction UC3M</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work presents our five approaches to early risk detection of anorexia on social media in CLEF eRisk 2019. Our models make use of different kinds of deep neural networks to classify the users in a danger situation. We show the effectiveness of our models by using the validation and test datasets. The best model obtains a F1 score of 0.57 over the objective class in the validation and a 0.20 over the test.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Anorexia is an eating disorder which presents symptoms such as fear of gaining weight or a distorted and delirious perception of the own body. This disease is often associated with severe psychological alterations that cause changes in the emotional behaviour. These psychologycal alterations are discernible in the behaviour of the affected and are usually reflected in social media as posts and comments. Currently several anorexia detection methods exist <ref type="bibr" coords="1,412.51,449.26,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="1,429.67,449.26,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="1,439.08,449.26,12.73,8.74" target="#b17">19,</ref><ref type="bibr" coords="1,453.47,449.26,12.73,8.74">14,</ref><ref type="bibr" coords="1,467.87,449.26,12.73,8.74" target="#b16">18,</ref><ref type="bibr" coords="1,134.77,461.22,11.62,8.74" target="#b12">13]</ref>, which are mainly based in behavioural analysis. Anorexia symptoms are usually very diverse and probably hidden by the subjects of study, which makes it harder to make a decision, delaying the diagnoses.</p><p>Much research has been carried out to early detect these symptoms in social media in an automatic way. Even being a well known problem, anorexia is still hard to diagnose, due to it having wide variety of symptoms as well as the long periods needed for them to show up, as in the amenorrhea case <ref type="bibr" coords="1,410.36,532.95,9.96,8.74" target="#b4">[5]</ref>. Because getting to diagnose the patients is an arduous task, patients will receive treatment in later stages of anorexia. This, in turn, will make the therapy longer and more expensive than if the problem was promptly diagnosed. The automatic detection, with the highest possible accuracy, of anorexia in its early stages would mean great time savings as well as considerable patient health improvements who had been treated quickly.</p><p>Five different approaches were carried out in order to address this problem. These approaches are explained in further details in section 4. Both, the results obtained by the validation and testing dataset are included.</p><p>The paper is structured as follows. Section 2 gathers the state of the art of Natural Language Processing techniques applied to the risk prediction domain. Next, in section 3 the dataset and tools used are named. It is followed by section 4 where the methods as well as the neural architectures proposed are described. In section 5 the results obtained are shown. Finally in section 6 the conclusions and the future work are gathered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">State of the Art</head><p>This section gathers the main works related to early risk prediction on the internet. The usage of machine learning techniques in mental illness detection such as anorexia is quite recent. Even so, there is considerable bibliography on the matter <ref type="bibr" coords="2,167.44,316.66,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="2,184.61,316.66,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="2,194.02,316.66,12.73,8.74" target="#b17">19,</ref><ref type="bibr" coords="2,208.40,316.66,12.73,8.74">14,</ref><ref type="bibr" coords="2,222.80,316.66,12.73,8.74" target="#b16">18,</ref><ref type="bibr" coords="2,237.19,316.66,11.62,8.74" target="#b12">13]</ref>.</p><p>In <ref type="bibr" coords="2,161.05,329.56,14.61,8.74" target="#b17">[19]</ref>, Deep Learning techniques have been applied to the problem of anorexia and depression detection for the CLEF eRisk 2018 tasks <ref type="bibr" coords="2,391.11,341.51,9.96,8.74" target="#b8">[9]</ref>. The authors approach the problem by turning it into a sentence classification one, where the sentences are classified as positive if they have been written by an ill user and negative otherwise. They make use of the TF-IDF algorithm to get the most representative words for each one of the classes. Then, the sentences are encoded by means of a Convolutional Neural Network (CNN). They managed to obtain F1 scores of 0.64 and 0.85 as well as ERDE5 of 8.78 and 11.40 in the depression and anorexia tasks, respectively.</p><p>Our first approach is quite similar to the one previously described, but we also make use of word or char embeddings in every model, as well as a fully connected layer after the CNN ones, which have been shown to improve the results of the classifier.</p><p>In <ref type="bibr" coords="2,163.59,486.86,15.50,8.74">[14]</ref> approach to the CLEF eRisk 2018 tasks, different machine learning techniques are presented, such as Linear Regression <ref type="bibr" coords="2,380.95,498.82,14.61,8.74" target="#b11">[12]</ref>, Super Vector Machines <ref type="bibr" coords="2,164.37,510.77,14.61,8.74" target="#b14">[16]</ref>, Ada Boost <ref type="bibr" coords="2,235.47,510.77,14.61,8.74" target="#b13">[15]</ref>, Random Forests <ref type="bibr" coords="2,330.87,510.77,9.96,8.74" target="#b0">[1]</ref>, and Recursive Neural Network (RNN) <ref type="bibr" coords="2,167.64,522.73,14.61,8.74" target="#b15">[17]</ref>. Texts are represented using different features such as Bag Of Words (BOW) and Unified Medical Language System (UMLS). Experiments show that the best results are obtained by BOW and using the classifiers Ada Boost and the Random Forests. They managed to obtain F1 scores of 0.58 and 0.67 as well as ERDE5 of 9.81 and 12.17 in the depression and anorexia tasks, respectively.</p><p>In <ref type="bibr" coords="2,163.89,583.44,15.50,8.74" target="#b16">[18]</ref> approach to the CLEF eRisk 2017 task <ref type="bibr" coords="2,370.45,583.44,9.96,8.74" target="#b7">[8]</ref>, several combinations of user-level linguistic metadata, BoW <ref type="bibr" coords="2,309.04,595.40,14.61,8.74" target="#b19">[21]</ref>, neural word embeddings <ref type="bibr" coords="2,447.17,595.40,9.96,8.74" target="#b2">[3]</ref>, and CNN <ref type="bibr" coords="2,159.94,607.35,10.52,8.74" target="#b6">[7]</ref> are used. Obtaining an F1 value of 0.48 and an ERDE5 of 12.73 on the depression task.</p><p>There have been some interesting approaches not so heavily focused into machine and deep learning techniques such as the one described in <ref type="bibr" coords="2,433.44,644.16,14.61,8.74" target="#b12">[13]</ref>, which focuses into Author Profiling (AP). It consists in analysing texts to predict general or demographic attributes of authors such as: gender, age, personality, native language, and political orientation, among others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Materials</head><p>This section gathers the materials used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>The dataset for this task has the same format as the one described in <ref type="bibr" coords="3,441.85,238.01,14.61,8.74" target="#b9">[10]</ref>. The collection provided, for training and validation, is composed by 152 subjects, of them 20 are anorexic and 132 are not. The texts from these subjects are formed by a total of 253,341 posts and comments, of which 24,874 come from ill subjects and 228,467 are from healthy people. As it can be seen, the training set is very unbalanced, which in turn makes the whole task harder to perform.</p><p>For every different subject, we get all their writings with several information fields, being them the title of the post (sometimes blank), as well as the date and time. It also contains info about the platform where the post was made, may it be reddit or other, and the posted text itself.</p><p>The test dataset is hosted as a server that iteratively yields user writings to the participating teams. These iterations go across time to get the writtings of each user in a more real-world-like scenario. It will only give back the writings when all runs of a timestep for a team are sent. This dataset counts with 2000 timestep for over 800 users. Being them "id", "nick", "redditor", "title", "content", and "date". The "nick" is used as the subject id, and the "title", "content" and "date" ones are used as their homonyms in the training dataset. "Redditor" and "id" do not relate with any of the training dataset and finally number indicates the iteration on the test dataset, which is used for validation purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tools</head><p>Google Colab was used to run the experiments. It consists of a machine with an Intel(R) Xeon(R) CPU @ 2.20GHz as a CPU and its equipped with 12Gb of RAM. The most interesting part of it for us is the GPU they provide, being it a Tesla K80 GPU with 12Gb of memory as well.</p><p>The experiments were developed using python, and its libraries Keras and Tensorflow for DL models. Some other libraries were used such as Pandas or NumPy for the processing of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>In this section the method followed for the development of the approaches is explained. This method includes all the pre and post processing of the data.  As a preprocessing step, all texts are cleaned by removing stop words, numbers, punctuation and words with less than three characters. Then a TF-IDF algorithm is used in order to lower the volume of words while retaining the most representative ones.</p><p>For the models A, B and C, the posts are tokenized and cropped or padded to a fixed length of 50 words per post in models A and C. This padding and cropping takes place because the input for the neural networks must have a fixed shape. The reason why longer texts are cropped is because too much padding will add too much noise to the networks. This is because than most texts have less than 50 words, as shown in figure <ref type="figure" coords="4,302.04,583.07,8.49,8.74" target="#fig_1">1a</ref>. The selected length of the B model is 350. Contrary to the one selected in the previous models, this length is chosen due to the prohibitive size of the network past it. The ideal value would have been 1000, as can be seen in figure <ref type="figure" coords="4,289.32,618.94,8.86,8.74" target="#fig_1">1b</ref>.</p><p>For the models E and D instead of tokenizing the texts and fixing them to a certain length, another preprocessing step is added, based in splitting the words into characters. This operation is needed in order to make use of char embed-  <ref type="bibr" coords="5,383.35,303.59,14.61,8.74" target="#b18">[20]</ref>. Char embeddings has its advantages against word embeddings: they do not face problems when processing unseen words as every word is formed with characters. Another characteristic advantage is the robustness against misspelled words. Furthermore, char embeddings are usually low dimensional ones, which in turn improves the speed of the models. Then each text is fixed to a length of 400 characters. The length was picked by hand and it was done regarding the fig 1c in the same way as for models A, B and C. Finally the characters were fed into the different neural networks.</p><p>Finally, and after the processing performed by the different models, the output of the networks is compared with a threshold to determine if it was a risk situation or not. This threshold was obtained empirically for each model by subduing the results to several tests in which the threshold value iterated in the range of 0.1 and 0.9. Then, the threshold with the highest F1 of the active class was selected. The thresholds are shown in table 1. An illustrative example of this process can be seen in figure <ref type="figure" coords="5,281.84,482.91,4.98,8.74">2</ref> where the evaluation of A and C thresholds is shown.  Several experiments were performed to find out the best hyper-parameter configuration for each one of the models, which can be found in table <ref type="table" coords="5,451.52,608.30,3.87,8.74" target="#tab_2">3</ref>. The tuned hyperparameters regarding the model can be found in detail in table <ref type="table" coords="5,465.94,620.25,4.98,8.74" target="#tab_1">2</ref> Some of the hyper-parameters checked were regarding the model themselves, such as load emb, emb size, trainable emb, cnn size, rnn size, dropout, dnn size, and batch size. Some others were specific of the type of networks used; in the CNN was cnn filter, which determines the size of the kernel used, and in the RNN we can find cell type, determining the type of the cell used, being it GRU or LSTM, bidirectional that indicates if the layers were bidirectional ones, and attention which, as its own name depicts, determines if an attention mechanism was used or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A model</head><p>This model is a simple first approach to classify the different records independently. The posts are taken as if they were independent, and they are labelled to 0 or 1 taking into account if the user who wrote them was control class or positive class patient. This model gets as input the different texts, which then will undergo a Word Embedding layer, whose output is fed to a one-dimensional CNN. Finally, the output of the former layer is fed into a fully connected layer just before the output one (see Img 4a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">B model</head><p>This model similar approach to the the previous one. But in this case, instead of taking the texts as independent bits of information, all of the texts of the same user are processed together. This way, the input to the net is all the tokenized text a user has ever posted and the objective value is if the subject is in risk of suffering anorexia or not.</p><p>This model gets the text input which, in the same way as in the previous model, undergo a Word Embedding layer, whose output is in the same way fed to a RNN layer. The result is then fed to a fully connected layer which is placed just before the output one (see Img 4b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">C model</head><p>This model is a more sophisticated one in the sense that it uses previous A models in order to generate what we call "writing embeddings" by means of transfer learning <ref type="bibr" coords="7,212.74,476.81,9.96,8.74" target="#b5">[6]</ref>. Then they are fed to a RNN layer, which allows us to process varying number of texts. This is crucial due to the dataset having very variable number of texts per user as can be seen in fig <ref type="bibr" coords="7,377.52,500.72,3.87,8.74" target="#b2">3</ref>.</p><p>It is composed of the whole best A model without the two last layers. Those outputs are used as "writing embeddings" which represent the different texts in just a 32 dimension vector. Then, the "writing embeddings" are fed into the RNN layers, whose output is then passed trough a fully connected layer before the output layer (see Img 5a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">D model</head><p>This model follows the same idea as the A, which is to classify the different texts independently. But it differs from the previous one in the fact that it does not use word embeddings, but char embeddings instead.</p><p>This model gets as input the different chars from every post, which then will undergo a Char Embedding layer, which mainly differs from the word embedding one in the dimensions of the vector which is way shorter, as well as in the vocabulary which is, as well, way smaller. The output of this layer is then fed into a one-dimensional CNN in the same way as the A model. Finally, the output of the CNN is fed into a fully connected layer, and then the output one (see Img 4c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">E model</head><p>This model follows the same idea as the C, which is to use a pre-trained model to generate "writing embeddings". But it differs from the previous one in the fact that instead of using a pre-trained model on word embeddings, it uses a char embeddings pre-trained model. This model also takes advantage on the RNN layers that allow it to process the users no matter the number of texts they individually have, which, as aforementioned, is really disperse (see <ref type="bibr" coords="8,426.80,504.46,25.33,8.74">Fig 3)</ref>.</p><p>This model makes use of the best D model weights, but without the two last layers. The outputs resulting of the processing with the cropped D model, which are given in the form of a 64 dimension vector, are fed into the RNN layers. Finally, likewise the previous models, the output of the former layer is fed into a fully connected network, and then it goes under the output one (see Img 5b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In this section, the results obtained by the five different approaches are shown. We divide this section in the validation results and the test results. The evaluation has taken place by means of the test server presented in section 3. We also include the best results obtained in the challenge.  The common measure of performance in terms of precision and recall is the F1-score <ref type="bibr" coords="9,174.08,322.33,9.96,8.74" target="#b3">[4]</ref>. This metric is the harmonic mean of the precision and recall. As we are mostly concerned about the performance over the positive class, only the F1 of that class is shown in the validation results. We also add the Macro F1 due to it being a good measure of the performance with unbalanced classes, where the most important is the least represented one. Finally we add the weighted F1 as a comparison.</p><p>The best results of each model can be seen in the table 4. The best results are in bold. These metrics are very limited in comparison with the ones provided by the challenge organisers. Still the validation metrics provided are promising, specially the ones obtained by the C approach. Still further work must be performed in order to improve the overall results.</p><p>The results obtained in the official evaluation are shown in table <ref type="table" coords="9,431.53,454.30,3.87,8.74" target="#tab_4">5</ref>. The best results obtained for each metric are also shown in the aforementioned table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>Five different approaches to the CLEF eRisk 2019 task 1 have been described. All the approaches make use of some kind of neural networks and two of them benefit from concepts such as transfer learning. Several hyper-parameters of those models were finely-tuned in order to achieve the better performance possible. Although our official results are very low, we can conclude that our models provide promising results for the early detection of anorexia in social media, obtaining an F1 score up to a 0.57 in the positive class. Not so good results were obtained in the test experimentation, F1-wise, even so, for ERDE5 and ERDE50, results close to the best ones were obtained.</p><p>Still, further work is needed. We would like to feed the different approaches with more kinds of embeddings such as concept embeddings, as well as to put to test the usage of word embeddings and char embeddings in the same model.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,160.67,215.88,121.20,6.04;4,214.81,221.24,13.60,6.04;4,150.71,211.64,2.49,6.04;4,140.73,202.00,12.48,6.04;4,140.73,192.36,12.48,6.04;4,140.73,182.72,12.48,6.04;4,140.73,173.08,12.48,6.04;4,138.23,163.44,14.97,6.04;4,138.23,153.80,14.97,6.04;4,138.23,144.15,14.97,6.04;4,138.23,134.51,14.97,6.04;4,131.32,160.81,6.04,22.47;4,209.52,121.60,24.17,7.25;4,134.77,236.33,169.45,7.86;4,134.77,247.29,25.92,7.86;4,337.04,215.88,2.49,6.04;4,354.89,215.88,96.36,6.04;4,391.18,221.24,13.60,6.04;4,327.08,211.64,2.49,6.04;4,322.09,198.95,7.49,6.04;4,319.59,186.26,9.98,6.04;4,319.59,173.57,9.98,6.04;4,319.59,160.87,9.98,6.04;4,319.59,148.18,9.98,6.04;4,319.59,135.49,9.98,6.04;4,312.68,160.81,6.04,22.47;4,385.89,121.60,24.17,7.25;4,311.13,236.33,169.46,7.86;4,311.13,247.29,57.03,7.86;4,248.85,358.25,117.82,5.37;4,302.99,363.61,13.60,5.37;4,238.89,354.01,2.49,5.37;4,228.91,344.36,12.48,5.37;4,228.91,334.71,12.48,5.37;4,228.91,325.06,12.48,5.37;4,228.91,315.41,12.48,5.37;4,226.42,305.76,14.97,5.37;4,226.42,296.11,14.97,5.37;4,226.42,286.46,14.97,5.37;4,226.42,276.80,14.97,5.37;4,220.01,302.67,5.37,22.47;4,297.70,264.07,24.19,6.44;4,222.95,378.19,169.46,7.86;4,222.95,389.15,43.35,7.86"><head></head><label></label><figDesc>Hist. of post length used in A and C in words. Hist. of total subject posts length used in B in words. Hist. of post length used in D and E in characters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,146.74,410.09,321.88,8.74"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Histograms of length of posts of models A and C, B, and D and E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,184.39,268.23,246.57,8.74"><head></head><label></label><figDesc>Fig. 2: F1 variations with variable threshold experiments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,160.07,357.93,295.22,8.74"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Number of writings per user training and validation datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,221.35,310.57,172.66,8.74;8,134.77,115.83,103.75,166.28"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Structure of models A, B and D.</figDesc><graphic coords="8,134.77,115.83,103.75,166.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,228.48,748.18,158.40,8.74;11,134.77,435.02,345.83,284.70"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Structure of models C and E.</figDesc><graphic coords="11,134.77,435.02,345.83,284.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,224.77,535.44,165.82,8.74"><head>Table 1 :</head><label>1</label><figDesc>Best found model thresholds.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,136.16,127.36,373.37,206.90"><head>Table 2 :</head><label>2</label><figDesc>Hyperparameters tunned in the experiments.</figDesc><table coords="6,136.16,139.70,373.37,194.56"><row><cell cols="2">Hyperparameter Type</cell><cell>Explanation</cell></row><row><cell>load emb</cell><cell>Boolean</cell><cell>Determines if the model will use pre-trained embeddings</cell></row><row><cell>emb size</cell><cell cols="2">Numerical Defines the dimension of the embedding vector</cell></row><row><cell cols="2">trainable emb Boolean</cell><cell>Determines if the embeddings are further trainable or not</cell></row><row><cell>cnn size</cell><cell>Vector</cell><cell>Defines not only the number of kernels of the CNN, but also the number of stacked CNN layers the model will have</cell></row><row><cell>cnn filter</cell><cell>Vector</cell><cell>Defines the size of the kernel used, as well as the number of them</cell></row><row><cell>rnn size</cell><cell>Vector</cell><cell>Defines the number of stacked RNN layers as well as their dimensions</cell></row><row><cell>cell type</cell><cell cols="2">Categorical Determines the RNN cell used will be a LSTM or a GRU one</cell></row><row><cell>bidirectional</cell><cell>Boolean</cell><cell>Defines if a bidirectional RNN is used or not</cell></row><row><cell>attention</cell><cell>Boolean</cell><cell>Determines if the output of the RNN goes trhough an attention mechanism</cell></row><row><cell>dropout</cell><cell cols="2">Numerical Defines the dropout the network will be using in training phase</cell></row><row><cell>dnn size</cell><cell>Vector</cell><cell>Defines the number of stacked fully connected layers as well as their dimensions</cell></row><row><cell>batch size</cell><cell cols="2">Numerical Defines the number of instances processed in the same batch</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,136.16,127.36,354.11,75.39"><head>Table 3 :</head><label>3</label><figDesc>Best found model configuration.</figDesc><table coords="9,136.16,139.70,354.11,63.06"><row><cell cols="8">Models Emb Size CNN cells Kernel Size RNN RNN Type Bidirectional FNN Batch Size</cell></row><row><cell>A</cell><cell>300</cell><cell>128</cell><cell>3</cell><cell cols="2">None None</cell><cell>None</cell><cell>32 1024</cell></row><row><cell>B</cell><cell>300</cell><cell>None</cell><cell>None</cell><cell>64</cell><cell>GRU</cell><cell>True</cell><cell>32 32</cell></row><row><cell>C</cell><cell>None</cell><cell>None</cell><cell>None</cell><cell>64</cell><cell>LSTM</cell><cell>True</cell><cell>32 1</cell></row><row><cell>D</cell><cell>50</cell><cell>128</cell><cell>10</cell><cell cols="2">None None</cell><cell>None</cell><cell>64 1024</cell></row><row><cell>E</cell><cell>None</cell><cell>None</cell><cell>None</cell><cell>64</cell><cell>GRU</cell><cell>False</cell><cell>32 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,191.42,228.65,232.52,53.48"><head>Table 4 :</head><label>4</label><figDesc>F1 validation scores achieved by the models.</figDesc><table coords="9,234.97,240.99,145.42,41.14"><row><cell>Model</cell><cell>A B C</cell><cell>D E</cell></row><row><cell cols="3">F1 (class 1) 0.27 0.12 0.57 0.26 0.23</cell></row><row><cell cols="3">Macro F1 0.59 0.52 0.75 0.54 0.57</cell></row><row><cell cols="3">Weighted 0.85 0.81 0.89 0.77 0.83</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,198.75,127.36,217.85,75.39"><head>Table 5 :</head><label>5</label><figDesc>F1 official scores achieved by the models.</figDesc><table coords="10,223.77,139.70,167.83,63.06"><row><cell>Model</cell><cell>A B C D E Best</cell></row><row><cell>F1</cell><cell>0.20 0.15 0.13 0.16 0.16 0.71</cell></row><row><cell>ERDE5</cell><cell>0.07 0.11 0.13 0.09 0.10 0.06</cell></row><row><cell>ERDE50</cell><cell>0.07 0.08 0.12 0.07 0.08 0.03</cell></row><row><cell>Latency -Weighted F1</cell><cell>0.20 0.15 0.09 0.16 0.16 0.69</cell></row></table><note coords="11,283.23,412.65,48.90,7.86;11,283.17,727.24,49.03,7.86"><p>(a) C model (b) E model</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="programName">Research Program</rs> <rs type="funder">of the Ministry of Economy and Competitiveness -Government of Spain</rs>, (<rs type="projectName">DeepEMR</rs> project <rs type="grantNumber">TIN2017-87548-C2-1-R</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_QneRcc8">
					<idno type="grant-number">TIN2017-87548-C2-1-R</idno>
					<orgName type="project" subtype="full">DeepEMR</orgName>
					<orgName type="program" subtype="full">Research Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,144.86,278.40,7.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,205.30,144.88,61.96,7.86">Random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,273.90,144.88,69.14,7.86">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,156.30,337.63,7.86;12,151.52,167.26,329.08,7.86;12,151.52,178.22,279.86,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,331.31,156.30,149.28,7.86;12,151.52,167.26,26.22,7.86">Quantifying mental health signals in twitter</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Coppersmith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,198.75,167.26,281.85,7.86;12,151.52,178.22,205.78,7.86">Proceedings of the workshop on computational linguistics and clinical psychology: From linguistic signal to clinical reality</title>
		<meeting>the workshop on computational linguistics and clinical psychology: From linguistic signal to clinical reality</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,189.64,337.63,7.86;12,151.52,200.60,298.90,7.86" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3722</idno>
		<title level="m" coord="12,253.79,189.64,226.80,7.86;12,151.52,200.60,136.67,7.86">word2vec explained: deriving mikolov et al.&apos;s negativesampling word-embedding method</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.96,212.01,337.64,7.86;12,151.52,222.97,329.07,7.86;12,151.52,233.93,158.14,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,256.41,212.01,224.19,7.86;12,151.52,222.97,152.94,7.86">A probabilistic interpretation of precision, recall and fscore, with implication for evaluation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,327.71,222.97,152.88,7.86;12,151.52,233.93,34.93,7.86">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="345" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,245.35,337.63,7.86;12,151.52,256.31,188.68,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,250.84,245.35,179.89,7.86">Alteraciones menstruales y anorexia nerviosa</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">E</forename><surname>Gutiérrez-Barquín</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,437.70,245.35,42.89,7.86;12,151.52,256.31,107.29,7.86">Trastornos de la conducta alimentaria</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="277" to="284" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,267.73,337.63,7.86;12,151.52,278.69,329.07,7.86;12,151.52,289.64,302.83,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,454.84,267.73,25.75,7.86;12,151.52,278.69,242.42,7.86">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,416.16,278.69,64.44,7.86;12,151.52,289.64,252.02,7.86">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,301.06,337.63,7.86;12,151.52,312.02,329.07,7.86;12,151.52,322.98,86.01,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,328.09,301.06,152.50,7.86;12,151.52,312.02,103.94,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,275.64,312.02,200.74,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,334.40,337.64,7.86;12,151.52,345.36,329.07,7.86;12,151.52,356.32,329.07,7.86;12,151.52,367.28,25.60,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,316.35,334.40,164.24,7.86;12,151.52,345.36,183.48,7.86">erisk 2017: Clef lab on early risk prediction on the internet: experimental foundations</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,355.60,345.36,125.00,7.86;12,151.52,356.32,235.77,7.86">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="346" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,378.69,337.64,7.86;12,151.52,389.65,329.07,7.86;12,151.52,400.61,220.04,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,311.48,378.69,169.11,7.86;12,151.52,389.65,45.43,7.86">Overview of erisk: Early risk prediction on the internet</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,216.41,389.65,264.18,7.86;12,151.52,400.61,96.22,7.86">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="343" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,412.03,337.98,7.86;12,151.52,422.99,329.07,7.86;12,151.52,433.95,329.07,7.86;12,151.52,444.91,275.83,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,315.66,412.03,164.93,7.86;12,151.52,422.99,88.92,7.86">Overview of eRisk 2019: Early Risk Prediction on the Internet</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,260.50,422.99,220.09,7.86;12,151.52,433.95,329.07,7.86;12,151.52,444.91,16.79,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 10th International Conference of the CLEF Association, CLEF 2019</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,456.32,337.98,7.86;12,151.52,467.28,329.07,7.86;12,151.52,478.22,114.97,7.89" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,318.31,456.32,162.29,7.86;12,151.52,467.28,218.97,7.86">Personal sensing: Understanding mental health using ubiquitous sensors and machine learning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">C</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Schueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,378.26,467.28,102.33,7.86;12,151.52,478.24,43.82,7.86">Annual review of clinical psychology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="23" to="47" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,489.66,337.98,7.86;12,151.52,500.62,180.75,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="12,346.38,489.66,134.22,7.86;12,151.52,500.62,30.35,7.86">Introduction to linear regression analysis</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">C</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Peck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Vining</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">821</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,512.04,337.98,7.86;12,151.52,523.00,329.07,7.86;12,151.52,533.95,74.01,7.86;12,134.76,545.37,345.83,7.86;12,151.52,556.33,249.33,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="12,169.41,523.00,311.18,7.86;12,151.52,533.95,74.01,7.86;12,134.76,545.37,7.85,7.86;12,281.43,545.37,199.17,7.86;12,151.52,556.33,249.33,7.86">Early detection of signs of anorexia and depression over social media using effective machine learning frameworks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Ortega-Mendoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>López-Monroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Franco-Arcega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Kalyani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Basu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Peimex at erisk2018: Emphasizing personal information for depression and anorexia detection 14</note>
</biblStruct>

<biblStruct coords="12,142.61,567.75,337.97,7.86;12,151.52,578.71,25.60,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,215.31,567.75,81.23,7.86">Explaining adaboost</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,318.29,567.75,76.62,7.86">Empirical inference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="37" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,590.13,337.97,7.86;12,151.52,601.09,222.60,7.86" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<title level="m" coord="12,262.96,590.13,217.62,7.86;12,151.52,601.09,144.76,7.86">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,612.50,337.97,7.86;12,151.52,623.46,329.07,7.86;12,151.52,634.42,236.09,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,411.57,612.50,69.02,7.86;12,151.52,623.46,256.12,7.86">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,430.85,623.46,49.74,7.86;12,151.52,634.42,152.01,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="801" to="809" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,645.84,337.98,7.86;12,151.52,656.80,272.68,7.86" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="12,313.02,645.84,167.58,7.86;12,151.52,656.80,272.68,7.86">Word embeddings and linguistic metadata at the clef 2018 tasks for early detection of depression and anorexia</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Trotzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,119.67,337.98,7.86;13,151.52,130.63,231.63,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="13,320.20,119.67,160.39,7.86;13,151.52,130.63,231.63,7.86">A neural network approach to early risk detection of depression and anorexia on social media text</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen12</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,141.59,337.97,7.86;13,151.52,152.55,97.80,7.86" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01710</idno>
		<title level="m" coord="13,262.47,141.59,144.18,7.86">Text understanding from scratch</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.62,163.51,337.98,7.86;13,151.52,174.44,329.07,7.89;13,151.52,185.43,51.70,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,284.30,163.51,196.29,7.86;13,151.52,174.47,40.35,7.86">Understanding bag-of-words model: a statistical framework</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,200.74,174.47,248.15,7.86">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="43" to="52" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
