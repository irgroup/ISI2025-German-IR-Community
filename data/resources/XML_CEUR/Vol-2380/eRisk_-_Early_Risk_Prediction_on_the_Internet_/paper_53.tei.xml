<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.23,133.89,320.91,12.62">eRISK: Comparing Word Embedding Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,139.61,171.56,48.98,8.74"><forename type="first">Elena</forename><surname>Fano</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Uppsala University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Gavagai</orgName>
								<address>
									<settlement>Stockholm</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.72,171.56,62.85,8.74"><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Gavagai</orgName>
								<address>
									<settlement>Stockholm</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<settlement>Stockholm</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.55,183.51,58.55,8.74"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Uppsala University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Uppsala University and Gavagai at CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.23,133.89,320.91,12.62">eRISK: Comparing Word Embedding Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">537F48BAE6C559BF0B30B1E3BEE5681A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic vectors</term>
					<term>Word embeddings</term>
					<term>Author classification 1 eRISK and the Challenge of Sequence Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes an experiment to evaluate the performance of three different types of semantic vectors or word embeddingsrandom indexing, GloVe, and ELMo-and two different classification architectures-linear regression and multi-layer perceptrons-for the specific task of identifying authors with eating disorders from writings they publish on a discussion forum. The task requires the classifier to process texts written by the authors in the sequence they were published, and to identify authors likely to be at risk of suffering from eating disorders as early as possible. The data are part of the eRISK evaluation task of CLEF 2019 and evaluated according to the eRISK metrics. Contrary to our expectations, we did not observe a clear-cut advantage using the recently popular contextualized ELMo vectors over the commonly used and much more light-weight GloVe vectors, or the more handily learnable random indexing vectors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>metrics have been formulated to penalise missed cases, false positives, and late detection <ref type="bibr" coords="2,177.93,130.95,14.61,8.74" target="#b12">[13]</ref>.</p><p>Most authors in the test set discuss a broad range of innocuous topics unrelated to self-harm and eating disorders. Many authors discuss eating disorders without themselves being afflicted, or even to discuss how they overcame their ailments and no longer suffer from them. To some extent, the hypotheses of the challenge task is that even other writings may reveal personality traits or social context of relevance for a diagnosis, but mostly, the task is about identifying relevant texts among many less relevant ones, and to do so quickly, since waiting incurs a penalty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>In 2017, CLEF (Conference and Labs of the Evaluation Forum) introduced a new laboratory, with the purpose to set up a shared task for Early Risk Prediction on the Internet (eRisk). The first edition was mainly meant as a trial run to chart the specific challenges and possibilities of this task.</p><p>The first full-fledged shared task was launched in <ref type="bibr" coords="2,369.43,332.70,18.15,8.74">2018</ref>. In what follows, we will go over some of the strategies used by the teams that submitted a system for Task 2, detection of anorexia, focusing on the approached most similar to ours.</p><p>Roughly, the solutions can be divided into traditional machine learning approaches and other approaches based on different types of document and feature representations, but many teams used a combination of both. Some researchers also came up with innovative solutions to deal with the temporal aspect of the task.</p><p>A common theme was to focus on the difference in performance between manually engineered (meta-)linguistic features and automatic text vectorization methods. For example, contributions of <ref type="bibr" coords="2,304.56,464.63,15.50,8.74" target="#b23">[24]</ref> and <ref type="bibr" coords="2,340.81,464.63,15.50,8.74" target="#b19">[20]</ref> both dealt with this research question. For a more detailed description of <ref type="bibr" coords="2,325.87,476.58,15.50,8.74" target="#b23">[24]</ref> see below. The other team used a combination of over 50 linguistic features for two of their models, and doc2vec <ref type="bibr" coords="2,134.77,500.49,14.61,8.74" target="#b10">[11]</ref>, which is a neural text vectorization method, for the other three. When they submitted their 5 runs, they used the feature-based models alone or in combination with the text vectorization models, but they report that they did not submit any doc2vec model alone because of the poor performance shown in their development experiments.</p><p>Probably the most specific challenge of this task was building a model which could take the temporal progression into account. One of the teams that obtained the best scores, the UNSL team <ref type="bibr" coords="2,316.37,584.39,9.96,8.74" target="#b5">[6]</ref>, built a time-aware system which used algorithms invented specifically for this task. Among the other teams, <ref type="bibr" coords="2,465.09,596.34,15.50,8.74" target="#b18">[19]</ref> use an approach that bears some resemblance to our system. They stacked two classifiers, the first one which predicted what they call the "mood" of the texts (positive or negative), and the second which was in charge of making a decision given this prediction. The main difference is that they were operating with a chunk based system, so they had to build models of different sizes to be able to make a prediction without having seen all the chunks, whereas our second classifier operates on a text-by-text basis. Furthermore, their first models uses Bayesian inversion on the text vectorization models, whereas we used a feedforward neural network with LSTMs.</p><p>Other notable approaches were to look specifically at sentences which referred to the user in the first person <ref type="bibr" coords="3,262.26,188.16,14.61,8.74" target="#b14">[15]</ref>, or to build different classifiers that specialized in accurately predicting positive cases and negative cases <ref type="bibr" coords="3,389.90,200.12,9.96,8.74" target="#b1">[2]</ref>. If one of the two models' output rose above a predetermined threshold of confidence, that decision was emitted; if none of the models or both of them were above the threshold, the decision was delayed. Another team used latent topics to help in classification and focused on topic extraction algorithms <ref type="bibr" coords="3,325.06,247.94,14.61,8.74" target="#b13">[14]</ref>.</p><p>The FHDO team <ref type="bibr" coords="3,228.41,269.29,15.50,8.74" target="#b23">[24]</ref> employed a machine learning approach rather similar to ours for some their models. They submitted five runs to Task 2, and they obtained the best score in three out of five evaluation measures. Models three and four were regular machine learning models, whereas models one, two and five were ensemble models that combined different types of classifiers to make predictions. This team used some hand-crafted metadata features for their first model, for example the number of personal pronouns, the occurrence of some phrases like "my therapist", and the presence of words that mark cognitive processes.</p><p>Their first and second models consisted of an ensemble of logistic regression classifiers, three of them based on bags of words with different term weightings and the fourth, present only in their first model, based on the metadata features. The predictions of the classifiers were averaged and if the result was higher than 0.4 the user was classified as at risk. These models did not obtain any high scores, contrary to other models submitted by this team.</p><p>Their third and fourth models were convolutional neural networks (CNN) with two different types of word embeddings: GloVe and FastText. The GloVe embeddings were 50-dimensional, pre-trained on Wikipedia and news texts, whereas the FastText embeddings were 300-dimensional, and trained on social media texts expressly for this task. The architecture of the CNN was the same for both models, with one convolutional layer and 100 filters. The threshold to emit a decision of risk was set to 0.4 for model 3 and 0.7 for model 4. Unsurprisingly, the model with larger embedding size and specifically trained vectors performed best, reporting the highest recall (0.88) and the lowest ERDE 50 (5.96%) in the 2018 edition of eRisk. ERDE stands for Early Risk Detection Error and is an evaluation metric created to track the performance of early risk detection systems (see Section 4).</p><p>The fifth model presented in <ref type="bibr" coords="3,279.69,620.25,15.50,8.74" target="#b23">[24]</ref> was an ensemble of the two CNN models and their first model, the bag of words model with metadata features. This model obtained the highest F1 in the shared task, namely 0.85, and came close to the best scores even for the two ERDE measures.</p><p>We have two experimental foci: the representation of lexical items, and the classification step given such representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic Vectors or Word Embeddings</head><p>We represent lexical items in the posts under analysis as word embeddings, vectors of real numbers, under the assumption that a vector space representation allows for generalisation from the lexical items themselves to a more conceptual level of semantics. By allowing the classification scheme to relax the representation to include near neighbours in semantic space, we hope to achieve better recall than otherwise were possible. Semantic vectors are convenient as a learning representation, allowing for aggregation of distributional context, but if used blindly, risk bringing in contextual information of little or even confounding relevance. In general, semantic spaces built from similar data sets with similar aggregation parameters should represent the same information and the actual aggregation process is of less importance, but implementational details may have effects on the usefulness of the semantic space. Parameters of importance have obviously to do with size and selection of data set, but also how the distributional context is defined, the dimensionality or level of compression of the representation, weighting of items based on their information content, and how rare or previously unseen items are treated. In these experiments we compare three semantic vector models: Random Indexing which is used in commercial applications; GloVe, which is used in a broad range of recent academic experiments; and the recently published ELMo, which has shown great promise to provide a better and more sensitive representation for general purpose application.</p><p>Random Indexing Random indexing is based on the Sparse Distributed Memory model formulated by Pentti Kanerva <ref type="bibr" coords="4,311.04,476.79,15.50,8.74" target="#b9">[10]</ref> which is intended to be both neurophysiologically plausible and efficiently implementable for large streaming data. Random indexing is built for seamless online learning without explicit compilation steps, and is based on a fixed-dimensional representation of typically around 1000 dimensions. The vectors are built by simple operations: each lexical item is assigned a randomly generated sparse index vector and an initially empty context vector. The latter is populated for each lexical item by, for each observed occurrence of it, adding in index vectors of items observed within a context of interest such as a window of preceding and succeeding items. If the objective of the semantic space is to encode synonymy or other close semantic relations, a window of two preceding and succeeding items is used as a context. Preceding and succeeding items are kept separate to preserve sequential information in the representation, implemented by applying separate permutations for preceding and succeeding items <ref type="bibr" coords="4,234.67,632.21,14.61,8.74" target="#b22">[23]</ref>. In the present experiments, we use a large 2000dimensional semantic space trained on several years of social and news media by Gavagai for inclusion in their commercial tools <ref type="bibr" coords="4,358.68,656.12,14.61,8.74" target="#b21">[22]</ref>. Vectors are normalised to length 1 and items that are not found in the vocabulary are represented with empty vectors and thus do not contribute to the classification.</p><p>GloVe Global Vectors (or GloVe for short) are semantic vectors which are built to provide downstream processes with handy access to lexical cooccurrence data from large data sets <ref type="bibr" coords="5,222.04,187.50,14.61,8.74" target="#b16">[17]</ref>. The vectors are populated with data from cooccurrence within a 15-word window, thus providing a more associative relation between items than the random indexing model above. The quality of the vectors has proven useful for a wide range of tasks and GloVe vectors have in recent years been used as a standard way of achieving a conceptual generalisation from simple words in text. There are several GloVe vector sets that can be retrieved at no cost, and in these experiments we chose a 200-dimensional set provided by the Stanford NLP group trained on microblog data which we judged to be the closest fit to the data under analysis. <ref type="foot" coords="5,266.03,281.57,3.97,6.12" target="#foot_0">4</ref> Items that are not found in the vocabulary are replaced with a stand-in vector populated with values from a normal distribution where the mean and standard deviation are obtained from all available vectors.</p><p>ELMo Semantic vector models in general produce vectors that are intended to encode information from language usage in general (or language usage in the training set). They do not accommodate to the specific task at hand and are trained on large amounts of previous knowledge. Recent approaches try to address the challenge of domain and task accommodation more explicitly by combining a previously trained general representation with a more rapid learning process on the data set under analysis. For linguistic data, ELMo (Embeddings from Language Models) proposed by <ref type="bibr" coords="5,298.90,423.38,15.50,8.74" target="#b17">[18]</ref> is one such model. ELMo representations are different from traditional semantic vectors in that individual vectors are generated for each token in the data under analysis, based on a large pretrained language model represented in a richer three-level representation trained on sentence-by-sentence cooccurrences. The ELMo processing model incorporates a character-based model which means that no items will be out of vocabulary: previously unseen items inherit a representation based on the similarity of their character sequence to other known items. We use the AllenNLP python package to generate vectors <ref type="bibr" coords="5,259.41,519.02,9.96,8.74" target="#b6">[7]</ref>. Each lexical item is represented by an average of the three ELMo layers in one 1024-dimensional vector and they are passed in, sentence by sentence to the classifier.</p><p>Baseline representation As a baseline we use randomly initialized word embeddings obtained from the Keras embedding layer. First a tokenizer is used to obtain a list of all lexical items in the training set. Only the top most common 10,000 words are considered for the classification task, and they are converted into 100-dimensional word vectors generated by Keras. These vectors thus contain no information about previous usage of the lexical items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classifier</head><p>The first step in our processing pipeline involves building a text classifier. Texts are classified to be written either by authors with eating disorders or by authors without eating disorders. This is in keeping with the underlying hypothesis above, that some characteristics of authors with eating disorders may be discernible even in texts about other topics. Text classification is done with a Recurrent Neural Network (RNN) implemented with Long Short-Term Memory cells (LSTMs). Recurrent neural networks are neural architectures where the output of the hidden layer at each time step is also used as input for the hidden layer at the next time step. This type of processing model is particularly suitable for tasks that involve processing of sequences, for example sentences in natural language. LSTM cells retain information over longer distances than regular RNN cells <ref type="bibr" coords="6,217.54,272.83,9.96,8.74" target="#b7">[8]</ref>. Our neural architecture consists of an embedding layer, two hidden layers of size 100 and a fully connected layer with one neuron and sigmoid activation (as illustrated in Figure <ref type="figure" coords="6,322.29,296.74,3.87,8.74">3</ref>.2). The embedding layer differs according to which type of representations we use for each model, whereas the rest of the model is equivalent for all of our neural models. The output layer with a sigmoid activation function makes sure that the network assigns a probability to each text instead of a class label. We set the maximum sentence length to 100 words and the vocabulary to 10,000 words in order to make the training process more efficient. This recurrent neural network takes care of the text classification task: it outputs the probability that each text belongs to the 1 (at risk) class. The output of the text classifier is passed on as input to a second author classifier in a feature vector composed of the following elements:</p><p>-The number of texts seen up to that point, min-max scaled to match the order of magnitude of the other features -The average score of the texts seen up to that point -The standard deviation of the scores seen up to that point -The average score of the top 20% texts with the highest scores -The difference between the average of the top 20% and the bottom 20% of texts.</p><p>We experimented with two architectures for the author classifier: logistic regression and multi-layer perceptron. Logistic regression is a linear classifier that uses a logistic function to model the probability that an instance belongs to the default class in a binary classification problem. A multi-layer perceptron, on the other hand, is a deep feed-forward neural network, and therefore a non-linear classifier. We tested their performance by feeding each architecture with identical input from the text classifier. We varied different hyperparameters such as embedding size, hidden layer size, number of layers, vocabulary size, etc., to find the best combination, also taking practical issues such as training time into account. One important factor to keep in mind is that we wanted to compare word embedding methods, so it was desirable to have the same (or very similar) settings for all models. During our development phase we found that often a hyperparameter setting that worked well for one model was not ideal for another model, and compromises had to be made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Practical Considerations</head><p>For the implementation we use Sci-kit learn <ref type="bibr" coords="7,338.43,357.24,15.50,8.74" target="#b15">[16]</ref> and Keras <ref type="bibr" coords="7,408.97,357.24,9.96,8.74" target="#b2">[3]</ref>, two popular Python packages that support traditional machine learning algorithms as well as deep learning architectures, and we use NLTK for preprocessing purposes <ref type="bibr" coords="7,467.31,381.15,9.96,8.74" target="#b0">[1]</ref>. We pre-processed the documents in the same way for all our runs: we used the stop-word list provided with the package Natural Language Toolkit, but we did not remove any pronouns, as they have been found to be more prominent in the writing style of mental health patients. We replaced URLs and long numbers with ad hoc tokens and the Keras tokenizer filters out punctuation, symbols and all types of blank space characters.</p><p>We only took into consideration those messages where at least one out of the text and title fields was not blank. Similarly, at test time we did not process blank documents, instead we repeated the prediction from the previous round if we encountered an empty document. If any empty documents appeared in the first round, we emitted a decision of 0, following the rationale that in absence of evidence we should assume that the user belongs to the majority class.</p><p>The text classifier is trained on the training set with a validation split of 0.2 using model checkpoints to save the models at each epoch, and early stopping based on validation loss. Two dropout layers are added after the hidden LSTM layers with a probability of 0.5. Both early stopping and dropout are intended to avoid overfitting, given that the noise in the data makes the model more prone to this type of error.</p><p>For the author classifier, we experimented with different settings for logistic regression and the multi-layer perceptron. For the logistic regression classifier, we used the SAGA optimizer <ref type="bibr" coords="7,247.37,632.21,9.96,8.74" target="#b3">[4]</ref>. We used balanced class weight to give the minority class (the positive cases) more importance during training. For the multi-layer perceptron, we used two hidden layers of size 10 and 2.</p><p>Since we need to focus on early prediction of positive cases and on recall, precision in our system tends to suffer. In order to improve precision as much as possible, we experimented with different cut-off points for the probability scores to try to reduce the number of false positives as much as possible. We ended up using a high cut-off probability of 0.9 for a positive decision, because we found that this did not affect our recall score too badly, and it did help improve precision. We made the practical assumption that a good balance between precision and recall would more useful in a real-life setting than really good scores on the early prediction metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Metrics</head><p>Precision and Recall Precision and recall are calculated over only the positive items in the test set and they are combined into the F 1 score in the traditional way.</p><p>ERDE Originally proposed by the eRisk organisers in 2016 <ref type="bibr" coords="8,409.10,322.51,15.50,8.74" target="#b11">[12]</ref> and applied in every year since, the Early Risk Detection Error (ERDE) score takes into account both the correctness of the decision and the number of texts needed to emit that decision. ERDE assigns each classification decision-in this case, identifying a user to be ill or healthy-by a system an editorially determined cost: c f n for false negatives, c f p for false positives, c tn for true negatives, and c tp for true positives. The true positive score c tp is weighted by a latency cost factor lc(o, k), where k is the number of texts seen by the system before a decision is made and o is a parameter to control for how many texts are considered to be acceptable or expected before a decision is made. The lc(o, k) factor increases rapidly after o texts have been seen. The objective is to minimise this score. In the 2019 evaluation cycle, c tn was set to 0, c f n to 1, c f p to the relative frequency of the positive items in the test set, c tp to 1, and o variously to 5 and 50, shown as ERDE 5 and ERDE 50 respectively.</p><p>Latency Proposed by Sadeque et al <ref type="bibr" coords="8,304.56,507.25,14.61,8.74" target="#b20">[21]</ref>, the latency measure is the median of the number of documents seen by the system until it makes a determination that a user is at risk. This is only computed for true positives, and thus carries no penalty for false or missed positives. The latency score is can be reformulated as a speed factor which is used to rescore the raw F 1 score to a latency-weighted F latency score. A system which identifies positive items from their first writing will have F 1 = F latency .</p><p>Ranking based metrics: P@10, nDCG@10, nDCG@100 The participating systems were required to rank the users in order of assessed risk, and then the precision of that list was measured at 10 items, and compared to a perfect ranking at 10 and at 100 using the normalised discounted cumulative gain measure (nDCG) <ref type="bibr" coords="8,212.82,656.12,9.96,8.74" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Official Results</head><p>We submitted 5 runs to the official test phase, the maximum number allowed for each team. They are listed in Table <ref type="table" coords="9,288.38,172.73,3.87,8.74" target="#tab_0">1</ref>. A total of 13 teams successfully completed at least one run in eRISK. Some teams stopped processing texts before the stream of 2000 texts was exhausted. Unfortunately, due to a processing error, our submissions were among them: we only processed the first round of texts and emitted our decisions based on that. Our official scores are thus not based on the entire test material but are an extreme case of early risk detection, based on the first text round only. The results are given in Tables <ref type="table" coords="9,397.44,244.47,4.98,8.74" target="#tab_1">2</ref> and<ref type="table" coords="9,425.12,244.47,3.87,8.74">3</ref>. The model with the best performance was run 4, with random indexing vectors and a multi-layer perceptron. This holds for both the decision-based evaluation and the ranking-based evaluation. The only exception to this is the best Table <ref type="table" coords="10,164.20,185.28,4.13,7.89">3</ref>. An extract of the results table provided by the organizers for the rankingbased evaluation, concerning our team UppsalaNLP.</p><formula xml:id="formula_0" coords="9,230.13,288.33,17.02,7.86">Run</formula><p>recall score, obtained by the baseline model with a logistic regression classifier.</p><p>In development experiments we found that the random indexing model had the least number of false positives and that the multi-layer perceptron balances precision and recall well. We believe that the GloVe model, in combination with the multi-layer perceptron, is too conservative to give a good performance after only one round of texts, whereas the random indexing model strikes a better balance early on in the data stream.</p><p>Compared to the other submissions, our scores for the decision-based evaluation were excellent in terms of latency, speed, and ERDE 5 , since we always made our decisions at the first possible time. On most other evaluation parameters our offical scores were ranked in the lower third, if compared to the best scores of the other participants. For the ranking results, given in Table <ref type="table" coords="10,455.01,373.22,3.87,8.74">3</ref>, the results were more respectable (although due to the processing error, they did not change as more data was processed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Continued Experimentation</head><p>After the official testing period was over, the organizers made the test set available to the participating teams. This allowed us to carry out continued experiments, including ELMo which was not practicable during the official training period due to lengthy processing times. Table <ref type="table" coords="10,340.91,488.75,4.98,8.74" target="#tab_2">4</ref> shows the performance of our models on the official test set. We used a script provided by the organizers to evaluate precision, recall, F 1 , and ERDE, so the results should be comparable to the official ones. These results should be comparable to the results reported in the table, because they are obtained under the same testing conditions. We found that once the processing error was sorted out, we were able to produce scores on par with the top participants: our best F 1 score on the test set was 0.68, whereas the best F 1 in the shared task was 0.71, and we obtained a recall of 0.9 which is close to the best score of 1.0, obtained by a team that heavily sacrificed precision. For ERDE 5 and ERDE 50 more than one team shared the first place with the same non-perfect scores of respectively 0.06 and 0.03. These values are likely rounded up the the nearest percentage point, and if we do the same thing with our continued results, we actually obtain an ERDE 5 of 0.04 for all the vector representations in using the logistic regression model, and an ERDE 50 of 0.02 for our GloVe/ELMo and logistic regression models. More de-tails about these further experiments can be found in a comprehensive report by Fano <ref type="bibr" coords="11,173.09,130.95,9.96,8.74" target="#b4">[5]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Lessons Learnt</head><p>We found that in the continued experiments, the model with GloVe embeddings and the multi-layer perceptron classifier had the best precision, without sacrificing recall. ELMo vectors did not make much of a difference for the multi-layer perceptron condition, but held a slight edge on the generally lower performing logistic regression classifier. In general, the benefit of using knowledge from the generalised vector models was relatively small. Compared to the baseline, the three pre-trained models show a better balance between precision and recall, and they also show worse ERDE scores, which are a symptom of a more conservative behavior, especially in the early phases.</p><p>Regarding the difference between the logistic regression and multi-layer perceptron classifiers, we could detect a clearer trend on the test set than we had on the development set. We had already observed that logistic regression seemed to lead to worse precision scores, but on the test set we could also determine that it also gave rise to better ERDE scores. This result can be explained as follows: if the system incurs in many false positives, it will likely also be able to correctly identify the true positives, and zooming in on many true positives early on also leads to good ERDE scores.</p><p>The more far-reaching conclusions that can be drawn from our experiments is that the choice of representation and classifier does have some effect on the results, and that the chronological aspect of this task made clear the compound effect of learning curves and robustness of the combination of the two: more conservative models which are likely to perform better in the long run suffer from not daring to pronounce a decision early in the sequence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,169.03,509.98,277.30,7.89;6,134.77,398.92,345.82,96.29"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The diagram illustrates the architecture of the text classifier.</figDesc><graphic coords="6,134.77,398.92,345.82,96.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,134.77,277.32,345.82,171.35"><head>Table 1 .</head><label>1</label><figDesc>Summary of the models used in the 5 runs submitted to the eRisk 2019 shared task.</figDesc><table coords="9,243.06,277.32,142.16,141.36"><row><cell></cell><cell>Vector type</cell><cell></cell></row><row><cell>ID</cell><cell>for</cell><cell>Author classifier</cell></row><row><cell></cell><cell>text classifier</cell><cell></cell></row><row><cell>0</cell><cell>Baseline</cell><cell>Logistic Regression</cell></row><row><cell>1</cell><cell>Baseline</cell><cell>Multi-layer Perceptron</cell></row><row><cell>2</cell><cell>GloVe</cell><cell>Logistic Regression</cell></row><row><cell>3</cell><cell>GloVe</cell><cell>Multi-layer Perceptron</cell></row><row><cell>4</cell><cell>Random indexing</cell><cell>Multi-layer Perceptron</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,134.77,578.01,345.83,18.85"><head>Table 2 .</head><label>2</label><figDesc>An extract of the results table provided by the organizers for the decisionbased evaluation, concerning our team UppsalaNLP.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,134.77,165.46,345.82,192.48"><head>Table 4 .</head><label>4</label><figDesc>The results of our experiments on the test set. The scores are reported in percentages, except precision, recall and F1 which are reported in decimal points.</figDesc><table coords="11,159.94,165.46,295.48,162.48"><row><cell></cell><cell cols="5">Baseline Rand. Ind. GloVe ELMo Best official score</cell></row><row><cell>LSTM classifier</cell><cell>Accuracy 96.17</cell><cell>96.57</cell><cell cols="2">96.79 96.67</cell></row><row><cell></cell><cell cols="2">Accuracy 96.33 95.8</cell><cell cols="2">93.76 95.64</cell></row><row><cell>Logistic regression classifier</cell><cell>Precision 0.35 Recall 0.89 F1 0.5 ERDE5 4.01</cell><cell>0.34 0.89 0.49 4.22</cell><cell cols="2">0.4 0.9 0.55 0.56 0.41 0.9 4.49 3.64</cell><cell>0.77 1.0 0.71 6</cell></row><row><cell></cell><cell>ERDE50 2.68</cell><cell>2.58</cell><cell cols="2">2.45 2.27</cell><cell>3</cell></row><row><cell></cell><cell>Accuracy 97.76</cell><cell>97</cell><cell cols="2">97.48 98.13</cell></row><row><cell>Multi-layer perceptron classifier</cell><cell>Precision 0.49 Recall 0.77 F1 0.6 ERDE5 5.08</cell><cell>0.64 0.63 0.63 6.51</cell><cell cols="2">0.68 0.65 0.67 0.7 0.68 0.67 6.98 6.81</cell><cell>0.77 1.0 0.71 6</cell></row><row><cell></cell><cell>ERDE50 3.46</cell><cell>4.46</cell><cell>4.3</cell><cell>3.85</cell><cell>3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="5,144.73,656.80,164.03,7.86"><p>https://nlp.stanford.edu/projects/glove/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,218.28,337.63,7.86;12,151.52,229.24,269.46,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="12,271.21,218.28,209.38,7.86;12,151.52,229.24,152.28,7.86">Natural language processing with Python: analyzing text with the natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,240.99,337.64,7.86;12,151.52,251.95,329.07,7.86;12,151.52,262.91,167.87,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,376.18,240.99,104.41,7.86;12,151.52,251.95,129.00,7.86">Analysis and experiments on early detection of depression</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Cacheda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">F</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Nóvoa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,303.09,251.95,177.50,7.86;12,151.52,262.91,139.20,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,274.66,203.96,7.86" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<title level="m" coord="12,226.66,274.66,21.39,7.86">Keras</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,286.42,337.63,7.86;12,151.52,297.37,329.07,7.86;12,151.52,308.33,278.75,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,334.20,286.42,146.39,7.86;12,151.52,297.37,276.00,7.86">SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1407.0202" />
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Computing Research Repository</note>
</biblStruct>

<biblStruct coords="12,142.96,320.09,337.64,7.86;12,151.52,331.05,237.68,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="12,189.47,320.09,291.13,7.86;12,151.52,331.05,59.64,7.86">A comparative study of word embedding methods for early risk prediction on the Internet</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Uppsala University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="12,142.96,342.80,337.63,7.86;12,151.52,353.76,329.07,7.86;12,151.52,364.72,329.07,7.86;12,151.52,375.68,64.51,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,313.55,353.76,148.62,7.86">Unsls participation at erisk 2018 lab</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Funez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J G</forename><surname>Ucelay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">G</forename><surname>Burdisso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">C</forename><surname>Cagnina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Errecalde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,364.72,324.27,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,387.43,337.64,7.86;12,151.52,398.39,329.07,7.86;12,151.52,409.35,149.92,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,300.60,398.39,179.99,7.86;12,151.52,409.35,77.42,7.86">Allennlp: A deep semantic natural language processing platform</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,421.10,337.64,7.86;12,151.52,432.03,45.75,7.89" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,288.76,421.10,100.73,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,398.69,421.10,81.90,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,443.81,337.64,7.86;12,151.52,454.74,299.60,7.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,272.75,443.81,203.78,7.86">Cumulated gain-based evaluation of ir techniques</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,151.52,454.77,207.59,7.86">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,466.52,337.97,7.86;12,151.52,477.48,329.07,7.86;12,151.52,488.44,160.00,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,309.50,466.52,171.09,7.86;12,151.52,477.48,69.18,7.86">Random indexing of text samples for latent semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kristoferson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,242.91,477.48,237.69,7.86;12,151.52,488.44,97.48,7.86">Proceedings of the 22nd Annual Meeting of the Cognitive Science Society (CogSci)</title>
		<meeting>the 22nd Annual Meeting of the Cognitive Science Society (CogSci)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,500.20,337.98,7.86;12,151.52,511.15,272.96,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,236.58,500.20,224.94,7.86">Distributed representations of sentences and documents</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,511.15,180.01,7.86">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,522.91,337.97,7.86;12,151.52,533.87,329.07,7.86;12,151.52,544.83,248.07,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,259.23,522.91,221.36,7.86;12,151.52,533.87,11.56,7.86">A test collection for research on depression and language use</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,184.82,533.87,295.78,7.86;12,151.52,544.83,219.41,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction -7th International Conference of the CLEF Association</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,556.58,337.98,7.86;12,151.52,567.54,329.07,7.86;12,151.52,578.50,320.29,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,315.66,556.58,164.93,7.86;12,151.52,567.54,88.92,7.86">Overview of eRisk 2019: Early Risk Prediction on the Internet</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,260.50,567.54,220.09,7.86;12,151.52,578.50,291.61,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 10th International Conference of the CLEF Association</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,590.25,337.98,7.86;12,151.52,601.21,329.07,7.86;12,151.52,612.17,149.95,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,265.48,590.25,215.12,7.86;12,151.52,601.21,114.22,7.86">Using topic extraction on social media content for the early detection of depression</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Maupomé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Meurs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,287.46,601.21,193.14,7.86;12,151.52,612.17,121.28,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,623.92,337.98,7.86;12,151.52,634.88,329.07,7.86;12,151.52,645.84,329.07,7.86;12,151.52,656.80,117.27,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,151.52,634.88,329.07,7.86;12,151.52,645.84,73.58,7.86">Peimex at erisk2018: Emphasizing personal information for depression and anorexia detection</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Ortega-Mendoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>López-Monroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Franco-Arcega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Gómez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,248.11,645.84,232.49,7.86;12,151.52,656.80,88.59,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,119.67,337.98,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,329.07,7.86;13,151.52,152.52,278.77,7.89" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,394.26,141.59,86.33,7.86;13,151.52,152.55,73.64,7.86">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,232.82,152.55,155.11,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,163.51,337.97,7.86;13,151.52,174.47,329.07,7.86;13,151.52,185.43,156.66,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,318.45,163.51,162.14,7.86;13,151.52,174.47,22.82,7.86">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,196.54,174.47,284.06,7.86;13,151.52,185.43,127.98,7.86">Proceedings of the 2014 conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,196.39,337.98,7.86;13,151.52,207.34,329.07,7.86;13,151.52,218.30,329.07,7.86;13,151.52,229.26,329.07,7.86;13,151.52,240.22,58.46,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,195.75,207.34,167.04,7.86">Deep contextualized word representations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,384.19,207.34,96.40,7.86;13,151.52,218.30,329.07,7.86;13,151.52,229.26,174.91,7.86">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,251.18,337.97,7.86;13,151.52,262.14,329.07,7.86;13,151.52,273.10,283.07,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,415.36,251.18,65.23,7.86;13,151.52,262.14,310.75,7.86">Temporal mood variation: at the clef erisk-2018 tasks for early risk detection on the internet</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ragheb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Moulahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Azé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bringay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,151.52,273.10,185.97,7.86">CLEF: Conference and Labs of the Evaluation</title>
		<imprint>
			<biblScope unit="page">2125</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,284.06,337.98,7.86;13,151.52,295.02,150.64,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,392.08,284.06,49.81,7.86">Irit at e-risk</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ramiandrisoa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Benamara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,151.52,295.02,66.27,7.86">E-Risk workshop</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,305.98,337.97,7.86;13,151.52,316.93,329.07,7.86;13,151.52,327.89,195.08,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,286.97,305.98,193.62,7.86;13,151.52,316.93,47.87,7.86">Measuring the latency of depression detection in social media</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sadeque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,221.55,316.93,259.05,7.86;13,151.52,327.89,135.89,7.86">Proceedings of the 11th ACM International Conference on Web Search and Data Mining (WSDM)</title>
		<meeting>the 11th ACM International Conference on Web Search and Data Mining (WSDM)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,338.85,337.98,7.86;13,151.52,349.81,329.07,7.86;13,151.52,360.77,329.07,7.86;13,151.52,371.73,25.60,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="13,333.68,349.81,107.57,7.86">The Gavagai living lexicon</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Gyllensten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Espinoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Hamfors</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Persson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,463.04,349.81,17.56,7.86;13,151.52,360.77,294.56,7.86">Proceedings of the Language Resources and Evaluation Conference (LREC)</title>
		<meeting>the Language Resources and Evaluation Conference (LREC)</meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,382.69,337.97,7.86;13,151.52,393.65,329.07,7.86;13,151.52,404.61,95.49,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="13,303.22,382.69,177.37,7.86;13,151.52,393.65,42.78,7.86">Permutations as a means to encode order in word space</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kanerva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,215.23,393.65,265.36,7.86;13,151.52,404.61,66.81,7.86">Proceedings of The 30th Annual Meeting of the Cognitive Science Society (CogSci)</title>
		<meeting>The 30th Annual Meeting of the Cognitive Science Society (CogSci)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,415.56,337.98,7.86;13,151.52,426.52,329.07,7.86;13,151.52,437.48,329.07,7.86;13,151.52,448.44,25.60,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="13,313.02,415.56,167.58,7.86;13,151.52,426.52,273.39,7.86">Word embeddings and linguistic metadata at the clef 2018 tasks for early detection of depression and anorexia</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Trotzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,446.53,426.52,34.06,7.86;13,151.52,437.48,283.95,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
