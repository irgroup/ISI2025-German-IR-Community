<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,182.26,116.95,250.84,12.62;1,139.09,134.89,337.18,12.62;1,285.69,152.82,43.98,12.62">MLT-DFKI at CLEF eHealth 2019: Multi-label Classification of ICD-10 Codes with BERT</title>
				<funder ref="#_HQAp9Ew">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,185.50,190.49,67.41,8.74"><forename type="first">Saadullah</forename><surname>Amin</surname></persName>
							<email>saadullah.amin@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="institution">DFKI GmbH</orgName>
								<address>
									<addrLine>Campus D3 2</addrLine>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,261.60,190.49,73.13,8.74"><forename type="first">Günter</forename><surname>Neumann</surname></persName>
							<email>guenter.neumann@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="institution">DFKI GmbH</orgName>
								<address>
									<addrLine>Campus D3 2</addrLine>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.62,190.49,81.82,8.74"><forename type="first">Katherine</forename><surname>Dunfield</surname></persName>
							<email>katherine.dunfield@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="institution">DFKI GmbH</orgName>
								<address>
									<addrLine>Campus D3 2</addrLine>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,146.59,202.44,69.43,8.74"><forename type="first">Anna</forename><surname>Vechkaeva</surname></persName>
							<email>anna.vechkaeva@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="institution">DFKI GmbH</orgName>
								<address>
									<addrLine>Campus D3 2</addrLine>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.07,202.44,117.86,8.74"><forename type="first">Kathryn</forename><forename type="middle">Annette</forename><surname>Chapman</surname></persName>
							<email>kathryn_annette.chapman@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="institution">DFKI GmbH</orgName>
								<address>
									<addrLine>Campus D3 2</addrLine>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.22,202.44,95.23,8.74"><forename type="first">Morgan</forename><forename type="middle">Kelly</forename><surname>Wixted</surname></persName>
							<email>morgan.wixted@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="institution">DFKI GmbH</orgName>
								<address>
									<addrLine>Campus D3 2</addrLine>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,182.26,116.95,250.84,12.62;1,139.09,134.89,337.18,12.62;1,285.69,152.82,43.98,12.62">MLT-DFKI at CLEF eHealth 2019: Multi-label Classification of ICD-10 Codes with BERT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E2E5CC1CD396DDA55CDFD83E34BA7A2D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic Indexing</term>
					<term>Transfer Learning</term>
					<term>Multi-label Classification</term>
					<term>ICD-10 Codes</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the adoption of electronic health record (EHR) systems, hospitals and clinical institutes have access to large amounts of heterogeneous patient data. Such data consists of structured (insurance details, billing data, lab results etc.) and unstructured (doctor notes, admission and discharge details, medication steps etc.) documents, of which, latter is of great significance to apply natural language processing (NLP) techniques. In parallel, recent advancements in transfer learning for NLP has pushed the state-of-the-art to new limits on many language understanding tasks. Therefore, in this paper, we present team DFKI-MLT's participation at CLEF eHealth 2019 Task 1 of automatically assigning ICD-10 codes to non-technical summaries (NTSs) of animal experiments where we use various architectures in multi-label classification setting and demonstrate the effectiveness of transfer learning with pre-trained language representation model BERT (Bidirectional Encoder Representations from Transformers) and its recent variant BioBERT. We first translate task documents from German to English using automatic translation system and then use BioBERT which achieves an F1-micro of 73.02% on submitted run as evaluated by the challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>EHR systems offer rich source of data that can be utilized to improve health care systems by applying information extraction, representation learning and predictive modeling <ref type="bibr" coords="2,224.36,119.99,15.50,8.74" target="#b31">[32]</ref> techniques. Among many other applications, one such task is the automatic assignment of International Statistical Classification of Diseases (ICD) codes <ref type="bibr" coords="2,229.95,143.90,15.50,8.74" target="#b26">[27]</ref> to clinical notes, otherwise called semantic indexing of clinical documents <ref type="bibr" coords="2,217.28,155.86,9.96,8.74" target="#b8">[9]</ref>. The problem is to learn a mapping from natural language free-texts to medical concepts such that, given a new document, the system can assign one or more codes to it. Approximating the mapping in this setting can be seen as multi-label classification and is one way to solve the problem, besides hierarchical classification <ref type="bibr" coords="2,246.46,203.68,14.61,8.74" target="#b32">[33]</ref>, learning to rank and unsupervised methods.</p><p>In this study, we describe our work on CLEF eHealth 2019 <ref type="bibr" coords="2,411.11,215.63,15.50,8.74" target="#b18">[19]</ref> Task 1 <ref type="bibr" coords="2,462.32,215.63,14.61,8.74" target="#b25">[26]</ref>, which is about multilingual information extraction from German non-technical summaries (NTSs) of animal experiments collected from AnimalTestInfo database to classify according to ICD-10 codes, German modification version 2016 1 . The AnimalTestInfo database was developed in Germany to make the non-technical summaries (NTSs) of animal research studies available in a searchable and easily accessible web-based format. Each NTS was manually assigned an ICD-10 code with the goal of advancing the integrity and reporting of responsible animal research <ref type="bibr" coords="2,174.42,311.27,9.96,8.74" target="#b4">[5]</ref>. This task requires an automated approach to classify the NTSs, whereby the data exhibits challenging attributes of multilingualism, domain specificity and codes skewness with hierarchical structure.</p><p>We explore various models, starting with traditional bag-of-words support vector machines (SVM) to standard deep learning architectures of convolutional neural networks (CNN) and recurrent neural networks (RNN) with three types of attention mechanisms; namely, hierarchical attention Gated Recurrent Unit (GRU) <ref type="bibr" coords="2,169.72,394.96,9.96,8.74" target="#b7">[8]</ref>, self-attention Long-Short Term Memory (LSTM) <ref type="bibr" coords="2,412.97,394.96,14.61,8.74" target="#b13">[14]</ref>, and codes attentive LSTM. Finally, we show the effectiveness of fine-tuning state-of-the-art pre-trained BERT models <ref type="bibr" coords="2,250.70,418.87,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="2,267.49,418.87,11.62,8.74" target="#b21">22]</ref>, which requires minimal task specific changes and works well for small datasets. However, the significant performance boost comes from translating the German NTSs to English and then applying the same models, yielding an absolute gain of 6.22% f-score on dev set, from best German model to English model. This can be attributed to the fact that each language has its own linguistic and cultural characteristics that may contain different signals to effectively classify a specific class <ref type="bibr" coords="2,301.74,490.60,9.96,8.74" target="#b0">[1]</ref>. Given translated texts, we also find that domain specific embeddings have more effect when considering static word embeddings <ref type="bibr" coords="2,189.12,514.51,14.61,8.74" target="#b24">[25]</ref>, giving an avg. gain of 2.77% over contextual embeddings <ref type="bibr" coords="2,462.33,514.51,14.61,8.74" target="#b33">[34]</ref>, where the gain is 0.86%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Automatic assignment of ICD codes <ref type="bibr" coords="2,298.98,580.26,10.52,8.74" target="#b8">[9]</ref> to health related documents has been well studied, both in previous CLEF shared tasks and in general. Traditional approaches range from rule based and dictionary look ups <ref type="bibr" coords="2,390.73,604.17,10.52,8.74" target="#b5">[6]</ref> to machine learning models <ref type="bibr" coords="2,186.61,616.13,14.60,8.74" target="#b11">[12]</ref>. However, more recently the focus has been on applying deep learning.</p><p>Many techniques have been proposed using CNNs, RNNs and hybrid systems. <ref type="bibr" coords="3,162.90,131.95,15.50,8.74" target="#b10">[11]</ref> uses shallow CNN and improves its predictions for rare labels by dictionary-based lexical matching. <ref type="bibr" coords="3,284.31,143.90,10.52,8.74" target="#b3">[4]</ref> addresses the challenges of long documents and high cardinality of label space in MIMIC-III <ref type="bibr" coords="3,355.89,155.86,15.50,8.74" target="#b16">[17]</ref> by modifying Hierarchical Attention Network <ref type="bibr" coords="3,238.32,167.81,15.50,8.74" target="#b38">[39]</ref> with labels attention. More recent focus has been on using sequence-to-sequence (seq2seq) <ref type="bibr" coords="3,316.58,179.77,15.50,8.74" target="#b34">[35]</ref> based encoder-decoder based architectures. <ref type="bibr" coords="3,188.33,191.72,15.50,8.74" target="#b30">[31]</ref> first builds a multilingual death cause extraction model using LSTMs encoder-decoder, with concatenated French, Hungarian and Italian fast-Text emebddings as inputs and causes extracted from ICD-10 dictionaries as outputs. The output representations are then passed to an attention based biLSTM classifier which predicts the codes. <ref type="bibr" coords="3,290.04,239.54,15.50,8.74" target="#b14">[15]</ref> uses character level CNN <ref type="bibr" coords="3,423.88,239.54,15.50,8.74" target="#b40">[41]</ref> encoders for French and Italian, which are genealogically related languages and similar on a character level, with a biRNN decoder. <ref type="bibr" coords="3,327.09,263.45,15.50,8.74" target="#b15">[16]</ref> enriches word embeddings with language-specific Wikipedia and creates an ensemble model from a CNN classifier and GRU encoder-decoder. Few other techniques have also been proposed to use sequence-to-sequence framework and obtained good results <ref type="bibr" coords="3,424.32,299.32,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="3,436.13,299.32,11.62,8.74" target="#b23">24]</ref>.</p><p>While successful, these approaches make an auto-regressive assumption on output codes, which may hold true only when there is one distinct path from parent to child code for a given document. However, in the ICD codes assignment, a document can have multiple disjoint paths in a directed acyclic graph (DAG), formed by concepts hierarchy <ref type="bibr" coords="3,266.01,359.35,14.61,8.74" target="#b32">[33]</ref>. Also, for a smaller dataset, the decoder may suffer from low variance vocabulary and data sparsity issues. In <ref type="bibr" coords="3,430.32,371.31,9.96,8.74" target="#b6">[7]</ref>, a novel Hierarchical Multi-label Classification Network (HMCN) with feed-forward and recurrent variations is proposed that jointly optimizes local and global loss functions for discovering local hierarchical class-relationships in addition to global information from the entire class hierarchy while penalizing hierarchical violations (a child node getting a higher score than parent). However, they only consider tree based hierarchies where a node strictly has one parent.</p><p>Contextualized word embeddings, such as ELMo <ref type="bibr" coords="3,372.09,455.26,15.50,8.74" target="#b28">[29]</ref> and BERT <ref type="bibr" coords="3,444.57,455.26,14.61,8.74" target="#b9">[10]</ref>, derived from pre-trained bidirectional language models (biLMs) and trained on large texts have shown to substantially improve performance on many NLP tasks; question answering, entailment and sentiment classification, constituency parsing, named entity recognition, and text classification. Such transfer learning involves fine-tuning of these pre-trained models on a down-stream supervised task to get good results with minimal effort. In this sense, they are simple, efficient and performant. Motivated by this, and recent work of <ref type="bibr" coords="3,399.95,538.94,14.61,8.74" target="#b21">[22]</ref>, we use BERT models for this task and achieve better results than CNN and RNN based methods. We also show great improvements with translated English texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>The dataset contains 8,385 training documents (including dev set) and 407 test documents, all in German. Each document has six text fields: document title, uses (goals) of the experiment, possible harms caused to animals and comments about replacement, reduction and refinement (in the scope of 3R principles). The documents are assigned one or more codes from ICD-10-GM (German Modification version 2016) which exhibits a hierarchy forming a DAG <ref type="bibr" coords="4,434.33,222.56,14.61,8.74" target="#b32">[33]</ref>, where the highest-level nodes are called chapters and their direct child nodes are called groups. The depth of most chapters is one but in some cases it goes to secondlevel (e.g. M00-M25, T20-T32) and, in one case, up to third-level (C00-C97). Documents are assigned mixed codes such that a parent and child node can coexists and a child node can have multiple parents. Moreover, 91 documents are missing one or more of six text fields and only 6,472 have labels (5,820 in train set and 652 in dev set), while 52 of them have only chapter level codes. Table <ref type="table" coords="4,134.77,318.20,4.98,8.74" target="#tab_0">1</ref> shows top-5 most frequent codes. These classes account for more than 90% of the dataset leading to a high imbalance. Due to a shallow hierarchy, we consider the problem as multi-label classification instead of hierarchical classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>Since the documents are domain specific and in German, we argue that it might be difficult for open-domain and multilingual pre-trained models to do effective transfer learning. Furthermore, <ref type="bibr" coords="4,292.83,418.01,10.52,8.74" target="#b0">[1]</ref> suggests that each language has its own linguistic and cultural characteristics that may contain different signals to effectively classify a specific class. Based on this, and the fact that translations are always available as domain-free parallel corpora, we use them in our system and show improvements across all models. Since English has readily more accessible biomedical literature available as free texts, we use English translations for our documents. To perform a thorough case study, we tested several models and pre-trained embeddings. Below we describe each of them.</p><p>Baseline For baseline we use a TF-IDF weighted bag-of-words based linear SVM model.</p><p>CNN Convolutional Neural Network (CNN) learns local features of input representation through varying number and sizes of filters performing convolution operation. They have been very successful in many text classification tasks <ref type="bibr" coords="4,448.71,585.38,15.94,8.74" target="#b17">[18,</ref><ref type="bibr" coords="4,464.65,585.38,11.96,8.74" target="#b40">41]</ref>. While many advanced CNN architectures exist, we use a shallow model of <ref type="bibr" coords="4,461.59,597.34,14.61,8.74" target="#b19">[20]</ref>.</p><p>Attention Models Attention is a mechanism that was initially proposed in sequence-to-sequence based Neural Machine Translation (NMT) <ref type="bibr" coords="4,416.14,633.20,10.52,8.74" target="#b2">[3]</ref> to allow decoder to attend to encoder states while making predictions. More generally, attention generates a probability distribution over features, allowing models to put more weight on relevant features. In our study, we used three attention based models.</p><p>HAN Hierarchical Attention Network (HAN) deals with the problem of long documents classification by modeling attention at each hierarchical level of document i.e. words and sentences <ref type="bibr" coords="5,273.50,301.47,14.61,8.74" target="#b38">[39]</ref>. This allows the model to first attend word encoder outputs, in a sentence, followed by attending the sentence encoder outputs to classify a document. Like <ref type="bibr" coords="5,279.40,325.38,14.61,8.74" target="#b38">[39]</ref>, we also use bidirectional Gated Recurrent Units (GRUs) as word and sentence encoder.</p><p>SLSTM Self-Attention Long-Short Term Memory (SLSTM) network is a simple single layer network based on bidirectional LSTMs encoder. An input sequence is first passed through the encoder and encoded representations are self-attended to produce outputs.</p><p>CLSTM All ICD codes have a textual description, e.g. code A80-A89 is about viral infections of the central nervous system that can help a model while classifying. Fig. <ref type="figure" coords="5,189.17,450.89,4.98,8.74" target="#fig_0">1</ref> shows a document containing words related to those found in the descriptions of their labeled codes. Such words may or may not be present but the intuition is to use this additional meta-data to enrich the encoder representation by attention. To the best of our knowledge, this is the first time that the codes' descriptions are directly used to align with input text via attention. The closest work is from <ref type="bibr" coords="5,242.40,510.67,9.96,8.74" target="#b3">[4]</ref>, where author uses codes attention but they directly consider code as a unit of representation creating an embedding lookup. We also create an embedding layer for codes but using their texts where a code representation is obtained via average of word embeddings of each token. We call this network as Codes Attentive LSTM (CSLSTM) and describe it more formally.</p><p>Let X = {x 1 , x 2 , ..., x n } ∈ R n×d be an n-length input document sequence, where x i is a d-dimensional embedding vector for input word w i belonging to documents vocabulary V D . Let T = {t 1 , t 2 , ..., t m } ∈ R m×l be m-codes by llength titles representation matrix, where each t i = {t i1 , t i2 , ..., t i l } ∈ R l×d and t ij is d-dimensional embedding vector for code i's title word j, belonging to titles vocabulary V T . The embedding matrices are different for documents and codes titles, this is because the title words can be missing in documents vocab.</p><p>Similarly, we used different LSTM encoders for document and code words (shared encoder under performed on dev set; not reported). The network then transforms input as X out = CLSTM(X, T ), with following operations:</p><formula xml:id="formula_0" coords="6,211.68,173.23,191.50,160.49">X enc = [x 1enc , x 2enc , ..., x nenc ] x ienc = LSTM W (x i ) T enc = [t 1enc , t 2enc , ..., t menc ] t ienc = 1 l l j=1 LSTM C (t ij ) X out = [X enc ; T enc ] ∈ R (n+m)×h A = softmax(X out X T out ) ∈ R (n+m)×(n+m) X out = X out + A T X out X out = 1 n n j=1 X outj</formula><p>where, X enc is a sequence of word encoder LSTM W outputs and T enc is a sequence of averaged title words encoding by code encoder LSTM C . We concatenate document words sequence with titles sequence and perform self-attention A, followed by residual connection and average over resulting sequence to get final representation.</p><p>BERT Pre-training large models on unsupervised corpus with language modeling objective and then fine-tuning the same model for a downstream supervised task eliminates the need of heavily engineered task-specific architectures <ref type="bibr" coords="6,134.77,448.84,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="6,151.67,448.84,12.73,8.74" target="#b28">29,</ref><ref type="bibr" coords="6,165.80,448.84,11.62,8.74" target="#b29">30]</ref>. Bidirectional Encoder Representations from Transformers (BERT) is a recently proposed such model, following ELMo and OpenAI GPT. BERT is a multi-layer bidirectional Transformer (feed-forward multi-headed self-attention) <ref type="bibr" coords="6,134.77,484.71,15.50,8.74" target="#b36">[37]</ref> encoder that is trained with two objectives, masked language modeling (predicting a missing word in a sentence from the context) and next sentence prediction (predicting whether two sentences are consecutive sentences). BERT has improved the state-of-the-art in many language understanding tasks and recent works show that it sequentially model NLP pipeline, POS tagging, parsing, NER, sematic roles and coreference <ref type="bibr" coords="6,263.32,544.48,14.61,8.74" target="#b35">[36]</ref>. Similar works <ref type="bibr" coords="6,346.49,544.48,16.69,8.74" target="#b12">[13,</ref><ref type="bibr" coords="6,363.18,544.48,12.52,8.74" target="#b39">40]</ref> have been performed to understand and interpret BERT's learning capacity. We therefore use BERT in our task and show that it achieves best results compared to other models and is nearly agnostic to domain specific pre-training (BioBERT; <ref type="bibr" coords="6,393.16,580.35,14.76,8.74" target="#b21">[22]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pre-processing</head><p>We consider each document as one text field i.e. all six fields are joined together to form one input text. As mentioned in section 3, only 6,472 documents are labeled, out of which 654 are in dev set form total of 840. Since there is no gold standard for these documents we cannot evaluate them, so we ignored them during training. We also abstained from adding an extra "no" class (i.e. proxy for predicting nothing) for such documents because we assume that all NTSs should be indexed (e.g. like MEDLINE auto-indexing of new PubMed articles) and therefore inherently has one or more true ICD-10 codes assigned to them. However, the official evaluation script penalizes model predictions for such documents by considering them all false positives. We will cover this in detail in results section.</p><p>To translate German documents to English we used automatic translation from Google Translate API v2 2 . For both, German and English, we use language specific sentence and word tokenizer offered by NLTK <ref type="bibr" coords="7,374.26,264.76,15.50,8.74" target="#b22">[23]</ref> and spaCy 3 , respectively. Tokens with document frequencies outside 5 and 60% of training corpus were removed and only top-10000 tokens were kept to limit the vocabulary. This applies to all models other than BERT, which uses WordPiece tokenizer <ref type="bibr" coords="7,446.31,300.63,15.50,8.74" target="#b37">[38]</ref> and builds its own vocabulary. Lastly, we remove all the classes with frequency less than 15. All the experiments were performed without any cross-validation on dev set to find best parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pre-trained Embeddings</head><p>We use following pre-trained models for German:</p><p>• FT de : fastText DE Common Crawl (300d) 4</p><p>• BERT de : BERT-Base, Multilingual Cased (768d) 5   and following for English:</p><p>• FT en : fastText EN Common Crawl (300d) • PubMed en : PubMed word2vec (400d) 6</p><p>• BERT en : BERT-Base, Cased (768d) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Models</head><p>TF-IDF + Linear SVM For baseline, we use scikit-learn implementation of LinearSVC with one-vs-all training <ref type="bibr" coords="8,290.82,151.29,14.61,8.74" target="#b27">[28]</ref>.</p><p>For all the models, except BERT, we used a batch size of 64, max sequence length of 256, learning rate of 0.001 with Adam <ref type="bibr" coords="8,350.27,187.15,15.50,8.74" target="#b20">[21]</ref> and 50 epochs with early stopping. We used binary cross-entropy for each class as our objective function and F 1 -micro score as performance metrics. All the experiments were performed on single 12 GB Nvidia TitanXp GPU. We implemented these models and our code is publicly available.<ref type="foot" coords="8,246.07,233.40,3.97,6.12" target="#foot_0">9</ref> </p><p>CNN We configured CNN with 64 channels and filter sizes of 3, 4 and 5.</p><p>HAN Following <ref type="bibr" coords="8,208.75,282.80,14.61,8.74" target="#b38">[39]</ref>, we also used biGRU encoders but with hidden size of 300. We set the maximum number of sentences in a documents and maximum number of words in a sentence as 40 and 10 respectively.</p><p>SLSTM A biLSTM encoder with hidden size of 300.</p><p>CLSTM Similar to SLSTM, but with additional T matrix of size total number of titles (230, collected from ICD-10-GM) × max title sequence length of 10.</p><p>BERT We used PyTorch's implementation of BERT<ref type="foot" coords="8,363.17,388.82,7.94,6.12" target="#foot_1">10</ref> with default parameters.</p><p>To avoid memory issues, we used maximum sequence length of 256 with batch size 6.</p><p>Ensemble Based on dev set results, we also created an ensemble of top-2 models as weighted combination of their raw scores, where then the prediction for each example is given by:</p><formula xml:id="formula_1" coords="8,200.19,493.07,215.16,12.01">ŷ = 1{σ(κ × S 1 + (1 -κ) × S 2 ) &gt; 0.5} ∈ {0, 1} |C|</formula><p>S 1 , S 2 are raw probability scores from first and second best model respectively, while σ is sigmoid function and |C| is number of classes. We select best value of κ on dev set such that the f1 score of ensemble is higher than individual models. Fig. <ref type="figure" coords="8,155.10,567.27,4.98,8.74">2</ref> shows κ variation with performance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>Table <ref type="table" coords="8,162.20,615.92,4.98,8.74" target="#tab_2">2</ref> summarizes the results on the dev set for all models with different pretrained embeddings. In all of our experiments, working with translated texts Fig. <ref type="figure" coords="9,162.82,297.84,7.05,6.14">2:</ref> The graph shows the effect of varying κ to create an ensemble of top-2 models which achieves a score of 84%, higher than its component models, on dev set at κ=0.63. This value was later used for test predictions.</p><p>(English) improved the score by an avg. of 4.07%. This can be attributed to the fact that there is much more English texts than other languages, but it can also be argued that English may have stronger linguistic signals to classify the classes where German models make mistakes <ref type="bibr" coords="9,301.93,364.96,9.96,8.74" target="#b0">[1]</ref>.</p><p>The baseline proved to be a strong one, with the highest precision of all and outperforming HAN and CNN models, for both German and English, with common crawl embeddings. HAN performs better when documents are relatively long e.g. <ref type="bibr" coords="9,175.40,414.53,10.52,8.74" target="#b3">[4]</ref> reports strong results with HAN based models on MIMIC dataset <ref type="bibr" coords="9,134.77,426.48,14.61,8.74" target="#b16">[17]</ref>, where the average document size exceeds 1900 tokens. After pre-processing, the averaged document length in our case was approximately 340. For CNN, we believe advanced variants may perform better.</p><p>SLSTM and CLSTM, both being just one layer, performed comparably and better than baseline. SLSTM is much simpler and relies purely on self-attention, which also compliments higher scores by BERT models, which are stacked multiheaded self-attention networks. For CLSTM, since many documents are missing the title words (in fact many title words never appeared in corpus), the model had weak alignment signals between documents and this additional meta-data. However, it still performed really well, getting second best score with PubMed embeddings.</p><p>BERT performed better than other models, both in German and English with an avg. score of 6% points higher. BioBERT en performed slightly (+0.86%) better than BERT en , this was also noticeable in Relation Extraction task in <ref type="bibr" coords="9,462.32,585.38,14.61,8.74" target="#b21">[22]</ref>, where domain specific and general BERT performed comparably. This partly shows BERT's ability to generalize and being robust to domain shifts (learning from only 5k training docs), however, this contradicts the recent findings of <ref type="bibr" coords="9,134.77,633.20,14.61,8.74" target="#b39">[40]</ref>, where authors reflect on such issues, and catastrophic forgetting in BERTlike models. On other hand, the effect of using in-domain pre-trained models was more significant for static-embeddings; using pre-trained PubMeden vectors out-performed open-domain FT en by an avg. of 2.77%. Such analysis was not performed for German due to lack of medical domain German vectors. BERT models had highest recall but relatively poor precision. This is preferable in real-world medical applications, where the recall is of much more importance. We also combined our top-2 models, BioBERT en and CLSTSM-PubMed en , to get an ensemble which performed better than both and got highest score of 84.67% on dev set. The intuition was to improve on BERT's precision without losing too much of recall. At κ = 0.63 we got the highest score. This increased BioBERT en precision by 7.24% at loss of 2.5% recall. Since our focus was mainly on single model systems therefore we used best single model for submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Submission and test scores</head><p>The test set contains 407 documents, which we first translate to English and then run predictions with BioBERT en as our submitted model. We obtained a test f1micro of 73% with 86% recall and 64% precision as posted by official results. Our system ranked second but the there was significant difference between test and dev set performances, especially, low precision. After the gold set was released, we probed it and realized that the official script provided by the challenge considers all predictions on test examples for which there is no gold label (93 of them) as false positives. We think that it is intrinsically impossible to compare examples with predictions where gold standard is not available. To emphasize, we give an example, if we take test document with id=20486 where the gold labels are {C00-C97, C76-C80, II} and our best model predicted {C00-C97, C76-C80, II} i.e. a perfect match with maximum score. Given official evaluation, if this example did not had gold standard available then our model predictions would all had been considered as false positives, which severely degrades precision of a model which may have generalized well to predict on future examples. Table <ref type="table" coords="11,425.24,502.85,4.98,8.74" target="#tab_3">3</ref> shows this comparison on test set, where in "Original" column we use the same evaluation as provided by the task and in "Modified" we remove all documents from evaluation for which gold labels are not available. As can be seen, recall column is just the same as original with only precision column changes which changes f1-score as well. With the modification, all the models have similar performance as it was on dev set, as we also evaluated trained and evaluated on dev set by removing unlabeled examples. With modification, the submitted system achieves a test score of 80.82% now compared to that of 82.90% on dev set. Finally, ensemble model gets highest scores of 77.98% and 82.49% with original and modified evaluation respectively.</p><p>Biomedical text mining is generally a challenging field but recent progresses of transfer learning in NLP can significantly reduce the engineering required to come up with domain sensitive models. Unsupervised data is cheap, and can be obtained in abundance to learn general language patterns <ref type="bibr" coords="12,399.63,183.06,14.61,8.74" target="#b39">[40]</ref>, however, such data may not be readily available when dealing with in-domain and low-resource languages (e.g. Estonian medical documents). Such deficiencies encourage research for better cross-lingual and cross-domain embedding alignment methods that can transfered effectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,148.49,215.43,318.38,6.14;5,171.03,223.42,273.31,6.12;5,152.06,116.83,311.24,87.84"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: An example document tagged with codes E10-E14 (diabetes mellitus) and E65-E68 (obesity and other overeating) containing related words to codes descriptions.</figDesc><graphic coords="5,152.06,116.83,311.24,87.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,152.06,116.83,311.23,175.23"><head></head><label></label><figDesc></figDesc><graphic coords="9,152.06,116.83,311.23,175.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,237.47,118.03,134.97,73.43"><head>Table 1 :</head><label>1</label><figDesc>Top-5 most frequent codes</figDesc><table coords="4,237.47,118.03,134.97,54.95"><row><cell>ICD-10 Code</cell><cell>No. of documents (train + dev)</cell></row><row><cell>II</cell><cell>1515</cell></row><row><cell>C00-C97</cell><cell>1479</cell></row><row><cell>IX</cell><cell>930</cell></row><row><cell>VI</cell><cell>799</cell></row><row><cell>C00-C75</cell><cell>732</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,137.50,521.76,342.44,144.15"><head>7 •</head><label>7</label><figDesc>BioBERT en : BioBERT (768d)8   </figDesc><table coords="7,137.50,557.40,342.44,108.51"><row><cell>2 https://cloud.google.com/translate/docs/translating-text</cell></row><row><cell>3 https://spacy.io/usage/models</cell></row><row><cell>4 https://fasttext.cc/docs/en/crawl-vectors.html</cell></row><row><cell>5 https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_</cell></row><row><cell>H-768_A-12.zip</cell></row><row><cell>6 https://archive.org/details/pubmed2018_w2v_400D.tar</cell></row><row><cell>7 https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_</cell></row><row><cell>A-12.zip</cell></row><row><cell>8 https://github.com/naver/biobert-pretrained/releases/tag/v1.</cell></row><row><cell>0-pubmed-pmc</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,134.77,120.82,345.83,271.24"><head>Table 2 :</head><label>2</label><figDesc>Results on development set (where blue and red are best and worst score for each column and overall best is boldfaced)</figDesc><table coords="10,203.60,120.82,204.28,243.74"><row><cell></cell><cell>Models</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell></cell><cell>TF-IDF de</cell><cell>90.72</cell><cell>58.73</cell><cell>71.30</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>TF-IDFen</cell><cell>90.69</cell><cell>65.45</cell><cell>76.03</cell></row><row><cell></cell><cell>FT de</cell><cell>86.08</cell><cell>57.37</cell><cell>68.85</cell></row><row><cell>CNN</cell><cell>FTen</cell><cell>85.76</cell><cell>61.59</cell><cell>71.69</cell></row><row><cell></cell><cell>PubMeden</cell><cell>87.95</cell><cell>65.10</cell><cell>74.82</cell></row><row><cell></cell><cell>FT de</cell><cell>78.86</cell><cell>58.79</cell><cell>67.37</cell></row><row><cell>HAN</cell><cell>FTen</cell><cell>83.52</cell><cell>64.50</cell><cell>72.79</cell></row><row><cell></cell><cell>PubMeden</cell><cell>85.10</cell><cell>69.61</cell><cell>76.58</cell></row><row><cell></cell><cell>FT de</cell><cell>85.55</cell><cell>64.86</cell><cell>73.76</cell></row><row><cell>SLSTM</cell><cell>FTen</cell><cell>87.53</cell><cell>67.65</cell><cell>76.32</cell></row><row><cell></cell><cell>PubMeden</cell><cell>87.33</cell><cell>70.09</cell><cell>77.77</cell></row><row><cell></cell><cell>FT de</cell><cell>83.60</cell><cell>63.97</cell><cell>72.48</cell></row><row><cell>CLSTM</cell><cell>FTen</cell><cell>84.39</cell><cell>69.14</cell><cell>76.01</cell></row><row><cell></cell><cell>PubMeden  †</cell><cell>87.87</cell><cell>70.21</cell><cell>78.05</cell></row><row><cell></cell><cell>Multi de</cell><cell>70.96</cell><cell>83.41</cell><cell>76.68</cell></row><row><cell>BERT</cell><cell>BERTen</cell><cell>79.63</cell><cell>84.60</cell><cell>82.04</cell></row><row><cell></cell><cell>BioBERTen  ‡</cell><cell>80.35</cell><cell>85.61</cell><cell>82.90</cell></row><row><cell cols="2">Ensemble ( †,  ‡)</cell><cell>86.29</cell><cell>83.11</cell><cell>84.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,134.77,120.82,345.83,291.17"><head>Table 3 :</head><label>3</label><figDesc>Results on test set (where blue and red are best and worst score for each column and overall best is boldfaced). Original column refers to official evaluation setup and Modified refers to the case where we ignore test documents without gold labels for evaluation.</figDesc><table coords="11,142.55,120.82,326.38,255.70"><row><cell></cell><cell>Models</cell><cell></cell><cell>Original</cell><cell></cell><cell></cell><cell>Modified</cell><cell></cell></row><row><cell></cell><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell></cell><cell>TF-IDF de</cell><cell>89.58</cell><cell>52.74</cell><cell>66.39</cell><cell>93.01</cell><cell>52.74</cell><cell>67.31</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>TF-IDFen</cell><cell>88.31</cell><cell>60.79</cell><cell>72.01</cell><cell>91.53</cell><cell>60.79</cell><cell>73.06</cell></row><row><cell></cell><cell>FT de</cell><cell>80.30</cell><cell>54.66</cell><cell>65.04</cell><cell>86.99</cell><cell>54.66</cell><cell>67.13</cell></row><row><cell>CNN</cell><cell>FTen</cell><cell>78.09</cell><cell>58.74</cell><cell>67.05</cell><cell>83.33</cell><cell>58.74</cell><cell>68.91</cell></row><row><cell></cell><cell>PubMeden</cell><cell>80.89</cell><cell>64.36</cell><cell>71.69</cell><cell>86.74</cell><cell>64.36</cell><cell>73.90</cell></row><row><cell></cell><cell>FT de</cell><cell>71.45</cell><cell>54.66</cell><cell>61.93</cell><cell>80.60</cell><cell>54.66</cell><cell>65.14</cell></row><row><cell>HAN</cell><cell>FTen</cell><cell>75.88</cell><cell>62.70</cell><cell>68.67</cell><cell>82.10</cell><cell>62.70</cell><cell>71.10</cell></row><row><cell></cell><cell>PubMeden</cell><cell>79.51</cell><cell>66.41</cell><cell>72.37</cell><cell>84.82</cell><cell>66.41</cell><cell>74.49</cell></row><row><cell></cell><cell>FT de</cell><cell>79.17</cell><cell>64.11</cell><cell>70.85</cell><cell>85.37</cell><cell>64.11</cell><cell>73.23</cell></row><row><cell>SLSTM</cell><cell>FTen</cell><cell>82.53</cell><cell>65.77</cell><cell>73.20</cell><cell>86.26</cell><cell>65.77</cell><cell>74.63</cell></row><row><cell></cell><cell>PubMeden</cell><cell>77.13</cell><cell>68.07</cell><cell>72.32</cell><cell>83.15</cell><cell>68.07</cell><cell>74.85</cell></row><row><cell></cell><cell>FT de</cell><cell>83.60</cell><cell>63.97</cell><cell>72.48</cell><cell>87.52</cell><cell>63.97</cell><cell>73.91</cell></row><row><cell>CLSTM</cell><cell>FTen</cell><cell>75.74</cell><cell>65.00</cell><cell>69.96</cell><cell>82.62</cell><cell>65.00</cell><cell>72.76</cell></row><row><cell></cell><cell>PubMeden  †</cell><cell>82.15</cell><cell>68.19</cell><cell>74.52</cell><cell>86.82</cell><cell>68.19</cell><cell>76.39</cell></row><row><cell></cell><cell>Multi de</cell><cell>54.10</cell><cell>83.39</cell><cell>65.62</cell><cell>68.23</cell><cell>83.39</cell><cell>75.05</cell></row><row><cell>BERT</cell><cell>BERTen</cell><cell>62.09</cell><cell>83.26</cell><cell>71.11</cell><cell>75.20</cell><cell>83.26</cell><cell>79.03</cell></row><row><cell></cell><cell>BioBERTen  ‡</cell><cell>63.68</cell><cell>85.56</cell><cell>73.02</cell><cell>76.57</cell><cell>85.56</cell><cell>80.82</cell></row><row><cell cols="2">Ensemble ( †,  ‡)</cell><cell>74.44</cell><cell>81.86</cell><cell>77.98</cell><cell>83.13</cell><cell>81.86</cell><cell>82.49</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_0" coords="8,144.73,647.48,293.34,7.47"><p>https://github.com/suamin/multilabel-classification-bert-icd10</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_1" coords="8,144.73,658.44,255.19,7.47"><p>https://github.com/huggingface/pytorch-pretrained-BERT</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Stalin Varanasi</rs> for helpful discussions. This work was partially funded by <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under grant agreement No. <rs type="grantNumber">777107</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HQAp9Ew">
					<idno type="grant-number">777107</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,379.22,337.64,7.86;12,151.52,390.17,329.07,7.86;12,151.52,401.13,97.09,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="12,151.52,390.17,256.07,7.86">Translations as additional contexts for sentence classification</title>
		<author>
			<persName coords=""><forename type="first">Reinald</forename><surname>Kim Amplayo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinyeong</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seung-Won</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05516</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.96,412.75,337.64,7.86;12,151.52,423.71,329.07,7.86;12,151.52,434.67,240.37,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="12,310.29,423.71,170.30,7.86;12,151.52,434.67,180.61,7.86">Ixamed at clef ehealth 2018 task 1: Icd10 coding with a sequence-to-sequence approach</title>
		<author>
			<persName coords=""><surname>Atutxa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Casillas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ezeiza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fresno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gojenola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Oronoz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Perez-De Vinaspre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,446.28,337.64,7.86;12,151.52,457.24,329.07,7.86;12,151.52,468.20,20.99,7.86" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m" coord="12,390.83,446.28,89.76,7.86;12,151.52,457.24,190.68,7.86">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.96,479.82,337.63,7.86;12,151.52,490.78,329.07,7.86;12,151.52,501.74,329.07,7.86;12,151.52,512.70,72.28,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,223.50,490.78,257.09,7.86;12,151.52,501.74,42.68,7.86">Multi-label classification of patient notes: case study on icd code assignment</title>
		<author>
			<persName coords=""><forename type="first">Tal</forename><surname>Baumel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jumana</forename><surname>Nassour-Kassis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noémie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,217.90,501.74,262.69,7.86;12,151.52,512.70,44.51,7.86">Workshops at the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,524.31,337.63,7.86;12,151.52,535.27,329.07,7.86;12,151.52,546.23,329.07,7.86;12,151.52,557.19,275.09,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,151.52,546.23,329.07,7.86;12,151.52,557.19,118.55,7.86">Rethinking 3r strategies: Digging deeper into animaltestinfo promotes transparency in in vivo biomedical research</title>
		<author>
			<persName coords=""><forename type="first">Bettina</forename><surname>Bert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antje</forename><surname>Dörendahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nora</forename><surname>Leich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julia</forename><surname>Vietze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Steinfath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justyna</forename><surname>Chmielewska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Hensel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Grune</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gilbert</forename><surname>Schönfelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,278.07,557.19,50.63,7.86">PLoS biology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">2003217</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,568.81,337.64,7.86;12,151.52,579.77,329.07,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="12,345.88,568.81,134.71,7.86;12,151.52,579.77,270.16,7.86">Tlemcen university at celf ehealth 2018 team techno: Multilingual information extraction-icd10 coding</title>
		<author>
			<persName coords=""><forename type="first">Rabia</forename><surname>Bounaama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>El</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amine</forename><surname>Abderrahim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,591.38,337.63,7.86;12,151.52,602.34,329.07,7.86;12,151.52,613.30,143.77,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,432.42,591.38,48.17,7.86;12,151.52,602.34,213.16,7.86">Hierarchical multi-label classification using local neural networks</title>
		<author>
			<persName coords=""><forename type="first">Ricardo</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">André Cplf De</forename><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,375.79,602.34,104.79,7.86;12,151.52,613.30,63.49,7.86">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="56" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,624.92,337.64,7.86;12,151.52,635.88,329.07,7.86;12,151.52,646.84,329.07,7.86;12,151.52,657.79,92.39,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,368.97,635.88,111.62,7.86;12,151.52,646.84,264.16,7.86">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.96,120.67,337.64,7.86;13,151.52,131.63,329.07,7.86;13,151.52,142.59,329.07,7.86;13,151.52,153.55,259.71,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,215.78,131.63,173.98,7.86">Automatic code assignment to medical text</title>
		<author>
			<persName coords=""><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Partha</forename><surname>Pratim Talukdar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,408.81,131.63,71.78,7.86;13,151.52,142.59,325.19,7.86">Proceedings of the workshop on bionlp 2007: Biological, translational, and clinical language processing</title>
		<meeting>the workshop on bionlp 2007: Biological, translational, and clinical language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,165.30,337.97,7.86;13,151.52,176.26,329.07,7.86;13,151.52,187.22,132.38,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="13,439.96,165.30,40.63,7.86;13,151.52,176.26,293.36,7.86">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.62,198.97,337.97,7.86;13,151.52,209.93,148.43,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="13,229.62,198.97,250.97,7.86;13,151.52,209.93,89.51,7.86">Ecstra-aphp@ clef ehealth2018-task 1: Icd10 code extraction from death certificates</title>
		<author>
			<persName coords=""><forename type="first">Rémi</forename><surname>Flicoteaux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,221.68,337.98,7.86;13,151.52,232.64,51.32,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="13,286.93,221.68,189.74,7.86">Instance-based learning for icd10 categorization</title>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Gobeill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Ruch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,244.39,77.28,7.86;13,240.61,244.39,156.38,7.86;13,417.70,244.39,62.90,7.86;13,151.52,255.35,97.09,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="13,240.61,244.39,152.99,7.86">Assessing bert&apos;s syntactic abilities</title>
		<author>
			<persName coords=""><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05287</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.62,267.10,337.98,7.86;13,151.52,278.06,125.83,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,325.43,267.10,97.75,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,431.76,267.10,48.84,7.86;13,151.52,278.06,31.76,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,289.81,337.98,7.86;13,151.52,300.77,329.07,7.86;13,151.52,311.73,329.07,7.86;13,151.52,322.69,329.07,7.86;13,151.52,333.65,329.07,7.86;13,151.52,344.60,70.90,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,169.00,300.77,311.60,7.86;13,151.52,311.73,266.89,7.86">Kcl-health-nlp@ clef ehealth 2018 task 1: Icd-10 coding of french and italian death certificates with character-level convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Julia</forename><surname>Ive</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Natalia</forename><surname>Viani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">André</forename><surname>Bittar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sumithra</forename><surname>Velupillai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,435.71,311.73,44.88,7.86;13,151.52,322.69,324.74,7.86">19th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2018</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2018-09-10">10 September 2018 through 14 September 2018. 2018</date>
			<biblScope unit="page">2125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,356.36,337.98,7.86;13,151.52,367.31,329.07,7.86;13,151.52,378.27,329.07,7.86;13,151.52,389.23,122.97,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="13,389.02,367.31,91.57,7.86;13,151.52,378.27,329.07,7.86;13,151.52,389.23,118.73,7.86">Toronto cl at clef 2018 ehealth task 1: Multi-lingual icd-10 coding using an ensemble of recurrent and convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Serena</forename><surname>Jeblee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akshay</forename><surname>Budhkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saša</forename><surname>Milic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chloé</forename><surname>Pou-Prom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krishnapriya</forename><surname>Vishnubhotla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Rudzicz</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,400.98,337.98,7.86;13,151.52,411.94,329.07,7.86;13,151.52,422.90,329.07,7.86;13,151.52,433.86,61.43,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,215.43,422.90,197.84,7.86">Mimic-iii, a freely accessible critical care database</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mengling</forename><surname>Lehman Li-Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leo</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,420.97,422.90,55.67,7.86">Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">160035</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,445.61,337.98,7.86;13,151.52,456.57,329.07,7.86;13,151.52,467.53,314.70,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,282.41,445.61,198.19,7.86;13,151.52,456.57,74.20,7.86">Deep pyramid convolutional neural networks for text categorization</title>
		<author>
			<persName coords=""><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,245.35,456.57,235.25,7.86;13,151.52,467.53,120.45,7.86">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="13,142.62,479.28,337.98,7.86;13,151.52,490.24,329.07,7.86;13,151.52,501.20,329.07,7.86;13,151.52,512.16,329.07,7.86;13,151.52,523.12,329.07,7.86;13,151.52,534.08,329.07,7.86;13,151.52,545.03,326.10,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,227.74,501.20,188.53,7.86">Overview of the CLEF eHealth evaluation lab</title>
		<author>
			<persName coords=""><forename type="first">Liadh</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lorraine</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rene</forename><surname>Spijker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harrisen</forename><surname>Scells</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">João</forename><surname>Palotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,466.81,512.16,13.78,7.86;13,151.52,523.12,329.07,7.86;13,151.52,534.08,299.12,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Tenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="13,151.52,545.03,140.53,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,556.79,337.98,7.86;13,151.52,567.75,127.67,7.86" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="13,207.52,556.79,236.84,7.86">Convolutional neural networks for sentence classification</title>
		<author>
			<persName coords=""><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.62,579.50,337.97,7.86;13,151.52,590.46,153.46,7.86" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="13,297.84,579.50,178.65,7.86">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.62,602.21,337.98,7.86;13,151.52,613.17,329.07,7.86;13,151.52,624.12,329.07,7.86;13,151.52,635.08,20.99,7.86" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="13,292.53,613.17,188.06,7.86;13,151.52,624.12,183.40,7.86">Biobert: pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08746</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.62,646.84,337.98,7.86;13,151.52,657.79,72.26,7.86" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="13,282.06,646.84,134.04,7.86">Nltk: the natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<idno>arXiv preprint cs/0205028</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,120.67,337.97,7.86;14,151.52,131.63,329.07,7.86;14,151.52,142.59,94.29,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,334.65,120.67,145.94,7.86;14,151.52,131.63,282.47,7.86">Kfu at clef ehealth 2017 task 1: Icd-10 coding of english death certificates with recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">Zulfat</forename><surname>Miftahutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elena</forename><surname>Tutubalina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,455.98,131.63,24.61,7.86;14,151.52,142.59,66.10,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,153.56,337.98,7.86;14,151.52,164.52,317.26,7.86" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="14,402.05,153.56,78.54,7.86;14,151.52,164.52,155.71,7.86">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.62,175.48,337.98,7.86;14,151.52,186.44,329.07,7.86;14,151.52,197.40,329.07,7.86;14,151.52,208.36,329.07,7.86;14,151.52,219.32,329.07,7.86;14,151.52,230.28,329.07,7.86;14,151.52,241.24,150.94,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="14,328.06,186.44,152.53,7.86;14,151.52,197.40,143.29,7.86">Overview of the CLEF eHealth 2019 Multilingual Information Extraction</title>
		<author>
			<persName coords=""><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Butzke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antje</forename><surname>Dörendahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nora</forename><surname>Leich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benedikt</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gilbert</forename><surname>Schönfelder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Grune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,319.25,208.36,161.34,7.86;14,151.52,219.32,329.07,7.86;14,151.52,230.28,129.35,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Tenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="14,311.63,230.28,138.07,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,252.20,337.98,7.86;14,151.52,263.16,276.15,7.86" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="14,268.52,252.20,212.07,7.86;14,151.52,263.16,89.43,7.86">International statistical classification of diseases and related health problems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>World Health Organization</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,274.13,337.97,7.86;14,151.52,285.09,329.07,7.86;14,151.52,296.05,329.07,7.86;14,151.52,307.01,211.84,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="14,259.44,296.05,166.09,7.86">Scikit-learn: Machine learning in python</title>
		<author>
			<persName coords=""><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,437.96,296.05,42.63,7.86;14,151.52,307.01,103.02,7.86">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,317.98,337.97,7.86;14,151.52,328.94,329.07,7.86;14,151.52,339.89,192.50,7.86" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m" coord="14,334.16,328.94,146.43,7.86;14,151.52,339.89,26.48,7.86">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.62,350.86,337.97,7.86;14,151.52,361.82,329.07,7.86;14,151.52,372.78,89.13,7.86" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="14,439.36,350.86,41.23,7.86;14,151.52,361.82,208.71,7.86">Improving language understanding with unsupervised learning</title>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Time</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>OpenAI</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="14,142.62,381.48,337.97,10.13;14,151.52,394.71,329.07,7.86;14,151.52,405.67,141.03,7.86" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="14,340.62,383.75,139.97,7.86;14,151.52,394.71,329.07,7.86;14,151.52,405.67,81.37,7.86">Wbi at clef ehealth 2018 task 1: Language-independent icd-10 coding using multi-lingual embeddings and recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">Jurica</forename><surname>Ševa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mario</forename><surname>Sänger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulf</forename><surname>Leser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,416.63,337.97,7.86;14,151.52,427.59,329.07,7.86;14,151.52,438.55,329.07,7.86;14,151.52,449.51,91.64,7.86" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="14,460.23,416.63,20.36,7.86;14,151.52,427.59,329.07,7.86;14,151.52,438.55,111.33,7.86">Deep ehr: a survey of recent advances in deep learning techniques for electronic health record (ehr) analysis</title>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Shickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>James Tighe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Azra</forename><surname>Bihorac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Parisa</forename><surname>Rashidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,272.26,438.55,204.33,7.86">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1589" to="1604" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,460.48,337.98,7.86;14,151.52,471.44,329.07,7.86;14,151.52,482.40,35.84,7.86" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="14,300.22,460.48,180.37,7.86;14,151.52,471.44,114.70,7.86">A survey of hierarchical classification across different application domains</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Silla</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,274.22,471.44,154.45,7.86">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="72" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,493.36,337.98,7.86;14,151.52,504.32,132.38,7.86" xml:id="b33">
	<monogr>
		<title level="m" type="main" coord="14,214.17,493.36,236.90,7.86">Contextual word representations: A contextual introduction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06006</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.62,515.29,337.97,7.86;14,151.52,526.25,329.07,7.86;14,151.52,537.21,68.09,7.86" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="14,339.22,515.29,141.37,7.86;14,151.52,526.25,63.57,7.86">Sequence to sequence learning with neural networks</title>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,240.61,526.25,209.27,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,548.18,337.97,7.86;14,151.52,559.14,196.30,7.86" xml:id="b35">
	<monogr>
		<title level="m" type="main" coord="14,345.51,548.18,135.08,7.86;14,151.52,559.14,30.27,7.86">Bert rediscovers the classical nlp pipeline</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05950</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.62,570.10,337.97,7.86;14,151.52,581.06,329.07,7.86;14,151.52,592.02,314.70,7.86" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="14,376.46,581.06,99.93,7.86">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,163.04,592.02,202.74,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,602.99,337.98,7.86;14,151.52,613.95,329.07,7.86;14,151.52,624.91,329.07,7.86;14,151.52,635.87,244.49,7.86" xml:id="b37">
	<monogr>
		<title level="m" type="main" coord="14,151.52,624.91,329.07,7.86;14,151.52,635.87,78.40,7.86">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName coords=""><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.62,646.84,337.98,7.86;14,151.52,657.79,329.07,7.86;15,151.52,120.67,329.07,7.86;15,151.52,131.63,297.90,7.86" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="14,151.52,657.79,236.90,7.86">Hierarchical attention networks for document classification</title>
		<author>
			<persName coords=""><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,408.28,657.79,72.31,7.86;15,151.52,120.67,329.07,7.86;15,151.52,131.63,197.55,7.86">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,142.59,337.98,7.86;15,151.52,153.55,329.07,7.86;15,151.52,164.51,329.07,7.86;15,151.52,175.46,97.09,7.86" xml:id="b39">
	<monogr>
		<title level="m" type="main" coord="15,200.52,164.51,214.96,7.86">Learning and evaluating general linguistic intelligence</title>
		<author>
			<persName coords=""><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cyprien</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jerome</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11373</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.62,186.42,337.98,7.86;15,151.52,197.38,329.07,7.86;15,151.52,208.34,84.02,7.86" xml:id="b40">
	<analytic>
		<title level="a" type="main" coord="15,342.96,186.42,137.63,7.86;15,151.52,197.38,107.44,7.86">Character-level convolutional networks for text classification</title>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,276.52,197.38,199.93,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
