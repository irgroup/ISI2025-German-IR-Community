<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,174.85,115.96,265.66,12.62;1,187.12,133.89,241.12,12.62;1,221.75,153.92,171.85,10.52">Automatic Thresholding by Sampling Documents and Estimating Recall ILPS@UVA at TAR Task 2.2</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,233.21,190.14,30.44,8.74"><forename type="first">Dan</forename><surname>Li</surname></persName>
							<email>d.li@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<postCode>1098XH</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.81,190.14,86.87,8.74"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<email>e.kanoulas@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<postCode>1098XH</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,174.85,115.96,265.66,12.62;1,187.12,133.89,241.12,12.62;1,221.75,153.92,171.85,10.52">Automatic Thresholding by Sampling Documents and Estimating Recall ILPS@UVA at TAR Task 2.2</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">09B5ACDB4D408B2DDD5D5202815094FD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Continuous active learning</term>
					<term>Active sampling</term>
					<term>R estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe the participation of the Information and Language Processing System (ILPS) group at CLEF eHealth 2019 Task 2.2: Technologically Assisted Reviews in Empirical Medicine. This task is targeted to produce an efficient ordering of the documents and to identify a subset of the documents which contains as many of the relevant abstracts for the least effort. Participants are provided with systematic review topics with each including a review title, a boolean query constructed by Cochrane experts, and a set of PubMed Document Identifiers (PID's) returned by running the boolean query in MEDLINE. We handle the problem under the Continuous Active Learning framework by jointly training a ranking model to rank documents, and conducting a "greedy" sampling to estimate the real number of relevant documents in the collection. We finally submitted four runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Systematic reviews are a type of literature review that uses systematic methods to reliably bring together the findings from multiple studies that address a question and are often used to inform policy and practice, e.g. the development of medical guideline in evidence-based medicine <ref type="bibr" coords="1,334.06,542.93,9.96,8.74" target="#b5">[6]</ref>. In order to write a systematic review, researchers have to come up with a Boolean query and conduct a search that will retrieve all the documents that are relevant. This is a difficult task, known in the Information Retrieval (IR) domain as the total recall problem.</p><p>The CLEF eHealth Task 2 "Technology Assisted Reviews in Empirical Medicine Introduction" aims to automate this process <ref type="bibr" coords="1,332.14,602.90,10.52,8.74" target="#b2">[3]</ref>  <ref type="bibr" coords="1,346.08,602.90,9.96,8.74" target="#b3">[4]</ref>. It consists of two subtasks. Task 2.1 focuses on the construction of the Boolean query, and Task 2.2 focuses on producing an efficient ordering of the documents, such that all of the relevant abstracts are retrieved as early as possible, and identifying a subset which contains all or as many of the relevant abstracts for the least effort.</p><p>We participated Task 2.2 and submitted 4 runs: abs-th-ratio-ilps@uva, abshh-ratio-ilps@uva, doc-th-ratio-ilps@uva, doc-hh-ratio-ilps@uva.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>Task 2.2 is Abstract and Title Screening. The participants are given the document collection extracted through the Boolean Search in Task 2.1, and are asked to produce an the efficient ordering of the documents, such that all of the relevant abstracts are retrieved as early as possible, and at the same time to identify a subset of A which contains all or as many of the relevant abstracts for the least effort (i.e. total number of abstracts to be assessed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The model</head><p>In this paper, we propose a novel model for the TAR process inspired by <ref type="bibr" coords="2,470.08,390.34,10.52,8.74" target="#b0">[1]</ref> and <ref type="bibr" coords="2,155.08,402.30,9.96,8.74" target="#b4">[5]</ref>. The model mainly consists of a ranking module, a sampling module, an assessment module, an estimation module and a stopping module. Given a topic and a document collection C the reviewer is interested in, together with a target recall level r target that the reviewer wants to achieve, the model iteratively outputs a set of documents until the estimated recall exceeds the target recall. We elaborate the model in Algorithm 1.</p><p>Let S denote the set of sampled documents and n denote the number of documents in S, L t denote the labelled documents (the training set) at batch t and U t denote the unlabelled documents at t. Initially, L t is empty and we fill it with a pseudo document d 0 which is made of the description of the topic provided. In line 3, k documents are uniformly sampled from U t , and temporarily labeled non-relevant and added to L t . In line 4, a ranking model is trained on L t . In line 5-7, a sampling distribution P t is constructed based on the ranked list of documents produced by the ranking model and a fixed number of b documents are independently and with replacement sampled from P t . In line 8, reviewers assess the relevance of the sampled documents. Note that the sampled b documents may contain duplicates, therefore reviewers only need to assess the unique documents. In line 10, R t and var( R t ) are calculated. In lines 11-15, the stopping module uses R t and var( R t ) to decide whether to stop or not. In line 17, produce the ordering of documents by sampled relevant, sampled non-relevant, un-sampled, with the stopping threshold at the first un-sampled documents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ranking module</head><p>We use the TF-IDF vector of a document as its features. Considering effectiveness and efficiency we employ Logistic Regression as the ranking model. We use its implementation in scikit-learn <ref type="foot" coords="3,278.35,508.39,3.97,6.12" target="#foot_0">1</ref> . At each batch t, a new model is trained from scratch using the current training data L t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sampling module</head><p>Sampling distribution Note that in Algorithm 1 we need to sample documents from a distribution P t = p t i (for notation simplicity we use P in this section). Ideally, the selection probability p i should be positively correlated with the relevance labels, which allows an estimator R with low variance (see Section 3.4). However, the relevance labels are not known until documents are assessed by the reviewers. What we have instead is a ranking model that can predict the probability of relevance and output a list of ranked documents, which we can use to construct P. We use Power Law distribution which assumes the selection probability of a document is a power function of its position in the ranked list, defined as</p><formula xml:id="formula_0" coords="4,220.27,173.53,260.32,23.22">p i = 1 Z 1 r i α r i ∈ [1, N ], α ∈ (0, +∞)<label>(1)</label></formula><p>where N is the number of documents in C, r i is the rank position,</p><formula xml:id="formula_1" coords="4,422.86,207.66,56.04,16.45">Z = N i=1 1 r α i</formula><p>is the normalization factor.</p><p>Inclusion probability We derive the first-order and second-order inclusion probabilities, which is indispensable to calculate R. We adopt sampling with replacement as our sampling method. At each batch t and for each draw, a document is sampled independently from one of the aforementioned distributions.</p><p>Let selection probability denote the probability that a document is sampled for a draw, and inclusion probability the probability that a document is included in the sample set considering all the draws. Under sampling-with-replacement design, the first-order inclusion probability π i is given by</p><formula xml:id="formula_2" coords="4,256.02,363.05,224.57,30.20">π i = 1 - T t=1 1 -p t i nt<label>(2)</label></formula><p>The second-order inclusion probability π i,j -the probability of any two different document d i and d j being included -is given by</p><formula xml:id="formula_3" coords="4,215.98,440.12,264.61,30.20">π i,j = π i + π j -1 - T t=1 1 -p t i -p t j nt<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Estimation module</head><p>We employ Horvitz-Thompson estimator and Hansen-Hurwitz estimator to estimate R and var(R). Both of them are designed for sampling with unequal probabilities, Hansen-Hurwitz estimator is only restricted for with-replacement sampling, while Horvitz-Thompson estimator is for any design. For more details of the derivation the reader can refer to Chapter 6 in <ref type="bibr" coords="4,369.89,563.22,9.96,8.74" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Horvitz-Thompson estimator</head><p>The Horvitz-Thompson estimator provides an unbiased estimator of population total under a general sampling theory <ref type="bibr" coords="4,449.51,608.30,9.96,8.74" target="#b1">[2]</ref>. Let τ = i∈S y i denote the population total. With any sampling design, with or without replacement, the unbiased Horvitz-Thompson estimator of the population total is τ = i∈S yi πi , where S is the subset of S only containing unique documents, and π i is the inclusion probability for document i.</p><p>In our case, the population total is the total number of relevant documents for a target topic, denoted as R = N i=1 y i . The Horvitz-Thompson estimator of R is</p><formula xml:id="formula_4" coords="5,273.04,156.13,203.31,30.47">R HT t = i∈ S t y i π i (<label>4</label></formula><formula xml:id="formula_5" coords="5,476.35,162.87,4.24,8.74">)</formula><p>where S t = ∪ t k=1 S k denote the accumulated sample set till batch t, y t i is relevance of document d t i . We use to denotes the operation to remove duplicated documents, and to denote the operation to cumulate documents in all previous batches.</p><p>Hansen-Hurwitz estimator Hansen-Hurwitz estimator provides an unbiased estimator of population total under sampling with replacement from the same distribution <ref type="bibr" coords="5,189.09,286.65,9.96,8.74" target="#b6">[7]</ref>.</p><p>In our case, the sampling distribution changes at each batch and converges to the ultimate distribution produced by the ranking model trained on the whole documents. The Hansen-Hurwitz estimator of R on S t is</p><formula xml:id="formula_6" coords="5,241.92,349.91,238.67,27.94">R HH t = 1 n t k∈{1,2,...,t},i∈S k y i p k i (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Stopping module</head><p>We propose a stopping strategy based on R. With sampling continuing, the strategy repeatedly examines whether r t &gt; R•r target , and if so stop TAR process. The intuition is straight forward, if we have collected more relevant than the target number we estimated, we should feel confident to stop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>The dataset consists of 72 topics for training and 31 topics for testing. For each topic, participants will be provided with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Runs</head><p>The proposed method is topic-wise in the sense that it repeatedly trains a new ranker based on the current assessed documents. It doesn't need extra training topics. Our runs are directly run on test data.</p><p>We submitted four runs: abs-th-ratio-ilps@uva, abs-hh-ratio-ilps@uva, docth-ratio-ilps@uva, doc-hh-ratio-ilps@uva. abs and doc denote whether qrels at abstract level or at content level is used for the relevance feedback in assessment module. th and hh denote whether Horvitz-Thompson estimator or Hansen-Hurwitz estimator is used to estimate R. A description of each run is presented below.</p><p>1. abs-th-ratio-ilps@uva abs qrels and Horvitz-Thompson estimator 2. abs-hh-ratio-ilps@uva abs qrels and Hansen-Hurwitz estimator 3. doc-th-ratio-ilps@uva doc qrels and Horvitz-Thompson estimator 4. doc-hh-ratio-ilps@uva doc qrels and Hansen-Hurwitz estimator For all the four runs, we set α = 0.8, b = 100, k = 100, target recall r target = 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Our method targets on finding a stopping threshold given a target recall. We re-rank all the sampled relevant documents on the top, followed by all the nonrelevant documents and all the un-sampled documents. The stopping threshold is set at the position of the first un-sampled documents. As all the sampled documents are before the stopping threshold, the stopping threshold also indicates the cost. As a consequence it is not valid to apply ranking metrics such as Average Precision, we report thresholding based metrics instead.</p><p>Table <ref type="table" coords="6,178.46,416.08,4.98,8.74">1</ref> shows the thresholding result on the test set. First, on both abs and content level, the Horvitz-Thompson estimator has recall threshold closer to the target recall 0.8 than the Hansen-Hurwitz estimator, which indicates a more accurate estimation of R. Second, both estimators stop at early stage when sampled documents are less than 50% of the complete documents.</p><p>Figure <ref type="figure" coords="6,182.58,476.34,4.98,8.74" target="#fig_3">1</ref> and 2 shows the topic-wise recall threshold v.s. norm threshold. Horvitz-Thompson estimator stops at various recall for different topics, while Hansen-Hurwitz estimator stops between 0.8 -1.0 for most topics. It indicates the estimation of R can help to stop viewing documents, but the variance of the estimated R is large for different topics.</p><p>Table <ref type="table" coords="6,238.47,568.08,3.87,8.74">1</ref>: Thresholding results on the test set RUN norm threshold recall threshold abs-th-ratio-ilps@uva 0.423 0.838 abs-hh-ratio-ilps@uva 0.47 0.89 doc-th-ratio-ilps@uva 0.392 0.894 doc-hh-ratio-ilps@uva 0.426 0.95</p><p>In this paper, we presented the runs we submitted to the CLEF 2019 eHealth Task 2.2. We handle the problem under the Continuous Active Learning framework by jointly training a ranking model to rank documents, and conducting a "greedy" sampling to estimate the real number of relevant documents in the collection. We finally submitted four runs.</p><p>The result indicates the method can retrieve most relevant documents (around 80% to 90%) with the cost viewing less than 50% of the complete documents. The estimation of R can help to stop viewing documents, but the variance of the estimated R is large for different topics. Further work needs to be done to reduce the variance of the estimated R. Topicwise recall_threshold v.s. norm_threshold (abs-th-ratio-ilps@uva).</p><p>(a) abs-th-ratio-ilps@uva Topicwise recall_threshold v.s. norm_threshold (abs-hh-ratio-ilps@uva).</p><p>(b) abs-hh-ratio-ilps@uva Topicwise recall_threshold v.s. norm_threshold (doc-th-ratio-ilps@uva).</p><p>(a) doc-th-ratio-ilps@uva Topicwise recall_threshold v.s. norm_threshold (doc-hh-ratio-ilps@uva).</p><p>(b) doc-hh-ratio-ilps@uva Fig. <ref type="figure" coords="9,174.14,622.62,3.87,8.74">2</ref>: Topicwise recall threshold v.s. norm threshold at content level.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,142.24,121.70,216.34,8.77;3,149.71,139.19,261.08,7.89;3,149.71,150.15,258.82,7.89;3,140.21,164.99,191.78,7.86;3,140.21,179.79,84.18,7.89;3,165.05,195.28,42.37,7.47;3,140.21,206.93,4.51,6.14;3,165.05,205.59,284.39,7.86;3,170.03,216.55,83.74,7.86;3,140.21,228.85,4.51,6.14;3,165.05,227.51,118.53,7.86;3,140.21,239.81,4.51,6.14;3,165.05,238.47,241.43,7.86;3,140.21,250.77,4.51,6.14;3,165.05,249.43,256.59,7.86;3,140.21,261.73,4.51,6.14;3,165.05,260.38,182.01,7.86;3,140.21,272.68,4.51,6.14;3,165.05,271.34,306.12,7.86;3,140.21,283.64,4.51,6.14;3,165.05,282.30,307.67,7.86;3,170.03,293.26,132.51,7.86;3,165.05,308.73,51.78,7.47;3,135.70,322.70,9.03,6.14;3,165.05,321.36,136.36,7.86;3,165.05,336.82,80.02,7.47;3,135.70,350.79,9.03,6.14;3,165.05,349.42,193.64,7.89;3,135.70,361.75,9.03,6.14;3,180.39,360.41,48.96,7.86;3,135.70,372.71,9.03,6.14;3,165.05,371.34,16.87,7.89;3,135.70,383.67,9.03,6.14;3,180.39,382.33,50.40,7.86;3,135.70,394.63,9.03,6.14;3,165.05,393.26,16.66,7.89;3,135.70,405.22,30.67,7.89;3,135.70,416.20,333.38,7.86;3,154.69,427.16,307.55,7.86"><head>Algorithm 1 : 4 Train a ranking model on Lt. 5 6 7 Sample b document from the distribution Pt. 8</head><label>145678</label><figDesc>Automatic thresholding algorithm Input: Target topic; document collection C, target recall rtarget. Output: A list of retrieved documents with stopping threshold. 1 Lt = {pseudo document d0 labelled relevant} 2 while not stop do // Sample 3 Temporarily augment Lt by uniformly sampling k documents from Ut, labeled non-relevant. Rank all the documents in C by the trained over Lt ranker. Construct sampling distribution Pt over the ranked documents. Render relevance assessments for the sampled documents that belong to Ut. 9 Remove the k temporary documents from Lt. Place the b labeled documents in Lt, and remove them from Ut. // Estimate 10 Calculate Rt and var( Rt) via (4). // Stop condition 11 if R and var( Rt) satisfy stopping strategy then 12 17 Produce the ordering of documents by sampled relevant, sampled non-relevant, un-sampled, with the stopping threshold at the first un-sampled documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,164.43,622.62,286.49,8.74"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Topicwise recall threshold v.s. norm threshold at abs level.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,656.80,96.91,7.86"><p>https://scikit-learn.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,305.98,342.25,7.86;7,146.91,316.91,333.68,7.89;7,146.91,327.89,131.41,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,295.01,305.98,185.59,7.86;7,146.91,316.93,185.72,7.86">Autonomy and reliability of continuous active learning for technology-assisted review</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<idno>CoRR abs/1504.06868</idno>
		<ptr target="http://arxiv.org/abs/1504.06868" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,338.85,342.24,7.86;7,146.91,349.78,333.68,7.89;7,146.91,360.77,42.49,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,278.95,338.85,201.64,7.86;7,146.91,349.81,82.75,7.86">A generalization of sampling without replacement from a finite universe</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,236.09,349.81,186.92,7.86">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">260</biblScope>
			<biblScope unit="page" from="663" to="685" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,371.73,342.24,7.86;7,146.91,382.69,333.68,7.86;7,146.91,393.65,192.37,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,345.43,371.73,135.16,7.86;7,146.91,382.69,153.76,7.86">Clef 2019 technologically assisted reviews in empirical medicine overview</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Spijker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,321.38,382.69,159.21,7.86;7,146.91,393.65,112.11,7.86">CLEF 2019 Evaluation Labs and Workshop: Online Working Notes</title>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,404.61,342.25,7.86;7,146.91,415.56,333.68,7.86;7,146.91,426.52,333.68,7.86;7,146.91,437.48,245.64,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,323.38,415.56,157.22,7.86;7,146.91,426.52,12.29,7.86">Overview of the clef ehealth evaluation lab</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Spijker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jimmy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,203.79,426.52,272.00,7.86">CLEF 2019 -10th Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="7,146.91,437.48,170.30,7.86">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,448.44,342.24,7.86;7,146.91,459.40,333.68,7.86;7,146.91,470.36,152.31,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,229.82,448.44,246.79,7.86">Active sampling for large-scale information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,161.94,459.40,318.65,7.86;7,146.91,470.36,49.34,7.86">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,481.32,342.25,7.86;7,146.91,492.28,333.68,7.86;7,146.91,503.21,188.54,7.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,438.59,481.32,42.00,7.86;7,146.91,492.28,333.68,7.86;7,146.91,503.24,42.65,7.86">Using text mining for study identification in systematic reviews: a systematic review of current approaches</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>O'mara-Eves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mcnaught</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,196.91,503.24,76.41,7.86">Systematic reviews</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,514.19,342.24,7.86;7,146.91,525.15,25.60,7.86" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Thompson</surname></persName>
		</author>
		<title level="m" coord="7,217.94,514.19,35.50,7.86">Sampling</title>
		<meeting><address><addrLine>Hoboken, New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>3 edn.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
