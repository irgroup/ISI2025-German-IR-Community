<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.43,116.95,292.50,12.62;1,152.60,134.89,310.16,12.62;1,243.99,152.82,127.39,12.62">CheckThat! Automatic Identification and Verification of Claims: IIT(ISM) @CLEF&apos;19 Check Worthiness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.34,190.49,58.88,8.74"><forename type="first">Ritesh</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian institute of Technology (Indian School of Mines) Dhanbad</orgName>
								<address>
									<postCode>826004</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,213.09,190.49,74.73,8.74"><forename type="first">Shivansh</forename><surname>Prakash</surname></persName>
							<email>helloshivanshprakash@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian institute of Technology (Indian School of Mines) Dhanbad</orgName>
								<address>
									<postCode>826004</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.83,190.49,75.08,8.74"><forename type="first">Shashank</forename><surname>Kumar</surname></persName>
							<email>shashank0218@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian institute of Technology (Indian School of Mines) Dhanbad</orgName>
								<address>
									<postCode>826004</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,393.61,190.49,76.41,8.74"><forename type="first">Rajendra</forename><surname>Pamula</surname></persName>
							<email>rajendrapamula@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian institute of Technology (Indian School of Mines) Dhanbad</orgName>
								<address>
									<postCode>826004</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.43,116.95,292.50,12.62;1,152.60,134.89,310.16,12.62;1,243.99,152.82,127.39,12.62">CheckThat! Automatic Identification and Verification of Claims: IIT(ISM) @CLEF&apos;19 Check Worthiness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6F35B1710F002514ED9A4DF041822CF0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LSTM</term>
					<term>SVM</term>
					<term>Feature extraction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the work that we did at Indian Institute of Technology (ISM) towards CheckThat!: Automatic Identification and Verification of Claims for CLEF 2019. As per requirement of CLEF-2019 we submit the only one run in its Check Worthiness Task. Two-fold crossvalidation is used to select a model for submission to CheckThat Lab at CLEF 2019. For our run, we use SVM and LSTM method. Overall, our performance is not satisfactory. However, as new entrant to the field, our scores are encouraging enough to work for better results in future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Investigative journalists and volunteers have been working hard to get to the core of a claim and present solid evidence in favor or against it. And in this day and age of information abundant amount of info/data is readily available, thus manual fact-checking is very time-consuming and therefore automatic methods were proposed as a means of speeding up the process <ref type="bibr" coords="1,365.58,497.34,9.96,8.74" target="#b0">[1]</ref>. Also, some steps of the fact-checking pipeline are given less attention, e.g. checking worthiness estimates is severely mis-understood as a problem. That is why we took this problem of checking worthiness estimates of statements/claims. The overview of of shared task for CheckThat! can be found in <ref type="bibr" coords="1,305.76,545.16,9.96,8.74" target="#b1">[2]</ref>. In brief, we have to identify which sentence should be prioritized for fact-checking given a political debate or a transcribed speech, segmented into sentences with annotated speakers. This is a ranking task and systems are required to produce a score per sentence that will perform the ranking. The task is performed in English. The rate at which a statement in an interview, a press release, or a tweet can spread almost instantly across the globe has created an unprecedented situation <ref type="bibr" coords="2,134.77,119.99,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="2,146.95,119.99,7.01,8.74" target="#b3">4]</ref>. There is almost no time to cross-check a statement or a claim against facts and this has proved critical in politics, e.g. during the 2016 US Presidential Campaign, whose results is said to be affected by fake news and claims spread using social media. As it became apparent that this is a problem that can create great affects in our lives, a number of fact-checking programs have started, led by organizations such as FactCheck and Snopes <ref type="bibr" coords="2,346.02,179.77,7.75,8.74" target="#b4">[5]</ref><ref type="bibr" coords="2,353.77,179.77,3.87,8.74" target="#b5">[6]</ref><ref type="bibr" coords="2,357.65,179.77,7.75,8.74" target="#b6">[7]</ref>.</p><p>The organization of the rest of the paper is as follows. Section 2 describes about the dataset. We describe our methodology: field categories and indexing, which document and topic fields we used for retrieval in Section 3. In Section 4 we describe our results. Finally, we conclude in Section 5 with directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>We participate in Task 1. The detail description of dataset can be found in <ref type="bibr" coords="2,134.77,306.94,9.96,8.74" target="#b7">[8]</ref>. The training data consisted of 19 files which include political debates and speeches. Each file contains the debate/speech split into sentences. Each line contains a single sentence, its speaker and a label which is annotated by experts as check-worthy or not. The sentence is labelled 0 if it is not worthy of checking and 1 if it is worthy for checking. The data consists of a total of 16,421 sentences, of which 440 were labeled as check-worthy. There is an imbalance in the dataset with only 2.68% of the dataset labelled same as that of the target class i.e. check-worthy. A few instances of this training data, along with their speakers and labels, are presented below : 300 SANDERS Let's talk about climate change. 0 301 SANDERS Do you think there's a reason why not one Republican has the guts to recognize that climate change is real, and that we need to transform our energy system? 1 The test data is a collection of seven files consisting of debates and speeches. In this task, we do not use any external knowledge other than domain-independent language resources such as parsers and lexicons. Instead, we concentrate on extracting linguistic features that can indicate the check-worthiness of the sentences .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Preprocessing and Feature Extraction</head><p>Data Processing is required to prepare data to be used by systems to classify and rank statements according to the check-worthiness score. Rich feature extraction is needed to perform ranking according to language constructs rather than relying on heuristic or encyclopedia Knowledge. Syntactic and semantic features were extracted from both speeches and debates to represent sentences consistently and converted every sentence into vector. These features are: Lexical Features, Sentence Embedding, Stylometric Features, Semantic Features, affective Features and Metadata Features. First of all we perform speaker normalization by assigning each speaker an unique Id. The reason for this is that the same speaker is present in the training data with different names in different instances. The sentences in the training data are first tokenized. We also remove stop-words and the remaining tokens are stemmed using a stemmer. A single file which contains sentences extracted from required file is made to be used as training data. Syntactic and semantic features are extracted from both speeches and debates to represent sentences consistently and converted every sentence into vector. We use two approaches for designing the model for this task Support Vector Machine (SVM) and Recurrent Neural Network using Long Short Term Layers (LSTM). We use SVM because of its simplicity, ease of use and its ability to avoid over-fitting using regularization parameters. However, since the datasets are more aptly described using a sequence of sentences, a model which considers this sequence-time nature of the data will obviously be a better choice. Since SVMs do not consider this, we also try to implement a recurrent neural network model having memory units, in the form OF LSTM layers. For our model, we use a layer of LSTM cells having an input shape of (batch size, no. of time steps, feature dimension). There were 64 units of LSTM cells in each layer hence, the output shape of the layer will be (64). We then add a dense layer of 64 neurons after the LSTM layer with a Rectified Linear Unit (ReLU) activation function. A dropout layer is also added to avoid over-fitting of data. Finally, a softmax output layer is added producing output probabilities of the classes for each input instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The complete training data is divided into two parts i.e. training data and validation data. The validation data consists of a speech and a debate selected from the training part. The models are trained using the training part and evaluated using validation part. Many models with different parameters are generated and evaluated and the model giving the best results is selected from them. We are provided with some code that helped us to evaluate our models in various evaluation metrics. We get the best result using Recurrent neural network model rather than SVMs. The scores obtained by our six runs are given in Table <ref type="table" coords="3,472.84,536.46,3.87,8.74" target="#tab_0">1</ref>. The official evaluation measure provided by CLEF'19 is M AP . We show the best score in the task demonstrated by run-id Copenhagen(*), for the sake of comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This year we participate in Task 1 of the Check That!: Automatic Identification and Verification of Claim. A SVM and a Recurrent Neural Network model is used with supervised learning to classify and rank sentences according to their check-worthy score in political debates and speeches. A rich feature set is extracted to represent sentences in best possible way to tackle class imbalance and avoid heuristic or encyclopedia Knowledge. Two-fold cross-validation is used to select a model for submission to CheckThat Lab at CLEF 2019. While there can be no denial of the fact that our overall performance is average, initial results are suggestive as to what should be done next. Our work has shown us lot of interesting possibilities for future work. There is a lot of room for improvements in our task. Linguistic form of information is under-studied and can be explored in more depth for better result. We use shallow syntactic features in our project, this can be improved by using very deep syntactic feature to represent sentence. Further study is required for including more linguistic and non-linguistics features which may affect check-worthy score of phrases. Furthermore, we can use more complex recurrent neural network models to achieve better result. We shall be exploring some of these tasks in the coming days.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,174.20,604.22,266.96,57.35"><head>Table 1 .</head><label>1</label><figDesc>Results -The official evaluation Measure by CLEF 2019</figDesc><table coords="3,216.88,626.98,180.32,34.59"><row><cell>RUN ID</cell><cell>Rank MAP RR R -P</cell></row><row><cell>Copenhagen(*)</cell><cell>1 .1660 .4176 .1387</cell></row><row><cell cols="2">ISMD16titlefield 10 .0835 .2238 .0714</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,142.96,381.92,337.63,7.86;4,151.52,392.88,329.07,7.86;4,151.52,403.84,329.07,7.86;4,151.52,414.80,114.68,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,392.07,381.92,88.52,7.86;4,151.52,392.88,174.12,7.86">Linguistic Models for Analyzing and Detecting Biased Language</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,349.70,392.88,130.89,7.86;4,151.52,403.84,226.16,7.86">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s" coord="4,424.92,403.84,47.30,7.86">Long Papers</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1650" to="1659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,142.96,425.09,337.63,7.86;4,151.52,436.05,329.07,7.86;4,151.52,447.00,329.07,7.86;4,151.52,457.96,329.07,7.86;4,151.52,468.92,261.71,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,151.52,447.00,329.07,7.86;4,151.52,457.96,55.54,7.86">Overview of the CLEF-2019 CheckThat!: Automatic Identification and Verification of Claims</title>
		<author>
			<persName coords=""><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maram</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reem</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giovanni</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pepa</forename><surname>Atanasova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,214.53,457.96,266.05,7.86;4,151.52,468.92,23.81,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09">September, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,142.96,479.21,337.64,7.86;4,151.52,490.17,176.31,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="4,209.82,479.21,198.89,7.86">Snowball: A Language for Stemming Algorithms</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://snowballtartarus.org/texts/introduction.html" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,142.96,500.45,337.63,7.86;4,151.52,511.41,329.07,7.86;4,151.52,522.37,329.07,7.86;4,151.52,533.33,135.39,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,306.05,500.45,174.53,7.86;4,151.52,511.41,111.59,7.86">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,283.92,511.41,196.68,7.86;4,151.52,522.37,179.80,7.86">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">7986</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,142.96,543.62,337.64,7.86;4,151.52,554.58,274.14,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="4,341.36,543.62,139.23,7.86;4,151.52,554.58,112.36,7.86">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,142.96,564.86,32.26,7.86;4,205.59,564.86,10.24,7.86;4,246.26,564.86,39.81,7.86;4,316.48,564.86,39.94,7.86;4,386.76,564.86,18.43,7.86;4,435.54,564.86,45.06,7.86;4,151.52,575.82,194.14,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="4,246.26,564.86,39.81,7.86;4,316.48,564.86,39.94,7.86;4,386.76,564.86,18.43,7.86;4,435.54,564.86,40.96,7.86">TextBlob: Simplified Text Processing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Loria</surname></persName>
		</author>
		<ptr target="http://textblob.readthedocs.org/en/dev/(2014" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,142.96,586.11,337.64,7.86;4,151.52,597.07,329.07,7.86;4,151.52,608.03,265.57,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,416.94,586.11,63.65,7.86;4,151.52,597.07,149.14,7.86">A content management perspective on fact-checking</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cazalens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lamarre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,330.05,597.07,150.54,7.86;4,151.52,608.03,232.45,7.86">Journalism, Misinformation and Fact Checking&quot; alternate paper track of&quot; The Web Conference</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,142.96,618.31,337.63,7.86;4,151.52,629.27,329.07,7.86;4,151.52,640.23,329.07,7.86;4,151.52,651.19,68.75,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,283.58,629.27,197.01,7.86;4,151.52,640.23,211.18,7.86">Overview of the CLEF-2019 CheckThat! Lab on Automatic Identification and Verification of Claims</title>
		<author>
			<persName coords=""><forename type="first">Pepa</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georgi</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Da San</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giovanni</forename><surname>Martino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,371.20,640.23,104.98,7.86">Task 1: Check-Worthiness</title>
		<imprint>
			<publisher>CLEF-ceur</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
