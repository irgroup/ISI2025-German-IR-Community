<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.87,115.96,333.61,12.62;1,207.20,133.89,200.96,12.62">bigIR at CLEF 2019: Automatic Verification of Arabic Claims over the Web</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.67,171.73,66.62,8.74"><forename type="first">Fatima</forename><surname>Haouari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Qatar University</orgName>
								<address>
									<settlement>Doha</settlement>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.27,171.73,51.20,8.74"><forename type="first">Zien</forename><surname>Sheikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Qatar University</orgName>
								<address>
									<settlement>Doha</settlement>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,359.26,171.73,64.43,8.74"><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
							<email>telsayed@qu.edu.qa</email>
							<affiliation key="aff0">
								<orgName type="institution">Qatar University</orgName>
								<address>
									<settlement>Doha</settlement>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.87,115.96,333.61,12.62;1,207.20,133.89,200.96,12.62">bigIR at CLEF 2019: Automatic Verification of Arabic Claims over the Web</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C9938D818C429893A21442C57AAF156B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fact Checking</term>
					<term>Arabic Retrieval</term>
					<term>Learning to Rank</term>
					<term>Web Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the proliferation of fake news and its prevalent impact on democracy, journalism, and public opinions, manual fact-checkers become unscalable to the volume and speed of fake news propagation. Automatic fact-checkers are therefore needed to prevent the negative impact of fake news in a fast and effective way. In this paper, we present our participation in Task 2 of CLEF-2019 CheckThat! Lab, which addresses the problem of finding evidence over the Web for verifying Arabic claims. We participated in all of the four subtasks and adopted a machine learning approach in each with different set of features that are extracted from both the claim and the corresponding retrieved Web search result pages. Our models, trained solely over the provided training data, for the different subtasks exhibited relatively-good performance. Our official results, on the testing data, show that our best performing runs achieved the best overall performance in subtasks A and B among 7 and 8 participating runs respectively. As for subtasks C and D, our best performing runs achieved the median overall performance among 6 and 9 participating runs respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fake news is witnessing an explosion recently, and it is considered as one of the biggest threats to democracy, journalism, and public trust in governments. In combating fake news, the number of manual fact-checking organizations increased by 239% in a period of four years, where it reached 149 fact-checkers in 2018 as apposed to only 44 in 2014 1 .</p><p>One of the main challenges is that manual fact-checking does not scale with the volume of daily fake news. This mismatch can be attributed to the gap between the time the claim is made and the time the claim is checked and published, as it is very time-consuming for journalists to find check-worthy claims and verify them. Another challenge is that fact-checking requires advanced writing skills in order to convince the readers whether the claim is true or false <ref type="bibr" coords="2,455.53,154.86,9.96,8.74" target="#b5">[6]</ref>. In fact, it is estimated that check-worthiness of a claim and writing an article about it can take up to one day <ref type="bibr" coords="2,250.17,178.77,9.96,8.74" target="#b6">[7]</ref>. Moreover, manual fact-checkers are outdated <ref type="bibr" coords="2,467.31,178.77,9.96,8.74" target="#b5">[6]</ref>. Most of the fact-checking frameworks adopt the old content management systems specialized for traditional blogs and newspapers, but not built for the current modern journalism. A new approach is therefore needed for automated fake news detection and verification.</p><p>The industry and academia have shown an overwhelming interest in fake news to address the challenges of its detection and verification. Many pioneering ideas were proposed to address many aspects of fact-checking systems with their focus varies between detecting check-worthy claims <ref type="bibr" coords="2,369.87,274.41,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="2,382.04,274.41,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="2,396.43,274.41,7.01,8.74" target="#b6">7]</ref>, checking claims factuality <ref type="bibr" coords="2,179.57,286.37,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="2,196.73,286.37,12.73,8.74" target="#b14">15,</ref><ref type="bibr" coords="2,211.12,286.37,12.73,8.74" target="#b15">16,</ref><ref type="bibr" coords="2,225.51,286.37,11.62,8.74" target="#b19">20]</ref>, checking news media factuality <ref type="bibr" coords="2,384.00,286.37,9.96,8.74" target="#b1">[2]</ref>, and proposing full automatic fact-checking systems <ref type="bibr" coords="2,282.78,298.32,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="2,294.96,298.32,12.73,8.74" target="#b13">14,</ref><ref type="bibr" coords="2,309.35,298.32,11.62,8.74" target="#b21">22]</ref>. There are also some shared tasks proposed and open to the research community interested in the problem such as FEVER-2018 task for fact extraction and verification <ref type="bibr" coords="2,368.97,322.23,15.50,8.74" target="#b17">[18]</ref> and CheckThat! 2018 lab on automatic identification and verification in political debates at CLEF <ref type="bibr" coords="2,462.32,334.19,14.61,8.74" target="#b12">[13]</ref>.</p><p>This year, CLEF-2019 CheckThat! Lab <ref type="bibr" coords="2,329.79,346.14,10.52,8.74" target="#b3">[4]</ref> introduced two tasks to tackle two main problems of automated fact-checking systems. The main objective of the first is to detect check-worthy-claims to be prioritized for fact-checking <ref type="bibr" coords="2,467.31,370.05,9.96,8.74" target="#b0">[1]</ref>, while the second focuses on evidence extraction to support fact-checking a claim <ref type="bibr" coords="2,134.77,393.96,9.96,8.74" target="#b4">[5]</ref>. In this paper, we present the approach adopted by our bigIR group at Qatar University to address the second task.</p><p>Task 2 (Evidence and Factuality) addresses the problem of finding evidence over the Web for verifying Arabic claims. It assumes the system is given an Arabic claim (as a short sentence) and a corresponding ranked list of Web pages that were retrieved by a Web search engine for that claim. The system then needs to address four sub-problems, each is defined as a subtask as follows:</p><p>1. Subtask A: Rank the retrieved pages based on how useful they are for verifying the claim. 2. Subtask B: Classify the Web pages as "very useful" for verification, "useful", "not useful", or "not relevant". 3. Subtask C: Within each useful page, identify which passages are useful for claim verification. 4. Subtask D: Determine the true factuality of the claim, i.e., whether it is "True" or "False".</p><p>We have participated in all of the four subtasks. Since it is the first year of the task (and thus our first attempt), we generally adopted a simple machine learning approach, where learning models were trained only on the given training data over hand-crafted features. We applied feature ablation to assess the impact of each feature on the performance of our models.</p><p>For subtask A, to re-rank the pages based on their usefulness, we adopted a pairwise learning-to-rank approach with features extracted either from the page as a whole (such as source popularity, URL links, and number of quotes), from the relevant segments in the page (such as the similarity score of the most relevant sentence), or from the search results (such as the original rank of the page). Additionally, we extracted claim-dependent features such as the similarity between the claim and the title and the snippet of the page.</p><p>For subtask B, we adopted a multi-class classification approach to classify the Web pages. We considered several features including word embeddings, named entities, similarity scores, number of relevant sentences in the page, and URLbased features (such as URL length, URL scheme, and URL domain).</p><p>For subtask C, we adopted a binary classification approach to classify the passages within a useful page. Features included Bag-Of-Words (BOW), named entities, number of quotes, score of most relevant sentence from each passage, and the similarity score between the claim and the passage.</p><p>For subtask D, we also adopted a binary classification approach to discover the claim's factuality given the retrieved Web pages. To classify the claim, we first identify the most similar pages to the claim for feature extraction. For the selected pages, we consider their similarity scores, source popularity, and the sentiment of the page.</p><p>Our contribution in this work is two-fold:</p><p>1. We participated in all of the four subtasks adopting a machine learning approach with relatively-different set of features in each. The features are extracted from both the claims and the retrieved Web pages. 2. Our best performing runs exhibited the best performance in both subtasks A and B among the submitted runs.</p><p>The remainder of this paper is organized as follows. Section 2 describes how we processed and extracted features from the claims and retrieved pages. Sections 3, 4, 5, and 6 outline our approach and discuss our experimental evaluation in detail for subtask A, B, C, and D respectively. Finally, Section 7 concludes and discusses possible future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preprocessing &amp; Feature Extraction</head><p>In our work, we apply common main preprocessing for all subtasks to parse documents, identify relevant segments, and extract features. However, we include or exclude some features in each subtask. In this section, we describe in detail the preprocessing steps and introduce and motivate the features we extracted at all levels. For each page, we extract two types of features: features that depend on the claim/page relationship (claim-dependent) and features that depend solely on the page (page-dependent).</p><p>In what follows, a text segment in a page is centered by one sentence, but also includes both the sentence that precedes and the sentence that follows it, as defined by Yasser et al. <ref type="bibr" coords="3,251.74,656.12,14.61,8.74" target="#b20">[21]</ref>, to consider the context of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">HTML Parsing</head><p>As the Web pages are in raw HTML format, we parse each page by extracting only the clean version of the textual body discarding images, videos, and scripts using newspaper<ref type="foot" coords="4,206.71,163.09,3.97,6.12" target="#foot_0">2</ref> and BeautifulSoup<ref type="foot" coords="4,295.71,163.09,3.97,6.12" target="#foot_1">3</ref> Python libraries. We removed stopwords using Python NLTK <ref type="foot" coords="4,221.87,175.04,3.97,6.12" target="#foot_2">4</ref> Arabic stopwords. We also discard the sentences containing less than 3 words, motivated by the empirical study done by Zhi et al. <ref type="bibr" coords="4,445.27,188.57,14.60,8.74" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text Vector Representations</head><p>In extracting our features, we consider two text vector representations:</p><p>-Bag-of-Words (BOW): We consider BOW representation to represent full passages (mainly for subtask C). We considered only the terms that appeared at least 7 times in the training data, based on some preliminary experiments. -Distributed Representation (W2V): We consider word2vec embeddings <ref type="bibr" coords="4,479.64,299.67,15.50,8.74" target="#b11">[12]</ref> to represent the claim and the segments of a page; each is represented as the average vector of the embeddings of terms in the claim/segment. We used the pre-trained AraVec embeddings model proposed by Soliman et al. <ref type="bibr" coords="4,459.38,335.54,14.61,8.74" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relevant Segments Identification</head><p>To identify relevant segments in a page for a given claim, we represent the claim and each sentence in the page by their average of term W2V vectors. We then compute the cosine similarity between the vectors of the claim and each segment.</p><p>Segments are considered relevant if the similarity score is higher than a threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Page-Dependent Features</head><p>We extracted two types of page-dependent features: credibility and content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Credibility Features</head><p>To indicate the credibility of the page, we consider the following features:</p><p>-Source Popularity (SrcPop): This feature may indicate trustworthiness, as it captures how popular a particular website is. We used Amazon Alexa rank<ref type="foot" coords="4,171.38,565.76,3.97,6.12" target="#foot_3">5</ref> motivated by Baly et al. <ref type="bibr" coords="4,295.51,567.34,10.52,8.74" target="#b1">[2]</ref> that used this feature to estimate the reliability of media sources. We consider this feature as a categorical feature by binning the ranking values into 10 categories, then we convert it to a one hot encoding vector of 10 binary features.</p><p>-URL Features: these features were used by Baly et al. <ref type="bibr" coords="5,408.31,118.99,10.52,8.74" target="#b1">[2]</ref> to detect the reliability of web sources. We used Python URL handling library urlib<ref type="foot" coords="5,463.46,129.37,3.97,6.12" target="#foot_4">6</ref> to parse the URL and extract the following orthographic features:</p><p>• Length (URLLen) and Number of Sections (URLSecs): The length of the URL path and the number of sections separated by '/' help indicate whether the website is legitimate, irregular, or a phishing website. • Scheme (URLScheme): The URL protocol (https or http) indicates the trustworthiness of the website. We extracted the URL scheme then we used scikit-learn label encoder <ref type="foot" coords="5,313.67,225.79,3.97,6.12" target="#foot_5">7</ref> to encode string values of schemes to integers. • Domain Suffix(URLSfx): The suffix of a URL domain determines the source and credibility of the website. For example, a website with domain suffix .gov is a federal government site and is more credible than a commercial website with a suffix of .com. We used label encoder to encode their string values into integers.</p><p>Content Features From page body, we extract the following linguistic and similarity features: We used Python polyglot NLP tool<ref type="foot" coords="5,419.89,463.97,3.97,6.12" target="#foot_6">8</ref> to recognize location, organizations, and persons entities in the most relevant segment of the page. We form a vector of 3 integer values representing the number of occurrences of every entity type in the segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Claim-Dependent Features</head><p>We extracted the following features based on the claim-page interaction:</p><p>-Original Rank (Rank): This feature is available from the search results and it represents how the page is potentially-relevant to the claim according to the search engine. -Similarity: This includes cosine similarity between claim and title (ClmTtlSim), claim and snippet(ClmSnptSim), and claim and a passage (ClmPsgSim).</p><p>-Number of Relevant Sentences (NRelSent): For every page, we compute the similarity between the claim and each sentence. We count the number of relevant sentences in each page as it might indicate the relevance of the page. -Number of relevant webpages (NRelPages): For every claim, we count the number of webpages with a similarity score between claim and most relevant sentence higher than a certain threshold. -Score of the most Relevant Segment (MostRelSeg): This feature indicates how similar the most relevant segment is to the claim. -Sentiment (SntCnt): Sentiment analysis can help identify if the stance of the page is positive, negative, or neutral. This may help in identifying whether the page agrees with the claim or not. We use polyGlots Sentiment model<ref type="foot" coords="6,177.99,259.79,3.97,6.12" target="#foot_7">9</ref> to extract sentiments. From the most relevant segment, we get two values, the number of words with positive polarity and the number of words with negative polarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Subtask A: Reranking Retrieved Pages</head><p>In this subtask <ref type="bibr" coords="6,203.16,338.16,9.96,8.74" target="#b2">[3]</ref>, the goal is to rerank the retrieved pages based on their usefulness for verifying a specific claim. In this section, we present our proposed approach, experimental setup and results, our selected runs for CLEF submissions, and finally we will present the CLEF results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approach</head><p>Our approach is based on learning-to-rank (L2R). We propose a pairwise L2R model considering three different L2R classifiers, namely, SVM C-Support Vector Classification (SVC), which is implemented based on libsvm<ref type="foot" coords="6,398.30,443.26,7.94,6.12" target="#foot_8">10</ref> , Gaussian Naïve Bayes (Gaussian NB), and the ensemble classifier Random Forest (RF), using Scikit-learn Python library. <ref type="foot" coords="6,255.47,467.17,7.94,6.12" target="#foot_9">11</ref> We consider the following features (discussed in Section 2):</p><p>-Basic features: Rank, SrcPop, and MostRelSeg.</p><p>-Similarity features: ClmTtlSim and ClmSnptSim.</p><p>-NLinks.</p><p>-NQts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head><p>Parameters We experimented with the three different classifiers mentioned in 3.1. We set the kernel for SVC to linear, and set the number of estimators for the RF models to 100 (based on preliminary experiments). For the NB models, we did not tune any hyper-parameters and used the default settings.</p><p>Baselines We compare our models against a baseline that returns the pages ranked in their original ranks (i.e., based on relevance scores of the search engine, not on usefulness for fact-checking).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation on Training</head><p>As we were constrained by the size of the training data, containing only 10 claims, we adopted leave-one-claim-out (LOO) cross validation to evaluate the trained models. We optimized our models using the graded relevance measure NDCG@20.</p><p>We first experimented with different values of the cosine similarity threshold (0.4, 0.5, 0.6, and 0.7) when extracting relevant segments. In our unreported preliminary experiments, we observed that the best performing models were the ones trained with features extracted using a similarity threshold of 0.4 and 0.7, presented in Fig. <ref type="figure" coords="7,211.12,303.01,4.98,8.74" target="#fig_1">1</ref> and Fig. <ref type="figure" coords="7,258.89,303.01,4.98,8.74" target="#fig_2">2</ref> respectively. We also tried different combinations of features as shown in both figures.</p><p>The results show that our models could not beat the baseline with only the basic features. However, NB models outperformed the baseline when other features were introduced. We also notice that introducing the ClmTtlSim and ClmSnptSim to the basic features improved the performance of our models, while excluding the SrcPop feature improved the performance. Moreover, our proposed NLinks and NQts features did not have a noticeable impact on the performance of the models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">CLEF Evaluation</head><p>Runs As shown in Fig. <ref type="figure" coords="8,245.27,372.74,3.87,8.74" target="#fig_1">1</ref>, NB models outperform other L2R models over the training data, therefore we picked the 3 best NB models to submit to CLEF:. Moreover, when the cosine similarity threshold was set to 0.7, RF outperformed other models, as shown in Fig. <ref type="figure" coords="8,271.53,503.47,3.87,8.74" target="#fig_2">2</ref>, so we also picked its best performing model:</p><p>4. RF trained with Basic, ClmTtlSim, and ClmSnptSim features, and excluding SrcPop.</p><p>Results As shown in Table <ref type="table" coords="8,259.20,572.43,3.87,8.74" target="#tab_1">1</ref>, the official CLEF evaluation shows that our best performing model on the test data was the NB model trained with basic, ClmT-tlSim, and ClmSnptSim features (excluding SrcPop) which achieved NDCG@20 value of 0.55. This was the maximum score achieved among 7 runs submitted for this subtask. We observed that the performance of our models on training data was better than on testing data; this can be attributed to the small size of the training dataset, containing only 395 pages from 10 claims, which could be insufficient and not a good representative to train the models. The main goal of this subtask <ref type="bibr" coords="9,273.52,305.22,10.52,8.74" target="#b2">[3]</ref> is to classify all retrieved Web pages based on how useful they are in detecting the claim's veracity. A webpage is useful if it has enough evidence to verify the claim and if its source is trustworthy. In this section, we present our approach, experimental setup, training results, and CLEF results for our submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Approach</head><p>In our approach for this subtask, we use different machine learning algorithms to perform multi-class classification. We consider SVC as it shows to learn well from small datasets. We also include Gradient Boosting (GB) and RF as an ensemble model. As mentioned in 3.1, we use Scikit-learn Python library for our implementation. We consider the following features:</p><p>-Basic features: Rank, SrcPop and MostRelSeg.</p><p>-NEs in the relevant segment.</p><p>-NQts.</p><p>-URL features.</p><p>-W2V representation of both the claim and the relevant segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>Parameters For SVC, we used an RBF kernel with regularization parameter C = 15 and L2 penalty, and we set γ to 0.01 to avoid over-fitting. For GB and RF models, we set the number of estimators to 100 and 150 respectively (based on preliminary experiments).</p><p>Baselines As a baseline we adopted Wang et al. <ref type="bibr" coords="9,348.56,632.21,15.50,8.74" target="#b18">[19]</ref> method for feature extraction and classification. Their dataset consists of short passages where passages are classified into five different categories. This baseline was selected because the feature extraction methods are implemented on short passages similar to the size of our extracted relevant segments. Moreover, they are working on fine-grain classification.</p><p>Since our training data is highly imbalanced, we also used the Zero Rule algorithm as a baseline for this subtask. Zero Rule algorithm predicts the majority class in the dataset. In our training data, class -1 (non-relevant) is the majority class with 65% of the labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on Training</head><p>We conducted multiple experiments in attempt to find which features combination will result in the best F 1 score. We split our dataset into 70% for training and 30% for testing. From our experiments, we noticed that varying the similarity threshold when extracting relevant segments had a significant impact on the overall score. We concluded that our best performing models were the ones trained with features extracted with similarity thresholds of 0.4 and 0.7. Fig. <ref type="figure" coords="10,134.77,333.46,4.98,8.74" target="#fig_3">3</ref> and Fig. <ref type="figure" coords="10,185.51,333.46,4.98,8.74" target="#fig_4">4</ref> show the results obtained from our experiments using similarity thresholds 0.4 and 0.7 respectively.</p><p>We observed that when training the classifiers with basic features and NEs the performance improved. On the other hand, incorporating some content features like URL features and W2V vectors had a negative impact on the performance of the classifiers. We also note that ensemble classifiers (GB and RF) outperformed the baselines and other classifiers all the time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">CLEF Evaluation</head><p>Runs As concluded in section 4.3, ensemble classifiers have outperformed SVC classifiers. So, for our runs we picked the GB and RF models. We selected the following models with cosine similarity threshold of 0.7:</p><p>1. GB Classifier trained with basic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">GB Classifier trained with basic features and NEs.</head><p>We also picked the following models when cosine similarity threshold is set to 0.4: 3. GB Classifier trained with basic features and NQts. 4. RF Classifier trained with basic features and NQts.</p><p>Results Table <ref type="table" coords="11,201.93,494.57,4.98,8.74" target="#tab_2">2</ref> shows our training results compared to the official CLEF testing results. We notice that our best validation model with F 1 score of 0.52 that combines basic features with NEs has achieved lower testing score. Meanwhile, our model that combines basic features with NQts has scored a testing F 1 score of 0.31. The inconsistency between train and test F 1 scores can be justified due to the small training dataset of only 395 webpages. Also, the imbalance in the classes of the dataset could have caused the models to overfit. Our best model that achieved F 1 score value of 0.31 is the highest among all submitted runs for this subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Subtask C: Classifying Passages</head><p>In this subtask <ref type="bibr" coords="11,208.04,644.16,9.96,8.74" target="#b2">[3]</ref>, the goal is to extract useful passages for claim verification within the useful retrieved pages. In this section, we present our proposed methodology, experimental evaluation, selected runs for this subtask, and CLEF results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Approach</head><p>Deciding whether a passage within a useful page is useful or not is a classification problem. Therefore, our methodology is based on using different machine learning classifiers namely SVC, NB, and RF. We consider the following features for this subtask:</p><p>-BOW of the passage.</p><p>-MostRelSeg in the passage.</p><p>-ClmPsgSim.</p><p>-NQts in the passage.</p><p>-NEs in the passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>Parameters The three different classifiers mentioned in section. 5.1 were used in our experiments. We set the kernel for SVC to linear, and the number of estimators for the RF models to 100 in all the experiments. For the Gaussian NB, we did not tune any hyperparameters and we based our experiments on the default settings.</p><p>Baselines We compare our models against the majority baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation on Training</head><p>Since we have only 6 claims in the dataset provided for subtask C, which contains only 167 passages from 31 different pages, we considered LOO cross validation in our experiments. We used F 1 score as our evaluation metric. As shown in Fig. <ref type="figure" coords="12,154.75,572.43,3.87,8.74" target="#fig_5">5</ref>, SVC outperformed all other models with all groups of features. However, when the BOW features were excluded, the Gaussian NB achieved the best among all. We also observed that the two best performing models are the SVC model when the NEs features were excluded, and the SVC model when the NQts feature was excluded achieving an F 1 score of 0.444 and 0.43 respectively. We also noticed that the performance of the SVC model trained with all features improved compared to when trained with BOW features only, achieving an F 1 score of 0.427 as apposed to 0.387. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">CLEF Evaluation</head><p>Runs As shown in Fig. <ref type="figure" coords="13,247.32,402.94,3.87,8.74" target="#fig_5">5</ref>, SVC models outperformed other classifiers except when the BOW features were excluded, in which case the NB model achieved the best F 1 score. Therefore, we picked the 3 best SVC models and the best NB model to submit:</p><p>1. SVC trained with all features. 2. SVC trained with all features excluding the NQts feature.</p><p>3. SVC trained with all features excluding NEs features. 4. NB trained with all features excluding BOW features.</p><p>Results As shown in Table <ref type="table" coords="13,265.14,519.78,3.87,8.74" target="#tab_3">3</ref>, in the official CLEF evaluation, our best performing model in the test phase was the SVC model trained with all features excluding the NQts features, which achieved F 1 score value of 0.4. The low F 1 of our models can be attributed to the big difference in training and testing data including passages from 6 claims and 59 claims respectively. Our highest scoring model is ranked 3 rd out of the six runs submitted to the lab, and the maximum score achieved among all runs submitted for this subtask was 0.56.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Subtask D: Verifying Claims</head><p>The goal of this subtask is to identify whether the claim is "True" or "False". For a claim to be true, it should have supporting evidence that verifies its factuality. In this section, we present our approach, experimental setup, and training results for verifying the claims. Then, we discuss CLEF results for our submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Approach</head><p>Deciding the factuality of a claim is a binary classification problem. Therefore, we propose a supervised learning approach using different classifiers: GB, RF and Linear Discriminant Analysis (LDA).</p><p>For this subtask, we select the most significant features from webpages to classify the claim. Unlike previous tasks, we consider SntCnt features to find the polarity of the webpage. In addition, we consider the usefulness of the article by using the most relevant segment extracted as explained in Section 2 to represent the webpage. In our experiments, we consider the following features for our binary classifiers:</p><p>-Similarity Scores: out of all webpages associated with a claim, we only consider three different scores: maximum ClmTtlSim, ClmSnptSim, and MostRelSeg. -NRelPages.</p><p>-For every claim, we select the webpage with maximum MostRelSeg value and extract the following features from it: SrcPop and SntCnt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Setup</head><p>Parameters For GB and RF classifiers, we found that the default parameters are the best (based on preliminary experiments). For LDA classifier, we found that using 5 components for linear discrimination is most effective in terms of accuracy.</p><p>Baseline As a baseline for this subtask, we implemented Karadzhov et al. <ref type="bibr" coords="14,465.10,572.43,15.50,8.74" target="#b10">[11]</ref> method. They classify claims as "True" or "False" based on the top returned search results from several engines. They used an SVC classifier with RBF kernel in their experiments. The inputs to the classifier are word embeddings of the most relevant segment in the webpage, webpage snippet, and the claim. In addition to the word embeddings, the average and maximum similarity scores of the segments and snippets are included as features. We also adopt their method of segment extraction to compare with our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Evaluation on Training</head><p>We conducted multiple experiments to find which features combination will result in the best factuality classification. Due to the limitation in the size of training dataset, we used 8-fold cross validation on all our models for this subtask.</p><p>We first experimented with different values of the cosine similarity threshold (0.4, 0.5, 0.6, and 0.7) when extracting relevant segments. In our unreported preliminary experiments, we observed that the best performing models were the ones trained with features extracted using a similarity threshold of 0.6 presented in Fig. <ref type="figure" coords="15,166.29,236.22,3.87,8.74" target="#fig_6">6</ref>. We noticed that the GB model trained with all features outperformed all other models. We also observed that our models outperformed the baseline score most of the time except when the NRelPages were excluded from the features. Furthermore, we conclude that NRelPages and SntCnt features are useful in classification of a claim. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">CLEF Evaluation</head><p>Runs Based on our training results presented in section 6.3, we decided to use the models trained on all features to classify the claims factuality on testing data. We selected the best ensemble classifiers with two different similarity thresholds.</p><p>1. GB classifier, with similarity threshold 0.7. 2. GB classifier, with similarity threshold 0.4. 3. RF classifier, with similarity threshold 0.4. 4. RF classifier, with similarity threshold 0.6.</p><p>Results Table <ref type="table" coords="16,201.93,118.99,4.98,8.74" target="#tab_4">4</ref> shows our training results compared to the official CLEF testing results. Runs for subtask D were submitted over two cycles. In the first cycle, we classify the claims factuality using all webpages provided. In the second cycle, we classify the claims factuality using only useful webpages. We present the results for the second cycle in this section.</p><p>As presented in Table <ref type="table" coords="16,248.96,179.03,3.87,8.74" target="#tab_4">4</ref>, we notice that all models achieved very similar F 1 test scores. However, our GB model trained with all features has the highest training and testing scores, achieving F 1 score of 0.91 and 0.53 for training and testing respectively. Our highest scoring model is ranked 4 th out of the nine runs submitted to the lab, and the maximum score achieved among all runs submitted for this subtask was 0.62. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we present our approach for task 2 of CLEF-2019 CheckThat! Lab. For subtask A, we proposed pairwise learning-to-rank approach using different learning models to rank the retrieved pages based on their usefulness. Our best performing model trained using the basic and similarity features (excluding source popularity) achieved an NDCG@20 of 0.55, which is the highest score among 7 runs submitted for this subtask. For subtask B, we proposed a classification model incorporating source popularity feature along with named entities. Our best performing model achieved an F 1 score of 0.31, which is the highest score achieved among the 8 runs submitted for this subtask. For subtask C, we proposed a classification model considering BOW, named entities, and the number of quotes features extracted from passages. Our best performing model trained with all features (excluding the number of quotes) achieved an F 1 score of 0.4 and got 3 rd place. For subtask D, we proposed a classification model using sentiment features to find the polarity of the page, in addition to the number of potentially-relevant pages. Our best model trained with all features achieved an F 1 score of 0.53 and got 4 th place. That was our first attempt using a very small training data that was provided by the track organizers. With larger datasets, we plan to improve our classification models with more features including word embeddings, that are trained specifically for this task, and probably with deep learning models as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,140.99,358.40,339.60,8.77;5,151.70,370.39,328.89,8.74;5,151.70,382.34,328.89,8.74;5,151.70,394.30,328.89,8.74;5,151.70,406.25,137.46,8.74;5,140.99,417.93,339.60,8.77;5,151.70,429.92,328.89,8.74;5,151.70,441.87,88.73,8.74;5,140.99,453.56,339.60,8.77;5,151.70,465.54,107.69,8.74"><head>-</head><label></label><figDesc>Number of Quotes (NQts): For each page, we count the number of quotes in all relevant segments. This feature may be very useful to rank web pages and decide how useful they are for claim verification as it may indicate the credibility of the page by quoting sources. In our work, we considered only quotes with five words or more. -Number of URL links (NLinks): This feature represents the number of URL links in the retrieved page. It may indicate the credibility of the source by giving references. -Named Entities (NEs): Pages mentioning named entities may indicate the truthfulness of the page.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,134.77,618.23,345.83,7.89;7,134.77,629.21,190.53,7.86;7,134.77,432.13,345.83,171.33"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Subtask A: Performance of L2R models on training data with combinations of features (cosine similarity threshold set to 0.4).</figDesc><graphic coords="7,134.77,432.13,345.83,171.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,134.77,302.04,345.83,7.89;8,134.77,313.02,190.53,7.86;8,134.77,115.83,345.83,171.43"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Subtask A: Performance of L2R models on training data with combinations of features (cosine similarity threshold set to 0.7).</figDesc><graphic coords="8,134.77,115.83,345.83,171.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,134.77,617.04,345.83,7.89;10,134.77,628.03,190.53,7.86;10,134.77,442.39,345.82,159.88"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Subtask B: Performance of classifiers on training data with combinations of features (cosine similarity threshold set to 0.4).</figDesc><graphic coords="10,134.77,442.39,345.82,159.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,134.77,286.45,345.83,7.89;11,134.77,297.43,190.53,7.86;11,134.77,115.84,345.83,155.84"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Subtask B: Performance of classifiers on training data with combinations of features (cosine similarity threshold set to 0.7).</figDesc><graphic coords="11,134.77,115.84,345.83,155.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="13,134.77,343.21,345.82,7.89;13,134.77,354.20,44.62,7.86;13,134.77,115.84,345.83,212.60"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Subtask C: Performance of classifiers models on training data with combinations of features.</figDesc><graphic coords="13,134.77,115.84,345.83,212.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="15,134.77,486.13,345.83,7.89;15,134.77,497.12,233.33,7.86;15,134.77,314.07,345.83,157.29"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Subtask D: Performance of classification models on training data with combinations of features (cosine similarity threshold set to 0.6).</figDesc><graphic coords="15,134.77,314.07,345.83,157.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,134.77,115.91,294.51,174.76"><head>Table 1 .</head><label>1</label><figDesc>Subtask A: Performance of CLEF submitted runs.</figDesc><table coords="9,134.77,134.94,290.90,155.73"><row><cell>Features</cell><cell>Classifier</cell><cell>NDCG@20 on train</cell><cell>NDCG@20 on test</cell></row><row><cell>{Basic+Sim} -SrcPop</cell><cell>RF</cell><cell>0.704</cell><cell>0.47</cell></row><row><cell>{Basic+Sim+NQts} -SrcPop</cell><cell>NB</cell><cell>0.693</cell><cell>0.52</cell></row><row><cell>{Basic+Sim} -SrcPop</cell><cell>NB</cell><cell>0.692</cell><cell>0.55</cell></row><row><cell>{Basic+Sim</cell><cell></cell><cell></cell><cell></cell></row><row><cell>+NQts+NLinks}</cell><cell>NB</cell><cell>0.688</cell><cell>0.51</cell></row><row><cell>-SrcPop</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">4 Subtask B: Classifying Retrieved Pages</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,186.28,115.91,242.81,83.31"><head>Table 2 .</head><label>2</label><figDesc>Subtask B: Performance of CLEF submitted runs.</figDesc><table coords="12,198.22,134.97,218.91,64.25"><row><cell>Features</cell><cell>Classifier</cell><cell>F1 on Train</cell><cell>F1 on Test</cell></row><row><cell>Basic Features</cell><cell>GB</cell><cell>0.48</cell><cell>0.16</cell></row><row><cell>Basic Features + NEs</cell><cell>GB</cell><cell>0.52</cell><cell>0.22</cell></row><row><cell>Basic Features + NQts</cell><cell>GB</cell><cell>0.47</cell><cell>0.31</cell></row><row><cell>Basic Features + NQts</cell><cell>RF</cell><cell>0.45</cell><cell>0.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="14,185.06,115.91,245.23,83.31"><head>Table 3 .</head><label>3</label><figDesc>SubTask C. Performance of CLEF submitted runs.</figDesc><table coords="14,227.51,134.97,160.33,64.25"><row><cell cols="2">Features Classifier</cell><cell>F1 on train</cell><cell>F1 on test</cell></row><row><cell>All</cell><cell>SVC</cell><cell>0.423</cell><cell>0.39</cell></row><row><cell>All-NQts</cell><cell>SVC</cell><cell>0.43</cell><cell>0.4</cell></row><row><cell>All-NEs</cell><cell>SVC</cell><cell>0.44</cell><cell>0.19</cell></row><row><cell>All-BOW</cell><cell>NB</cell><cell>0.38</cell><cell>0.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="16,186.02,268.89,243.32,83.31"><head>Table 4 .</head><label>4</label><figDesc>Subtask D: Performance of CLEF submitted runs.</figDesc><table coords="16,225.14,287.95,165.07,64.25"><row><cell cols="2">Features Classifier</cell><cell>F1 on Train</cell><cell>F1 on Test</cell></row><row><cell>All</cell><cell>GB</cell><cell>0.91</cell><cell>0.53</cell></row><row><cell>All</cell><cell>GB</cell><cell>0.83</cell><cell>0.51</cell></row><row><cell>All</cell><cell>RF</cell><cell>0.80</cell><cell>0.53</cell></row><row><cell>All</cell><cell>RF</cell><cell>0.66</cell><cell>0.51</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="4,144.73,623.92,159.39,7.86"><p>https://pypi.org/project/newspaper3k/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,144.73,634.88,120.97,7.86"><p>https://pypi.org/project/bs4/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="4,144.73,645.84,123.74,7.86"><p>https://pypi.org/project/nltk/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="4,144.73,656.80,99.37,7.86"><p>https://www.alexa.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="5,144.73,634.88,133.74,7.86"><p>https://pypi.org/project/urllib3/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="5,144.73,645.84,338.39,7.86"><p>scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="5,144.73,656.80,164.41,7.86"><p>https://github.com/aboSamoor/polyglot</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="6,144.73,634.88,232.37,7.86"><p>https://polyglot.readthedocs.io/en/latest/Sentiment.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8" coords="6,144.73,645.84,295.82,7.86"><p>https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9" coords="6,144.73,656.80,168.13,7.86"><p>https://scikit-learn.org/stable/index.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="17,142.96,139.31,337.63,7.86;17,151.52,150.26,329.07,7.86;17,151.52,161.22,183.77,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="17,151.52,150.26,329.07,7.86;17,151.52,161.22,71.76,7.86">Overview of the CLEF-2019 CheckThat! Lab on Automatic Identification and Verification of Claims</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Check-Worthiness</note>
</biblStruct>

<biblStruct coords="17,142.96,171.53,337.63,7.86;17,151.52,182.46,329.07,7.89;17,151.52,193.44,162.64,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="17,418.59,171.53,62.00,7.86;17,151.52,182.48,220.89,7.86">Predicting Factuality of Reporting and Bias of News Media Sources</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Alexandrov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno>CoRR abs/1810.01765</idno>
		<ptr target="http://arxiv.org/abs/1810.01765" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,203.75,337.64,7.86;17,151.52,214.70,329.07,7.86;17,151.52,225.66,329.07,7.86;17,151.52,236.62,116.26,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="17,297.18,214.70,183.40,7.86;17,151.52,225.66,117.80,7.86">Checkthat! at clef 2019: Automatic identification and verification of claims</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,289.96,225.66,186.75,7.86">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="309" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,246.92,337.63,7.86;17,151.52,257.88,329.07,7.86;17,151.52,268.84,329.07,7.86;17,151.52,279.80,329.07,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="17,267.18,257.88,213.41,7.86;17,151.52,268.84,158.92,7.86">Overview of the CLEF-2019 CheckThat!: Automatic Identification and Verification of Claims</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,331.45,268.84,149.14,7.86;17,151.52,279.80,137.56,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2019-09">September 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,290.10,337.64,7.86;17,151.52,301.06,329.07,7.86;17,151.52,312.02,173.92,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="17,442.93,290.10,37.66,7.86;17,151.52,301.06,329.07,7.86;17,151.52,312.02,36.43,7.86">Overview of the CLEF-2019 CheckThat! Lab on Automatic Identification and Verification of Claims</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,195.34,312.02,130.09,7.86">Task 2: Evidence and Factuality</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,322.32,337.64,7.86;17,151.52,333.28,329.07,7.86;17,151.52,344.24,122.61,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="17,151.52,333.28,147.28,7.86">The quest to automate fact-checking</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Adair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">T</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tremayne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,320.30,333.28,160.30,7.86;17,151.52,344.24,93.94,7.86">Proceedings of the 2015 Computation+ Journalism Symposium</title>
		<meeting>the 2015 Computation+ Journalism Symposium</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,354.54,337.64,7.86;17,151.52,365.50,329.07,7.86;17,151.52,376.46,329.07,7.86;17,151.52,387.42,131.31,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,344.91,354.54,135.68,7.86;17,151.52,365.50,213.22,7.86">Toward automated fact-checking: Detecting check-worthy factual claims by claimbuster</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tremayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,385.64,365.50,94.95,7.86;17,151.52,376.46,329.07,7.86;17,151.52,387.42,11.14,7.86">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1803" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,397.72,337.63,7.86;17,151.52,408.68,329.07,7.86;17,151.52,419.64,286.22,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="17,291.46,397.72,189.13,7.86;17,151.52,408.68,59.20,7.86">Detecting check-worthy factual claims in presidential debates</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tremayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,231.98,408.68,248.61,7.86;17,151.52,419.64,164.81,7.86">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1835" to="1838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,429.94,337.63,7.86;17,151.52,440.90,329.07,7.86;17,151.52,451.84,329.07,7.89;17,151.52,462.82,47.10,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="17,351.39,440.90,129.20,7.86;17,151.52,451.86,109.44,7.86">Claimbuster: The first-ever endto-end fact-checking system</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Caraballo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gawsane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Nayak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,268.17,451.86,153.98,7.86">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1945" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,473.12,337.98,7.86;17,151.52,484.08,329.07,7.86;17,151.52,495.04,97.80,7.86" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Jaradat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gencheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07587</idno>
		<title level="m" coord="17,453.46,473.12,27.13,7.86;17,151.52,484.08,257.04,7.86">Claimrank: Detecting check-worthy claims in arabic and english</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,142.61,505.34,337.98,7.86;17,151.52,516.30,329.07,7.86;17,151.52,527.26,25.60,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Koychev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00341</idno>
		<title level="m" coord="17,460.24,505.34,20.35,7.86;17,151.52,516.30,190.51,7.86">Fully automated fact checking using external sources</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,142.61,537.56,337.97,7.86;17,151.52,548.52,329.07,7.86;17,151.52,559.48,217.12,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="17,407.41,537.56,73.17,7.86;17,151.52,548.52,232.55,7.86">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,404.48,548.52,76.12,7.86;17,151.52,559.48,123.83,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="3111" to="3119" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,569.78,337.98,7.86;17,151.52,580.74,329.07,7.86;17,151.52,591.70,329.07,7.86;17,151.52,602.66,329.07,7.86;17,151.52,613.62,186.33,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="17,384.29,580.74,96.30,7.86;17,151.52,591.70,325.12,7.86">Overview of the CLEF-2018 CheckThat! lab on automatic identification and verification of political claims</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kyuchukov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,166.33,602.66,314.26,7.86;17,151.52,613.62,62.52,7.86">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,623.92,337.98,7.86;17,151.52,634.88,329.07,7.86;17,151.52,645.84,329.07,7.86;17,151.52,656.80,181.10,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="17,361.30,623.92,119.29,7.86;17,151.52,634.88,162.43,7.86">Credeye: A credibility lens for analyzing and explaining misinformation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Strötgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,335.28,634.88,145.32,7.86;17,151.52,645.84,141.28,7.86">Companion of the The Web Conference 2018 on The Web Conference</title>
		<imprint>
			<publisher>International World Wide Web Conferences Steering Committee</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="155" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,119.67,337.97,7.86;18,151.52,130.63,329.07,7.86;18,151.52,141.59,329.07,7.86;18,151.52,152.55,70.14,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="18,381.87,119.67,98.72,7.86;18,151.52,130.63,246.25,7.86">Truth of varying shades: Analyzing language in fake news and political fact-checking</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,420.96,130.63,59.63,7.86;18,151.52,141.59,308.69,7.86">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2931" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,163.51,337.98,7.86;18,151.52,174.47,329.07,7.86;18,151.52,185.43,161.53,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="18,280.81,163.51,195.84,7.86">Csi: A hybrid deep model for fake news detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ruchansky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,166.12,174.47,314.47,7.86;18,151.52,185.43,49.34,7.86">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,196.39,337.97,7.86;18,151.52,207.32,314.47,7.89" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="18,321.99,196.39,158.60,7.86;18,151.52,207.34,110.99,7.86">Aravec: A set of arabic word embedding models for use in arabic nlp</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Soliman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Eissa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>El-Beltagy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,269.43,207.34,111.02,7.86">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="256" to="265" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,218.30,337.98,7.86;18,151.52,229.26,329.07,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="18,402.42,218.30,78.18,7.86;18,151.52,229.26,166.20,7.86">Fever: a large-scale dataset for fact extraction and verification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05355</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="18,142.62,240.22,337.98,7.86;18,151.52,251.18,329.07,7.86;18,151.52,262.14,329.07,7.86;18,151.52,273.10,50.81,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="18,354.95,240.22,125.64,7.86;18,151.52,251.18,134.35,7.86">Five shades of untruth: Finergrained classification of fake news</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,329.21,251.18,151.39,7.86;18,151.52,262.14,268.47,7.86">IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="593" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,284.06,337.97,7.86;18,151.52,295.02,201.55,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="18,212.49,284.06,92.67,7.86">Liar, liar pants on fire</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00648</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,315.51,284.06,165.08,7.86;18,151.52,295.02,35.49,7.86">A new benchmark dataset for fake news detection</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="18,142.62,305.98,337.97,7.86;18,151.52,316.93,329.07,7.86;18,151.52,327.89,329.07,7.86;18,151.52,338.85,102.92,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="18,303.56,305.98,177.03,7.86;18,151.52,316.93,146.00,7.86">Re-ranking Web Search Results for Better Fact-Checking: A Preliminary Study</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yasser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,319.59,316.93,161.00,7.86;18,151.52,327.89,260.97,7.86">Proceedings of 27th ACM International Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>27th ACM International Conference on Information and Knowledge Management (CIKM)<address><addrLine>Turin, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1783" to="1786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,349.81,337.97,7.86;18,151.52,360.77,329.07,7.86;18,151.52,371.73,329.07,7.86;18,151.52,382.69,25.60,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="18,331.11,349.81,149.48,7.86;18,151.52,360.77,184.55,7.86">Claimverif: a real-time claim verification system using the web and fact databases</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,358.19,360.77,122.40,7.86;18,151.52,371.73,236.82,7.86">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2555" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
