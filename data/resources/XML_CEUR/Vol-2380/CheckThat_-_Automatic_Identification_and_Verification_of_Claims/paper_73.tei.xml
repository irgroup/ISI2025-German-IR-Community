<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.89,115.90,311.57,12.90;1,237.15,133.83,141.05,12.90">Automatic Verification of Political Claims based on morphological features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,225.07,171.88,67.00,8.64"><forename type="first">Ibtissam</forename><surname>Touahri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Sciences</orgName>
								<orgName type="institution">University Mohamed First</orgName>
								<address>
									<settlement>Oujda</settlement>
									<country key="MA">Morocco</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,311.44,171.88,78.84,8.64"><forename type="first">Azzeddine</forename><surname>Mazroui</surname></persName>
							<email>azze.mazroui@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Sciences</orgName>
								<orgName type="institution">University Mohamed First</orgName>
								<address>
									<settlement>Oujda</settlement>
									<country key="MA">Morocco</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.89,115.90,311.57,12.90;1,237.15,133.83,141.05,12.90">Automatic Verification of Political Claims based on morphological features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FD97EDC017EC415EE644331B15FF6DCA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fact-checking</term>
					<term>relevance</term>
					<term>stance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fact-checking aims to identify the veracity of the enormous amount of data generated on the web. It enhances and automates human effort in detecting harmful misinformation. In this paper, we present a description of the system with which we participated in three subtasks A, B, and D of Task2 at CLEF 2019 Check That lab. For the subtask A, we propose an unsupervised approach that extracts information from the important parts of a document to rank web pages based on their usefulness, and subsequently verify the target claim. Hence, we assign weights for important document fields in order to compute field scores that are employed besides field lengths to determine the overall score of a given web page. For the subtask B, we adopt a supervised approach that generates a model using an SVM classifier from training data, then we apply it on test data to determine the usefulness degree of a web page. Subtask D aims to tag claim factuality as true or false. Thus, we use lemma, the morphology form that represents many inflected terms, in order to find the claim terms in target web pages. We proceed to detect stance, then we determine whether analyzed expressions confirm or contradict a given claim.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The old fashioned rhythm to check claims veracity can't be kept by Fact checkers as it is less effective in term of detection time due to its slowness in comparison to the need of momentary detection of fake news in a large amount of information on the web. Thus the need for an automatic system for Fact-checking is indispensable. CLEF-2019 CheckThat! Lab <ref type="bibr" coords="1,252.93,544.29,11.62,8.64" target="#b6">[7]</ref> aims at determining evidence and factuality for given claims by proposing two tasks. The first task <ref type="bibr" coords="1,317.78,556.24,11.62,8.64">[2]</ref> runs in English and it determines the prioritized claims for Fact-checking within a piece of text. Whereas, the second task is in Arabic and it identifies evidence and factuality. The second task <ref type="bibr" coords="1,407.98,580.15,11.62,8.64">[8]</ref> is divided into four subtasks where subtask A aims to rank web pages based on their usefulness for verifying a given claim. Subtask B classifies web pages based on the degree of their usefulness to verify a given claim, thus each page is tagged as either very useful, useful, not useful or not relevant. Subtask C extracts passages from useful pages in order to find the ones that are useful for claim verification. Whereas, subtask D targets identifying claim factuality as true or false. CheckThat! Lab released both training and test data for the aforementioned subtasks. In this paper, we present our approaches to deal with Fact-checking problem related to the second task. For subtask A, we rank web pages based on the overall score computed by addressing weights of document fields namely, meta-data, title, and body text. Subtask B is a classification task, thus we extract features from web pages using documents related information which is URL data and page content as well as the extraction of named entities. In order to meet the need of subtask D, which is identifying whether a claim is true or false, we proceed to detect stance by looking for terms from web pages that match claim terms, then we determine whether an expression confirms or contradicts the claim. The remainder of this paper is organized as follows. We start by situating our approaches in comparison to related work. Then, we describe our system by giving details related to each subtask, and after we give the obtained official evaluation results. We conclude by giving some perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In the literature, many works pointed at generating methods and approaches that help for Fact-checking. Jaradat et al. in <ref type="bibr" coords="2,274.66,384.03,11.62,8.64" target="#b8">[9]</ref> presented an online claim rank system that supports Arabic besides English language. The system was trained on political debates. Moreover, it works with other kinds of data. Baly et al. in <ref type="bibr" coords="2,371.21,407.94,11.62,8.64" target="#b2">[3]</ref> described a unified corpus for stance detection and Fact-checking. Their system noticed the main challenge when using only stance detection to predict facts as their system illustrates that for true claims the major part of documents supports them, however, a major part supports also false claims. Thus, the insufficiency of stance detection to predict claim factuality. At CLEF-2018 CheckThat! Lab, Authors of <ref type="bibr" coords="2,298.49,467.72,16.60,8.64" target="#b9">[10]</ref> proposed a learning to rank approach that gathers NLP features, namely sentiment analysis and named entities. Afterward, they extracted relevant segments from a set of web pages in order to predict claim veracity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task 2: Evidence and Factuality</head><p>This task aims to identify useful web pages and passages which assist in the human Fact-checking task by following the three first subtasks, the last subtask aims to find supporting and contradicting information within given web pages, thus, identifying claim factuality either true or false.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System approach</head><p>Our system approach addresses both web pages and Arabic language specificities, thus it proceeds to unify input data in term of format and content.</p><p>Information extraction In order to rank web pages based on their usefulness, we proceed as follows. We assume that the most important parts in a document are metadata, title, and body text, with an advantage to the first two fields over the third, thus using Jsoup that is a java library that parses HTML documents, we extract the three contents from each web page. It is worth noting that even word and text documents were converted into HTML format in order to unify the system input.</p><p>Preprocessing The aim of preprocessing is giving a standard representation that addresses Arabic language specificities to both claim and web page content, thus claim, meta-data, title and body text are preprocessed by removing Latin characters, punctuation marks and digits in order to keep Arabic characters only, Moreover, we remove stop words that don't carry any important information as they have the same function wherever they are used. The used stop words list contains 157 terms.</p><p>SubstaskA We assign weight for document fields, thus we give three points to both of meta-data and title content and one point to body content as we consider that meta-data have an advantage over the body as they are considered a communication way between SEO and search engines, the title too is distinguished as it appears to the user before the body.</p><formula xml:id="formula_0" coords="3,276.23,355.97,204.36,9.75">Sp = F W × T Occ<label>(1)</label></formula><p>We compute the score points (Sp) for each field using formula <ref type="bibr" coords="3,395.97,373.42,10.58,8.64" target="#b0">(1)</ref>, where, the score is the product of the given field weight (F W ) multiplied by the occurrence of the field terms that belong to the claim.</p><formula xml:id="formula_1" coords="3,259.77,415.26,220.82,23.47">Os = M Sp + T Sp + BT Sp M L + T L + BT L (2)</formula><p>We compute the overall score (Os) of a given web page by using formula (2), where, M Sp , T Sp , and BT Sp are score points of meta-data, title, and body text respectively and M L , T L , and BT L are their lengths. After computing the overall score for each web page, we rank web pages by sorting obtained values in descending order.</p><p>SubstaskB We consider subtask B a classification task that classifies web pages based on their usefulness degree for claim verification. Thus, we proceed to extract two categories of features, where each one contains four features.</p><p>Category1 uses document fields as features, thus we have four features which are normalized score points of URL, meta-data, title, and body text. It is worth noting that the URL takes the same weight as meta-data and title.</p><formula xml:id="formula_2" coords="3,284.60,600.68,196.00,23.33">NSp = F Sp F L<label>(3)</label></formula><p>The normalized score points of each field are calculated using formula (3). Where, F Sp value determines the score points of a specific field either URL, meta-data, title or body text divided by the field length (F L ).</p><p>Category2 gives named entities count using Farasa that is an Arabic text processing tool that offers many functionalities, among which named entity tag <ref type="bibr" coords="4,416.47,131.27,10.58,8.64" target="#b5">[6]</ref>. For a given term Farasa gives either LOC tag if the entity is a location, PERS if the entity identifies a person or ORG when the entity is an organization unless it gives none, in other words, none tag is given when the entity doesn't present any of the aforementioned tags. Thus the sum of tagged and non-tagged entities count presents document length. The system gathers features of category1 and category2 to give a standard representation to all web pages. Hence, in summary, we have eight features, namely, normalized score points of URL, meta-data, title, and body text besides named entity features, that are LOC, PERS, ORG tags count and the count of terms that were not tagged by Farassa. The system starts by training a model using the extracted features from training web pages and SVM classifier, then it labels test data based on the generated model. SVM classifier has been chosen as it had a considerable improvement over Naïve Bayes (NB) and K-Nearest Neighbors (KNN).</p><p>SubstaskC The subtask C points at defining the useful passages within a web page, thus, we proceed to extract passages that constitute the web page, then, we compute cosine similarity between the claim and each passage, afterward, we sort passages in descending order by cosine similarity value after removing their redundancy, then we consider the first five passages in the sorted list as useful.</p><p>SubstaskD As subtask D aims to identify either a claim is true or false, we proceed to determine web pages terms that match claim terms, thus we use Alkhalil analyzer <ref type="bibr" coords="4,134.77,406.24,11.62,8.64" target="#b4">[5]</ref> that doesn't address context, as a result, it gives all potential lemmas of a given term. We use it over all claim terms and for each analyzed term we obtain a list of corresponding lemmas. However, for web pages content, we use Alkhalil lemmatizer <ref type="bibr" coords="4,134.77,442.10,11.62,8.64" target="#b3">[4]</ref> that handles the context, thus it gives one equivalent lemma for each analyzed term within a piece of text. We detect all terms that match between claim and web page content by comparing lemmas. Alkhalil analyzer was used for claim text instead of Alkhalil lemmatizer due to claim short length, thus the use of Alkhalil lemmatizer may not give the correct corresponding lemmas. Supporting and contradicting expressions identification is performed by using a list of negation words. The negation words list contains 36 terms manually constructed by browsing different sources. For instance, we have the claim the first sentence in Table <ref type="table" coords="4,160.91,545.52,4.98,8.64" target="#tab_0">1</ref> matches the claim, however, the second contradicts it as contradicts and contradicts . In other words, sentence one supports the claims as none of its terms are negated, whereas, the second sentence contradicts the claim as two of its terms among the ones that match claim terms are negated. Thus, wherever our system finds contradicting sentences, it tags the claim as false, otherwise, it gives it true tag. Subtask D runs two times at different evaluation cycles, thus we describe our approach for both of them. We apply the aforedescribed process to predict claim factuality for the two cycles, where:</p><p>Cycle1 uses unlabelled web pages to estimate claim factuality. Our approach for this cycle is gathering all useful passages from unlabelled pages related to a specific claim, then generating their corresponding in context lemmas. In other words, we use five passages from each unlabelled web page related to the claim. For this cycle, we sent one run.</p><p>Cycle2 uses useful web pages to estimate claim factuality. Our approach is the same as for cycle1 except that we use useful passages from useful pages related to the claim only instead of using all pages. For this cycle, we sent three runs, run1 that uses the given test passages from useful web pages that are related to 32 claims besides useful passages selected by our approach that are related to 27 claims to detect factuality of the given 59 claims, the useful passages identified by our approach were added to cover the 27 remainder claims, run2 that uses the given test passages from useful web pages that are related to 32 claims only, and run3 that gathers the five selected passages from each useful web page related to a given claim and it tests factuality of 57 claims.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation results</head><p>SubtaskA In order to rank web pages, we follow an unsupervised method that is based on weighted document fields. Table <ref type="table" coords="5,284.80,481.22,4.98,8.64" target="#tab_1">2</ref> shows the results in the official evaluation, the results reached for this task are between 0.19 and 0.5, where our system achieved an AP value of 0.48. In the official evaluation, our run is ranked 3 rd (out of 7 runs from 3 teams). However, all runs were outperformed by the baseline. Hence, our system needs further development in order to rank pages by usefulness. SubtaskB In this task, we point at classifying test data based on a model generated using training data and SVM classifier. Table <ref type="table" coords="5,315.91,656.44,4.98,8.64" target="#tab_2">3</ref> shows the obtained results in the official evaluation, where our system achieved an accuracy value of 0.79 when testing two classes per claim, 0.78 when testing two classes over all claims and 0.58 when testing four classes over all claims. For the 4-way classification (very useful vs. useful vs. not useful vs. not relevant) our system outperformed the baseline with a slight improvement. However, it was not the case for the 2-way classification (useful vs. not useful), and this may be due to training our system on a limited set of useful pages. SubtaskD In this task, we aim to detect claim factuality as true or false. This task runs at two different cycles the one that uses all web pages to detect claim factuality and the one that uses only useful web pages. Table <ref type="table" coords="6,314.09,339.59,4.98,8.64" target="#tab_3">4</ref> shows the official evaluation results for subtask D. Our team participates with one run for cycle1 and three runs for cycle2. For cycle1 our system reached 0.53 of accuracy, for cycle2, it reached 0.46 for both run1 and run2 and 0.6 for run3. However, run2 and run3 can't be compared to the overall results as they predict only claims that have related useful passages and useful web pages. In the official evaluation, our system outperformed the baseline in both cycles. For cycle1 our run was the only evaluated run. However, in cycle2 our run reached the minimum accuracy. Thus, the use of a supervised approach in order to improve our system is essential. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we present a description of our participation in CLEF-2019 CheckThat! lab. We start by describing the followed approach to rank web pages based on their usefulness, thus we use information related to document content by giving weights to document parts according to their importance. Then, we classify web pages by usefulness level. The classification task is performed by constructing a model using training data and SVM classifier, then applying the model on the test data. Afterward, we extract useful passages from the given pages using cosine similarity, then, we select the most important passages that belong to the first positions in the sorted list in descending order according to cosine similarity value. The extraction of useful passages helps to meet the need of subtask C and subtask D at the same time. However, our participation points only at subtask D. For this task, we detect stance by defining terms that match between claim and page content, then by using a list of negation words, we detect supporting and contradicting parts to define claim factuality. Our system reached promising results, however, further work with various approaches and several experiments is needed. Thus we intend to improve the supervised approach of subtask B by extracting more significant features as well as using supervised approaches for both subtask A and subtask D instead of unsupervised ones, besides using not only lemmas but synonyms and antonyms as well to meet the need of subtask D.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,152.12,115.83,311.12,58.16"><head>Table 1 .</head><label>1</label><figDesc>Stance and relation between claim and sentences</figDesc><table coords="5,152.12,135.33,311.12,38.66"><row><cell>N Sentence</cell><cell>W1 W2 W3 W4 W5 W6 W7</cell></row><row><cell>1</cell><cell>S S S S S S S</cell></row><row><cell>2</cell><cell>S S S C S C S</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,190.11,560.31,194.88,38.63"><head>Table 2 .</head><label>2</label><figDesc>Subtask A official results</figDesc><table coords="5,190.11,579.81,194.88,19.13"><row><cell>Our system</cell><cell>Min</cell><cell>Median</cell><cell>Max</cell></row><row><cell>0.48</cell><cell>0.19</cell><cell>0.45</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,192.47,210.16,218.51,60.55"><head>Table 3 .</head><label>3</label><figDesc>Subtask B official results</figDesc><table coords="6,192.47,229.67,218.51,41.05"><row><cell>Accuracy</cell><cell cols="2">Our system Min</cell><cell cols="2">Median Max</cell></row><row><cell cols="2">2 classes per claim results 0.79</cell><cell>0.5</cell><cell cols="2">0.785 0.79</cell></row><row><cell cols="2">2 classes over all claims 0.78</cell><cell>0.49</cell><cell>0.78</cell><cell>0.78</cell></row><row><cell cols="2">4 classes over all claims 0.58</cell><cell>0.24</cell><cell>0.58</cell><cell>0.60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,160.37,466.31,254.36,71.91"><head>Table 4 .</head><label>4</label><figDesc>Subtask D official results</figDesc><table coords="6,160.37,485.81,254.36,52.41"><row><cell>Accuracy</cell><cell>Our system</cell><cell>Min</cell><cell>Median</cell><cell>Max</cell></row><row><cell>Cycle1</cell><cell>0.53</cell><cell>0.53</cell><cell>0.53</cell><cell>0.53</cell></row><row><cell>Cycle2</cell><cell>Run1 0.46</cell><cell>0.46</cell><cell>0.54</cell><cell>0.63</cell></row><row><cell></cell><cell>Run2 0.46</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Run3 0.6</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.61,318.22,337.98,7.77;7,150.95,329.18,227.16,7.77" xml:id="b0">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="7,150.96,318.22,275.22,7.77">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="7,432.25,318.22,48.34,7.77;7,150.95,329.18,61.41,7.77">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS.org</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,340.14,337.98,7.77;7,150.95,351.09,329.64,7.77;7,150.95,362.05,102.16,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,445.59,340.14,35.00,7.77;7,150.95,351.09,325.69,7.77">Overview of the CLEF-2019 CheckThat! Lab on Automatic Identification and Verification of Claims</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,150.95,362.05,16.71,7.77">Task</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Check-Worthiness</note>
</biblStruct>

<biblStruct coords="7,142.61,373.01,337.98,7.77;7,150.95,383.97,315.97,7.77" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08012</idno>
		<title level="m" coord="7,416.78,373.01,63.81,7.77;7,150.95,383.97,165.50,7.77">Integrating stance detection and fact checking in a unified corpus</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,142.61,394.93,337.98,7.77;7,150.95,405.89,329.64,7.77;7,150.95,416.85,207.80,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,263.21,394.93,217.38,7.77;7,150.95,405.89,74.34,7.77">Approche hybride pour le développement d&apos;un lemmatiseur pour la langue arabe</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Boudchiche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mazroui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,245.17,405.89,235.42,7.77;7,150.95,416.85,75.65,7.77">13th African Conference on Research in Computer Science and Applied Mathematics</title>
		<meeting><address><addrLine>Hammamet, Tunisia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">147</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,427.81,337.98,7.77;7,150.95,438.77,329.64,7.77;7,150.95,449.38,213.42,8.12" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,431.65,427.81,48.94,7.77;7,150.95,438.77,198.34,7.77">Alkhalil morpho sys 2: A robust arabic morpho-syntactic analyzer</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Boudchiche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mazroui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">O A O</forename><surname>Bebah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lakhouaja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Boudlal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,356.36,438.77,124.24,7.77;7,150.95,449.72,129.74,7.77">Journal of King Saud University-Computer and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,460.68,337.98,7.77;7,150.95,471.64,329.64,7.77;7,150.95,482.60,199.26,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,199.10,460.68,277.43,7.77">Named entity recognition using cross-lingual resources: Arabic as an example</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Darwish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,162.74,471.64,317.86,7.77">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s" coord="7,193.13,482.60,42.92,7.77">Long Papers</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1558" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,493.56,337.98,7.77;7,150.95,504.52,329.64,7.77;7,150.95,515.48,329.64,7.77;7,150.95,526.44,168.80,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,203.12,504.52,277.47,7.77;7,150.95,515.48,57.08,7.77">Overview of the CLEF-2019 CheckThat!: Automatic Identification and Verification of Claims</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,225.94,515.48,251.23,7.77">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2019-09">September 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,537.40,337.98,7.77;7,150.95,548.35,329.64,7.77;7,150.95,559.31,94.49,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,420.55,537.40,60.04,7.77;7,150.95,548.35,297.95,7.77">Overview of the CLEF-2019 CheckThat! Lab on Automatic Identification and Verification of Claims</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,454.88,548.35,25.71,7.77;7,150.95,559.31,85.31,7.77">Task 2: Evidence and Factuality</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,570.27,337.99,7.77;7,150.95,581.23,300.48,7.77" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Jaradat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gencheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07587</idno>
		<title level="m" coord="7,402.96,570.27,77.63,7.77;7,150.95,581.23,150.24,7.77">Claimrank: Detecting check-worthy claims in arabic and english</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,142.24,592.19,338.35,7.77;7,150.95,603.15,146.43,7.77" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,278.85,592.19,201.74,7.77;7,150.95,603.15,81.13,7.77">bigir at clef 2018: Detection and verifi cation of checkworthy political claims</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yasser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<editor>Cappellato et al.</editor>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
