<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.63,115.96,292.09,12.62;1,229.16,133.89,157.04,12.62">TheEarthIsFlat&apos;s Submission to CLEF&apos;19 CheckThat! Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.50,171.56,55.08,8.74"><forename type="first">Luca</forename><surname>Favano</surname></persName>
							<email>luca.favano@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Milano</orgName>
								<address>
									<postCode>20133</postCode>
									<region>MI</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.13,171.56,72.70,8.74"><forename type="first">Mark</forename><forename type="middle">J</forename><surname>Carman</surname></persName>
							<email>mark.carman@polimi.it3pierluca.lanzi@polimi.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Milano</orgName>
								<address>
									<postCode>20133</postCode>
									<region>MI</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,354.76,171.56,69.62,8.74"><forename type="first">Pier</forename><forename type="middle">Luca</forename><surname>Lanzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Milano</orgName>
								<address>
									<postCode>20133</postCode>
									<region>MI</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.63,115.96,292.09,12.62;1,229.16,133.89,157.04,12.62">TheEarthIsFlat&apos;s Submission to CLEF&apos;19 CheckThat! Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D3A34C8AB5D206F3CD71948E61F0C60C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Automated Fact Checking</term>
					<term>Claim Detection</term>
					<term>Text Classification</term>
					<term>Natural Language Processing</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report details our investigations in applying state-ofthe-art pre-trained Deep Learning models to the problems of Automated Claim Detection and Fact Checking, as part of the CLEF'19 Lab: Check-That!: Automatic Identification and Verification of Claims. The report provides an overview of the experiments performed on these tasks, which continue to be extremely challenging for current technology. The research focuses mainly on the use of pre-trained deep neural text embeddings that through transfer learning can allow for improved classification performance on small and unbalanced text datasets. We also investigate the effectiveness of external data sources for improving prediction accuracy on the claim detection and fact checking tasks. Our team submitted runs for every task/subtask of the challenge. The results appeared satisfactory for task 1 and promising but less satisfactory for task 2. A detailed explanation of the steps performed to obtain the submitted results is provided, including comparison tables between our submissions and other techniques investigated.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this report we describe our efforts to use state-of-the-art pre-trained deep neural text embeddings for tackling the different subtasks of the CheckThat! challenge <ref type="bibr" coords="1,179.18,545.31,9.96,8.74" target="#b5">[6]</ref>. In order to achieve good results, a great number of experiments were performed. In the following sections we provide descriptions and results for the most interesting of these experiments in the hope of inspiring future research in this area. In Section 2 we will explain all the steps that brought to our final submission for Task 1, from the choice of the architecture to the fine-tuning of the chosen setup. In Section 3 we explain the text pair classification approach that we applied for the subtasks of Task 2. 2 Task 1 -Check-Worthiness</p><p>The first task <ref type="bibr" coords="2,200.96,282.90,10.52,8.74" target="#b0">[1]</ref> for the CheckThat challenge involved classifying individual statements within political debates as check-worthy (i.e. constituting a claim that is worth fact checking) or not check-worthy. The training data consisted of 19 debates, while the test data contained seven. An example section from one of the debates<ref type="foot" coords="2,196.25,329.14,3.97,6.12" target="#foot_0">1</ref> is shown in Table <ref type="table" coords="2,283.96,330.72,3.87,8.74" target="#tab_0">1</ref>. Note that each debate is a dialog with the speaker information available for each utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminary Experiments</head><p>Recent years have seen a proliferation of pretrained-embeddings for language modeling and text classification tasks, starting from basic word embeddings such as word2vec <ref type="bibr" coords="2,192.06,420.12,15.50,8.74" target="#b11">[12]</ref> and GloVe <ref type="bibr" coords="2,262.60,420.12,14.61,8.74" target="#b13">[14]</ref>, and moving to sub-word and character-level embeddings like FastText <ref type="bibr" coords="2,252.45,432.07,14.61,8.74" target="#b10">[11]</ref>. More recently pre-trained deep networks have become available, which make use of BiLSTM <ref type="bibr" coords="2,338.48,444.03,10.52,8.74" target="#b7">[8]</ref> or self-attention layers <ref type="bibr" coords="2,452.92,444.03,15.50,8.74" target="#b15">[16]</ref> to build deep text processing models like ELMo <ref type="bibr" coords="2,335.31,455.98,15.50,8.74" target="#b14">[15]</ref> and BERT <ref type="bibr" coords="2,404.69,455.98,9.96,8.74" target="#b4">[5]</ref>. These models offer improved transfer learning ability, taking advantage of massive corpora of unlabeled text data from the Web to learn the structure of language, and then leveraging that knowledge to identify better features and improve prediction performance on subsequent supervised learning tasks.</p><p>In this work, we make use of a number of state-of-the-art pre-trained models for text-processing, namely: BERT <ref type="bibr" coords="2,291.48,528.09,9.96,8.74" target="#b4">[5]</ref>, ELMo <ref type="bibr" coords="2,339.39,528.09,14.61,8.74" target="#b14">[15]</ref>, Infersent <ref type="bibr" coords="2,403.16,528.09,9.96,8.74" target="#b3">[4]</ref>, FastText <ref type="bibr" coords="2,462.33,528.09,14.61,8.74" target="#b10">[11]</ref>, and the Universal Sentence Encoder (USE) <ref type="bibr" coords="2,326.66,540.04,9.96,8.74" target="#b2">[3]</ref>.</p><p>When competing in the challenge we first ran a preliminary experiment over validation data comparing the performance of these toolkits in order to decide which one to use for our submission. We repeated this comparison after the annotated test set for the challenge was published, so that we could provide results on the held-out test data. Those test results for Task 1 are reported in Table <ref type="table" coords="2,162.67,612.14,3.87,8.74" target="#tab_1">2</ref>. Note that default (hyper)parameters were used for each system, with the exception of the number of training steps (or epochs), which was set based on validation performance.  <ref type="bibr" coords="3,260.98,174.79,14.34,7.86" target="#b14">[15]</ref> 0.0587 0.3466 0.0729 Infersent <ref type="bibr" coords="3,271.05,185.75,9.73,7.86" target="#b3">[4]</ref> 0.1057 0.2503 0.1034 FastText <ref type="bibr" coords="3,271.41,196.70,14.34,7.86" target="#b10">[11]</ref> 0.1445 0.5303 0.1545 USE <ref type="bibr" coords="3,254.20,207.66,9.72,7.86" target="#b2">[3]</ref> 0.1871 0.3679 0.2071 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Modifying the Training Data</head><p>Results of the preliminary experiment indicated that the Universal Sentence Encoder (USE) was a model that could provide reasonable performance for the claim detection task. We then investigated a number of different settings for how to train a USE-based classifier and how to modify the training dataset in order to improve prediction performance. The modifications to the training dataset considered included appending speaker information or previous utterances to the input and also the use of external training data.</p><p>For the classification task, the network architecture used was to append a fully connected Feed-Forward (FF) Neural Network with two hidden layers to the output from the Universal Sentence Encoder. The training (hyper)parameters for the network were set to the values shown in Table <ref type="table" coords="3,373.84,523.72,3.87,8.74" target="#tab_2">3</ref>. Note that the weights of the USE encoding were not fine-tuned<ref type="foot" coords="3,316.08,534.10,3.97,6.12" target="#foot_1">2</ref> during training of the classifier due to the small quantities of labelled training data available.</p><p>The following experimental setups were evaluated. We report results for each setting on the test data (not available at the time of run submission) in Table <ref type="table" coords="3,472.85,571.66,3.87,8.74" target="#tab_3">4</ref>.</p><p>1. Training on Task 1 dataset only, using each individual sentence only as the input text. 2. Same as setup 1, but concatenating the speaker information to the sentence text.</p><p>3. Same as setup 1, but using as input the concatenation of the two previous sentences with the current sentence. 4. Same as setup 1, but applying basic text pre-processing, in which contractions in the text are expanded and the text is stripped of accented characters, special characters or extra white spaces, and then converted to lower-case. 5. Same as setup 1, but activating the Trainable parameter of the USE-module to fine-tune the weights of the sentence encoder. 6. Supplementing the Task 1 dataset with additional positive examples extracted from the LIAR dataset <ref type="bibr" coords="4,295.91,217.11,14.61,8.74" target="#b17">[17]</ref>. The LIAR dataset contains a set of political sentences from various sources that have been fact-checked by Poli-tiFact <ref type="foot" coords="4,177.30,239.45,3.97,6.12" target="#foot_2">3</ref> and assigned a truth label. It is safe to assume that all the sentences included in the LIAR dataset were once considered worthy of fact checking.</p><p>Based on this assumption all the sentences in the dataset make for a valid set of additional positive instances for the fact checking task. Moreover there is a strong motivation for adding positive examples to the Task 1 training set, since the training data is highly skewed toward the negative class with only a small percentage of positive training instances. An obvious limitation of this idea is that by adding only positive instances which come from a different source from the training data (and therefore may not share the same vocabulary distribution), we may simply end up training the classifier to distinguish between instances from the two datasets (the Task 1 political debate instances and the LIAR fact-checked claims dataset). 7. Training first on the LIAR dataset <ref type="bibr" coords="4,307.79,385.31,14.61,8.74" target="#b17">[17]</ref>, but keeping the 0 and 1 labels the same as they were in the original LIAR dataset (where 1 indicates a false statement and 0 indicates a true statement), and then train again on Task 1 dataset. 8. Training on a much larger Headlines+Wikipdia dataset consisting of one million headlines from news articles sourced from an Australian news source <ref type="foot" coords="4,476.12,444.33,3.97,6.12" target="#foot_3">4</ref>and one million randomly chosen sentences from the content of Wikipedia articles <ref type="foot" coords="4,173.90,468.24,3.97,6.12" target="#foot_4">5</ref> . The assumption here is that random chosen sentences from Wikipedia are generally not making claims nor worth fact-checking, while headlines from news articles are more likely to state a claim and are interesting and therefore likely worth fact checking. After first training on the 2 million sentence corpus, we then further train (fine-tune) the model on the Task 1 dataset.</p><p>We note from Table <ref type="table" coords="4,241.23,552.00,4.98,8.74" target="#tab_3">4</ref> that none of the tested modifications to the training data resulted in improvements over the basic USE-based classifier. Of all the techniques, the most interesting appears to be that of adding millions of positive and negative examples from the Headlines+Wikipedia dataset, which caused relatively small degradation in Average Precision (MAP) while providing a marked increase in Reciprocal Rank (RR). We leave to future work an investigation of why that was the case and whether modifications to that dataset and its use could result in positive gains in MAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Comparing Different Encoder &amp; Discriminator Architectures</head><p>The Universal Sentence Encoder (USE) offers two different pre-trained models that differ in their internal architecture. The standard USE module is trained with a Deep Averaging Network (DAN) <ref type="bibr" coords="5,320.09,460.81,14.61,8.74" target="#b9">[10]</ref>, while the larger version of the module is trained with a Transformer <ref type="bibr" coords="5,301.86,472.76,15.50,8.74" target="#b15">[16]</ref> encoder. <ref type="foot" coords="5,356.96,471.19,3.97,6.12" target="#foot_5">6</ref>Performance for the two versions of the USE encoder on the test data are shown in Table <ref type="table" coords="5,202.05,496.89,3.87,8.74" target="#tab_4">5</ref>. We note a much higher MAP value for the larger, transformerbased model.</p><p>In order to provide a discriminative model able to predict check-worthiness labels, two different network architectures have been layered on top of the USE architecture. The relative performance of the two models is shown in Table <ref type="table" coords="5,472.84,544.93,3.87,8.74" target="#tab_5">6</ref>, and their descriptions are as follows:</p><p>1. The architecture used to produce most of the results in this report is a Feed Forward Deep Neural Network (FF-DNN) with two hidden layers, obtained by using the TensorFlow DNNClassifier component. 2. A second architecture consists of a dense layer with a ReLU <ref type="bibr" coords="5,418.74,613.76,15.50,8.74" target="#b12">[13]</ref> activation function, followed by a softmax layer allows to categorize the results. This architecture was implemented in Keras<ref type="foot" coords="6,325.12,117.42,3.97,6.12" target="#foot_6">7</ref> applying a lambda layer to wrap the USE output.</p><p>Performance for the TensorFlow implementation (on the validation data) outperformed the Keras ReLU architecture, so we continued with that model in the other experiments. In order to decide how many steps to train each model for, we examined performance of the models against the number of training steps on individual debates from the training data as shown for the Large USE model in Table <ref type="table" coords="6,472.84,328.22,3.87,8.74" target="#tab_6">7</ref>.</p><p>For that particular model we decided to train the model for only 600 steps based on the average results across the training debates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Submitted Runs for Task 1</head><p>For the submitted runs we made use of both the standard and large USE architectures compared in Table <ref type="table" coords="6,272.51,565.49,3.87,8.74" target="#tab_4">5</ref>. The standard USE model has been used for the first two runs: Primary and Contrastive 1, while the large USE model was used for Contrastive 2. Table <ref type="table" coords="6,262.07,589.40,4.98,8.74" target="#tab_7">8</ref> contains the results for the submitted runs <ref type="foot" coords="6,453.35,587.83,3.97,6.12" target="#foot_7">8</ref> . The difference between the first two runs, which both use the standard USE model, is that for the first we used the Adagrad optimiser and a feed-forward network with two hidden layers of size 512/128 while for the second we employed the Adam optimiser with two hidden layers of size 100 and 8. We note that our last run (Contrastive 2) obtained the best MAP score over all runs submitted by any team for Task 1. The USE standard model had been chosen as the primary run because it had provided better peak results during training, while the large model provided more stable results. Note the results on the training data shown in Table <ref type="table" coords="7,472.84,306.49,3.87,8.74" target="#tab_8">9</ref>, where the standard model outperformed the large model on two of the three debates used for training.</p><p>Independently from the model used, we see that there is large variation in the performance across the debates in the training set. Dealing with such large variation effectively is something that ought be addressed in future work. We note that on the test data, where the average MAP value is around 0.18, the average precision across the individual debates varies from 0.05 (for the 2015-12-19 debate) to 0.5 (for the 2018-01-31 debate).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task 2 -Evidence and Factuality</head><p>The second task of the challenge <ref type="bibr" coords="7,282.17,459.39,10.52,8.74" target="#b8">[9]</ref> contains multiple subtasks which together form a path that aims at automating the fact-checking process. Given a claim and a set of the web pages, the subtasks consist of:</p><p>1. Ranking the web-pages based on how useful they are to assess the veracity of the claim. 2. Labelling the web-pages based on their usefulness into four categories: very useful, useful, not useful, not relevant. 3. Labelling individual passages within those pages that are useful for determining the veracity of the claim. 4. Labelling the claims as true or false given the discovered information.</p><p>Unlike Task 1 for which all the data was written in English, for Task 2 all content was written in Arabic. We generally worked directly with the Arabic text but also experimented with translating the content into English as discussed below.</p><p>Every subtask has been tackled using a similar setup: after processing the data to obtain a dataset that consists of two strings of text and a label to predict, we feed this pairs into a pre-trained BERT model <ref type="bibr" coords="8,395.50,211.10,10.52,8.74" target="#b4">[5]</ref> that we train to classify the relationships between the two texts. In some cases, we have also investigated adding external data that could be useful, given that the datasets for the subtasks were extremely small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task 2A and 2B -Determining Relevant Web-pages</head><p>For the first two subtasks we used an almost identical approach: We extracted the claim text and associated with each web page text using the Beautiful Soup parser <ref type="foot" coords="8,161.44,316.23,3.97,6.12" target="#foot_8">9</ref> to remove HTML markup. The training sets then consisted of 395 labelled text pairs (claims, corresponding webpages and relationship labels).</p><p>A set of experiments on the dataset were performed using a small portion of the training data as a validation set. The accuracy results in Table <ref type="table" coords="8,447.19,353.67,9.96,8.74" target="#tab_9">10</ref> have been averaged over three runs to account for the variation due to very small training/validation sets. The techniques investigated were the following:</p><p>1. BERT model is trained on the Task 2-AB dataset. 2. BERT model is trained on external data using a dataset that was previously used for stance detection for the FakeNewsChallenge <ref type="bibr" coords="8,384.80,419.26,10.52,8.74" target="#b6">[7]</ref> challenge. 3. BERT model has been first trained on the FakeNewsChallenge dataset then on the Task 2-AB dataset. 4. The Task 2-AB dataset has been translated to English before feeding it to the model as in 1.</p><p>Given that training BERT over large sections of text has very large memory requirements, the standard pre-trained BERT model was used instead of the biggest one available <ref type="foot" coords="8,225.60,507.18,7.94,6.12" target="#foot_9">10</ref> . This limited the text sections to be no more than 100 to 150 words. BERT automatically reduces the information in longer context windows such that the this limit is enforced, implying that some information is necessarily lost from the text of longer webpages. We observe in Table <ref type="table" coords="8,239.81,556.57,9.96,8.74" target="#tab_9">10</ref> improved performance using the FakeNewsChallenge dataset and translating the Arabic text to English, but caution that the results are subject to significant variation due to small sample sizes.</p><p>The ranking for subtask 2A was computed using the predicted confidence value with which the pages were being classified as useful. Analyzing the Challenge's "Results Summary", it can be noted that while the system learnt to classify not relevant and not useful pairs of texts, it was not able to learn to classify useful and very useful pairs. Thus in subtask 2A the test results we obtained were quite poor, while for subtask 2B (see Table <ref type="table" coords="9,374.30,431.28,9.22,8.74" target="#tab_10">12</ref>) we indeed achieved a high Accuracy value (0.79) for two-class classification but a zero Precision value, indicating that the classifier is predicting only the negative class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task 2C -Finding Useful Passages</head><p>For this subtask the dataset consisted of each claim text paired with a paragraph that was linked to it. Again the set over which the results could be measured was too small to compare the different parameter settings for the model. In this case the scores obtained without using any external data were quite promising and Table <ref type="table" coords="9,162.16,549.68,9.96,8.74" target="#tab_0">11</ref> shows the performance versus the number of epochs used for training.</p><p>The results for Task 2C in Table <ref type="table" coords="9,290.97,561.63,9.96,8.74" target="#tab_10">12</ref> show scores that are much lower than the ones obtain in Table <ref type="table" coords="9,227.48,573.59,8.49,8.74" target="#tab_0">11</ref>, nonetheless this submission got the best scores among the teams over Precision (0.41), Recall (0.94) and F1 (0.56), while obtaining a slightly lower result for Accuracy (0.51).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task 2D -Assessing Claim Veracity</head><p>Subtask D has been tackled thinking about how external data might be leveraged to learn a model for assessing claim factuality. Two different datasets have been while the second was again the FakeNewsChallenge <ref type="bibr" coords="10,359.70,312.33,10.52,8.74" target="#b6">[7]</ref> stance detection dataset. The two datasets have been used to judge the relationship between the claims and the text that composed the web pages. While in the first case the entailment or contradiction confidence score is used, in the second case the confidence over the labels agree or disagree (how much a text agrees or disagrees with a given headline) was used instead.</p><p>The results obtained have been evaluated only over a subset of 31 claims and in this case the best Accuracy value obtained is 0.52.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this report we have described our investigations in applying state-of-the-art pre-trained deep learning models to the problems of automated claim detection and fact checking, as part of the CLEF'19 Lab: CheckThat!: Automatic Identification and Verification of Claims.</p><p>For Task A we investigated the use of pre-trained deep neural embeddings for the problem of check-worthiness prediction. Over a set of embeddings, we found the Universal Sentence Encoder (USE) <ref type="bibr" coords="10,330.66,524.53,10.52,8.74" target="#b2">[3]</ref> to provide the best performance with little out-of-the-box tuning required. We investigated different techniques for pre-processing the political debate data and also the use of external datasets for augmenting the small and highly unbalanced training dataset, but did not observe performance improvements in either case. Thus our runs for the challenge were built by simply training a Feed-Forward neural network on top of the USE encoding(s), without further modification of the training data.</p><p>The results obtained for the first task were quite inspiring. With a more judicial choice of validation set it may have been possible to determine that the best choice of model was indeed that used for our third run, which obtained the highest MAP value over all teams for the task. Further work should be aimed at levelling the differences in performance over the different debates.</p><p>The various subtasks of Task 2 involved predicting the usefulness of webpages and passages for determining the veracity of a particular claim as well as predicting the veracity of the claim itself. For this task we made use of the BERT <ref type="bibr" coords="11,165.99,154.86,10.52,8.74" target="#b4">[5]</ref> model, which can be trained on text pairs to directly predict a relationship label. We found this approach to the task promising, but hampered by insufficient training data and large memory requirements for the BERT model. Furthermore, we found that external datasets (from the FakeNewsChallenge <ref type="bibr" coords="11,466.20,190.72,10.79,8.74" target="#b6">[7]</ref>) may be useful for improving performance on these tasks, despite the fact that they are in a different language (English) from the training/test data for the task (Arabic).</p><p>In conclusion, the preliminary results show that pre-trained deep learning models can be effective for a variety of tasks. The use of small or unbalanced datasets is a renown problem for deep learning, yet the transfer learning techniques that we used to face the challenge proved quite successful and may offer an opportunity in overcoming deep learning limitations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,134.77,122.36,345.83,104.12"><head>Table 1 :</head><label>1</label><figDesc>Example sequence of utterances and their corresponding binary labels (checkworthy or not check-worthy) from the CheckThat! challenge Task 1 dataset</figDesc><table coords="2,156.58,158.83,72.17,7.86"><row><cell>Speaker Sentence</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,198.48,122.36,218.40,60.29"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on Task 1 data only</figDesc><table coords="3,232.83,147.87,147.77,34.78"><row><cell>Toolkit</cell><cell>MAP</cell><cell>RR</cell><cell>R-P</cell></row><row><cell>BERT [5]</cell><cell cols="3">0.0824 0.3112 0.0776</cell></row><row><cell>ELMo</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,152.88,238.31,309.59,115.08"><head>Table 3 :</head><label>3</label><figDesc>(Hyper)Parameter settings used for the Universal Sentence Encoder</figDesc><table coords="3,243.85,263.81,124.58,89.58"><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Total Steps</cell><cell>600</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row><row><cell cols="2">Width of Hidden Layers 100/8</cell></row><row><cell>Activation Function</cell><cell>ReLU</cell></row><row><cell>Dropout</cell><cell>0.1</cell></row><row><cell>Learning Rate</cell><cell>0.005</cell></row><row><cell>Trainable Parameter</cell><cell>False</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,148.88,122.36,314.52,126.04"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison between different setups</figDesc><table coords="5,148.88,147.87,314.52,100.54"><row><cell cols="2">Setup Modification to Training Data</cell><cell>MAP</cell><cell>RR</cell><cell>R-P</cell></row><row><cell>1</cell><cell>-</cell><cell cols="3">0.1821 0.4187 0.1937</cell></row><row><cell>2</cell><cell>Include speaker name</cell><cell cols="3">0.1687 0.3206 0.2054</cell></row><row><cell>3</cell><cell>Include 2 previous sentences</cell><cell cols="3">0.1255 0.3123 0.1735</cell></row><row><cell>4</cell><cell>Pre-process text (case-folding, etc.)</cell><cell cols="3">0.1676 0.3466 0.1976</cell></row><row><cell>5</cell><cell>Fine-tune encoder weights (Trainable=true)</cell><cell cols="3">0.1294 0.3729 0.1495</cell></row><row><cell>6</cell><cell cols="4">Add external data from LIAR [17] as positive 0.1262 0.1698 0.1487</cell></row><row><cell>7</cell><cell cols="4">Add external data from LIAR [17] as true/false 0.1288 0.3884 0.1333</cell></row><row><cell>8</cell><cell cols="4">Add external data from Headlines+Wikipedia 0.1694 0.4441 0.1793</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,134.77,271.38,345.83,71.25"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison between the two different Universal Sentence Encoder (USE) models available</figDesc><table coords="5,187.51,307.84,240.34,34.78"><row><cell>Model</cell><cell>MAP</cell><cell>RR</cell><cell>R-P</cell></row><row><cell cols="4">Standard: Deep Averaging Network 0.1597 0.1953 0.2052</cell></row><row><cell>Large: Transformer Network</cell><cell cols="3">0.1821 0.4187 0.1937</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,180.98,213.69,253.40,60.29"><head>Table 6 :</head><label>6</label><figDesc>Performance comparisons using different architectures</figDesc><table coords="6,234.18,239.19,147.01,34.78"><row><cell cols="2">Architecture MAP</cell><cell>RR</cell><cell>R-P</cell></row><row><cell>FF-DNN</cell><cell cols="3">0.1821 0.4187 0.1937</cell></row><row><cell>ReLU [13]</cell><cell cols="3">0.1703 0.2238 0.1988</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,134.77,390.46,345.83,93.16"><head>Table 7 :</head><label>7</label><figDesc>Average precision scores on individual debates from the validation data, computed while training the USE Large FF-DNN model for a given number of steps</figDesc><table coords="6,184.31,426.92,243.66,56.70"><row><cell cols="5">Steps Trump-Pelosi Trump-World Oval-Office Average</cell></row><row><cell>600</cell><cell>0.60</cell><cell>0.36</cell><cell>0.22</cell><cell>0.393</cell></row><row><cell>1200</cell><cell>0.53</cell><cell>0.41</cell><cell>0.22</cell><cell>0.387</cell></row><row><cell>1500</cell><cell>0.57</cell><cell>0.26</cell><cell>0.26</cell><cell>0.363</cell></row><row><cell>3000</cell><cell>0.48</cell><cell>0.28</cell><cell>0.20</cell><cell>0.320</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,136.16,179.18,347.94,71.25"><head>Table 8 :</head><label>8</label><figDesc>Scores for TheEarthIsFlat's official submissions to the challenge.</figDesc><table coords="7,136.16,204.68,347.94,45.74"><row><cell cols="2">Submission Model</cell><cell>Parameters</cell><cell>MAP</cell><cell>RR</cell><cell>R-P</cell></row><row><cell>Primary</cell><cell cols="5">Standard FF-DNN(512/128) Adagrad 1500 steps 0.1597 0.1953 0.2052</cell></row><row><cell cols="3">Contrastive1 Standard FF-DNN(100/8) Adam 1500 steps</cell><cell cols="3">0.1453 0.3158 0.1101</cell></row><row><cell cols="2">Contrastive2 Large FF-DNN(100/8)</cell><cell>Adam 600 steps</cell><cell cols="3">0.1821 0.4187 0.1937</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="8,154.80,122.36,305.76,60.29"><head>Table 9 :</head><label>9</label><figDesc>Scores evaluated on a subset of debates from the validation set</figDesc><table coords="8,154.80,147.87,305.76,34.78"><row><cell>Model</cell><cell cols="3">Trump-Pelosi Trump-World Oval-Office</cell></row><row><cell>Standard: Deep Averaging Network</cell><cell>0.580</cell><cell>0.561</cell><cell>0.294</cell></row><row><cell>Large: Transformer Network</cell><cell>0.511</cell><cell>0.495</cell><cell>0.376</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="9,134.77,122.36,345.82,257.67"><head>Table 10 :</head><label>10</label><figDesc>Accuracy on the validation data of predicting the usefulness of a webpage for subtasks A and B. (The same prediction model was used for both tasks.)</figDesc><table coords="9,134.77,158.83,345.82,221.21"><row><cell cols="3">Setup Treatment</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average Accuracy</cell></row><row><cell>1</cell><cell cols="2">Use Task 2-AB dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.481</cell></row><row><cell>2</cell><cell cols="3">Use FakeNewsChallenge data</cell><cell></cell><cell></cell><cell></cell><cell>0.502</cell></row><row><cell>3</cell><cell cols="6">Use FakeNewsChallenge + Task 2-AB data</cell><cell>0.575</cell></row><row><cell>4</cell><cell cols="4">Translate Task 2-AB data to English</cell><cell></cell><cell></cell><cell>0.527</cell></row><row><cell cols="8">Table 11: Accuracy results on validation data for subtask C while varying the number</cell></row><row><cell cols="2">of training epochs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Epochs Accuracy</cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell></row><row><cell>3 5 6 7</cell><cell></cell><cell>0.762 0.904 0.870 0.881</cell><cell>Accuracy</cell><cell>0.8</cell><cell></cell><cell></cell></row><row><cell>8</cell><cell></cell><cell>0.904</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">10</cell><cell>0.857</cell><cell></cell><cell>0.7</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Epochs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="10,134.77,122.36,345.83,186.75"><head>Table 12 :</head><label>12</label><figDesc>Scores for TheEarthIsFlat's official submissions for subtasks 2B and 2C.</figDesc><table coords="10,137.17,147.87,337.94,122.46"><row><cell>Subtask External Data</cell><cell cols="5">Evaluation Method Precision Recall F1 Accuracy</cell></row><row><cell>2B (run1) No</cell><cell>2 Classes per claims</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.79</cell></row><row><cell></cell><cell>2 Classes over claims</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.78</cell></row><row><cell></cell><cell cols="2">4 Classes over claims 0.28</cell><cell cols="2">0.36 0.31</cell><cell>0.59</cell></row><row><cell cols="2">2B (run2) FakeNewsChallenge 2 Classes per claims</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.79</cell></row><row><cell></cell><cell>2 Classes over claims</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.78</cell></row><row><cell></cell><cell cols="2">4 Classes over claims 0.27</cell><cell cols="2">0.35 0.3</cell><cell>0.6</cell></row><row><cell>2C (run1) No</cell><cell>2 Classes per claims</cell><cell>0.35</cell><cell cols="2">0.72 0.42</cell><cell>0.52</cell></row><row><cell></cell><cell cols="2">2 Classes over claims 0.41</cell><cell cols="3">0.87 0.55 0.53</cell></row><row><cell>2C (run2) No</cell><cell>2 Classes per claims</cell><cell>0.4</cell><cell cols="2">0.87 0.49</cell><cell>0.51</cell></row><row><cell></cell><cell>2 Classes over claims</cell><cell>0.4</cell><cell cols="2">0.94 0.56</cell><cell>0.51</cell></row></table><note coords="10,134.77,300.37,345.83,8.74"><p><p>considered: The first was the Stanford Natural Language Inference Corpus,</p><ref type="bibr" coords="10,470.08,300.37,10.52,8.74" target="#b1">[2]</ref> </p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,656.80,263.12,7.86"><p>Sample sentences extracted from the file "20160209-msnbc-dem".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,645.84,335.87,7.86;3,144.73,656.80,28.16,7.86"><p>Investigations with the parameter Trainable set to true resulted in degraded performance.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.73,634.88,110.38,7.86"><p>https://www.politifact.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.73,645.84,206.72,7.86"><p>https://www.kaggle.com/therohk/million-headlines</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,144.73,656.80,234.93,7.86"><p>https://www.kaggle.com/mikeortman/wikipedia-sentences</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,144.73,645.84,335.87,7.86;5,144.73,656.80,277.06,7.86"><p>A third version of the encoder, called "lite", is specifically designed for systems with limited computational resources, and thus was not investigated here.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="6,144.73,645.84,62.84,7.86"><p>https://keras.io</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="6,144.73,656.80,190.95,7.86"><p>Note that some values are the same as Table5.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="8,144.73,634.88,209.93,7.86"><p>https://www.crummy.com/software/BeautifulSoup/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="8,144.73,645.84,335.86,7.86;8,144.73,656.80,101.84,7.86"><p>We conjecture that the use of the bigger BERT model would have increased performance on these subtasks.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,346.53,337.63,7.86;11,151.52,357.49,329.07,7.86;11,151.52,368.45,183.77,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="11,151.52,357.49,329.07,7.86;11,151.52,368.45,71.76,7.86">Overview of the CLEF-2019 CheckThat! Lab on Automatic Identification and Verification of Claims</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Check-Worthiness</note>
</biblStruct>

<biblStruct coords="11,142.96,379.84,337.63,7.86;11,151.52,390.80,329.07,7.86;11,151.52,401.76,329.07,7.86;11,151.52,412.71,135.39,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,375.87,379.84,104.72,7.86;11,151.52,390.80,158.60,7.86">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,333.15,390.80,147.45,7.86;11,151.52,401.76,329.07,7.86;11,151.52,412.71,106.72,7.86">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,424.10,337.63,7.86;11,151.52,435.06,329.07,7.86;11,151.52,445.99,241.88,7.89" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,151.52,446.02,106.81,7.86">Universal sentence encoder</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno>CoRR abs/1803.11175</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,457.40,337.64,7.86;11,151.52,468.36,329.07,7.86;11,151.52,479.32,329.07,7.86;11,151.52,490.28,292.90,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,402.10,457.40,78.50,7.86;11,151.52,468.36,309.41,7.86">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,479.32,329.07,7.86;11,151.52,490.28,40.95,7.86">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2017-09">September 2017</date>
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,501.67,337.63,7.86;11,151.52,512.63,329.07,7.86;11,151.52,523.58,25.60,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,346.99,501.67,133.60,7.86;11,151.52,512.63,189.89,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.96,534.97,337.63,7.86;11,151.52,545.93,329.07,7.86;11,151.52,556.89,329.07,7.86;11,151.52,567.85,329.07,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,267.18,545.93,213.41,7.86;11,151.52,556.89,158.92,7.86">Overview of the CLEF-2019 CheckThat!: Automatic Identification and Verification of Claims</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,331.45,556.89,149.14,7.86;11,151.52,567.85,137.56,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2019-09">September 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,579.23,337.64,7.86;11,151.52,590.19,303.39,7.86" xml:id="b6">
	<monogr>
		<ptr target="http://www.fakenewschallenge.org" />
		<title level="m" coord="11,151.53,579.23,325.09,7.86">FakeNewsChallenge stance detection dataset</title>
		<imprint>
			<date type="published" when="2016-12-01">2016. December 1st 2016</date>
		</imprint>
	</monogr>
	<note>FakeNewsChallenge organizers</note>
</biblStruct>

<biblStruct coords="11,142.96,601.58,337.64,7.86;11,151.52,612.51,329.07,7.89;11,151.52,623.49,25.60,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,271.10,601.58,209.49,7.86;11,151.52,612.54,180.30,7.86">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,339.61,612.54,68.27,7.86">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,634.88,337.64,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,173.92,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,442.93,634.88,37.66,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,36.43,7.86">Overview of the CLEF-2019 CheckThat! Lab on Automatic Identification and Verification of Claims</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,195.34,656.80,130.09,7.86">Task 2: Evidence and Factuality</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,119.67,337.98,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,329.07,7.86;12,151.52,152.55,329.07,7.86;12,151.52,163.51,329.07,7.86;12,151.52,174.47,215.36,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,392.18,119.67,88.41,7.86;12,151.52,130.63,227.85,7.86">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,403.11,130.63,77.49,7.86;12,151.52,141.59,329.07,7.86;12,151.52,152.55,329.07,7.86;12,151.52,163.51,219.22,7.86">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">July 26-31, 2015. 2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="12,142.62,185.43,337.98,7.86;12,151.52,196.39,215.47,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<title level="m" coord="12,360.75,185.43,119.84,7.86;12,151.52,196.39,49.79,7.86">Bag of tricks for efficient text classification</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,207.34,337.97,7.86;12,151.52,218.30,329.07,7.86;12,151.52,229.26,217.12,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,407.41,207.34,73.17,7.86;12,151.52,218.30,232.55,7.86">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,404.48,218.30,76.12,7.86;12,151.52,229.26,123.83,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="3111" to="3119" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,240.22,337.98,7.86;12,151.52,251.18,329.07,7.86;12,151.52,262.14,126.06,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,250.88,240.22,229.71,7.86;12,151.52,251.18,23.08,7.86">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,196.79,251.18,283.80,7.86;12,151.52,262.14,41.58,7.86">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,273.10,337.97,7.86;12,151.52,284.06,130.21,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,329.82,273.10,150.77,7.86;12,151.52,284.06,35.30,7.86">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,219.41,284.06,33.66,7.86">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,295.02,337.98,7.86;12,151.52,305.98,328.14,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,195.45,305.98,166.59,7.86">Deep contextualized word representations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,383.14,305.98,67.85,7.86">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,316.93,337.97,7.86;12,151.52,327.89,329.07,7.86;12,151.52,338.85,329.07,7.86;12,151.52,349.81,233.46,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,231.84,327.89,94.43,7.86">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="12,430.47,338.85,50.12,7.86;12,151.52,349.81,157.25,7.86">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,387.52,349.81,93.07,7.86;12,151.52,360.77,283.33,7.86" xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Inc</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,371.73,337.97,7.86;12,151.52,382.66,170.31,7.89" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="12,212.45,371.73,268.14,7.86;12,151.52,382.69,35.49,7.86">liar, liar pants on fire&quot;: A new benchmark dataset for fake news detection</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR abs/1705.00648</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
