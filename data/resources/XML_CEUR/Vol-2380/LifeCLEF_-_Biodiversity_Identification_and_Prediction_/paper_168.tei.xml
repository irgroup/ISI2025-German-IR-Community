<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,185.82,115.96,243.70,12.62;1,218.90,133.89,177.56,12.62;1,149.14,151.82,317.08,12.62">Plant Identification on Amazonian and Guiana Shield Flora: NEUON submission to LifeCLEF 2019 Plant</title>
				<funder>
					<orgName type="full">Malaysia</orgName>
				</funder>
				<funder>
					<orgName type="full">NEUON AI SDN. BHD.</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,135.82,189.49,57.80,8.74"><forename type="first">Sophia</forename><surname>Chulif</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">NEUON AI</orgName>
								<address>
									<postCode>94300</postCode>
									<settlement>Sarawak</settlement>
									<country>Malaysia http, www</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.14,189.49,41.09,8.74"><forename type="first">Jing</forename><surname>Kiat</surname></persName>
							<email>kiatjing@neuon.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">NEUON AI</orgName>
								<address>
									<postCode>94300</postCode>
									<settlement>Sarawak</settlement>
									<country>Malaysia http, www</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,245.55,189.49,68.64,8.74"><roleName>Teck</roleName><forename type="first">Wei</forename><surname>Heng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">NEUON AI</orgName>
								<address>
									<postCode>94300</postCode>
									<settlement>Sarawak</settlement>
									<country>Malaysia http, www</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.50,189.49,103.36,8.74"><roleName>MD</roleName><forename type="first">Abdullah</forename><forename type="middle">Al</forename><surname>Chan</surname></persName>
							<email>abdullah@neuon.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">NEUON AI</orgName>
								<address>
									<postCode>94300</postCode>
									<settlement>Sarawak</settlement>
									<country>Malaysia http, www</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,424.19,189.49,30.83,8.74;1,265.68,201.45,52.44,8.74"><forename type="first">Yang</forename><forename type="middle">Loong</forename><surname>Monnaf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">NEUON AI</orgName>
								<address>
									<postCode>94300</postCode>
									<settlement>Sarawak</settlement>
									<country>Malaysia http, www</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.45,201.45,28.23,8.74"><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">NEUON AI</orgName>
								<address>
									<postCode>94300</postCode>
									<settlement>Sarawak</settlement>
									<country>Malaysia http, www</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,185.82,115.96,243.70,12.62;1,218.90,133.89,177.56,12.62;1,149.14,151.82,317.08,12.62">Plant Identification on Amazonian and Guiana Shield Flora: NEUON submission to LifeCLEF 2019 Plant</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">72421E5A73F2A503A9523E7913FC71CA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Plant identification</term>
					<term>computer vision</term>
					<term>convolutional neural networks</term>
					<term>data cleaning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper will look into the use of fine-tuned Inception-v4 and Inception-ResNet-v2 models to automate the classification of 10,000 plant species. Prior to training the networks, the training dataset was pre-processed to remove the noisy data. The team submitted three runs which achieved comparable performances to human experts on the test dataset comprising 745 observations for all the evaluation metrics. For the trained systems to generalise better, the systems were trained for multi-task classification and is able to classify plant images based on their species, with support of their genus and family labels. In particular, an ensemble of Inception-v4 and Inception-ResNet-v2 networks achieved a Top-1 accuracy of 0.316 and 0.246 for the test set identified by experts and the whole test set respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The plant identification challenge of the Conference and Labs of the Evaluation Forum (CLEF) is an annual challenge focusing on the automation of plant identification. In recent years, automated identification of plants has become a high interest for botanical specialists and computer experts alike <ref type="bibr" coords="1,405.17,531.45,9.96,8.74" target="#b2">[3]</ref>. Transitioning from the conventional computer vision through feature descriptor methods <ref type="bibr" coords="1,462.33,543.40,14.61,8.74" target="#b13">[14]</ref>, deep learning based methods have been able to significantly improve accuracy of the automation of plant identification <ref type="bibr" coords="1,298.53,567.31,11.25,8.74" target="#b6">[7,</ref><ref type="bibr" coords="1,309.78,567.31,7.50,8.74" target="#b7">8,</ref><ref type="bibr" coords="1,317.29,567.31,11.25,8.74" target="#b10">11]</ref>. This application is essential in the utilisation, management and conservation of flora of any kind. When the plant identification challenge first commenced in 2011, the main focus was to be able to identify over 70 different tree species given 3996 leaf images as training data <ref type="bibr" coords="1,134.77,615.13,9.96,8.74" target="#b4">[5]</ref>. The total number of species and training images in the challenge has then notably increased each year. With the training dataset initially covering only leaf images, it has now expanded to different plant organs and multiple views of the plant. Since 2017, the total number of plant species has reached 10,000 along with the presence of noisy images (images that are unrelated to the plant of interest) in the training dataset -this posed a great challenge to the participants in evaluating the extend of a noisy dataset competing with a trusted dataset <ref type="bibr" coords="2,134.77,190.72,9.96,8.74" target="#b1">[2]</ref>. Furthermore, the plants in the training dataset shared a low intra-class and high inter-class similarity. They may belong to the same class but look clearly different from one another <ref type="bibr" coords="2,254.21,214.64,9.96,8.74" target="#b0">[1]</ref>. Besides the variance, some of the distinct plant organs and views may not be captured in the given training images, resulting in the lack of training data thus difficulty in predicting the species. This depicts the realistic challenge of the real-world applications in plant identification.</p><p>For PlantCLEF 2019, the dataset provided was focused on the Guiana shield and the Amazon rainforest which is known to be the largest collection of living plants and species in the world <ref type="bibr" coords="2,268.67,286.72,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="2,279.18,286.72,7.01,8.74" target="#b3">4]</ref>. The task was to predict 10,000 plant species and the participants were provided with 448,071 images of training data. Along with a test set containing 2,974 images and 745 observations, the main evaluation method of the competition was the Top 1 accuracy of the submitted predictions based on the 745 observations. The primary challenge of PlantCLEF 2019 was the decreased average number of images per species in the training dataset. Many species contained only a few images and some even contained only one image. Moreover, the training dataset consisted of noisy data that constituted from non-plant images and duplicate images that may come with incorrect labels.</p><p>This paper presents the preparation made prior to the training of the system. The system was fine-tuned from pretrained Inception-v4 and Inception-ResNet-v2 models to automate the classification of the given 10,000 plant species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Preparation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Analysis</head><p>The dataset provided consisted of 10,000 species which mainly focused on the Guiana shield and the Amazon rainforest. The reported training dataset size is 448,071 (based on the number of rows in PlantCLEF2019MasterTraining.csv) however, the downloaded training dataset size was 433,810. Therefore, the actual downloaded dataset included the training dataset of 433,810 images which consisted of 10,000 classes (species), and a test dataset of size 2,974 from 745 different observations.</p><p>As mentioned in the challenges data collection description, among these 10,000 species many contain only a few images and some of them contain only one image. Moreover, it was observed that there were many variations in the training dataset that could affect the performance of the plant identification.</p><p>Other than real-world plant images i.e. fruit, branch, trunk, stem, root, flower, leaf etc., the training dataset initially contained many images that do not look like real actual plants i.e. sketches, paintings, illustrations, plant herbariums, and small regions of the interested plants. In addition, the dataset also contained non-plant images i.e. animals, logos, book covers, graphs, table, medicine bottles, humans, chemical-bond diagrams, presentation slides, maps etc. Besides this, the dataset consisted of images with duplicate names in different classes (folders). Furthermore, the dataset consisted of duplicate images with different names within the same folder as well as in different folders. Having different labels for the same images could cause confusion to the machine.</p><p>These characteristics observed could affect the performance of the plant identification therefore the approach was to pre-processed the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Cleaning</head><p>From the analysis in subsection 2.1, the dataset was then pre-processed (cleaned) to allow better execution of plant identification. First, the images with duplicate names were removed as those images are actually the same. The dataset given consisted of 433,810 images and 10,000 classes of plants. After eliminating the duplicate names, the dataset was reduced to 279,183 images and 8,468 classes.</p><p>Then, the duplicate images (without having the same name) were further removed. After removing them, 263,987 images were left with 8,419 classes. Finally, the non-plant images were eliminated, resulting in a total of 250,646 images and 8,263 of classes for training <ref type="foot" coords="3,253.43,372.56,3.97,6.12" target="#foot_0">1</ref> .</p><p>In the approach of eliminating images with duplicate names in different folders, since there is no method to decide which class they actually belong to (no experts to verify), all images with the same name were removed.</p><p>On the other hand, in order to remove duplicate images within the same folder as well as different folders, inception-v4's feature extractor layer (Mixed 7d) was used to compare images so that those with 0.99 cosine similarities in features can be eliminated. If the duplicate images only exist in the same class (folder), only one of the images will be retained. Then, the difference hash algorithm was used to detect more duplicates within the dataset. The hash value for every image was calculated and then compared to get those with very little difference. Images with little difference hash values mean they are identical. Likewise, if the identical images only exist in the same class (folder), only one image is retained, the rest are eliminated.</p><p>To detect non-plant images, a discrimination network for identifying plant and non-plant images was trained. This process consisted of 3 phases.</p><p>In phase 1, the positive samples (plant) were taken from PlantClef 2016 while the negative samples (non-plant) were taken from ImageNet2012 (excluding the plant classes). The training dataset size was 4,000, with each class having 2,000 samples. Meanwhile the validation dataset size was 2,000, with each class having 1,000 samples. An Inception-V4 plant and non-plant classifier was then trained using these datasets. In phase 2, 5,000 samples were randomly selected from PlantClef 2019 and predicted using the trained classifier. The performance on this sample was evaluated manually. After evaluating its performance, the training set was refined by manually correcting the prediction and adding them back to the training set. Then, the Inception-V4 plant and non-plant classifier was retrained using the new training set. Phase 2 was repeated until the performance satisfied the accuracy of over 90%. In this case it was repeated 4 times.</p><p>In phase 3, the Inception-V4 plant and non-plant classifier was applied to the whole dataset to remove the non-plant images. Only the softmax probability of 0.98 for the non-plant class was regarded as non-plant images.</p><p>The entire process is visualised in Fig. <ref type="figure" coords="4,320.09,463.64,3.87,8.74" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>This section will describe the motivations for using Inception-v4 and Inception-ResNet-v2 for the plant classification challenge, the training setup, network hyperparameters and validation results obtained from the trained networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>The backbone networks used for classifying PlantCLEF 2019 images were Inception-v4 and Inception-ResNet-v2 models. These two models were adopted in this plant classification task as they come with the lowest Top-1 Error and Top-5</p><p>Error for single-crop -single-model experimental results, when being proposed in the original paper <ref type="bibr" coords="4,224.00,644.16,14.61,8.74" target="#b11">[12]</ref>. Besides, the use of inception modules allows filters with multiple sizes to perform convolution on the same level to cater to the variation in the location of the information on the image <ref type="bibr" coords="5,345.04,118.99,14.61,8.74" target="#b12">[13]</ref>, which is applicable on the training dataset. Both of the networks implement Convolutional Neural Network (CNN) architecture, which typically consists of convolutional layers, pooling layers, dropout layers and fully-connected layers. The models were fine-tuned from pre-trained weights on the ImageNet dataset <ref type="bibr" coords="5,281.55,178.89,14.61,8.74" target="#b9">[10]</ref>.</p><p>In fact, Inception-v4 and Inception-Resnet-v2 networks share an identical stem as the input part of these networks. However, these two models demonstrate differences in architecture after the stem.</p><p>For Inception-v4 model, there are three main inception modules following the stem, namely Inception-A, Inception-B and Inception-C modules. The output will then be concatenated and fed to the next module. Meanwhile, there is a reduction module after Inception-A and Inception-B modules to alter the width and height of the grid. These modules work together to serve as the model's feature extractor.</p><p>On the other hand, Inception-ResNet-v2 model makes use of residual connections in addition to inception modules after the implementation of stem. This allows Inception-ResNet-v2 to achieve higher accuracies within a shorter time frame, while being similarly computationally expensive as the Inception-v4 model. For residual connections to work, the pooling layers from pure Inception modules are replaced with residual connections. Similarly, there is a respective reduction module following Inception-ResNet-A and Inception-ResNet-B modules.</p><p>Both Inception-v4 and Inception-ResNet-v2 model have the same structures for the classification part, which consists of an average pooling layer, a dropout layer, and a fully-connected layer which return the softmax probabilities over predicted output classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Setup</head><p>During the network training, two types of classification methods were investigated, namely single label classification and multi-task classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Label Classification</head><p>The first classification method is the conventional classification method which is based on single label prediction model. The labels are the plant species, therefore there are 10,000 classes.</p><p>Multi-Task Classification Another method used in this plant identification task is the multi-task classification whereby the labels "Species","Genus" and "Family" of the samples were used in training. This allowed the network to regularise and generalise better on images from a large number of classes. The total number of family class is 248, while the genus class is 1,780<ref type="foot" coords="5,421.05,624.53,3.97,6.12" target="#foot_1">2</ref> .</p><p>Library Used The networks were implemented using TensorFlow Slim library, with the weights being pre-trained on ImageNet datset <ref type="bibr" coords="6,377.16,130.95,14.61,8.74" target="#b9">[10]</ref>. The networks were then fine-tuned accordingly using the hyperparameters described in Sub-section 3.4. Since the adopted models pre-trained on ImageNet have only 1,000 classes, the fully-connected layer of the adopted models was adapted to 10,000 classes during transfer learning for PlantCLEF 2019. For multi-task classification, there were two additional fully-connected layers which were catered to 248 "Family" classes and 1,780 "Genus" classes.</p><p>Training Data The input image size of the training was 299 × 299 × 3. By separating 20,000 samples from the training samples as a validation set, the total remaining training set comprised 230,646 images. Although the total number of classes is reduced from 10,000 classes to 8,263 classes, the network was still trained to classify 10,000 classes through class mapping. This allows model update when missing classes are to be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Augmentation</head><p>Data augmentation was performed on the training images to increase the training sample size. With this, the CNN network can learn features that are invariant to their locations in the images and various transforms, which then effectively reduces the chance of overfitting <ref type="bibr" coords="6,280.24,382.78,9.96,8.74" target="#b8">[9]</ref>. Random cropping, horizontal flipping and colour distortion (brightness, saturation, hue, and contrast) of images were applied to the training dataset to increase the possibility of classifying the correct plants regardless of their different environments or orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network Hyperparameters</head><p>The learning dropout rate was set to 0.2 (keeping 80 % of the neurons before the fully-connected layer) while the optimizer used was Adam Optimizer. The optimizer took on an initial learning rate of 0.0001. Meanwhile, the gradient was clipped at 1.25 to prevent the occurrence of exploding gradients. Softmax cross entropy loss was used to compute the error between the predicted labels and true labels; while the L2 regularization loss was added with weight decay of 0.00004. The network hyperparameters can be summarised in Table <ref type="table" coords="6,395.21,543.36,3.87,8.74" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Validation</head><p>Prior to training, 20,000 samples were randomly separated from the cleaned training dataset of 250,646 images as validation set. It was ensured that each of the remaining 8,263 classes in the training set has at least one sample left for training. A validation sample was not added if there is only one sample left for training in each class. In other words, a class will not have a validation sample if it has only one sample. There were 4 approaches in testing the validation set on 4 different kinds of network. The approaches include "Top 1 Centre Crop", "Top 1 Centre Crop + Corner Crop", "Top 5 Centre Crop", and "Top 5 Centre Crop + Corner Crop". Meanwhile the 4 different networks included "Inception-v4", "Inception-v4 Multi-task", "Inception-ResNet-v2 Multi-task", and "Inception-v4 Multi-task + Inception-ResNet-v2 Multi-task" with different dataset sizes. Note that all the trained networks were validated upon the same 20,000 validation images.</p><p>The "Centre Crop" approach considers the centre region of the sample. The centre region was cropped and resized then passed into the network for testing. The "Corner Crop" approach on the other hand focused on the centre, top left, top right, lower left, and lower right region of the image. Each region was cropped and resized then passed into the network for testing. The "Top 1" and "Top 5" approaches represent the Top 1 and Top 5 predictions based on the testing results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Inference Procedures</head><p>The following procedures were adopted for inferencing the predictions of the test dataset. There were three models used for inference, namely "Multi-Task Inception-v4", "Multi-Task Inception-ResNet-v2" and "Multi-Task Inception-v4 + Multi-Task Inception-ResNet-v2".</p><p>1. The test images with the same observation ID were grouped together. 2. A total of five center crop and corner crop images were then produced for a single test image. 3. The test images grouped under the same observation ID were fed into the CNN models for label predictions, as shown in Fig. <ref type="figure" coords="7,383.72,555.94,3.87,8.74" target="#fig_1">2</ref>. This would mean a total of five predictions for a single test image. 4. The probabilities of each prediction were averaged over the total number of test image crops grouped under the same observation ID. 5. The top 100 probabilities with value greater than 0.001 were collected for result tabulations. 6. Step 2 to 5 were repeated for every different observation ID.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussions</head><p>The automated identification of plants for PlantCLEF 2019 has shown a notable decrease in performance compared to the previous editions. This may be due to the decrease of per class samples as mentioned in the challenge's description. In such real-world condition, the dataset requires pre-processing before training is done. Although it has been seen that training with noisy data can be more profitable <ref type="bibr" coords="13,180.33,203.71,9.96,8.74" target="#b1">[2]</ref>, the noisy data in this challenge worsens the performance of the automatic plant identification. At this stage of understanding, the team believe the portion of noisy images should not be occupying a large margin (duplicates file names are 55.13% out of the whole training set) of training set to act as good regularisation agent.</p><p>Therefore, the dataset was cleaned before training. This prevents the network from getting trained by large amount of noisy data such as the non-plant images and duplicate images/names (which may come with incorrect labels). Moreover, by adding multi-task classification in our system, it has helped in regularising the network and improving the performance. Modelling a relationship between Species, Genus and Family is essential for better plant recognition.</p><p>Since the cleaned dataset is reduced to 8,263 classes, there was no capability to distinguish the missing classes. Hence, the label for the missing classes was unable to be predicted if it is present in the test set resulting in a lower performance. Additionally, 20,000 images had been sidelined for validation purposes which are believed to be helpful in increasing the performance if they were added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The task of the PlantCLEF 2019 challenge was to return the most likely matching species for each observation (a set of images of the same plant) of the test set. In this paper, the team has presented the overview and results of our approach in doing so. With regards to the diversity of the dataset, the team has found that the cleaning of the dataset and multi-task classification of Species, Genus and Family improved the prediction results.</p><p>According to the competition results released, our trained model was better than one of the experts in Top 1 and Top 3 accuracy. Additionally, it has a close performance with Expert 4. Our model even outperformed 2 experts for Top 5 accuracy. Overall, 3 of our submitted runs obtained the good results in every machine category.</p><p>The identification of plants based off images is indeed a difficult task even for some of the botanical experts. Relying on plant images alone is usually insufficient to determine the correct species as they may contain only partial information of the plant <ref type="bibr" coords="13,233.81,632.21,9.96,8.74" target="#b2">[3]</ref>. Based on the competition results of PlantCLEF 2018 and 2019, it can be considered that with sufficient data for training, machines are able to perform nearly equal or better than a human.</p><p>For future work, the plant images from the missing classes can be retrieved if the ground-truth labels are known and will be used for training the networks for better predictions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,247.02,300.50,121.33,7.89;4,134.77,115.83,345.82,169.90"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Data cleaning process.</figDesc><graphic coords="4,134.77,115.83,345.82,169.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,182.03,303.29,251.30,7.89;8,134.77,115.84,345.84,172.68"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Process of feeding test images into the trained models.</figDesc><graphic coords="8,134.77,115.84,345.84,172.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,188.83,280.03,237.69,7.89;11,152.06,120.75,311.23,144.50"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Top-1 accuracy achieved by all the submitted runs.</figDesc><graphic coords="11,152.06,120.75,311.23,144.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,188.83,465.00,237.69,7.89;11,152.06,305.73,311.23,144.50"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Top-5 accuracy achieved by all the submitted runs.</figDesc><graphic coords="11,152.06,305.73,311.23,144.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,208.55,650.11,198.26,7.89;11,152.06,490.70,311.24,144.64"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. MRR achieved by all the submitted runs.</figDesc><graphic coords="11,152.06,490.70,311.24,144.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="12,144.34,333.66,326.68,7.89;12,152.06,159.68,311.24,159.21"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Top-1 accuracy achieved by all the submissions including human experts.</figDesc><graphic coords="12,152.06,159.68,311.24,159.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="12,144.34,611.19,326.68,7.89;12,152.06,437.20,311.24,159.21"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Top-5 accuracy achieved by all the submissions including human experts.</figDesc><graphic coords="12,152.06,437.20,311.24,159.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="10,169.35,115.84,276.64,202.18"><head></head><label></label><figDesc></figDesc><graphic coords="10,169.35,115.84,276.64,202.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,179.77,115.91,255.82,113.37"><head>Table 1 .</head><label>1</label><figDesc>Network hyperparameters used for training networks.</figDesc><table coords="7,213.60,139.03,182.02,90.25"><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Batch Size</cell><cell>256</cell></row><row><cell>Optimizer</cell><cell>Adam Optimizer</cell></row><row><cell>Initial Learning Rate</cell><cell>0.0001</cell></row><row><cell>Gradient Clipping</cell><cell>1.25</cell></row><row><cell>Loss Function</cell><cell>Softmax Cross Entropy</cell></row><row><cell>Weight Decay</cell><cell>0.00004</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,645.84,335.87,8.12;3,144.73,657.44,108.27,7.47"><p>The cleaned list can be found at Github via https://github.com/changyangloong/ plantclef2019_challenge</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,144.73,645.84,335.86,8.12;5,144.73,657.44,108.27,7.47"><p>The multi-task label can be found at https://github.com/changyangloong/ plantclef2019_challenge</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>The resources of this project is supported by <rs type="funder">NEUON AI SDN. BHD.</rs>, <rs type="funder">Malaysia</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Validation Results</head><p>Inception-v4 and Inception-ResNet-v2 were the 2 networks used for training and classification in these 4 approaches. The Multi-task description represents the classification on multiple labels, i.e. Species, Family and Genus.</p><p>The validation results are shown in Table <ref type="table" coords="8,340.93,435.58,3.87,8.74">2</ref>. The results have shown that while eliminating the duplicate image could increase the performance of the single-task network, multi-task classification method was able to achieve a even higher accuracy on the validation dataset.</p><p>Interestingly enough, removing non-plant images after filtering duplicate images out leads to a slight drop in the single-task Inception-v4 network's performance. Since no Inception-v4 network is trained without the non-plant images (i.e. keeping potentially duplicate images but of plants only) as benchmark, there is no way to deduce whether the presence of duplicate images or non-plant images is the main factor to the performance drop in our current experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Submitted Runs</head><p>The team submitted a total of three runs based on three different trained models which achieved the top-3 validation accuracy. The models are described in the followings:</p><p>Holmes Run 1 utilises fine-tuned Inception-v4 model catered to multi-task classification (which are Species, Genus and Family). Holmes Run 2 is an ensemble of Inception-v4 and Inception-ResNet-v2, and is also catered to multi-task classification.</p><p>Holmes Run 3 summarises prediction results from fine-tuned Inception-ResNet-v2 catered to multi-task classification.</p><p>The run files were formatted as ObservationID; ClassID; Probability; Rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sample of Predictions</head><p>Fig. <ref type="figure" coords="9,156.56,434.46,4.98,8.74">3</ref> depicts the prediction output of test images grouped under the same observation ID. This was to visualise the predictions generated by the models for evaluating the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">LifeCLEF 2019 Plant Challenge results</head><p>The team submitted three runs with Holmes Run2 being the best performance among our submission in terms of Top-1 accuracy,Top-3 accuracy and Top-5 accuracy. Holmes Run 2 achieved Top 1 accuracy of 31.6% for the test set identified by experts, despite the occurrence of missing classes after data cleaning. This proves that using an ensemble of two different state-of-the-art CNN models can increase the robustness of the system and return better predictions. Table <ref type="table" coords="9,475.61,572.03,4.98,8.74">3</ref> summaries the results achieved by the team. Mean Reciprocal Rank (MRR) was also used to evaluate the performance of the submitted runs. The MRR is the average of the reciprocal ranks of the whole test set, with the reciprocal rank of a query response being the multiplicative inverse of the rank of the first correct answer. It can be mathematically represented as shown, with | Q | being the frequency of plant occurrences in the test set.   </p><p>Holmes Run 2, being our best performing machine, has achieved a MRR of 0.362 and 0.298 for the test set identified by experts and the whole test set respectively, as shown in Fig. <ref type="figure" coords="10,265.44,538.82,3.87,8.74">6</ref>.</p><p>At the same time, all the three runs submitted by the team outperform 1 out of 5 human experts of the Amazonian French Guina flora according to Top-1 (Fig. <ref type="figure" coords="10,134.77,574.69,4.43,8.74">7</ref>) and Top-3 accuracies. As for Top-5 accuracy, Holmes Run 2 outperformed 2 human experts, as captured in Fig. <ref type="figure" coords="10,291.15,586.64,3.87,8.74">8</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="14,142.96,237.30,337.63,7.86;14,151.52,248.26,58.66,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="14,203.90,237.30,249.12,7.86">The different forms of flowers on plants of the same species</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Darwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1877">1877</date>
			<publisher>John Murray</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,258.23,337.64,7.86;14,151.52,269.18,329.07,7.86;14,151.52,280.14,207.81,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,282.89,258.23,197.70,7.86;14,151.52,269.18,212.56,7.86">Plant identification based on noisy web data: the amazing performance of deep learning (lifeclef 2017)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,386.44,269.18,94.15,7.86;14,151.52,280.14,136.96,7.86">CLEF 2017-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,290.11,337.63,7.86;14,151.52,301.07,288.19,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,278.87,290.11,201.72,7.86;14,151.52,301.07,196.17,7.86">Overview of expertlifeclef 2018: how far automated identification systems are from the best experts?</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,364.84,301.07,46.20,7.86">CLEF 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,311.04,337.63,7.86;14,151.52,321.99,327.70,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,281.81,311.04,198.78,7.86;14,151.52,321.99,172.86,7.86">Overview of lifeclef plant identification task 2019: diving into data deficient tropical countries</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,345.40,321.99,105.15,7.86">CLEF working notes 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,331.96,337.64,7.86;14,151.52,342.92,329.07,7.86;14,151.52,353.88,170.92,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,298.56,342.92,182.03,7.86;14,151.52,353.88,15.40,7.86">The imageclef 2011 plant images classi cation task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,187.92,353.88,49.02,7.86">ImageCLEF</title>
		<imprint>
			<biblScope unit="page" from="0" to="0" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,363.85,337.63,7.86;14,151.52,374.80,329.07,7.86;14,151.52,385.76,329.07,7.86;14,151.52,396.72,196.70,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,401.58,374.80,79.01,7.86;14,151.52,385.76,329.07,7.86;14,151.52,396.72,39.12,7.86">Overview of lifeclef 2019: Identification of amazonian plants, south &amp; north american birds, and niche prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Kah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fabian-Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,211.70,396.72,107.84,7.86">Proceedings of CLEF 2019</title>
		<meeting>CLEF 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,406.69,337.64,7.86;14,151.52,417.65,329.07,7.86;14,151.52,428.61,209.90,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,353.59,406.69,127.00,7.86;14,151.52,417.65,142.65,7.86">Deep-plant: Plant identification with convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wilkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,317.53,417.65,163.06,7.86;14,151.52,428.61,98.07,7.86">2015 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="452" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,438.57,337.64,7.86;14,151.52,449.53,329.07,7.86;14,151.52,460.49,329.07,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,357.75,438.57,122.85,7.86;14,151.52,449.53,262.31,7.86">Hgo-cnn: Hybrid generic-organ convolutional neural network for multi-organ plant classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,436.15,449.53,44.44,7.86;14,151.52,460.49,210.50,7.86">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4462" to="4466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,470.46,337.64,7.86;14,151.52,481.42,329.07,7.86;14,151.52,492.37,192.95,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,293.21,470.46,187.39,7.86;14,151.52,481.42,140.77,7.86">Data augmentation for improving deep learning in image classification problem</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miko Lajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grochowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,315.48,481.42,165.10,7.86;14,151.52,492.37,80.39,7.86">2018 international interdisciplinary PhD workshop (IIPhDW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="117" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,502.34,337.97,7.86;14,151.52,513.30,329.07,7.86;14,151.52,524.26,329.07,7.86;14,151.52,535.22,60.92,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,420.31,513.30,60.28,7.86;14,151.52,524.26,130.65,7.86">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,288.93,524.26,160.68,7.86">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,545.18,337.98,7.86;14,151.52,556.14,261.25,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,276.58,545.18,204.01,7.86;14,151.52,556.14,105.70,7.86">Plant recognition by inception networks with testtime class prior estimation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,264.36,556.14,98.24,7.86">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,566.11,337.97,7.86;14,151.52,577.07,329.07,7.86;14,151.52,588.03,146.20,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,360.37,566.11,120.22,7.86;14,151.52,577.07,200.58,7.86">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,372.47,577.07,108.13,7.86;14,151.52,588.03,117.53,7.86">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,597.99,337.98,7.86;14,151.52,608.95,329.07,7.86;14,151.52,619.91,329.07,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,282.96,608.95,127.48,7.86">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,432.52,608.95,48.07,7.86;14,151.52,619.91,265.41,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,629.88,337.97,7.86;14,151.52,640.84,329.07,7.86;14,151.52,651.80,88.91,7.86" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>O' Mahony</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Krpalkova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Velasco-Hernandez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harapanahalli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Riordan</surname></persName>
		</author>
		<title level="m" coord="14,343.24,640.84,137.35,7.86;14,151.52,651.80,47.95,7.86">Deep learning vs. traditional computer vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
