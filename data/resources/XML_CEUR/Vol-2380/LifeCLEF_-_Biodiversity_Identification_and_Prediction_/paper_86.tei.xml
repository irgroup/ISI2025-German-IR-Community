<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,168.50,152.67,253.73,12.64;1,422.35,149.63,4.50,8.10">Bird Species Identification in Soundscapes *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,268.01,191.70,59.04,8.96"><forename type="first">Mario</forename><surname>Lasseck</surname></persName>
							<email>mario.lasseck@mfn.berlin</email>
							<affiliation key="aff0">
								<orgName type="institution">Museum für Naturkunde Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,168.50,152.67,253.73,12.64;1,422.35,149.63,4.50,8.10">Bird Species Identification in Soundscapes *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">11A676C7C1B678C418CCC3DB6878A8D5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bird Species Identification</term>
					<term>Biodiversity Assessment</term>
					<term>Soundscapes</term>
					<term>Convolutional Neural Networks</term>
					<term>Deep Learning</term>
					<term>Data Augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents deep learning techniques for audio-based bird identification in soundscapes. Deep Convolutional Neural Networks are trained to classify 659 species. Different data augmentation techniques are applied to prevent overfitting and improve model accuracy and generalization. The proposed approach is evaluated in the BirdCLEF 2019 campaign and provides the best system to identify bird species in wildlife monitoring recordings. With an ensemble of different single-and multi-label classification models it obtains a classification mean average precision (c-mAP) of 35.6 % and a retrieval mean average precision (r-mAP) of 74.6 % on the official BirdCLEF test set. In terms of classification precision, single model performance surpasses previous stateof-the-art by more than 20 %.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the LifeCLEF bird identification task participating teams have to identify different bird species in a large collection of audio recordings. The 2019 edition mainly focuses on soundscapes. This is a more difficult task compared to previous editions where species had to be identified mostly in mono-directional recordings with usually only one prominent species present in the foreground. Soundscapes on the other hand are recorded in the field, e.g. for wildlife monitoring, not targeting any specific direction or individual animal. There can be a large number of simultaneously singing species overlapping in time and frequency, arbitrary background noise depending on weather conditions and sometimes very distant and faint calls. Identifying as many species as possible in such a scenario remains challenging but is an important step towards real-world wildlife monitoring and reliable biodiversity assessment. An overview and further details about the BirdCLEF task is given in <ref type="bibr" coords="1,378.38,596.75,10.71,8.96" target="#b0">[1]</ref>. It is among others part of the LifeCLEF 2019 evaluation campaign <ref type="bibr" coords="1,319.80,608.75,10.74,8.96" target="#b1">[2]</ref>.</p><p>The approach described in this paper uses neural networks and deep learning. It builds on the work of previous solutions to the task and combines proven techniques with new methods for data augmentation and multi-label classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Preparation</head><p>All audio recordings are first high pass filtered at a frequency of 2 kHz (Q = 0.707) and then resampled to 22050 Hz with the Sound eXchange (SoX) v14.4.1 audio processing tool <ref type="bibr" coords="2,178.34,285.21,10.69,8.96" target="#b2">[3]</ref>. Soundscapes from the validation set are prepared for training by cutting them into individual files according to their annotations. Starting from the beginning of a file, whenever the label or set of labels changes, a new audio file is generated with the corresponding labels. Additionally, a "noise only" file is created from each soundscape by merging all parts without bird activity via concatenation. Those files containing only background noise are later used together with other background recordings for noise augmentation.</p><p>In order to also use the validation set for training, it is split into 8 folds via iterative stratification for multi-label data <ref type="bibr" coords="2,258.49,381.21,10.72,8.96" target="#b3">[4]</ref>. As a result, a small part of the validation set can be used to evaluate model performance while the rest of the set can be added to the Xeno-Canto <ref type="bibr" coords="2,176.06,405.21,11.72,8.96" target="#b4">[5]</ref> training set.</p><p>To allow faster prototyping and to create a more diverse set of models for later ensembling, different data subsets are formed targeting different numbers of species or sound classes: The smallest data set covers all 78 species present in the annotated soundscapes of the validation set and only contains training files belonging to those classes (not considering background species). The second data set consists of all files belonging to species mainly present in the recording locations of the United States. To find out which species are likely to be recorded in the US, the additionally provided eBird <ref type="bibr" coords="2,428.35,553.91,11.60,8.96" target="#b5">[6]</ref> data is taken into account and all files belonging to a species with a frequency value above zero for any time of the year are added to the first data set. The third data set finally covers all 659 species and all available training files. The eBird data is also used to create a list of unlikely species for both the Colombia and the US recording locations. For some submissions this list is later used to set predictions of unlikely species to zero for soundscapes in the test set depending on their recording location.</p><formula xml:id="formula_0" coords="2,124.70,461.27,4.58,9.05"></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Setup</head><p>For audio-based bird species identification Deep Convolutional Neural Networks pretrained on ImageNet <ref type="bibr" coords="3,209.43,186.18,11.85,8.96" target="#b6">[7]</ref> are fine-tuned with mel scaled spectrogram images representing short audio chunks. Models are trained with PyTorch <ref type="bibr" coords="3,366.50,198.18,11.77,8.96" target="#b7">[8]</ref> utilizing PySoundFile and LibROSA [9] python packages for audio file reading and processing. The same basic pipeline as for the BirdCLEF 2018 task is used for data loading and can be summarized as follows:</p><p> Extract audio chunk from file with a duration of ca. <ref type="bibr" coords="3,344.40,254.34,4.98,8.96" target="#b4">5</ref>  Training is done with a batch size of ca. 100 -200 samples using up to 3 GPUs (Nvidia 1080, 1080 Ti, Titan RTX). Categorical cross entropy [10] is used as loss function for single-label classification considering only foreground species as ground truth targets. Stochastic gradient descent is used as optimizer with momentum 0.9, weight decay 1e-4 and an initial learning rate of 0.1. Learning rate is decreased at least once during training by ca. 10 -1 whenever performance on the validation set stops improving. If more than one species is assigned to an audio chunk, in case of validation soundscapes, one species or label is chosen randomly as ground truth target during training. Background species annotated for Xeno-Canto files are not taken into account.</p><p>Besides the common single-label classification approach, multi-label classification models are trained as well to take advantage of the fact that there are multi-label annotations existing for validation soundscapes with two or more species present at the same time in many cases. For soundscapes, the multi-label approach also seems the more suited classification method since recordings are mostly not focused on a single target species. Two loss functions are tested for multi-label training. PyTorch's Mul-tiLabelSoftMarginLoss creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy <ref type="bibr" coords="3,238.49,563.75,15.52,8.96">[11]</ref>. The loss function BCEWithLogitsLoss combines a sigmoid layer and a binary cross entropy layer <ref type="bibr" coords="3,312.38,575.75,15.47,8.96">[12]</ref>.</p><p>For the validation and test set audio chunks are extracted successively from each file with an overlap of 10 % for validation files during training and 80 % for files in the test set. Predictions are summarized for each file and time interval by taking the maximum over all chunks. For most submissions, different models are ensembled by averaging their predictions for each species after normalizing the entire prediction matrix to have a minimum of 0.0 and a maximum value of 1.0. To increase ensemble performance a little further, in some cases it helped to clip very low and high prediction values to -7.0 and 10.0, respectively before normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation</head><p>To increase model performance and improve generalization to different recording conditions and habitats, the most effective data augmentation techniques from the previous BirdCLEF edition <ref type="bibr" coords="4,237.50,198.18,16.91,8.96" target="#b9">[13]</ref>  Local time stretching and pitch shifting in time domain. The audio signal is divided into segments, each having a randomly chosen duration between 0.5 and 4 seconds. To each segment time stretching or pitch shifting or both is applied individually using the LibROSA library. The time stretching factor is randomly chosen from a gauss distribution with a mean value of 1 and a standard deviation of 0.05. The pitch is shifted by an offset randomly chosen from a gauss distribution with a mean value of 0 and a standard deviation of 25 cents (8 th of a tone).</p><p>Filter with random transfer function. With a chance of ca. 20 %, audio chunks are filtered in time domain using a butterworth filter design with variable transfer function. The following filter parameters are chosen randomly:  Type: lowpass, highpass, bandpass, bandstop  Order: 1-5  Cutoff frequency: 1-22049 Hz For bandpass and bandstop filter types the second (high) cutoff frequency is chosen between the (low) cutoff frequency + 1 and 22049 Hz (nyquist frequency -1). Depending on filter parameters and audio input, filter stability is not always guarantied. To prevent unbounded signals the original input is passed as output if the filter output contains anything that is not a number between -1.0 and 1.0. Examples of a randomly filtered audio recording are visualized in Figure <ref type="figure" coords="4,318.06,657.02,3.76,8.96" target="#fig_2">1</ref>. Mixing random audio chunks for multi-label classification. For multi-label classification, audio chunks from random files are mixed together and their corresponding labels added to the target label set during training. Up to four audio chunks are added with random signal amplitude to the original training sample with conditional probabilities of 50, 40, 30 and 20 %. A similar technique is originally used by <ref type="bibr" coords="5,422.55,416.25,16.79,8.96" target="#b10">[14]</ref> for image classification and has shown good results for multi-label audio classification as well <ref type="bibr" coords="5,144.62,440.25,15.42,8.96" target="#b11">[15]</ref>. Here, however, labels are not weighted by signal amplitudes (or influenced by weighting of the linear combination).</p><p>For background noise augmentation, besides using noise from validation files, recordings without bird activity of the Bird Audio Detection (BAD) task 2018 <ref type="bibr" coords="5,415.87,488.27,16.64,8.96" target="#b12">[16]</ref> are used. The BAD data set is part of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2018 <ref type="bibr" coords="5,345.79,512.27,15.34,8.96">[17]</ref>. It consists of audio files from three separate bird sound monitoring projects each recorded under differing conditions regarding recording equipment and background sounds.</p><p>The audio chunk (or sum of chunks) is transformed to frequency domain via shorttime Fourier transform with a window size of 1536 samples and a hop length of 360 samples. Frequencies are mel scaled with low and high frequencies removed resulting in a spectrogram with 310 mel bands representing a range of approximately 160 to 10300 Hz. Normalization and logarithm is applied to the power spectrogram yielding a dynamic range of approximately 100 dB. The final spectrogram image is resized to 299x299 pixel to fit the input dimension of the InceptionV3 <ref type="bibr" coords="5,369.43,620.27,16.76,8.96" target="#b13">[18]</ref> network or 224x224 pixel for ResNet <ref type="bibr" coords="5,195.77,632.27,16.76,8.96" target="#b14">[19]</ref> models. Resizing is performed with the Python Image Library fork Pillow using randomly chosen interpolation filters of different qualities. Because audio chunks are extracted with a random length (e.g. between 4.55 and 5.45 s by applying a duration jitter of ca. half a second) a global time stretching effect is obtained after resizing the variable length spectrogram images to a fixed width. Image resizing is also applied to individual vertical and horizontal spectrogram segments to accomplish piecewise or local time and frequency stretching (see below and <ref type="bibr" coords="6,439.13,162.18,16.81,8.96" target="#b9">[13]</ref> for more details). Since networks are pre-trained on RGB images, the grayscale image is copied to all three colour channels. Further augmentation is applied in frequency domain to the spectrogram image during training:  Global frequency shifting/stretching  Local time and frequency stretching  Different interpolation filters for spectrogram resizing  Colour jitter (brightness, contrast, saturation, hue) Table <ref type="table" coords="6,151.07,275.10,4.98,8.96" target="#tab_1">1</ref> demonstrates the effect of augmentation and single-vs. multi-label training on model performance. All models were trained with the 78 classes data set using the same parameter settings with a learning rate of 0.1 and 0.01 until performance on the validation set stopped improving. More details on individual augmentation methods and their effect on identification and detection performance in previous challenges can be found in <ref type="bibr" coords="6,389.90,513.11,17.05,8.96" target="#b9">[13]</ref> and <ref type="bibr" coords="6,426.41,513.11,15.35,8.96" target="#b15">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>For the first two submitted runs a single model was used to predict the species for each file and time interval in the test set. All other runs used an ensemble of different models. The main properties of individual models are listed in Table <ref type="table" coords="6,411.28,594.11,3.76,8.96" target="#tab_2">2</ref>. Selected results on the official BirdCLEF test set are summarized in Table <ref type="table" coords="6,398.94,606.11,4.98,8.96" target="#tab_3">3</ref> and further described in the next section. Run 2. For the second run, a single model (M2) was trained with main properties listed in Table <ref type="table" coords="7,186.92,497.39,3.76,8.96" target="#tab_2">2</ref>. Also validation soundscapes were used for training and noise augmentation. The third generation Inception model M2 used all above mentioned augmentations except the new time domain methods filtering and local time stretching and pitch shifting. For this and all following models, BAD 2018 files were used for noise augmentation. As a result, it is not necessary any more to segment training files into signal and noise parts to get background material from the training set for noise augmentation like done in previous challenge editions. This greatly simplifies the preprocessing step. A c-mAP of 25.9 % and a r-mAP of 69.1 % is obtained on the test set resulting in a performance increase of 21.6 % and 54.6 %, respectively compared to previous state-of-the-art (M1). Since M1 and M2 didn't use the exact same training set and for the training of M2 not all new time domain augmentations were applied the given progress is only a rough approximation. The run 5 ensemble obtained the highest classification mAP of 35.6 % on the test set.</p><p>Run 6 to 10. Different combinations of the previously mentioned models were used for run 6 to 10. Also different snapshots of the same model were included for ensembling and two models were trained on different folds of the validation set. Nevertheless, no further progress on identification performance on the test set was obtained.</p><p>For run 9 the same ensemble as for run 8 was used except run 9 didn't use the eBird data to set predictions of unlikely species to zero in the post-processing step. This demonstrates once again, performance can be increased when unlikely birds are filtered out for a certain recording location where species composition is known in advance.  <ref type="table" coords="9,429.52,345.21,3.62,8.96" target="#tab_1">1</ref>).</p><p>With additional methods like filtering audio chunks with random transfer function or applying local time stretching and pitch shifting in time domain, identification performance can be further increased (E4 vs. E5 &amp; E6 vs. E7 in Table <ref type="table" coords="9,421.24,381.21,3.68,8.96" target="#tab_1">1</ref>). Unfortunately, training takes significantly longer especially if LibROSA's time stretching and pitch shifting algorithms are applied very frequently. Due to the longer training time it was not possible to investigate the individual influence of each method or different parameter settings on model performance. To save time, those techniques were only applied to the original training sample (first audio chunk in the mix) and not, or only with very low chance, to chunks added for augmentation. Both algorithms also seem to blur the resulting spectrogram even with very subtle use (time stretching factor close to 1, pitch shifting offset close to 0). Maybe a more efficient implementation regarding processing time and quality would be a better choice in the future.</p><p>Multi-label training was successfully applied for the 78 classes set (E4 vs. E6 in Table <ref type="table" coords="9,151.68,513.23,3.62,8.96" target="#tab_1">1</ref>). In contrast to the single-label approach, for multi-label classification the residual network obtained better results compared to the Inception architecture (3 rd vs. 4 th column in Table <ref type="table" coords="9,207.19,537.23,3.62,8.96" target="#tab_1">1</ref>). Unfortunately, training with a larger number of classes didn't work so well even when passing a weight vector as argument to the BCEWithLo-gitsLoss function to compensate for class imbalances. One explanation for this might be the exponential growth of possible label combinations depending on the number of individual labels (classes) and the number of labels considered to be possible for a single audio chunk. Maybe if species combination constrains are known a priori (e.g. by distinguishing between diurnal and nocturnal birds) and applied to reduce the number of possible label sets, models can also be trained successfully in a multi-label fashion for a much larger number of species.</p><p>To reproduce results and to provide a baseline for future BirdCLEF challenges and further research on bird species identification, source code will be made available at: www.animalsoundarchive.org/RefSys/BirdCLEF2019.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,136.10,461.37,238.43,8.96;2,124.70,473.53,259.78,9.06;2,124.70,485.77,272.27,9.06;2,397.27,483.68,3.24,5.83"><head></head><label></label><figDesc>Data set 1: 78 classes (with 7342 files from the training set)  Data set 2: 254 classes (with 21542 files from the training set)  Data set 3: 659 classes (with all 50145 files from the training set) 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,257.33,198.18,213.39,8.96;4,124.70,210.18,345.74,8.96;4,124.70,222.18,149.28,8.96;4,124.70,242.24,183.77,9.06;4,124.70,254.48,68.23,9.06;4,124.70,266.96,179.18,9.06;4,124.70,279.11,167.84,9.06;4,124.70,291.11,91.74,9.06;4,124.70,303.35,276.59,9.06;4,124.70,315.59,334.91,9.06;4,124.70,328.07,345.87,9.06;4,124.70,339.95,230.59,9.06;4,124.70,352.19,99.03,9.06;4,124.70,372.35,345.74,9.06;4,124.70,384.45,210.48,8.96"><head></head><label></label><figDesc>are applied in both time and frequency domain. New methods are highlighted and explained below. The following methods are applied in time domain regarding audio chunks:  Chunk extraction at random position in file  Duration jitter  Local time stretching and pitch shifting  Filter with random transfer function  Random cyclic shift  Adding audio chunks from files containing only background noise  Adding audio chunks from files belonging to the same bird species (single-label)  Adding audio chunks from files belonging to random bird species (multi-label)  Random signal amplitude of chunks before summation  Time interval dropout A few new methods are added for this year's challenge to augment individual audio chunks in time domain before mixing them together:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,195.77,333.02,203.75,8.10;5,126.65,243.40,79.05,79.05"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of a randomly filtered audio recording.</figDesc><graphic coords="5,126.65,243.40,79.05,79.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,124.70,653.42,247.87,8.96;7,372.67,651.20,6.48,5.83;7,382.39,653.42,88.19,8.96;7,124.70,665.42,345.51,8.96;7,124.70,677.42,306.20,8.96;8,124.70,150.18,55.36,8.96;8,180.14,147.99,5.04,5.83;8,188.06,150.18,282.64,8.96;8,124.70,162.18,345.89,8.96;8,124.70,174.18,188.93,8.96;8,124.70,198.18,345.53,8.96;8,124.70,210.18,345.82,8.96;8,124.70,222.18,299.36,8.96;8,124.70,242.24,172.31,9.06;8,124.70,254.48,194.19,9.06;8,124.70,266.72,189.02,9.06;8,124.70,278.87,188.46,9.06;8,124.70,291.11,184.43,9.06;8,124.70,303.35,163.08,9.06;8,124.70,315.59,192.67,9.06;8,124.70,327.83,200.68,9.06;8,124.70,339.95,215.99,9.06;8,124.70,352.19,214.43,9.06"><head>Run 3 .Run 4 .Run 5 .</head><label>345</label><figDesc>For the third run, two models were ensembled: the 2 nd run model and a Res-Net-152 model trained on the 254 classes training set. For this and the following runs, predictions of unlikely species regarding recording location were set to zero. The 4 th run used the ensemble of run 3 plus an additional multi-label classification model (M7) trained on the 78 classes set. This 3 model ensemble obtained the highest retrieval mAP of 74.6 % on the test set. The ensemble of run 5 consists of all previous models (except M1) plus an additional 254 classes ResNet-152 model (M6) yielding a higher temporal resolution of spectrogram image inputs. It mainly differs in the following parameters:  FFT size: 512 (instead of 1536) samples  FFT hop length: 256 (instead of 360) samples  Chunk duration: 2.6 (instead of 5.0) seconds  Duration jitter: 0.2 (instead of 0.45) seconds  Number of mel bands: 155 (instead of 310)  Start frequency: 0 (instead of 160) Hz  End frequency: 11025 (instead of 10300) Hz  Local time stretch chance: 40 (instead of 50) %  Local time stretch factor min.: 0.95 (instead of 0.9)  Local time stretch factor max.: 1.05 instead of 1.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,153.74,334.94,284.03,149.57"><head>Table 1 .</head><label>1</label><figDesc>Influence of data augmentation on model performance.</figDesc><table coords="6,153.74,354.50,284.03,130.01"><row><cell>ID Description</cell><cell>InceptionV3</cell><cell>ResNet-152</cell></row><row><cell></cell><cell>c-mAP [%]</cell><cell>c-mAP [%]</cell></row><row><cell>E1 Baseline</cell><cell>26.5</cell><cell>21.1</cell></row><row><cell>E2 E1 with BAD noise augmentation</cell><cell>40.1</cell><cell>38.3</cell></row><row><cell>E3 E2 with validation files for training</cell><cell>42.5</cell><cell>38.7</cell></row><row><cell>E4 E3 with validation noise augmentation</cell><cell>50.9</cell><cell>51.3</cell></row><row><cell>E5 E4 with 2019 augmentation methods</cell><cell>53.1</cell><cell>52.2</cell></row><row><cell>E6 E4 with multi-label training 2</cell><cell>49.7</cell><cell>52.9</cell></row><row><cell>E7 E6 with 2019 augmentations (post</cell><cell>50.1</cell><cell>54.7</cell></row><row><cell>challenge result)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.70,149.99,416.83,320.34"><head>Table 2 .</head><label>2</label><figDesc>Main properties of models used for submitted runs. In order to better compare results and to find out if and how much progress was made on identification performance since last year, the best performing model of the 2018 BirdCLEF edition<ref type="bibr" coords="7,240.41,425.37,16.76,8.96" target="#b9">[13]</ref> was retrained on this year's data set. All files from the Xeno-Canto training set but no validation soundscapes were used for training. The model obtained a classification mAP of 21.3 % and a retrieval mAP of 44.7 % on the test set.</figDesc><table coords="7,88.70,169.55,416.83,240.78"><row><cell>Model ID</cell><cell>M1</cell><cell>M2</cell><cell>M3</cell><cell>M4</cell><cell>M5</cell><cell>M6</cell><cell>M7</cell><cell>M8</cell></row><row><cell>Included in run</cell><cell>1</cell><cell>2-10</cell><cell>6-10</cell><cell>3,4</cell><cell>5-10</cell><cell>5-10</cell><cell>4-10</cell><cell>6-10</cell></row><row><cell>Number of classes</cell><cell>659</cell><cell>659</cell><cell>254</cell><cell>254</cell><cell>254</cell><cell>254</cell><cell>78</cell><cell>78</cell></row><row><cell>Network type</cell><cell cols="2">Incept. Incept.</cell><cell>Incept.</cell><cell cols="5">ResNet ResNet ResNet ResNet ResNet</cell></row><row><cell>Input size [pixel]</cell><cell>299</cell><cell>299</cell><cell>299</cell><cell>224</cell><cell>224</cell><cell>224</cell><cell>224</cell><cell>224</cell></row><row><cell cols="2">Chunk duration [s] 5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>2.6</cell><cell>5</cell><cell>5</cell></row><row><cell>Label type</cell><cell cols="2">single single</cell><cell>single</cell><cell>single</cell><cell>single</cell><cell>single</cell><cell>multi</cell><cell>multi</cell></row><row><cell>Loss function</cell><cell>Cross</cell><cell>Cross</cell><cell>Cross</cell><cell>Cross</cell><cell>Cross</cell><cell>Cross</cell><cell>Multi</cell><cell>BCEWit</cell></row><row><cell></cell><cell>entropy</cell><cell>entropy</cell><cell>entropy</cell><cell>entropy</cell><cell>entropy</cell><cell>entropy</cell><cell>Label</cell><cell>hLogits</cell></row><row><cell>BAD data used</cell><cell>no</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell></row><row><cell cols="2">Val. data (labelled) no</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell></row><row><cell>Val. data (noise)</cell><cell>no</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell></row><row><cell cols="2">New augmentations no</cell><cell>no</cell><cell>no</cell><cell>no</cell><cell>yes</cell><cell>no</cell><cell>no</cell><cell>no</cell></row><row><cell cols="2">Val. set c-mAP [%] 29.7</cell><cell>49.1</cell><cell>51.3</cell><cell>51.2</cell><cell>53.9</cell><cell>52.1</cell><cell>53.8</cell><cell>49.1</cell></row><row><cell cols="2">Test set c-mAP [%] 21.3</cell><cell>25.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Run 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,163.58,516.28,268.10,167.10"><head>Table 3 .</head><label>3</label><figDesc>Official scores on the BirdCLEF 2019 test set (for selected runs). increase was obtained by adding random background noise from other and/or similar habitats. A very good source for noise augmentation is the data set of the DCASE 2018 Bird Audio Detection challenge (E1 vs. E2 in Table1). It is easily available and published under the Creative Commons Attribution licence CC-BY 4.0[17]. The BAD recordings cover a wide range of background noise and atmosphere from a diverse set of different monitoring scenarios and are therefore well suited to improve model generalization. On the other hand, in cases where the target monitoring location is known in advance, a model can specifically be designed for a certain habitat and greatly benefit by using background sounds of this particular recording location for noise augmentation during training (E3 vs. E4 in Table</figDesc><table coords="8,171.50,535.84,249.85,147.54"><row><cell cols="3">Run #Models #Snap-</cell><cell>c-mAP [%]</cell><cell>c-mAP [%]</cell><cell>r-mAP [%]</cell></row><row><cell></cell><cell></cell><cell>shots</cell><cell>Val. set</cell><cell>Test set</cell><cell>Test set</cell></row><row><cell>1</cell><cell>1</cell><cell>1</cell><cell>29.7</cell><cell>21.3</cell><cell>44.7</cell></row><row><cell>2</cell><cell>1</cell><cell>1</cell><cell>49.1</cell><cell>25.9</cell><cell>69.1</cell></row><row><cell>3</cell><cell>2</cell><cell>2</cell><cell>57.5</cell><cell>29.7</cell><cell>71.0</cell></row><row><cell>4</cell><cell>3</cell><cell>3</cell><cell>62.0</cell><cell>30.9</cell><cell>74.6</cell></row><row><cell>5</cell><cell>4</cell><cell>4</cell><cell>63.7</cell><cell>35.6</cell><cell>71.5</cell></row><row><cell>7</cell><cell>7</cell><cell>9</cell><cell>64.9</cell><cell>34.9</cell><cell>73.2</cell></row><row><cell>8</cell><cell>9</cell><cell>12</cell><cell>-</cell><cell>35.1</cell><cell>74.4</cell></row><row><cell>9</cell><cell>9</cell><cell>12</cell><cell>-</cell><cell>32.8</cell><cell>71.1</cell></row><row><cell>10</cell><cell>all</cell><cell>all</cell><cell>-</cell><cell>35.5</cell><cell>72.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,136.10,686.23,316.95,8.10"><p>8 files of the Xeno-Canto training set are excluded because they are corrupt or too small</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,136.10,686.23,184.90,8.10"><p>Loss function: torch.nn.MultiLabelSoftMarginLoss</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. I would like to thank <rs type="person">Stefan Kahl</rs>, <rs type="person">Alexis Joly</rs>, <rs type="person">Hervé Goëau</rs>, <rs type="person">Willem-Pier Vellinga</rs> and <rs type="person">Hervé Glotin</rs> for organising this task, <rs type="person">Xeno-Canto</rs> for providing the training data and the <rs type="institution">Cornell Lab of Ornithology</rs> and <rs type="person">Paula Caycedo</rs> for providing and annotating the soundscapes. I also want to thank the <rs type="institution">Museum für Naturkunde</rs> for supporting my research.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,132.67,254.99,337.67,8.10;10,141.74,266.03,328.78,8.10;10,141.74,277.10,18.05,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,422.84,254.99,47.50,8.10;10,141.74,266.03,227.58,8.10">Overview of BirdCLEF 2019: large-scale bird recognition in soundscapes</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,391.46,266.03,79.07,8.10">CLEF working notes</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,288.02,337.62,8.10;10,141.74,299.06,329.10,8.10;10,141.74,310.10,328.64,8.10;10,141.74,321.02,51.97,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,295.22,299.06,175.62,8.10;10,141.74,310.10,265.92,8.10">Overview of LifeCLEF 2019: Identification of Amazonian Plants, South &amp; North American Birds, and Niche Prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,426.35,310.10,44.03,8.10;10,141.74,321.02,31.61,8.10">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,332.06,259.28,8.10" xml:id="b2">
	<monogr>
		<ptr target="http://sox.sourceforge.net/" />
		<title level="m" coord="10,141.74,332.06,54.78,8.10">SoX Homepage</title>
		<imprint>
			<date type="published" when="2019-06-19">2019/06/19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,343.10,337.87,8.10;10,141.74,354.02,328.60,8.10;10,141.74,365.06,329.12,8.10;10,141.74,376.10,161.82,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,307.40,343.10,146.96,8.10">On the Stratification of Multi-Label Data</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sechidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,384.18,354.02,86.16,8.10;10,141.74,365.06,205.74,8.10">Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2011</title>
		<title level="s" coord="10,354.17,365.06,116.68,8.10;10,141.74,376.10,14.95,8.10">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Malerba</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6913</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,387.02,296.22,8.10" xml:id="b4">
	<monogr>
		<ptr target="https://www.xeno-canto.org/,lastaccessed2019/06/19" />
		<title level="m" coord="10,141.74,387.02,82.74,8.10">Xeno-Canto Homepage</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,398.06,249.09,8.10" xml:id="b5">
	<monogr>
		<ptr target="https://ebird.org/home,lastaccessed2019/06/19" />
		<title level="m" coord="10,141.74,398.06,58.80,8.10">eBird Homepage</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,409.10,338.07,8.10;10,141.74,420.02,253.67,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,216.70,409.10,184.89,8.10">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,420.40,409.10,50.34,8.10;10,141.74,420.02,179.79,8.10">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,431.06,273.22,8.10;10,129.26,442.10,6.81,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,223.75,431.06,132.15,8.10">Automatic differentiation in PyTorch</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,374.68,431.06,31.21,8.10;10,129.26,442.10,3.41,8.10">NIPS-W 9</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,141.74,442.10,328.85,8.10;10,141.74,453.02,144.78,8.10" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,351.39,442.10,64.97,8.10">librosa/librosa: 0</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Balke</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.2564164</idno>
		<ptr target="https://doi.org/10.5281/zenodo.2564164" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,497.08,338.46,8.10;10,141.74,508.12,170.97,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,210.41,497.08,260.45,8.10;10,141.74,508.12,44.70,8.10">Audio-based Bird Species Identification with Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,205.23,508.12,87.12,8.10">Working notes of CLEF</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,519.04,338.22,8.10;10,141.74,530.08,268.50,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,342.10,519.04,128.52,8.10;10,141.74,530.08,32.67,8.10">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,192.55,530.08,193.64,8.10">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,541.12,338.34,8.10;10,141.74,552.04,234.75,8.10" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,266.64,541.12,204.09,8.10;10,141.74,552.04,150.01,8.10">Mixup-Based Acoustic Scene Classification Using Multi-Channel Convolutional Neural Network</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mi</forename><forename type="middle">H</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07319</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,562.99,338.34,8.19;10,141.74,574.12,328.66,8.10;10,141.74,585.04,92.26,8.10;10,124.82,596.08,344.19,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,374.99,562.99,95.74,8.18;10,141.74,574.12,278.20,8.10">Automatic acoustic detection of birds through deep learning: the first Bird Audio Detection challenge</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Pamuła</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<ptr target="http://dcase.community/challenge2018/task-bird-audio-detection,lastaccessed2019/06/19" />
	</analytic>
	<monogr>
		<title level="m" coord="10,438.97,574.12,31.44,8.10;10,141.74,585.04,92.26,8.10">Methods in Ecology and Evolution</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,607.12,338.10,8.10;10,141.74,618.04,168.40,8.10" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,295.77,607.12,174.73,8.10;10,141.74,618.04,43.32,8.10">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,132.40,629.08,338.03,8.10;10,141.74,640.12,45.99,8.10" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,281.47,629.08,172.37,8.10">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,141.74,640.12,20.71,8.10">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,651.04,338.09,8.10;10,141.74,662.11,328.65,8.10;10,141.74,673.15,328.85,8.10;10,141.74,684.07,45.06,8.10" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,210.06,651.04,243.85,8.10">Acoustic Bird Detection with Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,243.51,662.11,226.88,8.10;10,141.74,673.15,131.58,8.10">Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</editor>
		<meeting>the Detection and Classification of Acoustic Scenes and Events 2018 Workshop<address><addrLine>DCASE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="143" to="147" />
		</imprint>
		<respStmt>
			<orgName>Tampere University of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
