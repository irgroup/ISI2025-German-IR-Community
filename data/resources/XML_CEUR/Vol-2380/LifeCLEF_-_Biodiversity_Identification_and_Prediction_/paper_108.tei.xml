<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.81,116.04,345.74,13.99;1,136.60,133.97,342.16,13.99;1,198.10,160.90,219.15,11.66">Recognition of the Amazonian flora by Inception Networks with Test-time Class Prior Estimation CMP submission to PlantCLEF 2019</title>
				<funder ref="#_6jcZ9hk">
					<orgName type="full">Center for Informatics</orgName>
				</funder>
				<funder ref="#_3VKUFDD">
					<orgName type="full">UWB</orgName>
				</funder>
				<funder ref="#_QWtMmUP">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,198.02,196.92,52.91,9.69"><forename type="first">Lukáš</forename><surname>Picek</surname></persName>
							<email>picekl@kky.zcu.cz</email>
							<idno type="ORCID">0000-0002-6041-9722</idno>
							<affiliation key="aff0">
								<address>
									<country>of</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.45,196.92,46.78,9.69"><forename type="first">Milan</forename><surname>Šulc</surname></persName>
							<email>sulcmila@cmp.felk.cvut.cz</email>
							<idno type="ORCID">0000-0002-6321-0131</idno>
							<affiliation key="aff0">
								<address>
									<country>of</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.13,196.92,44.78,9.69"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
							<email>matas@cmp.felk.cvut.cz</email>
							<idno type="ORCID">0000-0003-0863-4844</idno>
							<affiliation key="aff0">
								<address>
									<country>of</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.81,116.04,345.74,13.99;1,136.60,133.97,342.16,13.99;1,198.10,160.90,219.15,11.66">Recognition of the Amazonian flora by Inception Networks with Test-time Class Prior Estimation CMP submission to PlantCLEF 2019</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DFAFA40ACFAA6D24618BB3C54EE1F21C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Plant Recognition</term>
					<term>Computer Vision</term>
					<term>Convolutional Neural Networks</term>
					<term>Machine Learning</term>
					<term>Class Prior Estimation</term>
					<term>Fine-grained</term>
					<term>Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper describes an automatic system for recognition of 10,000 plant species, with focus on species from the Guiana shield and the Amazon rain forest. The proposed system achieves the best results on the PlantCLEF 2019 test set with 31.9% accuracy. Compared against human experts in plant recognition, the system performed better than 3 of the 5 participating human experts and achieved 41.0% accuracy on the subset for expert evaluation. The proposed system is based on the Inception-v4 and Inception-ResNet-v2 Convolutional Neural Network (CNN) architectures. Performance improvements were achieved by: adjusting the CNN predictions according to the estimated change of the class prior probabilities, replacing network parameters with their running averages, testtime data augmentation, filtering the provided training set and adding additional training images from GBIF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The paper describes an automatic system for visual recognition of plants among 10,000 species, developed for the the PlantCLEF 2019 plant identification challenge <ref type="bibr" coords="1,160.75,568.41,10.52,9.69" target="#b3">[4]</ref> organized in connection with the LifeCLEF 2019 workshop <ref type="bibr" coords="1,439.66,568.41,10.52,9.69" target="#b4">[5]</ref> at the Conference and Labs of the Evaluation Forum. Compared to previous Plant-CLEF challenges <ref type="bibr" coords="1,213.09,592.32,10.80,9.69" target="#b0">[1,</ref><ref type="bibr" coords="1,223.89,592.32,7.20,9.69" target="#b1">2,</ref><ref type="bibr" coords="1,231.09,592.32,7.20,9.69" target="#b2">3]</ref>, which contained mainly species living in Europe and North America, the 2019 task is focused on the recognition of species from "data deficient regions" -mainly the Guiana shield and the Amazon rain forest.</p><p>The proposed approach is based on CMP's winning submission to PlantCLEF 2018 <ref type="bibr" coords="2,158.60,130.95,14.62,9.69" target="#b10">[11]</ref>. Checkpoints of our models from PlantCLEF 2018 have been shared with other participants of PlantCLEF 2019 in order to provide a good starting point to all participants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Cleaning and extending the training dataset</head><p>The PlantCLEF 2019 training set covers 10,000 species and consists of:    A brief manual inspection showed that the provided training set is afflicted with noisy samples -wrongly labeled images, including images of non-flora objects. Examples of noisy samples are in Figure <ref type="figure" coords="3,337.37,343.42,3.88,9.69" target="#fig_2">3</ref>. We therefore decided to detect non-flora images by a pre-trained Darknet53 448x448 <ref type="bibr" coords="3,363.54,355.38,10.52,9.69" target="#b7">[8]</ref> classifier. Out of 428,702 images from the official training set, we removed 6,181 images detected as nonflora, After that our training data missed approximately 2000 classes, so we had to gather additional training images to fill that gap. We created a new training set<ref type="foot" coords="3,163.71,401.63,3.97,6.80" target="#foot_1">4</ref> including external training data downloaded from GBIF<ref type="foot" coords="3,417.41,401.63,3.97,6.80" target="#foot_2">5</ref> , described in Table <ref type="table" coords="3,162.16,415.16,3.88,9.69" target="#tab_3">1</ref>. Changes in the dataset statistics are visualized in Figure <ref type="figure" coords="3,423.91,415.16,3.88,9.69" target="#fig_3">4</ref>. To make sure that none of the additional training images (or its resized or cropped versions) downloaded from GBIF appear in the test set, we used the image retrieval pipeline of Radenovic et al. <ref type="bibr" coords="3,321.74,599.07,10.52,9.69" target="#b6">[7]</ref> with VGG-16 and whitening. The  nearest neighbours of test images among the downloaded images are vizualized in Figure <ref type="figure" coords="4,177.83,482.53,3.88,9.69" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Convolutional Neural Networks</head><p>The proposed system is based on two CNN architectures -Inception ResNet v2 and Inception v4 <ref type="bibr" coords="4,230.41,544.98,14.62,9.69" target="#b11">[12]</ref>. The TensorFlow-Slim API was used to adjust and fine-tune the networks from the publicly available 6 PlantCLEF 2018 winning checkpoints.</p><p>All networks in our experiments shared the optimizer settings enumerated in Table <ref type="table" coords="4,162.16,592.87,3.88,9.69" target="#tab_4">2</ref>. The networks and their input resolutions are listed in Table <ref type="table" coords="4,437.58,592.87,3.88,9.69" target="#tab_5">3</ref>.</p><p>The following image pre-processing techniques were used for training:</p><p>• Random image crop with aspect ratio range (0.75, 1.33) and content at least 80% of origin image.  • Random left-right flip.</p><p>• Brightness and saturation distortion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Test time data augmentation</head><p>At test-time, 3 predictions per image are generated by using 3 crops:</p><p>• 1x Full image,</p><p>• 1x Central crop covering 80% of the original image,</p><p>• 1x Central crop covering 60% of the original image.</p><p>In submissions 4,5,6,7 the mirrored versions of all three crops were also evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Adjusting Class Priors at Test Time</head><p>The training set data distribution is highly unbalanced and we can not guarantee that the test images were drawn from the same distribution: as described in Section 2.1, the training set comes from different sources, where the class frequencies may not correspond with the test-time priors. Following the notation of <ref type="bibr" coords="6,263.50,254.89,14.62,9.69" target="#b9">[10]</ref>, the predictions p(c k |x i ) of a network trained on a dataset with class prior probabilities p(c k ) should be corrected in case of evaluation on a test set with different class priors p e (c k ):</p><formula xml:id="formula_0" coords="6,204.24,304.46,276.35,51.00">p e (c k |x i ) = p(c k |x i ) p e (c k ) p(c k ) K j=1 p(c j |x i ) p e (c j ) p(c j ) ∝ p(c k |x i ) p e (c k ) p(c k )<label>(1)</label></formula><p>Since the test-time priors p e (c j ) are unknown, we propose three different estimates of adjusting the predictions: UNIFORM: As the simplest option, we adjust the test predictions by assuming a uniform prior for all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLE:</head><p>As the second option, we compute a Maximum Likelihood Estimate of the test time prior p e (c k ) using the EM algorithm of Saerens et al. <ref type="bibr" coords="6,417.70,478.08,9.97,9.69" target="#b8">[9]</ref>, comprising of the following two steps:</p><formula xml:id="formula_1" coords="6,226.54,515.50,254.05,56.06">E: p (s) e (c k |x i ) = p(c k |x i ) p (s) e (c k ) p(c k ) K j=1 p(c j |x i ) p (s) e (c j ) p(c j )<label>(2)</label></formula><formula xml:id="formula_2" coords="6,231.69,588.39,248.90,30.32">M: p (s+1) e (c k ) = 1 N N i=1 p (s) e (c k |x i )<label>(3)</label></formula><p>MAP: As the third option, we use the Maximum a Posteriori estimate proposed in <ref type="bibr" coords="6,146.39,656.12,14.62,9.69" target="#b9">[10]</ref>:</p><formula xml:id="formula_3" coords="7,215.47,127.00,265.12,123.81">P MAP = arg max P p(P|(x 1 , .., x N )) = arg max P p(P) N i=1 p(x i |P) = arg max P log p(P) + N i=1 log p(x i |P) s.t. K k=1 P k = 1; ∀k : P k ≥ 0<label>(4)</label></formula><p>We model the prior knowledge about the categorical distribution p e (c k ) by the symmetric Dirichlet distribution:</p><formula xml:id="formula_4" coords="7,257.70,289.61,222.89,30.55">p(P) = 1 B(α) K k=1 P α-1 k (5)</formula><p>where the normalization factor for the symmetric case is</p><formula xml:id="formula_5" coords="7,396.02,325.58,66.25,23.89">B(α) = Γ(α) K Γ(αK)</formula><p>. As in <ref type="bibr" coords="7,146.39,351.69,14.62,9.69" target="#b9">[10]</ref>, we use α = 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Table <ref type="table" coords="7,161.73,632.21,4.98,9.69" target="#tab_6">4</ref> describes eight final runs used for the evaluation. An ensemble of all five networks from Section 2.2 was used in all runs and predictions were averaged over all networks and all test image augmentations from Section 2.3. The evaluation results are shown in Figures <ref type="figure" coords="8,348.70,273.54,7.75,9.69" target="#fig_0">1,</ref><ref type="figure" coords="8,356.45,273.54,3.88,9.69" target="#fig_5">6</ref>. From the class prior estimation methods, MAP estimation with the Dirichlet hyperprior achieves the best results. This corresponds to the results of <ref type="bibr" coords="8,336.52,297.45,14.62,9.69" target="#b9">[10]</ref>, where adding the hyperprior brought noticeable improvement over the MLE estimation, which may have a tendency to overfit. Note that the results from Table <ref type="table" coords="8,379.88,321.36,4.98,9.69" target="#tab_6">4</ref> are the official postchallenge evaluation not included in the challenge leaderboard, as our predictions were wrongly exported into the challenge run-files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>The proposed system achieves the best accuracy on the PlantCLEF 2019 test set -31.9% on the full set and 41.0% on the test subset for plant identification experts. The results show that even for "data-deficient" plant species, automatic image recognition systems achieve human expert accuracy in visual recognition of plants: The proposed method performed better than 3 of the 5 participating experts in plant recognition. Although the results are promising, there are many opportunities for further improvement of automatic plant recognition systems for data-deficient species, such as one-shot learning and open long-tailed recognition <ref type="bibr" coords="8,134.77,500.50,10.52,9.69" target="#b5">[6]</ref> methods.</p><p>The increasing precision of the automated plant recognition methods should allow for a better assistance to both nature lovers and biological experts in the fields. For example, showing a shortlist of potential species candidates can decrease the time needed for decision and potentially increase the recognition rate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,385.21,345.82,8.83;2,134.77,396.20,345.82,8.74;2,134.77,407.16,274.64,8.74;2,134.77,186.58,345.84,183.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparison of automatic plant recognition methods against human experts. The results of our method are shown in red as "Post Challenge" (our results submitted at the challenge deadline, shown in orange, were wrongly exported).</figDesc><graphic coords="2,134.77,186.58,345.84,183.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,271.96,345.82,8.83;3,134.77,282.94,50.50,8.74;3,138.54,182.03,65.20,65.19"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Randomly selected images from the LifeCLEF 2019 training set (top) and test set(bottom).</figDesc><graphic coords="3,138.54,182.03,65.20,65.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,150.42,539.00,314.51,8.83;3,206.81,449.07,65.20,65.20"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Randomly selected noisy images from the LifeCLEF 2019 training set.</figDesc><graphic coords="3,206.81,449.07,65.20,65.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,134.77,316.24,345.83,8.83;4,134.77,327.23,345.83,8.74"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Numbers of training images per class in the original dataset (blue), cleaned dataset (orange) and cleaned and extended (green), sorted for each dataset separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,147.32,285.64,320.71,8.83"><head>6Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Six nearest couples of test set images (top) and GBIF images (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,134.77,548.07,345.82,8.83;7,134.77,559.06,271.48,8.74;7,106.42,382.86,380.41,150.44"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison of automatic plant recognition methods on the PlantCLEF 2019 test set. "Post Challenge" submissions are marked with red border.</figDesc><graphic coords="7,106.42,382.86,380.41,150.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,143.05,530.17,337.53,21.65"><head>•</head><label></label><figDesc>PlantCLEF 2019 EOL: 72,260 images covering 4,197 classes from the Encyclopedia of Life</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,134.77,348.25,345.82,95.69"><head>Table 1 .</head><label>1</label><figDesc>Training data (after cleaning and extending the provided training set) used in the experiments.</figDesc><table coords="4,170.22,379.60,271.83,64.33"><row><cell>Data Source</cell><cell cols="3">Classes Non EOL classes Number of Images</cell></row><row><cell>EOL</cell><cell>4197</cell><cell>0</cell><cell>58548</cell></row><row><cell>Noisy Google</cell><cell>6262</cell><cell>3800</cell><cell>64863</cell></row><row><cell>Noisy Bing</cell><cell>8666</cell><cell>5069</cell><cell>305291</cell></row><row><cell>GBIF (additional)</cell><cell>9402</cell><cell>5734</cell><cell>238009</cell></row><row><cell>All</cell><cell>9998</cell><cell>5801</cell><cell>666711</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,139.06,306.13,337.25,106.35"><head>Table 2 .</head><label>2</label><figDesc>Optimizer hyper-parameters, common to all networks in the experiments.</figDesc><table coords="5,206.00,326.63,200.29,85.85"><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Batch size</cell><cell>32</cell></row><row><cell>Optimizer</cell><cell>rmsprop</cell></row><row><cell>RMSProp momentum</cell><cell>0.9</cell></row><row><cell>RMSProp decay</cell><cell>0.9</cell></row><row><cell>Initial learning rate</cell><cell>0.0075</cell></row><row><cell>Learning rate decay type</cell><cell>Exponential (stairs)</cell></row><row><cell>Learning rate decay factor</cell><cell>0.975</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,172.21,491.45,270.95,84.43"><head>Table 3 .</head><label>3</label><figDesc>Networks and hyper-parameters used in the experiments.</figDesc><table coords="5,199.48,511.95,213.32,63.94"><row><cell># Net architecture</cell><cell>Input Resolution</cell></row><row><cell>1 Inception v4</cell><cell>299 × 299</cell></row><row><cell>2 Inception v4 (second)</cell><cell>299 × 299</cell></row><row><cell>3 Inception v4</cell><cell>598 × 598</cell></row><row><cell>4 Inception ResNet v2</cell><cell>299 × 299</cell></row><row><cell>5 Inception ResNet v2 (second)</cell><cell>299 × 299</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,142.81,116.41,329.73,129.53"><head>Table 4 .</head><label>4</label><figDesc>Description of our (corrected/post-challenge) submissions.</figDesc><table coords="8,142.81,137.71,329.73,108.23"><row><cell></cell><cell>Run description</cell><cell></cell><cell></cell><cell cols="2">Test accuracy</cell><cell></cell></row><row><cell>Name</cell><cell cols="6">Test-time augm. Prior est. Top1 Top1 Exp. Top5 All Top5 Exp.</cell></row><row><cell>CMP Run 2</cell><cell>3×scale</cell><cell cols="2">(none) 0,244</cell><cell>0,325</cell><cell>0,356</cell><cell>0,410</cell></row><row><cell>CMP Run 3</cell><cell>3×scale</cell><cell cols="2">uniform 0,247</cell><cell>0,316</cell><cell>0,360</cell><cell>0,419</cell></row><row><cell>CMP Run 4</cell><cell>3×scale</cell><cell>MAP</cell><cell>0,301</cell><cell>0,402</cell><cell>0,453</cell><cell>0,573</cell></row><row><cell>CMP Run 5</cell><cell>3×scale</cell><cell>MLE</cell><cell>0,307</cell><cell>0,402</cell><cell>0,451</cell><cell>0,573</cell></row><row><cell cols="4">CMP Run 6 3×scale + mirrors (none) 0,311</cell><cell>0,402</cell><cell>0,454</cell><cell>0,538</cell></row><row><cell cols="4">CMP Run 7 3×scale + mirrors uniform 0,311</cell><cell>0,410</cell><cell>0,461</cell><cell>0,564</cell></row><row><cell cols="3">CMP Run 4* 3×scale + mirrors MAP</cell><cell>0,319</cell><cell>0,402</cell><cell>0,468</cell><cell>0,581</cell></row><row><cell cols="3">CMP Run 5* 3×scale + mirrors MLE</cell><cell cols="2">0,319 0,410</cell><cell>0,470</cell><cell>0,581</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,657.44,84.73,8.33"><p>http://www.eol.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="3,144.73,623.81,335.86,8.74;3,144.73,634.77,335.86,8.97;3,144.73,646.37,61.20,8.33"><p>For full reproducibility, a list of removed samples as well as an archive with additional training images are shared at http://cmp.felk.cvut.cz/~sulcmila/ LifeCLEF2019/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="3,144.73,657.44,94.15,8.33"><p>http://www.gbif.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>LP was supported by the <rs type="funder">UWB</rs> project No. <rs type="grantNumber">SGS-2019-027</rs>. MŠ and JM were supported by <rs type="grantNumber">OP VVV</rs> project <rs type="grantNumber">CZ.02.1.01/0.0/0.0/16019/000076 Research</rs> <rs type="funder">Center for Informatics</rs>. We'd like to thank <rs type="person">Tomáš Jeníček</rs> for his assistance with the image retrieval pipeline in Section 2.1.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3VKUFDD">
					<idno type="grant-number">SGS-2019-027</idno>
				</org>
				<org type="funding" xml:id="_QWtMmUP">
					<idno type="grant-number">OP VVV</idno>
				</org>
				<org type="funding" xml:id="_6jcZ9hk">
					<idno type="grant-number">CZ.02.1.01/0.0/0.0/16019/000076 Research</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,142.59,337.63,8.74;9,151.53,153.55,147.93,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,277.67,142.59,198.82,8.74">Plant identification in an open-world (lifeclef 2016)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,165.61,153.55,83.65,8.74">CLEF working notes</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,164.51,337.63,8.74;9,151.53,175.47,329.06,8.74;9,151.53,186.42,25.61,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,283.05,164.51,197.54,8.74;9,151.53,175.47,181.00,8.74">Plant identification based on noisy web data: the amazing performance of deep learning (lifeclef</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,361.39,175.47,119.20,8.74">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,197.38,337.63,8.74;9,151.53,208.34,329.06,8.74;9,151.53,219.30,25.61,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,279.01,197.38,201.57,8.74;9,151.53,208.34,201.94,8.74">Overview of expertlifeclef 2018: how far automated identification systems are from the best experts?</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,372.55,208.34,108.04,8.74">CLEF working notes 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,230.26,337.62,8.74;9,151.53,241.22,327.79,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,281.94,230.26,198.64,8.74;9,151.53,241.22,172.92,8.74">Overview of lifeclef plant identification task 2019: diving into data deficient tropical countries</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,345.48,241.22,105.16,8.74">CLEF working notes 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,252.18,337.63,8.74;9,151.53,263.14,329.06,8.74;9,151.53,274.10,329.06,8.74;9,151.53,285.05,183.93,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,376.87,263.14,103.71,8.74;9,151.53,274.10,329.06,8.74;9,151.53,285.05,26.45,8.74">Overview of lifeclef 2019: Identification of amazonian plants, south &amp; north american birds, and niche prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,198.92,285.05,107.86,8.74">Proceedings of CLEF 2019</title>
		<meeting>CLEF 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,296.01,337.63,8.74;9,151.53,306.97,329.06,8.74;9,151.53,317.93,219.52,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,390.11,296.01,90.47,8.74;9,151.53,306.97,110.93,8.74">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,282.11,306.97,198.47,8.74;9,151.53,317.93,126.21,8.74">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,328.89,337.62,8.74;9,151.53,339.85,329.07,8.74;9,151.53,350.81,25.61,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,312.16,328.89,168.42,8.74;9,151.53,339.85,71.61,8.74">Fine-tuning cnn image retrieval with no human annotation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,229.66,339.85,250.93,8.74">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,361.77,337.63,8.97;9,151.53,372.73,89.39,8.97" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<ptr target="http://pjreddie.com/darknet/(2013-2016" />
		<title level="m" coord="9,204.37,361.77,175.23,8.74">Darknet: Open source neural networks in c</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,383.68,337.62,8.74;9,151.53,394.62,329.05,8.83;9,151.53,405.60,25.61,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,325.68,383.68,154.90,8.74;9,151.53,394.64,184.81,8.74">Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Latinne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Decaestecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,344.17,394.64,81.20,8.74">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="41" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,416.56,337.96,8.74;9,151.53,427.52,142.94,8.74" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08235v2</idno>
		<title level="m" coord="9,232.66,416.56,219.00,8.74">Improving cnn classifiers by estimating test-time priors</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.62,438.48,337.97,8.74;9,151.53,449.44,261.31,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,276.59,438.48,203.99,8.74;9,151.53,449.44,105.74,8.74">Plant recognition by inception networks with testtime class prior estimation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,264.40,449.44,98.25,8.74">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,460.40,337.97,8.74;9,151.53,471.36,329.06,8.74;9,151.53,482.31,146.26,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,360.34,460.40,120.25,8.74;9,151.53,471.36,200.57,8.74">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,372.46,471.36,108.13,8.74;9,151.53,482.31,117.58,8.74">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
