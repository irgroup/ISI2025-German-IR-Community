<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.47,115.96,326.40,12.62;1,247.25,133.89,120.85,12.62">Inception-v3 Based Method of LifeCLEF 2019 Bird Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,218.27,171.56,50.42,8.74"><forename type="first">Jisheng</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,279.25,171.56,54.10,8.74"><forename type="first">Bolun</forename><surname>Wang</surname></persName>
							<email>blwang@mail.nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.91,171.56,48.71,8.74"><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<email>chenjf@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.80,183.51,62.40,8.74"><forename type="first">Jianfeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.14,183.51,62.96,8.74"><forename type="first">Zhong-Hua</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.47,115.96,326.40,12.62;1,247.25,133.89,120.85,12.62">Inception-v3 Based Method of LifeCLEF 2019 Bird Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C0768D87E4CA4435F522AC873FA94B6D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bird sound classification</term>
					<term>Inception-v3</term>
					<term>Data augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a method of bird recognition based on Inception-v3. The goal of the LifeCLEF2019 Bird Recognition is to detect and classify 659 bird species within the provided soundscape recordings. Log-Mel spectrograms are extracted as features and Inception-v3 is used for bird sound detection. Some data augmentation techniques are applied to improve the robustness and generalization of the model. Finally, we evaluated our system in BirdCLEF test data and achieved 0.055 of classification mean average precision (c-mAP).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning is proven to outperform traditional methods in bird sound classification <ref type="bibr" coords="1,169.72,438.95,9.96,8.74" target="#b3">[5]</ref>. Convolutional neural networks(CNNs) architecture performs well on many computer vision tasks and the convergence of image-based architectures such as Inception-v4 can obtain best performance in sound classification or what ever the targeted domain <ref type="bibr" coords="1,247.73,474.81,9.96,8.74" target="#b4">[6]</ref>.</p><p>The training data of BirdCLEF2019 <ref type="bibr" coords="1,307.74,486.77,9.96,8.74" target="#b5">[7]</ref>, which is a sub task of LifeCLEF2019 <ref type="bibr" coords="1,134.77,498.72,10.52,8.74" target="#b0">[2]</ref> contains about 50,000 recordings taken from xeno-canto.org and covers 659 common species from North and South America <ref type="bibr" coords="1,353.99,510.68,9.96,8.74">[1]</ref>. More than 15 and up to 100 recordings are contained per species. And validate split contains 77 recordings. All recordings vary in quality, sampling rate and encoding. Each recording includes metadata providing information of location, latitude, longitude, etc.</p><p>To recognize 659 species and train such amount of recordings, we use Inception networks instead of shallow CNN architectures. As for features, we selected log-Mel spectrogram as input. Data augmentation methods are applied during the preprocessing. We use Ttensorflow to train model and python librosa library to calculate features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data preparation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Audio processing</head><p>To separate bird sound and background noise, similar method is applied. As it is presented in <ref type="bibr" coords="2,190.25,179.67,15.50,8.74" target="#b10">[12]</ref> and used in <ref type="bibr" coords="2,261.27,179.67,10.52,8.74" target="#b7">[9]</ref> and <ref type="bibr" coords="2,293.70,179.67,9.96,8.74" target="#b1">[3]</ref>, we refer to their methods and divide all recordings into 659 different bird song species and one total noise class. Details are described as following:</p><p>-Every recording is read in a sample rate of 44100Hz.</p><p>-Short-time Fourier transform(STFT) function is use to calculate spectrogram with a window length of 512 and hop length of 256.</p><p>-Then we calculate each row and column median, then we set every element in spectrogram to 1 if it is three times bigger than the median of its related row and column, otherwise its set to 0.</p><p>-Then we apply binary erosion and dilation to distinguish noise and signal part. The filter size is 4 by 4 square.</p><p>-Here we create a one-dimension vector named indicator vector, its i th element is set to 1 if its related column has at least one 1, or it is 0.</p><p>-Finally, we smooth the indicator vector twice by a dilation filter of size 4 by 1. And we use it as a mask to divide original bird recordings. Every recording can be divided into many signal and noise parts, all signal parts are concatenated as one and the same as noise.</p><p>We cut all recordings of every species into 5 seconds parts, because we would train model, predict validate and test data every 5 seconds. After all the steps, we can get 659 folders contain every species of 5s recordings and one noise folder of all the noise parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data augmentation</head><p>Data augmentation techniques are widely applied in last few years results. All recordings are resampled to 22050 Hz and then filtered by a high pass filter.Then some similar time and frequency augmentation methods used in <ref type="bibr" coords="2,415.70,595.89,10.52,8.74" target="#b7">[9]</ref> and <ref type="bibr" coords="2,448.61,595.89,15.50,8.74" target="#b10">[12]</ref> are described as following:</p><p>-Read a bird sound file from random position (it starts from beginning if it reach the end). 3 Network architecture</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transfer learning from Inception-v3</head><p>Inception-v3 is one of the state of art architectures in image classification challenge <ref type="bibr" coords="3,161.11,608.30,14.61,8.74" target="#b11">[13]</ref>. And it is confirmed that Inception-based convolutional neural networks on Mel spectrograms provide the best performance <ref type="bibr" coords="3,387.76,620.25,9.96,8.74" target="#b2">[4]</ref>. The best network for bird song detection seems to be the Inception-v3 architecture and it preforms better than even the more recent architectures <ref type="bibr" coords="3,339.86,644.16,14.61,8.74" target="#b9">[11]</ref>. So we selected Inception-v3 as our base model.</p><p>Inception models were fine-tuned using neural networks pre-trained on the Large Scale Visual Recognition Challenge (ILSVRC) <ref type="bibr" coords="4,334.49,130.95,15.50,8.74" target="#b8">[10]</ref> version of ImageNet, a dataset with almost 1.5 million photographs of 1000 object categories scraped from the web. As it is mentioned in <ref type="bibr" coords="4,259.47,154.86,9.96,8.74" target="#b6">[8]</ref>, strat training model with pre-trained weights can quickly train and get better performance. But if train model only with last classification layers can lead to worse result, also re-train the whole network cant reach the best performance either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training strategy</head><p>During the training, categorical cross entropy was used as loss function and stochastic gradient descent as optimizer with Nesterov momentum 0.9, weight decay of 1e-4 and a constant learning rate of 0.01. We generated 20 different folders as training data, every folder was augmented with different parameters. We trained these folders with a train batch of 72 and train random order for 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The evaluation metric is the classification mean Average Precision (c-mAP), considering each class c of the ground truth as a query. This means that for each class c, all predictions are extracted from the run file with ClassId(c), rank them by decreasing probability and compute the average precision for that class, which can be expressed as</p><formula xml:id="formula_0" coords="4,247.81,411.72,228.54,26.24">c -mAP = C c-1 AveP (c) C (<label>1</label></formula><formula xml:id="formula_1" coords="4,476.35,422.39,4.24,8.74">)</formula><p>where C is the number of species in the ground truth and AveP (c) is the average precision for a given species c computed as:</p><formula xml:id="formula_2" coords="4,238.17,471.89,242.42,26.33">AveP (c) = n k=1 P (k) × rel(k) n rel (c)<label>(2)</label></formula><p>where k is the rank of an item in the list of the predicted segments containing c, n is the total number of predicted segments containing c, P (k) is the precision at cut-off k in the list, rel(k) is an indicator function equaling 1 if the segment at rank k is a relevant one (i.e. is labeled as containing c in the ground truth) and n rel is the total number of relevant segments for c.</p><p>On the validation dataset, we selected max 100 probabilities and it got a c-mAP score of 0.088 and r-mAP (retrieval mean Average Precision) of 0.176. Meanwhile, the max 5 probabilities turned out to be 0.068 and 0.156.</p><p>-result0: Due to the limited time, it is a pity that we only submitted 1 run.</p><p>We predicted all the test data and selected max 5 probabilities per 5 seconds as final and the only one submission. Finally we got the 3 th rank among the teams and got a c-mAP score of 0.055 and r-mAP of 0.145. Details are showm in Table <ref type="table" coords="4,190.72,656.12,3.87,8.74">1</ref>.</p><p>-result1: We submitted another run after the deadline, and it got c-mAP of 0.065 and r-mAP of 0.164. This run contains max 100 probabilities in a 5-second period in 2.</p><p>Item c-Map r-Map Sapsucker 0.082 0.165 Columbia 0.094 0.156 Overall 0.065 0.164 Table <ref type="table" coords="5,266.30,386.19,4.13,7.89">2</ref>. Results of additional run</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and future work</head><p>We presented a system based on Inception model with some data augmentation techniques for bird recognition and got final c-mAP score of 0.055. And there is a 0.01 c-mAP score improvement of evaluating max 100 probabilities compared to max 5 probabilities in a 5-second sound. To handle more than 50,000 recordings, we selected Inception-v3 which has less parameters and greater feature extracted ability. During training, data augmentation methods were applied to prevent overfitting and improve generalization performance. Due to the limited time, we could not submit more results and compare the influence of different parameters or architectures. Ensemble of networks could significantly improve results, and it would be apply next year. We will also focus on the performance of CRNN and capsule network for bird recognition. Features can also have great impact on performance sometimes, and some unique data augmentation should be experimented to detect bird species. There is still a lot of room to improve in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,216.54,319.37,182.27,7.89;3,181.31,115.80,252.58,154.18"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of amerob sound signal part</figDesc><graphic coords="3,181.31,115.80,252.58,154.18" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We would like to thank <rs type="person">Stefan Kahl</rs>, <rs type="person">Herv Goau</rs>, <rs type="person">Alexis Joly</rs>, <rs type="person">Herve Glotin</rs> and <rs type="person">Willem-Pier Vellinga</rs> for organizing this task. I especially want to thank <rs type="person">Mario Lasseck</rs> for sharing his knowledge. I would also like to thank <rs type="institution">Northwestern Polytechnical University CIAIC</rs> for computer resource.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,253.32,337.64,7.86;6,151.52,264.28,329.07,7.86;6,151.52,275.24,238.42,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,442.93,253.32,37.66,7.86;6,151.52,264.28,329.07,7.86;6,151.52,275.24,80.84,7.86">Overview of lifeclef 2019: Identification of amazonian plants, south &amp; north american birds, and niche prediction</title>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Herv</forename><surname>Goau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B S K M S H G P B W P V R P F R S H M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,253.43,275.24,107.84,7.86">Proceedings of CLEF 2019</title>
		<meeting>CLEF 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,285.99,337.64,7.86;6,151.52,296.95,308.64,7.86" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fazeka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lidy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04448</idno>
		<title level="m" coord="6,339.48,285.99,141.11,7.86;6,151.52,296.95,142.85,7.86">A multi-modal deep neural network approach to bird-song identification</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,142.96,307.71,337.64,7.86;6,151.52,318.67,329.07,7.86;6,151.52,329.63,25.60,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,431.25,307.71,49.35,7.86;6,151.52,318.67,239.02,7.86">Overview of birdclef 2018: monophone vs. soundscape bird identification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">G</forename><surname>Herv Goau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">CLEF working notes</note>
</biblStruct>

<biblStruct coords="6,142.96,340.38,337.64,7.86;6,151.52,351.34,329.07,7.86;6,151.52,362.30,329.07,7.86;6,151.52,373.26,329.07,7.86;6,151.52,384.22,62.50,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,198.76,351.34,281.84,7.86;6,151.52,362.30,209.62,7.86">Overview of lifeclef 2018: a large-scale evaluation of species identification and recommendation algorithms in the era of ai</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,381.15,362.30,99.44,7.86;6,151.52,373.26,269.59,7.86">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="247" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,394.97,337.63,7.86;6,151.52,405.93,329.07,7.86;6,151.52,416.89,329.07,7.86;6,151.52,427.85,329.07,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,348.80,405.93,131.79,7.86;6,151.52,416.89,152.70,7.86">Lifeclef 2017 lab overview: multimedia species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,325.61,416.89,154.98,7.86;6,151.52,427.85,208.98,7.86">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="255" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,438.61,337.64,7.86;6,151.52,449.57,329.08,7.86;6,151.52,460.52,71.22,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,442.94,438.61,37.65,7.86;6,151.52,449.57,245.84,7.86">Overview of birdclef 2019: Large-scale bird recognition in soundscapes</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Stter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,420.31,449.57,60.29,7.86;6,151.52,460.52,42.54,7.86">CLEF working notes 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,471.28,337.63,7.86;6,151.52,482.24,329.07,7.86;6,151.52,493.20,226.90,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,204.17,471.28,258.12,7.86">Acoustic bird detection with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,151.52,482.24,329.07,7.86;6,151.52,493.20,40.26,7.86">Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop</title>
		<meeting>the Detection and Classification of Acoustic Scenes and Events 2018 Workshop<address><addrLine>DCASE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11">2018. November 2018</date>
			<biblScope unit="page" from="143" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,503.95,337.64,7.86;6,151.52,514.89,192.49,7.89" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,202.79,503.95,277.80,7.86;6,151.52,514.91,33.97,7.86">Audio-based bird species identification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,192.81,514.91,98.24,7.86">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,525.67,337.97,7.86;6,151.52,536.63,329.07,7.86;6,151.52,547.56,327.83,7.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,345.21,536.63,135.39,7.86;6,151.52,547.59,61.88,7.86">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,220.38,547.59,161.65,7.86">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,558.34,337.97,7.86;6,151.52,569.30,329.07,7.86;6,151.52,580.26,329.07,7.86;6,151.52,591.22,229.73,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,246.73,558.34,233.85,7.86;6,151.52,569.30,185.16,7.86">Audio bird classification with inception-v4 extended with time and time-frequency attention mechanisms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sevilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1866/paper177.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="6,357.36,569.30,123.23,7.86;6,151.52,580.26,184.33,7.86">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,601.98,337.97,7.86;6,151.52,612.93,236.19,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="6,357.14,601.98,123.45,7.86;6,151.52,612.93,160.42,7.86">Audio based bird species identification using deep learning techniques</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sprengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct coords="6,142.62,623.69,337.97,7.86;6,151.52,634.65,329.07,7.86;6,151.52,645.61,254.31,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,393.24,623.69,87.35,7.86;6,151.52,634.65,148.57,7.86">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,321.62,634.65,158.98,7.86;6,151.52,645.61,161.31,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
