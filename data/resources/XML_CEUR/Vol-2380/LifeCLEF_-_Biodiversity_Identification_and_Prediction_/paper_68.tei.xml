<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.64,115.90,318.08,12.68;1,249.07,133.83,117.20,12.68">Bird Sound Classification using Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,149.39,171.69,68.34,8.80"><forename type="first">Chih-Yuan</forename><surname>Koh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<postCode>30013</postCode>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.86,171.69,74.56,8.80"><forename type="first">Jaw-Yuan</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<postCode>30013</postCode>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.56,171.69,66.26,8.80"><forename type="first">Chiang-Lin</forename><surname>Tai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<postCode>30013</postCode>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,399.38,171.69,59.35,8.80"><forename type="first">Da-Yo</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<postCode>30013</postCode>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,228.38,183.64,73.42,8.80"><forename type="first">Han-Hsing</forename><surname>Hsieh</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National Chiao Tung University</orgName>
								<address>
									<postCode>30010</postCode>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,331.74,183.64,50.77,8.80"><forename type="first">Yi-Wen</forename><surname>Liu</surname></persName>
							<email>ywliu@ee.nthu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<postCode>30013</postCode>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.64,115.90,318.08,12.68;1,249.07,133.83,117.20,12.68">Bird Sound Classification using Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F4E448C1E7DFAC9329D74C8F8E5D6B67</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Inception-v3</term>
					<term>Bird sound recognition</term>
					<term>Bird-CLEF2019</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate prediction of bird species from audio recordings is beneficial to bird conservation. Thanks to the rapid advance in deep learning, the accuracy of bird species identification from audio recordings has greatly improved in recent years. This year, the BirdCLEF2019[4] task invited participants to design a system that could recognize 659 bird species from 50,000 audio recordings. The challenges in this competition included memory management, the number of bird species for the machine to recognize, and the mismatch in signal-to-noise ratio between the training and the testing sets. To participate in this competition, we adopted two recently popular convolutional neural network architectures -the ResNet[1] and the inception model <ref type="bibr" coords="1,346.94,400.95,16.48,7.92" target="#b12">[13]</ref>. The inception model achieved 0.16 classification mean average precision (c-mAP) and ranked the second place among five teams that successfully submitted their predictions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Public consciousness about environmental conservation and sustainable development has awakened in recent years. Demands for automatic bird call classification have also been rising owing to the key role of birds in the ecosystem. Compared to video-based monitoring, sounds have the advantage of propagation to a long distance without being occluded by objects in between the emitting source (a bird in this case) and the recording devices. Therefore, a robust system to identify bird vocalization may become useful for monitoring species diversity at a fixed location as well as detecting bird migration along a route. Realizing the importance of this task, a competition called BirdCLEF has been hosted every year by the LifeCLEF lab <ref type="bibr" coords="2,245.85,166.75,11.90,8.80" target="#b2">[3]</ref> since 2014. The goal of the competition is to identify bird species in audio recordings. In the competition this year, participants needed to detect bird calls in every 5 seconds of the soundscape recordings from the Xeno-Canto database <ref type="foot" coords="2,245.49,201.06,3.97,6.16" target="#foot_0">3</ref> and identify the species if bird calls are present.</p><p>Previous attempts to use machine learning approaches for bird call identification include decision trees, convolutional neural networks (CNN), and recurrent neural networks (RNN). For instance, randomized decision trees were applied <ref type="bibr" coords="2,470.08,240.63,10.51,8.80" target="#b5">[6]</ref> and the input consists of features derived from statistics of the spectrograms. By ranking the feature importance returned from the decision trees, one can find relevant segments to identify each sound class. Due to the computation load in deriving the statistics from spectrograms, the decision-tree technique might not be most suitable for the current BirdCLEF challenge; its ability to handle more than 600 species remains a concern, too. The RNN-based model was adopted in last year's BirdCLEF challenge. In particular, the bidirectional long shortterm memory (LSTM) architecture was applied <ref type="bibr" coords="2,350.54,336.27,9.96,8.80" target="#b7">[8]</ref>. It made use of sequential information in bird calls audio. However, because of the gradient vanishing and explosion problems associated with the sigmoid gate function, the model is difficult to reach convergence. Besides, due to the nature of RNN, preprocessing and augmentation are difficult to implement. Therefore, it seems that CNN-based models become the most common approach in bird call recognition. In general, the spectrogram of bird sound audio is regarded as the input and the model would treat the bird-call identification task as an image classification problem. This is intuitive, because features of bird calls unique to each species, such as the pitch and the timbre, can be observed in the spectrograms by experienced human eyes.</p><p>To participate in BirdCLEF 2019, we thus decided to apply two modern CNN-based models, ResNet and Inception. The rest of this paper is organized as follows. In Section 2, we briefly review the background of ResNet and Inception. More details concerning model implementation and training are described in Section 3. Experiments and results are described and analyzed in Section 4. In retrospect, Section 5 points out several flaws of our attempts, and conclusions and future directions are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we briefly review the spirits underpinning Inception and ResNet.  Titled "Going Deeper with Convolutions", the GoogleNet <ref type="bibr" coords="3,379.92,302.33,18.92,8.80" target="#b11">[12]</ref> presented a brand new neural network structure which, rather than blindly stacking convolution layers, emphasized on reducing the sparsity of the feature map. It replaced a general convolution layer by what is called the inception module; that is, instead of using a large-sized kernel in the convolution layer for feature extraction, smaller kernels were constructed in parallel. The concept is depicted in Fig. <ref type="figure" coords="3,433.43,362.10,8.48,8.80" target="#fig_1">1a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep residual learning for image recognition</head><p>Titled "Deep Residual Learning for Image Recognition" <ref type="bibr" coords="3,376.37,422.32,12.94,8.80" target="#b0">[1]</ref>, the ResNet mainly addressed the issue of model degradation. Because of the nonlinear activation function, the back-propagation gradients might vanish, which degrades the performance of deep CNNs. Hence, a highway pass between the upper and the lower layers was introduced and the result is known as the residual block. It can be expressed in the following general form,</p><formula xml:id="formula_0" coords="3,256.53,506.09,224.06,9.71">y i = h(x i ) + F (x i , W i ),<label>(1)</label></formula><p>where x i and y i are the input and the output of the i th block, and F denotes a flexible residual function which is parameterized by the weight array W i . A common practice is to set h(x i ) as the identity mapping and use the rectified linear unit (ReLU) between the weight function inside F , and Fig. <ref type="figure" coords="3,424.34,565.95,10.51,8.80" target="#fig_1">1b</ref> illustrates the main idea of ResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>This section describes details concerning the data processing and how the models were trained and tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>Spectrogram extraction Our strategy for the task was to treat bird sound classification as image classification; hence we need to visualize bird sounds.</p><p>A commonly used technique is the MEL-scale <ref type="bibr" coords="4,336.93,162.98,18.92,8.80" target="#b10">[11]</ref> log-amplitude spectrogram, a kind of time-frequency representation that takes human auditory sensitivity with respect to frequency into consideration. Since the occurrence of bird calls could be sparse, we chopped the signal into 1-second chunks instead of extracting a spectrogram for the entire 5-second recording. A band-pass filter with cut-off frequencies of 500Hz and 15kHz was applied, since most bird species vocalize within this frequency range.</p><p>. Parameters for computing the spectrograms are shown in Table <ref type="table" coords="4,427.80,409.33,3.87,8.80" target="#tab_0">1</ref>. Note that the hop length was determined so that each clip of one second contains exactly 255 frames. With this specification, each a Mel-scale log amplitude spectrogram has a size of 128 × 256. To decide whether each second contains bird calls, we applied a simple signal-to-noise ratio (SNR) threshold based on the methods described by BirdCLEF participants in previous years <ref type="bibr" coords="4,368.72,469.11,12.06,8.80" target="#b4">[5,</ref><ref type="bibr" coords="4,380.77,469.11,12.06,8.80" target="#b9">10]</ref>. Fig. <ref type="figure" coords="4,420.59,469.11,4.98,8.80" target="#fig_3">2</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Means Description</head><p>Gaussian white noise Additive noise with zero mean and a variance of 0.05</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adjusting brightness</head><p>Randomly multiply the whole spectrogram with a value between 1 ± 0.15</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random multiply</head><p>Randomly multiply each pixel with a value between 1 ± 0.25</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image blur</head><p>Replace each pixel with the mean of its 3×3 kernel neighbors.</p><p>Vertical and horizontal roll Random shift by a value between ±0.5 of the height and ±0.15 of the width, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random crop</head><p>Here we only cropped the height of spectrogram. The width remained the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotation</head><p>The spectrogram was randomly rotated by an angle bewteen ±10 degrees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dropout</head><p>Apply a random uniform matrix as a weight of spectrogram and set the weight to zero if that element is less than 0.25; otherwise set it to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blackout</head><p>Set the value in randomly chosen consecutive 25 columns to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation</head><p>We found class imbalance due to two reasons; first, the SNR in some classes (i.e., bird species) is low. Secondly, some classes simply have few recordings in the database. Hence, we randomly applied several augmentation methods (Table <ref type="table" coords="5,230.24,466.91,4.43,8.80" target="#tab_1">2</ref>) to the spectrograms of those classes with insufficient training samples. By inspection, the spectrograms of soundscape recordings in the validation set all seem to contain noise. Therefore, in two of our submissions, we added Gaussian noise to all of spectrograms, and fed both the orginal spectrograms and those with Gaussian noise to the neural-network models.</p><p>Normalization In two of our submissions, normalization was applied. The mean and the variance for transformation was calculated from the entire training dataset.</p><p>Output format By observing the training data, we find out that bird vocalization could be quite sporadic. Also, in the validation data, audibly different bird calls rarely occur at the same time. Hence, we determined to decode the output of the neural network into a one-hot vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network architecture and configuration</head><p>ResNet In one attempt, we adopted the ResNet-18 <ref type="bibr" coords="6,368.64,140.68,13.81,8.80" target="#b0">[1]</ref> classifier for bird-call identification. The optimizer was stochastic gradient descent (SGD) with momentum and the batch size was set to 64. Weight initialization was applied, which sampled from normal distribution N (0, 0.02) for convolution layers and N (1, 0.02) for batch normalization2d layers. the number of channels in ResNet has been reduced to 1 (because spectrograms do not have RGB colors); as the number of input features decreased, the model complexity can also be lowered so as to increase the training efficiency within a limited amount of time.</p><p>Inception model On the basis of Inception v3 <ref type="bibr" coords="6,349.47,256.07,16.28,8.80" target="#b12">[13]</ref>, we substituted two connected convolution layers with kernel size (1, 7) and (7, 1) for the convolution layer with kernel size <ref type="bibr" coords="6,233.57,280.04,11.62,8.74" target="#b6">(7,</ref><ref type="bibr" coords="6,246.86,280.04,7.75,8.74" target="#b6">7)</ref>. Meanwhile, an additional activation function has been added between the two small convolution layers. The optimizer was changed to Adam and the batch size was set to 64. The same weight initialization that was applied to ResNet has also been applied here. The number of channels was decreased before the data flows into Inception module 5c. Other details about modification of the Inception model can be found in our github contribution. <ref type="foot" coords="6,470.70,338.20,3.97,6.16" target="#foot_1">4</ref> . Table <ref type="table" coords="6,178.10,508.19,4.98,8.80" target="#tab_2">3</ref> shows the best performance that we obtained from the two models on validation data. For some reasons, inception obviously was better than ResNet. For more details about hyperparameter and corresponding validation result, please see the appendix. The official main score, cmAP, was defined by the competition organizers as follows,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><formula xml:id="formula_1" coords="6,253.11,576.50,227.48,25.41">cmAP = C c=1 AveP(c) C ,<label>(2)</label></formula><p>and</p><formula xml:id="formula_2" coords="6,238.24,618.86,242.35,26.33">AveP(c) = n k=1 P (k) × rel(k) n rel (c) .<label>(3)</label></formula><p>In Eq. ( <ref type="formula" coords="7,169.92,118.93,3.87,8.80" target="#formula_2">3</ref>), C denotes the number of species in the ground truth, and AveP(c) is the average precision for a given species c, which is defined in Eq. ( <ref type="formula" coords="7,448.63,130.89,4.24,8.80">4</ref>)k denotes the rank of an item in the list of the predicted segments containing c, n denotes the total number of predicted segments containing c, P (k) denotes the precision at cut-off k in the list, rel(k) is an indicator function that equals 1 if the segment at rank k is labeled as containing c in the ground truth (and zero otherwise), and n rel is the total number of relevant segments for c. Due to the comparatively better performance of the inception model, we adopted it in all of our submissions. Table <ref type="table" coords="7,318.98,319.44,4.98,8.80" target="#tab_3">4</ref> shows the official evaluation of each run. In the first two runs, Gaussian noise was not added to the whole spectrograms. The only difference between them was in the threshold of SNR with 0.005 and 0.001, respectively. In the third and forth runs, we added Gaussian noise to all spectrograms and normalized all of the spectrograms before training. The difference between them was the epochs we chose. The official results surprised us since the first two runs actually performed better when evaluated in the validation set. Otherwise, compared to other teams, our rmAP peculiarly was not much higher than cmAP. This is not surprising, probably because we assigned five classes to most of the five-second segments that contains bird sound, which increases the denominator in the equation of rmAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The audio in the testing set contains various kinds of environmental noise (such as the sounds of insects), but we ran out of time in designing a generalized method to deal with it. The noise results in difficulties to extract the correct feature from spectrograms. The SNR threshold we applied to determine the bird calls' presence also has some concerns, since we might have included a spectrogram with intense noise instead of targets, and those spectrograms contain only noise would be treated as training data. In data augmentation, though most of the means in Table <ref type="table" coords="7,220.04,572.37,4.98,8.80" target="#tab_1">2</ref> could be useful in image processing, but in retrospect they are not always suitable to spectrograms. In particular, the two axes of spectrogram have different meanings (time vs. frequency), so a similiar shape occuring at different locations may correspond to features that are unique to different bird species. Hence, augmentation such as rotation might actually have been harmful during the training stage.</p><p>We deduce a simple reason why Inception-v3 <ref type="bibr" coords="7,352.83,644.10,17.43,8.80" target="#b12">[13]</ref> outperformed ResNet[1] could be because of the number of parameters. The more parameters a model has, the more accurately the model can representation the mapping between the input and the output. In our experiment, the parameter size of ResNet-18 is 9.12 MB, the parameter size of ResNet-34 is 10.41 MB, and the parameter size of Inception-v3 is 92.3 MB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future work</head><p>Our work is based on the baseline of BirdCLEF last year. The main difference is that we change the model to Inception-v3. Future work will focus on improving the preprocessing. We need to enhance clear bird sound features on the spectrograms of soundscape recordings so that it can be similar to the training set. In the current approach, we added Gaussian white noise as a mean of data augmentation. We would like to change it to the noise from the recording environments. Moreover, since the current preprocessing mainly focuses on the magnitude spectrogram, it might beneficial to learn additionally information from the phase spectrogram, especially when multiple recording channels are available.</p><p>On the part of model, due to the shift-invariance and the parameter-sharing property, CNN may be in trouble distinguishing spectrograms that contain features of similar shapes but occuring in different frequencies. A possible way to mend this would be tiled convolution. Although it is still a CNN model, a tiled convolution model <ref type="bibr" coords="8,217.43,374.02,10.51,8.80" target="#b8">[9]</ref> has locally confined receptive field; that is, the parameter sharing of tiled convolution is not global. Employing attention mechanisms can also be recommended, since not all the neurons would end up equally important as the signal comes out from a specific range of frequency <ref type="bibr" coords="8,383.66,409.88,9.96,8.80" target="#b6">[7]</ref>. Yet another neural architecture, the SENet <ref type="bibr" coords="8,235.16,421.84,13.79,8.80" target="#b1">[2]</ref>, is worth trying because the resolution of spectrograms can be increased in order to preserve more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgement</head><p>This research is supported by Airoha Technology Corp. We are also grateful to LifeCLEF2019 team for holding this challenge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,161.08,238.89,293.18,8.80;3,137.46,127.55,173.95,82.94"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Basic concepts of two recently developed techniques in CNN</figDesc><graphic coords="3,137.46,127.55,173.95,82.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,428.93,469.11,51.66,8.80;4,134.77,481.06,345.82,8.80;4,134.77,493.02,345.82,8.80;4,134.77,504.97,345.82,8.80;4,134.77,516.93,345.83,8.80;4,134.77,528.88,345.82,8.80;4,134.77,540.84,41.30,8.80"><head></head><label></label><figDesc>shows a few examples of spectrograms with different SNRs. By inspection, a spectorgram with a higher SNR indeed contains clearer traces that indicate the presence of a bird call. In contrast, a spectrogram with a low SNR may only contain background noise. Based on the SNR, we could set a threshold to include only the spectrograms with a sufficiently high SNR as samples for training the neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,198.07,652.47,219.20,8.80;4,137.96,570.46,107.21,53.60"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Extracted spectrograms with different SNR</figDesc><graphic coords="4,137.96,570.46,107.21,53.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,214.28,268.75,180.66,112.06"><head>Table 1 :</head><label>1</label><figDesc>Parameters of spectrograms</figDesc><table coords="4,214.28,293.59,180.66,87.22"><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Sampling rate</cell><cell>44100 Hz</cell></row><row><cell>Window length</cell><cell>1024</cell></row><row><cell>Hop length</cell><cell>172</cell></row><row><cell>Number of Mel filters banks</cell><cell>128</cell></row><row><cell>Minimum frequency</cell><cell>500 Hz</cell></row><row><cell>Maximum frequency</cell><cell>15000 Hz</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,167.59,127.30,280.19,8.80"><head>Table 2 :</head><label>2</label><figDesc>The means for data augmentation applied in this study</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,167.65,428.60,280.05,48.49"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of Inception and ResNet via validation data</figDesc><table coords="6,225.50,442.48,167.96,34.62"><row><cell>model</cell><cell cols="3">Inception ResNet18 ResNet34</cell></row><row><cell>cmAP</cell><cell>0.23</cell><cell>0.13</cell><cell>0.11</cell></row><row><cell>rmAP</cell><cell>0.39</cell><cell>0.05</cell><cell>0.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,202.85,232.64,204.60,48.50"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of each run</figDesc><table coords="7,202.85,246.52,204.60,34.62"><row><cell>run</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>cmAP</cell><cell>0.140</cell><cell>0.149</cell><cell>0.160</cell><cell>0.154</cell></row><row><cell>rmAP</cell><cell>0.110</cell><cell>0.117</cell><cell>0.164</cell><cell>0.184</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,656.74,118.57,7.92"><p>https://www.xeno-canto.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="6,144.73,656.74,200.46,7.92"><p>https://github.com/jimmy133719/BirdCLEF2019</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Hyperparameter sheet and the corresponding validation results The momentum of optimizer SGD is set to 0.9. The β 1 and β 2 for optimizer Adam are set to 0.9 and 0.999, respectively.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.95,548.64,337.64,7.92;8,151.52,559.59,328.21,7.92" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,290.95,548.64,172.54,7.92">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,151.52,559.59,244.17,7.92">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,570.06,337.63,7.92;8,151.52,581.02,287.75,7.92" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,273.07,570.06,131.77,7.92">Squeeze-and-excitation networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,431.41,570.06,49.18,7.92;8,151.52,581.02,194.50,7.92">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,591.48,337.64,7.92;8,151.52,602.44,329.07,7.92;8,151.52,613.40,329.07,7.92;8,151.52,624.36,238.37,7.92" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,442.94,602.44,37.64,7.92;8,151.52,613.40,329.07,7.92;8,151.52,624.36,80.83,7.92">Overview of lifeclef 2019: Identification of amazonian plants, south &amp; north american birds, and niche prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christophe</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maximillien</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,253.40,624.36,107.82,7.92">Proceedings of CLEF 2019</title>
		<meeting>CLEF 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,634.83,337.63,7.92;8,151.52,645.79,329.07,7.92;8,151.52,656.74,71.20,7.92" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,442.94,634.83,37.64,7.92;8,151.52,645.79,245.84,7.92">Overview of birdclef 2019: Large-scale bird recognition in soundscapes</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,420.31,645.79,60.27,7.92;8,151.52,656.74,42.53,7.92">CLEF working notes 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,119.62,337.63,7.92;9,151.52,130.58,329.07,7.92;9,151.52,141.54,123.56,7.92" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,168.54,130.58,293.34,7.92">Large-scale bird sound classification using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,141.54,94.89,7.92">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,152.50,337.63,7.92;9,151.52,163.46,329.08,7.92;9,151.52,174.42,305.03,7.92" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,208.72,152.50,271.86,7.92;9,151.52,163.46,106.25,7.92">Bird song classification in field recordings: winning solution for NIPS4B 2013 competition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,281.84,163.46,198.76,7.92;9,151.52,174.42,80.12,7.92">Proc. Int. Symp. Neural Information Scaled for Bioacoustics, sabiod</title>
		<meeting>Int. Symp. Neural Information Scaled for Bioacoustics, sabiod<address><addrLine>, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
	<note>.org/nips4b, joint to NIPS</note>
</biblStruct>

<biblStruct coords="9,142.95,185.37,337.64,7.92;9,151.52,196.33,329.07,7.92;9,151.52,207.29,329.07,7.92;9,151.52,218.25,25.59,7.92" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,431.38,185.37,49.21,7.92;9,151.52,196.33,329.07,7.92;9,151.52,207.29,132.24,7.92">Sound event classification by a deep neural network with attention and minimum variance distortionless response enhancement</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,305.40,207.29,175.18,7.92">IEEE DCASE Challenge Technical Reports</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,229.21,337.64,7.92;9,151.52,240.17,298.11,7.92" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,245.81,229.21,214.18,7.92">Bird sound classification using a bidirectional LSTM</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,240.17,264.47,7.92">Working Notes of CLEF 2018 (Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,251.13,337.63,7.92;9,151.52,262.09,329.07,7.92;9,151.52,273.05,47.09,7.92" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,403.32,251.13,77.27,7.92;9,151.52,262.09,61.21,7.92">Tiled convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,232.26,262.09,203.82,7.92">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1279" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,284.00,337.98,7.92;9,151.52,294.96,308.78,7.92" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,357.14,284.00,123.45,7.92;9,151.52,294.96,160.39,7.92">Audio based bird species identification using deep learning techniques</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sprengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,319.03,294.96,39.92,7.92">LifeCLEF</title>
		<imprint>
			<biblScope unit="page" from="547" to="559" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,305.92,337.97,7.92;9,151.52,316.87,294.47,7.93" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,335.52,305.92,145.07,7.92;9,151.52,316.88,120.38,7.92">A scale for the measurement of the psychological magnitude pitch</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Volkmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">B</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,278.77,316.88,80.53,7.92">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="190" />
			<date type="published" when="1937">1937</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,327.84,337.98,7.92;9,151.52,338.80,329.07,7.92;9,151.52,349.76,260.11,7.92" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,283.36,338.80,127.78,7.92">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,433.44,338.80,47.15,7.92;9,151.52,349.76,194.50,7.92">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,360.72,337.98,7.92;9,151.52,371.68,329.07,7.92;9,151.52,382.63,173.03,7.92" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,393.25,360.72,87.34,7.92;9,151.52,371.68,147.08,7.92">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,319.40,371.68,161.20,7.92;9,151.52,382.63,79.78,7.92">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
