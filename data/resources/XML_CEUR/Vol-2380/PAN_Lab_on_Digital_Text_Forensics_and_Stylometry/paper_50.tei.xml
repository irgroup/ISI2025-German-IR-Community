<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.86,146.22,310.84,16.66;1,172.82,163.66,252.91,16.66;1,217.33,183.77,163.90,13.88">Improving Cross-domain Authorship Attribution by Combining Lexical and Syntactic Features Notebook for PAN at CLEF 2019</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,223.33,221.31,63.79,9.21"><forename type="first">Martijn</forename><surname>Bartelds</surname></persName>
							<email>m.bartelds.2@student.rug.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.84,221.31,61.51,9.21"><forename type="first">Wietse</forename><surname>De Vries</surname></persName>
							<email>w.de.vries.21@student.rug.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.86,146.22,310.84,16.66;1,172.82,163.66,252.91,16.66;1,217.33,183.77,163.90,13.88">Improving Cross-domain Authorship Attribution by Combining Lexical and Syntactic Features Notebook for PAN at CLEF 2019</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">529732A08E832B33C8E5A84166346F0F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authorship attribution is a problem in information retrieval and computational linguistics that involves attributing authorship of an unknown document to an author within a set of candidate authors. Because of this, PAN-CLEF 2019 organized a shared task that involves creating a computational model that can determine the author of a fanfiction story. The task is cross-domain because of the open set of fandoms to which the documents belong. Additionally, the set of candidate authors is also open since the actual author of a document may not be among the candidate authors. We extracted character-level, word-level and syntactic information from the documents in order to train a support vector machine. Our approach yields an overall macro-averaged F1 score of 0.687 on the development data of the shared task. This is an improvement of 18.7% over the character-level lexical baseline. On the test data, our model achieves an overall macro F1 score of 0.644. We compare different feature types and find that character n-grams are the most informative feature type though all tested feature types contribute to the performance of the model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.29" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.29" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.29" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.29" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.29" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.29" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.29" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.29" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.29" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.29" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.29" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.29" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Authorship attribution is an established research area in computational linguistics that aims to determine the author of a document by taking the writing style of the author into account. Typically, a system assigns a candidate author to an anonymous text by comparing the anonymous text to a set of possible author writing samples. Currently, the field of authorship attribution can be considered as a topic of pivotal interest as the authenticity of information presented in the media is often questioned. Following this, any successfully attempt in revealing the authors behind a text will result in improved transparency and ideally removes any uncertainty with respect to the validity of information presented. As a consequence, authorship attribution can be determined as being closely related to research tailored to the privacy domain, law, cyber-security, and social media analysis.</p><p>In the PAN-CLEF 2019 shared task on authorship attribution, a cross-domain authorship attribution task was proposed based on fanfiction texts. More specifically, fanfiction texts are written by admirers of a certain author and these fanfiction texts are known to substantially borrow characteristics from the original work. This task can be considered cross-domain, since the documents of known authorship are not necessarily collected within the same thematic domain or genre. Moreover, this task is extended beyond closed-set attribution conditions, as the true author of a given text in the target domain is not by definition included in the set of candidate authors.</p><p>In this work, we present the methodology and results of our submission to the PAN-CLEF 2019 cross-domain authorship attribution task. We developed our approach with respect to the documents of all four languages provided in the PAN-CLEF 2019 data set. These languages include: English, French, Italian, and Spanish. Previous research showed the effectiveness of textual features such as character-level n-grams to authorship attribution problems, since these are capable of representing more abstract level writing style characteristics rather than generating a representation that is purely related to the content of the document <ref type="bibr" coords="2,272.97,349.79,15.34,9.21" target="#b17">[18,</ref><ref type="bibr" coords="2,289.57,349.79,12.11,9.21" target="#b18">19,</ref><ref type="bibr" coords="2,302.95,349.79,11.51,9.21" target="#b20">21]</ref>. However, limitations of these features arise together with the observation that they often result in sparse representations for documents of insufficient size. Therefore, we aimed to create an approach in which we create textual representations across different levels inside the training documents. As such, we intended to prevent overfitting by developing a model that yields robust performance across the different genres.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we will describe some of the most successful modern authorship attribution methods. Furthermore, a brief overview of the subfield of cross-domain authorship attribution will be provided.</p><p>Generally, there is a distinction between the use of profile-based and instancebased approaches in solving authorship identification problems <ref type="bibr" coords="2,383.73,511.29,14.85,9.21" target="#b19">[20]</ref>. In profile-based approaches, all available training texts per author are concatenated to create a cumulative representation of the author's writing style. In contrast, instance-based approaches treat each training text as an individual representation of the author's writing style. When these approaches are compared, it is shown that profile-based approach may be advantageous when only training documents of limited size are available <ref type="bibr" coords="2,426.69,569.44,14.85,9.21" target="#b19">[20]</ref>. As a result, the concatenation of the training documents may lead to a more reliable representation of the author's writing style. In contrast, the implementation of an instancebased approach ensures that interactions between several stylometric features can be captured, even when the distributions of these features differ between documents that are written by the same author. As an extension to both the profile-based and instancebased approaches, hybrid approaches are proposed that integrates aspects of both the profile-based and instance-based approaches <ref type="bibr" coords="2,307.72,650.84,10.29,9.21" target="#b3">[4]</ref>. Then, a single vector per author will be produced by averaging the sum of the individually represented training texts. We argue that these hybrid approaches might be superior to the profile-based and instance-based approaches, since they could capture more reliable characteristics of writing style across multiple documents of the same author.</p><p>As can be observed from the results of the PAN-CLEF 2018 shared task on crossdomain authorship attribution, we examine that the best performance was obtained by the implementation of character-level and word-level n-grams as textual features <ref type="bibr" coords="3,453.74,198.39,10.29,9.21" target="#b6">[7]</ref>. These were normalized by a tf-idf weighting scheme and used in combination with support vector machines. Previous research on cross-domain authorship attribution endorses the effectiveness of a support vector machines applied on character-level ngrams <ref type="bibr" coords="3,157.33,244.90,15.34,9.21" target="#b17">[18,</ref><ref type="bibr" coords="3,174.01,244.90,12.11,9.21" target="#b18">19,</ref><ref type="bibr" coords="3,187.47,244.90,11.51,9.21" target="#b20">21]</ref>. This suggests that the use of these methodologies can still be determined as a valuable strategy in solving cross-domain authorship attribution problems.</p><p>In previous research, thirty-nine different types of textual features that are often used in modern authorship attribution studies were compared <ref type="bibr" coords="3,370.80,280.02,10.29,9.21" target="#b2">[3]</ref>. In this study, a token normalized version of the punctuation frequency was the most successful feature used to discriminate between the different authors. Moreover, character-level bi-grams and tri-grams were also among the most promising textual features presented, and this result is substantiated by the results of numerous other research findings <ref type="bibr" coords="3,409.05,326.54,10.49,9.21" target="#b1">[2,</ref><ref type="bibr" coords="3,421.03,326.54,7.27,9.21" target="#b5">6,</ref><ref type="bibr" coords="3,429.78,326.54,7.27,9.21" target="#b6">7,</ref><ref type="bibr" coords="3,438.53,326.54,12.11,9.21" target="#b12">13,</ref><ref type="bibr" coords="3,452.12,326.54,11.51,9.21" target="#b21">22]</ref>. Following this, we decided to include these features into our own approach by creating such a feature representation that is tailored to the data set we had available.</p><p>Furthermore, research on cross-domain authorship attribution showed that topicdependent information can be discarded from documents by carrying out several preprocessing steps <ref type="bibr" coords="3,196.48,384.91,10.29,9.21" target="#b8">[9]</ref>. It was suggested to replace all digits by zero, separate punctuation marks from their adjacent words, and replace named entities with a dummy symbol. The latter pre-processing step is effective, since named entities are often strongly related to the topic of a document. After these steps character-level n-grams were extracted from the documents, and an increase in performance was reported when these pre-processed character-level n-grams were used. Moreover, attribution performance can be improved by applying a frequency threshold to the extracted character-level n-gram representations <ref type="bibr" coords="3,152.92,466.32,10.29,9.21" target="#b8">[9]</ref>. As such, the least frequent occurring n-grams associated with topic-specific information should be removed from the model. In our work, we decided to extend on their work by implementing an adaptation of their suggested frequency threshold. Moreover, we attempt to improve our model performance by applying these pre-processing steps not solely on character-level n-grams, but on multiple textual feature levels inside their training documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data and Resources</head><p>A development data set is provided by the organizers of the authorship attribution task. The goal of the shared task is not to train a model on known training data and to test it on unknown test data, but rather to design a model that can be trained on unknown training data and then be tested on unknown test data. The development data set is not called a training data set, because there is no overlap in candidate authors in the development data and the undisclosed data that the models are trained and evaluated on for the shared task. Instead, the development data contains twenty separate problem sets with each a training part and a test part. The final evaluation will be performed on a similar set of problem sets.</p><p>The development data set contains twenty problem sets in four different languages, resulting in five problem sets per language. The set of languages consists of English, French, Italian and Spanish. Each problem set contains nine candidate authors with seven known documents each. The task is to assign each document in a set of unknown documents to a candidate author within the problem set, if the author unknown document is actually in the candidate set. There is also the possibility that the actual author is unknown and in that case the unknown document should be given an unknown label. For evaluation a separate corpus is held back with similar characteristics as the development corpus. The evaluation data contains problem sets in the same languages as the development data, but there is no overlap in authors in the development and test sets. Therefore, no features can be learned for specific authors before testing.</p><p>The documents that are to be classified consist of a set of fanfiction stories with a length of 500 to 1000 tokens each. The stories were scraped from an online fanfiction website. Candidate authors write stories in different fandoms, so it is important that the model will learn author-specific textual features and not features that are inherent to the fandom or the universe that the story takes place in. Because of the large content differences between fandoms, the fandoms are considered to be different domains.</p><p>The amount of unknown documents per problem set is highly variable, since it ranges between 46 and 561 documents per problem set. Moreover, occurrences of candidate authors in the unknown documents are not uniformly distributed either. This is not inherently important for development and methodology design choices. However, very rare occurrences of certain candidate authors may have large effects on evaluation scores during development, since scores are macro-averaged per candidate within the problem sets. The fraction of unknown documents that are not written by any known author is also variable. Although, within the development set overall a third of all documents are written by an unknown author.</p><p>The imbalance of the testing parts of the problem sets in the development data does not influence model training, since the distribution is unknown at training time. During the development of our methodology, the average result may however be strongly influenced by unbalanced problem sets. The goal is to let our model be able to work with problem sets with unknown distributions, so the development data should be balanced when we want to trust the results. Our algorithmic and hyper-parameter choices are therefore not based on the given configuration of the development data set. Instead, we merged the problem sets per language and evaluated on random permutations of candidate authors. Our new shuffled problem sets contained nine random candidate authors in the same language with seven random known documents for each author. It can be possible that these documents originate from documents that were originally meant for model testing. The test data for our new shuffled problem sets contained up to 100 documents per candidate author that were not used for training. The set of test documents is extended with documents that are not written by the candidate authors. These documents served as the documents with an unknown author, and a third of each generated problem set consists of these documents with an unknown author. This method enabled us to generate a large amount of unique permutations of problem sets. For the development of our methodology, we evaluated our choices by using a fixed set of twenty shuffled problem sets per language, as opposed to the original five problem sets per language. The original development problem sets are used for validation and the results in this paper are evaluated on the original problem sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>The code that is used for our authorship attribution approach is fully open-source and available at https://github.com/wietsedv/pan19-cross-domain-authorship-attribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Classification approach</head><p>Because of their success in previous authorship attribution approaches, we choose to use a support vector machine with document level features. We extract different types of textual features from the known documents which are used to train a support vector machine (SVM) model for each problem set. We used the SVM implementation that is available in the Scikit-learn Python package <ref type="bibr" coords="5,304.18,325.09,14.85,9.21" target="#b11">[12]</ref>. Hyper-parameters are tuned globally using a grid search method with our randomly permuted problem sets. Therefore, hyperparameters are equal for all languages. The specific values of the hyper-parameters will be discussed in Section 4.3. Hyper-parameters that are specific for feature types are tuned separately from each other using separate grid searches within intuitively plausible parameter ranges. This constrained grid search is chosen because of computational limitations, but also to prevent overfitting on the hyper-parameters.</p><p>The classifier that is used is a support vector machine classifier with a linear kernel. Multiple classes are handled using the one-vs-rest scheme. We also tried using other SVM kernels as well as the using a random forest classifier, but preliminary results indicated that these options are unlikely to lead to better classification accuracy results in this task.</p><p>The support vector machine classifier has to be reasonably certain in its candidate author choice, since there are also test documents that have an unknown author. This is achieved by setting a probability threshold for the support vector machine classifications. Probabilities are calculated in the SVM model by scaling the distance of the sample to each hyper-plane between zero and one. SVM predictions and probability values can be heavily influenced by single training documents because of the small amount of documents for each author and the high risk of learning fandom specific features. Additionally, SVM outputs may not be reliable estimators for probability. Therefore, a good solution in finding more reliable probabilities would be to use probability calibration <ref type="bibr" coords="5,164.30,569.39,14.85,9.21" target="#b9">[10]</ref>. In this process, five-fold cross validation is applied on the training data to train five separate classifiers. In each of these classifiers, probability estimations are calibrated using Platt scaling <ref type="bibr" coords="5,245.47,592.65,14.85,9.21" target="#b15">[16]</ref>. Probability estimations of the five classifiers are averaged to get the final probabilities.</p><p>Each test document is attributed to the most probable author if and only if the difference between the maximum probability and the second highest probability score is at least 0.1. As opposed to an absolute minimum probability, this minimum difference threshold is less sensitive to different probability distributions. Contrasting distributions in different languages or problem sets may result in very differing maximum probabilities. However, we are only interested in cases where the most probable choice is more likely distinguishable than the second most probable choice. The choice of a minimum probability difference of 0.1 is arrived at by using a grid search with values between 0.01 and 0.3 with intervals of 0.05.</p><p>The features that are used with the SVM consist of an union of six different feature types that will be described in Section 4.3. The different types of features rely on different representations of the documents for which pre-processing is required. In the next section (Section 4.2), the pre-processing steps are described that are needed to extract these linguistic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-processing</head><p>The first preprocessing step is to tokenize the documents. Tokenization is done using the UDPipe <ref type="bibr" coords="6,164.81,290.77,16.14,9.21" target="#b22">[23]</ref> tokenizer to get tokens in the format that can be used by both the part-ofspeech tagger and the dependency parser. For part-of-speech tagging, Structbilty <ref type="bibr" coords="6,448.90,302.39,14.85,9.21" target="#b14">[15]</ref>, a Bi-LSTM sequence tagger, is trained on Universal Dependencies data sets <ref type="bibr" coords="6,430.09,314.02,14.85,9.21" target="#b10">[11]</ref>. Validation set accuracy scores after training are all between between 0.95 and 0.98 for the four different languages. The part-of-speech tagger is trained on Universal Dependencies data sets. More precisely, we used UD_English-EWT, UD_French-GSD, UD_Italian-ISDT and UD_Spanish-GSD.</p><p>Dependency parses are provided by the UUParser <ref type="bibr" coords="6,342.27,372.17,10.29,9.21" target="#b7">[8]</ref>. UUParser was trained using the same data sets as were used by the part-of-speech tagger. When using the document tokens as input, the parser achieves validation LAS scores between 0.83 and 0.89 on the Universal Dependencies development treebanks. To improve performance, we also trained the parser by using ELMo embeddings <ref type="bibr" coords="6,320.95,418.68,16.14,9.21" target="#b13">[14]</ref> of the documents instead of the tokens. The ELMo embeddings were extracted using pre-trained ELMo representations <ref type="bibr" coords="6,131.09,441.94,10.29,9.21" target="#b0">[1]</ref>. As a result, the validation scores after training the parser ranged between 0.86 and 0.90, which is a considerable improvement over the original model. Calculating ELMo representations is however an expensive process and because of hardware limitations in the shared task setup, we decided to use the token based dependency parses instead of the ELMo based parses. This compromise does not not have large negative effects on the classifier performance as will be discussed in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Features</head><p>Different textual feature types are extracted from the documents independently from each other. Each feature type yields numeric features that are linearly scaled between zero and one. Subsequently, the dimensionality of each feature type is reduced to 150 by applying truncated singular value decomposition. After this dimensionality reduction for each feature type, all features are combined into a single feature set. The dimensionality is reduced before combining the features to make sure that all feature types are fairly represented in the feature set. As discussed before, the hyper-parameters for feature extraction are tuned per feature using a grid search approach. Fine tuned hyperparameters for features include the n-gram range, the use of tf-idf, maximum document frequencies and minimum group document frequencies. The minimum group document frequency is a threshold that we have created that ensures that any feature must at least be present in n documents with the same target label during training. This created hyper-parameter eliminates features that are only present in few documents written by an author, which suggests that the feature is domain related instead of author related.</p><p>For each of the feature types, we explored n-gram ranges between one and five. Subsequently, the following features were extracted from the documents:</p><p>Character n-grams The first feature type that is included in our model is based on the tf-idf scores of character n-grams in the raw document text. The value of n after tuning ranges between two and four.</p><p>Punctuation n-grams This feature type consists of n-grams of consecutive punctuation tokens where non-punctuation tokens are skipped. For example: this feature type contains the bi-gram ",." if a sentence contains a comma and ends with a dot. Occurrences of uni-and bi-grams are counted and used as features.</p><p>Token n-grams This feature type consists of the counts of token n-grams in the tokenized text. Only bi-grams are counted for this feature type, and only bi-grams that occur in at least five documents are included.</p><p>Part-of-speech n-grams This feature type consists of the counts of n-grams of the part-of-speech tags corresponding to each of the tokens in the document text. The value of n after tuning ranges between one and four.</p><p>Dependency relations syntactic n-grams This feature type consists of sequences of dependency relations. These sequences are created by chaining the syntactic relations between words. For instance, a bi-gram consists of the relation between a word and its head, and the relation between the head and its head. Note that this chaining procedure is different from the positional ordering of the words. For dependency relation syntactic n-grams, only uni-grams and bi-grams are included that occur at least thrice in the training document of a candidate author. The dependency relation syntactic bi-grams for instance include nsubj ROOT , if a sentence contains a nominal subject that is connected to the root of the sentence.</p><p>Token syntactic n-grams This feature type consists of actual words in syntactic ngram relations. The same chaining procedure is used but actual tokens are chained instead of relation labels. Token syntactic n-grams also have a minimum group document frequency of three. The n-gram range for this feature type is two to three.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Following the description of the PAN-CLEF 2019 shared task, we aimed to determine the author of a fanfiction text among a list of candidate authors. Following this, we have created a support vector machine approach with multiple features derived from different textual levels inside the documents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Individual feature types</head><p>In Figure <ref type="figure" coords="8,170.67,406.45,3.63,9.21" target="#fig_0">1</ref>, both the performance of the individual feature types and the overall performance of our approach are visualised. Just like the evaluation of the shared task, the performance was evaluated by calculating macro-averaged F1 scores that were calculated when the unknown target labels were excluded <ref type="bibr" coords="8,333.09,441.34,10.29,9.21" target="#b4">[5]</ref>. From these results, we observe that the character-level n-grams yielded the best performance among the tested feature types. In contrast, the lowest performance for all languages was obtained with the textual representation that counted occurrences of syntactic token n-grams. These observations apply to all the languages that were included in our data set. Examining the individual languages in particular, we note that our model performed well for the Italian language. This observation applied to both the performance of the individual feature types as well as the performance when the all feature types are combined. Furthermore, the performance per feature type is the lowest when we look at the the English and French languages, suggesting that the variation of features might have less predictive power for these languages as compared to the other languages present. However, the difference may also be an artifact of the data set.</p><p>When we compare the performance of our character-level n-gram feature representation to the performance of the baseline approach that was provided by PAN-CLEF 2019, we observe that we outperformed the baseline approach by 6.7%. More specifically, the PAN-CLEF 2019 baseline approach obtained a macro averaged F1 score of 0.579 across all languages. This approach consisted of a character-level tri-gram representation in combination with a linear support vector machine, and a simple probability threshold rejection option was included to assign an unknown document to the unknown class. Our performance gain was calculated across all four languages that were included in the data set, and an even larger performance gain with respect to the baseline is re- ported when we compare the performance of our system with all features included. Then, we outperformed the baseline approach by 18.7%. In order to clarify the contributions of the individual features to the overall performance of our approach, we performed an ablation study. Initially, we started with the complete feature set, after which we eliminated the individual features in the feature set, respectively. As shown in Table <ref type="table" coords="9,255.64,336.46,3.63,9.21" target="#tab_0">1</ref>, we examine that the largest decrease in performance (13.7%) was obtained when the character-level n-gram feature was omitted from the feature set. These findings correspond well to the observed effect that was previously described and visualised in Figure <ref type="figure" coords="9,263.80,371.34,3.63,9.21" target="#fig_0">1</ref>. Also, the same method of reasoning can be applied when we compare the remaining results of the ablation study with their corresponding counterparts that can be found in Figure <ref type="figure" coords="9,286.54,394.60,3.63,9.21" target="#fig_0">1</ref>. We observed higher scores for the individual performance of the token-level n-grams as compared to the individual performance of the part-of-speech-level n-grams. When we compare this observation to the outcomes of the ablation study in Table <ref type="table" coords="9,244.13,429.49,3.63,9.21" target="#tab_0">1</ref>, we observe the opposite effect. This suggests that solely using token-level n-grams achieves better performance than solely using part-of-speech based n-grams. However, the information that is captured by token-level n-grams seems also to be captured by other feature types whereas part-of-speech based n-grams provide additional information. This observation confirms the power of combining different feature types that may not be good predictors individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ELMo embeddings</head><p>As illustrated in Table <ref type="table" coords="9,224.37,534.55,3.63,9.21" target="#tab_1">2</ref>, we compared the performance of the dependency relation syntactic n-grams and contrasted these results with the performance of the complete feature set. With this comparison, we wanted to examine whether the use of ELMo embeddings improved the general performance of our approach, and we wanted to observe the effect of ELMo embeddings on the results produced by the dependency parser. As illustrated in Table <ref type="table" coords="9,208.70,592.69,3.63,9.21" target="#tab_1">2</ref>, the performance can be observed per language, and we distinguished between F1 scores that were obtained when we trained the parser by using ELMo embeddings, and F1 scores that were obtained when we trained the dependency parser using regular tokens. Following this, we observe that the inclusion of ELMo embeddings had the largest advantageous effect on the textual problems related to the English language for both the dependency tag syntactic n-gram feature type and the complete feature set. An additional increase in performance was observed for the dependency tag syntactic n-gram feature type when looking at the French language. In all other cases the use of ELMo embeddings did not have any effect or even resulted in a decrease in performance. Given the fact that the calculation of ELMo embeddings was an expensive process, we argue that including ELMo embeddings for the derivation of syntactic information is not beneficial for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Development results</head><p>In conclusion, with the best feature settings included in our approach we obtained an average macro F1 score of 0.687 based on the development data. More specifically, this score was obtained by calculating the average of the F1 scores of the four individual languages. In more detail, when we observe the F1 scores per language, we are able to conclude that our model performed best for the Italian problem sets (0.777), and this score is followed by the Spanish (0.730), French (0.684) and English (0.556) problem sets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Test results</head><p>Based on the previously described results, we submitted the full model to the TIRA submission platform with all feature types for the shared task testing phase <ref type="bibr" coords="10,430.80,457.96,14.85,9.21" target="#b16">[17]</ref>. The overall macro F1 score of our model is 0.644, which is slightly lower than our macro F1 score on the development data. The English and French testing scores are 0.558 and 0.687, respectively. These two scores are marginally higher than the scores on the development data, which indicates that our methodology appears to be robust for these languages. The Italian and Spanish scores were highest during development, but these scores have dropped to 0.700 and 0.629, respectively. The Italian and Spanish testing scores are more similar to the English and French results, which indicates that our model may perform more consistently across languages than what seemed during development. Therefore, the decrease in performance for Italian and Spanish may be a positive result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we presented a support vector machine approach with multiple features derived from different textual levels inside the documents. The implemented support vector machine made use of a linear kernel function, and the multiple classes that were presented to the classifier were handled using the one-vs-rest scheme. In order to be able to deal with the open-set attribution conditions, we implemented a probability threshold that was taken into account when computing the support vector machine classifications. The textual features that were used in this task consisted of a union of six different feature types that each correspond to a unique representational textual level. As such, we included character-level n-grams, punctuation-level n-grams, tokens-level n-grams, part-of-speech n-grams, dependency relations syntactic n-grams, and token syntactic n-grams. After the hyper-parameter tuning for these features, we obtained an average macro F1 score of 0.687 on the development data, and an average macro F1 score of 0.644 on the test data.</p><p>Even though we outperformed the baseline by 18.7%, we still note that crossdomain authorship attribution studies are challenging. We have demonstrated that more sophisticated features, like the inclusion of dependency tag syntactic n-grams, are capable of capturing those stylometric elements represented in texts, but without help of other feature types their predictive power is not great. However, in combination with other more simple and straightforward lexical n-grams, they do improve model performance. Further research should aim to determine whether these more elaborate textual features are able to provide an accurate and reliable basis that can be used to capture valuable elements of an author's writing style.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,215.19,351.22,168.17,10.41"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. F-scores per feature type per language</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,156.24,147.00,286.07,106.20"><head>Table 1 .</head><label>1</label><figDesc>F-scores per language from the ablation study</figDesc><table coords="9,156.24,167.40,286.07,85.81"><row><cell>Feature</cell><cell>English French Italian Spanish Average</cell></row><row><cell>Not character n-grams</cell><cell>0.547 0.538 0.651 0.637 0.593</cell></row><row><cell>Not punctuation n-grams</cell><cell>0.550 0.678 0.764 0.729 0.680</cell></row><row><cell>Not token n-grams</cell><cell>0.545 0.676 0.756 0.729 0.676</cell></row><row><cell>Not POS tag n-grams</cell><cell>0.532 0.626 0.744 0.700 0.651</cell></row><row><cell cols="2">Not dependency tag syntactic n-grams 0.547 0.684 0.766 0.729 0.682</cell></row><row><cell>Not token syntactic n-grams</cell><cell>0.538 0.665 0.774 0.736 0.678</cell></row><row><cell>All</cell><cell>0.556 0.684 0.777 0.730 0.687</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,149.44,147.00,299.68,73.70"><head>Table 2 .</head><label>2</label><figDesc>F-scores per language with optional ELMo embeddings</figDesc><table coords="10,149.44,167.40,299.68,53.31"><row><cell>Feature</cell><cell cols="2">ELMo English French Italian Spanish Average</cell></row><row><cell cols="2">Dependency tag syntactic n-grams Yes</cell><cell>0.249 0.163 0.298 0.267 0.244</cell></row><row><cell cols="2">Dependency tag syntactic n-grams No</cell><cell>0.199 0.121 0.298 0.267 0.221</cell></row><row><cell>All</cell><cell>Yes</cell><cell>0.556 0.684 0.777 0.730 0.687</cell></row><row><cell>All</cell><cell>No</cell><cell>0.548 0.685 0.777 0.730 0.685</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,138.72,395.28,289.02,8.29;11,146.83,405.94,320.47,8.29;11,146.83,416.60,280.17,8.29;11,146.83,427.26,309.36,8.29;11,146.83,437.92,212.25,8.29" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,310.55,395.28,117.19,8.29;11,146.83,405.94,250.73,8.29">Towards better UD parsing: Deep contextualized word embeddings, ensemble, and treebank concatenation</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/K18-2005" />
	</analytic>
	<monogr>
		<title level="m" coord="11,415.23,405.94,52.08,8.29;11,146.83,416.60,280.17,8.29;11,146.83,427.26,47.17,8.29">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10">October 2018</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.72,448.91,319.93,8.29;11,146.83,459.57,311.11,8.29;11,146.83,470.23,307.13,8.29;11,146.83,480.89,173.94,8.29" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,323.99,448.91,134.66,8.29;11,146.83,459.57,86.51,8.29">Local histograms of character n-grams for authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y Gómez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,250.58,459.57,207.36,8.29;11,146.83,470.23,222.34,8.29">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="288" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.72,491.88,315.65,8.29;11,146.83,502.54,153.35,8.29" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,184.76,491.88,220.97,8.29">Quantitative authorship attribution: An evaluation of techniques</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Grieve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,411.50,491.88,42.87,8.29;11,146.83,502.54,71.96,8.29">Literary and linguistic computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="270" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.72,513.53,323.57,8.29;11,146.83,524.19,300.67,8.29" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,201.65,513.53,260.64,8.29;11,146.83,524.19,17.96,8.29">Author verification by linguistic profiling: An exploration of the parameter space</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">V</forename><surname>Halteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,170.56,524.19,223.89,8.29">ACM Transactions on Speech and Language Processing (TSLP)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.72,535.18,322.27,8.29;11,146.83,545.84,307.83,8.29;11,146.83,556.50,184.82,8.29" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,310.79,545.84,143.87,8.29;11,146.83,556.50,29.73,8.29">Nearest neighbors distance ratio open-set classifier</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R M</forename><surname>Júnior</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">D O</forename><surname>Werneck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">V</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">V</forename><surname>Pazinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>De Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">D S</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,181.72,556.50,64.18,8.29">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="386" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.72,567.49,324.12,8.29;11,146.83,578.15,288.55,8.29;11,146.83,588.81,186.43,8.29" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,308.79,567.49,154.05,8.29;11,146.83,578.15,35.15,8.29">N-gram-based author profiles for authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kešelj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cercone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,199.23,578.15,236.15,8.29;11,146.83,588.81,34.87,8.29">Proceedings of the conference pacific association for computational linguistics</title>
		<meeting>the conference pacific association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="255" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.72,599.80,311.12,8.29;11,146.83,610.46,287.44,8.29;11,146.83,621.12,309.79,8.29;11,146.83,631.78,315.26,8.29;11,146.83,642.44,75.09,8.29" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,193.84,610.46,240.43,8.29;11,146.83,621.12,168.40,8.29">Overview of the author identification task at pan-2018: cross-domain authorship attribution and style change detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tschuggnall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Specht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,332.70,621.12,123.92,8.29;11,146.83,631.78,74.68,8.29">Working Notes Papers of the CLEF 2018 Evaluation Labs</title>
		<editor>et al.</editor>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018/Cappellato. 2018</date>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.72,653.43,325.42,8.29;11,146.83,664.08,295.50,8.29;11,146.83,674.74,168.58,8.29" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,283.49,653.43,180.65,8.29;11,146.83,664.08,79.72,8.29">Arc-hybrid non-projective dependency parsing with a static-dynamic oracle</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Lhoneux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Stymne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,243.96,664.08,198.37,8.29;11,146.83,674.74,101.89,8.29">Proceedings of the The 15th International Conference on Parsing Technologies (IWPT)</title>
		<meeting>the The 15th International Conference on Parsing Technologies (IWPT)<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.72,152.29,320.28,8.29;12,146.83,162.95,300.36,8.29;12,146.83,173.61,201.05,8.29" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,286.34,152.29,172.66,8.29;12,146.83,162.95,75.14,8.29">Improving cross-topic authorship attribution: The role of pre-processing</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,239.60,162.95,207.60,8.29;12,146.83,173.61,91.80,8.29">International Conference on Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="289" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,183.51,319.76,8.29;12,146.83,194.17,307.62,8.29;12,146.83,204.83,44.95,8.29" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,266.73,183.51,187.96,8.29">Predicting good probabilities with supervised learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,158.70,194.17,244.36,8.29">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,214.74,266.85,8.29;12,146.83,225.40,303.44,8.29;12,146.83,236.06,302.83,8.29;12,146.83,246.72,36.47,8.29" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,281.78,214.74,95.81,8.29">Universal dependencies 2.3</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abrams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ž</forename><surname>Agić</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/11234/1-2895" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>Institute of Formal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics, Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,256.63,329.11,8.29;12,146.83,267.29,292.34,8.29;12,146.83,277.95,313.51,8.29;12,146.83,288.61,187.71,8.29" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,287.86,277.95,140.78,8.29">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,434.66,277.95,25.68,8.29;12,146.83,288.61,107.77,8.29">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,298.51,293.76,8.29;12,146.83,309.17,312.42,8.29;12,146.83,319.83,296.83,8.29;12,146.83,330.49,208.83,8.29" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,313.91,298.51,118.20,8.29;12,146.83,309.17,169.48,8.29">Language independent authorship attribution using character level language models</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Keselj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,334.28,309.17,124.97,8.29;12,146.83,319.83,248.33,8.29">Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics</title>
		<meeting>the tenth conference on European chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="267" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,340.40,328.04,8.29;12,146.83,351.06,313.69,8.29;12,146.83,361.72,304.01,8.29;12,146.83,372.38,218.08,8.29" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,447.99,340.40,18.40,8.29;12,146.83,351.06,124.47,8.29">Deep contextualized word representations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,288.79,351.06,171.73,8.29;12,146.83,361.72,304.01,8.29;12,146.83,372.38,44.83,8.29">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="12,138.35,382.29,287.06,8.29;12,146.83,392.95,311.01,8.29;12,146.83,403.61,303.54,8.29;12,146.83,414.27,161.47,8.29" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,216.85,382.29,208.57,8.29;12,146.83,392.95,77.35,8.29">Distant supervision from disparate sources for low-resource part-of-speech tagging</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ž</forename><surname>Agić</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D18-1061" />
	</analytic>
	<monogr>
		<title level="m" coord="12,241.78,392.95,216.07,8.29;12,146.83,403.61,101.24,8.29">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="614" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,424.17,301.96,8.29;12,146.83,434.83,311.24,8.29" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,198.68,424.17,241.63,8.29;12,146.83,434.83,106.52,8.29">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,259.50,434.83,125.90,8.29">Advances in large margin classifiers</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,444.74,326.24,8.29;12,146.83,455.40,300.71,8.29;12,146.83,466.06,203.32,8.29" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,324.75,444.74,136.33,8.29">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,252.44,455.40,195.10,8.29;12,146.83,466.06,140.97,8.29">Information Retrieval Evaluation in a Changing World -Lessons Learned from 20 Years of CLEF</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,475.97,312.46,8.29;12,146.83,486.63,300.68,8.29;12,146.83,497.29,314.41,8.29;12,146.83,507.95,113.13,8.29" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,321.74,475.97,129.07,8.29;12,146.83,486.63,136.18,8.29">Not all character n-grams are created equal: A study in authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,300.25,486.63,147.26,8.29;12,146.83,497.29,314.41,8.29;12,146.83,507.95,42.71,8.29">Proceedings of the 2015 conference of the North American chapter of the association for computational linguistics: Human language technologies</title>
		<meeting>the 2015 conference of the North American chapter of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,517.86,298.71,8.29;12,146.83,528.52,291.14,8.29;12,146.83,539.18,313.20,8.29;12,146.83,549.84,23.25,8.29" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,357.85,517.86,79.22,8.29;12,146.83,528.52,137.56,8.29">Cross-topic authorship attribution: Will out-of-topic data help?</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,298.44,528.52,139.53,8.29;12,146.83,539.18,255.08,8.29">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1228" to="1237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,559.74,294.38,8.29;12,146.83,570.40,287.15,8.29" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,201.58,559.74,177.04,8.29">A survey of modern authorship attribution methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,384.77,559.74,47.96,8.29;12,146.83,570.40,205.76,8.29">Journal of the American Society for information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,580.31,311.49,8.29;12,146.83,590.97,303.28,8.29;12,146.83,601.63,187.77,8.29" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,201.58,580.31,148.37,8.29">Authorship attribution using text distortion</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,367.24,580.31,82.60,8.29;12,146.83,590.97,299.84,8.29">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<title level="s" coord="12,184.71,601.63,42.28,8.29">Long Papers</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1138" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,611.54,314.41,8.29;12,146.83,622.20,294.12,8.29;12,146.83,632.86,88.73,8.29" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,222.90,611.54,214.15,8.29">Ensemble-based author identification using character n-grams</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,146.83,622.20,290.75,8.29">Proceedings of the 3rd International Workshop on Text-based Information Retrieval</title>
		<meeting>the 3rd International Workshop on Text-based Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="41" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,642.76,310.77,8.29;12,146.83,653.42,312.27,8.29;12,146.83,664.08,305.35,8.29;12,146.83,674.74,318.83,8.29" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,233.09,642.76,216.03,8.29;12,146.83,653.42,22.22,8.29">Tokenizing, pos tagging, lemmatizing and parsing ud 2.0 with udpipe</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Straková</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/K/K17/K17-3009.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="12,186.80,653.42,272.30,8.29;12,146.83,664.08,109.30,8.29">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08">August 2017</date>
			<biblScope unit="page" from="88" to="99" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
