<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.51,190.10,81.21,8.60"><forename type="first">Lukas</forename><surname>Muttenthaler</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.02,190.10,57.94,8.60"><forename type="first">Gordon</forename><surname>Lucas</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,240.39,202.06,55.32,8.60"><forename type="first">Janek</forename><surname>Amann</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Copenhagen (UCPH</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FA365B3A7DCCDBFA01EDCA27DDAD6510</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>authorship attribution</term>
					<term>n-grams</term>
					<term>tf-idf</term>
					<term>Support Vector Machine</term>
					<term>Singular Value Decomposition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of authorship attribution (AA) requires text features to be represented according to rigorous experiments. In the current study, we aimed to develop three different n-gram models to identify authors of various fanfictional texts. Each of the three models was developed as a variable-length ngram model. We implemented both a standard character n-gram model (2 -5 gram), a distorted character n-gram model (1 -3 gram) and a word n-gram model (1-3 gram) to not only capture the syntactic features, but also the lexical features and content of a given text. Token weighting was performed through term-frequency inverse-document frequency (tf-idf) computation. For each of the three models, we implemented a linear Support Vector Machine (SVM) classifier, and in the end applied a soft voting procedure to take the average of the classifiers' results. Results showed, that among the three individual models, the standard character n-gram model performed best. However, the combination of all three classifier's predictions yielded the best results overall. To enhance computational efficiency, we computed dimensionality reduction using Singular Value Decomposition (SVD) before fitting the SVMs with training data. With a run time of approximately 180 seconds for all 20 problems, we achieved a macro F1-score of 70.5% for the development corpus and a F1-score of 69% for the competition's test corpus, which significantly outperformed the PAN 2019 baseline classifier. Thus, we have shown that it is not a single feature representation that will yield accurate classifications, but rather the combination of various text representations that will depict an author's writing style most thoroughly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Authorship Attribution (AA) is the task of determining the author of a text from a set of candidates. It can be a daunting exercise for both humans and machines, if one does not know which parts of a document represent an author's writing style. However, if features are represented according to rigorous experiments (e.g., through the use of regular expressions and hyper parameter optimization) and adequately capture the syntactic use of language, it may well support automated systems which aim to recognize a text's author. In the context of machine learning, AA can be regarded as a multi-class, single-label text classification problem <ref type="bibr" coords="2,367.24,225.72,10.01,8.60" target="#b5">[6]</ref>. Its applications include plagiarism detection and forensic linguistics as well as research in literature <ref type="bibr" coords="2,455.08,237.68,10.93,8.60" target="#b3">[4,</ref><ref type="bibr" coords="2,466.02,237.68,10.93,8.60" target="#b10">11]</ref>. For comprehensive surveys of this topic see <ref type="bibr" coords="2,316.34,249.63,10.92,8.60" target="#b4">[5]</ref> and <ref type="bibr" coords="2,347.32,249.63,14.57,8.60" target="#b11">[12]</ref>.</p><p>In this working notes paper, we describe our approach to the cross-domain AA task of the PAN 2019 competition, which comprised of the identification of the authors of fan fiction <ref type="bibr" coords="2,212.57,285.50,10.01,8.60" target="#b7">[8]</ref>. Fan fiction describes literary works written by fans based on a previous, original literary work (also called the fandom). Fan fiction usually includes the characters from the original, it does, however, change or reinterpret other parts of the story, such as settings or endings, or explores a less prominent character in more detail <ref type="bibr" coords="2,183.90,333.32,10.01,8.60" target="#b6">[7]</ref>. In recent years, fan fiction has generated some controversy concerning the violation of intellectual rights <ref type="bibr" coords="2,292.07,345.27,10.01,8.60" target="#b7">[8]</ref>. In this years PAN competition, one of the tasks constitutes of 20 author identification problems in English, French, Italian and Spanish (5 for each language). Each problem featured 9 candidate authors, for each of which 7 known texts were provided. The term cross-domain in this particular context refers to the fact that the texts with known authorship were from different fandoms, whereas the unknown texts were in a single and different fandom <ref type="bibr" coords="2,440.44,405.05,10.02,8.60" target="#b7">[8]</ref>. While PAN 2018 featured a closed set of candidate authors, this years task presents an open set problem: The real author of some tests cases are unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Representation</head><p>Similarly to last year's best performing team <ref type="bibr" coords="2,318.56,501.04,10.01,8.60" target="#b1">[2]</ref>, we deployed three different n-gram models to represent the fan-fictional texts. We implemented both character and word n-gram models. All models are variable length n-gram models as, according to recent experiments, variable length n-gram models both represent an author's style more adequately and yield higher precision and recall scores than fixed n-gram models which do not capture the full scope of syntactic and lexical features <ref type="bibr" coords="2,413.42,560.82,10.48,8.60" target="#b1">[2,</ref><ref type="bibr" coords="2,423.89,560.82,6.98,8.60" target="#b2">3,</ref><ref type="bibr" coords="2,430.88,560.82,6.98,8.60" target="#b5">6]</ref>.</p><p>The first model we implemented, which was a standard character n-gram model, consisted of bigram, trigram, four-gram and five-gram token representations (i.e., 2 -5 grams). Hyperparameter tuning experiments revealed that, 2 -5 gram models yield the best results and represent an author's text more thoroughly than any other additive lower or higher n-gram text representations. However, for this standard character model we also computed an additional vector of unigram punctuation marks, which we then concatenated with all other character n-gram representations after pre-processing computation. We kept punctuation marks as we consid-ered them important features for an author's use of language. The use, and in particular the frequency, of punctuation marks in a given text, reveals crucial information about an author's writing style <ref type="bibr" coords="3,261.92,143.25,10.32,8.60" target="#b0">[1,</ref><ref type="bibr" coords="3,272.24,143.25,6.88,8.60" target="#b3">4]</ref>. Furthermore, we converted each non-zero digit into a 0, since it is not a digit that represents a unique writing style, but rather how numbers are depicted in a given text (e.g., 1k vs. 1000; 1, 000 vs. 1.000). The latter step was implemented for all three models. This particular model primarily captured syntactic and morphological features.  The second model, which may be deemed a distorted character model, consisted of unigram, bigram and trigram token representations (i.e., 1 -3 grams). This model particularly draws attention to punctuation marks, spacing, diacritics, and all other non-alphabetical characters, including numbers <ref type="bibr" coords="3,339.56,371.44,10.02,8.60" target="#b1">[2]</ref>. We replaced all non-diacritics through an asterisk symbol (*) to yield a uniform representation of dispensable characters and hence distorts the text (see Table <ref type="table" coords="3,312.52,395.35,3.50,8.60" target="#tab_0">1</ref>). We deployed this model to account for an author's orthography and punctuation that is not necessarily dependent on other aspects of language use. We suspected that punctuation and orthography might well represent individual features of language. To provide an example, an author might have preferences for ellipses, or might leave out accents in Spanish or French texts. Lastly, we computed a variable-length word n-gram model, which consisted of unigram, bigram and trigram token representations (i.e., 1 -3 grams). As a word ngram model, contrary to a character n-gram model, rather captures the vocabulary and register than morhological and syntactic features and thus draws attention to the content, we removed punctuation marks and special characters as such tokens do not reveal decisive information about an author's choice of words. We only kept alphanumerical characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text Preprocessing</head><p>To summarize, for all three n-gram models, we computed the following text preprocessing steps:</p><p>-Each non-zero digit was represented by a 0 digit place-holder. This step was computed, since different digits neither reveal crucial information about the vocabulary and register nor about the syntax of a given text. What's important is whether a digit or number appeared in a text and how frequently it was used. Furthermore, to gather information about syntactic features, it is indispensable to know how numerical characters were depicted by an author (see section on standard character n-gram model). -Each hyperlink, that appeared in a given text, was replaced by an @ sign. We computed the latter step, as we did not deem hyperlinks crucial features for an author's use of language. What's decisive is whether and how frequently hyperlinks were used within a given text but not the actual hyperlink that was cited. -For each string, new lines denoted as either "\n" or "\t" were replaced by the empty string. -We did not perform text lower-casing as we suppose, that the usage (and in particular its corresponding frequency) of upper-case letters (e.g., nouns, proper nouns, entities, named entities) reveal important information about the writing style of an author. -We did not remove stop words as we believe, that function words (e.g., "the", "a", "have") -which, in the vast majority of documents, are the most frequently used words -reveal decisive insights into an author's use of language. However, we still assign lower weights to terms that appear highly frequently across the entire corpus. -Weighting for both character and word n-gram models was performed through term-frequency inverse-document-frequency (tf-idf) computation to assign lower weights to words, which appear more frequently across the entire corpus, and higher weights to words which are found only in specific documents and thus reveal crucial information about a text. To prevent a division by zero, we computed smooth inverse document frequency instead of normal idf. Smoothing idf means adding a 1 to the denominator within the logarithmic fraction: log N 1+n t . -To decrease dimensionality, enhance computational efficiency and reduce time complexity, we computed Singular Value Decomposition (SVD) for each of the three variable length n-gram models. The final number of dimensions chosen (d = 63) was dependent on the number of samples per problem. As each problem consisted of 63 texts and d cannot be higher than the number of samples in SVD, we deemed this number as an appropriate size for d to capture the most variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Models</head><p>For each of the three models (standard character, distorted character and word ngrams) we developed linear, multi-class Support Vector Machines (SVM). In so doing, we made use of Python's Sklearn library <ref type="bibr" coords="4,299.63,555.39,10.01,8.60" target="#b8">[9]</ref>. Each classifier was cross-validated three times.</p><p>Results of the three individual SVMs were combined in a soft-voting manner: We simply averaged the probabilities for each candidate author across the three models. Initial experiments revealed that a pure averaging yields better results than feeding the probabilities into a new classifier. Hence, we did not deploy a fourth model (see Figure <ref type="figure" coords="4,163.15,627.12,3.50,8.60" target="#fig_1">1</ref>). The soft voting computation was performed as follows:</p><formula xml:id="formula_0" coords="5,275.37,495.36,205.22,25.38">arg max k i =1 p i k (1)</formula><p>where p i is the probabilities vector for each individual model and k = 3 The procedure was as follows:</p><p>-Firstly, the predicted probabilities obtained by each individual classifier were concatenated into a n × d matrix, where n is the number of classifiers (n = 3) and d denotes the number of classes / candidates. -Secondly, for each text, we computed the average among the three 1×d probability vectors, which, according to soft voting, yields a more accurate probabilities depiction than individual probability distributions. -Lastly, for each text, a "new" average probability distribution was computed, which served as the probabilities vector for the final prediction.</p><p>Our classifiers were required to consider that a test text may have been written by an unknown author. To enhance this algorithmic decision-making process, our models classified a text's author as unknown, if the difference between the model's highest and second highest probabilities was below 0.1, or if the highest probability was below 0.25. The latter served as an additional feature (compared to the PAN 2019 baseline model), we regarded as crucial to include in our algorithm. The hyper parameters of the final analysis pipeline are summarized in Table <ref type="table" coords="6,375.74,314.60,3.64,8.60" target="#tab_1">2</ref>. Our final model for the PAN 2019 shared task was deployed on a virtual machine using the TIRA architecture <ref type="bibr" coords="6,134.77,338.51,14.57,8.60" target="#b9">[10]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Table <ref type="table" coords="7,159.19,134.54,4.86,8.60" target="#tab_2">3</ref> summarizes our results obtained for the development corpus. We compared performances between the official PAN 2019 baseline SVM classifier, our three individual classifiers and the soft vote average model. The highest scores for each problem are displayed in bold face. No changes were made to the provided baseline classifier; as such it utilized only character trigrams, a minimum document frequency of 5 (character trigrams that appeared less frequently than five times across the corpus were not included in the vocabulary), no text lower casing and no weighting of the bag of words (only using a count-based approach), a one-vs-rest multi-class strategy. It classified documents as unknown, if and only if, the highest and second highest predictions were less than 0.1 apart. Results showed that the standard character n-gram model performed generally better than the variable length word n-gram or distorted character n-gram model. However, in the vast majority of runs, the soft voting classifier resulted in a higher score than any of the individual models (see Table <ref type="table" coords="7,345.04,632.55,3.50,8.60" target="#tab_1">2</ref>). According to the mean scores obtained for the three individual models, the word n-gram model showed the worst performance. This is in line with the assumption, that authorship manifests itself in style and thus a text's syntax and morphology rather than in a text's vocabulary. Word n-grams primarily encode lexical information about a document, whereas standard character n-grams and diacritic characters n-grams rather capture the syntactic and morphological characteristics of the text, which further reveal information about an author's writing style.</p><p>SVD implementation reduced our algorithm's computational time from approx. 30 minutes to just 180 seconds. In the final evaluation, we, unfortunately, were unable to deploy SVD due to time constraints, which is why the run time for our PAN 2019 model is 30 minutes and not 180 seconds. The macro F1-score (approx. 70%), however, did not change as a result of dimensionality reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Results displayed that the soft voting procedure notably outperformed the individual classifiers. Intuitively, one might assume that averaging over classifiers with lower macro F1-scores might yield a worse and not a better performance. However, we did not perform averaging across the F1-scores, but across the predicted probabilities ŷi obtained for each classifier. This served as an additive factor as all three feature representations were combined into one thorough representation.</p><p>One limitation to our approach might have been that we did not apply a weighted voting procedure. This could and shall be addressed in future research. Moreover, it might be interesting to consider a hard instead of a soft voting procedure. We further encourage others to deploy different machine learning classifiers, such as Multinomial Logistic Regression, Multinomial Naive Bayes or Neural Networks. Additionally, further experiments might compute Principal Component Analysis (PCA) instead of Singular Value Decomposition (SVD) to reduce dimensionality and enhance computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presented our soft voting ensemble classifier for cross-domain authorship attribution. We combined a standard character n-gram model, a distorted character n-gram model and a word n-gram model to achieve more accurate predictions than the individual models themselves. All three models represented the texts as variable length n-grams, which were weighted by term frequency-inverse document frequency (tf-idf). Our algorithm can be perceived as an enhancement of the PAN 2019 baseline system. One may infer from the results we obtained, that authorship attribution models generally benefit from the inclusion of different representations of text. It is not a single feature representation that will yield accurate classifications, but rather the combination of various document representations that will depict an author's writing style.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,309.07,262.73,144.40,8.60;3,309.07,274.68,170.08,8.60;3,309.07,286.64,170.08,8.60;3,309.07,298.59,107.14,8.60"><head>"</head><label></label><figDesc>***," *** ****, "*** ***** ** *** ****..." "***," *** *****. "** ***** ** *** ***** ***** ** ** ****, *** ** ***** ** ***** **. **** ** * *****'* **** ** ****** **, *** ********</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,550.54,345.82,7.74;5,134.77,561.49,345.83,7.74;5,134.77,572.45,170.28,7.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the model architecture. After text pre-processing, features for the individual models were extracted. The three individual probability outputs are combined through a soft voting procedure into a final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,136.20,225.51,317.90,93.64"><head>Table 1 :</head><label>1</label><figDesc>Showcase of text distortion. The text was extracted from problem set 2.</figDesc><table coords="3,136.20,246.00,233.23,73.15"><row><cell>Original Text</cell><cell>Distorted Text</cell></row><row><cell>"Yes," she says, "the needs of the many..."</cell><cell></cell></row><row><cell>"Yes," you agree. "</cell><cell></cell></row><row><cell>It would be the worst thing in my life, but</cell><cell></cell></row><row><cell>it would be worth it. Even if I weren't able</cell><cell></cell></row><row><cell>to choose it, for whatever</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,162.05,380.63,289.18,199.28"><head>Table 2 :</head><label>2</label><figDesc>Settings of the final model</figDesc><table coords="6,162.05,400.90,289.18,179.02"><row><cell>term extraction</cell><cell>n-gram range</cell><cell>std_char (2-5)</cell></row><row><cell></cell><cell></cell><cell>dist_char (1-3)</cell></row><row><cell></cell><cell></cell><cell>word (1-3)</cell></row><row><cell>feature extraction</cell><cell>tf</cell><cell>sublinear</cell></row><row><cell></cell><cell>idf</cell><cell>smoothed</cell></row><row><cell></cell><cell>norm</cell><cell>L2</cell></row><row><cell></cell><cell cols="2">proportion of n-grams used 0.5</cell></row><row><cell>scaling</cell><cell>MaxAbsScaler</cell><cell></cell></row><row><cell cols="2">dimensionality reduction SVD</cell><cell>63 components</cell></row><row><cell>classification</cell><cell>classifier</cell><cell>SVM</cell></row><row><cell></cell><cell>decision procedure</cell><cell>soft vote</cell></row><row><cell></cell><cell>metric</cell><cell>average</cell></row><row><cell></cell><cell>min difference</cell><cell>0.1</cell></row><row><cell></cell><cell>min probability</cell><cell>0.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,134.77,196.51,345.83,300.88"><head>Table 3 :</head><label>3</label><figDesc>Comparison of Macro F1 scores for our different models on the PAN 2019 AA development corpus</figDesc><table coords="7,166.86,227.74,279.58,269.66"><row><cell cols="4">Problem Language nr test texts baseline char dist word soft vote</cell></row><row><cell>01</cell><cell>en</cell><cell>561</cell><cell>0.695 0.741 0.742 0.631 0.857</cell></row><row><cell>02</cell><cell>en</cell><cell>137</cell><cell>0.447 0.552 0.423 0.455 0.553</cell></row><row><cell>03</cell><cell>en</cell><cell>211</cell><cell>0.491 0.620 0.579 0.489 0.738</cell></row><row><cell>04</cell><cell>en</cell><cell>273</cell><cell>0.331 0.417 0.238 0.299 0.537</cell></row><row><cell>05</cell><cell>en</cell><cell>264</cell><cell>0.473 0.481 0.417 0.475 0.585</cell></row><row><cell>06</cell><cell>fr</cell><cell>121</cell><cell>0.702 0.711 0.655 0.437 0.777</cell></row><row><cell>07</cell><cell>fr</cell><cell>92</cell><cell>0.499 0.551 0.427 0.469 0.588</cell></row><row><cell>08</cell><cell>fr</cell><cell>430</cell><cell>0.506 0.569 0.474 0.411 0.673</cell></row><row><cell>09</cell><cell>fr</cell><cell>239</cell><cell>0.599 0.656 0.636 0.437 0.723</cell></row><row><cell>10</cell><cell>fr</cell><cell>38</cell><cell>0.442 0.544 0.481 0.303 0.658</cell></row><row><cell>11</cell><cell>it</cell><cell>139</cell><cell>0.651 0.708 0.662 0.505 0.780</cell></row><row><cell>12</cell><cell>it</cell><cell>116</cell><cell>0.594 0.685 0.527 0.584 0.658</cell></row><row><cell>13</cell><cell>it</cell><cell>196</cell><cell>0.687 0.762 0.572 0.625 0.786</cell></row><row><cell>14</cell><cell>it</cell><cell>46</cell><cell>0.583 0.680 0.725 0.464 0.750</cell></row><row><cell>15</cell><cell>it</cell><cell>54</cell><cell>0.745 0.778 0.451 0.654 0.785</cell></row><row><cell>16</cell><cell>sp</cell><cell>164</cell><cell>0.768 0.826 0.536 0.705 0.843</cell></row><row><cell>17</cell><cell>sp</cell><cell>112</cell><cell>0.584 0.653 0.497 0.634 0.723</cell></row><row><cell>18</cell><cell>sp</cell><cell>238</cell><cell>0.704 0.803 0.706 0.610 0.823</cell></row><row><cell>19</cell><cell>sp</cell><cell>450</cell><cell>0.556 0.635 0.441 0.505 0.682</cell></row><row><cell>20</cell><cell>sp</cell><cell>170</cell><cell>0.511 0.530 0.141 0.294 0.479</cell></row><row><cell>mean</cell><cell>all</cell><cell>203</cell><cell>0.578 0.645 0.516 0.499 0.705</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.42,142.90,338.17,7.74;9,150.69,153.86,329.91,7.74;9,150.69,164.81,201.91,7.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,187.00,142.90,293.58,7.74;9,150.69,153.86,60.28,7.74">Authorship attribution using principal component analysis and competitive neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Can</surname></persName>
		</author>
		<idno type="DOI">10.3390/mca19010021</idno>
		<ptr target="https://doi.org/https://doi.org/10.3390/mca19010021" />
	</analytic>
	<monogr>
		<title level="j" coord="9,218.85,153.86,181.54,7.74">Mathematical and Computational Applications</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="36" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.42,175.77,338.17,7.74;9,150.69,186.73,303.52,7.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,249.81,175.77,215.70,7.74">EACH-USP ensemble cross-domain authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Custódio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Paraboni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,150.69,186.73,278.39,7.74">Proceedings of the Ninth International Conference of the CLEF Association</title>
		<meeting>the Ninth International Conference of the CLEF Association</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.42,197.69,338.17,7.74;9,150.69,208.65,329.91,7.74;9,150.69,219.61,90.20,7.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,248.80,197.69,231.78,7.74;9,150.69,208.65,98.89,7.74">Gender Identification in Twitter using N-grams and LSA Notebook for PAN at CLEF 2018</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Daneshvar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,267.66,208.65,212.93,7.74;9,150.69,219.61,65.08,7.74">Proceedings of the Ninth International Conference of the CLEF Association</title>
		<meeting>the Ninth International Conference of the CLEF Association</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.42,230.57,338.17,7.74;9,150.69,241.53,329.91,7.74;9,150.69,252.49,329.91,7.74;9,150.69,263.44,139.44,7.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,280.79,230.57,199.80,7.74;9,150.69,241.53,32.68,7.74">N-Gram Feature Selection for Authorship Identification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Houvardas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<idno type="DOI">10.1007/11861461{_}10</idno>
		<ptr target="https://doi.org/10.1007/11861461_10" />
	</analytic>
	<monogr>
		<title level="m" coord="9,215.78,241.53,264.81,7.74;9,150.69,252.49,125.44,7.74">International conference on artificial intelligence: Methodology, systems, and applications</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.42,274.40,338.17,7.74;9,150.69,285.36,329.90,7.74;9,150.69,296.32,153.26,7.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,288.26,274.40,188.76,7.74">Computational methods in authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schlier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.20961</idno>
		<ptr target="https://doi.org/10.1002/asi.20961" />
	</analytic>
	<monogr>
		<title level="j" coord="9,150.69,285.36,283.99,7.74">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="26" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.42,307.28,338.17,7.74;9,150.69,318.24,329.91,7.74;9,150.69,329.20,329.91,7.74;9,150.69,340.16,174.27,7.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,313.68,307.28,166.90,7.74;9,150.69,318.24,129.59,7.74">Improving cross-topic authorship attribution: The role of pre-processing</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-77116-8{_}21</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-77116-8_21" />
	</analytic>
	<monogr>
		<title level="m" coord="9,305.15,318.24,175.44,7.74;9,150.69,329.20,170.55,7.74">International Conference on Computational Linguistics and Intelligent Text Processing</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="289" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.42,351.12,338.17,7.74;9,150.69,362.07,329.91,7.74;9,150.69,373.03,256.08,7.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,235.39,351.12,241.54,7.74">Beyond Canonical Texts: A Computational Analysis of Fanfiction</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Milli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bamman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1218</idno>
		<ptr target="https://doi.org/10.18653/v1/d16-1218" />
	</analytic>
	<monogr>
		<title level="m" coord="9,162.98,362.07,317.62,7.74;9,150.69,373.03,25.81,7.74">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2048" to="2053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.42,383.99,338.17,7.74;9,150.69,395.77,280.75,6.31" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><surname>Pan</surname></persName>
		</author>
		<ptr target="https://pan.webis.de/clef19/pan19-web/author-identification.html" />
		<title level="m" coord="9,171.55,383.99,214.01,7.74">PAN @ CLEF 2019: Cross-domain Authorship Attribution</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.42,405.91,338.17,7.74;9,150.69,416.87,329.91,7.74;9,150.69,427.83,329.91,7.74;9,150.69,438.79,171.03,7.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,265.38,427.83,153.65,7.74">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ã</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,425.64,427.83,54.95,7.74;9,150.69,438.79,91.64,7.74">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.06,449.75,338.53,7.74;9,150.69,460.70,329.91,7.74;9,150.69,471.66,204.87,7.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,335.00,449.75,141.89,7.74">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,265.98,460.70,214.61,7.74;9,150.69,471.66,141.92,7.74">Information Retrieval Evaluation in a Changing World -Lessons Learned from 20 Years of CLEF</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.06,482.62,338.53,7.74;9,150.69,493.58,329.91,7.74;9,150.69,504.54,329.91,7.74;9,150.69,515.50,329.91,7.74;9,150.69,526.46,138.49,7.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,333.50,482.62,147.09,7.74;9,150.69,493.58,152.24,7.74">Not All Character N-grams Are Created Equal: A Study in Authorship Attribution</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/n15-1010</idno>
		<ptr target="https://doi.org/10.3115/v1/n15-1010" />
	</analytic>
	<monogr>
		<title level="m" coord="9,321.68,493.58,158.91,7.74;9,150.69,504.54,329.91,7.74;9,150.69,515.50,70.24,7.74">Proceedings of the 2015 conference of the North American chapter of the association for computational linguistics: Human language technologies</title>
		<meeting>the 2015 conference of the North American chapter of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics (ACL)</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.06,537.42,338.53,7.74;9,150.69,548.38,329.90,7.74;9,150.69,559.34,185.27,7.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,211.84,537.42,205.54,7.74">A Survey of Modern Authorship Attribution Methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.21001</idno>
		<ptr target="https://doi.org/https://doi.org/10.1002/asi.21001" />
	</analytic>
	<monogr>
		<title level="j" coord="9,425.74,537.42,54.85,7.74;9,150.69,548.38,236.63,7.74">Journal of the American Society for information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
