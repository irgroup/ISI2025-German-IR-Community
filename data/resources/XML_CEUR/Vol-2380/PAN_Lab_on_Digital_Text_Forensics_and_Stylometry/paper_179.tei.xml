<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.45,115.90,328.46,12.90;1,215.53,133.83,184.31,12.90;1,223.43,153.68,168.50,10.75">Profiling Twitter Users Using Autogenerated Features Invariant to Data Distribution Notebook for PAN at CLEF 2019</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,235.50,190.08,55.12,8.64"><forename type="first">Tiziano</forename><surname>Fagni</surname></persName>
							<email>tiziano.fagni@iit.cnr.it</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">National Research Council (CNR)</orgName>
								<orgName type="department" key="dep2">Institute of Informatics and Telematics (IIT) via</orgName>
								<address>
									<addrLine>G. Moruzzi 1</addrLine>
									<postCode>56124</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.99,190.08,69.86,8.64"><forename type="first">Maurizio</forename><surname>Tesconi</surname></persName>
							<email>maurizio.tesconi@iit.cnr.it</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">National Research Council (CNR)</orgName>
								<orgName type="department" key="dep2">Institute of Informatics and Telematics (IIT) via</orgName>
								<address>
									<addrLine>G. Moruzzi 1</addrLine>
									<postCode>56124</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.45,115.90,328.46,12.90;1,215.53,133.83,184.31,12.90;1,223.43,153.68,168.50,10.75">Profiling Twitter Users Using Autogenerated Features Invariant to Data Distribution Notebook for PAN at CLEF 2019</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E04D5BAC0302913B3576E0D4560A8302</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the diffusion of Web and Social Media, automatic user profiling classifiers applied to digital contents have become extremely important in application contexts related to social and forensic studies. In many research papers on this topic, an important part of the work is devoted to a costly manual "feature engineering" phase, where the semantic, syntactic, and often languagedependent features need to be accurately chosen to be relevant for profilation task. Differently from this approach, in this work we propose a Twitter user profiling classifier which exploits deep learning techniques to automatically generate user features being a) optimal for user profilation task, and b) able to fight covariance shift problem due to data distribution differences in training and test sets. In the best configuration found, the built system is able to achieve very interesting accuracy results on both English and Spanish languages, with an average final accuracy of more than 0.83.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since the 19th century, authorship identification (AI) analysis have been used in several contexts <ref type="bibr" coords="1,170.42,472.50,16.60,8.64" target="#b12">[13,</ref><ref type="bibr" coords="1,187.02,472.50,12.45,8.64" target="#b27">28]</ref> in order to draw some conclusion about specific cases where the original author was uncertain. In the last years, with the advent of Web and Social Media, the attention of researchers has moved to the analysis of digital contents (e-mail, blogs, Twitter, etc.) in contexts related to social studies and forensic analysis. Author profiling, one the main sub-tasks of AI, is a method of analyzing a corpus of texts in order to identify the main stylistic and content-based features characterizing the user profiles (e.g., gender, age, etc.) we are interested in. Every year, PAN workshop proposes a specific challenge on this topic, offering multilingual annotated datasets where participants can test their own techniques. This year, "Bots and Gender Profiling" task is focused on the profilation of Twitter users considering both the user's type (bot vs. human) and, in case of human, the user's gender (male vs. female) <ref type="bibr" coords="1,343.33,592.05,15.27,8.64" target="#b17">[18]</ref>. Differently from multimodal data in author profiling task of PAN 2018 <ref type="bibr" coords="1,303.41,604.01,15.27,8.64" target="#b18">[19]</ref>, this year we only have textual contents ready for analysis, making it similar to the edition of 2017 <ref type="bibr" coords="1,369.42,615.96,15.27,8.64" target="#b19">[20]</ref>.</p><p>Previous editions have shown a prevalence of software solutions based on traditional machine learning approaches, based on costly manual feature engineering phases. Except for some works, the usage of language modeling and deep learning techniques for textual features encoding has been quite limited and the level of accuracy reachable by using only these techniques is unclear. In this work we thus propose a software solution strongly based on these recent approaches able to a) perform textual encoding in a language-agnostic way and with almost no manual feature engineering, and b) attenuate the data covariate shift <ref type="bibr" coords="2,243.85,203.00,16.60,8.64" target="#b23">[24]</ref> between training and test datasets in order to improve classifier accuracy.</p><p>The rest of this paper is organized as follow. In Section 2 we briefly introduce related works on this topic, and in Section 3 we show how we performed text encoding. After having introduced the covariate shift problem of challenge datasets in Section 4, we describe in details the design of the proposed solution in Section 5, the experimental results in Section 6, and the conclusions on Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Until 2017, "Author profiling" task in PAN challenges was focused on age and gender identification <ref type="bibr" coords="2,190.64,345.12,15.77,8.64" target="#b21">[22,</ref><ref type="bibr" coords="2,206.41,345.12,11.83,8.64" target="#b20">21]</ref>, while in 2017 the age part was replaced by a sub-task focused on language variety identification <ref type="bibr" coords="2,273.73,357.08,15.27,8.64" target="#b19">[20]</ref>. In 2018, the main focus of the challenge was instead targeted to gender prediction but using different types of data sources: text and images. In the following, let's describe the types of software solutions proposed on author profiling task in the last 2 editions of the challenge.</p><p>Two main different types of feature representation have been used in past works. A first one is a traditional approach with features (content-based or style-based) selected through a manual "feature engineering" phase. Many works <ref type="bibr" coords="2,377.09,429.05,11.62,8.64" target="#b8">[9,</ref><ref type="bibr" coords="2,388.71,429.05,7.75,8.64" target="#b1">2,</ref><ref type="bibr" coords="2,396.45,429.05,7.75,8.64" target="#b7">8,</ref><ref type="bibr" coords="2,404.20,429.05,7.75,8.64" target="#b2">3,</ref><ref type="bibr" coords="2,411.95,429.05,11.62,8.64" target="#b25">26]</ref> use chars and word n-grams (or a combination of them) for basic feature representations, while others <ref type="bibr" coords="2,150.13,452.96,16.00,8.64" target="#b14">[15,</ref><ref type="bibr" coords="2,166.13,452.96,8.00,8.64" target="#b5">6,</ref><ref type="bibr" coords="2,174.12,452.96,8.00,8.64" target="#b1">2,</ref><ref type="bibr" coords="2,182.12,452.96,12.00,8.64" target="#b13">14]</ref> also used stylistic features obtained by counting or computing ratios for stopwords, hashtags, user mentions, character flooding, emoticons, and laugher expressions. Another type of feature representation, similar in spirit to what we propose in this work, has been the usage of features obtained through neural networks. Some works <ref type="bibr" coords="2,161.47,500.78,12.17,8.64" target="#b0">[1,</ref><ref type="bibr" coords="2,173.65,500.78,12.17,8.64" target="#b9">10,</ref><ref type="bibr" coords="2,185.82,500.78,12.17,8.64" target="#b26">27]</ref> exploited word embeddings, chars embeddings, or a their combination with traditional features to encode textual contents. In this work, in addition to this standard approaches, we propose also a deep learning network able to learn the latent syntax encoding of a text (see Section 3.2).</p><p>The classification approaches used by many works <ref type="bibr" coords="2,366.05,548.84,11.70,8.64" target="#b0">[1,</ref><ref type="bibr" coords="2,377.75,548.84,7.80,8.64" target="#b8">9,</ref><ref type="bibr" coords="2,385.55,548.84,11.70,8.64" target="#b25">26,</ref><ref type="bibr" coords="2,397.26,548.84,7.80,8.64" target="#b2">3]</ref> are based on traditional machine learning algorithms like SVM, Random Forest, logistic regression, Naive-Bayes, or ensemble of these methods. An increasing number of works <ref type="bibr" coords="2,439.08,572.75,16.60,8.64" target="#b11">[12,</ref><ref type="bibr" coords="2,455.68,572.75,12.45,8.64" target="#b9">10,</ref><ref type="bibr" coords="2,468.13,572.75,12.45,8.64" target="#b24">25]</ref> use instead deep learning approaches adopting mainly RNN (like LSTM, GRU) and CNN nets. Often these networks are combined together or mixed with traditional machine learning methods in order to get best of both worlds or differentiate the strategy used to analyze a specific type of content (e.g., texts and images). In this work we heavily exploit deep learning algorithms to generate automatically a set of optimal and distribution-invariant user features for problem's representation, while we use SVM over these computed features to build the final classifier.</p><p>A common approach to encode textual contents is to use a NLP (Natural Language Processing) processing pipeline <ref type="bibr" coords="3,251.61,153.76,15.27,8.64" target="#b22">[23]</ref>. This type of solution suffers from a costly "feature engineering" phase necessary to try and test several types of features in order to obtain sufficiently informative data representations for the problem to deal with. Furthermore, domain knowledge and the availability of high quality analysis tools in the target language are critical for successfully performing this step: unfortunately these conditions can not always be met. In order to overcome these limitations, in the following we propose 3 different approaches based on language modeling techniques and deep learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">W2V: a Semantic Encoding Based on Word2Vec</head><p>The first type of text encoding used (called W2V) is based on the usage of a language modeling algorithm very popular in the last years: word2vec <ref type="bibr" coords="3,384.67,296.37,15.27,8.64" target="#b10">[11]</ref>. This technique allow to compute word vectors (called embeddings) by analyzing a set of unannotated texts according to an objective function based on distributional hypothesis <ref type="foot" coords="3,428.48,318.61,3.49,6.05" target="#foot_0">1</ref> . The resulting vectors have very interesting properties matching the relationship existent between words (e.g., W ("queen") ∼ = W ("king") -W ("man") + W ("woman")).</p><p>Given a raw tweet, in order to obtain a vector representation to be used as text encoding with machine learning methods, we proceed as following. We first convert the text to lowercase, next we replace each distinct URL with the word "url" and we remove all punctuation characters. If present, we remove also all the emoticons occurring in the text. The remaining textual content is then tokenized into distinct tokens (eventually appearing more than 1 time) and the final encoding vector is computed as</p><formula xml:id="formula_0" coords="3,242.89,434.75,69.63,30.32">V tweet = 1 N N i=1</formula><p>we(token(i))</p><p>where V tweet is the final tweet vector, N is the total number of tokens belonging to final tokenized tweet, token(i) is the function returning the token at position i in the tokenized vector, and we(x) is the function returning the word vector associated to token x.</p><p>For both target languages, we used precomputed 300-dimensional embedding models built on big data collections: GoogleNews for English<ref type="foot" coords="3,349.33,532.35,3.49,6.05" target="#foot_1">2</ref> and SBWCE for Spanish<ref type="foot" coords="3,455.15,532.35,3.49,6.05" target="#foot_2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SYN: a Syntax-Modeling Encoding Based on Deep Learning</head><p>The W2V encoding described previously has the main drawback in trying to only capture semantic information from Twitter data, skipping all the remaining knowledge related to write-style and syntax used by the different type of users to write tweets. In  order to extract syntax-related information from textual data, we proposed an encoding method based on deep learning technology, as shown on Figure <ref type="figure" coords="4,389.82,358.23,7.93,8.64" target="#fig_0">1a</ref>.</p><p>The original tweet text is tokenized before being fed to the encoding neural network. The tokenization process is performed in the following way. We replace all distinct URLs with the word "url" but we do not remove neither the punctuation nor the emoticons found on text. Each distinct user mention (e.g. @realdonaldtrump) and hashtag (e.g. #politic) is replaced respectively by the words mention and hashtag. Each remaining word is transformed into one of these 3 words: uppercase_w for a completely uppercase word, lowercase_w for a completely lowercase word, and mixed_w for a word including both uppercase and lowercase characters.</p><p>In order to learn a tweet encoder using the architecture proposed in Figure <ref type="figure" coords="4,454.10,473.35,7.93,8.64" target="#fig_0">1a</ref>, we used only training data split and transform the dataset of our original problem (proposed to classify users into profiles) into a new dataset suitable to classify tweets into user profiles. For each user profile in training data, we thus take its tweets and its original assigned label (bot, female, or male) and we extract 100 different tweets where each one has assigned the label of the user. At the end of this process we have produced 2 new training datasets, one for English and one for Spanish, consisting in respectively 288,000 and 208,000 training documents, which can next be used to learn the two profile classifiers. The proposed neural network uses two sub-networks to learn different feature types. A first part exploits a GRU net <ref type="bibr" coords="4,321.01,580.94,11.62,8.64" target="#b4">[5]</ref> to find interesting temporal patterns in the sequence of input tokens. A second sub-network based on a CNN <ref type="bibr" coords="4,424.53,592.90,11.62,8.64" target="#b6">[7]</ref> try instead to identify spatial features which are invariant to respect of original position in the text. These two types of found features are next concatenated together and used to find an optimal vector encoding for the tweet, before performing the final classification.</p><p>The syntax classifiers so obtained are not interesting in themselves, what it does matter for us is the possibility to take the trained nets and extract, as tweet encoding vector, the output produced by "Encoder layer" layer. The resulting vector should be in fact an effective representation of the syntax information convey by the original tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CHARS: a Characters Encoding Based on Deep Learning</head><p>This encoding, differently from W2V method, want to exploit data at sub-word level in order to find interesting recurrent patterns and potentially useful information for user profile classification. The tokenization process of original tweets is simpler than the others two seen encoders. In this case, we have defined the set of valid symbols in the characters dictionary as (A -Za -z0 -9), the punctuation symbols, and all the distinct emoticons found on the training data split. The tokenization process thus simply consists in removing all invalid characters from input raw text. In the same manner as described for SYN encoder, starting from training data split, we create two new different training datasets to solve a tweet ⇒ user_profile classification task. As shown on Figure <ref type="figure" coords="5,134.77,282.41,8.30,8.64" target="#fig_0">1b</ref>, the neural network architecture used to learn the encoder is slightly different from that used in SYN encoder. The network only use a CNN sub-network for encoding the tweet's data and the layer taken as final tweet encoder is the CNN layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Mismatch in Contest Dataset</head><p>The first attempt to build a user's profile classifier for the "Bots and Gender Profiling" task was to take all the data included in train and validation splits, merge them together, and then try to perform a k-fold evaluation <ref type="bibr" coords="5,312.86,379.76,16.60,8.64" target="#b22">[23]</ref> on the resulting dataset using one of the textual encoding defined in the previous section and a simple feed forward neural network classifier. With a similar attempt, we obtained a final software accuracy on profiling task of 0.973 for both English and Spanish classifiers. Unfortunately these results were a lot worse if we used the splits suggested by organizers, with a final classifier accuracy of 0.785 for English and just 0.721 for Spanish. From additional performed tests, we have verified that if just use only training data split, we have no evidence neither of bias nor variance <ref type="bibr" coords="5,282.51,463.45,11.62,8.64" target="#b3">[4]</ref> in the built classifier, the accuracy discrepancy is only evident when the classifier learned on training data is applied on test data of validation split. If we reasonably assume that provided validation split matches the data distribution of the unknown test dataset used to evaluate the system for the challenge, we can infer that there is a "covariance shift" problem <ref type="bibr" coords="5,346.32,511.27,16.60,8.64" target="#b23">[24]</ref> between the two distributions that need to be treated appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The Proposed User's Profile Classifier</head><p>For each language, we designed a single-label multiclass classifier <ref type="bibr" coords="5,400.66,572.75,16.60,8.64" target="#b22">[23]</ref> able to assign a user to exactly 1 of the 3 defined classes: bot, male, and female. The high level architecture of the software is shown on Figure <ref type="figure" coords="5,304.89,596.66,3.74,8.64" target="#fig_1">2</ref>. The tweets of a user timeline are encoded through a specific text encoder TE and then passed to a component called User Encoder with the aim to analyze the timeline and to produce a user vector including useful information for final profilation. The vector so obtained is later passed to an SVM classifier which will determine the label to assign to the user.</p><p>We tested three different types of textual encodings:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture of final user's profile classifier</head><note type="other">TE Tweet 2</note><p>...  W2V This encoding is the same encoding as defined in Section 3.1. The idea here is to exploit only semantic information from original data. W2V-SYN This encoding is the concatenation of W2V encoding and the encoding generated by SYN encoder (described in Section 3.2). The idea in this case is to use a representation conveying both syntactic and semantic information from original data. W2V-SYN-CHARS This encoding is the concatenation of W2V encoding, SYN encoding, and the encoding generated by CHARS encoder (described in Section 3.3).</p><p>The idea here is the same as W2V-SYN representation but with the additional ability to learn also information at sub-string level, useful for handling particular cases such as orthographic errors.</p><p>We used 3 different types of user encoder configurations, as described in the following:</p><p>C BUE A configuration producing final user vector by only applying a BUE encoder (described in Section 5.1). C DUE A configuration producing final user vector by applying in sequence first a BUE encoder and then a DUE encoder (described in Section 5.2). C maskDUE A configuration similar to C DUE but with the addition of a final step used to mask the vector features most discriminative for data distributions (described in Section 5.3).</p><p>In the remainder on the section, we will describe the two proposed core user encoders (BUE and DUE encoders) used in tested configurations, the strategy used to mask most discriminating features, and how we built the final user's profile classifier.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning a Base Twitter User Encoder</head><p>In this section we describe a first type of user encoder (called BUE, Base User Encoder) which is able to process user timelines and provide good vector representations of the user profiles. In order to auto-learn very informative features for the final task, we designed a classification system based on a deep learning architecture (see Figure <ref type="figure" coords="7,134.77,412.45,8.48,8.64" target="#fig_3">3a</ref>) and trained it using only train data split. The final built classifier, thanks to CNN sub-network <ref type="foot" coords="7,184.47,422.74,3.49,6.05" target="#foot_3">4</ref> , learn how to encode properly the sequence of tweets submitted by the user by identifying the space-invariant inter-tweets patterns most relevant to solve the classification problem. In particular, the trained network can be used to compute a good user vector representation starting from its timeline. We tuned the network to have 512 as vector size for user encoding layer (layer "User encoding layer"), while for CNN we used 512 different filters with a window size set to 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Learning a Discriminant User Encoder</head><p>User encodings produced by BUE encoder are optimal user representations for training data split and final classification task, but as we have seen before are suboptimal for encoding and classify test datasets due to different characteristics of data. The main aim of this step is thus to start from BUE vector representations and try to find similar user representations, suitable for classification task but where the features high discriminative for training and test distributions are highlighted and easily recognizable. In order to tackle this issue, we propose a discriminative user encoder (called DUE, Discriminative User Encoder) based on the neural network architecture shown on Figure <ref type="figure" coords="7,468.13,615.96,8.30,8.64" target="#fig_3">3b</ref>.</p><p>This encoder is trained on a new dataset composed by both train and validation data but where each user has different target labels respect to original task: "True" if the user was originally included in validation split, "False" otherwise. The neural network structure is very simple. It takes as input a user vector representation computed by BUE encoder and then in the first hidden dense layer (Discriminant user encoding) try to learn a user representation optimal for this new binary classification task. To ensure that the net would learn a user representation suitable also for original "Bots and Gender Profiling" task, we introduce a constraint of what the net can learn at this level. In particular, we force the net to minimize two different losses in gradient descent optimization: the loss on labels related to data distribution (L distribution ), and the loss due to the difference from learned encoding and the encoding produced by BUE encoder (L user ). With the imposed constraints we aim to obtain a trained net which will produce user vectors of dimension 512 very similar to those produced by BUE encoder but containing some specific feature resulting very discriminative in recognizing the original data distribution of a user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Masking most Discriminating Features</head><p>DUE encoder provides user vector representations containing some high informative features for identifying the origin of the data distribution of a user. A feature is high discriminative for data distribution if, after having built a binary classifier using only the considered feature, we are able to distinguish quite well the distribution origin of a user. To weight the discriminative power of a feature we can use ROC-AUC <ref type="bibr" coords="8,454.05,381.90,15.27,8.64" target="#b22">[23]</ref>, a measure able to telling how much a model is able to distinguish between classes. To measure ROC values for all features, we built a new balanced dataset (D mask ) mixing some data from train split and some from validation split and measured classification performance of each feature using a Random Forest classifier.</p><p>On Figure <ref type="figure" coords="8,192.04,441.67,4.98,8.64">4</ref> we show a typical distribution of the features related to their ROC-AUC values emerged from the tests. The majority of features have a ROC value around 0.55 but a long queue exists on the right with few features having high discriminative power (up to more than 0.8). These last type of features are the those we want to mask in the final user encoding vector. Indeed, without these features the user vector should be less sensible to distribution origin and therefore, in the successive learning phase, we force the classifier to concentrate on the other distribution-invariant features to solve the final task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Building the Final Task Classifier</head><p>The final user's profile classifier has been choosed and tuned using remaining train and validation data not included in dataset D mask . We tested two different classifiers (Random Forest and SVM) but we finally adopted SVM because it performed better in terms of accuracy in all tested configurations. The classifier has been optimized by fine-tuning model parameters through the use of a k-fold validation. For both languages and independently from the used text encoding, the best found configuration uses an RBF kernel and standard parameters "gamma" and "C" set to respectively 0.01 and 0.1 . The best threshold determining the ROC minimal value over which we apply feature masking depends on the adopted text encoding and it will be reported in the experimental section.</p><p>After found the best model parameters and chosen a threshold for ROC value, we built the final optimized classifier using train split as training data and using validation split as test data to measure the accuracy of a specific solution. The final models submitted on early test dataset for evaluation have instead been built using all the data available (the union of data in training and validation splits). Detailed results of the tested configurations will be presented in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>In this section we will report all the results obtained in our experimentation. A detailed description of the dataset used in the challenge is available in <ref type="bibr" coords="9,380.30,460.18,15.27,8.64" target="#b17">[18]</ref>, while the final evaluation on test sets has been obtained by using TIRA platform <ref type="bibr" coords="9,386.28,472.14,15.27,8.64" target="#b15">[16]</ref>. The base measure used to evaluate the goodness of the proposed software solutions is the accuracy <ref type="bibr" coords="9,461.50,484.09,15.27,8.64" target="#b22">[23]</ref>. The software has been implemented entirely in Python using various external libraries for specific functions. In particular, the deep learning part has been developed using Keras <ref type="foot" coords="9,157.75,518.29,3.49,6.05" target="#foot_4">5</ref> with a Tensorflow<ref type="foot" coords="9,235.28,518.29,3.49,6.05" target="#foot_5">6</ref> backend, while we used scikit-learn <ref type="foot" coords="9,383.31,518.29,3.49,6.05" target="#foot_6">7</ref> for traditional machine learning methods.</p><p>For each language, on Table <ref type="table" coords="9,266.83,543.87,4.98,8.64" target="#tab_3">1</ref> we report the system global accuracy on validation dataset obtained a) using the three different strategies seen above for encoding tweets and b) using three ways to encode the user vector before building the final classifier. In particular, we report the 3 types of user encoder configurations introduced in Section 5, together with cut threshold for masking features ("Masking threshold" column). This first type of experiment has the main aim in identifying, for each language and each tweet encoding, the best user encoding which next will be used to test the system performance on early test dataset. For this reason, for each language and each tweet encoding tested, we also report in bold the best user encoding found. On both languages, the results for W2V-SYN and W2V-SYN-CHARS configurations show that the better results are obtained by using C maskDUE configuration, while for W2V the best ones make use of C DUE without using feature masking. The optimal threshold identified for the various configurations is very similar and is always comprised between 0.65 and 0.70. It is also interesting to note that the accuracy for W2V-SYN always increases while passing from C BUE to C maskDUE configurations. W2V-SYN-CHARS instead, thanks probably to a richer representation of input data, seems instead to be a strong competitor also using the base C BUE user encoding.</p><p>On Table <ref type="table" coords="10,190.43,572.75,3.74,8.64" target="#tab_4">2</ref>, we report a comparison between the accuracy obtained for both languages on validation data and on the early test dataset provided by the challenge organizers. The reported results are referred to the best configurations identified on Table <ref type="table" coords="10,475.61,596.66,4.98,8.64" target="#tab_3">1</ref> and show the accuracy obtained on the 2 sub-tasks defined in the competition ("Type task" and "Gender task" columns) together with the averaged global accuracy of the system ("Global" column). For each type of dataset and each language, we have also reported in bold the best results. Globally, the results show clearly that W2V-SYN is the best performer, in particular, "TYPE" TASK "GENDER" TASK Method English Spanish English Spanish Global MAJORITY <ref type="bibr" coords="11,239.75,140.78,14.94,7.77" target="#b17">[18]</ref> 0.5000 0.5000 0.5000 0.5000 0.5000 RANDOM <ref type="bibr" coords="11,237.01,152.14,14.94,7.77" target="#b17">[18]</ref> 0.4905 0.4861 0.3716 0.3700 0.4295 CHAR N-GRAMS <ref type="bibr" coords="11,251.34,163.50,14.94,7.77" target="#b17">[18]</ref> 0.9360 0.8972 0.7920 0.7289 0.8385 WORD N-GRAMS <ref type="bibr" coords="11,252.54,174.85,14.94,7.77" target="#b17">[18]</ref> 0.9356 0.8833 0.7989 0.7244 0.8355 WORD EMBEDDINGS <ref type="bibr" coords="11,261.25,186.21,14.94,7.77" target="#b17">[18]</ref>   on early test dataset and for both languages, it always provided the best accuracy. W2V-SYN-CHARS seems to be competitive only on validation dataset but on early test dataset the accuracy is not very good, especially on "Gender task". A possible explanation of this worst performance respect to W2V-SYN is that the addition of features generated by CHARS encoder confuse the classifier while learning the best parameters to build a classification model invariant to data distribution. W2V is instead the worst performer on both datasets, probably a sign that this type of text encoding has a limited expressivity in representing the data of the classification task.</p><p>If we observe the results from the point of view of languages and classification subtasks, we can note the the proposed configurations always perform better on English, with a remarkable difference in terms of accuracy between the two classification subtasks. In particular, solving the "Type" task seems a lot easier than solving the "Gender" task. This difference is probably due from one side to the greater number of training examples available for "Type task", from the other side for the greater difficulty <ref type="foot" coords="11,465.21,460.50,3.49,6.05" target="#foot_7">8</ref> in identifying the differences in the write style between males and females. Finally, on Table <ref type="table" coords="11,221.70,488.06,4.98,8.64" target="#tab_6">3</ref> we show the accuracy results obtained by our best configuration (W2V-SYN with C maskDUE encoder) on the test dataset used for final evaluation of system performance in the challenge. In addition, on the table we report also the accuracy obtained by various baseline methods proposed by organizers. Our method outperforms all the baselines, in particular seems more effective especially on Spanish language. On both languages, the global accuracy results obtained by our algorithm on final test dataset are quite similar to those obtained on early test dataset (0.8409 and 0.8366 vs. 0.8523 and 0.8250). An interesting observation is that in final test dataset the difference in terms of accuracy between the two sub-tasks "Type" and "Gender" is very similar for both languages, suggesting that the proposed solution is a good balanced system and not sensible to the target language.</p><p>In this work we tackled the "Bots and Gender Profiling" task of PAN 2019 challenge by proposing a software solution making extensive use of state of the art language modeling and deep learning techniques. In particular, these type of algorithms have been used in order to automatically performing feature encoding of textual contents, avoid almost completely the usual costly and manual "feature engineering" phase. In addition, deep learning has been also exploited to find textual features invariant to train and test datasets, helping fighting data covariate shift which usually has remarkable negative effect on final classifier accuracy. The experimental results show that, among all tested configurations, the solution using W2V-SYN textual encoding and combined with user encoder C maskDUE is globally the best performer on both validation and early test datasets. In particular, on the final test set, such configuration allow to obtain a balanced multilingual classifier able to obtain very interesting accuracy results on both languages, with a global combined accuracy of 0.8387.</p><p>Possible future works could try to use more advanced language modeling techniques like BERT and Elmo for encoding. These methods provide contextual embeddings for text representation, by automatically disambiguate both the terms and the used syntax in analyzed contents. A comparison with the textual encoding methods proposed here could thus be very helpful in order to address future research directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,157.17,302.52,301.02,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Deep learning architectures for SYN and CHARS tweet encoders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,194.71,311.01,225.94,8.64"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: High level architecture of the proposed system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,157.17,310.20,301.02,8.64"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Deep learning architectures for SYN and CHARS tweet encoders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,238.47,72.92,239.43,15.32;9,238.47,90.12,53.90,15.32;9,415.12,142.45,113.75,10.41;9,415.12,153.91,93.42,10.41;9,428.49,165.37,110.86,10.41;9,428.49,176.83,42.31,10.41;9,220.51,277.74,211.05,10.41;9,190.96,274.23,233.44,8.64"><head>3 )</head><label>3</label><figDesc>Mask most important discriminant features ▷ RandomForest classifier ▷ Mask features with ROC-AUC value &gt;= chosen threshold. Features obtained by DistributionUserEncoder net Figure 4: ROC-AUC values computed for all user features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,151.72,117.72,311.93,270.93"><head>Table 1 :</head><label>1</label><figDesc>Classification global accuracy of the software on validation data split.</figDesc><table coords="10,161.32,117.72,292.72,270.93"><row><cell cols="2">Tweet encoding</cell><cell cols="6">C BUE C DUE Masking threshold C maskDUE</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ENGLISH</cell><cell></cell><cell></cell><cell></cell></row><row><cell>W2V</cell><cell></cell><cell cols="3">0.7960 0.8032</cell><cell>0.65</cell><cell></cell><cell>0.7998</cell></row><row><cell cols="2">W2V-SYN</cell><cell cols="3">0.8065 0.8225</cell><cell>0.65</cell><cell></cell><cell>0.8282</cell></row><row><cell cols="5">W2V-SYN-CHARS 0.8203 0.8169</cell><cell>0.70</cell><cell></cell><cell>0.8266</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SPANISH</cell><cell></cell><cell></cell><cell></cell></row><row><cell>W2V</cell><cell></cell><cell cols="3">0.6920 0.6967</cell><cell>0.65</cell><cell></cell><cell>0.6945</cell></row><row><cell cols="2">W2V-SYN</cell><cell cols="3">0.7670 0.7695</cell><cell>0.65</cell><cell></cell><cell>0.7741</cell></row><row><cell cols="5">W2V-SYN-CHARS 0.7789 0.7771</cell><cell>0.70</cell><cell></cell><cell>0.7793</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">VALIDATION SPLIT</cell><cell cols="3">EARLY TEST DATASET</cell></row><row><cell>Tweet encoding</cell><cell cols="2">User encoding</cell><cell>Type task</cell><cell>Gender task</cell><cell>Global</cell><cell>Type task</cell><cell>Gender task</cell><cell>Global</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ENGLISH</cell><cell></cell><cell></cell><cell></cell></row><row><cell>W2V</cell><cell>CBUE</cell><cell></cell><cell cols="6">0.8693 0.7370 0.8032 0.8902 0.7576 0.8239</cell></row><row><cell>W2V-SYN</cell><cell cols="8">CmaskDUE 0.9080 0.7483 0.8282 0.9091 0.7955 0.8523</cell></row><row><cell cols="9">W2V-SYN-CHARS CmaskDUE 0.8967 0.7564 0.8266 0.8939 0.7424 0.8181</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SPANISH</cell><cell></cell><cell></cell><cell></cell></row><row><cell>W2V</cell><cell>CBUE</cell><cell></cell><cell cols="6">0.8630 0.5304 0.6967 0.8778 0.6889 0.7833</cell></row><row><cell>W2V-SYN</cell><cell cols="8">CmaskDUE 0.8863 0.6619 0.7741 0.8944 0.7556 0.8250</cell></row><row><cell cols="9">W2V-SYN-CHARS CmaskDUE 0.8826 0.6760 0.7793 0.8778 0.6722 0.7750</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,138.45,402.24,338.46,8.64"><head>Table 2 :</head><label>2</label><figDesc>Comparison of software accuracy between validation and early test datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,134.77,232.69,345.82,32.55"><head>Table 3 :</head><label>3</label><figDesc>Final software accuracy on challenge test set using W2V-SYN and C maskDUE configurations. On the table we report also the accuracy of the baselines proposed by challenge organizers.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,623.83,335.86,7.77;3,144.73,634.79,33.12,7.77"><p>The meaning of this hypothesis is that words appearing in similar contexts often have a similar meaning.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,645.94,118.69,7.77"><p>Available at http://tiny.cc/0vhz6y</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,657.08,129.38,7.77"><p>Available at https://bit.ly/2Q9DBzU</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="7,144.73,635.17,335.86,7.77;7,144.73,646.13,335.86,7.77;7,144.73,657.08,335.86,7.77"><p>We tested other variants of deep learning architectures (e.g., using sub-nets based on LSTM, GRU, or a combination of them with CNN) but we finally choosed CNN because it was able to return back slightly better user vectors encoding (giving better effectiveness on final classifier).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="9,144.73,634.95,104.87,7.77"><p>Available at https://keras.io/ .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="9,144.73,646.10,148.99,7.77"><p>Available at https://www.tensorflow.org/ .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="9,144.73,657.08,154.61,7.77"><p>Available at https://scikit-learn.org/stable/ .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="11,144.73,635.17,335.86,7.77;11,144.73,646.13,335.86,7.77;11,144.73,657.08,185.52,7.77"><p>If we manually inspect a timeline of a human user of the dataset, it is not always so easy identify its gender. Generally, a bot user is instead easily recognizable because certain types of writing patterns occurred frequently in the timeline.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.61,403.99,327.75,7.77;12,150.95,414.95,207.84,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,298.57,403.99,171.79,7.77;12,150.95,414.95,79.25,7.77">Twitter author profiling using word embeddings and logistic regression</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Akhtyamova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cardiff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ignatov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,248.19,414.95,84.45,7.77">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,426.04,318.64,7.77;12,150.95,436.99,110.60,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,288.69,426.04,156.85,7.77">Arabic tweeps gender and dialect prediction</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Alrifai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rebdawi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ghneim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,150.95,436.99,84.45,7.77">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,448.08,312.42,7.77;12,150.95,459.04,319.76,7.77;12,150.95,470.00,118.80,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,329.75,448.08,125.29,7.77;12,150.95,459.04,73.17,7.77">Word unigram weighing for author profiling at pan 2018</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Daniken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Grubenmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cieliebak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,242.61,459.04,228.10,7.77;12,150.95,470.00,69.49,7.77">Proceedings of the Ninth International Conference of the CLEF Association (CLEF</title>
		<meeting>the Ninth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,481.09,330.27,7.77;12,150.95,491.70,286.25,8.12;12,150.95,503.01,149.53,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,299.88,481.09,168.79,7.77">Neural networks and the bias/variance dilemma</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bienenstock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Doursat</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1992.4.1.1</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.1992.4.1.1" />
	</analytic>
	<monogr>
		<title level="j" coord="12,150.95,492.05,57.78,7.77">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="1992-01">Jan 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,514.10,331.21,7.77;12,150.95,525.06,315.74,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,307.96,514.10,165.86,7.77;12,150.95,525.06,44.62,7.77">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,213.45,525.06,167.22,7.77">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,536.15,317.45,7.77;12,150.95,547.11,304.61,7.77;12,150.95,558.06,329.64,7.77;12,150.95,569.02,229.20,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,335.26,536.15,124.79,7.77;12,150.95,547.11,179.75,7.77">Authorship profiling without using topical information: Notebook for pan at clef 2018</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Esposito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gratton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kanerva</surname></persName>
		</author>
		<ptr target="CEUR-WS" />
	</analytic>
	<monogr>
		<title level="m" coord="12,349.18,547.11,106.38,7.77;12,150.95,558.06,210.07,7.77">19th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2018</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-10">10 September 2018 through 14 September 2018. 2018</date>
			<biblScope unit="page">2125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,580.11,336.53,7.77;12,150.95,591.07,317.20,7.77;12,150.95,602.03,23.90,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,308.34,580.11,170.80,7.77;12,150.95,591.07,55.78,7.77">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,225.07,591.07,183.22,7.77">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,613.12,317.08,7.77;12,150.95,624.08,316.55,7.77;12,150.95,635.04,23.90,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,312.29,613.12,147.40,7.77;12,150.95,624.08,214.34,7.77">Language-and subtask-dependent feature selection and classifier parameter tuning for author profiling</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gómez-Adorno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,383.05,624.08,84.45,7.77">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,646.13,320.75,7.77;12,150.95,657.08,225.17,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,324.04,646.13,139.33,7.77;12,150.95,657.08,96.62,7.77">Pan 2017: Author profiling-gender and language variety prediction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Martinc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Skrjanec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zupan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pollak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,265.53,657.08,84.45,7.77">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,119.96,329.54,7.77;13,150.95,130.92,318.76,7.77;13,150.95,141.88,74.21,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,272.51,119.96,199.27,7.77;13,150.95,130.92,28.11,7.77">Multilingual gender classification with multi-view deep learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Martinc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Skrlj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pollak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,197.02,130.92,272.69,7.77;13,150.95,141.88,24.91,7.77">Proceedings of the Ninth International Conference of the CLEF Association (CLEF</title>
		<meeting>the Ninth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,152.16,327.51,7.77;13,150.95,163.12,311.62,7.77;13,150.95,174.08,299.93,7.77;13,150.95,185.04,168.09,7.77;13,150.95,196.00,186.21,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,363.35,152.16,106.40,7.77;13,150.95,163.12,160.55,7.77">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2999792.2999959" />
	</analytic>
	<monogr>
		<title level="m" coord="13,329.59,163.12,132.98,7.77;13,150.95,174.08,202.70,7.77;13,150.95,185.04,29.42,7.77">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>NIPS&apos;13</note>
</biblStruct>

<biblStruct coords="13,142.24,206.28,338.24,7.77;13,150.95,217.24,214.85,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,343.03,206.28,137.45,7.77;13,150.95,217.24,85.86,7.77">Author profiling with word+ character neural attention network</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ohkuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,255.21,217.24,84.45,7.77">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,227.52,306.98,7.77" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Mosteller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wallace</surname></persName>
		</author>
		<title level="m" coord="13,247.02,227.52,176.05,7.77">Inference and disputed authorship: The federalist</title>
		<imprint>
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,237.80,320.90,7.77;13,150.95,248.76,235.64,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,255.48,237.80,207.65,7.77;13,150.95,248.76,107.32,7.77">Using character n-grams and style features for gender and language variety classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F O</forename><surname>Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,275.99,248.76,84.45,7.77">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,259.04,336.82,7.77;13,150.95,270.00,323.00,7.77;13,150.95,280.96,23.90,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,235.74,259.04,227.48,7.77">Dd. multimodal author profiling for arabic, english, and spanish</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">G</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">G</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,150.95,270.00,299.84,7.77">Proceedings of the Ninth International Conference of the CLEF Association (CLEF</title>
		<meeting>the Ninth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,291.25,335.40,7.77;13,150.95,302.20,309.15,7.77;13,150.95,313.16,209.02,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,333.86,291.25,140.16,7.77">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,259.53,302.20,200.58,7.77;13,150.95,313.16,144.92,7.77">Information Retrieval Evaluation in a Changing World -Lessons Learned from 20 Years of CLEF</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,323.45,308.54,7.77;13,150.95,334.40,324.24,7.77;13,150.95,345.36,221.42,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,307.56,323.45,143.22,7.77;13,150.95,334.40,107.79,7.77">A low dimensionality representation for language variety identification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,276.49,334.40,198.70,7.77;13,150.95,345.36,109.32,7.77">International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="156" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,355.65,323.41,7.77;13,150.95,366.60,316.92,7.77;13,150.95,377.56,241.55,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,229.20,355.65,236.44,7.77;13,150.95,366.60,58.90,7.77">Overview of the 7th Author Profiling Task at PAN 2019: Bots and Gender Profiling</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="13,425.77,366.60,42.10,7.77;13,150.95,377.56,72.99,7.77">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="13,230.42,377.56,85.63,7.77">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019-09">Sep 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,387.85,323.95,7.77;13,150.95,398.81,309.12,7.77;13,150.95,409.76,120.15,7.77" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,394.58,387.85,71.61,7.77;13,150.95,398.81,272.57,7.77">Overview of the 6th author profiling task at pan 2018: multimodal gender identification in twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,428.91,398.81,31.16,7.77;13,150.95,409.76,94.00,7.77">Working Notes Papers of the CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,420.05,325.95,7.77;13,150.95,431.01,319.43,7.77;13,150.95,441.96,61.27,7.77" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,312.90,420.05,155.29,7.77;13,150.95,431.01,224.00,7.77">Overview of the 5th author profiling task at pan 2017: Gender and language variety identification in twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,380.34,431.01,90.04,7.77;13,150.95,441.96,35.12,7.77">Working Notes Papers of the CLEF</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,452.25,329.23,7.77;13,150.95,463.21,329.42,7.77;13,150.95,474.17,326.56,7.77;13,150.95,485.12,90.65,7.77" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,426.76,452.25,44.71,7.77;13,150.95,463.21,231.09,7.77">Overview of the 4th author profiling task at pan 2016: cross-genre evaluations</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,400.04,463.21,80.33,7.77;13,150.95,474.17,123.85,7.77">Working Notes Papers of the CLEF 2016 Evaluation Labs</title>
		<title level="s" coord="13,280.98,474.17,111.69,7.77">CEUR Workshop Proceedings/</title>
		<editor>et al.</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="750" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,495.41,330.50,7.77;13,150.95,506.37,327.30,7.77;13,150.95,517.32,137.86,7.77" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="13,437.74,495.41,35.00,7.77;13,150.95,506.37,133.73,7.77">Overview of the 3rd author profiling task at pan</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Rangel Pardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Celli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,321.55,506.37,156.71,7.77;13,150.95,517.32,78.86,7.77">CLEF 2015 Evaluation Labs and Workshop Working Notes Papers</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,527.61,320.34,7.77;13,150.95,538.22,232.46,8.12;13,150.95,549.52,154.27,7.77" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="13,203.54,527.61,178.98,7.77">Machine learning in automated text categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
		<idno type="DOI">10.1145/505282.505283</idno>
		<ptr target="http://doi.acm.org/10.1145/505282.505283" />
	</analytic>
	<monogr>
		<title level="j" coord="13,388.29,527.61,74.29,7.77">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002-03">Mar 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,559.81,336.55,7.77;13,150.95,570.77,298.50,7.77;13,150.95,581.73,269.23,7.77" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="13,413.80,559.81,64.99,7.77;13,150.95,570.77,282.70,7.77">Direct importance estimation with model selection and its application to covariate shift adaptation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">V</forename><surname>Buenau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,150.95,581.73,183.22,7.77">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="1433" to="1440" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,592.01,319.19,7.77;13,150.95,602.97,329.18,7.77;13,150.95,613.93,70.98,7.77" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="13,430.57,592.01,30.86,7.77;13,150.95,602.97,243.31,7.77">Text and image synergy with feature cross technique for gender identification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tahara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagatani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ohkuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,399.81,602.97,80.33,7.77;13,150.95,613.93,44.83,7.77">Working Notes Papers of the CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,624.21,330.27,7.77;13,150.95,635.17,320.52,7.77;13,150.95,646.13,311.20,7.77;13,150.95,657.08,74.21,7.77" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="13,150.95,635.17,320.52,7.77;13,150.95,646.13,20.05,7.77">Gender identification through multi-modal tweet analysis using microtc and bag of visual words</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">S</forename><surname>Tellez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miranda-Jiménez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moctezuma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Salgado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ortiz-Bejar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,189.46,646.13,272.69,7.77;13,150.95,657.08,24.91,7.77">Proceedings of the Ninth International Conference of the CLEF Association (CLEF</title>
		<meeting>the Ninth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,119.96,313.07,7.77;14,150.95,130.92,320.99,7.77;14,150.95,141.88,209.44,7.77" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="14,370.38,119.96,84.92,7.77;14,150.95,130.92,165.06,7.77">Using translated data to improve deep learning author profiling models</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Veenhoven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Snijders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Van Der Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Van Noord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,334.48,130.92,137.46,7.77;14,150.95,141.88,160.13,7.77">Proceedings of the Ninth International Conference of the CLEF Association (CLEF</title>
		<meeting>the Ninth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,152.84,305.44,7.77;14,150.95,163.45,285.81,8.12" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="14,194.03,152.84,253.64,7.77;14,150.95,163.80,166.86,7.77">On sentence-length as a statistical characteristic of style in prose: With application to two cases of disputed authorship</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">U</forename><surname>Yule</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,323.70,163.80,39.85,7.77">Biometrica</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="363" to="390" />
			<date type="published" when="1939">1939</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
