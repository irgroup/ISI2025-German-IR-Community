<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.33,115.90,310.70,12.90;1,257.77,133.83,99.82,12.90;1,223.43,153.68,168.50,10.75">Bot and gender recognition on tweets using feature count deviations Notebook for PAN at CLEF 2019</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,270.73,190.08,73.89,8.64"><forename type="first">Hans</forename><surname>Van Halteren</surname></persName>
							<email>hvh@let.ru.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Language Studies</orgName>
								<orgName type="institution">Radboud University Nijmegen</orgName>
								<address>
									<postBox>P.O. Box 9103</postBox>
									<postCode>NL-6500HD</postCode>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.33,115.90,310.70,12.90;1,257.77,133.83,99.82,12.90;1,223.43,153.68,168.50,10.75">Bot and gender recognition on tweets using feature count deviations Notebook for PAN at CLEF 2019</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">47C04E3236F75C0BD5187F689C3E1B62</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the system with which I participated in the Author Profiling task at PAN2019, which entailed first profiling Twitter authors into bots and humans, and then the humans into females and males. The system checked to which degree feature counts for a test sample were compatible with the corresponding feature count ranges in the training data. Two features sets were used, one with surface features (token unigrams and character n-grams with n from 1 to 5), the second one with overall measurements (e.g. percentage of retweets, typetoken ratio and variation in tweet length). On the training set, recognition quality was extremely high, but much lower on the test set, indicating that some type of overtraining must have taken place.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Author Profiling task in PAN2019 was the differentiation between bots and humans, and subsequently for humans the differentiation between female and male authors, for samples of 100 tweets in English or in Spanish. A detailed description of the task is given by Rangel and Rosso [11]. <ref type="bibr" coords="1,314.33,477.57,3.49,6.05" target="#b0">1</ref> As early experiments showed a severe risk of overtraining, the organisers provided splits into training and development sets, where the development sets were said to be similar to the eventual test sets. The provided tweets were not preprocessed. My approach for this task 2 built on earlier work. First of all, there was the long term work on authorship and other text classification tasks, which used to be published under the name Linguistic Profiling, which because Copyright c 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CLEF 2019, 9-12 September 2019, Lugano, Switzerland. <ref type="bibr" coords="1,139.00,589.28,2.99,5.18" target="#b0">1</ref> In this paper, I will focus on my own approach. I refer the reader to the overview paper and the other papers on the PAN2019 profiling task for related work. Not only will this prevent overlap between the various papers, but most of the other papers, and hence information on the current state of the art, are not available at the time of writing of this paper. <ref type="bibr" coords="1,139.00,633.30,2.99,5.18" target="#b1">2</ref> I also participated in the Author Attribution task. The differences in handling the two tasks were such that I preferred to describe the other task in a separate paper [5]. However, there will obviously be some overlap.</p><p>of ambiguity of that term has now been replaced by the working title "Feature Deviation Rating Learning System" (henceforth Federales). Although the full name implies a specific learning technique, the acronym indicates a combination approach. Which form of combination was used in this task is described below (Section 3). Furthermore, I reused specific previous work related to the current task. [7] addressed, among other things, recognition of Twitter bots (for Dutch tweets) by noticing that their artificial language use leads to overall measurements (e.g. type-token ratio) different from that of the more variable language use of human authors. [4] addressed gender recognition on, again, Dutch tweets, concluding that counts of all words are the best performing features: with these we measure the authors' (described) life rather than their language use. Although the two studies differed in language (Dutch versus English/Spanish), sample size (full production over several years versus 100 tweets) and time period (when the majority of Twitter users were still reporting on themselves versus when the majority was slowly moving towards business users), I kickstarted the current experiment from the basics of the earlier work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Feature Extraction</head><p>The most important choice in any classification task is the selection of features for the learning components. For this task, I mostly wanted to investigate the potential of features relating to regular, botlike, language use. In support I included more standard features, but kept these simple, by taking only character n-ngrams and token unigrams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tokenization</head><p>As some of the features were to be based on tokens, I tokenized all text samples, using a specialized tokenizer for tweets, as used before for [7]. Apart from normal tokens like words, numbers and dates, it is also able to recognize a wide variety of emoticons. <ref type="bibr" coords="2,476.61,461.15,3.49,6.05" target="#b2">3</ref> The tokenizer is able to identify hashtags and Twitter user names to the extent that these conform to the conventions used in Twitter, i.e. the hash (#) resp. at (@) sign are followed by a series of letters, digits and underscores. URLs and email addresses are not completely covered. The tokenizer counts on clear markers for these, e.g. http, www or one of a number of domain names for URLs. Assuming that any sequence including periods is likely to be a URL proves unwise, given that spacing between normal words is often irregular. And actually checking the existence of a proposed URL was infeasible as I expected the test machine [9] to be shielded from internet. Finally, as the use of capitalization and diacritics is quite haphazard in tweets, the tokenizer strips all words of diacritics and transforms them to lower case. <ref type="bibr" coords="2,323.76,580.70,3.49,6.05" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Surface Frequency Features</head><p>Although I generally prefer to use a wide range of features, including syntactic ones (cf. the notebook on the attribution task [5]), tweets do not lend themselves well for many such features. I therefore decided on more local patterns, also since token unigrams performed best in the experiments in [7]. Given the "informal" nature of tweets, I complemented unigrams with character n-grams (with n from 1 to 5). Token unigrams were built with the normalized tokens, whereas character n-grams were built on the basis of the original tweet. Both types of features were counted separately in original tweet and retweets. In order to be included in the feature set, a feature needed to be observed in at least five different authors (in the full training data). This led to about 1.23M features for English and 0.94M features for Spanish. However, in any specific classification, only those features present in training or test texts were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Overall Measurement Features</head><p>In addition to the straightforward frequency counts, I followed the strategy described in [4]. The idea is that bots show more regular behaviour than humans, as they are driven by algorithms. Such regularities should lead to relatively extreme behaviour, such as low or high numbers of URLs or out-of-vocabulary words. Other examples might be low type-token ratio because a limited vocabulary is used, high type-token ratio because the tweets are less mutually related, or low standard deviations for tweet length. In total I took 71 measurements, including totals over all tweets, means and coefficients of variation for specific measurements per tweet, and some richness measures. Specific examples (those actually used in the experiments) are shown in Tables <ref type="table" coords="3,417.03,398.60,4.98,8.64" target="#tab_0">1</ref> and<ref type="table" coords="3,441.37,398.60,3.74,8.64" target="#tab_1">2</ref>. <ref type="bibr" coords="3,448.84,396.93,9.96,6.05">5 6</ref> Now many of these measurements are mutually correlated. In the initial phases of my work on the task, I handpicked<ref type="foot" coords="3,289.11,420.84,3.49,6.05" target="#foot_4">7</ref> subsets for English and Spanish that together yielded the best classification by themselves. In later phases it turned out that the standard Federales models performed particularly well (on the training data), especially after splitting the data into clusters. There was no time to return to the measurement features, so in principle this part of the system can still be improved. This will have to wait for future work. Information for the features that have been used in the current experiments is listed in Tables <ref type="table" coords="3,262.02,494.24,4.98,8.64" target="#tab_0">1</ref> and<ref type="table" coords="3,288.17,494.24,3.74,8.64" target="#tab_1">2</ref>. The counts for low and high values here are based on a threshold of 2 for a z-score with regard to the mean and standard deviation for human authors, as listed in the tables.<ref type="foot" coords="3,298.30,516.48,3.49,6.05" target="#foot_5">8</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Techniques</head><p>As stated above, I used two types of features. These were processed in different ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Frequency Vector Comparison</head><p>The Federales system builds on the Linguistic Profiling system, which has been used in various studies, such as authorship recognition [1][2], language proficiency [6], source language recognition [3], and gender recognition [7]. The approach is based on the assumption that (relative) counts for each specific feature typically move within a specific range for a class of texts and that deviations from this typical behavior indicate that the deviating text does not belong to the class in question. If the frequency range for a feature is very large, the design of the scoring mechanism ensures that the system mostly ignores that feature. For each feature, the relative counts <ref type="bibr" coords="4,356.29,530.85,3.49,6.05" target="#b8">9</ref> for all samples in the class are used to calculate a mean and a standard deviation. <ref type="bibr" coords="4,334.91,542.80,6.97,6.05" target="#b9">10</ref> The deviation of the feature count for a specific test sample is simply the z-score with respect to this mean and standard deviation, and is viewed as a penalty value. Hyperparameters enable the user to set a threshold below which deviations are not taken into account (the smoothing threshold), a power to apply to the z-score in order to give more or less weight to larger or smaller deviations (deviation power), and a penalty ceiling to limit the impact of extreme deviations. When comparing two classes, a further hyperparameter sets a power value for the difference between the two distributions (difference power), the result of which is then multiplied with the deviation value. The optimal behaviour in cases where a feature is seen in the training texts for the class but not in the test sample, or vice versa, is still under consideration. In the current task, features only seen in the test sample are ignored; features only seen in the training texts are counted as they are, namely with a count of 0 in the test sample. The penalties for all features are added. A set of benchmark texts is used to calculate a mean and standard deviation for the penalty totals, to allow comparison between different models. For verification, the z-score for the penalty total is an outcome by itself; for comparison between two models, the difference of the z-scores can be taken; for attribution within larger candidate sets (such as the clusters described in Section 5), the z-scores can be compared. In all cases, a threshold can be chosen for the final decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Extreme Language Use (XLU) Determination</head><p>The features for the overall measurements could have been mixed into the general feature set, but there their influence would be minimal seeing the enormous number of surface features. Instead I processed these measurements in a different way, for now dubbed an XLU score (eXtreme Language Use). After investigation of the values on the training data, I set a consideration threshold of 0.7 on the z-score. Any feature having a z-score with regard to the mean and standard deviation for human authors higher than 0.7 or lower than -0.7 scores the excess is counted as XLU points. The XLU score for the sample is simply the sum of the XLU points over all selected features. My expectation was that bots should be recognizable by their high XLU score. As explained above (Section 2.3) feature selection was done early in the work on the task and may not be optimal. This is also true for the threshold of 0.7. However, the remaining hyperparameter, a threshold above which an author is predicted to be a bot, has been chosen in the final phases, separately for each cluster (see below in Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training Procedure</head><p>From the initially provided data sets, I held out 400 English authors (200 bots, 100 females, 100 males) and 300 Spanish authors (150 bots, 75 females, 75 males), all randomly selected from the whole data set, as development material. Later during the task, the organisers provided their own train-development split, with 620/310/310 and 460/230/230 samples held out. With both divisions, I trained XLU and Federales models on the training sets (just the human samples for the gender tests) and classified the development sets. Using the gold standard annotations, I selected optimal hyperparameter settings and tested the system. The results were surprising. Where accuracies were over 90% for my original held-out test set, they were much lower (for Spanish gender under 70%) for the organizer train-development split. Although overtraining is likely to play a role (as perfect and near perfect scores are inherently suspicious), the differences between the two train-development splits are much larger than one would expect from simple overtraining effects. Not having information about the exact composition of the train and development sets provided by the organisers, one of my hypotheses <ref type="bibr" coords="6,438.31,360.56,6.97,6.05" target="#b10">11</ref> was that the text in the development set was somehow different from that in the training set. A visual check did not immediately show clear differences. However, when I trained a classifier for distinguishing between training and development set, most settings led to an accuracy over 90% and some even over 95%. Unfortunately, inspection of the most distinguishing unigrams still did not provide a clear picture of the exact difference between the sets. Seeing that no information was available on the nature of the sets, and obviously also not on the composition of the eventual test set, I had to adapt to potential unknown differences. The first step in this was a simplification. The different sets led to very different optimal hyperparameter settings. I therefore decided to drop hyperparameter tuning and select the simplest hyperparameters: no smoothing threshold, a penalty ceiling of 40, no power applied to deviation and to model distance difference. I would have preferred to avoid score thresholds for Federales score as well, which would ideally be at 0. However, it turned out that on the training and development data the optimal thresholds were not 0. As a result, I picked the various thresholds by hand. In addition, thresholds were also needed for XLU scores, which have no natural threshold. These too I picked by hand. An author was predicted to be a bot if either score exceeded its threshold. <ref type="bibr" coords="6,174.34,563.80,6.97,6.05" target="#b11">12</ref> The second adaptation was all but a simplification. As different subsets of the data proved to lead to different outcomes, it seemed a good idea to split the authors into (data-driven) subsets. Any new author could then first be assigned to a subset, after which the models for that subset would be applied. Given the size of the whole data sets, I intuitively decided on seven subsets of authors. How these were derived is described in the next section.</p><p>For both languages, I build frequency lists of normalized original tokens<ref type="foot" coords="7,429.78,141.93,6.97,6.05" target="#foot_10">13</ref> in the full data set, i.e. training plus development. I then examined the top of the list to select around 1000 most frequent tokens. For English this led to a list of 1162 tokens occurring in at least 300 authors and for Spanish 1294 tokens occurring in at least 150 authors. I then built frequency vectors for each sample and used k-means clustering to produce seven clusters. <ref type="bibr" coords="7,227.36,201.71,6.97,6.05">14</ref> The resulting clusters had sizes 189, 448, 270, 138, 418, 277 and 320 for English, and 357, 268, 141, 145, 66, 225 and 298 for Spanish. For these seven clusters per language, I ran Federales classification (with the hyperparameters mentioned above) using the full dataset for both training and testing. Only samples for which the score for their own cluster was higher than 0.5, and for which the second highest scoring cluster was more than 10% behind, were used as prototype samples for the cluster, i.e. used as training samples in the final cluster classification. For English, clusters 1 and 4 kept all their samples, cluster 2 lost 46 (3 assigned to other cluster, 43 unassigned), cluster 3 lost 8 (0, 8), cluster 5 lost 84 (18, 66), cluster 6 lost 17 (6, 11) and cluster 7 lost 20 (0, 20). For Spanish, cluster 1 lost 124 (30 assigned to other clusters, 94 unassigned), cluster 2 lost 27 (3, 24), cluster 3 lost 4 (1, 3), cluster 4 lost 4 (2, 2), cluster 5 lost none, cluster 6 lost 32 (5, 27) and cluster 7 lost 47 (11, 36). In the final classification, the threshold for acceptance was lowered to 0, but the minimum distance to the runner up was kept to 10%. Tables <ref type="table" coords="7,303.15,358.79,4.98,8.64" target="#tab_2">3</ref> and<ref type="table" coords="7,328.25,358.79,4.98,8.64" target="#tab_3">4</ref> show the final attribution to clusters for the training and development sets. For some clusters we see that there are indeed differences between training and development set, e.g. English cluster 5 where predominance of females/males switches from training to development set, or Spanish cluster 4 where the training set still has 18 males on a total of 117 authors but the development set no males at all versus 55 females. We also see that clustering already goes quite far in distinguishing between bots and humans. Females and males on the other hand are well present in all clusters. <ref type="bibr" coords="7,241.29,440.81,6.97,6.05">15</ref> In the prediction phase, each test sample was submitted to an attribution choice between a cluster and the set of all human train samples, for each of the seven clusters. The models for the cluster with the strongest attribution score were applies to that sample. <ref type="bibr" coords="7,473.12,476.75,6.97,6.05">16</ref> Models were also created from the samples in the training data that were not assigned to any sample, to be used for unattributed test samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Training Results</head><p>Application of the finally submitted system on the training and development set led to the confusion tables shown in Tables <ref type="table" coords="7,294.04,570.83,4.98,8.64" target="#tab_4">5</ref> and<ref type="table" coords="7,318.42,570.83,3.74,8.64" target="#tab_5">6</ref>. The corresponding accuracies are also shown in these Tables. For this material, it appeared to be possible to separate bots from humans and females from males with an extremely high level of accuracy. If the test samples were sufficiently similar to the training data, they too should be classified with high accuracy.<ref type="foot" coords="8,192.06,492.43,6.97,6.05" target="#foot_14">17</ref> I would like to point out that, theoretically, the high accuracy was not a natural consequence of testing on the training data. Both XLU and Federales are greedy methods based on means and z-scores over all samples. It was not as if the classifier could recognize an individual sample and reproduce its class. The feature values for each sample were embedded in distributions for all samples in the class training set, which tended to be tens or hundreds of samples.</p><p>On the other hand, the accuracy was too high to be believable. The earlier discrepancy between tests on a small random held-out set and on the organizer-provided train-test split also was reason for doubt. Unfortunately, if indeed there was some regu- larity in the current training data which would not reoccur in the test data, and scores on the test data therefore would turn out to be much lower, more extensive metadata was needed to determine the nature of this regularity and from there the way to adapt the system to become robust against such overtraining. A side-effect on the high accuracy of the Federales models was that the XLU scores hardly played a role anymore. As this was a major focus of my initial plan, I ran a separate prediction with only XLU, with new thresholds. <ref type="bibr" coords="9,369.81,539.73,6.97,6.05">18</ref> The results are found in Table <ref type="table" coords="9,159.14,553.36,3.74,8.64">7</ref>.</p><p>In general, results were very good. For Spanish, the bot-dominated cluster 5 was problematic, as many humans also had high scores. For English, there were no botdominated clusters but cluster 7 had a slightly larger minority of bots and also yielded somewhat lower results; cluster 1 was similar but did not seem to be problematic. Again, these were the results on the training data, with optimal thresholds. Table <ref type="table" coords="10,158.02,115.83,3.36,8.06">7</ref>. Results on the training data for scoring with XLU alone. A threshold of 0 means that all authors in the cluster were predicted to be bot, and 999 that all were predicted to be human. Given the promising results on the training data, I selected these models and thresholds for the system to upload to TIRA for a blind test [9], in order to see if the quality would hold up on the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Test Results</head><p>As stated in the previous section, results on the training data were good, in fact suspiciously good. <ref type="bibr" coords="10,196.99,397.77,6.97,6.05">19</ref> Still, before the test run, the system appeared to be the best choice at that time. The test results showed, however, that it was not. With bot recognition scores of 89.6% (English) and 82.8% (Spanish),<ref type="foot" coords="10,326.47,421.68,6.97,6.05" target="#foot_17">20</ref> the system was not even close to the best scores (96.0% for English and 93.3% for Spanish) <ref type="bibr" coords="10,360.54,433.63,6.97,6.05">21</ref> and worse than the serious baselines, based on character n-grams (93.6%/89.7%), word n-grams (93.6%/88.3%), word2vec (90.3%/84.4%) and LDSE [12] (90.5%/83.7%). The enormous gap between training scores and test scores demonstrates that some kind of overtraining must have occurred. The clustering, which improved scores on the training data, now probably only served to aggravate the overtraining.</p><p>The question, now, is what the cause of the overtraining was. Generally, in machine learning, overtraining is the result of insufficient similarity between training and test data. If the test authors had been drawn randomly from the same pool as the training authors, the system should in principle have done better. If, however, the test authors stem from another source, this would explain the rather disappointing quality in the test run. However, the author profiling task was not presented as a cross-genre task, like the author attribution task was, so this should not be the main cause. To determine which other factor(s) might still have been at work, I will have to investigate the test data and, possibly even more importantly, the metadata describing the sources and their sampling.</p><p>The data that was provided for training could be modeled very well with both feature sets that I applied, token unigrams and character n-grams on one side and overall measurements on the other. Classification quality was high when training on the set as a whole, and improved further after clustering the authors and applying separate thresholds for the various clusters.</p><p>However, the derived model performed disappointingly on the test data. The reasons for this can only be determined when the test data and metadata become available, and will therefore have to wait for future work. This future work will then also have to show the real potential of my proposed approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,205.11,115.83,205.14,216.59"><head>Table 1 .</head><label>1</label><figDesc>Overall measurements for English.</figDesc><table coords="4,205.11,137.15,205.14,195.27"><row><cell>Measurement</cell><cell cols="3">Human Human Low High</cell></row><row><cell></cell><cell cols="3">mean sdev bot% bot%</cell></row><row><cell cols="2">Original tweets (vs retweets) 0.64</cell><cell>0.27 3.8</cell><cell></cell></row><row><cell>Properties original tweets</cell><cell></cell><cell></cell><cell></cell></row><row><cell>@-mentions</cell><cell cols="2">0.057 0.042</cell><cell>0.7</cell></row><row><cell>URLs</cell><cell cols="2">0.029 0.024</cell><cell>21.6</cell></row><row><cell>OOV tokens</cell><cell cols="3">0.15 0.066 8.7 12.2</cell></row><row><cell>Upper case characters</cell><cell cols="3">0.20 0.017 9.9 21.4</cell></row><row><cell>Average IDF</cell><cell>2.64</cell><cell cols="2">0.77 5.4 14.5</cell></row><row><cell>Type-token ratio</cell><cell cols="3">0.43 0.091 29.3 1.8</cell></row><row><cell>Variation length in chars</cell><cell>0.52</cell><cell cols="2">0.16 38.6 0.5</cell></row><row><cell>Variation length in tokens</cell><cell>0.61</cell><cell cols="2">0.18 34.8 0.7</cell></row><row><cell>Variation @-mentions</cell><cell>1.72</cell><cell>1.09</cell><cell>7.1</cell></row><row><cell>Variation hashtags</cell><cell>3.09</cell><cell>2.43</cell><cell>6.1</cell></row><row><cell>Properties retweets</cell><cell></cell><cell></cell><cell></cell></row><row><cell>@-mentions</cell><cell cols="3">0.066 0.022 84.7 0.9</cell></row><row><cell>URLs</cell><cell cols="2">0.023 0.013</cell><cell>7.0</cell></row><row><cell>Average IDF</cell><cell>2.64</cell><cell cols="2">0.53 0.5 5.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,203.98,115.83,207.41,194.67"><head>Table 2 .</head><label>2</label><figDesc>Overall measurements for Spanish.</figDesc><table coords="5,203.98,137.15,207.41,173.35"><row><cell>Measurement</cell><cell cols="3">Human Human Low High</cell></row><row><cell></cell><cell cols="3">mean sdev bot% bit%</cell></row><row><cell cols="2">Original tweets (vs retweets) 0.59</cell><cell>0.29 4.2</cell><cell></cell></row><row><cell>Properties original tweets</cell><cell></cell><cell></cell><cell></cell></row><row><cell>URLs</cell><cell cols="2">0.031 0.034</cell><cell>8.9</cell></row><row><cell>OOV tokens</cell><cell cols="3">0.34 0.058 8.7 11.9</cell></row><row><cell>Type-token ratio</cell><cell cols="3">0.45 0.074 34.8 1.9</cell></row><row><cell>Variation length in chars</cell><cell>0.54</cell><cell cols="2">0.16 54.6 0.2</cell></row><row><cell>Variation length in tokens</cell><cell>0.64</cell><cell cols="2">0.21 30.0 0.4</cell></row><row><cell>Variation @-mentions</cell><cell>2.11</cell><cell>1.44</cell><cell>6.1</cell></row><row><cell>Variation URLs</cell><cell>2.41</cell><cell>1.54</cell><cell>5.7</cell></row><row><cell>Variation hashtags</cell><cell>2.99</cell><cell>2.57</cell><cell>3.9</cell></row><row><cell>Variation OOV tokens</cell><cell>0.52</cell><cell cols="2">0.11 42.5 1.9</cell></row><row><cell cols="2">Variation amount punctuation 0.91</cell><cell>0.46</cell><cell>5.2</cell></row><row><cell>Properties retweets</cell><cell></cell><cell></cell><cell></cell></row><row><cell>URLs</cell><cell cols="2">0.021 0.013</cell><cell>6.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,204.10,115.83,204.91,139.48"><head>Table 3 .</head><label>3</label><figDesc>Cluster composition of training sets for English.</figDesc><table coords="8,217.39,137.15,180.59,118.16"><row><cell></cell><cell></cell><cell>Train</cell><cell></cell><cell>Dev</cell><cell></cell></row><row><cell></cell><cell cols="5">bot female male bot female male</cell></row><row><cell>Cluster 1</cell><cell>25</cell><cell>80</cell><cell cols="2">75 20 33</cell><cell>32</cell></row><row><cell>Cluster 2</cell><cell>6</cell><cell cols="2">117 180 0</cell><cell>37</cell><cell>69</cell></row><row><cell>Cluster 3</cell><cell>0</cell><cell cols="2">58 130 0</cell><cell>22</cell><cell>72</cell></row><row><cell>Cluster 4</cell><cell>1</cell><cell>80</cell><cell>13 0</cell><cell>46</cell><cell>17</cell></row><row><cell>Cluster 5</cell><cell>0</cell><cell cols="2">107 132 0</cell><cell>81</cell><cell>20</cell></row><row><cell>Cluster 6</cell><cell>4</cell><cell cols="3">106 97 12 55</cell><cell>39</cell></row><row><cell>Cluster 7</cell><cell cols="4">33 148 69 12 31</cell><cell>57</cell></row><row><cell cols="3">Not in cluster 1371 24</cell><cell cols="2">24 576 5</cell><cell>4</cell></row><row><cell>Total</cell><cell cols="5">1440 720 720 620 310 310</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,203.61,279.93,205.90,139.48"><head>Table 4 .</head><label>4</label><figDesc>Cluster composition of training sets for Spanish.</figDesc><table coords="8,217.39,301.24,180.59,118.16"><row><cell></cell><cell></cell><cell>Train</cell><cell></cell><cell>Dev</cell><cell></cell></row><row><cell></cell><cell cols="5">bot female male bot female male</cell></row><row><cell>Cluster 1</cell><cell>2</cell><cell cols="2">77 120 4</cell><cell>23</cell><cell>41</cell></row><row><cell>Cluster 2</cell><cell>6</cell><cell cols="2">49 122 5</cell><cell>24</cell><cell>63</cell></row><row><cell>Cluster 3</cell><cell>29</cell><cell>53</cell><cell>55 4</cell><cell>40</cell><cell>21</cell></row><row><cell>Cluster 4</cell><cell>8</cell><cell>99</cell><cell>18 0</cell><cell>55</cell><cell>0</cell></row><row><cell>Cluster 5</cell><cell cols="2">255 35</cell><cell cols="2">46 89 21</cell><cell>12</cell></row><row><cell>Cluster 6</cell><cell>5</cell><cell cols="2">117 33 3</cell><cell>35</cell><cell>34</cell></row><row><cell>Cluster 7</cell><cell>1</cell><cell cols="3">73 109 10 29</cell><cell>47</cell></row><row><cell cols="3">Not in cluster 734 17</cell><cell cols="2">17 345 3</cell><cell>12</cell></row><row><cell>Total</cell><cell cols="5">1040 520 520 460 230 230</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,150.95,115.83,311.22,139.48"><head>Table 5 .</head><label>5</label><figDesc>Confusion table for prediction per cluster on the training material for English.</figDesc><table coords="9,168.80,137.15,277.76,118.16"><row><cell>Actual</cell><cell>bot</cell><cell></cell><cell>female</cell><cell></cell><cell>male</cell><cell></cell><cell>Accuracy</cell></row><row><cell>Predicted</cell><cell cols="6">bot female male bot female male bot female male</cell></row><row><cell>Cluster 1</cell><cell>45</cell><cell></cell><cell>113</cell><cell></cell><cell></cell><cell>107</cell><cell>100</cell></row><row><cell>Cluster 2</cell><cell></cell><cell>6</cell><cell>154</cell><cell></cell><cell></cell><cell>249</cell><cell>98.5</cell></row><row><cell>Cluster 3</cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell>202</cell><cell>100</cell></row><row><cell>Cluster 4</cell><cell>1</cell><cell></cell><cell>126</cell><cell></cell><cell></cell><cell>30</cell><cell>99.4</cell></row><row><cell>Cluster 5</cell><cell></cell><cell></cell><cell>188</cell><cell></cell><cell></cell><cell>152</cell><cell>100</cell></row><row><cell>Cluster 6</cell><cell>16</cell><cell></cell><cell>161</cell><cell></cell><cell></cell><cell>136</cell><cell>100</cell></row><row><cell>Cluster 7</cell><cell>45</cell><cell></cell><cell>178</cell><cell>3</cell><cell>1</cell><cell>123</cell><cell>98.7</cell></row><row><cell cols="2">Not in cluster 1947</cell><cell>2</cell><cell>27</cell><cell>2</cell><cell></cell><cell>26</cell><cell>99.8</cell></row><row><cell>Total</cell><cell>2053 1</cell><cell cols="3">6 2 1027 1 5</cell><cell cols="3">0 1025 99.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,150.45,284.31,312.22,139.48"><head>Table 6 .</head><label>6</label><figDesc>Confusion table for prediction per cluster on the training material for Spanish.</figDesc><table coords="9,169.05,305.63,277.26,118.16"><row><cell>Actual</cell><cell>bot</cell><cell></cell><cell>female</cell><cell></cell><cell>male</cell><cell></cell><cell>Accuracy</cell></row><row><cell>Predicted</cell><cell cols="6">bot female male bot female male bot female male</cell></row><row><cell>Cluster 1</cell><cell>1</cell><cell>5</cell><cell>100</cell><cell></cell><cell></cell><cell>161</cell><cell>98.1</cell></row><row><cell>Cluster 2</cell><cell></cell><cell>11</cell><cell>73</cell><cell></cell><cell></cell><cell>185</cell><cell>95.9</cell></row><row><cell>Cluster 3</cell><cell>33</cell><cell></cell><cell>93</cell><cell></cell><cell></cell><cell>76</cell><cell>100</cell></row><row><cell>Cluster 4</cell><cell>8</cell><cell></cell><cell>154</cell><cell></cell><cell></cell><cell>18</cell><cell>99.6</cell></row><row><cell>Cluster 5</cell><cell>344</cell><cell>2</cell><cell>56</cell><cell></cell><cell></cell><cell>56</cell><cell>99.6</cell></row><row><cell>Cluster 6</cell><cell>8</cell><cell></cell><cell>152</cell><cell></cell><cell></cell><cell>67</cell><cell>96.5</cell></row><row><cell>Cluster 7</cell><cell></cell><cell>11</cell><cell>102</cell><cell></cell><cell></cell><cell>156</cell><cell>95.9</cell></row><row><cell cols="2">Not in cluster 1079</cell><cell>1</cell><cell>19</cell><cell></cell><cell></cell><cell>29</cell><cell>99.9</cell></row><row><cell>Total</cell><cell>1457 16</cell><cell cols="2">27 1 749</cell><cell>0 2</cell><cell>0</cell><cell>748</cell><cell>98.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,602.10,277.92,7.77"><p>The importance of this has dropped seriously after the introduction of emojis.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,144.73,613.25,335.86,7.77;2,144.73,624.21,335.86,7.77;2,144.73,635.17,335.86,7.77;2,144.73,646.13,335.86,7.77;2,144.73,657.08,65.50,7.77"><p>The system has worked suboptimally here, as the check for out-of-vocabulary words was implemented incorrectly, comparing the normalized word forms from the samples with the unnormalized word forms in the word list. For English, the difference was probably negligible, but for Spanish, with all its diacritics, the OOV counts were greatly exaggerated, as we will see in Section 2.3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="3,144.73,536.13,335.86,7.77;3,144.73,547.09,335.86,7.77;3,144.73,558.04,335.86,7.77;3,144.73,569.00,335.86,7.77;3,144.73,579.96,335.86,7.77;3,144.73,590.92,335.86,7.77;3,144.73,601.88,335.86,7.77;3,144.73,612.84,184.31,7.77"><p>Words were called out-of-vocabulary if they did not occur in the word lists I had available. For Spanish I used the file espanol.txt as provided at http://www.gwicks.net/dictionaries.htm. Unfortunately, the words in the list were not normalized as the words from the text were, which led to the rather high OOV measurements of 34% and 52%. However, as both human and bot authors are mismeasured in the same way, the results still hold information. For English I used a wordlist derived from the British National Corpus combined with a wordlist which on double checking the software turned out (major embarrassment) to be the wrong one, namely one for Dutch. Obviously, both lists can be improved upon.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="3,144.73,623.99,335.86,7.77;3,144.73,634.95,40.60,7.77"><p>IDF for English is based on the British National Corpus. For Spanish I did not have access to an IDF list.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="3,144.73,645.94,238.16,7.77"><p>This procedure can be automated, but I did not do this at this time.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="3,144.73,657.08,240.46,7.77"><p>In the actual recognition, all values over 0.7 are taken into account.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6" coords="4,144.73,613.06,335.86,7.77;4,144.73,624.02,335.86,7.77;4,144.73,634.98,115.90,7.77"><p>I.e. the absolute count divided by the corresponding number of items, e.g. count of a token in a retweet divided by all tokens within retweets, or a character n-gram count divided by the number of characters in the text.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7" coords="4,144.73,646.13,335.86,7.77;4,144.73,657.08,219.93,7.77"><p>Theoretically, this is questionable, as most counts will not be distributed normally, but the system appears quite robust against this theoretical objection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8" coords="6,144.73,645.94,151.75,7.77"><p>The others including a bug in my systems.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_9" coords="6,144.73,657.08,335.86,7.77"><p>Unfortunatly, the testing phase showed that this "tuned" thresholding again led to overtraining.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_10" coords="7,144.73,590.76,205.91,7.77"><p>In the full feature set, these are the features marked CTO.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_11" coords="7,144.73,601.91,335.86,7.77;7,144.73,612.87,213.87,7.77"><p>I used the function stats::kmeans in R,<ref type="bibr" coords="7,280.61,601.91,15.44,7.77" target="#b9">[10]</ref> with the Hartigan-Wong method<ref type="bibr" coords="7,410.63,601.91,11.88,7.77" target="#b7">[8]</ref>, a maximum of 40 iterations, 20 restarts and obviously a target of 7 centers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_12" coords="7,144.73,624.02,335.86,7.77;7,144.73,634.98,335.69,7.77"><p>It would be very interesting to investigate how the clusters differ from each other. I have postponed this investigation until the test data and hopefully metadata have become available.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_13" coords="7,144.73,646.13,335.86,7.77;7,144.73,657.08,130.09,7.77"><p>I contemplated a combination of all clusters accepting the sample, but I deemed this too complicated for the current experiments.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_14" coords="8,144.73,624.21,335.86,7.77;8,144.73,635.17,335.86,7.77;8,144.73,646.13,335.86,7.77;8,144.73,657.08,210.40,7.77"><p>The original version of this paper was written before the test results were available. For the revised version, the test scores were known, but not the test data or metadata. I have decided to leave this section in its original form, showing my reasoning before the test phase, and to insert a new section below, commenting on the test results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_15" coords="9,144.73,646.13,335.86,7.77;9,144.73,657.08,64.75,7.77"><p>The thresholds in the submitted run were tuned for optimal correction of mistakes by the Federales models.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_16" coords="10,144.73,623.83,291.85,7.77"><p>This section was inserted into the paper after the test results were made available.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_17" coords="10,144.73,634.98,335.86,7.77;10,144.73,645.94,269.31,7.77"><p>As gender scores (English 74.2% and Spanish 67.3%) are partly based on the bot scores, I cannot judge at this time how well my gender recognition worked by itself.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_18" coords="10,144.73,657.08,118.03,7.77"><p>Not reached by the same system.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.61,294.31,310.57,7.77;11,150.95,305.27,159.28,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,214.72,294.31,222.95,7.77">Linguistic Profiling for authorship recognition and verification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,150.95,305.27,82.02,7.77">Proceedings ACL 2004</title>
		<meeting>ACL 2004</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,316.22,305.46,7.77;11,150.95,326.83,321.05,8.12;11,150.95,338.14,23.90,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,214.72,316.22,233.35,7.77;11,150.95,327.18,56.55,7.77">Author verification by Linguistic Profiling: An exploration of the parameter space</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,213.44,327.18,230.18,7.77">ACM Transactions on Speech and Language Processing (TSLP)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,349.10,319.29,7.77;11,150.95,360.06,324.01,7.77;11,150.95,371.02,23.90,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,214.72,349.10,175.85,7.77">Source language markers in Europarl translations</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,408.37,349.10,53.54,7.77;11,150.95,360.06,49.13,7.77">Proceedings of COLING2008</title>
		<title level="s" coord="11,227.41,360.06,196.97,7.77">International Conference on Computational Linguistics</title>
		<meeting>COLING2008</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="937" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,381.98,291.76,7.77;11,150.95,392.59,251.79,8.12" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,214.72,381.98,215.91,7.77">Metadata induction on a Dutch Twitter corpus. initial phases</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,150.95,392.94,192.02,7.77">Computational Linguistics in the Netherlands Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,403.90,337.33,7.77;11,150.95,414.85,316.42,7.77;11,150.95,425.81,319.53,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,214.72,403.90,265.23,7.77;11,150.95,414.85,37.42,7.77">Cross-domain authorship attribution with Federales, Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="11,405.61,414.85,61.77,7.77;11,150.95,425.81,53.32,7.77">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="11,210.75,425.81,198.98,7.77">Notebook Papers. CEUR Workshop Proceedings. CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Losada</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,436.77,320.28,7.77;11,150.95,447.73,298.20,7.77;11,150.95,458.69,250.06,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,263.29,436.77,199.60,7.77;11,150.95,447.73,39.65,7.77">Linguistic Profiling of texts for the purpose of language verification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Oostdijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,208.35,447.73,240.81,7.77;11,150.95,458.69,38.59,7.77">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">966</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,469.65,309.43,7.77;11,150.95,480.26,205.21,8.12" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,265.76,469.65,127.14,7.77">Gender recognition of Dutch tweets</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Speerstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,398.74,469.65,53.31,7.77;11,150.95,480.61,136.48,7.77">Computational Linguistics in the Netherlands Journal</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="171" to="190" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,491.57,337.98,7.77;11,150.95,502.18,290.72,8.12" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,254.92,491.57,183.66,7.77">Algorithm AS 136: A k-means clustering algorithm</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,444.51,491.57,36.08,7.77;11,150.95,502.53,207.05,7.77">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,513.49,335.03,7.77;11,150.95,524.44,309.15,7.77;11,150.95,535.40,209.02,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,333.86,513.49,140.16,7.77">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,259.53,524.44,200.58,7.77;11,150.95,535.40,144.92,7.77">Information Retrieval Evaluation in a Changing World -Lessons Learned from 20 Years of CLEF</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,546.36,333.38,7.77;11,150.95,557.32,254.66,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,253.56,546.36,209.57,7.77">R: A Language and Environment for Statistical Computing</title>
		<author>
			<orgName type="collaboration" coords="11,150.96,546.36,95.90,7.77">R Development Core Team</orgName>
		</author>
		<ptr target="http://www.R-project.org" />
	</analytic>
	<monogr>
		<title level="m" coord="11,469.63,546.36,5.98,7.77;11,150.95,557.32,133.63,7.77">R Foundation for Statistical Computing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,568.28,318.14,7.77;11,150.95,579.24,314.42,7.77;11,150.95,590.20,313.05,7.77;11,150.95,601.16,23.90,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,229.20,568.28,231.18,7.77;11,150.95,579.24,56.46,7.77">Overview of the 7th author profiling task at PAN 2019: Bots and gender profiling</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="11,423.28,579.24,42.10,7.77;11,150.95,590.20,72.99,7.77">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="11,230.42,590.20,198.98,7.77">Notebook Papers. CEUR Workshop Proceedings. CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Losada</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,612.12,335.79,7.77;11,150.95,623.07,308.78,7.77;11,150.95,634.03,321.10,7.77;11,150.95,644.99,72.97,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,273.28,612.12,204.75,7.77;11,150.95,623.07,46.26,7.77">A low dimensionality representation for language variety identification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,214.97,623.07,244.77,7.77;11,150.95,634.03,202.18,7.77">Proceedings of the 17th International Conference on Intelligent Text Processing and Computational Linguistics (CICLingŠ16)</title>
		<meeting>the 17th International Conference on Intelligent Text Processing and Computational Linguistics (CICLingŠ16)</meeting>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2018">9624. 2018</date>
			<biblScope unit="page" from="156" to="169" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
