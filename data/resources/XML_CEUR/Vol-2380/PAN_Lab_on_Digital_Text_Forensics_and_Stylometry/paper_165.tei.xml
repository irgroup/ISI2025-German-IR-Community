<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.91,115.90,327.54,12.90;1,223.43,135.76,168.50,10.75">Cross-Domain Authorship Attribution with Federales Notebook for PAN at CLEF 2019</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,270.73,172.16,73.89,8.64"><forename type="first">Hans</forename><surname>Van Halteren</surname></persName>
							<email>hvh@let.ru.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Language Studies</orgName>
								<orgName type="institution">Radboud University Nijmegen</orgName>
								<address>
									<postBox>P.O. Box 9103</postBox>
									<postCode>NL-6500HD</postCode>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.91,115.90,327.54,12.90;1,223.43,135.76,168.50,10.75">Cross-Domain Authorship Attribution with Federales Notebook for PAN at CLEF 2019</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6982AB531C9540BDE6AE0C1FF7451E3F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the system with which I participated in the Cross-Domain Authorship Attribution task at PAN2019, which entailed attributing fanfiction texts from a specific "fandom" (texts in a given fictional world) on the basis of training material from other fandoms. As underlying system I used a combination of 5 or 3 (depending on language) feature sets and two author verification methods. In reaction to the genre differences, I added a second round of attribution, this time in-genre, for those authors for whom enough target fandom texts could be identified in the first round. On the training dataset, attribution quality was well over the baseline scores, but with ample space for further improvement. On the test dataset, gain over the baseline scores was lower, indicating some kind of overtraining. In general, performance showed that more work is needed on (automated) hyperparameter selection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Cross-Domain Authorship Attribution task in PAN2019 is the attribution of socalled "fan-fiction" texts in four languages (English, French, Italian and Spanish; see [10] for a full description of the task). <ref type="bibr" coords="1,293.15,474.36,3.49,6.05" target="#b6">1</ref> Fans of popular fiction books or series, e.g. Sherlock Holmes or Harry Potter, write their own stories in the corresponding fictional worlds. The stories in such a world are dubbed a "fandom". The training material for the PAN2019 task was divided into 20 "problems", five for each language, in which a number of texts from a specific fandom are to be attributed to nine known authors or "none of these", in all cases on the basis of seven known texts from each author, stemming from other fandoms. The problems were to be solved in isolation, i.e. it was not allowed to use information from one problem in solving the others. This severely limited the compilation of a background corpus. However, three baseline systems were also provided, one of which (the Impostor baseline [11]) was supported with a set of 5,000 texts per language, from various (unknown) fandoms and authors, which could serve as a background corpus. The unknown texts for each problem also contained texts from other authors than the nine given ones. The number of such texts was 20%, 40%, 60%, 80% and 100% of the number of texts by the target authors. Although it seemed likely that the test material would have a similar composition, this was not specified. <ref type="bibr" coords="2,470.89,177.42,3.49,6.05" target="#b7">2</ref> Authorial style might or might not have been adapted to the style of the emulated book(s), but it is likely that the language use was influenced by topic and general genre. In authorship attribution, it is well-known that the difficulty of the task is much higher if unknown texts are from a different genre than the known texts. In the current task, we seemed to be somewhere in between, as all texts were fiction. However, the fact that all unknown texts were from the same fandom might have pushed the language use in a similar direction for all authors, adding an additional confounding factor.</p><p>My approach for this task<ref type="foot" coords="2,253.59,274.06,3.49,6.05" target="#foot_1">3</ref> built on earlier work on authorship and other text classification tasks, which used to be published under the name Linguistic Profiling, which because of ambiguity of that term has now been replaced by the working title "Feature Deviation Rating Learning System" (henceforth Federales). Although the full name implies a specific learning technique, the acronym already indicates my predilection for combination approaches. Which form of combination was used in this task is described below (Section 3).</p><p>On top of the basic verification system, I wanted to add some way to deal with the genre differences. A first idea was to model each fandom and learn how a specific author behaved in relation to these fandom models. If relative behaviour were consistent between fandoms, correction factors could be applied to target fandom measurements, hopefully leading to a better attribution. However, there was insufficient material outside the known texts to follow this strategy. <ref type="bibr" coords="2,306.72,418.02,3.49,6.05" target="#b9">4</ref> A second option was to ignore all features that appeared to be affected by genre (i.e. fandom). In a small pilot study on English, it turned out that so many features were removed that attribution quality went down rather than up. <ref type="bibr" coords="2,191.95,453.89,3.49,6.05" target="#b10">5</ref> In the end, I did not attempt to apply corrections to the feature measurements. Instead, I took the authors for which a sufficient number of texts was identified with (relatively) high confidence, and used those texts as known texts for a verification model within the target fandom. For those authors, the in-genre models were then used, whereas the other authors had to be attributed with the cross-genre model, albeit with the additional knowledge that some competing authors could be ruled out on the basis of the in-genre verification.</p><p>An additional complication of the shared task was that the final evaluation was in the form of a blind test using TIRA [15]. My main approach on earlier projects was a careful investigation of (the known texts of) each individual problem, followed by a manual combination of the most promising components and system settings. Given that there was insufficient time to fully automate this procedure, I was forced here to choose components (and hyperparameters) for all the (probably varied) test data at once, without having access to the known texts or even knowledge of the involved genres. Moreover, in the development of the current software, I had not yet reached the state of an integrated system, which was obviously needed for TIRA. In the end, complications in building the integrated system took away much time that could have been used on better system tuning.</p><p>In the next sections, I first describe the features I used (Section 2), the verification techniques (Section 3) and how all this fitted together into a full system (Section 4). After this I continue with performance measurements (Section 5) and a short discussion to conclude the paper (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Feature Extraction</head><p>For English, I used both surface and syntactic features. The simplest type was that of the character n-grams, with n from 1 to 6, which were counted directly on the raw text. For all other types, I analysed all texts with the Stanford CoreNLP system [12].</p><p>From the POS tagging, of which an example is shown on the left side of Figure <ref type="figure" coords="3,473.11,620.57,3.74,8.64">1</ref>, I extracted token n-grams, with n from 1 to 3. Each position in the n-gram was taken by the word itself, the POS tag or the word's group. The latter was looked up in a list of word groups deemed relevant for authorship attribution, which I had available only for English and which contained reporting verbs and various types of adverbs. <ref type="bibr" coords="4,457.55,359.44,3.49,6.05" target="#b11">6</ref> The token n-grams were generated in two forms. The lexical form (lex) included the words themselves. In the abstract form (abs) the words were replaced by an indication of their IDF value (low, middle or high) if their IDF exceeded a certain value. <ref type="bibr" coords="4,412.96,395.30,3.49,6.05" target="#b12">7</ref> In this way, the abstract features were supposed to reflect the language use of the author rather than the topics discussed.</p><p>The CoreNLP system also yielded a dependency analysis. However, as the dependency structure was less amenable to variation studies than a constituency structure, I first transformed the trees, aiming for a representation similar to that in the TOSCA Project [1]. <ref type="bibr" coords="4,177.10,473.00,3.49,6.05" target="#b13">8</ref> Apart from restructuring the tree, the transformation program lexicalized the analysis by percolating the head words upwards. As an example the parse of the (English) sentence in Figure <ref type="figure" coords="4,255.46,498.58,4.98,8.64">1</ref> is shown in Figure <ref type="figure" coords="4,346.06,498.58,3.74,8.64" target="#fig_0">2</ref>. From these transformed trees, syntactic features were derived, namely slices from the trees representing presence of constituents, dominance and linear precedence, as well as full rewrites. Again a lexical and an abstract form were generated, this time with the word (or IDF indication) concatenated with the POS tag. <ref type="bibr" coords="4,261.86,544.74,3.49,6.05" target="#b14">9</ref> An overview of the feature types, with examples for English, is given in Table <ref type="table" coords="4,238.18,558.36,3.74,8.64" target="#tab_1">1</ref>.  For all three Romance languages, no appropriate syntactic parser was available. <ref type="bibr" coords="6,473.12,260.73,6.97,6.05" target="#b15">10</ref> Instead, I POS-tagged the texts with FreeLing [14]. An example for French is shown on the right side of Figure <ref type="figure" coords="6,232.60,286.31,3.74,8.64">1</ref>. From this output I extracted only surface features, leading to three different feature sets. Furthermore, the token n-grams were less involved, as no word groups were present, nor IDF statistics, so that the abstract form replaced all words of open word classes.</p><p>For each problem, the system took all known and unknown texts, plus the 5,000text background corpus, and extracted all features which occurred in at least three texts. The numbers of features for each problem and feature type (for the training data) are indicated in Table <ref type="table" coords="6,208.55,371.25,3.74,8.64" target="#tab_2">2</ref>. The large ranges within each language were caused mostly by the greatly varying sizes of the unknown text sets, from 38 texts for problem 10 (French) to 561 for problem 1 (English).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Techniques</head><p>Apart from a combination of feature types, I also used a combination of learning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Value versus Profile Range Comparison</head><p>The Federales system built on the Linguistic Profiling system, which had been used in various studies, such as authorship recognition [4][5], language proficiency [8], source language recognition [6], and gender recognition [9]. The approach is based on the assumption that (relative) counts for each specific feature typically move within a specific range for a class of texts and that deviations from this typical behavior indicate that the deviating text does not belong to the class in question. If the frequency range for a feature is very large, the design of the scoring mechanism ensures that the system mostly ignores that feature.</p><p>For each feature, the relative counts <ref type="bibr" coords="7,292.19,117.64,6.97,6.05" target="#b16">11</ref> for all samples in the class are used to calculate a mean and a standard deviation. <ref type="bibr" coords="7,281.84,129.60,6.97,6.05" target="#b17">12</ref> The deviation of the feature count for a specific test sample is simply the z-score with respect to this mean and standard deviation, and is viewed as a penalty value. Hyperparameters enable the user to set a threshold below which deviations are not taken into account (the smoothing threshold), a power to apply to the z-score in order to give more or less weight to larger or smaller deviations (deviation power), and a penalty ceiling to limit the impact of extreme deviations. When comparing classes, a further hyperparameter sets a power value for the difference between the two distributions (difference power), the result of which is then multiplied with the deviation value. The optimal behaviour in cases where a feature is seen in the training texts for the class but not in the test sample, or vice versa, is still under consideration. In the current task, features only seen in the training texts were counted as they are, namely with a count of 0 in the test sample; features only seen in the test sample were compared against the lowest mean and standard deviation from among the training features, which should correspond more or less to the scores for hapaxes in the training texts.</p><p>The penalties for all features are added. A set of benchmark texts is used to calculate a mean and standard deviation for the penalty totals, to allow comparison between different models. For verification, the z-score for the penalty total is an outcome by itself; for comparison between two models, the difference of the z-scores can be taken; for attribution within larger candidate sets (as in the current task), the z-scores can be compared. In all cases, a threshold can be chosen for the final decision.</p><p>Even though optimal settings for one author were often bad settings for another author, I started with the fallback strategy of using a single, basic choice for the hyperparameters, namely no smoothing threshold, no deviation or difference power (i.e. power 1), and a penalty ceiling of 10. The next step, automated tuning, did not take place anymore, due to lack of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Support Vector Regression</head><p>As a second learning method to assign feature vectors to classes, I used Support Vector Regression as provided by the libsvm package [3]. For each author, vectors for known texts of the author were given class 1 and vectors for texts by other authors with class -1. svm-scale was used with its default settings. Here too, a single, simple list of hyperparameters was used: ν-regression with an RBF kernel, and the defaults for cost (c = 1) and gamma (γ = 1/number_of _f eatures); only ν was set other than the default (0.1 versus a default of 0.5). To correct for a bias towards positive or negative examples because of different numbers of texts in the two classes, the hyperparameter w was used, with weights that exactly compensated for class size. By choosing regression, I received a score rather than a decision on the class, which could then be used in further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sample Score Combination</head><p>After all ten/six individual component runs, the component scores for each were normalized by factoring in a linear model which predicted the component score on the basis of the average component scores for the models and test samples in question, plus the number of feature comparisons made during scoring. The adjusted component score was the deviation from the predicted value. Finally, all adjusted component scores were normalized to z-scores with regard to all observed adjusted component scores for a model and with regard to all observed adjusted component scores for a test sample. These normalized component scores allow for an intuitive interpretation and provide comparability between different author models. The latter goal was needed here both for selecting a common threshold for verification and for score combination of the various components. In addition to the two individual normalized component scores (wrt model and wrt sample), their sum was also calculated. Which of these three normalized scores was used, depended on the phase in the attribution (cf. Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Complete System</head><p>After the normalized scores (henceforth simply "scores") for each feature set and learner became available, the system needed to combine these to determine an attribution for each text. This was done in several phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Phase 1: Cross-Genre Attribution</head><p>In the first phase, the system examined the scores produced by the various models so far. On the basis of the sum of all individual scores,<ref type="foot" coords="8,337.04,412.27,6.97,6.05" target="#foot_11">13</ref> a ranking of potential authors was produced. If the attribution task were a closed one, the first ranked author could now be selected. However, the test texts also included texts by other than the nine known authors. This meant that a decision had to be made whether or not to attribute to a known author at all.</p><p>For this, a number of statistics were calculated per test sample. For each potential author, these were: the mean of the reciprocal rank, the mean of the score, the mean of the distance to the score of the top-ranked author, and the highest and lowest score reached. For the sample as a whole, these were: the sum of the various best scores and distance in the individual models, plus the highest values for the five author statistics. Finally, the first principal component of a PCA of the seven whole-sample statistics was added. <ref type="bibr" coords="8,161.05,543.77,6.97,6.05" target="#b19">14</ref> In the various phases, all of the whole-sample statistics, apart from the reciprocal rank, were used for comparisons against thresholds to determine whether or not to accept the top-ranked author for attribution. The thresholds were set manually per language, in such a way that the average evaluation score over all 45 authors was optimized. <ref type="bibr" coords="8,167.14,603.55,6.97,6.05" target="#b20">15</ref> My hope was that possibly overtraining in this threshold selection process </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The evaluation measure for the PAN2019 attribution task was the open-set macroaveraged F1 score, which was calculated over the training classes without the "unknown" class, but counting false accepts of unknown-class texts against precision [13]. <ref type="bibr" coords="10,473.12,506.83,6.97,6.05">19</ref> Evaluation with the provided python script of my system's results on the training set yielded the scores listed in Table <ref type="table" coords="10,269.90,532.41,3.74,8.64" target="#tab_3">3</ref>. Also in Table <ref type="table" coords="10,337.49,532.41,4.98,8.64" target="#tab_3">3</ref> are the Macro-F1 scores for three baseline systems, for the description of which I refer the reader to [10]. <ref type="bibr" coords="10,418.07,542.70,6.97,6.05">20</ref> For the training material, my system outperformed the three baselines, but with substantial variation per problem (and even more so per author, but those results are not shown here for lack of space). It is positive that the largest improvement with regard to the baselines was for English, where overall the system reached a 25% gain in Macro-F1 over the best baseline (SVM). This is the language I have mostly been working on so far and this is the language for which the syntactic features could be used. For the Romance languages, my system was much less evolved. For Spanish, the relative performance (19% performance gain over again SVM) was therefore rather satisfying. Apparently, the syntactic structure used for English is reflected sufficiently in the morphology here. Italian was somewhere in between, with 9% gain, this time over compression. But French was apparently a serious problem, with a gain of only 1.5% (over compression) and a best score only for problem 8.</p><p>For the test material, the system performed much worse. For English, the Macro-F1 of 0.532 was only 8% over the compression baseline (0.493). For French, my 0.554 was even lower than compression's 0.595. For Italian, the test run went better than the training run, with 0.653 and a 12.5% improvement over the best baseline (0.580, again compression). For all three languages, the best scores were much higher: English 0.665, French 0.705 and Italian 0.717, all by "muttenthaler". Spanish was an exception, with SVM as the best baseline (0.577) and "neri" the best participating system (0.679), so that my 0.652 ended up on rank 3 with 13% over SVM, still lower than the training performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>For the PAN2019 Cross-Domain Authorship Attribution task, I built a system combining various feature types and two classification methods, and furthermore attempting to apply in-genre models when enough training texts could be identified in the cross-genre recognition. Under pressure of the shared task deadline, after losing too much time on building an integrated system that could run on TIRA, automation of the tuning procedure for hyperparameters and thresholds was no longer possible, and suboptimal values had to be used.</p><p>On the training set, the system outperformed three baselines, but with varying margins. The largest margin was for English, for which my software was the most developed and the features included ones derived from full syntactic analysis trees. On the test set, performance varied. In relation to the baselines, there was a drop for English (+25% for training to +8% for test), French (+1.5% to -7%) and Spanish (+19% to +13%), but an improvement for Italian (+9% to +12.5%). In relation to the best systems (for each language), however, we see bad scores for English (-20%) and French (-21.5%), slightly better scores for Italian (-9%), and acceptable scores for Spanish (-3%).</p><p>Given the large variation in relative scores, it would seem that success or failure with the single settings option is largely a matter of luck. The settings may work or they may not. This is clearly not an acceptable situation and automated hyperparameter tuning is a matter of utmost urgency. Once this is in place, I can also turn my attention to the relative contributions of the various feature types and components, as well as the partial in-genre attribution, which would be rather meaningless at the moment.</p><p>Obviously, careful analysis of the results is only possible with access to the test data. But I would like to go further and express the hope that the organisers will release even more fandom data, as this would allow a more detailed mapping of the behaviour of authors across (fiction) genres, which was not possible with the current selection. Once</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,220.10,576.37,175.16,8.12"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example syntactic analysis for English</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,184.32,115.83,244.75,194.61"><head>Table 1 .</head><label>1</label><figDesc>Feature types.</figDesc><table coords="4,184.32,137.15,244.75,129.06"><row><cell cols="2">Feature type Example</cell><cell>Explanation</cell></row><row><cell>Character</cell><cell>CG1_.</cell><cell>a period</cell></row><row><cell>n-grams (lex)</cell><cell cols="2">CG6_ed˜to˜word ending in ed followed by to and more space</cell></row><row><cell>Token</cell><cell>WF_and</cell><cell>the word and</cell></row><row><cell>n-grams (lex)</cell><cell>T3_PGW_VBG_GrpADVgely_at</cell><cell>-ing-participle, followed by an adverb ending in -ly</cell></row><row><cell></cell><cell></cell><cell>and the word at</cell></row><row><cell>Token</cell><cell>WFV_midfnn</cell><cell>a mid-IDF-range singular common noun</cell></row><row><cell>n-grams (abs)</cell><cell>T3V_PWW_DT_LIDFNN_ofIN</cell><cell>a determiner followed by a low-IDF-range singular</cell></row><row><cell></cell><cell></cell><cell>common noun and the word of as a preposition</cell></row><row><cell>Syntactic</cell><cell>SCWV_DT_alldt</cell><cell>the word all used as a determiner</cell></row><row><cell>n-grams (lex)</cell><cell cols="2">SCFFCCLV_NP_NPDT_DT(aDT)_ a noun phrase with determiner a and</cell></row><row><cell></cell><cell>NPPO_PP(ofIN)</cell><cell>a postmodifying prepositional phrase with preposition of</cell></row></table><note coords="4,184.32,273.24,18.54,4.32;4,231.68,273.24,181.19,4.32;4,184.32,284.20,27.53,4.32;4,245.85,284.20,35.96,4.32;4,316.16,284.20,94.20,4.32;4,231.68,295.16,46.38,4.32;4,310.49,295.16,90.74,4.32;4,245.85,306.12,38.63,4.32;4,316.16,306.12,66.80,4.32"><p><p>Syntactic</p>SCFFCCLV_S_SU_NP(HIDFNNP)_ a sentence in which the subject is a high-IDF-range n-grams (abs) V_VP(GrpVrepx) proper noun and the main verb a reporting verb SRFC_VP_AVB_MD_ a verb phrase which consists of a modal verb, A_AVP_MVB_VB an adverb phrase and a main verb</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,115.83,345.83,106.20"><head>Table 2 .</head><label>2</label><figDesc>Number of features used for the problems in the training data, per language and feature type.</figDesc><table coords="6,157.83,148.10,299.69,73.93"><row><cell cols="6">Problem Number of Char n-gram Tok n-gram Tok n-gram Syn n-gram Syn n-gram</cell></row><row><cell></cell><cell>unknown</cell><cell>(lex)</cell><cell>(abs)</cell><cell>(lex)</cell><cell>(abs)</cell></row><row><cell></cell><cell>texts</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">English 137-561 120-235K 119-331K 122-323K 179-426K 114-249K</cell></row><row><cell>French</cell><cell>38-430</cell><cell cols="2">78-214K 146-611K 130-531K</cell><cell></cell></row><row><cell>Italian</cell><cell>46-196</cell><cell cols="2">83-146K 174-390K 155-340K</cell><cell></cell></row><row><cell cols="4">Spanish 112-450 121-215K 274-718K 246-635K</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,189.43,115.83,236.50,306.65"><head>Table 3 .</head><label>3</label><figDesc>Results on the training set, compared to three baselines.</figDesc><table coords="10,189.43,137.15,236.50,285.33"><row><cell>Problem</cell><cell cols="3">Precision Recall F1 SVM Compression Impostor</cell></row><row><cell>1 (en)</cell><cell>0.803 0.716 0.751 0.711</cell><cell>0.682</cell><cell>0.699</cell></row><row><cell>2 (en)</cell><cell>0.667 0.633 0.629 0.444</cell><cell>0.336</cell><cell>0.396</cell></row><row><cell>3 (en)</cell><cell>0.652 0.583 0.572 0.491</cell><cell>0.501</cell><cell>0.428</cell></row><row><cell>4 (en)</cell><cell>0.378 0.729 0.451 0.277</cell><cell>0.490</cell><cell>0.471</cell></row><row><cell>5 (en)</cell><cell>0.641 0.580 0.583 0.474</cell><cell>0.340</cell><cell>0.272</cell></row><row><cell>Overall (en)</cell><cell>0.597 0.479</cell><cell>0.470</cell><cell>0.453</cell></row><row><cell>6 (fr)</cell><cell>0.742 0.701 0.693 0.702</cell><cell>0.691</cell><cell>0.564</cell></row><row><cell>7 (fr)</cell><cell>0.519 0.570 0.504 0.499</cell><cell>0.542</cell><cell>0.450</cell></row><row><cell>8 (fr)</cell><cell>0.628 0.596 0.658 0.505</cell><cell>0.492</cell><cell>0.384</cell></row><row><cell>9 (fr)</cell><cell>0.653 0.688 0.579 0.593</cell><cell>0.608</cell><cell>0.265</cell></row><row><cell>10 (fr)</cell><cell>0.410 0.635 0.447 0.442</cell><cell>0.501</cell><cell>0.393</cell></row><row><cell>Overall (fr)</cell><cell>0.576 0.548</cell><cell>0.567</cell><cell>0.411</cell></row><row><cell>11 (it)</cell><cell>0.857 0.751 0.763 0.651</cell><cell>0.595</cell><cell>0.441</cell></row><row><cell>12 (it)</cell><cell>0.636 0.662 0.631 0.599</cell><cell>0.508</cell><cell>0.561</cell></row><row><cell>13 (it)</cell><cell>0.675 0.830 0.706 0.687</cell><cell>0.731</cell><cell>0.543</cell></row><row><cell>14 (it)</cell><cell>0.694 0.676 0.680 0.583</cell><cell>0.780</cell><cell>0.553</cell></row><row><cell>15 (it)</cell><cell>0.841 0.857 0.840 0.760</cell><cell>0.712</cell><cell>0.360</cell></row><row><cell>Overall (it)</cell><cell>0.724 0.656</cell><cell>0.665</cell><cell>0.492</cell></row><row><cell>16 (sp)</cell><cell>0.928 0.835 0.875 0.767</cell><cell>0.705</cell><cell>0.668</cell></row><row><cell>17 (sp)</cell><cell>0.909 0.818 0.831 0.581</cell><cell>0.623</cell><cell>0.351</cell></row><row><cell>18 (sp)</cell><cell>0.861 0.914 0.874 0.713</cell><cell>0.659</cell><cell>0.549</cell></row><row><cell>19 (sp)</cell><cell>0.749 0.745 0.732 0.559</cell><cell>0.403</cell><cell>0.312</cell></row><row><cell>20 (sp)</cell><cell>0.443 0.585 0.417 0.515</cell><cell>0.223</cell><cell>0.323</cell></row><row><cell>Overall (sp)</cell><cell>0.746 0.627</cell><cell>0.523</cell><cell>0.441</cell></row><row><cell>Overall</cell><cell>0.656 0.578</cell><cell>0.556</cell><cell>0.449</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,144.73,525.01,239.77,7.77"><p>And therefore I did not use this expectation in building my system.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,144.73,536.16,335.86,7.77;2,144.73,547.11,335.86,7.77;2,144.73,558.07,335.86,7.77;2,144.73,569.03,131.70,7.77"><p>I also participated in the Author Profiling task. The differences in handling the two tasks were such that I preferred to describe the other task in a separate paper<ref type="bibr" coords="2,381.49,547.11,10.87,7.77" target="#b12">[7]</ref>. The main difference is that here I used verification, and for profiling comparison. Despite that, there will obviously be some overlap between the papers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="2,144.73,580.18,335.86,7.77;2,144.73,591.14,335.86,7.77"><p>As it is very unlikely that the same authors will be active in the same combinations of fandoms, the known texts were usually from various fandoms. The only other source was the set of</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3" coords="2,144.73,602.10,335.86,7.77;2,144.73,613.06,335.86,7.77;2,144.73,624.02,197.66,7.77;2,139.00,633.30,2.99,5.18;2,144.73,635.17,335.86,7.77;2,144.73,646.13,335.86,7.77;2,144.73,657.08,112.98,7.77"><p>5,000 sets by unknown authors per language, but this as well was too fragmented for proper modeling. I hope the organisers will make more data available after the workshop, so that I can investigate author behaviour over various fandoms.<ref type="bibr" coords="2,139.00,633.30,2.99,5.18" target="#b10">5</ref> It must be said that this pilot was in the initial phases of the work on the task. It may well be that the approach would be viable in the circumstances provided by the final system. Again, future work is a definite option.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="4,144.73,601.88,335.86,7.77;4,144.73,612.84,156.45,7.77"><p>The list was created during work on author recognition on texts from the British National Corpus<ref type="bibr" coords="4,168.97,612.84,12.12,7.77" target="#b7">[2]</ref> and was still under development.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="4,144.73,623.83,232.22,7.77"><p>IDFs were calculated on the texts in the British National Corpus.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="4,144.73,634.98,260.46,7.77"><p>Unfortunately, the parser(s) from that project are (currently) unavailable.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="4,144.73,646.13,335.86,7.77;4,144.73,657.08,20.58,7.77"><p>Unfortunately, there is no space for a more extensive description, but this will follow in future work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8" coords="6,144.73,646.13,335.86,7.77;6,144.73,657.08,262.11,7.77"><p>This was not because no parsers existed, but because there was no time to build tranformers from the parser output to the structure I wanted for the syntactic features.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9" coords="7,144.73,613.06,335.86,7.77;7,144.73,624.02,335.86,7.77;7,144.73,634.98,115.90,7.77"><p>I.e. the absolute count divided by the corresponding number of items, e.g. count of a token in a text divided by that of all tokens within the text, or a character n-gram count divided by the number of characters in the text.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10" coords="7,144.73,646.13,335.86,7.77;7,144.73,657.08,219.93,7.77"><p>Theoretically, this is questionable, as most counts will not be distributed normally, but the system appears quite robust against this theoretical objection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11" coords="8,144.73,623.83,263.93,7.77"><p>To be exact, the normalized scores with regard to both model and sample.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_12" coords="8,144.73,634.98,335.86,7.77;8,144.73,645.94,31.13,7.77"><p>The rotation to calculate this value was based on the known samples and applied to the test samples.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_13" coords="8,144.73,657.08,269.54,7.77"><p>This manual tuning process too should be, but has not yet been, automated.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_14" coords="9,144.73,590.95,335.86,7.77;9,144.73,601.91,335.86,7.77;9,144.73,612.87,335.86,7.77"><p>After receiving the test results, I must conclude that this was not the case, and that authordependent hyperparameter and threshold selection is indeed needed (cf. Section 6). Furthermore, in the circumstances, reporting on the manually selected thresholds does not seem worth-</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_15" coords="9,144.73,623.83,22.16,7.77;9,136.01,633.11,5.98,5.18;9,144.73,634.98,276.93,7.77"><p>while.17  For reasons of processing time, support vector regression was not included in</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_16" coords="9,423.90,634.98,37.86,7.77;9,136.01,644.26,5.98,5.18;9,144.73,646.13,335.86,7.77;9,144.73,657.08,112.57,7.77"><p>this phase.18  I hoped the task organisers were not too cruel in their text selection, in which case random selection should be on my side.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_17" coords="10,144.73,624.02,335.86,7.77;10,144.73,634.98,295.02,7.77"><p>As the Macro-F1 was taken as an average over all F1, and not calculated from the Macro-Precision and the Macro-Recall, the Macro-F1 could be lower than the two others.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_18" coords="10,136.01,644.26,5.98,5.18;10,144.73,646.13,335.86,7.77;10,144.73,657.08,144.63,7.77"><p> 20  I did not actually run the baseline systems myself, but these measurements were provided by Mike Kestemont, for which I thank him.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>would be of an acceptable level, seeing that the selected thresholds were chosen for 45 different authors each in five different fandoms. <ref type="bibr" coords="9,324.39,129.60,6.97,6.05">16</ref> 4.2 Phase 2: Selecting Training Texts for In-Genre Attribution The next step was to build an in-genre attribution process for those authors for which sufficient in-genre texts had been identified. If at least four texts were attributed to an author in phase 1, these texts were used to build in-genre models, with which the same set of suggested texts were scored. <ref type="bibr" coords="9,272.40,209.05,6.97,6.05">17</ref> These scores were then transformed to z-scores with regard to all text scores for the author in question and then sorted from high to low. Texts were removed from the set if a) they had a z-score lower than -2 and/or b) they were in the second half of the ranking and the gap to the next higher z-score was greater than 1. The second criterion was to correct for the situation where another author (known or unknown) was being falsely accepted as well. This correction fails if the authors are too much alike, but also if more samples from the unwanted author were included than by the wanted author. <ref type="bibr" coords="9,438.92,292.74,6.97,6.05">18</ref> The texts which were not removed were next used to build full in-genre attribution models. In addition, they were marked for final attribution to their respective authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Phase 3: In-Genre Attribution</head><p>On the basis of the texts selected in phase 2, the system now created an attribution model for each author for whom at least three in-genre texts were left. For the other authors, the cross-genre models were reused. With these new models, all unknown texts were processed in the same way as in phase 1, except that now only the normalized scores with regard to the sample were used (and no longer adding those for the models), and other thresholds were chosen.</p><p>The texts attributed to any of the in-genre models were marked for final attribution to the corresponding authors. All other texts were marked as NOT belonging to any of these authors and referred to phase 4, where they could be attributed to one of the other authors or remain unattributed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Phase 4: Cross-Genre Attribution of Underrepresented Authors</head><p>For this final attribution phase, the results of phase 1 were reused, but also using the normalized scores with regard to the sample.</p><p>All authors processed in phase 3 were struck from the rankings and the top-ranking remaining author was examined. If his/her mean and/or lowest scores over the individual runs was over selected (language-dependent) thresholds, he/she was selected for final attribution of the text in question. If not, the text remained unattributed.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="5,156.28,195.00,96.84,6.31" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Root</surname></persName>
		</author>
		<imprint>
			<pubPlace>&lt;UNHEAD&gt;</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,290.78,195.00,53.80,6.31" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration" coords="5,290.78,195.00,43.04,6.31">NOFUpunc</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,148.94,205.96,129.12,6.31;5,163.11,216.92,182.91,6.31;5,177.28,227.87,139.88,6.31;5,163.11,238.83,123.74,6.31;5,177.28,249.79,107.60,6.31;5,177.28,260.75,102.22,6.31;5,163.11,271.71,220.57,6.31;5,177.28,282.67,102.22,6.31;5,177.28,293.63,102.22,6.31;5,177.28,304.59,139.88,6.31" xml:id="b2">
	<monogr>
		<title level="m" coord="5,148.94,205.96,26.90,6.31;5,229.64,205.96,37.66,6.31;5,163.11,216.92,129.12,6.31;5,313.75,216.92,32.28,6.31;5,177.28,227.87,139.88,6.31;5,163.11,238.83,53.80,6.31;5,238.43,238.83,48.42,6.31;5,177.28,249.79,107.60,6.31;5,177.28,260.75,102.22,6.31;5,163.11,271.71,102.22,6.31;5,297.61,271.71,86.08,6.31;5,177.28,282.67,102.22,6.31;5,177.28,293.63,102.22,6.31;5,177.28,304.59,118.36,6.31">NPDT NPHD NPPO ] NPDT:DT(the) -&gt; the NPHD:CD(one) -&gt; one NPPO:SBAR(say|GrpVrepd</title>
		<imprint/>
	</monogr>
	<note>AVB MVB ] AVB:VBD(have) -&gt; had MVB:VBN(be) -&gt; been CS:NP-NPRPNP01(one)</note>
</biblStruct>

<biblStruct coords="5,191.46,315.55,161.39,6.31;5,205.63,326.51,75.32,6.31;5,205.63,337.46,156.01,6.31;5,219.80,348.42,145.26,6.31;5,205.63,359.38,86.08,6.31" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<title level="m" coord="5,315.19,315.55,37.66,6.31;5,205.63,326.51,75.32,6.31;5,205.63,337.46,107.60,6.31;5,334.75,337.46,26.90,6.31;5,219.80,348.42,107.60,6.31">A V A ] A:TO(to) -&gt; to V:VP(say|GrpVrepd)</title>
		<imprint>
			<publisher>SBARc</publisher>
		</imprint>
	</monogr>
	<note>MVB ] MVB:VB(say|GrpVrepd)</note>
</biblStruct>

<biblStruct coords="5,219.80,370.34,123.74,6.31;5,233.98,381.30,166.77,6.31;5,248.15,392.26,118.36,6.31;5,233.98,403.22,112.98,6.31;5,248.15,414.18,123.74,6.31;5,233.98,425.14,64.56,6.31" xml:id="b4">
	<analytic>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,300.50,370.34,43.04,6.31;5,233.98,381.30,102.22,6.31;5,368.47,381.30,32.28,6.31;5,248.15,392.26,118.36,6.31;5,233.98,403.22,53.80,6.31">NPHD ] NPHD:PRP(they) -&gt; they V:VP(need)</title>
		<imprint/>
	</monogr>
	<note>SU V A ] SU:NP-PRPNP00(they)</note>
</biblStruct>

<biblStruct coords="5,314.67,425.14,48.42,6.31;5,248.15,436.09,75.32,6.31;5,248.15,447.05,107.60,6.31;5,262.32,458.01,96.84,6.31;5,248.15,468.97,145.25,6.31;5,262.32,479.93,102.22,6.31;5,262.32,490.89,182.91,6.31;5,276.50,501.85,91.46,6.31;5,276.50,512.81,188.29,6.31;5,290.67,523.77,112.98,6.31;5,248.15,534.72,129.12,6.31;5,262.32,545.68,123.74,6.31;5,148.94,556.64,96.84,6.31;12,134.77,161.38,55.54,10.75" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>V A A</surname></persName>
		</author>
		<title level="m" coord="5,248.15,436.09,75.32,6.31;5,248.15,447.05,48.42,6.31;5,328.85,447.05,26.90,6.31;5,262.32,458.01,96.84,6.31;5,248.15,468.97,53.80,6.31;5,334.23,468.97,59.18,6.31;5,262.32,479.93,64.56,6.31;5,380.68,490.89,64.56,6.31;5,276.50,501.85,91.46,6.31;5,276.50,512.81,188.29,6.31;5,290.67,523.77,112.98,6.31;5,248.15,534.72,75.32,6.31">PREP PCOMP ] PREP:IN(of) -&gt; of PCOMP:NP-NPRPNP00(town) -&gt; [ NPHD ] NPHD:NN(town) -&gt; town A:AVP(first)</title>
		<imprint/>
	</monogr>
	<note>MVB ] MVB:VB(get) -&gt; get A:AVP(out). AVHD:RB(first) -&gt; first NOFUpunc</note>
</biblStruct>

<biblStruct coords="12,142.61,186.71,333.94,7.77;12,150.95,197.32,288.14,8.12" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,297.41,186.71,179.15,7.77;12,150.95,197.67,54.40,7.77">The linguistic annotation of corpora: The TOSCA analysis system</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Aarts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Oostdijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,211.48,197.67,148.43,7.77">International journal of corpus linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="210" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,208.63,317.81,7.77;12,150.95,219.59,130.66,7.77" xml:id="b7">
	<monogr>
		<ptr target="http://www.natcorp.ox.ac.uk/" />
		<title level="m" coord="12,218.71,208.63,139.34,7.77">The British National Corpus, version 3</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>BNC Consortium</orgName>
		</respStmt>
	</monogr>
	<note>BNC XML Edition</note>
</biblStruct>

<biblStruct coords="12,142.61,230.55,337.98,7.77;12,150.95,241.16,210.06,8.12" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,235.13,230.55,171.52,7.77">LIBSVM: A library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,412.85,230.55,67.74,7.77;12,150.95,241.50,139.84,7.77">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,252.46,310.57,7.77;12,150.95,263.42,159.28,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,214.72,252.46,222.95,7.77">Linguistic Profiling for authorship recognition and verification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,150.95,263.42,82.02,7.77">Proceedings ACL 2004</title>
		<meeting>ACL 2004</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,274.38,305.46,7.77;12,150.95,284.99,321.05,8.12;12,150.95,296.30,23.90,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,214.72,274.38,233.35,7.77;12,150.95,285.34,56.55,7.77">Author verification by Linguistic Profiling: An exploration of the parameter space</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,213.44,285.34,230.18,7.77">ACM Transactions on Speech and Language Processing (TSLP)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,307.26,319.29,7.77;12,150.95,318.22,324.01,7.77;12,150.95,329.18,23.90,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,214.72,307.26,175.85,7.77">Source language markers in Europarl translations</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,408.37,307.26,53.54,7.77;12,150.95,318.22,49.13,7.77">Proceedings of COLING2008</title>
		<title level="s" coord="12,227.41,318.22,196.97,7.77">International Conference on Computational Linguistics</title>
		<meeting>COLING2008</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="937" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,340.14,318.42,7.77;12,150.95,351.09,328.79,7.77;12,150.95,362.05,300.77,7.77;12,150.95,373.01,80.53,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,214.72,340.14,246.32,7.77;12,150.95,351.09,113.79,7.77">Bot and gender recognition on tweets using feature count deviations, notebook for PAN at CLEF2019</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="12,150.95,362.05,117.33,7.77">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="12,274.76,362.05,176.96,7.77;12,150.95,373.01,19.77,7.77">Notebook Papers. CEUR Workshop Proceedings. CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Losada</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,383.97,320.28,7.77;12,150.95,394.93,298.20,7.77;12,150.95,405.89,250.06,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,263.29,383.97,199.60,7.77;12,150.95,394.93,39.65,7.77">Linguistic Profiling of texts for the purpose of language verification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Oostdijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,208.35,394.93,240.81,7.77;12,150.95,405.89,38.59,7.77">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">966</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,416.85,309.43,7.77;12,150.95,427.46,205.21,8.12" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,265.76,416.85,127.14,7.77">Gender recognition of Dutch tweets</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Speerstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,398.74,416.85,53.31,7.77;12,150.95,427.81,136.48,7.77">Computational Linguistics in the Netherlands Journal</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="171" to="190" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,438.77,322.06,7.77;12,150.95,449.72,319.94,7.77;12,150.95,460.68,313.95,7.77;12,150.95,471.64,108.78,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,150.95,449.72,260.37,7.77">Overview of the Cross-domain Authorship Attribution Task at PAN 2019</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="12,305.73,460.68,117.33,7.77">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="12,429.54,460.68,35.36,7.77;12,150.95,471.64,48.03,7.77">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,482.60,317.48,7.77;12,150.95,493.21,325.09,8.12" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,237.17,482.60,219.02,7.77">Determining if two documents are written by the same author</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Winter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,150.95,493.56,241.42,7.77">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="178" to="187" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,504.52,312.14,7.77;12,150.95,515.48,322.76,7.77;12,150.95,526.44,222.05,7.77;12,150.95,537.40,181.13,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,440.44,504.52,13.94,7.77;12,150.95,515.48,194.53,7.77">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P/P14/P14-5010" />
	</analytic>
	<monogr>
		<title level="m" coord="12,363.13,515.48,110.58,7.77;12,150.95,526.44,151.62,7.77">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,548.35,306.04,7.77;12,150.95,559.31,319.46,7.77;12,150.95,569.92,258.48,8.12" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,373.80,559.31,96.61,7.77;12,150.95,570.27,81.86,7.77">Nearest neighbors distance ratio open-set classifier</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Mendes Júnior</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">D O</forename><surname>Werneck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">V</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">V</forename><surname>Pazinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>De Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A B</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">D S</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,238.12,570.27,65.98,7.77">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="386" />
			<date type="published" when="2017-03">Mar 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,581.23,333.87,7.77;12,150.95,592.19,329.42,7.77;12,150.95,603.15,42.58,7.77" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,246.85,581.23,157.83,7.77">FreeLing 3.0: Towards wider multilinguality</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Padró</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stanilovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,422.57,581.23,53.54,7.77;12,150.95,592.19,236.96,7.77">Proceedings of the Language Resources and Evaluation Conference (LREC 2012)</title>
		<meeting>the Language Resources and Evaluation Conference (LREC 2012)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,614.11,335.40,7.77;12,150.95,625.07,309.15,7.77;12,150.95,636.03,209.02,7.77" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,333.86,614.11,140.16,7.77">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,259.53,625.07,200.58,7.77;12,150.95,636.03,144.92,7.77">Information Retrieval Evaluation in a Changing World -Lessons Learned from 20 Years of CLEF</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
