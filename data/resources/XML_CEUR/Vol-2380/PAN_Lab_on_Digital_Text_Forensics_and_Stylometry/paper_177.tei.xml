<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.22,115.90,336.92,12.90;1,256.20,133.83,102.96,12.90;1,223.43,153.96,168.50,10.75">A Hierarchical Neural Network Approach for Bots and Gender Profiling Notebook for PAN at CLEF 2019</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.76,190.63,61.71,8.64"><forename type="first">Andrea</forename><surname>Cimino</surname></persName>
							<email>andrea.cimino@ilc.cnr.it</email>
							<affiliation key="aff0">
								<orgName type="department">Istituto di Linguistica Computazionale &quot;</orgName>
								<orgName type="institution">Antonio Zampolli&quot; (ILC-CNR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.83,190.63,72.77,8.64"><forename type="first">Felice</forename><surname>Dell'orletta</surname></persName>
							<email>felice.dellorletta@ilc.cnr.it</email>
							<affiliation key="aff0">
								<orgName type="department">Istituto di Linguistica Computazionale &quot;</orgName>
								<orgName type="institution">Antonio Zampolli&quot; (ILC-CNR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.22,115.90,336.92,12.90;1,256.20,133.83,102.96,12.90;1,223.43,153.96,168.50,10.75">A Hierarchical Neural Network Approach for Bots and Gender Profiling Notebook for PAN at CLEF 2019</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FE561BC1385820C29B60A43E0C1AE7EF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our participation in the Bots and Gender Profiling shared task of PAN@CLEF2019 for the English language. We tested three approaches based on three different document classification algorithms. The first approach is based on a SVM classifier with handcrafted features using a wide set of linguistic information. The second and the third approaches exploit recent advances in Natural Language Processing using a Hierarchical GRU-LSTM Neural Network using word embeddings trained on Twitter and finally an adaptation of the BERT system. After an in-house evaluation, we submitted the final run with the Hierarchical Neural Network model, which achieved a final accuracy of 0.9083 in the Bots Profiling task and a score of 0.7898 in the Gender Profiling task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays the increased importance of social media platforms in everyday life has made the users of these platforms extremely impressionable by messages and posts written by companies, political parties or even social media influencers. In the recent years it has been shown that such platforms were exploited in order to diffuse fake news or for commercial activities using sophisticated techniques, such as very smart Bots, for massive mind manipulation. For this reason, the biggest social media platforms such as Facebook or Twitter started using algorithms to automatically detect and delete such Bot accounts, but latest advances in Natural Language Generation such as GPT-2 <ref type="bibr" coords="1,466.48,518.98,10.58,8.64" target="#b0">[1]</ref>, makes the automatic Bot detection still a challenging problem. One of the approaches commonly used by these platforms in order to detect a Bot, is the classification of a set of documents (e.g. tweets) rather than a single document, since usually the set of documents written by a Bot follows a common lexical and stylistic pattern. With respect to the previous PAN shared task, the PAN 2019: Bot and Gender profiling task <ref type="bibr" coords="1,134.77,590.71,11.62,8.64" target="#b1">[2]</ref> introduces the novelty of asking to participants to identify, given a set of tweets, the type of a user (Bot or human) and, in case the type of user is classified as human, to predict the gender (male or female). We addressed the Bots and genre profiling task as a 3-class classification problem and we developed three different classifier models: one that uses a classic approach based on the extraction of linguistic features and lexicons lookups using the linear SVM algorithm, and two more recent neural network based solutions. The first is based on a hierarchical GRU-LSTM deep neural network, and the second on a language model based neural network (BERT), which we adapted in order to handle large documents.</p><p>This paper makes the following contributions:</p><p>1. We propose a comparison between a more classical classification approach with extremely handcrafted features learned by a linear SVM algorithm and a lowengineered approach based on neural network models. 2. We show that the Hierarchical GRU-LSTM deep neural network has better performance with respect BERT, a very famous pretrained language model based on neural network.</p><p>In the following sections of this paper, we first explain the related work in Section 2. The preprocessing step and the external resources used are described in Section 3. Our models are properly described in Section 4. The details of the experiments used to confirm the model performances are described in Section 5. Finally, we conclude the paper and outline future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>This year edition of the PAN Bots and Gender Profiling shared task focuses on automatic identification on the Twitter platform of the type of user (BOT or human), and in case of a human, to detect the human gender. This is a slightly different version of previous year shared task <ref type="bibr" coords="2,225.54,453.20,10.58,8.64" target="#b2">[3]</ref>, where participants where asked to identify the gender based not only on textual content, but exploiting also images posted on the Twitter platform.</p><p>For what concerns the classification task using only the textual component, the linear SVM learning algorithm with heavily engineered features has been shown to be very effective method for identification of the gender. Daneshvar and Inkpen <ref type="bibr" coords="2,424.78,501.02,11.62,8.64" target="#b3">[4]</ref> used word n-grams and character n-grams with dimensionality reduction techniques and achieved the best score on the English and Spanish language. A similar solution was used by the second best participant (Tellez et al. <ref type="bibr" coords="2,295.86,536.89,11.20,8.64" target="#b4">[5]</ref>) that achieved the second best score on the English language and the best score on the Arabic language. Surprisingly, deep learning based sequential models did not achieve very good results when just considering the textual components. The best model on the English language that used this kind of architecture was presented by Takashi et al. <ref type="bibr" coords="2,311.68,584.71,10.58,8.64" target="#b5">[6]</ref>. The authors used a textual component composed by a word embedding, recurrent neural network (GRU), pooling, and fully connected layers. When tested on the English language, they achieved an accuracy of 0.7864, 4 points less than the state of the art. On the other hand their model, when mixed with visual information, achieved the average best scores among the Arabic, Spanish and English language, showing that deep learning architectures have strong results specially when combining multi-modal information.</p><p>The training dataset provided by the task organizers consists of 4,120 examples, each example containing a set of tweets which were written on the Twitter platform by a bot, a male or a female. In our approach we concatenated the tweets contained in each sample to produce the document, which is our classification unit. We used the "SEP" token as tweets separator in order to preserve the tweet length information which is used by our models. Since the SVM model relies on morpho-syntactically tagged texts, both training and test data were automatically morpho-syntactically tagged by our POS tagger described in <ref type="bibr" coords="3,211.72,225.95,10.58,8.64" target="#b6">[7]</ref>. In addition, in order to improve the overall accuracy of our models, we used an existing sentiment polarity lexicon and developed a word embedding lexicon for English tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentiment Polarity Lexicon</head><p>We used the SentiWordnet 3.0 sentiment polarity lexicon <ref type="bibr" coords="3,365.57,297.77,10.58,8.64" target="#b7">[8]</ref>. This is a freely available lexicon for the English language<ref type="foot" coords="3,262.48,308.05,3.49,6.05" target="#foot_0">1</ref> and includes more than 117,000 English word entries. It was automatically created using a semi-supervised step and a final random-walk step for refining the final positive and negative polarity scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word embedding Lexicon</head><p>In order to extract semantic information from words we created a word embedding lexicon using the word2vec<ref type="foot" coords="3,245.69,391.82,3.49,6.05" target="#foot_1">2</ref> toolkit <ref type="bibr" coords="3,281.66,393.49,10.58,8.64" target="#b8">[9]</ref>. As recommended in <ref type="bibr" coords="3,382.96,393.49,10.58,8.64" target="#b8">[9]</ref>, we used the CBOW model that learns to predict the word in the middle of a symmetric window based on the sum of the vector representations of the words in the window. For our experiments, we considered a context window of 5 words. These models learn lower-dimensional word embeddings. The word embedding lexicon was built using a set of 19,700,117 English tweets downloaded from the Twitter platform. In order test the contribution of the embeddings in classification w.r.t. the vector size, we generated 16, 32, 64 and 128 sized vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The proposed models</head><p>In this section we will describe the 3 devised models proposed for our participation in the Bots and Gender profile shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The SVM Model</head><p>The SVM classifier exploits a wide set of features ranging across different levels of linguistic description. All these features were already tested in our previous participation at the EVALITA 2018 <ref type="bibr" coords="3,242.54,614.83,15.27,8.64" target="#b9">[10]</ref>, the periodic evaluation campaign of Natural Language Processing (NLP) and speech tools for the Italian language.</p><p>The features are organised into three main categories: raw and lexical text features, morpho-syntactic features and lexicon features. All the calculated features are the input of the linear SVM algorithm implemented in the liblinear <ref type="bibr" coords="4,376.57,143.22,16.60,8.64" target="#b10">[11]</ref> library which finally generates the final statistical model used then to classify unseen documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw and Lexical Text Features</head><p>Number of tokens: The average number of tokens of an analyzed tweet.</p><p>Character n-grams: presence or absence of contiguous sequences of characters in the analyzed tweets. Word n-grams: presence or absence of contiguous sequences of tokens in the analyzed tweets.</p><p>Lemma n-grams: presence or absence of contiguous sequences of lemma occurring in the analyzed tweets. Repetition of n-grams chars: presence or absence of contiguous repetition of characters in the analyzed tweets. Number of mentions: number of mentions (@) occurring in the analyzed tweets. Number of hashtags: number of hashtags occurring in the analyzed tweets. Punctuation: the number of tweets that ends with one of the following punctuation characters: "?", "!".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Morpho-syntactic Features</head><p>Coarse grained Part-Of-Speech n-grams: presence or absence of contiguous sequences of coarse-grained PoS, corresponding to the main grammatical categories (noun, verb, adjective). Fine grained Part-Of-Speech n-grams: presence or absence of contiguous sequences of fine-grained PoS, which represent subdivisions of the coarse-grained tags (e.g. the class of nouns is subdivided into proper vs common nouns, verbs into main verbs, gerund forms, past particles). Coarse grained Part-Of-Speech distribution: the distribution of nouns, adjectives, adverbs, numbers in the tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lexicon features</head><p>Emoticons: presence or absence of positive or negative emoticons in the analyzed tweet. The lexicon of emoticons was extracted from the site http://it.wikipedia.org/wiki/Emoticon and manually classified. Lemma sentiment polarity n-grams: for each n-gram of lemmas extracted from the analyzed tweet, the feature checks the polarity of each component lemma in the existing sentiment polarity lexicons. Lemma that are not present are marked with the ABSENT tag. This is for example the case of the trigram all very nice that is marked as "AB-SENT-POS-POS" since very and nice are marked as positive in the considered polarity lexicon and all is absent. The feature is computed exploiting the SentiWordnet 3.0 lexicon resource. Polarity modifier: for each lemma in the tweets occurring in the existing sentiment polarity lexicons, the feature checks the presence of adjectives or adverbs in a left context window of size 2. If this is the case, the polarity of the lemma is assigned to the modifier. This is for example the case of the bigram not interesting, where "interesting" is a positive word, and "not" is an adverb. Accordingly, the feature "not_POS" is created. The feature is computed exploiting the SentiWordnet 3.0 lexicon resource. Distribution of sentiment polarity: this feature computes the percentage of positive, negative and neutral lemmas that occur in the tweets. To overcome the sparsity problem, the percentages are rounded to the nearest multiple of 5. The feature is computed exploiting the SentiWordnet 3.0 lexicon resource. Most frequent sentiment polarity: the feature returns the most frequent sentiment polarity of the lemmas in the analyzed tweets. The feature is computed exploiting the SentiWordnet 3.0 lexicon resource. Word embeddings combination: the feature returns the vectors obtained by computing separately the average of the word embeddings of the nouns, adjectives and verbs of the tweet, obtaining a total of 3 vectors for each tweet. If a specific morphosyntactic category is not present, a feature indicating the absence of such category is added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The BERT Model</head><p>Following the latest advances in NLP, we wanted to test how well pretrained language model representations behave on the Bot and Gender stylistic profiling shared task. Context-free models such as word2vec generate a single vector for each word, which is independent by the context in which the word is found. For example, the word "bank" can have two different meanings with respect to the context in which the word is surrounded. Language models like BERT <ref type="bibr" coords="5,295.79,378.16,16.60,8.64" target="#b11">[12]</ref> or ELMo <ref type="bibr" coords="5,357.57,378.16,16.60,8.64" target="#b12">[13]</ref> allow to obtain a distinct vector for each word based also on the context, which make such models very suitable for many NLP downstream tasks. In order to test the performance of these models, we choose BERT since Google provides pretrained models <ref type="foot" coords="5,379.15,412.36,3.49,6.05" target="#foot_2">3</ref> , which need only to be fine-tuned with an inexpensive procedure. Among the models available on the github repository, we choose the recommended model: BERT-Base Multilingual Cased which is trained on 104 languages with 110M parameters. One of the limitations of this pretrained model is that such model was trained on sentences not longer than 512 tokens, which made the standard fine-tuning procedure not suitable for our case, since the training documents (the concatenation of the tweets) were much longer than 512 tokens. For this reason, we generated 5 different fined tuned downstream tasks models by considering 5 chunks of 500 tokens each. In testing phase, each document was still divided in 5 chunks. Each chunk was then classified by the previously 5 fine tuned models. We then choose as winning class among BOT, male and female, the majority class resulting by all the predictions of the 5 models on the 5 chunks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Hierarchical GRU/LSTM Model</head><p>GRU units are able to propagate an important features that came early in the input sequence over a long distance, thus capturing potential long-distance dependencies. Unfortunately, it has been shown that long dependencies are lost in case of very long sequences. For this reason, since we treat the batch of tweets to be classified as single document, we resorted to a two-layer hierarchical GRU/LSTM architecture. In addition, each document containing the set of tweets to be analyzed is first truncated to the first 2500 tokens. This operation is done since in-house experiments have not shown a significant drop in performance w.r.t. analyzing all the tweets contained in a single example. Moreover, the truncation allows a faster training in terms of time, considering the number of tweets belonging to the training set. Each document is then split in 5 chunks of 500 tokens, which are the input of five different GRU unit (48 dimensions), which produce 5 "chunk" embeddings. Finally, all the chunk embeddings are the input of a final LSTM (48 dimensions) layer. Figure <ref type="figure" coords="6,319.29,214.95,4.98,8.64" target="#fig_0">1</ref> shows a graphical representation of the hierarchical GRU-LSTM architecture. We applied a dropout factor to both input gates and to the recurrent connections in order to prevent overfitting which is a typical issue in neural networks <ref type="bibr" coords="6,211.77,250.82,15.27,8.64" target="#b13">[14]</ref>. We have chosen a dropout factor value of 0.55. For what concerns the optimization process, the categorical cross entropy function is used as a loss function and optimization is performed by the rmsprop optimizer <ref type="bibr" coords="6,396.81,274.73,15.27,8.64" target="#b14">[15]</ref>.</p><p>Furthermore, we performed a 5-fold training approach. More precisely we build 5 different models using different training and validation sets. These models are then exploited in the classification phase: the assigned labels are the ones that obtain the majority among all the models. The 5-fold approach strategy was chosen in order to generate a global model which should less be prone to overfitting or underfitting w.r.t. a single learned model. Each input word is represented by a vector which is composed by: Word embeddings: the word embedding extracted by the available word embedding lexicon (32 dimensions), and for each word embedding an extra component was added to handle the "unknown word" (1 dimension).</p><p>Word polarity: the corresponding word sentiment polarities obtained by using the Sen-tiWordnet 3.0 resource. This results in 3 components: 2 used for positive and negative values found in the resource, and one binary component set to 1 in case the word is not found in the lexicon. Is capitalized word: a component (1 dimension) indicating whether the word is capitalized. Is uppercased word: a component (1 dimension) indicating whether the word is uppercased.</p><p>Is URL: a component (1 dimension) indicating whether the word is an URL. Is hashtag: a component (1 dimension) indicating whether the word is an hastag. Is mention: a component (1 dimension) indicating whether the word contains a mention. Is separator: a component (1 dimension) indicating if the word is the "SEP" reserved token, which we use to divide the tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>In order to choose the model to submit for the final run of the Bots and Gender profiling shared task, we tested all the 3 devised models on the official development set distributed by the organizers.  <ref type="table" coords="7,161.91,466.91,3.88,8.64">1</ref>: Classification results of the proposed models on the official development set.</p><p>Table <ref type="table" coords="7,174.52,512.97,4.98,8.64">1</ref> reports the overall accuracies achieved by our proposed models on the official developments set. Is it worth noting that, since an official software evaluator was not distributed among with the training data, we developed our own internal evaluator. It can be noticed that the proposed models behave quite well in average: the average f-score ranges between 0.74 for the linear SVM to 0.80 for the hierachical GRU/LSTM model. Surprisingly, the Hierarchical GRU/LSTM model outperformed the BERT system. We think that such difference in performance is due to many reasons. First of all, the BERT pretrained language model is trained on generic texts and not on social media texts. Another possible cause of the difference in terms of performance w.r.t. to the GRU/LSTM model is that BERT does not use any handcrafted feature w.r.t. the other two models (SVM in particular). As it is shown in Table <ref type="table" coords="7,365.43,632.53,3.74,8.64">1</ref>, these handcrafted features could be the the reason of the highest performance of the SVM model when considering the BOT vs human classification task.</p><p>The obtained results on the development set lead us to choose the Hierarchical GRU/LSTM model for the final runs since this model behaves better when considering the average f-score on the 3 tasks (0.806). Table <ref type="table" coords="8,350.36,143.22,4.98,8.64" target="#tab_1">2</ref> reports the results obtained on the official test set. The result was obtained using the official scorer provided by the organizers on the TIRA <ref type="bibr" coords="8,225.94,167.13,19.92,8.64" target="#b15">[16]</ref> evaluation platform. In addition the table reports the baselines provided by the shared task organizers which are char n-grams, word n-grams, word2vec and LDSE <ref type="bibr" coords="8,220.56,191.04,16.60,8.64" target="#b16">[17]</ref> based and which are fully described in <ref type="bibr" coords="8,395.42,191.04,10.58,8.64" target="#b1">[2]</ref>. For what concerns the Bot vs Human task, we can notice that our proposed model outperformed both the W2V and LDSE baselines, but char n-grams and word-ngrams bases lines performed better than our model (+3% in accuracy). This suggest that these features are very important for this classification task. Such behaviour was shown also in our internal tests, but the gain in terms of accuracy was less than what was shown in the test set (+2% in accuracy).</p><p>For what concerns the Male vs Female task, also here our GRU-LSTM model performed well, being in line with all the proposed baselines. Unfortunately all the errors in classification made in the in the Bot vs Human task were propagated in the Male vs Female task. So most probably a combination of a SVM based model for the Bot vs Human task and the GRU-LSTM model for the Male vs Female task would result in the best solution to achieve the best scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented three systems for the Bots and Gender Profiling shared task, on a SVM classifier with handcrafted features using a wide set of linguistic information. The second and the third based on Hierarchical GRU-LSTM on on the the BERT system. After internal experiments, we participated with the Hierarchical GRU-LSTM model, which showed promising results, outperforming all the W2V and LDSE baselines. It would be interesting to incorporate char-level features in our GRU-LSTM in order to evaluate the difference in terms of performance w.r.t. our current model, which is only token based at the moment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,203.85,378.41,207.66,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Hierarchical GRU-LSTM architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.77,354.63,345.82,120.92"><head></head><label></label><figDesc>The development set was composed by 1,240 examples: 640 composed by BOT messages, 310 by males 310 by females. The training data (including the development set) is composed by 4,120 examples.</figDesc><table coords="7,137.53,414.96,332.59,60.58"><row><cell>Configuration</cell><cell cols="4">Bot F-score Male F-score Female F-score Avg F-score</cell></row><row><cell>linear SVM</cell><cell>0.94</cell><cell>0.71</cell><cell>0.59</cell><cell>0.746</cell></row><row><cell>Hierarchical GRU/LSTM 5 Fold</cell><cell>0.92</cell><cell>0.76</cell><cell>0.74</cell><cell>0.806</cell></row><row><cell>BERT Multi</cell><cell>0.90</cell><cell>0.72</cell><cell>0.71</cell><cell>0.776</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,227.43,345.82,94.86"><head>Table 2 :</head><label>2</label><figDesc>Classification results of Hierarchical GRU/LSTM model and of the baselines provided by the shared task organizers on the official test set.</figDesc><table coords="8,207.43,227.43,200.51,68.32"><row><cell>Dataset</cell><cell cols="2">Bot vs Human Male vs Female</cell></row><row><cell>GRU-LSTM model</cell><cell>0.9083</cell><cell>0.7898</cell></row><row><cell>char nGrams baseline</cell><cell>0.9360</cell><cell>0.7920</cell></row><row><cell>word nGrams baseline</cell><cell>0.9356</cell><cell>0.7989</cell></row><row><cell>W2V baseline</cell><cell>0.9030</cell><cell>0.7879</cell></row><row><cell>LDSE baseline</cell><cell>0.9054</cell><cell>0.7800</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,645.94,137.29,7.77"><p>https://github.com/aesuli/sentiwordnet</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,657.08,129.78,7.77"><p>http://code.google.com/p/word2vec/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,144.73,657.93,210.31,6.31"><p>https://github.com/google-research/bert</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.13,142.62,342.46,7.77;9,146.47,153.58,42.10,7.77" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,451.71,142.62,28.88,7.77;9,146.47,153.58,15.74,7.77">OpenAI Blog</title>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.13,164.49,342.46,7.77;9,146.47,175.45,334.12,7.77;9,146.47,186.41,313.05,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,261.52,164.49,219.07,7.77;9,146.47,175.45,75.71,7.77">Overview of the 7th Author Profiling Task at PAN 2019: Bots and Gender Profiling</title>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,437.68,175.45,42.91,7.77;9,146.47,186.41,72.99,7.77">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="9,225.94,186.41,198.98,7.77">Notebook Papers. CEUR Workshop Proceedings. CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Losada</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.13,197.32,342.46,7.77;9,146.47,208.27,334.12,7.77;9,146.47,219.23,334.12,7.77;9,146.47,230.19,334.12,7.77;9,146.47,241.15,29.72,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,170.48,208.27,310.11,7.77;9,146.47,219.23,68.21,7.77">Overview of the 6th Author Profiling Task at PAN 2018: Multimodal Gender Identification in Twitter</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Rangel</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Stein</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,230.94,219.23,249.65,7.77;9,146.47,230.19,37.57,7.77">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,341.27,230.19,133.95,7.77">CEUR Workshop Proceedings. CEUR</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.13,252.06,342.46,7.77;9,146.47,263.02,334.12,7.77;9,146.47,273.98,334.12,7.77;9,146.47,284.94,104.93,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,288.44,252.06,192.15,7.77;9,146.47,263.02,142.52,7.77">Gender Identification in Twitter using N-grams and LSA: Notebook for PAN at CLEF 2018</title>
		<author>
			<persName coords=""><forename type="first">Saman</forename><surname>Daneshvar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,305.71,263.02,174.89,7.77;9,146.47,273.98,107.72,7.77">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,417.16,273.98,63.44,7.77;9,146.47,284.94,67.19,7.77">CEUR Workshop Proceedings. CEUR</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.13,295.84,342.46,7.77;9,146.47,306.80,334.12,7.77;9,146.47,317.76,334.12,7.77;9,146.47,328.72,334.12,7.77;9,146.47,339.68,217.78,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,245.16,306.80,235.44,7.77;9,146.47,317.76,252.72,7.77">Gender Identification through Multi-modal Tweet Analysis using MicroTC and Bag of Visual Words: Notebook for PAN at CLEF 2018</title>
		<author>
			<persName coords=""><forename type="first">Eric Sadit</forename><surname>Tellez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sabino</forename><surname>Miranda-Jiménez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniela</forename><surname>Moctezuma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mario</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><surname>Salgado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Ortiz-Bejar</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,415.87,317.76,64.73,7.77;9,146.47,328.72,222.20,7.77">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,194.29,339.68,132.22,7.77">CEUR Workshop Proceedings. CEUR</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.13,350.59,342.46,7.77;9,146.47,361.55,334.12,7.77;9,146.47,372.51,334.12,7.77;9,146.47,383.47,334.12,7.77;9,146.47,394.42,104.93,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,213.25,361.55,267.34,7.77;9,146.47,372.51,145.51,7.77">Text and Image Synergy with Feature Cross Technique for Gender Identification: Notebook for PAN at CLEF 2018</title>
		<author>
			<persName coords=""><forename type="first">Takumi</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Takuji</forename><surname>Tahara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Koki</forename><surname>Nagatani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomoki</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomoko</forename><surname>Ohkuma</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,308.03,372.51,172.57,7.77;9,146.47,383.47,107.72,7.77">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,417.16,383.47,63.44,7.77;9,146.47,394.42,67.19,7.77">CEUR Workshop Proceedings. CEUR</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.13,405.33,342.46,7.77;9,146.47,416.29,334.12,7.77;9,146.47,427.25,334.12,7.77;9,146.47,438.21,279.29,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,289.07,405.33,191.52,7.77;9,146.47,416.29,23.50,7.77">Building the state-of-the-art in POS tagging of Italian Tweets</title>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Cimino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felice</forename><surname>Dell'orletta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,187.51,416.29,293.08,7.77;9,146.47,427.25,334.12,7.77;9,146.47,438.21,148.27,7.77">Proceedings of Third Italian Conference on Computational Linguistics (CLiC-it 2016) &amp; Fifth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2016)</title>
		<meeting>Third Italian Conference on Computational Linguistics (CLiC-it 2016) &amp; Fifth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2016)<address><addrLine>Napoli, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">December 5-7, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.13,449.12,342.46,7.77;9,146.47,460.08,334.12,7.77;9,146.47,471.04,334.12,7.77;9,146.47,481.99,20.92,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,345.13,449.12,135.47,7.77;9,146.47,460.08,204.87,7.77">SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining</title>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,366.75,460.08,113.85,7.77;9,146.47,471.04,231.64,7.77">Proceedings of the International Conference on Language Resource and Evaluation, LREC 2010</title>
		<meeting>the International Conference on Language Resource and Evaluation, LREC 2010<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05-23">17-23 May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.13,492.90,342.46,7.77;9,146.47,503.86,223.78,7.77" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,363.13,492.90,117.46,7.77;9,146.47,503.86,97.27,7.77">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>arXiv1:1301.3781</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.24,514.77,338.35,7.77;9,146.47,525.73,334.12,7.77;9,146.47,536.69,334.12,7.77;9,146.47,547.65,334.12,7.77;9,146.47,558.61,84.66,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,353.70,514.77,126.89,7.77;9,146.47,525.73,82.70,7.77">Multi-task Learning in Deep Neural Networks at EVALITA</title>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Cimino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lorenzo</forename><forename type="middle">De</forename><surname>Mattei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felice</forename><surname>Dell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">'</forename><surname>Orletta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,254.01,525.73,226.59,7.77;9,146.47,536.69,334.12,7.77;9,146.47,547.65,285.04,7.77">Proceedings of the Sixth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2018) co-located with the Fifth Italian Conference on Computational Linguistics (CLiC-it 2018)</title>
		<meeting>the Sixth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2018) co-located with the Fifth Italian Conference on Computational Linguistics (CLiC-it 2018)<address><addrLine>Turin, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-12">2018. December 12-13, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.24,569.51,338.35,7.77;9,146.47,580.47,334.12,7.77;9,146.47,591.43,99.46,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,463.16,569.51,17.43,7.77;9,146.47,580.47,185.72,7.77">LIB-LINEAR: A Library for Large Linear Classification</title>
		<author>
			<persName coords=""><forename type="first">Rong-En</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang-Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,338.06,580.47,138.64,7.77">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1187" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.24,602.34,338.35,7.77;9,146.47,613.30,334.12,7.77;9,146.47,624.26,114.85,7.77" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="9,381.81,602.34,98.78,7.77;9,146.47,613.30,201.95,7.77">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>arXiv1:1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.24,635.17,338.35,7.77;9,146.47,646.13,334.12,7.77;9,146.47,657.08,334.12,7.77;10,146.47,119.96,334.12,7.77;10,146.47,130.92,121.23,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,231.71,646.13,148.14,7.77">Deep contextualized word representations</title>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,394.67,646.13,85.93,7.77;9,146.47,657.08,334.12,7.77;10,146.47,119.96,187.20,7.77">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">June 1-6, 2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="10,142.24,141.88,338.35,7.77;10,146.47,152.84,207.89,7.77" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,283.19,141.88,197.40,7.77;10,146.47,152.84,83.41,7.77">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05287</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.24,163.80,338.35,7.77;10,146.47,174.76,334.12,7.77;10,146.47,185.71,36.12,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,294.09,163.80,186.50,7.77;10,146.47,174.76,129.94,7.77">Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName coords=""><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,292.88,174.76,183.73,7.77">COURSERA: Neural Networks for Machine Learn</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,196.67,338.35,7.77;10,146.47,207.63,334.12,7.77;10,146.47,218.59,123.91,7.77" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,385.69,196.67,94.90,7.77;10,146.47,207.63,334.12,7.77;10,146.47,218.59,61.93,7.77">TIRA Integrated Research Architecture. Information Retrieval Evaluation in a Changing World -Lessons Learned from 20 Years of CLEF</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matti</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.24,229.55,338.35,7.77;10,146.47,240.51,334.12,7.77;10,146.47,251.47,334.12,7.77;10,146.47,262.43,155.40,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,398.56,229.55,82.03,7.77;10,146.47,240.51,183.58,7.77">A Low Dimensionality Representation for Language Variety Identification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Rangel</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,346.54,240.51,134.06,7.77;10,146.47,251.47,251.10,7.77">Computational Linguistics and Intelligent Text Processing -17th International Conference, CICLing 2016</title>
		<meeting><address><addrLine>Konya, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">April 3-9, 2016</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers. Part II</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
