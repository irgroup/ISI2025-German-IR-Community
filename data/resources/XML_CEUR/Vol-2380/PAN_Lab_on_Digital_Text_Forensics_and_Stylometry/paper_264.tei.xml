<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.10,115.90,335.17,12.90;1,253.28,133.83,108.80,12.90">Overview of the Cross-Domain Authorship Attribution Task at PAN 2019</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,177.96,171.88,64.29,8.64"><forename type="first">Mike</forename><surname>Kestemont</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Antwerp</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.75,171.88,84.36,8.64"><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of the Aegean</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.27,171.88,79.14,8.64"><forename type="first">Enrique</forename><surname>Manjavacas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Antwerp</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,195.67,183.83,70.88,8.64"><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Antwerp</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.19,183.83,60.36,8.64"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Leipzig University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,366.23,183.83,48.99,8.64"><forename type="first">Benno</forename><surname>Stein</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.10,115.90,335.17,12.90;1,253.28,133.83,108.80,12.90">Overview of the Cross-Domain Authorship Attribution Task at PAN 2019</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">672D02E4296D17F304365EFA111C8B92</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authorship identification remains a highly topical research problem in computational text analysis, with many relevant applications in contemporary society and industry. In this edition of PAN, we focus on authorship attribution, where the task is to attribute an unknown text to a previously seen candidate author. Like in the previous edition we continue to work with fanfiction texts (in four Indo-European languages), written by non-professional authors in a crossdomain setting: the unknown texts belong to a different domain than the training material that is available for the candidate authors. An important novelty of this year's setup is the focus on open-set attribution, meaning that the test texts contain writing samples by previously unseen authors. For these, systems must consequently refrain from an attribution. We received altogether 12 submissions for this task, which we critically assess in this paper. We provide a detailed comparison of these approaches, including three generic baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="792.0" lry="612.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="792.0" lry="612.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Cross-Domain Authorship Attribution</head><p>Authorship attribution <ref type="bibr" coords="1,226.47,496.82,11.38,8.64" target="#b0">[1,</ref><ref type="bibr" coords="1,237.85,496.82,7.59,8.64" target="#b1">2,</ref><ref type="bibr" coords="1,245.44,496.82,7.59,8.64" target="#b2">3]</ref> continues to be an important problem in information retrieval and computational linguistics, and also in applied areas such as law and journalism where knowing the author of a document (such as a ransom note) may enable law enforcement to save lives. The most common framework for testing candidate algorithms is the closed-set attribution task: given a sample of reference documents from a finite set of candidate authors, the task is to determine the most likely author of a previously unseen document of unknown authorship. This task is quite challenging under cross-domain conditions where documents of known and unknown authorship come from different domains (such as a different thematic area or genre). In addition, it is often more realistic to assume that the true author of a disputed document is not necessarily included in the list of candidates <ref type="bibr" coords="1,291.11,616.37,10.58,8.64" target="#b3">[4]</ref>.</p><p>This year, we again focus on the attribution task in the context of transformative literature, more colloquially know as 'fanfiction'. Fanfiction refers to a rapidly expanding body of fictional narratives, typically produced by non-professional authors who self-identify as 'fans' of a particular oeuvre or individual work <ref type="bibr" coords="2,389.81,155.18,10.58,8.64" target="#b4">[5]</ref>. Usually, these stories (or 'fics') are openly shared online with a larger fan community on platforms such as fanfiction.net or archiveofourown.org. Interestingly, fanfiction is the fastest growing form of writing in the world nowadays <ref type="bibr" coords="2,361.07,191.04,10.58,8.64" target="#b5">[6]</ref>. When sharing their texts, fanfiction writers explicitly acknowledge taking inspiration from one or more cultural domains that are known as 'fandoms'. The resulting borrowings take place on various levels, such as themes, settings, characters, story world, and also style. Fanfiction usually is of an unofficial and unauthorized nature <ref type="bibr" coords="2,324.21,238.86,10.58,8.64" target="#b6">[7]</ref>, but because most fanfiction writers do not have any commercial purposes, the genre falls under the principle of 'Fair Use' in many countries <ref type="bibr" coords="2,209.46,262.77,10.58,8.64" target="#b7">[8]</ref>. From the perspective of writing style, fanfiction offers valuable benchmark data: the writings are unmediated and unedited before publication, which means that they should accurately reflect an individual author's writing style. Moreover, the rich metadata available for individual fics presents opportunities to quantify the extent to which fanfiction writers have modeled their writing style after the original author's style <ref type="bibr" coords="2,190.67,322.55,10.58,8.64" target="#b8">[9]</ref>.</p><p>In the previous edition of PAN we also dealt with authorship attribution in fanfiction and added extra difficulty with a cross-domain condition (i.e., different fandoms). This year we have further increased the difficulty of the task by focusing on open-set attribution conditions, meaning that the true author of a test text is not necessarily included in the list of candidate authors. More formally, an open cross-domain authorship attribution problem can be expressed as a tuple (A, K, U ), with A as the set of candidate authors, K as the set of reference (known authorship) texts, and U as the set of unknown authorship texts. For each candidate author a ∈ A, we are given K a ⊂ K, a set of texts unquestionably written by author a. Each text in U should either be assigned to exactly one a ∈ A, or the system should refrain from an attribution if the target author is not supposed to be in A. From a text categorization point of view, K is the training corpus and U is the test corpus. Let D K be (the names of) the fandoms of the texts in K. Then, all texts in U belong to a single fandom d U , with d U / ∈ D K .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Datasets</head><p>This year's shared tasks use datasets in four major Indo-European languages: English, French, Italian, and Spanish. For each language, 10 "problems" were constructed on the basis of a larger dataset obtained from archiveofourown.org in 2017. Per language, five problems were released as a development set to the participants in order to calibrate their systems. The final evaluation of the submitted systems was carried out on the five remaining problems that were not publicly released before the final results were communicated. Each problem had to be solved independently from the other problems. It should be noted that the development material could not be used as mere training material for supervised learning approaches since the candidate authors of the development corpus and the evaluation corpus do not overlap. Therefore, approaches should not be designed to particularly handle the candidate authors of the development corpus but should focus on their generalizability to other author sets.</p><p>One problem corresponds to a single open-set attribution task, where we distinguish between the "source" and the "target" material. The source material in each problem contains exactly 7 training texts for exactly 9 candidate authors. In the target material, these 9 authors are represented by at least one test text (possibly more). Additionally, the target material also contains so-called "adversaries", which were not written by one of the candidate authors (indicated by the author label "&lt;UNK&gt;"). The proportion of the number of target texts written by the candidate authors in problems, as opposed to &lt;UNK&gt; documents, was varied across the problems in the development dataset, in order to discourage systems from opportunistic guessing.</p><p>Let U K be the subset of U that includes all test documents actually written by the candidate authors, and let U U be the subset of U containing the rest of the test documents not written by any candidate author. Then, the adversary ratio r = |U U |/|U K | determines the likelihood of a test document to belong to one of the candidates. If r = 0 or close to 0), the problem is essentially a closed-set attribution scenario since all test documents belong to the candidate authors, or very few are actually written by adversaries. If r = 1, then it is equally probable for a test document to be written by a candidate author or by another author. For r &gt; 1 it is more likely for a test document to be written by an adversary not included in the list of candidates.</p><p>We examine cases where r ranges from 0.2 to 1.0. In greater detail, as can be seen in Table <ref type="table" coords="3,168.98,370.37,3.74,8.64" target="#tab_0">1</ref>, the development dataset comprises 5 problems per language that correspond to r = [0.2, 0.4, 0.6, 0.8, 1.0]. This dataset was released for the participants in order to develop and calibrate their submissions. The final evaluation dataset also includes 5 problems per language but with fixed r = 1. Thus, the participants are implicitly encouraged to develop generic approaches, because of the varying likelihood that a test document is written by a candidate or an adversary. In addition, it is possible to estimate the effectiveness of submitted methods when r &lt; 1 by ignoring their answers for specific subsets of U U in the evaluation dataset.</p><p>Each of the individual texts belongs to a single fandom, i.e., a certain topical domain. Fandoms were made available in the training material so that systems could exploit this information, as done by Seroussi et al. <ref type="bibr" coords="3,336.85,489.92,16.60,8.64" target="#b9">[10]</ref> for instance. We only selected works counting at least 500 tokens (according to the original database's internal token count), which is already a challenging text length for authorship analyses. Finally, we normalized document length: for fics longer than 1 000 tokens, we only included the middle 1 000 tokens of the text.</p><p>Another novelty this year was the inclusion of a set of 5 000 problem-external documents per language written by "imposter" authors (the authorship of these texts is also encoded as &lt;UNK&gt;). These documents could be freely used by the participants to develop their systems, for instance in the popular framework of imposter verification <ref type="bibr" coords="3,153.43,597.52,10.58,8.64" target="#b3">[4]</ref>. The release of this additional corpus should be understood against the backdrop of PAN's benchmarking efforts to improve the reproducibility and comparability of approaches. Many papers using imposter-based approaches do so on ad-hoc collected corpora, making it hard to compare the effect of the composition of the imposter collection. The imposter collection is given for the language as a whole and is thus not problem-specific. We provide the information that these texts were not written by authors who appear in the source or target sets for the problems in the language. When selecting these texts from the base dataset, we have given preference to texts from the fandoms covered in the problems, but when this selection was smaller than 5 000 texts, we have completed it with a random selection of other texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Framework</head><p>In the final evaluation phase, submitted systems were presented with 5 problems per language: for each problem, given a set of documents (known fanfics) by candidate authors, the systems had to identify the authors of another set of documents (unknown fanfics) in a previously unencountered fandom (target domain). Systems could assume that each candidate author had contributed at least one of the unknown fanfics to the problem, which all belonged to the same target fandom. Some of the fanfics in the target domain, however, were not written by any of the candidate authors. Like in the calibration set, the known fanfics belonged to several fandoms (excluding the target fandom), although not necessarily the same for all candidate authors. An equal number of known fanfics per candidate author was provided: 7 fanfics for 9 authors. By contrast, the unknown fanfics were not equally distributed over the authors. The submissions were separately evaluated in each attribution problem, based on their open-set macro-averaged F1 score (calculated over the training classes, i.e., when &lt;UNK&gt; is excluded) <ref type="bibr" coords="4,220.23,575.15,15.27,8.64" target="#b10">[11]</ref>. Participants were ranked according to their average open-set macro-F1 across all attribution problems of the evaluation corpus. A reference implementation of the evaluation script was made available to the participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline Methods</head><p>As usual, we provide the implementation of three baseline methods that provide an estimation of the overall difficulty of the problem given the state of the art in the field. These implementations are in Python (2.7+) and rely on Scikit-learn and its base packages <ref type="bibr" coords="5,154.75,131.27,16.60,8.64" target="#b11">[12,</ref><ref type="bibr" coords="5,171.35,131.27,12.45,8.64" target="#b12">13]</ref> as well as NLTK <ref type="bibr" coords="5,254.63,131.27,15.27,8.64" target="#b13">[14]</ref>. Participants were free to base their approach on one of these reference systems, or to develop their own approach from scratch. The provided baseline are as follows:</p><p>1. BASELINE-SVM. A language-independent authorship attribution approach, framing attribution as a conventional text classification problem <ref type="bibr" coords="5,375.39,184.86,15.27,8.64" target="#b14">[15]</ref>. It is based on a character 3-gram representation and a linear SVM classifier with a reject option. First, it estimates the probabilities of output classes based on Platt's method <ref type="bibr" coords="5,435.76,208.77,15.27,8.64" target="#b15">[16]</ref>. Then, it assigns an unknown document to the &lt;UNK&gt; class when the difference of the probabilities of the top two candidates is less than a predefined threshold. Let a 1 and a 2 , a 1 , a 2 ∈ A, be the two most likely authors of a certain test document while P r 1 and P r 2 are the corresponding estimated probabilities (i.e., all other candidates obtained lower probabilities). Then, if P r 1 -P r 2 &lt; 0.1, the document is left unattributed. Otherwise it is attributed to a 1 . 2. BASELINE-COMPRESSOR. A language-independent approach that uses text compression to estimate the distance of an unknown document to each of the candidate authors. This approach was originally proposed by <ref type="bibr" coords="5,359.58,315.92,16.60,8.64" target="#b16">[17]</ref> and was later reproduced by <ref type="bibr" coords="5,164.73,327.88,15.27,8.64" target="#b17">[18]</ref>. It uses the Prediction by Partial Matching (PPM) compression algorithm to build a model for each candidate author. Then, it calculates the cross-entropy of each test document with respect to the model of each candidate and assigns the document to the author with the lowest score. In order to adapt this method to the open-set classification scenario, we introduced a reject option. In more detail, a test document is left unattributed when the difference between the two most likely candidates is lower than a predefined threshold. Let a 1 and a 2 , a 1 , a 2 ∈ A, be the two most likely candidate authors for a certain test document while S 1 and S 2 are their cross-entropy scores (i.e., all other candidate authors have higher scores). If (S 1 -S 2 )/S 1 &lt; 0.01, then the test document is left unattributed. Otherwise, it is assigned to a 1 . 3. BASELINE-IMPOSTERS. Implementation of the language-independent imposters approach for authorship verification <ref type="bibr" coords="5,298.48,470.90,11.38,8.64" target="#b3">[4,</ref><ref type="bibr" coords="5,309.86,470.90,11.38,8.64" target="#b18">19]</ref>, based on character tetragram features. During a bootstrapped procedure, the technique iteratively compares an unknown text to each candidate author's training profile, as well as to a set of imposter documents, on the basis of a randomly selected feature subset. Then, the number of times the unknown document is found more similar to the candidate author's documents rather than to the imposters indicates how likely it is for that candidate to be the true author of the document. Instead of performing this procedure separately for each candidate author, we examine all candidate authors within each iteration (i.e., in each iteration, a maximum of one candidate author's score is increased). If after this repetitive process the highest score (corresponding to the most likely author) does not pass a fixed similarity threshold (here: 10% of repetitions), the document is assigned to the &lt;UNK&gt; class and is left unattributed. This baseline method is the only one that uses additional, problem-external imposter documents. We provided a collection of 5 000 imposter documents (fanfics on several fandoms) per language.</p><p>Finally, we also compare the participating systems to a plain "majority" baseline: through a simple voting procedure with random tie breaking, this baseline accepts a candidate for a given unseen text if the majority of submitted methods agree on it; otherwise, the &lt;UNK&gt; label is predicted. No meta-learning is applied to weigh the importance of the votes of individual systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Survey of Submissions</head><p>In total, 12 methods were submitted to the task and evaluated using the TIRA experimentation framework. All but one (Kipnis) of the submissions are described in the participants' notebook papers. Table <ref type="table" coords="6,285.90,222.92,4.98,8.64" target="#tab_4">5</ref> presents an overview of the main characteristics of the submitted methods as well as the baselines. We also record whether approaches made use of the language-specific imposter material or language-specific NLP resources, such as pretrained taggers and parsers. As can be seen, there is surprisingly little variance in the approaches. The majority of submissions follow the paradigm of the BASELINE-SVM or the winner approach <ref type="bibr" coords="6,320.44,282.70,16.60,8.64" target="#b19">[20]</ref> of PAN-2018 cross-domain authorship attribution task <ref type="bibr" coords="6,216.57,294.66,15.27,8.64" target="#b20">[21]</ref>, which is an ensemble of classifiers each of which based on a different text modality.</p><p>Compared to the baselines, most submitted methods attempt to exploit richer information that corresponds to different text modalities as well as variable-length ngrams <ref type="bibr" coords="6,163.21,342.48,16.60,8.64" target="#b21">[22]</ref> in contrast to fixed-length n-grams <ref type="bibr" coords="6,332.04,342.48,15.27,8.64" target="#b22">[23]</ref>. The most popular features are n-grams extracted from plain text modalities, such as character, word, token, part-ofspeech tag, or syntactic level sequences. Given the cross-domain conditions of the task, several participants attempted to use more abstract forms of textual information such as punctuation sequences <ref type="bibr" coords="6,238.71,390.30,16.60,8.64" target="#b23">[24,</ref><ref type="bibr" coords="6,255.32,390.30,12.45,8.64" target="#b21">22,</ref><ref type="bibr" coords="6,267.77,390.30,12.45,8.64" target="#b24">25]</ref> or n-grams extracted from distorted versions <ref type="bibr" coords="6,463.99,390.30,16.60,8.64" target="#b25">[26]</ref> of the original documents <ref type="bibr" coords="6,240.82,402.25,16.00,8.64" target="#b26">[27,</ref><ref type="bibr" coords="6,256.81,402.25,12.00,8.64" target="#b21">22,</ref><ref type="bibr" coords="6,268.81,402.25,12.00,8.64" target="#b24">25]</ref>. There is limited effort to enrich n-gram features with alternative stylometric measures like word and sentence length distributions <ref type="bibr" coords="6,463.99,414.21,16.60,8.64" target="#b27">[28]</ref> or features related to syntactic analysis of documents <ref type="bibr" coords="6,360.21,426.16,15.77,8.64" target="#b23">[24,</ref><ref type="bibr" coords="6,375.98,426.16,11.83,8.64" target="#b28">29]</ref>. Only one participant used word embeddings <ref type="bibr" coords="6,228.17,438.12,15.27,8.64" target="#b29">[30]</ref>. Other participants report that they were discouraged to use more demanding types of word and sentence embeddings due to hardware limitations of TIRA <ref type="bibr" coords="6,171.28,462.03,15.27,8.64" target="#b30">[31]</ref>, which points to important infrastructural needs that may be addressed in future editions. Those same teams, however, informally reported that the difference in performance (in the development dataset) when using such additional features is negligible.</p><p>With respect to feature weighting, tf-idf is the most popular option while the baseline methods are based on the simpler tf scheme. There is one attempt to use both of these schemes <ref type="bibr" coords="6,193.15,533.76,15.27,8.64" target="#b29">[30]</ref>. A quite different approach uses a normalization scheme based on zscores <ref type="bibr" coords="6,162.42,545.71,15.27,8.64" target="#b28">[29]</ref>. In addition, a few methods apply dimension reduction (PCA, SVD) to the features <ref type="bibr" coords="6,169.89,557.67,16.00,8.64" target="#b31">[32,</ref><ref type="bibr" coords="6,185.89,557.67,12.00,8.64" target="#b32">33,</ref><ref type="bibr" coords="6,197.89,557.67,12.00,8.64" target="#b21">22]</ref>. Judging from the results, such methods for dimension reduction have the potential to boost performance.</p><p>As concerns the classifiers, the most popular choices are SVMs and ensembles of classifiers, usually exploiting SVM base models followed by Logistic Regression (LR) models. In a few cases, the participants informally report that they have experimented with alternative classification algorithms (random forests, k-nn, naive Bayes) and found that SVM and LR are the most effective classifiers for this kind of task <ref type="bibr" coords="6,424.56,629.40,15.77,8.64" target="#b26">[27,</ref><ref type="bibr" coords="6,440.33,629.40,11.83,8.64" target="#b29">30]</ref>. None of the participant's methods is based on deep learning algorithms, most probably due to hardware limitations of TIRA or because of the discouraging reported results in the corresponding task of PAN-2018 <ref type="bibr" coords="7,268.32,131.27,15.27,8.64" target="#b20">[21]</ref>.</p><p>Given the fact that the focus of PAN-2019 edition of the task is on open-set attribution, it can be noted that none of the participants attempted to build a pure open-set classifier <ref type="bibr" coords="7,173.00,167.13,15.27,8.64" target="#b33">[34]</ref>. By contrast, they just use closed-set classifiers with a reject option (the classification prediction is dropped when the confidence of prediction is low), similar to the baseline methods <ref type="bibr" coords="7,231.06,191.04,15.27,8.64" target="#b34">[35]</ref>.</p><p>A crucial issue to improve the performance of authorship attribution is the appropriate tuning of hyperparameters. Most of the participants tune the hyperparameters of their approach globally based on the development dataset, that is, they estimate the most suitable parameter values that are applied to any problem of the test dataset. In contrast, a few participants attempt to tune the parameters of their method in a language-specific way, estimating the most suitable values for each language separately. None of the submitted methods attempts to tune parameter values for each individual attribution problem.</p><p>The submission of van Halteren focuses on the cross-domain difficulty of the task and attempts to exploit the availability of multiple texts of unknown authorship in the target domain within each attribution problem <ref type="bibr" coords="7,318.69,322.55,15.27,8.64" target="#b28">[29]</ref>. This submission performs a sophisticated strategy composed of different phases. Initially, a typical cross-domain classifier is built and each unknown document is assigned to its most likely candidate author but the prediction is kept only for the most confident cases. Then, a new in-domain classifier is built using the target domain documents (for which the predictions were kept in the previous phase) and the remaining target domain documents are classified accordingly. However, this in-domain classifier can only be useful for certain candidate authors, the ones with enough confident predictions in the initial phase. A final phase combines the results of cross-domain and in-domain classifiers and leaves documents with less confident predictions unattributed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Results</head><p>Table <ref type="table" coords="7,159.96,485.94,4.98,8.64" target="#tab_1">2</ref> shows an overview of the evaluation results of participants and their ranking according to their macro-F1 (averaged across all attribution problems of the dataset). As can be seen, all but one submission surpass the three baseline methods. In general, the submitted methods and the baselines achieve better macro-recall than macroprecision-which, interestingly, is not the case for the more prudent majority baseline. The two top-performing submissions obtain a very similar macro-F1 score. However, the winning approach of Muttenthaler et al. has better macro-precision while Bacciu et al. achieve better macro-recall. In terms of elapsed runtime, the winning approach of Muttenthaler et al. also proved to be a very efficient one.</p><p>Table <ref type="table" coords="7,173.99,593.53,4.98,8.64" target="#tab_2">3</ref> demonstrates the effectiveness (averaged macro-F1) of the submitted methods for each one of the four languages of the evaluation dataset. The winning approach of Muttenthaler et al. is more effective in English and French while the approach of Bacciu et al. achieves comparable performance in Italian and Spanish. In general, the variation of top-performing approaches across the four languages is low. On average, the highest performance is obtained for attribution problems in Italian; English proved to be the most difficult case. It is also remarkable that the baseline-compressor method achieves the best baseline results for English, French, and Italian, but it is not as competitive in Spanish. Furthermore, note that Muttenthaler et al.'s submission is the only one to outperform the otherwise very competitive majority baseline, albeit by a very small margin. The latter reaches a relatively high precision, but must sacrifice quite a bit of recall in return. That the winner outperforms the majority baseline is surprising: in previous editions of this shared task (e.g. <ref type="bibr" coords="8,307.97,434.42,14.94,8.64" target="#b35">[36]</ref>), similar meta-level approaches proved very hard to beat. This result is probably an artifact of the lack of diversity among the submissions in the top-scoring cohort, which seem to have produced very similar predictions (see below), thus reducing the beneficial effects of a majority vote among those systems.</p><p>In order to examine the effectiveness of the submitted methods for a varying adversary ratio, we performed the following additional evaluation process. As can be seen in Table <ref type="table" coords="8,159.69,518.11,3.74,8.64" target="#tab_0">1</ref>, all attribution problems of the evaluation dataset have a fixed adversary ratio r = 1, meaning that an equal number of documents written either by the candidate authors or adversary authors is included in the test set of each problem. Once the submitted methods processed the whole evaluation dataset, we calculated the evaluation measures with decreasing proportions of adversary documents at 100%, 80%, 60%, 40%, and 20%, resulting in an adversary ratio that ranges from 1 to 0.2. Table <ref type="table" coords="8,423.25,577.88,4.98,8.64" target="#tab_3">4</ref> presents the evaluation results (averaged macro-F1) for such a varying adversary ratio. In general, the performance of all methods increases when the adversary ratio decreases. Recall that r = 0 corresponds to a closed-set attribution case. The performance of the two top-performing approaches is very similar in the whole range of examined r-values. However, the method of Muttenthaler et al. is slightly better for high r-values while Bacciu et al. is slightly better for low r-values. Moreover, we have applied statistical significance tests to the systems' output. Especially since many systems have adopted a similar approach, it is worthwhile to discuss the extent to which submissions show statistically meaningful differences. Like in previous editions, we have applied approximate randomization testing, a non-parametric procedure that accounts for the fact that we should not make too many assumptions as to the underlying distributions for the classification labels. Table <ref type="table" coords="9,397.80,411.51,4.98,8.64" target="#tab_5">6</ref> lists the results for pairwise tests, comparing all submitted approaches to each other, based on their respective F1-scores for all labels in the problems. For 1 000 bootstrapped iterations, the test returns probabilities which we can interpret as the conventional p-values of one-sided, statistical tests-i.e., the probability of failing to reject the null hypothesis (H0) that the classifiers do not output significantly different scores. The symbolic notation takes into account the following straightforward thresholds: '=' (not significantly different: p &gt; 0.5), '*' (significantly different: p &lt; 0.05), '**' (very significantly different: p &lt; 0.01), '***' (highly significantly different: p &lt; 0.001). Interestingly, systems with neighboring ranks often do not yield significantly different scores; this is also true for the two top-performing systems. Note that almost all systems have produced an output that is significantly different from the three baselines (which also display a high degree of difference among one another). According to this test, the difference between Muttenthaler et al. and Bacciu et al. is not statistically significant, although the former is significantly different from the majority baseline.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The paper discussed the 12 submissions to the 2019 edition of the PAN shared task on authorship identification. Like last year, we focused on cross-domain attribution in fanfiction data. An important innovation this year was the focus on the open-set attribution set-up, where participating systems had to be able to refrain from attributing unseen texts as well. The analyses described above call for a number of considerations that are not without relevance to future development in the field of computational authorship identification. First of all, this year's edition was characterized by a relative low degree of diversity in approaches: especially the higher-scoring cohort almost exclusively adopted a highly similar approach, involving a combination of SVMs as classifier (potentially as part of an ensembles), character n-grams as features, and a rather simple thresholding mechanism to refrain from attributions. It is not immediately clear which directions future research might explore. Deep learning-based methods, which can be pretrained on external corpora, have so far not led to a major breakthrough in the field, despite the impressive improvements which have been reported for these methods in other areas of NLP. Also, a more promising research direction might be to move away from closed-set classifiers (with a naive reject-option), towards purely open-set classifiers <ref type="bibr" coords="13,154.41,334.51,16.60,8.64" target="#b33">[34]</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="12,174.11,271.70,71.96,8.06;12,276.15,272.04,25.98,7.77;12,322.48,272.04,212.20,7.77;12,551.12,272.04,89.66,7.77;12,211.71,283.75,34.36,8.06;12,297.07,284.10,343.71,7.77;12,199.50,295.80,46.57,8.06;12,322.48,296.15,5.06,7.77;12,347.88,296.15,182.32,7.77;12,551.12,296.15,89.66,7.77;12,164.46,307.86,81.61,8.06;12,347.88,308.21,5.06,7.77;12,373.28,308.21,5.06,7.77;12,398.69,308.21,8.97,7.77;12,424.10,308.21,106.67,7.77;12,551.12,308.21,89.66,7.77;12,170.12,319.91,75.95,8.06;12,373.29,320.26,5.06,7.77;12,398.69,320.26,5.06,7.77;12,424.10,320.26,30.46,7.77;12,474.90,320.26,8.97,7.77;12,500.31,320.26,4.48,7.77;12,525.71,320.26,5.06,7.77;12,551.12,320.26,89.66,7.77;12,185.56,331.97,60.51,8.06;12,398.69,332.32,4.48,7.77;12,424.09,332.32,85.18,7.77;12,525.72,332.32,5.06,7.77;12,551.12,332.32,89.66,7.77;12,217.18,344.02,28.89,8.06;12,424.10,344.37,30.46,7.77;12,474.91,344.37,5.06,7.77;12,500.31,344.37,5.06,7.77;12,525.71,344.37,5.06,7.77;12,551.12,344.37,89.66,7.77;12,206.34,356.08,39.73,8.06;12,449.50,356.43,8.97,7.77;12,474.90,356.43,5.06,7.77;12,500.31,356.43,8.97,7.77;12,525.71,356.43,30.46,7.77;12,576.53,356.43,64.25,7.77;12,223.15,368.13,22.92,8.06;12,474.91,368.48,5.06,7.77;12,500.31,368.48,5.06,7.77;12,525.72,368.48,115.07,7.77;12,195.01,380.19,51.06,8.06;12,500.31,380.54,5.06,7.77;12,525.71,380.54,115.08,7.77;12,191.03,392.24,55.04,8.06;12,525.72,392.59,8.97,7.77;12,551.12,392.59,89.66,7.77;12,218.67,404.30,27.40,8.06;12,551.12,404.65,89.66,7.77;12,193.52,416.35,50.31,8.06;12,576.52,416.70,64.26,7.77;12,167.04,428.41,79.03,8.06;12,601.93,428.76,38.85,7.77;12,172.10,440.46,71.73,8.06;12,627.33,440.81,13.45,7.77"><head></head><label></label><figDesc>*** *** *** *** *** *** *** ** *** *** *** *** Majority *** *** *** *** *** *** *** *** *** *** *** *** *** *** Bacciu et al. = *** *** *** *** *** *** *** * *** *** *** *** *** *** *** *** Van Halteren = *** *** *** *** *** Rahgouy et al. ** *** *** *** *** Gagala *** *** *** *** Baseline-svm *** *** *** Baseline-compressor *** *** baseline-impostors ***</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,115.83,345.83,180.01"><head>Table 1 .</head><label>1</label><figDesc>Details about the fanfiction datasets built for the cross-domain authorship attribution task. |A| refers to the size of candidates list, |Ka| is the amount of training documents per author a, a ∈ A, |U | is the amount of test documents, r is the adversary ratio, and l denotes the average length of the documents in words.</figDesc><table coords="4,156.37,173.63,302.86,122.21"><row><cell></cell><cell>Language</cell><cell># Problems</cell><cell>|A|</cell><cell>|Ka|</cell><cell>|U |</cell><cell>r</cell><cell>l</cell></row><row><cell>Development</cell><cell>English French Italian Spanish</cell><cell>5 5 5 5</cell><cell>9 9 9 9</cell><cell>7 7 7 7</cell><cell>137-561 38-430 46-196 112-450</cell><cell>0.2 -1.0 0.2 -1.0 0.2 -1.0 0.2 -1.0</cell><cell>804 790 814 846</cell></row><row><cell>Evaluation</cell><cell>English French Italian Spanish</cell><cell>5 5 5 5</cell><cell>9 9 9 9</cell><cell>7 7 7 7</cell><cell>98-180 48-290 34-302 172-588</cell><cell>1.0 1.0 1.0 1.0</cell><cell>817 790 821 838</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,115.83,345.83,223.58"><head>Table 2 .</head><label>2</label><figDesc>The final evaluation results of the cross-domain authorship attribution task. Participants and baselines are ranked according to macro-F1.</figDesc><table coords="8,161.23,150.94,292.89,188.47"><row><cell>Submission</cell><cell cols="5">Macro-Precision Macro-Recall Macro-F1 Runtime</cell></row><row><cell>Muttenthaler et al.</cell><cell></cell><cell>0.716</cell><cell>0.742</cell><cell>0.690</cell><cell>00:33:17</cell></row><row><cell>MAJORITY</cell><cell></cell><cell>0.748</cell><cell>0.708</cell><cell>0.686</cell></row><row><cell>Bacciu et al.</cell><cell></cell><cell>0.688</cell><cell>0.768</cell><cell>0.680</cell><cell>01:06:08</cell></row><row><cell>Custodio &amp; Paraboni</cell><cell></cell><cell>0.664</cell><cell>0.717</cell><cell>0.65</cell><cell>01:21:13</cell></row><row><cell>Bartelds &amp; de Vries</cell><cell></cell><cell>0.657</cell><cell>0.719</cell><cell>0.644</cell><cell>11:19:32</cell></row><row><cell>Rodríguez et al.</cell><cell></cell><cell>0.651</cell><cell>0.713</cell><cell>0.642</cell><cell>01:59:17</cell></row><row><cell>Isbister</cell><cell></cell><cell>0.629</cell><cell>0.706</cell><cell>0.622</cell><cell>01:05:32</cell></row><row><cell>Johansson</cell><cell></cell><cell>0.593</cell><cell>0.734</cell><cell>0.616</cell><cell>01:05:30</cell></row><row><cell>Basile</cell><cell></cell><cell>0.616</cell><cell>0.692</cell><cell>0.613</cell><cell>00:17:08</cell></row><row><cell>van Halteren</cell><cell></cell><cell>0.590</cell><cell>0.734</cell><cell>0.598</cell><cell>37:05:47</cell></row><row><cell>Rahgouy et al.</cell><cell></cell><cell>0.601</cell><cell>0.633</cell><cell>0.580</cell><cell>02:52:03</cell></row><row><cell>Gagala</cell><cell></cell><cell>0.689</cell><cell>0.593</cell><cell>0.576</cell><cell>08:22:33</cell></row><row><cell>BASELINE-SVM</cell><cell></cell><cell>0.552</cell><cell>0.635</cell><cell>0.545</cell></row><row><cell cols="2">BASELINE-COMPRESSOR</cell><cell>0.561</cell><cell>0.629</cell><cell>0.533</cell></row><row><cell cols="2">BASELINE-IMPOSTERS</cell><cell>0.428</cell><cell>0.580</cell><cell>0.395</cell></row><row><cell>Kipnis</cell><cell></cell><cell>0.270</cell><cell>0.409</cell><cell>0.259</cell><cell>20:20:21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.77,115.83,345.83,212.62"><head>Table 3 .</head><label>3</label><figDesc>Results (macro-F1) per language of the cross-domain authorship attribution task. Participants and baselines are ranked according to their overall macro-F1.</figDesc><table coords="9,167.17,150.94,281.02,177.51"><row><cell>Submission</cell><cell>English</cell><cell>French</cell><cell>Italian</cell><cell>Spanish</cell></row><row><cell>Muttenthaler et al.</cell><cell>0.665</cell><cell>0.705</cell><cell>0.717</cell><cell>0.673</cell></row><row><cell>Bacciu et al.</cell><cell>0.638</cell><cell>0.689</cell><cell>0.715</cell><cell>0.679</cell></row><row><cell>Custodio &amp; Paraboni</cell><cell>0.587</cell><cell>0.686</cell><cell>0.682</cell><cell>0.647</cell></row><row><cell>Bartelds &amp; de Vries</cell><cell>0.558</cell><cell>0.687</cell><cell>0.700</cell><cell>0.629</cell></row><row><cell>Rodríguez et al.</cell><cell>0.597</cell><cell>0.624</cell><cell>0.696</cell><cell>0.651</cell></row><row><cell>Isbister</cell><cell>0.529</cell><cell>0.644</cell><cell>0.691</cell><cell>0.623</cell></row><row><cell>Johansson</cell><cell>0.613</cell><cell>0.593</cell><cell>0.655</cell><cell>0.602</cell></row><row><cell>Basile</cell><cell>0.555</cell><cell>0.628</cell><cell>0.656</cell><cell>0.613</cell></row><row><cell>van Halteren</cell><cell>0.532</cell><cell>0.554</cell><cell>0.653</cell><cell>0.652</cell></row><row><cell>Rahgouy et al.</cell><cell>0.550</cell><cell>0.583</cell><cell>0.595</cell><cell>0.592</cell></row><row><cell>Gagala</cell><cell>0.554</cell><cell>0.564</cell><cell>0.566</cell><cell>0.619</cell></row><row><cell>BASELINE-SVM</cell><cell>0.490</cell><cell>0.548</cell><cell>0.566</cell><cell>0.577</cell></row><row><cell>BASELINE-COMPRESSOR</cell><cell>0.493</cell><cell>0.595</cell><cell>0.580</cell><cell>0.464</cell></row><row><cell>BASELINE-IMPOSTERS</cell><cell>0.359</cell><cell>0.409</cell><cell>0.410</cell><cell>0.400</cell></row><row><cell>Kipnis</cell><cell>0.301</cell><cell>0.232</cell><cell>0.285</cell><cell>0.220</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,134.77,268.70,345.83,237.49"><head>Table 4 .</head><label>4</label><figDesc>Evaluation results (macro-F1) of the cross-domain authorship attribution task for different values of the adversary ratio r. Participants and baselines are ranked according to their overall macro-F1.</figDesc><table coords="10,362.11,312.98,4.16,7.86"><row><cell>r</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,135.07,113.03,565.54,334.84"><head>Table 5 .</head><label>5</label><figDesc>Comparison of the core components of the submitted systems.</figDesc><table coords="11,135.07,138.45,565.54,309.42"><row><cell>Participant</cell><cell>Features</cell><cell>Weighting</cell><cell>Feature transfor-mation/selection</cell><cell>Parameter tuning</cell><cell>Classifier</cell><cell cols="2">Open-set criterion Use imposters data</cell><cell>Language-dependent resources</cell></row><row><cell></cell><cell>n-grams (char,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bacciu et al.</cell><cell>word, POS, stem,</cell><cell>tf-idf</cell><cell>NA</cell><cell>per language</cell><cell>Ensemble (SVM)</cell><cell>Reject</cell><cell>No</cell><cell>stemming, POS</cell></row><row><cell></cell><cell>distortion)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bartelds and de Vries</cell><cell>n-grams (char, token, POS, punctuation, ...</cell><cell>tf-idf</cell><cell>NA</cell><cell>global</cell><cell>SVM</cell><cell>Reject</cell><cell>No</cell><cell>POS, syntactic parse</cell></row><row><cell>Basile</cell><cell>n-grams (char and word)</cell><cell>tf-idf</cell><cell>NA</cell><cell>global</cell><cell>SVM</cell><cell>Reject</cell><cell>No</cell><cell>None</cell></row><row><cell>Custodio et al.</cell><cell>n-grams (char, word, distortion), ...</cell><cell>tf-idf</cell><cell>PCA</cell><cell>global</cell><cell>Ensemble (LR)</cell><cell>Reject</cell><cell>No</cell><cell>None</cell></row><row><cell>Gagala</cell><cell>n-grams (char, word)</cell><cell>tf-idf</cell><cell>PCA</cell><cell>global</cell><cell>Imposters (LR)</cell><cell>Verification</cell><cell>Yes</cell><cell>None</cell></row><row><cell></cell><cell>n-grams (char,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Isbister</cell><cell>word), word and</cell><cell>tf-idf</cell><cell>NA</cell><cell>global</cell><cell>SVM</cell><cell>Reject</cell><cell>No</cell><cell>None</cell></row><row><cell></cell><cell>sentence ...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>n-grams (char,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Johansson</cell><cell>word, POS,</cell><cell>tf-idf</cell><cell>NA</cell><cell>global</cell><cell>SVM</cell><cell>Reject</cell><cell>No</cell><cell>POS</cell></row><row><cell></cell><cell>distortion), word ...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>n-grams (char,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Muttenthaler et al.</cell><cell>word, distortion,</cell><cell>tf-idf</cell><cell>SVD</cell><cell>global</cell><cell>Ensemble (SVM)</cell><cell>Reject</cell><cell>No</cell><cell>None</cell></row><row><cell></cell><cell>punctuation)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>n-grams (char and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rahgouy et al.</cell><cell>word), word</cell><cell>tf-idf and tf</cell><cell>NA</cell><cell>global</cell><cell>Ensemble (SVM)</cell><cell>Reject</cell><cell>No</cell><cell>stemming</cell></row><row><cell></cell><cell>embeddings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>n-grams (char,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rodríguez et al.</cell><cell>typed, punctuation,</cell><cell>tf-idf</cell><cell>NA</cell><cell>global</cell><cell>Ensemble (SVM)</cell><cell>Reject</cell><cell>No</cell><cell>None</cell></row><row><cell></cell><cell>word)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Van Halteren</cell><cell>n-grams (char, token, syntactic)</cell><cell>z-score</cell><cell>NA</cell><cell>per language</cell><cell>Ensemble (distance-based and SVR)</cell><cell>Reject</cell><cell>No</cell><cell>POS, syntactic parse</cell></row><row><cell>baseline-SVM</cell><cell>n-grams (char)</cell><cell>tf</cell><cell>NA</cell><cell>global</cell><cell>SVM</cell><cell>Reject</cell><cell>No</cell><cell>None</cell></row><row><cell>baseline-Compressor</cell><cell>char sequences</cell><cell>none</cell><cell>NA</cell><cell>global</cell><cell>PPM</cell><cell>Reject</cell><cell>No</cell><cell>None</cell></row><row><cell>baseline-Imposters</cell><cell>n-grams (char)</cell><cell>tf</cell><cell>NA</cell><cell>global</cell><cell>distance-based</cell><cell>Verification</cell><cell>Yes</cell><cell>None</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,221.48,162.57,413.87,98.26"><head>Table 6 .</head><label>6</label><figDesc>Significance of pairwise differences in output between submissions and across all problems.</figDesc><table coords="12,258.27,185.24,377.08,75.59"><row><cell>Muttenthaler et al.</cell><cell>majority</cell><cell>Bacciu et al.</cell><cell>Custodio &amp; Paraboni</cell><cell>Bartelds &amp; de Vries</cell><cell>Rodríguez et al.</cell><cell>Isbister</cell><cell>Johansson</cell><cell>Basile</cell><cell>Van Halteren</cell><cell>Rahgouy et al.</cell><cell>Gagala</cell><cell>Baseline-svm</cell><cell>Baseline-compressor</cell><cell>Baseline-impostors</cell><cell>Kipnis</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.61,389.95,327.07,7.77;13,150.95,400.75,329.63,7.93" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,292.76,389.95,173.63,7.77">Computational methods in authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,150.95,400.75,259.24,7.73">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="26" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,411.71,305.82,7.93;13,150.95,422.83,73.47,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,183.83,411.87,78.72,7.77">Authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,269.07,411.71,175.86,7.73">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="334" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,433.62,308.87,7.93;13,150.95,444.74,77.95,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,233.88,433.78,182.01,7.77">A survey of modern authorship attribution methods</title>
		<author>
			<persName coords=""><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,423.20,433.62,24.24,7.73">JASIST</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,455.70,327.42,7.77;13,150.95,466.50,328.21,7.93;13,150.95,477.62,20.17,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,274.38,455.70,195.65,7.77;13,150.95,466.66,21.14,7.77">Determining if two documents are written by the same author</title>
		<author>
			<persName coords=""><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaron</forename><surname>Winter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,178.84,466.50,238.64,7.73">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="178" to="187" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,488.42,330.72,7.93;13,150.95,499.54,73.15,7.77" xml:id="b4">
	<monogr>
		<title level="m" coord="13,315.80,488.42,112.78,7.73">The Fan Fiction Studies Reader</title>
		<editor>
			<persName><forename type="first">Karen</forename><surname>Hellekson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kristina</forename><surname>Busse</surname></persName>
		</editor>
		<imprint>
			<publisher>University of Iowa Press</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,510.34,337.38,7.93;13,150.95,521.46,64.50,7.77" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="13,218.69,510.34,223.07,7.73">The Digital Afterlives of Jane Austen. Janeites at the Keyboard</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mirmohamadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Palgrave MacMillan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,532.25,314.42,7.93;13,150.95,543.37,127.40,7.77" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fathallah</surname></persName>
		</author>
		<title level="m" coord="13,197.63,532.25,256.03,7.73">Fanfiction and the Author. How FanFic Changes Popular Cultural Texts</title>
		<imprint>
			<publisher>Amsterdam University Press</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,554.17,327.16,7.93;13,150.95,565.13,175.96,7.93" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,195.38,554.33,218.22,7.77">Legal fictions: Copyright, fan fiction, and a new common law</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tushnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,420.45,554.17,49.32,7.73;13,150.95,565.13,125.66,7.73">Loyola of Los Angeles Entertainment Law Review</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,576.25,319.52,7.77;13,150.95,587.21,303.94,7.77;13,150.95,598.01,292.36,7.93;13,150.95,608.97,326.32,7.73;13,150.95,619.92,313.67,7.93;13,150.95,631.04,56.04,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,340.29,587.21,114.61,7.77;13,150.95,598.17,193.08,7.77">Overview of PAN 2018 -author identification, author profiling, and author obfuscation</title>
		<author>
			<persName coords=""><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><forename type="middle">M Rangel</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Tschuggnall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,360.64,598.01,82.67,7.73;13,150.95,608.97,326.32,7.73;13,150.95,619.92,86.63,7.73;13,395.31,619.92,42.77,7.73">Experimental IR Meets Multilinguality, Multimodality, and Interaction -9th International Conference of the CLEF Association, CLEF 2018</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
			<biblScope unit="page" from="267" to="285" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct coords="13,142.24,642.00,324.94,7.77;13,150.95,652.80,209.22,7.93" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,349.04,642.00,118.13,7.77;13,150.95,652.96,24.12,7.77">Authorship attribution with topic models</title>
		<author>
			<persName coords=""><forename type="first">Yanir</forename><surname>Seroussi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ingrid</forename><surname>Zukerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabian</forename><surname>Bohnert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,182.33,652.80,94.18,7.73">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="269" to="310" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,119.96,329.62,7.77;14,150.95,130.92,322.51,7.77;14,150.95,141.72,321.08,7.93;14,150.95,152.84,99.62,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,216.44,141.88,180.72,7.77">Nearest neighbors distance ratio open-set classifier</title>
		<author>
			<persName coords=""><forename type="first">Pedro</forename><forename type="middle">R Mendes</forename><surname>Júnior</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><forename type="middle">M</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rafael</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Werneck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><forename type="middle">V</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">V</forename><surname>Pazinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Waldir</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>De Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Otávio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ricardo</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anderson</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,403.44,141.72,64.70,7.73">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="386" />
			<date type="published" when="2017-03">Mar 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,163.80,323.73,7.77;14,150.95,174.76,287.09,7.77;14,150.95,185.71,302.87,7.77;14,150.95,196.51,221.58,7.93" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,305.14,185.71,144.73,7.77">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,150.95,196.51,138.96,7.73">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,207.63,289.34,7.77" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Travis</forename><surname>Oliphant</surname></persName>
		</author>
		<title level="m" coord="14,212.46,207.63,96.94,7.77">NumPy: A guide to NumPy</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Trelgol Publishing</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,218.43,328.97,7.93;14,150.95,229.55,81.19,7.77" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="14,316.26,218.43,151.01,7.73">Natural Language Processing with Python</title>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,240.35,309.54,7.93;14,150.95,251.31,96.60,7.93" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,202.03,240.51,181.97,7.77">Machine Learning in automated text categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,390.76,240.35,61.02,7.73;14,150.95,251.31,26.13,7.73">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,262.43,321.48,7.77;14,150.95,273.39,325.34,7.77;14,150.95,284.18,209.86,7.93" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,180.83,262.43,282.88,7.77;14,150.95,273.39,67.07,7.77">Probabilistic outputs for support vector machines and comparison to regularize likelihood methods</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,150.95,284.18,134.55,7.73">Advances in Large Margin Classifiers</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schoelkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,295.14,327.79,7.93;14,150.95,306.10,276.06,7.93" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="14,295.20,295.14,174.83,7.73;14,150.95,306.10,68.65,7.73">Using Compression-Based Language Models for Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">J</forename><surname>Teahan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="141" to="165" />
			<pubPlace>Netherlands; Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,317.22,336.17,7.77;14,150.95,328.18,304.50,7.77;14,150.95,339.14,299.84,7.77;14,150.95,350.10,308.53,7.77;14,150.95,361.06,326.73,7.77;14,150.95,372.02,318.66,7.77;14,150.95,382.97,329.64,7.77;14,150.95,393.77,303.05,7.93;14,150.95,404.89,144.21,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,179.83,361.06,297.86,7.77;14,150.95,372.02,82.30,7.77">Who wrote the web? Revisiting influential author identification research applicable to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarah</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tolga</forename><surname>Buz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabian</forename><surname>Duffhauss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Florian</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jörg</forename><surname>Marvin Gülzow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Winfried</forename><surname>Lötzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabian</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elisa</forename><surname>Maike</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernhard</forename><surname>Paßmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucas</forename><surname>Reinke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Rettenmeier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timo</forename><surname>Rometsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Träger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Efstathios</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,179.84,393.77,212.14,7.73">Proc. of the European Conference on Information Retrieval</title>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Josiane</forename><surname>Mothe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Giorgio</forename><surname>Maria</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Di</forename><surname>Nunzio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Claudia</forename><surname>Hauff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gianmaria</forename><surname>Silvello</surname></persName>
		</editor>
		<meeting>of the European Conference on Information Retrieval</meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="393" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,415.85,318.77,7.77;14,150.95,426.65,326.97,7.93;14,150.95,437.77,58.53,7.77" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,196.26,426.81,152.95,7.77">Authenticating the writings of julius caesar</title>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><forename type="middle">Anthony</forename><surname>Stover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Folgert</forename><surname>Karsdorp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,355.89,426.65,118.38,7.73">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="86" to="96" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,448.73,309.07,7.77;14,150.95,459.69,326.68,7.77;14,150.95,470.48,310.97,7.93;14,150.95,481.60,231.74,7.77" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,319.67,448.73,131.64,7.77;14,150.95,459.69,76.24,7.77">EACH-USP Ensemble cross-domain authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Eleandro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Custódio</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivandré</forename><surname>Paraboni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,179.84,470.48,277.80,7.73">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="14,150.95,481.60,185.73,7.77">CEUR Workshop Proceedings. CLEF and CEUR-WS</title>
		<editor>
			<persName><forename type="first">Linda</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Laure</forename><surname>Soulier</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,492.56,334.91,7.77;14,150.95,503.52,311.69,7.77;14,150.95,514.32,312.94,7.93;14,150.95,525.28,329.64,7.93" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,304.62,503.52,158.02,7.77;14,150.95,514.48,266.44,7.77">Overview of the author identification task at PAN-2018: cross-domain authorship attribution and style change detection</title>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Tschuggnall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Günther</forename><surname>Specht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,433.84,514.32,30.06,7.73;14,150.95,525.28,244.29,7.73">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<publisher>CEUR-WS.org</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,536.40,300.04,7.77;14,150.95,547.36,292.60,7.77;14,150.95,558.16,325.21,7.93;14,150.95,569.11,203.92,7.93" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="14,350.79,536.40,91.48,7.77;14,150.95,547.36,254.79,7.77">Authorship Attribution in Fan-fictional Texts Given Variable Length Character and Word n-grams</title>
		<author>
			<persName coords=""><forename type="first">Lukas</forename><surname>Muttenthaler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Janek</forename><surname>Amann</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="14,414.38,558.16,61.78,7.73;14,150.95,569.11,52.83,7.73">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="14,210.15,569.11,84.56,7.93">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">Linda</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,580.23,308.75,7.77;14,150.95,591.03,325.21,7.93;14,150.95,601.99,203.92,7.93" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="14,207.47,580.23,206.17,7.77">An Open-Vocabulary Approach to Authorship Attribution</title>
		<author>
			<persName coords=""><forename type="first">Angelo</forename><surname>Basile</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="14,414.38,591.03,61.78,7.73;14,150.95,601.99,52.83,7.73">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="14,210.15,601.99,84.56,7.93">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">Linda</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,613.11,335.53,7.77;14,150.95,624.07,316.20,7.77;14,150.95,634.87,322.77,7.93;14,150.95,645.99,79.04,7.77" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,289.79,613.11,187.98,7.77;14,150.95,624.07,151.40,7.77">Improving Cross-Domain Authorship Attribution by Combining Lexical and Syntactic Features</title>
		<author>
			<persName coords=""><forename type="first">Martijn</forename><surname>Bartelds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wietse</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vries</forename></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="14,287.06,634.87,116.85,7.73">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="14,410.28,634.87,63.45,7.73;14,150.95,645.99,18.87,7.77">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">Linda</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.24,119.96,303.58,7.77;15,150.95,130.92,312.50,7.77;15,150.95,141.88,323.29,7.77;15,150.95,152.68,318.22,7.93;15,150.95,163.64,243.78,7.93" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="15,150.95,141.88,317.87,7.77">Authorship Attribution through Punctuation n-grams and Averaged Combination of SVM</title>
		<author>
			<persName coords=""><forename type="first">Carolina Martín</forename><surname>Del Campo Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alejandro</forename><forename type="middle">Pérez</forename><surname>Álvarez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Efraín Maldonado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grigori</forename><surname>Sifuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ildar</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Batyrshin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gelbukh</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="15,447.25,152.68,21.92,7.73;15,150.95,163.64,92.68,7.73">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="15,250.01,163.64,84.56,7.93">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">Linda</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.24,174.59,327.28,7.93;15,150.95,185.55,329.64,7.73;15,150.95,196.51,309.77,7.93;15,150.95,207.63,20.17,7.77" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="15,233.88,174.76,152.53,7.77">Authorship attribution using text distortion</title>
		<author>
			<persName coords=""><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,402.68,174.59,66.83,7.73;15,150.95,185.55,326.11,7.73">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<title level="s" coord="15,188.56,196.51,43.68,7.73">Long Papers</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1138" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.24,218.59,338.25,7.77;15,150.95,229.55,316.02,7.77;15,150.95,240.51,322.80,7.77;15,150.95,251.31,325.36,7.93" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="15,218.09,229.55,248.88,7.77;15,150.95,240.51,79.31,7.77">Cross-domain Authorship Attribution Combining Instance Based and Profile Based Features</title>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">La</forename><surname>Massimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alessandro</forename><surname>Morgia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugenio</forename><forename type="middle">Nerio</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valerio</forename><surname>Nemmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julinda</forename><surname>Neri</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Stefa</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="15,208.37,251.31,116.85,7.73">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="15,331.59,251.31,84.56,7.93">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">Linda</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.24,262.43,306.08,7.77;15,150.95,273.39,326.01,7.77;15,150.95,284.18,325.36,7.93" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="15,282.13,262.43,166.19,7.77;15,150.95,273.39,82.67,7.77">FOI Cross-Domain Authorship Attribution for Criminal Investigations</title>
		<author>
			<persName coords=""><forename type="first">Fredrik</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Isbister</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="15,208.37,284.18,116.85,7.73">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="15,331.59,284.18,84.56,7.93">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">Linda</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.24,295.30,308.38,7.77;15,150.95,306.10,325.21,7.93;15,150.95,317.06,203.92,7.93" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="15,222.92,295.30,190.18,7.77">Cross-Domain Authorship Attribution with Federales</title>
		<author>
			<persName coords=""><surname>Hans Van Halteren</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="15,414.38,306.10,61.78,7.73;15,150.95,317.06,52.83,7.73">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="15,210.15,317.06,84.56,7.93">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">Linda</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.24,328.18,334.49,7.77;15,150.95,339.14,321.98,7.77;15,150.95,350.10,314.34,7.77;15,150.95,360.90,322.77,7.93;15,150.95,372.02,79.04,7.77" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="15,257.92,339.14,215.01,7.77;15,150.95,350.10,148.98,7.77">Cross-domain Authorship Attribution: Author Identification using a Multi-Aspect Ensemble Approach</title>
		<author>
			<persName coords=""><forename type="first">Mostafa</forename><surname>Rahgouy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamed</forename><surname>Babaei Giglou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Taher</forename><surname>Rahgooy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Karami Sheykhlan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erfan</forename><surname>Mohammadzadeh</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="15,287.06,360.90,116.85,7.73">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="15,410.28,360.90,63.45,7.73;15,150.95,372.02,18.87,7.77">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">Linda</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.24,382.97,338.35,7.77;15,150.95,393.77,329.07,7.93;15,150.95,404.73,278.70,7.93" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="15,386.26,382.97,94.33,7.77;15,150.95,393.93,43.44,7.77">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matti</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,351.40,393.77,128.63,7.73;15,150.95,404.73,216.45,7.73">Information Retrieval Evaluation in a Changing World -Lessons Learned from 20 Years of CLEF</title>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.24,415.85,323.38,7.77;15,150.95,426.81,310.32,7.77;15,150.95,437.61,296.83,7.93" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="15,285.06,415.85,180.56,7.77;15,150.95,426.81,38.42,7.77">Multi-channel Open-set Cross-domain Authorship Attribution</title>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Custódio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivandre</forename><surname>Paraboni</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="15,179.84,437.61,116.85,7.73">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="15,303.06,437.61,84.56,7.93">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">Linda</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.24,448.73,330.60,7.77;15,150.95,459.53,325.21,7.93;15,150.95,470.48,203.92,7.93" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="15,209.90,448.73,225.31,7.77">Authorship Attribution with Logistic Regression and Imposters</title>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Gagala</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="15,414.38,459.53,61.78,7.73;15,150.95,470.48,52.83,7.73">CLEF 2019 Labs and Workshops</title>
		<title level="s" coord="15,210.15,470.48,84.56,7.93">Notebook Papers. CEUR</title>
		<editor>
			<persName><forename type="first">Linda</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.24,481.60,331.82,7.77;15,150.95,492.40,282.33,7.93;15,150.95,503.52,86.92,7.77" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="15,415.32,481.60,58.73,7.77;15,150.95,492.56,39.49,7.77">Toward open set recognition</title>
		<author>
			<persName coords=""><forename type="first">Walter</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anderson</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Archana</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Terrance</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,197.27,492.40,232.60,7.73">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.24,514.48,315.28,7.77;15,150.95,525.28,226.44,7.93" xml:id="b34">
	<monogr>
		<title level="m" type="main" coord="15,356.65,514.48,100.87,7.77;15,150.95,525.44,75.97,7.77">Recent advances in open set recognition: A survey</title>
		<author>
			<persName coords=""><forename type="first">Chuanxing</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheng-Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Songcan</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08581</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.24,536.40,326.41,7.77;15,150.95,547.36,310.76,7.77;15,150.95,558.16,311.98,7.93;15,150.95,569.11,198.87,7.93" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="15,403.81,547.36,57.90,7.77;15,150.95,558.32,113.07,7.77">Overview of the author identification task at pan</title>
		<author>
			<persName coords=""><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Juola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Sánchez-Pérez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,299.37,558.16,163.56,7.73;15,150.95,569.11,77.97,7.73">CLEF 2014 Evaluation Labs and Workshop -Working Notes Papers</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09">2014. September 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
