<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,133.22,152.67,324.36,12.64;1,457.66,151.07,4.50,8.10">Event Sentence Detection Task Using Attention Model *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,231.53,198.42,53.39,8.96"><surname>Protestnews</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab at CLEF 2019</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,276.41,233.73,42.33,8.96"><forename type="first">Ali</forename><surname>Safaya</surname></persName>
							<email>alisafaya@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Sakarya University</orgName>
								<address>
									<postCode>54055</postCode>
									<settlement>AdapazarÄ±</settlement>
									<region>Sakarya</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,133.22,152.67,324.36,12.64;1,457.66,151.07,4.50,8.10">Event Sentence Detection Task Using Attention Model *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">55FBEDD16E4BF7E2EFF7842CB0062260</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information extraction</term>
					<term>Natural language processing</term>
					<term>Sequence classification</term>
					<term>Event sentence detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes and evaluates a model for event sentence detection in news articles using Attention Models with Bidirectional Gated Recurrent Network (GRU) and Word Embeddings. This model was developed for event sentence detection task in the competition that was organized by ProtestNews lab at CLEF 2019. We also evaluated the generalizability of NLP tools by training our model on data from one country and testing it on data from another country. The model was developed for this task was shown to have the highest score in the organized competition with average F1-score of 0.6547.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This task aims to identifying and labeling sentences that contain protest events in news articles. It follows the document labeling task which identifies news articles that contain protest events as identified in the Event Labeling Annotation Manual <ref type="bibr" coords="1,415.63,497.75,10.69,8.96" target="#b0">[1]</ref>. Once the news reports are classified as containing a protest event, what remains is to identify where in the article the relevant event information is presented. In terms of this task, we will analyze the sentences of the protest news articles one by one and classify them as event-sentence vs. non-event-sentence. Event sentences, those that are labeled as 1, should contain an explicit reference to any protest event that makes the document eligible for being classified as a protest article. Such reference can be any word or phrase which denotes the said event. They can be direct expressions of the event or the pronouns which stand for the event. The sentence must clearly indicate that the event in question has definitely happened in the past or is an ongoing event.</p><p>Copyright (c) 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CLEF 2019, 9-12 September 2019, Lugano, Switzerland.</p><p>Non-event sentences, i.e. those that are labeled as 0, are the ones which does not contain any event reference in the past, the present or the future.</p><p>The main goal for this task is to set a baseline in evaluating generalizability of the NLP tools. The setting was proposed facilitates testing and improving state-of-the-art methods for text classification and information extraction on English news article texts from India and China. The direction of ProtestNews lab work is towards developing generalizable information systems that perform comparatively well on texts from multiple countries <ref type="bibr" coords="2,183.98,234.21,10.71,8.96" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Collection and Methodology</head><p>ProtestNews lab organizing committee have collected online English news articles from India and China. The annotation process started by labeling articles in a sample of news articles as containing protest or not which will be used for Task 1. Sentences of these positively marked documents are then labeled as containing protest information or not. These sentences should contain either an event trigger or a reference to an event trigger in order to be labeled as positive <ref type="bibr" coords="2,257.09,351.21,10.69,8.96" target="#b1">[2]</ref>. Our deep learning based model was trained using Training-India set and Validated on Validation-India set, the data retrieved from China was not involved in training at all, so when the model was evaluated on the test sets we could obtain independent and generalized results.</p><p>F1-score metric (1), was used in the evaluation process for this task, as it gives more accurate assessment results for this kind of tasks where there is non-equal number of negative and positive samples.</p><formula xml:id="formula_0" coords="2,222.41,572.55,150.55,24.36">ğ¹ 1 = 2 Ã— ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› Ã— ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› + ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing and Tokenization</head><p>Before feeding data into the model, our text samples which are sentences taken from articles were to be cleaned and parsed into lower case words. Because word embeddings were used in the classification process, a word index had to be created according to the embeddings set in use and once word sequences were obtained, some of irrelevant words had to be dropped and those words were determined by the word index.</p><p>Also the sequences had to have fixed length of tokens, so sequence length was limited to 35 tokens. Longer sequences where truncated and shorter sequences were prepadded with 0 indexed token in order to reach 35 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word Embeddings</head><p>To feed those word sequences to our deep learning model every token had to be represented by some value or vector. In this model word based representations were used, so every word was replaced by embedding vector. This embeddings vector set was obtained from Google's pretrained set [3]. Which was trained using word2vec <ref type="bibr" coords="3,435.31,284.25,11.72,8.96" target="#b2">[4]</ref> algorithm on part of Google News database (100 billion words) and contains 300 dimensional vectors for 3 million English words. In this work only the most frequent 400000 vocabulary were used in the word index. Every token was replaced by 300 dimensional vector and maximum sequence length was limited to 35 token. So every sentence was represented by matrix of shape (300, 35).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bidirectional Gated Recurrent Unit (GRU)</head><p>In sequence classification tasks, Recurrent Neural Networks (RNN) and its variations had always been the state-of-art tool. After obtaining an embedding for each sample, the main approach will be using bidirectional GRU <ref type="bibr" coords="3,330.19,437.27,10.69,8.96" target="#b3">[5]</ref>. As Fig. <ref type="figure" coords="3,378.27,437.27,4.98,8.96">1</ref> shows, every layer of Bidirectional GRU, contents of GRU cells for each direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Bidirectional GRU network model</head><p>Every cell has two gates; an update gate ğ‘§ ğ‘¡ and reset gate ğ‘Ÿ ğ‘¡ (Fig. <ref type="figure" coords="4,391.13,150.18,3.64,8.96" target="#fig_0">2</ref>). Sigma representations demonstrate these gates: which allows a GRU to carry information over many time periods to influence a future time zone. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Attention Models</head><p>Attention Models were firstly represented in 2015 by Dzmitry Bahdanau et al <ref type="bibr" coords="4,437.35,405.71,10.71,8.96" target="#b3">[5]</ref>. Past conventional methods used to find features from the text by doing a keyword extraction and some words are more helpful in determining the category of a text than others. However, in this method the sequential structure of the text is not fully used. With deep learning methods, while we can take care of the sequence structure, the ability to give higher weight to more important words is lost. The firstly proposed model was meant for Machine Translation purposes, while using Attention Models mechanism for text classification tasks was proposed in the paper written jointly by CMU and Microsoft in 2016 <ref type="bibr" coords="4,313.61,501.71,10.69,8.96" target="#b4">[6]</ref>. In author's words: Not all words contribute equally to the representation of the sentence meaning. Hence, we introduce attention mechanism to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector In this model (see Fig. <ref type="figure" coords="4,230.46,585.74,4.17,8.96">3</ref>) the Attention layer is added after the last GRU layer. So the Attention Models output is the dot product of Attention Similarity Vector si and GRU cells output ai.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3. Bidirectional GRU with Attention Model</head><p>The main goal is to create scores (si) for every word in the text, which is the attention similarity score for a word. Here in Fig. <ref type="figure" coords="5,282.16,448.91,3.76,8.96" target="#fig_1">4</ref>, we could see how those scores are calculated. These final scores are then multiplied by GRU output for words to weight them according to their importance. After which the outputs are summed and sent through dense layers and then to the last output function <ref type="bibr" coords="6,318.17,174.18,10.70,8.96" target="#b5">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Modeling The Network and Evaluation</head><p>Machine learning model is shown in Fig. <ref type="figure" coords="6,294.02,231.21,3.76,8.96" target="#fig_1">4</ref>. After Embedding layer two Bidirectional GRU layers are introduced, with 128, 64 cells respectively. On the top of them an Attention with Context layer was added and followed by dense layer of 64 nodes with ReLU as their activation function. Finally an output layer was added with one node containing sigmoid function for binary classification output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5. Our deep learning models structure</head><p>The model was trained on Training-India dataset (see Table <ref type="table" coords="6,383.59,634.10,4.17,8.96" target="#tab_0">1</ref>) for 8 epochs using Nadam <ref type="bibr" coords="6,155.99,646.10,11.74,8.96" target="#b6">[8]</ref> as optimizer function and validated through Validation-India dataset.</p><p>While testing the model on test datasets it could be observed (as in Table <ref type="table" coords="6,444.01,658.10,4.17,8.96" target="#tab_1">2</ref>) that performance (F1-score) dropped from 0.70 on Test-India dataset which is the same source that Training data was obtained, to 0.60 on Test-China dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this task we worked on deep learning model which tries to classify and detect event sentences in news articles. The proposed model uses Bidirectional GRU with Attention Models. The results obtained from this model were the highest in the competition which had been organized by ProtestNews Lab.</p><p>With this experiment we could observe the effect of local data on NLP tools, our test results on datasets from the same source of training sets were noticeably higher than those on datasets from other sources.</p><p>For further work, we could evaluate POS based features of the words in the sentences by adding one more input layer in parallel with embedding layer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,248.09,324.63,99.02,9.05;4,124.68,207.36,345.96,108.84"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. GRU Cell structure 1</figDesc><graphic coords="4,124.68,207.36,345.96,108.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,220.97,621.80,153.26,9.05;5,131.16,470.04,332.88,144.00"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Context vector calculation method 2</figDesc><graphic coords="5,131.16,470.04,332.88,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,193.92,300.36,207.48,301.80"><head></head><label></label><figDesc></figDesc><graphic coords="6,193.92,300.36,207.48,301.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,136.10,375.04,270.10,92.97"><head>Table 1 .</head><label>1</label><figDesc>Distribution of collected data samples</figDesc><table coords="2,154.82,394.60,251.38,60.63"><row><cell>Data set</cell><cell>Negative</cell><cell>Positive</cell><cell>All</cell></row><row><cell>Training-India</cell><cell>4897</cell><cell>988</cell><cell>5885</cell></row><row><cell>Validation-India</cell><cell>525</cell><cell>138</cell><cell>663</cell></row><row><cell>Test-India</cell><cell>*</cell><cell>*</cell><cell>1107</cell></row><row><cell>Test-China</cell><cell>*</cell><cell>*</cell><cell>1235</cell></row></table><note coords="2,136.10,460.78,161.79,7.24"><p>(*) Was kept hidden by the commission of the lab.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,130.82,173.99,329.42,80.52"><head>Table 2 .</head><label>2</label><figDesc>Evaluation of trained model on different datasets Was kept hidden by the commission of the lab.</figDesc><table coords="7,130.82,191.10,329.42,63.42"><row><cell>Metric</cell><cell>Training-India</cell><cell>Validation-India</cell><cell>Test-India</cell><cell>Test-China</cell></row><row><cell>F1</cell><cell>0.7984</cell><cell>0.7094</cell><cell>0.7055</cell><cell>0.6039</cell></row><row><cell>Precision</cell><cell>0.7137</cell><cell>0.7402</cell><cell>*</cell><cell>*</cell></row><row><cell>Recall</cell><cell>0.9059</cell><cell>0.6812</cell><cell>*</cell><cell>*</cell></row><row><cell>(*)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,136.10,686.25,298.87,8.10"><p>http://colah.github.io/posts/2015-08-Understanding-LSTMs/ last accessed on June</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2019" xml:id="foot_1" coords=""><p></p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,132.67,467.32,337.98,8.10;7,141.74,478.24,42.81,8.10" xml:id="b0">
	<monogr>
		<ptr target="https://emw.ku.edu.tr/clef-protestnews-2019/" />
		<title level="m" coord="7,141.74,467.32,102.03,8.10">ProtestNews lab Homepage</title>
		<imprint>
			<date type="published" when="2019-05-23">23.05.2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,132.67,489.19,337.99,8.19;7,141.74,500.32,328.54,8.10;7,141.74,511.24,306.60,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,195.67,500.32,274.62,8.10;7,141.74,511.24,66.78,8.10">A Task Set Proposal for Automatic Protest Information Collection Across Multiple Countries</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>HÃ¼rriyetoÄŸlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>YÃ¶rÃ¼k</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>YÃ¼ret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ã‡</forename><surname>Yoltar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>GÃ¼rel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>DuruÅŸan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,224.23,511.24,169.87,8.10">European Conference on Information Retrieval</title>
		<imprint>
			<date type="published" when="2019-04">2019, April</date>
			<biblScope unit="page" from="316" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,132.67,544.27,337.60,8.10;7,141.74,555.31,274.66,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,366.38,544.27,103.89,8.10;7,141.74,555.31,109.25,8.10">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,266.69,555.31,122.66,8.10">Proceedings of Workshop at ICLR</title>
		<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,132.67,566.35,338.07,8.10;7,141.74,577.27,236.54,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,323.71,566.35,147.03,8.10;7,141.74,577.27,119.67,8.10">Neural Machine Translation By Jointly Learning To Align And Translate</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,276.98,577.27,74.11,8.10">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,132.67,588.31,337.96,8.10;7,141.74,599.35,328.77,8.10;7,141.74,610.27,20.45,8.10" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,371.23,588.31,99.40,8.10;7,141.74,599.35,125.05,8.10">Hierarchical Attention Networks for Document Classification</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<publisher>Redmond</publisher>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University, Microsoft Research</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,132.67,621.31,338.05,8.10;7,141.74,632.35,328.98,8.10;7,141.74,643.27,74.35,8.10" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,178.68,621.31,288.56,8.10">NLP Learning Series: Part 3-Attention, CNN and what not for Text Classification</title>
		<author>
			<persName coords=""><surname>Mlwhiz</surname></persName>
		</author>
		<ptr target="https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/,lastaccessed23.05.2019" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,132.67,654.31,338.10,8.10;7,141.74,665.35,214.90,8.10" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dozat</surname></persName>
		</author>
		<ptr target="http://cs229.stan-ford.edu/proj2015/054_report.pdf" />
		<title level="m" coord="7,195.89,654.31,197.49,8.10">Incorporating Nesterov Momentum into Adam</title>
		<imprint>
			<date type="published" when="2019-05-24">24.05.2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
