<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.28,115.96,314.79,12.62;1,245.60,133.89,124.15,12.62">Classification and Event Identification Using Word Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,136.35,171.84,67.88,8.74"><forename type="first">Anaïs</forename><surname>Ollagnier</surname></persName>
							<email>a.ollagnier@exeter.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">University of Exeter</orgName>
								<address>
									<postCode>EX4 4QE</postCode>
									<settlement>Exeter</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.62,171.84,68.69,8.74"><forename type="first">Hywel</forename><surname>Williams</surname></persName>
							<email>h.t.p.williams@exeter.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">University of Exeter</orgName>
								<address>
									<postCode>EX4 4QE</postCode>
									<settlement>Exeter</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.28,115.96,314.79,12.62;1,245.60,133.89,124.15,12.62">Classification and Event Identification Using Word Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CA885261D7B2C15BF936B2E21E27CC8C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data mining</term>
					<term>Classification</term>
					<term>Event detection</term>
					<term>Word embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our contribution to the CLEF 2019 Protest-News Track, which aims to classify and identify protest events in Englishlanguage news from India and China. We used traditional classification models, namely, support vector machines and XGBoost classifiers, combined with various word embedding approaches. Multiple models were tested for experimental purposes, in addition to the two models evaluated within the official campaign. Results show promising performance, especially in terms of precision on both document and sentence classification tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The CLEF ProtestNews Track was introduced in 2019 aiming to evaluate methods for event classification and detection from news articles across multiple countries. This track has two main goals: firstly, development of generalisable methods which can be applied to heterogeneous news article data; and secondly, to support surveys conducted in other scientific fields such as social and political studies by providing data on political conflict events (e.g. protests, riots). This track includes three tasks: news article classification, event sentence detection and event extraction. Our contribution is focused on the first two tasks. The news article classification task consists of identifying news articles associated with political conflicts through a binary classification scheme ("protest" vs. "non-protest"). The event sentence detection task focuses on identifying and labeling sentences that refer to protest events (e.g. riots, social events).</p><p>Both of the tasks attempted here relate to text classification and sentence classification. Recent work in natural language processing (NLP) and text mining shows many applications that leverage text classification at different levels of scope. At the document level, many classification techniques have been proposed and have achieved good results in the literature <ref type="bibr" coords="2,382.09,130.95,9.96,8.74" target="#b3">[4]</ref>. Logistic regression (LR) and support vector machines (SVMs) are two of the most-used techniques <ref type="bibr" coords="2,134.77,154.86,9.96,8.74" target="#b1">[2]</ref>. Recently "deep learning" models based on neural networks have become increasingly popular <ref type="bibr" coords="2,228.60,166.81,9.96,8.74" target="#b2">[3]</ref>. At the sentence level, classification must operate on texts that are much shorter than most documents (≤ 160 words), which reduces performance of traditional text classification algorithms. Main limitations concern the feature sparsity of short text which reduces the accuracy of traditional algorithms, such as the similarity algorithm based on word frequency and co-occurred words <ref type="bibr" coords="2,236.77,226.59,9.96,8.74" target="#b0">[1]</ref>. To tackle problems arising from short texts, various methods have been proposed to improve their capacity of semantic expression <ref type="bibr" coords="2,134.77,250.50,9.96,8.74" target="#b4">[5]</ref>. More recently, NLP has drawn attention in this context through the use of language models learned by word embeddings, especially in models based on neural networks <ref type="bibr" coords="2,207.10,274.41,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="2,217.62,274.41,7.01,8.74" target="#b6">7]</ref>.</p><p>At both document and sentence levels, effective feature extraction is important to help the accuracy and robustness of classification models. Inspired by recent work in efficient word representation learning <ref type="bibr" coords="2,371.15,311.96,10.96,8.74" target="#b8">[9,</ref><ref type="bibr" coords="2,382.11,311.96,7.31,8.74" target="#b7">8]</ref> and considering the topical scope of the proposed tasks CLEF ProtestNews Track 2019, here we propose two models that were submitted for official evaluation and also several other models that were developed for experimental purposes. All of them are based on word vector learning combined with linear classifiers. The main aim of these approaches is to find the most efficient feature extraction and classification method which can be applied at different levels of scope.</p><p>The rest of this paper is organized as follows. Section 2 presents an overview of the analytical framework. In Section 3, we describe a set of 9 different models using different kinds of word embedding and classifier, with and without dimension reduction. Then, we present results from experimental testing of these models (Section 4) before giving results for the two models submitted to the official CLEF ProtestNews track evaluation (Section 5) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the proposed framework</head><p>In this section, we present the proposed framework consisting of three parts: data processing, word vector learning and text classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data processing</head><p>For each task respectively documents and sentences were converted to lowercase, all URLs and stop-words were removed. After the tokenization process, all tokens based only on non-alphanumeric characters and all short tokens (with &lt; 3 characters) were also deleted. Then, we perform a morphological analysis of all tokens in order to identify lemmas, replacing each token by its lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word vector models</head><p>In word vector representations, each word is represented by a vector which is concatenated or averaged with other word vectors in a context to form a resulting vector which is used to predict other words in the context <ref type="bibr" coords="3,432.72,162.75,14.61,8.74" target="#b9">[10]</ref>. These vectors allow capture of hidden information about a language, like word analogies or semantic associations. In the literature, word vector representations have demonstrated efficiency in boosting accuracy of classification models. However, inconsistent performances are observed in some application contexts <ref type="bibr" coords="3,431.45,210.58,14.61,8.74" target="#b10">[11]</ref>. In this paper, we explore three popular embedding models, namely, Word2Vec, GloVe and FastText. Below, we introduce briefly their principle.</p><p>-Word2Vec <ref type="bibr" coords="3,206.37,256.29,15.50,8.74" target="#b13">[14]</ref> is a group of related models based on two-layer neural networks that are trained to reconstruct linguistic contexts of words. Two model architectures can be used: continuous bag-of-words (CBOW) or continuous skip-gram (SG). In CBOW architecture, the model predicts the current word from a window of surrounding context words. As in other bag-of-words approaches, the order of context words does not influence prediction. In the continuous SG architecture, the model uses the current word to define the surrounding window of context words. The SG architecture weights nearby context words more heavily than more distant context words. -GloVe <ref type="bibr" coords="3,186.44,363.87,15.50,8.74" target="#b12">[13]</ref> (global vectors for word representation) allows the user to obtain word vector representations by mapping words into a meaningful space where the distance between words is related to semantic similarity. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. -FastText <ref type="bibr" coords="3,199.41,435.58,15.50,8.74" target="#b11">[12]</ref> is based on the SG model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations.</p><p>Pre-trained word embeddings on large training sets are publicly available, such as those produced for word2vec <ref type="bibr" coords="3,260.12,505.23,14.61,8.74" target="#b13">[14]</ref>, GloVe <ref type="bibr" coords="3,310.21,505.23,15.50,8.74" target="#b12">[13]</ref> or Wiki word vectors for FastText<ref type="foot" coords="3,473.35,503.65,3.97,6.12" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Linear classifiers</head><p>Despite the popularity of models based on neural networks, linear classifiers stand as strong baselines for text classification problems. Furthermore the stateof-art about these models has proved their suitability and their robustness when they are combined with right features <ref type="bibr" coords="3,304.38,590.76,14.61,8.74" target="#b14">[15]</ref>. In addition, neural network models tend in practice to increase computational cost. Following empirical studies conducted on the training set provided for each CLEF ProtestNews task, SVM and XGBoost provided best performances in term of accuracy and log loss scores.</p><p>Nine models were explored for the news article classification and event sentence detection tasks. These models were selected from various combinations within the framework presented above. The best models were chosen according to their global performance in terms of precision, recall and F1-score obtained on the training sets provided for each task. Parameter tuning was performed using GridSearchCV<ref type="foot" coords="4,197.99,204.04,3.97,6.12" target="#foot_2">2</ref> in order to select parameter values that maximize the accuracy of each model. Top parameters are presented in tables following the description of each model below. The model architectures described below were used in similar ways for the two tasks (except for the sum ner model, see below).</p><p>- -The sum ner model uses slightly different text processing according to the application context.</p><p>• For Task 1, the text was trimmed to capture sentences that are most representative of the source document. In this way, we aimed to gain topical clarity and reduce the vocabulary space. Similar to a text summarization process, each sentence was scored as the sum of the weighted frequencies of its words within the whole document. The highest-scoring sentences were then chosen to give a concise representation of the document. The best performances were observed by keeping the first 4 sentences with the highest scores. • For both Task 1 (using sentences derived from document level as above)</p><p>and Task 2 (which begins at sentence level), we then apply text normalization using a named entity recognition tool <ref type="foot" coords="5,362.88,532.56,3.97,6.12" target="#foot_5">5</ref> . Only entities referring to a person, a location or an organisation are identified. Each entity localized is replaced by the name of its class. The aim with this process is to provide harmonized vector patterns which can be beneficial in word representation processes. • For both tasks, the final step is to perform classification using the XG-Boost technique. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head><p>In this section, we present experimental results obtained on the test sets provided for intermediate evaluation in the CLEF ProtestNews track. The ProtestNews evaluation process was divided into two phases. The first phase (intermediate evaluation) was conducted on test sets extracted from Indian news articles. The second phase (final evaluation) includes English-language news articles from both India and China (see below).</p><p>The training sets used for experimental testing of the different models are taken from the final evaluation phase (3429 documents and 5884 sentences extracted from India news articles). The test sets are from the intermediate phase and are composed of 457 documents and 663 sentences respectively, extracted from India news articles. Evaluation measures used for each task are precision, recall, F1-score and the average of F1-scores obtained in these two tasks (Avg.2).  <ref type="table" coords="7,176.13,118.99,4.98,8.74" target="#tab_2">1</ref> gives results obtained for the model architectures presented in Section 3. At the document level, we observe that the best precision is obtained by the w2v glove model. For recall and F1-score, best performances are observed with the svm fast wiki model. At the sentence level, the best precision is given by svm fast wiki. xgboost fast wiki obtains the best results both on recall and F1score. The best average of F1-scores (Avg.2) is given by the svm fast wiki model.</p><p>It is interesting to note that models based on pre-trained word vectors (i.e. x wiki, w2v glove) obtain higher performances than those built directly from the sources provided in the ProtestNews data sets. This finding suggests that, in this context, enriched word vector representation using external data improves global performance of the classification models. The combination of pre-trained word vectors with FastText and a SVM classifier provided a model that was robust and suitable for both levels of scope, especially in terms of precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Official CLEF ProtestNews results</head><p>In this section, we present results obtained for the final evaluation phase of CLEF ProtestNews 2019. Proposed models were evaluated on Task 1 (news article classification) and Task 2 (event sentence detection) using two different testing sets. Task 3 was not attempted.</p><p>As in the experimental results above, the training sets provided were composed of 3429 documents (Task 1) and 5884 sentences (Task 2) extracted from English-language news articles from India. Table <ref type="table" coords="7,347.80,388.71,4.98,8.74" target="#tab_3">2</ref> gives details of the final evaluation test sets. Table <ref type="table" coords="7,178.76,532.26,4.98,8.74" target="#tab_4">3</ref> gives results for the final evaluation phase. Evaluation measures used are the F1-score for each task and the average of F1-scores obtained across both tasks (Avg. 2). The models presented for this phase are based on the xgboost fast SVD and xgboost fast wiki SVD model architectures introduced in Section 3. <ref type="foot" coords="7,189.01,578.50,3.97,6.12" target="#foot_6">6</ref> The best model performance is given for comparison.</p><p>The best model achieved an average F1-score across the two tasks of 0.652. As we can observe, the results obtained by our proposed models are less effective, with respectively an average F1-score over the two tasks of 0.163 and 0.193. We suspect that dimension reduction did not offer an advantage here. Usually used on a large set of features, SVD appears not to have helped extraction of suitable discriminant features for these classification tasks. With these settings, results are better on the Task 1 than Task 2. This may be explained by the difference in length of the text records in these two levels of scope. However, it is interesting to observe that the use of a pre-trained model improved results obtained in Task 1.</p><p>Comparing performance between experimental testing and the final Protest-News evaluation, we see a worse Avg. 2 score for xgboost fast SVD and slightly better Avg.2 score for xgboost fast wiki SVD, for the final evaluation relative to the experimental test on intermediate evaluation data. We note that in the intermediate phase, models are tested and trained on the same kinds of content (Indian news), whereas in the final phase models are trained on Indian content and tested on both Indian and China content. It appears that use of a pretrained model is less effective in the sentence level than in the document level when models are applied on the same kind of content. Conversely models trained on similar content are more suitable. We conclude that with these settings, features extracted are less generalisable, while those extracted from a pre-trained model give a slight decrease in performance but are more robust when confronted with another type of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we presented our contribution to the CLEF 2019 ProtestNews Track. Models evaluated combined word-embedding techniques (Word2Vec, GloVe and FastText) with linear classifiers (SVM and XGBoost), as well as dimension reduction as a pre-processing step (SVD). Models showed worse performance when combined with dimension reduction. Word embedding, which is often sensitive to the domain of application, provided best performance when word vectors were generated from pre-trained models, independent of the level of scope.</p><p>In future work, we plan to evaluate all models proposed during the experimental phase on the datasets used in the final evaluation phase of CLEF Protest-News. This will help explore the portability of these models to datasets extracted from an another country and estimate their ability to adapt to new domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,143.40,266.30,339.23,358.93"><head></head><label></label><figDesc>The xgboost fast model uses word vector representations created by Fast-Text. Vectors were built from the training set provided for each task. Then the XGBoost classifier was used to identify the class of each input. The svm fast wiki model uses classifiers based on SVM. Word vector representations are built from the same pre-trained model that was used for the xgboost fast wiki model. The xgboost glove model uses a pre-trained word vector embedding as initialization for the representation of words. This embedding named GloVe 4 is composed of 300-dimensional vectors trained over a larger vocabulary of web data (840B words). Then XGBoost classifiers were used to identify the class of each input.</figDesc><table coords="4,148.48,310.61,317.64,266.55"><row><cell></cell><cell></cell><cell>Data</cell><cell cols="3">C Gamma Kernel</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Document 1</cell><cell>1</cell><cell>rbf</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Sentence 1</cell><cell>1</cell><cell>rbf</cell><cell></cell></row><row><cell>-Data</cell><cell>fraction of</cell><cell cols="3">Gamma tree max.</cell><cell>min. sum of</cell><cell cols="2">alpha fraction of</cell></row><row><cell></cell><cell>columns</cell><cell></cell><cell cols="2">depth</cell><cell>weights</cell><cell></cell><cell>observations</cell></row><row><cell>Document</cell><cell>0.8</cell><cell>0.0</cell><cell>6</cell><cell></cell><cell>8</cell><cell>0.05</cell><cell>0.75</cell></row><row><cell>Sentence</cell><cell>0.75</cell><cell>0.3</cell><cell>6</cell><cell></cell><cell>6</cell><cell>0.05</cell><cell>0.8</cell></row><row><cell>Data</cell><cell>fraction of</cell><cell cols="3">Gamma tree max.</cell><cell>min. sum of</cell><cell cols="2">alpha fraction of</cell></row><row><cell></cell><cell>columns</cell><cell></cell><cell cols="2">depth</cell><cell>weights</cell><cell></cell><cell>observations</cell></row><row><cell>Document Sentence Data</cell><cell>0.75 0.75 fraction of columns</cell><cell cols="3">0.4 0.4 Gamma tree max. 5 6 depth</cell><cell>6 6 min. sum of weights</cell><cell cols="2">0.005 0.001 alpha fraction of 0.8 0.85 observations</cell></row><row><cell>Document</cell><cell>0.8</cell><cell>0.0</cell><cell>6</cell><cell></cell><cell>12</cell><cell>0.001</cell><cell>0.8</cell></row><row><cell>Sentence</cell><cell>0.85</cell><cell>0.3</cell><cell>6</cell><cell></cell><cell>12</cell><cell>0.001</cell><cell>0.85</cell></row><row><cell>Data</cell><cell>fraction of</cell><cell cols="3">Gamma tree max.</cell><cell>min. sum of</cell><cell cols="2">alpha fraction of</cell></row><row><cell></cell><cell>columns</cell><cell></cell><cell cols="2">depth</cell><cell>weights</cell><cell></cell><cell>observations</cell></row><row><cell>Document</cell><cell>0.85</cell><cell>0.1</cell><cell>6</cell><cell></cell><cell>10</cell><cell>0</cell><cell>0.8</cell></row><row><cell>Sentence</cell><cell>0.8</cell><cell>0.3</cell><cell>5</cell><cell></cell><cell>12</cell><cell>0.05</cell><cell>0.75</cell></row><row><cell></cell><cell></cell><cell>Data</cell><cell cols="3">C Gamma Kernel</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Document 10</cell><cell>1</cell><cell>rbf</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Sentence 0.001 0.001 linear</cell><cell></cell></row></table><note coords="4,143.40,366.55,339.23,8.77;4,151.70,378.54,328.89,8.74;4,151.70,390.49,228.39,8.74;4,380.09,388.92,3.97,6.12;4,387.92,390.49,92.67,8.74;4,151.70,402.45,328.89,8.74;4,151.70,414.40,328.88,8.74;4,151.70,426.36,274.80,8.74;4,143.40,502.67,337.19,8.77;4,151.70,514.66,328.89,8.74;4,151.70,526.61,136.54,8.74;4,143.40,592.58,3.32,8.74;5,143.40,295.13,337.19,8.77;5,151.70,307.12,328.89,8.74;5,151.70,319.07,297.45,8.74"><p><p><p><p><p>-The xgboost fast wiki model uses the same architecture as the xgboost fast model except for word vector learning, which is performed through the use of pre-trained word embeddings. The pretrained model 3 is composed of 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset. These vectors in dimension 300 were obtained using the skip-gram model described in</p><ref type="bibr" coords="4,300.47,426.36,15.50,8.74" target="#b11">[12]</ref> </p>with default parameters.</p>-The svm fast model uses word vectors built from FastText. Vectors were designed from the training sets provided. Then SVM classifiers were used to identify the class of each input.</p>--The xgboost w2v model is designed from a word vector representations performed by Word2vec. Vectors were built from the training set provided for each task. Then XGBoost classifiers were used to classify inputs.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,143.40,117.78,337.20,164.46"><head></head><label></label><figDesc>-The xgboost fast SVD and xgboost fast wiki SVD models were created from the xgboost fast and xgboost fast wiki models above by the addition of dimension reduction alongside feature extraction. Dimension reduction was applied using the Singular Value Decomposition (SVD) method, a commonly applied technique, in order to reduce noise and increase model stability. Briefly, SVD is a matrix decomposition method for reducing a matrix to its constituent parts, to make certain subsequent matrix calculations simpler.</figDesc><table coords="6,148.48,117.78,317.64,41.14"><row><cell>Data</cell><cell>fraction of</cell><cell cols="2">Gamma tree max.</cell><cell>min. sum of</cell><cell cols="2">alpha fraction of</cell></row><row><cell></cell><cell>columns</cell><cell></cell><cell>depth</cell><cell>weights</cell><cell></cell><cell>observations</cell></row><row><cell>Document</cell><cell>0.85</cell><cell>0.1</cell><cell>4</cell><cell>12</cell><cell>0.01</cell><cell>0.85</cell></row><row><cell>Sentence</cell><cell>0.85</cell><cell>0.4</cell><cell>6</cell><cell>8</cell><cell>0.001</cell><cell>0.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,513.11,345.82,147.86"><head>Table 1 .</head><label>1</label><figDesc>Experimental results using intermediate evaluation data from CLEF 2019 ProtestNews Track.</figDesc><table coords="6,146.63,543.12,322.11,117.85"><row><cell>Model</cell><cell cols="2">Document precision recall F1-score precision recall F1-score Sentence</cell><cell>Avg. 2</cell></row><row><cell>xgboost fast SVD</cell><cell>0.533 0.392 0.452</cell><cell cols="2">0.344 0.072 0.120 0.256</cell></row><row><cell cols="2">xgboost fast wiki SVD 0.315 0.225 0.263</cell><cell>0.188 0.065 0.097</cell><cell>0.18</cell></row><row><cell>xgboost fast</cell><cell>0.773 0.568 0.655</cell><cell cols="2">0.152 0.021 0.037 0.346</cell></row><row><cell>xgboost fast wiki</cell><cell>0.822 0.637 0.718</cell><cell cols="2">0.750 0.391 0.514 0.616</cell></row><row><cell>svm fast</cell><cell>0.794 0.529 0.635</cell><cell cols="2">0.176 0.108 0.058 0.346</cell></row><row><cell>svm fast wiki</cell><cell>0.829 0.716 0.768</cell><cell cols="2">0.833 0.326 0.468 0.618</cell></row><row><cell>w2v glove</cell><cell>0.857 0.588 0.698</cell><cell cols="2">0.761 0.370 0.498 0.598</cell></row><row><cell>w2v xgboost</cell><cell>0.798 0.618 0.696</cell><cell cols="2">0.333 0.007 0.014 0.355</cell></row><row><cell>sum ner</cell><cell>0.535 0.147 0.230</cell><cell cols="2">0.461 0.043 0.079 0.155</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,207.64,429.90,200.06,72.90"><head>Table 2 .</head><label>2</label><figDesc>Description of final evaluation test sets.</figDesc><table coords="7,223.46,450.70,168.44,52.10"><row><cell>Test set</cell><cell cols="2">number of records source</cell></row><row><cell>task1 test</cell><cell>687</cell><cell>India</cell></row><row><cell>china test task1</cell><cell>1801</cell><cell>China</cell></row><row><cell>task2 test</cell><cell>1107</cell><cell>India</cell></row><row><cell>china test task2</cell><cell>1235</cell><cell>China</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,142.83,115.91,329.69,64.73"><head>Table 3 .</head><label>3</label><figDesc>Official results -Final evaluation phase CLEF 2019 ProtestNews Track.</figDesc><table coords="8,216.99,136.71,181.38,43.93"><row><cell>Model</cell><cell>Task 1 Task 2 Avg. 2</cell></row><row><cell>Best run 2019</cell><cell>0.746 0.558 0.652</cell></row><row><cell>xgboost fast SVD</cell><cell>0.232 0.094 0.163</cell></row><row><cell cols="2">xgboost fast wiki SVD 0.294 0.092 0.193</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,645.84,335.86,8.12;3,144.73,656.80,17.66,7.86"><p>https://fasttext.cc/docs/en/pretrained-vectors.html Date of access: 16th May</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2019" xml:id="foot_1" coords=""><p></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="4,144.73,623.92,335.87,8.12;4,144.73,634.88,62.72,7.86"><p>https://scikit-learn.org/stable/modules/grid_search.html Date of access: 16th May 2019.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="4,144.73,646.48,264.11,7.47;4,144.73,656.80,247.66,8.12"><p>https://dl.fbaipublicfiles.com/fasttext/vectors-english/ wiki-news-300d-1M.vec.zip Date of access: 16th May 2019.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4" coords="5,144.73,623.92,335.86,8.12;5,144.73,634.88,20.99,7.86"><p>http://nlp.stanford.edu/data/glove.840B.300d.zip Date of access: 17th June 2019.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5" coords="5,144.73,645.84,335.87,8.12;5,144.73,656.80,20.99,7.86"><p>https://nlp.stanford.edu/software/CRF-NER.shtml Date of access: 17th June 2019.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6" coords="7,144.73,634.88,335.86,7.86;7,144.73,645.84,335.87,7.86;7,144.73,656.80,84.81,7.86"><p>Due to problems with the ProtestNews submission system, only these two models were entered into the final evaluation, despite their relative poor performance in experimental testing.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,645.84,342.24,7.86;8,146.91,656.80,290.75,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,303.44,645.84,157.92,7.86">A survey on short text analysis in web</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">C</forename><surname>Rafeeque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sendhilkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,146.91,656.80,226.98,7.86">Third International Conference on Advanced Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,119.67,342.25,7.86;9,146.91,130.63,333.68,7.86;9,146.91,141.59,25.60,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,161.54,130.63,164.12,7.86">Text Classification Algorithms: A Survey</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Meimandi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heidarysafa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mendu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08067</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,138.35,152.55,342.24,7.86;9,146.91,163.51,333.68,7.86;9,146.91,174.47,28.16,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,249.32,152.55,227.31,7.86">Analysis methods in neural language processing: A survey</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,146.91,163.51,254.33,7.86">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="49" to="72" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,185.43,342.24,7.86;9,146.91,196.39,333.68,7.86;9,146.91,207.34,211.84,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,197.41,196.39,283.19,7.86;9,146.91,207.34,40.54,7.86">A brief survey of text mining: Classification, clustering and extraction techniques</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Allahyari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pouriyeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Assefi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Safaei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Trippe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kochut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02919</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,138.35,218.30,342.24,7.86;9,146.91,229.26,223.11,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,341.79,218.30,134.84,7.86">Short text classification: A survey</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,160.99,229.26,87.44,7.86">Journal of multimedia</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">635</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,240.22,342.24,7.86;9,146.91,251.18,333.68,7.86;9,146.91,262.14,333.68,7.86;9,146.91,273.10,333.68,7.86;9,146.91,284.06,28.16,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,443.98,240.22,36.62,7.86;9,146.91,251.18,298.19,7.86">Semantic clustering and convolutional neural network for short text categorization</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,467.79,251.18,12.80,7.86;9,146.91,262.14,333.68,7.86;9,146.91,273.10,251.12,7.86">the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="352" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,295.02,342.25,7.86;9,146.91,305.98,333.68,7.86;9,146.91,316.93,123.39,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,356.37,295.02,124.23,7.86;9,146.91,305.98,135.20,7.86">Recent trends in deep learning based natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,302.56,305.98,173.60,7.86">IEEE Computational intelligenCe magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="55" to="75" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,327.89,342.25,7.86;9,146.91,338.85,234.67,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,362.84,327.89,117.76,7.86;9,146.91,338.85,49.79,7.86">Bag of tricks for efficient text classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,138.35,349.81,342.24,7.86;9,146.91,360.77,333.68,7.86;9,146.91,371.73,184.54,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,292.64,349.81,187.95,7.86;9,146.91,360.77,123.76,7.86">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,293.47,360.77,187.12,7.86;9,146.91,371.73,68.09,7.86">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,382.69,337.98,7.86;9,146.91,393.65,333.67,7.86;9,146.91,404.61,333.67,7.86;9,146.91,415.56,47.62,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,306.39,382.69,174.20,7.86;9,146.91,393.65,143.72,7.86">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,310.92,393.65,169.67,7.86;9,146.91,404.61,114.81,7.86">the 48th annual meeting of the association for computational linguistics</title>
		<title level="s" coord="9,268.72,404.61,169.93,7.86">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,426.52,337.98,7.86;9,146.91,437.48,333.68,7.86;9,146.91,448.44,131.86,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,366.26,426.52,114.33,7.86;9,146.91,437.48,66.19,7.86">Word embedding evaluation and combination</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Esteve</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Camelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,235.87,437.48,244.73,7.86;9,146.91,448.44,42.62,7.86">the 10th edition of the Language Resources and Evaluation Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="300" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,459.40,337.98,7.86;9,146.91,470.36,333.67,7.86;9,146.91,481.32,144.99,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,367.33,459.40,113.27,7.86;9,146.91,470.36,82.49,7.86">Enriching word vectors with subword information</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,252.19,470.36,228.40,7.86;9,146.91,481.32,28.76,7.86">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,492.28,337.98,7.86;9,146.91,503.24,333.68,7.86;9,146.91,514.19,137.58,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,322.65,492.28,157.95,7.86;9,146.91,503.24,22.82,7.86">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,189.78,503.24,290.82,7.86;9,146.91,514.19,37.96,7.86">the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,525.15,337.98,7.86;9,146.91,536.11,290.32,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="9,349.90,525.15,130.70,7.86;9,146.91,536.11,109.59,7.86">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.62,547.07,337.98,7.86;9,146.91,558.03,333.68,7.86;9,146.91,568.99,321.72,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,272.80,547.07,207.79,7.86;9,146.91,558.03,72.22,7.86">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,238.61,558.03,241.99,7.86;9,146.91,568.99,39.05,7.86">the 50th annual meeting of the association for computational linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
