<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.92,153.03,303.59,12.22">ELMo Word Representations For News Protection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,247.95,192.05,96.38,8.85"><forename type="first">Elizaveta</forename><surname>Maslennikova</surname></persName>
							<email>maks_lizok@mail.ru</email>
							<affiliation key="aff0">
								<orgName type="institution">National Research University Higher School of Economics (HSE) N. Novgorod</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.92,153.03,303.59,12.22">ELMo Word Representations For News Protection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">195273B15392E936E1FF3646B6D96D54</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ELMo</term>
					<term>BiLM</term>
					<term>Text Classification</term>
					<term>NLP</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Within framework of the research for this article a new state-of-the-art ELMO model of the words representation to improve the quality of classical models for solving the problem of binary classification in different interpretations is considered. The article contains a description of various methods applied to the processing of source texts for their transformation into the format necessary for many models, a description of their advantages and disadvantages, principles for constructing and operating the context-dependent representation of ELMo with a detailed description of the algorithm for using it within the target model. For a competent assessment of the results obtained, all experiments are carried out using a real dataset including news articles from various sources in China and India. Comparative analysis includes consideration of the results of adding an ELMo model to standard target models of solving a problem in comparison with using Word2Vec. A comparison is also made for different problem statements -the classification of whole texts, individual sentences and the finding of specific passages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>At 1 the present, a person is surrounded by a large number of external sources of information. The newspapers, magazines, radio, TV, even unwittingly heard or seen news can sit in person's head, then reborn into some kind of idea or even a change of own principles. Now, in the sphere of high technologies, the Internet is becoming increasingly popular as a way to communicate, learn, search for information and as a guide to the world of the latest news. At the same time, the world wide web is growing at an incredible speed, the number of sites on various subjects is becoming more and more, news portals fully replace the news program on any channels, and social net-works multiply the number of users every day. But, unfortunately, such a speed of distribution creates great difficulties with the problem of controlling all published content, especially since such a large coverage of the World Wide Web contributes to increased interest from people or their groups who are trying to spread protest ideas, irrelevant or even prohibited content. Protest news also includes appeals to organize or participate in various events that are in the scope of contentious politics and characterized by riots and social movements, i.e. the "repertoire of contention" <ref type="bibr" coords="2,356.65,174.53,51.98,8.85">(Giugni 1998</ref><ref type="bibr" coords="2,408.63,174.53,57.64,8.85">, Tarrow 1994</ref><ref type="bibr" coords="2,466.27,174.53,4.51,8.85;2,124.80,186.53,40.42,8.85">, Tilly 1984</ref>). Therefore, the task of creating a mechanism capable of controlling the published content and identifying protest content, event and information connected with them is a very important and an urgent problem today. Thus, this article is devoted to the study of algorithms for solving the actual problem of binary classification by the presence of some kind of protest ideas in it. Currently, machine learning algorithms are gaining much popularity in the study of classification problems. At the same time, the most important problem and interesting part of the development is the creation of a model capable of translating a complex human language into a machine-friendly form. language is a very complex structure: letters combined in a different order constitute completely different words, and sometimes the same word has several meanings; words, combining sentences in a different order, can give a completely different emotional coloring, and sometimes a different meaning; etc. At the moment, researchers have proposed a large variety of different models for representing words in an "understandable" form for a computer. But mostly all of them are based on the coding of either the letters that make up the word, or the coding of the words themselves, without taking into account their lexical meaning and the whole context surrounding it. But, unfortunately, in the present conditions, when models for many tasks cannot be trained on a sufficient number of texts, and their writing style contains a very complex structure, these models are not sufficiently accurate and workable. Therefore, in recent years, researchers have been quite actively developing models for the representation of words that combine both syntax, semantics, and lexical meaning of individual tokens. One of such models is ELMo (Embeddings from Language Models) -the representation of words as a vector of features obtained from a neural network pre-trained on a huge text package using LSTM (Long short-term memory) layers. This model was developed and presented by researchers from Washington in 2018, and produced a great resonance, as its effectiveness was proven when applied to some well-known machine learning tasks (Named Entity Recognition, Questions answering on the text, etc.) -the quality of the best models found showed an even higher result when using ELMo as models for the representation of texts for further research than those models that were previously the most effective in their own right. At the same time, it was precisely the task of classifying texts that was not part of their research, although it is expected that the described representation will be able to improve the quality of models for classifying texts, especially in conditions of a small amount of training data set.</p><p>In this paper, the possibility of using the ELMo model for the task of recognizing protest ideas in texts is observed. The structure of the creation of the ELMo model, studying of other models of the words representations that were previously used for such tasks; carrying out their comparative analysis and drawing conclusions regarding the possibility of improving the quality of standard algorithms using ELMo is also presented in this work. It is as well necessary to consider the machine learning algorithms applied earlier to the problems of classification, implement them using the characteristics from ELMo and carry out a comparative analysis with the results obtained without it using on real data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>As mentioned earlier, the main problem in building a model for word processing is the choice of a method for converting text into a "understandable" form for a computer. For these purposes people often use various kinds of embedding. Embedding is a process of matching a certain object (text, word, picture, etc.) with a certain vector of numbers. Correspondingly, the source texts, encoded by matching the words to a point in ndimensional space, take the form of a computer model that can be processed. One of the most simple and widely used approaches for encoding words with a vector of numbers is the Bag-of-Words method <ref type="bibr" coords="3,307.75,315.65,10.64,8.85" target="#b0">[1]</ref>, the main idea of which is to form a dictionary of all the words of the source text, organize it, and then convert the texts in the vector of numbers, where the i-th element is equal to the number of occurrences of the i-th word from the dictionary in the given text. This approach gained its fame due to its simplicity and sufficiently large efficiency for processing a small number of texts. With the development of technology and the emergence of the Internet in human life, the number of processed texts and their complexity is growing every day, making models like the BoW inapplicable. Then, in 2013, the Word2Vec model was proposed, which not only is capable of working with a large volume of texts and a huge dictionary, but which only "wins" with the growth of information <ref type="bibr" coords="3,347.94,423.65,10.63,8.85" target="#b1">[2]</ref>. This approach is based on the locality hypothesis -words that occur in the text next to identical words will have the close coordinates of words at the output, i.e. it is assumed that only words that are combined with each other can stand next to them. However, these approaches allow only one context-independent representation for each source word. A little later, several more models were proposed that try to circumvent this drawback by examining individual vectors for each word value <ref type="bibr" coords="3,271.26,495.65,11.71,8.85" target="#b2">[3]</ref> or by enriching the original word with information on its subwords (using a letter-by-word representation) <ref type="bibr" coords="3,377.55,507.65,10.61,8.85" target="#b3">[4]</ref>.</p><p>Another quite popular word representation model is Context2Vec <ref type="bibr" coords="3,402.47,519.65,10.61,8.85" target="#b4">[5]</ref>, which uses a bidirectional network of long short-term memory <ref type="bibr" coords="3,320.55,531.65,11.69,8.85" target="#b5">[6]</ref> to encode a context around a word. Another approach to the study of contextual embedding includes the keyword itself in the presentation and is calculated, for example, using controlled neural machine translation <ref type="bibr" coords="3,149.99,567.65,10.64,8.85" target="#b6">[7]</ref>. Like the previous approaches, these models only "win" from a large amount of input data.</p><p>Previous studies on the topic of machine learning words processing also showed that different layers of deep bidirectional recurrent neural networks are capable of encoding various types of information. For example, the introduction of multitasking syntactic control (using the tags of parts of speech) at lower levels of LSTM can improve the overall performance of higher-level tasks <ref type="bibr" coords="3,293.45,639.65,10.64,8.85" target="#b7">[8]</ref>. Long Short-Term Memory -is the kind of recurrent networks which is capable of learning long-term dependencies, while mem-orizing and transmitting some information for long periods of time is their usual property, and not something that they hardly try to learn as with the simple architecture of recurrent neural network.</p><p>In the machine translation system, also based on recurrent networks, it was shown that the representations obtained at the first level in the two-layer LSTM predict tags of parts of speech better than at the second level <ref type="bibr" coords="4,306.19,210.53,10.63,8.85" target="#b8">[9]</ref>, and, finally, the upper level of LSTM for encoding is studying the meaning of the word <ref type="bibr" coords="4,324.58,222.53,10.56,8.85" target="#b4">[5]</ref>.</p><p>The described below ELMo model of the word's representation have all the advantages of considered models and in which their shortcomings will be also taken into account <ref type="bibr" coords="4,158.40,258.53,15.34,8.85" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Embeddings from Language Models</head><p>The model of the word's representation studied in this article differs from the traditional ones in that each token is assigned a representation, which is a function depending on the entire input sentence. In this case, I use vectors derived from a bi-directional network of long short-term memory, which is taught in advance on a large text package as a model representing the entire language used. Therefore, this approach received such a name: Embeddings from Language Models. Let we have a sequence of N tokens (𝑡 # , 𝑡 % , … , 𝑡 ' ). The forward language model (LM) calculates the probability of a given sequence, simulating the probability of the appearance of the token 𝑡 ) taking into account the history (𝑡 # , 𝑡 % , … , 𝑡 )*# ):</p><formula xml:id="formula_0" coords="4,213.58,433.84,257.15,12.67">𝒑(𝑡 # , 𝑡 % , … , 𝑡 ' ) = ∏ 𝒑(𝑡 ) |𝑡 # , 𝑡 % , … , 𝑡 )*# ) 𝑵 𝒌1𝟏<label>(1)</label></formula><p>Recent most popular language models compute a context-independent representation of tokens 𝑥 ) 45 (using, for example, convolutional neural networks above the letters that make up the original text), and then pass them through L layers of the forwarddirected LSTM. It turns out that for each position k, each layer of the LSTM network outputs context-dependent representation The backward language model is similar to the forward language model (1), with the only exception that the passage through the sequence is carried out in the reverse order, predicting the previous token, taking into account the subsequent context:</p><formula xml:id="formula_1" coords="4,197.42,602.56,273.31,12.67">𝒑(𝑡 # , 𝑡 % , … , 𝑡 ' ) = ∏ 𝒑(𝑡 ) |𝑡 );# , 𝑡 );% , … , 𝑡 ' ) 𝑵 𝒌1𝟏<label>(2)</label></formula><p>The implementation occurs by analogy with the forward language model, where on each reverse LSTM layer j= 1, ..., 𝐿 give a context-sensitive representation ℎ ⃖? ),8</p><p>Fig. <ref type="figure" coords="6,176.54,325.40,3.39,8.01">1</ref>. Architecture of the bi-directional language model using in the ELMo.</p><p>First of all, you need to consider the lowest layer of the original model. For most tasks in NLP, the source data has a similar structure, and, accordingly, similar data processing and architecture on the very first layer of the model, which makes the algorithm for adding the ELMo model quite universal. The standard way of processing the original sequence of tokens 𝑡 # , … , 𝑡 ' is applying to it some algorithm for generating a contextindependent representation of the word 𝑥 ) (possibly using a previously trained network or based on a symbolic representation). Then the data is transferred further. For adding ELMo to the final model, it is necessary to pass the initial representation of the token 𝑥 ) through the pre-trained biLM, and then send the received ELMo vector for training to the next layers of the original model. For some tasks using the V𝑥 ) , 𝐸𝐿𝑀𝑜 ) ]^M) W as the final representation will be more efficient, but for the problem studied in this article, with a very small initial training dataset, this representation of words greatly complicates the model, but not gives a higher result because the data is not enough for full-fledged training.</p><p>As previously described, I use a pre-trained on a huge corpus of text data (1 Billion Word Language Model Benchmark) network as a biLM in this work.</p><p>The final model for pre-trained biLM is a recurrent neural network with two biLSTM layers with 4096 and 512 dimensions respectively, and a residual connection between the first and second layers (i.e., after two layers, a representation is made up as the sum of from the 1st and from the 2nd layer). At the same time, the model is built in such a way as you can use not only the final ELMo output, but also get separately outputs from each layer of the network or immediately get a general representation for some set of tokens (using the convolutional mean-pooling layer). After this training and such setup, we have finished language model, which can be incorporated into the final model for most tasks from NLP to improving the most effective (State-of-the-art) methods. It should be noticed, that for some tasks, the ELMo model, even without adjusting the weights for the outputs from each layer, provides an increase in quality, especially if it is close to a set of training data for the final task for teaching a language model. In the scope of this article, I explore the application and effectiveness of the ELMo model with the SVM classifier, fully connected, convolutional and recurrent neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Experimental dataset is a set of texts taken from various English-language news sources from India and China. For different levels of complexity, the data is presented in a different format:</p><p>• Task 1 -the classical formulation of the original problem, in which, according to a set of news articles, it is necessary to determine protest event related news articles as a whole or any other news article;</p><p>• Task 2 is a more difficult task of binary classification, where a whole set of texts is also presented, but each of their sentences should being considered separately in the context of having a protest event trigger or a mention of it;</p><p>• Task 3 -this problem formulation is similar to the task of Named Entity Recognition. The main goal is to extract the event information that targets protest event.</p><p>Data package for all tasks is represented a set of text objects with an assigned class for each. The difference lies only in the object itself -this is either the whole text, its separate sentences or separate words. It should also be noted that only data from Indian resources are submitted for training, and data from both countries are presented for testing. This is necessary for a higher test of the possibility of generalizing the models, since it is assumed that the test data from India has the same distribution as the training data, since the distribution of the data from China should slightly differ <ref type="bibr" coords="7,412.24,677.09,15.34,8.85" target="#b10">[11]</ref>.</p><p>As it was noted earlier, for correctly testing of ELMo model performance, it is necessary to compare the result of the work of the target model for solving this problem using the ELMo word representation and without it (using, for instance, the Word2Vec model). At the same time, it is necessary to choose the final model correctly, which would be suitable within the context of the task and would give good results even without using ELMo. Therefore, in the framework of the experiments, several classifiers were used to solve this problem with the selection of all the necessary parameters. The final used architecture of fully connected network contains 4 Dense layers for Task 1 and 6 these layers for Task 2 and 3, the best architecture of convolution network in the scope of these task is a combination of Convolution and MaxPooling layers repeated 3 times, GlobalAveragePooling, Dropout and Dense layers and the final used recurrent network consists of 3 biLSTM layers. These architectures were chosen as the best of all reviewed after a series of experiments. All necessary parameters as the number of neurons on hidden layers, units in LSTM were selected using the grid search method for each task individually. As can be seen from obtained results (see Table <ref type="table" coords="8,325.62,457.73,3.62,8.85" target="#tab_0">1</ref>), SVM is a fairly good model for binary classifying whole texts, applying it to test data from the same distribution. But with respect to data from the second test set, SVM gives worse results than neural networks, which shows a poor generalizing ability of this model. Therefore, the most successful model for solving this problem in a general sense is the fully connected multilayer fully connected, which shows the best result for the Chinese data set and high result for the Indian. It should be noted right away that the SVM classifier is not suitable for other tasks, since it does not take into account the previous context in any way but treats each object as an independent unit. This is also confirmed by the rather low result of its application to Task 2. Moreover, for example, the recurrent network is not effective for Task 1, since, on the contrary, it tries to use the previous context, while each object is a separate unit under consideration in this task. Convolutional and recurrent neural networks showed good results, while the generalizing ability is better for the first one for task 2. As for Task 3, there, as it was expected, the recurrent network gives the best quality, since individual words do not represent the most meaningless context, especially if parts of information of protest news can include several words in a row.</p><p>As it was already mentioned, the data presented in the Table <ref type="table" coords="8,377.27,649.73,5.04,8.85" target="#tab_0">1</ref> represent the result of applying various target classifiers using ELMo model. At the same time, this model is built into neural networks with the ability to configure weights for output from its different layers, but SVM should get representation obtained after passing through ELMo with fixed weights installed during model formation.</p><p>To prove the effectiveness of the using ELMo model the Fig. <ref type="figure" coords="9,385.00,186.53,5.04,8.85" target="#fig_2">3</ref> provide a comparative analysis of the use of various application models with the Word2Vec and ELMo presentation. It is clearly seen that the ELMo model gives an increase in the quality of recognition for all models (from 1% to 10% of F1 score), this is especially clearly seen in Tasks 2 and Task 3. From the obtained result, we can say with confidence that the ELMo model is really able to improve the results of the classical models for solving the problem of binary classification. That is why, after its invention, this model received the status of the-state-of-the-art very quickly. Its effectiveness is also confirmed by the fact that the solutions presented in the framework of this article at the CLEF Protest-News 2019 competition took high prizes <ref type="bibr" coords="9,289.85,294.53,15.29,8.85" target="#b11">[12]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This article examined the possibility of using the ELMo word representation model to improve the quality of prediction of classical models for the problems of binary classifying according to the presence of protest ideas or information about it in them. Within the framework of the research, the predecessors of ELMo, the principles of its construction and operation, as well as the possibility of its introduction into classical models of problem solving in NLP were considered. After the experiments and analysis of obtained results, it is safe to say that ELMo really improves the quality of many models for solving text analysis problems in comparison with the use of other word representation algorithms in a model-friendly way. The ELMo method really deserves the title of state of the art at present. installed during model formation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,294.23,515.68,4.33,6.19;4,292.32,507.84,8.45,8.96;4,300.68,517.36,9.14,6.19;4,300.68,506.32,9.95,6.19;4,311.25,507.84,159.51,8.96;4,124.80,526.13,108.92,8.85;4,238.25,533.92,4.33,6.19;4,236.40,526.08,8.45,8.96;4,244.71,535.60,9.68,6.19;4,244.71,524.56,9.95,6.19;4,257.93,526.08,134.69,8.96;4,392.44,530.32,13.56,6.19;4,408.64,526.13,62.10,8.85;4,124.80,544.37,117.49,8.85"><head>6 → ), 4 45</head><label>64</label><figDesc>𝑗 = 1, ..., 𝐿 and the output of the upper layer of this network is used to predict the next token 𝑡 );# (calculating the Softmax activation function).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,134.05,669.56,327.44,8.01;6,124.70,441.40,345.90,218.60"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of the formation of the final presentation for the word in two-layer ELMo.</figDesc><graphic coords="6,124.70,441.40,345.90,218.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,125.77,507.32,343.98,8.01;9,138.42,518.36,318.65,8.01;9,124.70,303.40,345.90,194.75"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparative analysis of using SVM, Fully Connected, Convolution and Recurrent Neural Networks for binary classification with ELMo and Word2Vec words representations.</figDesc><graphic coords="9,124.70,303.40,345.90,194.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,126.61,342.44,343.48,97.50"><head>Table 1 .</head><label>1</label><figDesc>The results of applying different classifiers with ELMo for described tasks.</figDesc><table coords="8,126.61,361.01,343.48,78.93"><row><cell></cell><cell cols="2">Task 1</cell><cell cols="2">Task 2</cell><cell cols="2">Task 3</cell></row><row><cell>Classifiers</cell><cell>India</cell><cell>China</cell><cell>India</cell><cell>China</cell><cell>India</cell><cell>China</cell></row><row><cell>SVM + ELMO</cell><cell>79.13</cell><cell>57.30</cell><cell>61.17</cell><cell>55.42</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">FullyCon.NN + ELMo 79.37</cell><cell>60.00</cell><cell>64.93</cell><cell>59.11</cell><cell>33.46</cell><cell>21.72</cell></row><row><cell>Convol.NN + ELMo</cell><cell>79.07</cell><cell>59.84</cell><cell>65.39</cell><cell>64.86</cell><cell>35.78</cell><cell>25.16</cell></row><row><cell>Recur.NN + ELMo</cell><cell>73.12</cell><cell>54.67</cell><cell>65.54</cell><cell>63.92</cell><cell>52.40</cell><cell>42.50</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>As a future work I plan to explore effectiveness of ELMo model with different methods of its training previously, using different datasets. Moreover, the study of the possibility of combining different pre-trained language models is also included in the scope of my further research.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>for token 𝑡 ) , taking into account the following sequences (𝑡 );# , 𝑡 );% , … , 𝑡 ' ).</p><p>Accordingly, biLSTM combines these approaches for both forward <ref type="bibr" coords="5,399.92,150.53,11.69,8.85" target="#b0">(1)</ref> and backward (2) language models. In this presentation, the logarithmic probability will be jointly maximized taking into account both directions:</p><p>where Θ I -the representation of the token, Θ M -the result of applying the SoftMax layer, Θ ? ?⃗ 4KL5 and Θ ⃖? ? 4KL5 are the outputs after the LSTM layer, taking into account the previous and subsequent context respectively. The schematic representation of the architecture of the bi-directional language model using in ELMo is presented in Fig. <ref type="figure" coords="5,410.54,262.61,3.89,8.85">1</ref>. Embeddings from Language Models is a context-dependent specific to the task model of representing words, which is a combination of representations from the intermediate layers of the biLSTM network. For each token 𝑡 ) a network of depth L gives a total of 2L + 1 different representations (outputs from all layers):</p><p>where ℎ ),U 45 -input layer, ℎ ),8 45 = Vℎ ?⃗ ),8 45 , ℎ ⃖? ),8 45 W -representation from the j-th layer of biLSTM taking into account both directions, j = 1, ..., L. The overall final ELMo representation, which is "embedded" in the main model to solve the NLP problem, is a convolution of outputs from all layers or all vectors from R into one vector: 𝐸𝐿𝑀𝑜 ) = 𝐸(𝑅 ) , Θ [ ). In the simplest case, you can use the output from the entire network (the presentation from the last layer) 𝐸(𝑅 ) ) = ℎ ),4 45 , but then the representation of the words will be based only on the language model that we assume not quite accurate, and will not depend on the current problem being solved and it's training dataset. If we approach the problem more globally, then we can calculate the final presentation as some combination of outputs from each layer with the corresponding weights, which just will be selected in the process of learning the final model:</p><p>(5)</p><p>where the parameters 𝑠 ]^M) (normalized weights vector) and 𝛾 ]^M) (scalar parameter which is necessary to assist the optimization process) allow the entire ELMo vector to be scaled within a specific task (see Fig. <ref type="figure" coords="5,285.27,559.97,3.81,8.85">2</ref>).</p><p>Having a pre-trained biLM (Bidirectional Language Model) with the architecture described above and a certain algorithm for solving the target problem of NLP it is sufficiently easy to integrate the ELMo model into the existing final solution to improve it. For this, it is enough to run the biLSTM network on the available source data and save all views from each layer of this network, and "train", or rather, select the necessary weights for these models while training process using the algorithm described below.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,127.09,260.36,251.04,8.01" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,175.64,260.36,83.99,8.01">Distributional Structure</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,265.38,260.36,20.55,8.01">Word</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.09,271.40,343.67,8.01;10,136.15,282.44,334.55,8.01;10,136.15,293.48,26.27,8.01" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,293.95,271.40,176.81,8.01;10,136.15,282.44,47.04,8.01">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Greg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,202.60,282.44,268.10,8.01">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.09,304.52,343.70,8.01;10,136.15,315.32,251.97,8.01" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,342.07,304.52,128.71,8.01;10,136.15,315.32,174.77,8.01">Efficient non-parametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,329.10,315.32,30.57,8.01">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.09,326.36,343.63,8.01;10,136.15,337.40,153.73,8.01" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,310.46,326.36,160.26,8.01;10,136.15,337.40,76.26,8.01">Charagram: Embedding words and sentences via character n-grams</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,230.85,337.40,30.57,8.01">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.09,348.44,343.62,8.01;10,136.15,359.48,148.46,8.01" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<title level="m" coord="10,290.06,348.44,180.65,8.01;10,136.15,359.48,122.42,8.01">CoNLL,&quot; in context2vec: Learning generic context embedding with bidirectional lstm</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.09,370.52,343.67,8.01;10,136.15,381.32,26.27,8.01" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,254.82,370.52,91.48,8.01">Long Short-term Memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,353.52,370.52,74.03,8.01">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="35" to="80" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.09,392.36,343.67,8.01;10,136.15,403.40,111.51,8.01" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,323.30,392.36,147.47,8.01;10,136.15,403.40,45.49,8.01">Learned in translation: Con-textualized word vectors</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,199.63,403.40,19.57,8.01">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.09,414.44,343.65,8.01;10,136.15,425.48,208.99,8.01" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,335.00,414.44,135.74,8.01;10,136.15,425.48,132.12,8.01">A joint many-task model: Growing a neural network for multiple nlp tasks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,286.12,425.48,30.57,8.01">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.09,436.52,343.71,8.01;10,136.15,447.32,179.74,8.01" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,343.50,436.52,127.30,8.01;10,136.15,447.32,118.79,8.01">What do neural machine translation models learn about morphology?</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,269.37,447.32,18.08,8.01">ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,126.72,458.36,344.03,8.01;10,136.15,469.40,206.22,8.01" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,451.71,458.36,19.04,8.01;10,136.15,469.40,128.68,8.01">Deep contextualized word representations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,282.85,469.40,31.07,8.01">NAACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,126.72,480.44,344.01,8.01;10,136.15,491.48,41.06,8.01" xml:id="b10">
	<monogr>
		<ptr target="https://emw.ku.edu.tr/clef-protestnews-2019/" />
		<title level="m" coord="10,136.15,480.44,90.46,8.01">CLEF PROTESTNEWS</title>
		<imprint>
			<date type="published" when="2019-06-01">2019. 2019/06/01</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,126.72,502.52,343.95,8.01;10,136.15,513.32,89.55,8.01" xml:id="b11">
	<monogr>
		<ptr target="https://competitions.codalab.org/competitions/22349#results" />
		<title level="m" coord="10,136.15,502.52,107.23,8.01">CLEF 2019 Lab ProtestNews</title>
		<imprint>
			<date type="published" when="2019-06-01">2019/06/01</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
