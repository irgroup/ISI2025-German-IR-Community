<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.83,115.96,337.70,12.62;1,199.13,133.89,217.09,12.62">Linguistic parameters and word embeddings for protest news detection in text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,265.52,171.56,63.52,8.74"><forename type="first">Chedi</forename><surname>Bechikh</surname></persName>
							<email>chedi.bechikh@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LISI laboratory</orgName>
								<orgName type="institution">Universit√© de Carthage</orgName>
								<address>
									<settlement>Tunisie</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.83,115.96,337.70,12.62;1,199.13,133.89,217.09,12.62">Linguistic parameters and word embeddings for protest news detection in text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">07C0C1ED72279603383FBAA4EF3DF188</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Linguistic parameters</term>
					<term>compound nouns</term>
					<term>word embeddings</term>
					<term>supervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present in this paper our participation in ProtestNews lab at CLEF 2019 in task 1 and task 2. In task 1, the objective is to predict if an article contains protest news or not. In task 2, we must decide if a sentence contains a protest event or not. For these two tasks, we used a supervised machine learning approach based on the logistic regression model. We combine the supervised learning algorithm with two different natural language techniques. The first relies on text processing with linguistic properties. The second is based on the expansion of the text with related term using word embedding similarity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the participation of LISI laboratory at the Conference and Labs of the Evaluation Forum (CLEF) 2019 ProtestNews for the detection of a protest event in news articles. We submitted results obtained from different approaches. In this paper, we describe the different proposed approaches as well as the findings concerning the results. The ProtestNews lab proposed three tasks:</p><p>-Task 1 is a classification task, it consists to identify which text contain protest news. -Task 2 objective is to classify if a sentence contains and event-trigger of protest or not. -Task 3 is an information extraction task, the objective is to extract locations, participants and time about protest event.</p><p>The reminder of this paper is organized as follow, in section 2 we describe our methodolgy based on supervised classification algorithm. In section 3 we describe the linguistics characteristics that we use to process the documents. Then, in section 4 we present our document expansion approach. In section 5, the experimental results are presented. Then we conclude and present future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>This section describes the model that have been used to classify the test data of both task 1 and task 2. The model is applied for both tasks, and use the same set of linguistic settings.</p><p>The overall architecture of our proposed framework consists of two main phases: training and testing. In the training phase, the classifier learns from a set of labeled text. Subsequently, the classifier is capable of classifying new unlabeled documents in the testing phase.</p><p>Each phase consists of the following steps: documents preparation, features extraction/selection, and classification. We opt for a supervised approach based on a classification algorithm <ref type="bibr" coords="2,262.56,250.41,9.96,8.74" target="#b4">[5]</ref>. The classification algorithm was implemented using the scikit learn<ref type="foot" coords="2,224.46,260.79,3.97,6.12" target="#foot_0">1</ref> library which is a machine learning library for the Python programming language.</p><p>We notice that ProtestNews documents present different characteristics for each task: documents in task 1 are composed of long sentences, this may cause drift in the classification process. In the other side, documents for task 2 are short, so they don't contain enough context for the training step. This can lead to different problems: word ambiguity and word mismatch between training data and test data.</p><p>To deal with these two tasks, we compared different classification algorithms, among them logistic regression algorithm, random forest, and naive Bayes algorithm. Preliminary experiments were carried on the development data after a training step. Based on these finding, we decided to use the logistic regression algorithm for the rest of the experiments because it gives the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Linguistic preprocessing</head><p>Before extracting the feature vectors it is required to pre-process the data with stop words removal and text lemmatization. We rely on linguistic processings since they lead to good results in previous work for sentiment analyis task <ref type="bibr" coords="2,462.09,485.34,9.96,8.74" target="#b3">[4]</ref>.</p><p>-Stop word removal: We used an English stop words list provided by the Terrier information retrieval team of the School of Computing Science of Glasgow University. The list contains 733 stop words. -Lemmatization: We have chosen to lemmatize document words to treat the morphological variations and thus to increase the recall. Lemmatization allows transforming words into a reduced form that is the lemma, which leads to ignoring variations in number and gender. We rely on the part-of-speech tagger Treetagger After text processing, we proceed to feature extraction, The aims of the feature selection technique are to find the most relevant features for the classification task. We used unigram because it gives the best results. We didn't rely on any weighting scheme, because using the tf.idf scheme degraded the classification performances in preliminaries experiments with the development data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Document expansion with word embeddings</head><p>To deal with the term mismatch problem, we decided to expand documents with the most similar word for each token. Since in previous work using pretrained word embeddings has proven to have a postif impact on different natural language processing tasks: Word Sense Disambiguation, Relational Similarity, Semantic Relatedness <ref type="bibr" coords="3,223.42,393.50,9.96,8.74" target="#b1">[2]</ref>. We pose the hypothesis that adding similar or related terms can help to enhance the recall and so the overall performance of the classification process.</p><p>For this approach we trained two word embeddings <ref type="bibr" coords="3,376.72,429.66,9.96,8.74" target="#b2">[3]</ref>: wiki-emb: a word embeddings trained on text8<ref type="foot" coords="3,358.30,440.92,3.97,6.12" target="#foot_2">3</ref> dataset which is a sample of a Wikipedia dump 4 . protest-emb: a word embeddings trained on the India training dataset given for task 1.</p><p>We choose to train two word embeddings with different data sets, to see if a specialized dataset have an impact on the classification performance, in comparison with text8 dataset which is a 100 megabytes cleaned dataset. For the word embeddings training, we rely on the Gensim python library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and results</head><p>We studied the performances of the proposed approach and we performed different experiments using different setting and processing:</p><p>-Run1: consist of applying lemmatization and stop words removal on the training and test set.</p><p>-Run2: consist of combining lemmatization, stop words and named entities removal on both training and test data sets. -Run3: expanding every word in the sentences with the most similar word from the protest-emb word embeddings. -Run4: expanding every word in the sentences with the most similar word from wiki-emb word embeddings. -Run5: combining run 2 settings with the annotation of all CN.</p><p>Table <ref type="table" coords="4,177.06,217.74,4.98,8.74" target="#tab_1">1</ref> present the official submitted runs, there are some runs where there are results only for task 2. The analysis of the results shows that the first run (lemmatizing and eliminating stop word) allows to obtain 0.7612 for task 1 and it corresponds to our second best run. The best result for task 1 was achieved when we expanded the content of the text with bigram extracted from the same span of text. This run allows to obtain the best overall results, but the best result for task 1 (China and India) and the best result for task 2 (India). We note the degradation of the results for task 2 (China).</p><p>The best overall result for task 2 is obtained by a simple approach that consists of lemmatizing the text and eliminating stop words.</p><p>In a preliminary study phase with development data, we found that expanding text with the most similar word is only beneficial for task 2 and it degrades results for task1. We decided to study the impact of this approach only for task 2. We notice that the best overall results are obtained when training the word embedding on the training set. The best result for task 2 is obtained with word embedding trained on the same data, but the best result for China data is obtained when we used word embedding from another general corpus. This can be explained by the fact that the text8 dataset is bigger dataset and contain more tokens than task 1 training dataset. Our official final run was the Run6 and it was ranked sixth among 12 teams. We can note that with this run we achieved the best our results in task 1 and task 2 with India data. Degradation of the performance has been noticed for task 2 with China data, this can be explained by terms mismatch between CN in the training set with india data and the CN with the China data because CN in India data represents other concepts than those extracted in China dataset. This paper describes our participation in the ProtestNews detection lab at CLEF 2019. The aim of this work is to make a decision if a text contains protest news or not. The objective is to develop text classification tools. For this purpose, we used a classifier based on the Logistic regression algorithm. As the first step, we processed the linguistic data processing as a first step. Then, we use word embeddings to expand text with the most similar word. Also, we proposed to add CN extracted from the same text. This work is still in progress and needs more investigations. For future work, we plan to use deep neural network since it achieved good results for other NLP tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,140.99,589.21,339.60,46.15"><head></head><label></label><figDesc>2 to lemmatize the text. -Eliminating named entities (person, place, organization) form text content, because they can't represent protest news. Named entity lead to a drift in the classification process because they can present in both protest news and regular news. In this study, we define named entities as the words that were annotated with the tag "NP" by Treetagger. -Compound noun (CN) annotation: CN can capture important concepts in the content of the document such as event or concepts related to protest event (e.g.: district violence, armys action, tense situation, etc) [1]. To ensure that we extract CN related to protest events, we first extract CN from documents that are classified as protest news in the training set. Then, we annotate each corresponding CN in the test data. In this work, we only use CN composed of tow words, because in preliminary experiment yield better results than longer CN.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,140.51,471.90,331.56,82.91"><head>Table 1 .</head><label>1</label><figDesc>Classification results for task 1 and task 2 based on F1 measure</figDesc><table coords="4,140.51,491.35,331.56,63.46"><row><cell cols="8">model task1 test china task1 avg task1 task2 test china task2 avg task 2 avg task</cell></row><row><cell cols="2">Run1 0.7612</cell><cell>0.3846</cell><cell>0.5729</cell><cell>0.5657</cell><cell>0.4788</cell><cell cols="2">0.5223 0.5476</cell></row><row><cell cols="2">Run2 0.7612</cell><cell>0.4418</cell><cell>0.6015</cell><cell>0.4727</cell><cell>0.3960</cell><cell>0.4343</cell><cell>0.5179</cell></row><row><cell>Run3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.5692</cell><cell>0.4615</cell><cell>0.5150</cell><cell>-</cell></row><row><cell>Run4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.5748</cell><cell>0.4143</cell><cell>0.4945</cell><cell>-</cell></row><row><cell cols="2">Run 5 0.7676</cell><cell>0.5032</cell><cell>0.6354</cell><cell>0.5877</cell><cell>0.3086</cell><cell cols="2">0.44819 0.5418</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,645.84,92.30,7.86"><p>https://scikit-learn.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,656.80,249.34,7.86"><p>https://www.cis.uni-muenchen.de/ schmid/tools/TreeTagger/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,645.84,172.80,7.86"><p>http://mattmahoney.net/dc/textdata.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,138.35,294.02,342.24,7.86;5,146.91,304.95,299.16,7.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,321.81,294.02,158.78,7.86;5,146.91,304.98,93.71,7.86">Empirical evaluation of compounds indexing for turkish texts</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bechikh-Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Haddad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Slimani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,247.41,304.98,123.04,7.86">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="95" to="106" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,315.94,342.24,7.86;5,146.91,326.90,333.67,7.86;5,146.91,337.86,333.68,7.86;5,146.91,348.81,70.14,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,231.19,315.94,249.40,7.86;5,146.91,326.90,38.71,7.86">Do multi-sense embeddings improve natural language understanding?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,202.18,326.90,278.40,7.86;5,146.91,337.86,143.42,7.86">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">September 17-21, 2015. 2015</date>
			<biblScope unit="page" from="1722" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,359.77,342.24,7.86;5,146.91,370.73,333.68,7.86;5,146.91,381.69,333.68,7.86;5,146.91,392.65,333.68,7.86;5,146.91,403.61,70.14,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,406.99,359.77,73.60,7.86;5,146.91,370.73,234.34,7.86">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,402.17,370.73,78.42,7.86;5,146.91,381.69,333.68,7.86;5,146.91,392.65,76.72,7.86">Advances in Neural Information Processing Systems: 27th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">December 5-8, 2013. 2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,414.57,342.24,7.86;5,146.91,425.53,333.68,7.86;5,146.91,436.49,333.68,7.86;5,146.91,447.44,221.56,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,347.91,414.57,132.68,7.86;5,146.91,425.53,234.64,7.86">Tw-star at semeval-2018 task 1: Preprocessing impact on multi-label emotion classification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mulki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Haddad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Babaoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,402.67,425.53,77.93,7.86;5,146.91,436.49,308.46,7.86">Proceedings of The 12th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT</title>
		<meeting>The 12th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">June 5-6, 2018. 2018</date>
			<biblScope unit="page" from="167" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,458.40,342.24,7.86;5,146.91,469.36,333.68,7.86;5,146.91,480.32,333.68,7.86;5,146.91,491.28,85.37,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,305.75,458.40,174.84,7.86;5,146.91,469.36,112.43,7.86">Thumbs up? sentiment classification using machine learning techniques</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,281.00,469.36,199.59,7.86;5,146.91,480.32,229.28,7.86">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, EMNLP 2002</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing, EMNLP 2002<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">July 6-7, 2002 (2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
