<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,130.14,152.82,335.17,12.43;1,263.19,170.82,64.52,12.43;1,327.84,169.51,4.56,8.14">Deep Multimodal Learning for Medical Visual Question Answering 1</title>
				<funder>
					<orgName type="full">NVIDIA Corporation</orgName>
				</funder>
				<funder>
					<orgName type="full">Titan Xp GPU</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,222.50,209.42,29.19,9.00"><forename type="first">Lei</forename><surname>Shi</surname></persName>
							<email>lshi@wpi.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Worcester Polytechnic Institute</orgName>
								<address>
									<postCode>01609</postCode>
									<settlement>Worcester</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.92,209.42,41.98,9.00"><forename type="first">Feifan</forename><surname>Liu</surname></persName>
							<email>feifan.liu@umassmed.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts Medical School</orgName>
								<address>
									<postCode>01655</postCode>
									<settlement>Worcester</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.35,209.42,56.43,9.00"><forename type="first">Max</forename><forename type="middle">P</forename><surname>Rosen</surname></persName>
							<email>max.rosen@umassmemorial.org</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts Medical School</orgName>
								<address>
									<postCode>01655</postCode>
									<settlement>Worcester</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,130.14,152.82,335.17,12.43;1,263.19,170.82,64.52,12.43;1,327.84,169.51,4.56,8.14">Deep Multimodal Learning for Medical Visual Question Answering 1</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7CE7828404C3E37CECD2F1AB15F28812</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>Transfer Learning</term>
					<term>ETM</term>
					<term>Multi-modal Fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the University of Massachusetts Medical School in the ImageCLEF 2019 Med-VQA task. The goal is to predict the answers given the medical images and the questions. The categories of the questions are provided for the training and validation datasets. We implemented long-short-term memory (LSTM) for question textual feature extraction and transfer learning followed by the co-attention mechanism for image feature extraction. Due to the provided category information, we implemented the SVM model the predict the question category which is used as another feature for our system. In addition, we applied the embedding based topic model (ETM) to generate question topic distribution as one more feature for our system. To efficiently integrate different types of features, we employed the multi-modal factorized high-order pooling (MFH). For the answer prediction, we developed a two-channel framework to handle different categories of questions through single-label classification and multi-label classification respectively. We submitted 3 valid runs, and the best system achieved the accuracy of 0.566 and the BLEU score of 0.593, ranking the 5 th place among 17 participating groups.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given an image and a natural language question about the image, visual question answering (VQA) task is to provide an accurate natural language answer. This task combines computer vision (CV) and natural language processing (NLP). One of the challenges for VQA task is how to fuse different types of features. Various methods, like LinearSum and Multi-modal Factorized Bilinear Pooling (MFB), have been designed and practiced on the VQA task.</p><p>A lot of studies of VQA task are in the general domain. With increasing implementations of deep learning to support clinical decision making and improve patient engagement, some studies begin to focus on the VQA task in the medical domain. Im-ageCLEF 2019 <ref type="bibr" coords="2,187.00,198.38,11.69,9.00" target="#b0">[1]</ref> organized the inaugural edition of the Medical Domain Visual Question Answering (Med-VQA) Task <ref type="bibr" coords="2,262.43,210.38,10.64,9.00" target="#b1">[2]</ref>. Given a medical image with a clinically relevant question, the system is tasked with answering the question based on the visual image content. The dataset of this year is different from the last year. The categories of the questions are provided. For the questions in the first three categories, there are a limited number of answer candidates. And the answers to the questions in the last category are narrative.</p><p>In this work, we introduced the question category information and the question topic distribution as two additional information during the information fusion process. To develop an integrated system that is able to handle all four categories of questions, we developed two-channel structures for the answer prediction. One channel is to classify the image-question pairs into the close set of answer candidates. The other channel is to generate a narrative answer given an image-question pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System description</head><p>Our system consists of 6 components: transfer learning for image feature extraction, LSTM for question textual feature extraction, other features (including question category information and question topic distribution), co-attention mechanism, MFH for feature fusion, and answer generation. Fig. <ref type="figure" coords="2,298.43,459.50,5.04,9.00">1</ref> shows the architecture of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1.</head><p>Our system architecture at Med-VQA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Processing</head><p>A pre-trained biomedical word embedding (dimension of 200) is used as the embedding layer. After the word embedding layer, a bidirectional LSTM network is used to extract textual features of the question. During the training process, the embedding of the "unknown" token is first initialized randomly and then learned. The textual features are transformed to predict the attention weight of different grid locations, which generates the attentional features of the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Processing</head><p>We applied transfer learning to extract image features. The pre-trained ResNet-152 model of ImageNet (excluding the last 2 layers, pooling layer and fully-connected layer) is the image feature extractor. The parameters of the last 2 convolutional blocks of ResNet-152 model are fine-tuned during the training process. Then we applied the co-attention mechanism to generate the attentional features of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Question Topic Distribution</head><p>ETM <ref type="bibr" coords="3,148.74,334.46,11.69,9.00" target="#b2">[3]</ref> is applied to generate several topics from the questions. 10 topics are generated by applying ETM on the questions in the training dataset. Each question is assigned a vector of topic distribution according to the frequencies of topics' words appearing in the question. The topic distribution is used as another input feature of the MFH. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Question Categorization</head><p>According to the instruction of the 2019 Med-VQA, the questions are from 4 categories. SVM is used to classify the category of the question. We applied TF-IDF based unigram vectorization to extract textual features, and trained a support vector machines (SVM) model using the questions from the training dataset. The accuracy of the SVM model on validation dataset is 100%, which shows that the language used in different categories of questions are relatively unique and less ambiguous. The category information of the question is used as an additional input of the MFH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Feature Fusion</head><p>MFH <ref type="bibr" coords="4,149.22,224.30,11.69,9.00" target="#b3">[4]</ref> contains multiple dependent MFB blocks. The output from the expand stage of the previous MFB block is fed into the next MFB block as additional input, and the output from multiple MFB blocks are merged together as a final fused feature representation.</p><p>We applied a 2-block MFH model to fuse 4 types of features including image attentional features, question attentional features, question topic distribution, and question category, which is shown in Fig. <ref type="figure" coords="4,257.55,308.30,3.76,9.00" target="#fig_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Answer Prediction</head><p>According to the instruction of 2019 Med-VQA, for the questions of the first 3 categories, the corresponding answers are in a limited number of certain candidates. We regarded this case as a single-label classification task. On the other hand, the questions of "abnormality" category are the narrative type. This case is regarded as a multi-label classification task. So, we built two-channel structures which are shown in Fig. <ref type="figure" coords="4,444.04,406.46,3.76,9.00" target="#fig_1">3</ref>. One is for the single-label classification task and the other one is for the multi-label classification task. For multi-label classification, each unique word in the answer sentence is considered an answer label for the corresponding image-question pair. Based on the distribution of all the answer labels, the narrative answer is generated using the sampling method. Therefore, both a classification result and a distribution of words in the answer are predicted through our system for each pair of image-question. If the classification result is one of the certain candidates of the answers, the final answer is that candidate. Otherwise, the final answer is a combination of words generated by the sampling method.</p><p>The loss function of our system is an integration of two loss functions. We applied the cross-entropy loss function for the single-label classification structure and Kullback-Leibler divergence loss function for the multi-label classification structure. Given an image-question pair, the loss L is calculated as follows:</p><formula xml:id="formula_0" coords="4,180.20,594.62,286.58,9.06">ùêø = (1 -ùê¥) * ùê∂ùëüùëúùë†ùë†ùê∏ùëõùë°ùëüùëúùëùùë¶ùêøùëúùë†ùë† + ùê¥ * ùêæùêøùê∑ùëñùë£ùêøùëúùë†ùë† (<label>1</label></formula><formula xml:id="formula_1" coords="4,466.78,594.62,3.90,9.00">)</formula><p>where A is 1 if the predicted category of the question is "Abnormality" otherwise 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We experimented with 4 settings of the pre-trained ResNet-152 model on ImageNet:</p><p>(1) Res-2 is using the pre-trained ResNet-152 model excluding the last 2 layers (pooling layer and fully-connected layer); (2) Res-3 is using the pre-trained ResNet-152 model excluding the last 3 layers (last residual block, pooling layer and fully-connected layer);</p><p>(3) Res-2-tunable is using the pre-trained ResNet-152 model excluding the last 2 layers, and the last residual block is fine-tuned during the training process of our system; (4) ETM-Res-2 is using the pre-trained ResNet-152 model excluding the last 2 layers. We used the topics of the questions generated by the ETM model to label the corresponding images and fine-tuned this ResNet-152 model. A pre-trained word-embedding (dimension of 200) on PubMed and the clinical notes from MIMIC-III Clinical Database is used as the word embedding layer. We experimented with 2 settings to handle "unknown" token in the questions: (1) Fixed-Unknown is using a fixed 0 vector for "unknown" token; (2) Learned-Unknown is initializing a random vector for "unknown" token and this vector is trained during the training process of our system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance on Validation Dataset</head><p>Table <ref type="table" coords="6,150.39,170.30,5.04,9.00" target="#tab_0">1</ref> shows the performance of different settings on the validation dataset. We can see that ETM based transfer learning is not as helpful as shown in <ref type="bibr" coords="6,400.62,182.30,10.64,9.00" target="#b4">[5]</ref>. It is partially because the questions in 2019 data are less diverse and the topic labels derived from ETM are not distinctive enough to finetune the pre-trained ImageNet model during image classification training. We also found that visual features from different layers of Residual networks perform differently (Res-2 vs. Res-3 in Table <ref type="table" coords="6,394.51,230.30,3.62,9.00" target="#tab_0">1</ref>), which suggests that combining them together may help improve the system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Official Test Runs in ImageCLEF 2019</head><p>We submitted 3 runs on the test dataset and the settings and results are shown in Table <ref type="table" coords="6,124.80,304.46,3.76,9.00" target="#tab_0">1</ref>. The accuracy of our best system is 0.566 and the BLEU score is 0.593 on the test dataset, ranking fifth place. The team ranking first place of this competition obtained an accuracy of 0.624 and a BLEU score of 0.644. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Examples of System Outputs on Validation Dataset</head><p>Fig. <ref type="figure" coords="7,143.84,366.62,5.04,9.00">4</ref> and Fig. <ref type="figure" coords="7,188.71,366.62,5.04,9.00">5</ref> show some examples of poor predictions and good predictions that our best system makes on the validation dataset. More analysis is needed to investigate the system's performance on different question categories and identify different error patterns to inform future improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We experimented with three different settings of deep learning structures for MED-VQA task 2019, where we introduced two more types of features and we constructed two-channel structures for answer prediction. Due to the limited time, we did not implement Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" coords="7,431.77,495.50,11.69,9.00" target="#b5">[6]</ref> which will be explored in the future to extract question textual features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,238.88,585.43,117.84,8.14;3,217.65,391.40,160.00,185.00"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Feature fusion with MFH</figDesc><graphic coords="3,217.65,391.40,160.00,185.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,202.78,359.35,189.99,8.14;5,175.65,255.40,244.00,95.00"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Two-channel structures for answer prediction</figDesc><graphic coords="5,175.65,255.40,244.00,95.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,224.29,644.30,146.93,9.00;6,161.65,361.40,271.89,279.55"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Examples of poor predictions</figDesc><graphic coords="6,161.65,361.40,271.89,279.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,130.32,466.39,327.36,82.99"><head>Table 1 .</head><label>1</label><figDesc>Summary of experiments on the validation dataset</figDesc><table coords="5,130.32,483.02,327.36,66.36"><row><cell>Answer Max</cell><cell>Word Embedding</cell><cell>Image Feature</cell><cell>Classification</cell><cell>BLEU</cell></row><row><cell>Length</cell><cell></cell><cell>Extractor</cell><cell>Accuracy</cell><cell>Score</cell></row><row><cell>10</cell><cell>Fixed-Unknown</cell><cell>Res-2</cell><cell>0.575</cell><cell>0.473</cell></row><row><cell>10</cell><cell>Fixed-Unknown</cell><cell>Res-3</cell><cell>0.591</cell><cell>0.602</cell></row><row><cell>9</cell><cell>Fixed-Unknown</cell><cell>ETM-Res-2</cell><cell>0.558</cell><cell>0.457</cell></row><row><cell>6</cell><cell cols="2">Learned-Unknown Res-2-tunable</cell><cell>0.594</cell><cell>0.626</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,130.32,587.83,318.48,71.47"><head>Table 2 .</head><label>2</label><figDesc>Summary of submissions in ImageCLEF 2019</figDesc><table coords="5,130.32,604.46,318.48,54.84"><row><cell>Answer Max</cell><cell>Word Embedding</cell><cell>Image</cell><cell>Feature</cell><cell>Accuracy</cell><cell>BLEU</cell></row><row><cell>Length</cell><cell></cell><cell>Extractor</cell><cell></cell><cell></cell><cell>Score</cell></row><row><cell>9</cell><cell>Fixed-Unknown</cell><cell cols="2">ETM-Res-2</cell><cell>0.018</cell><cell>0.039</cell></row><row><cell>10</cell><cell>Fixed-Unknown</cell><cell>Res-3</cell><cell></cell><cell>0.48</cell><cell>0.509</cell></row><row><cell>6</cell><cell cols="3">Learned-Unknown Res-2-tunable</cell><cell>0.566</cell><cell>0.593</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We gratefully acknowledge the support of <rs type="funder">NVIDIA Corporation</rs> with the donation of the <rs type="funder">Titan Xp GPU</rs> used for this research.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,128.56,633.50,342.15,9.00;7,138.00,645.50,332.65,9.00;7,138.00,657.50,332.73,9.00;7,138.00,669.50,332.70,9.00;8,138.00,150.38,332.79,9.00;8,138.00,162.38,332.76,9.00;8,138.00,174.38,332.76,9.00;8,138.00,186.38,332.76,9.00;8,138.00,198.38,332.63,9.00;8,138.00,210.38,332.72,9.00;8,138.00,222.38,103.86,9.00" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,274.19,186.38,196.57,9.00;8,138.00,198.38,154.23,9.00">ImageCLEF 2019: Multimedia Retrieval in Medicine, Lifelogging, Security and Nature</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>P√©teri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitri</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aleh</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Triet</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Tranand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seco</forename><surname>Garc√≠a</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikos</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Konstantinos</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,452.45,150.38,18.35,9.00;8,138.00,162.38,332.76,9.00;8,138.00,174.38,53.07,9.00;8,312.25,198.38,158.38,9.00;8,138.00,210.38,122.45,9.00">Narciso Garcia, Ergina Kavallieratou, Carlos Roberto del Blanco, Carlos Cue-vasRodr√≠guez</title>
		<title level="s" coord="8,267.05,210.38,142.40,9.00">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Lugano</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,128.56,234.38,342.15,9.00;8,138.00,246.38,332.73,9.00;8,138.00,258.38,332.80,9.00;8,138.00,270.38,315.79,9.00" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,153.61,246.38,317.12,9.00;8,138.00,258.38,38.37,9.00">VQA-Med: Overview of the Medical Visual Question Answering Task at Im-ageCLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename></persName>
		</author>
		<ptr target="http://ceur-ws.org/)Vol.2380" />
	</analytic>
	<monogr>
		<title level="m" coord="8,224.11,258.38,110.97,9.00">CLEF2019 Working Notes</title>
		<title level="s" coord="8,344.09,258.38,126.71,9.00">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.56,282.38,342.35,9.00;8,138.00,294.38,240.81,9.00" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08496[cs</idno>
		<title level="m" coord="8,296.35,282.38,174.56,9.00;8,138.00,294.38,108.83,9.00">Topic Modeling over Short Texts by Incorporating Word Embeddings</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.56,306.38,342.17,9.00;8,138.00,318.38,332.86,9.00" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,267.42,306.38,203.30,9.00;8,138.00,318.38,201.36,9.00">Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01471[cs</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.56,330.38,342.15,9.00;8,138.00,342.38,213.11,9.00" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,275.24,330.38,195.47,9.00;8,138.00,342.38,46.95,9.00">UMass at ImageCLEF Medical Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,190.16,342.38,84.30,9.00">Med-VQA) 2018 Task</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.56,354.38,342.17,9.00;8,138.00,366.38,332.66,9.00;8,138.00,378.38,29.20,9.00" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,342.21,354.38,128.52,9.00;8,138.00,366.38,226.73,9.00">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805[cs</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
