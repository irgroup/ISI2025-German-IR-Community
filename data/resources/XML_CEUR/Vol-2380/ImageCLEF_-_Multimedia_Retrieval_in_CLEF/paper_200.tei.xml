<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,167.19,115.96,280.97,12.62">Overview of ImageCLEFcoral 2019 task</title>
				<funder ref="#_jYDgpfG">
					<orgName type="full">Innovate UK</orgName>
				</funder>
				<funder ref="#_87TxBjE">
					<orgName type="full">Operation Wallacea</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,181.63,153.63,74.88,8.74"><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">University of Essex</orgName>
								<address>
									<settlement>Colchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.07,153.63,79.43,8.74"><forename type="first">Antonio</forename><surname>Campello</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">University of Essex</orgName>
								<address>
									<settlement>Colchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Filament, Cargo Works</orgName>
								<address>
									<addrLine>1-2 Hatfields</addrLine>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.39,153.63,63.10,8.74"><forename type="first">Jessica</forename><surname>Wright</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">University of Essex</orgName>
								<address>
									<settlement>Colchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.08,165.58,46.41,8.74"><forename type="first">Louis</forename><surname>Clift</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">University of Essex</orgName>
								<address>
									<settlement>Colchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,238.05,165.58,57.62,8.74"><forename type="first">Adrian</forename><surname>Clark</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">University of Essex</orgName>
								<address>
									<settlement>Colchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.83,165.58,106.98,8.74"><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">University of Essex</orgName>
								<address>
									<settlement>Colchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,167.19,115.96,280.97,12.62">Overview of ImageCLEFcoral 2019 task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">278972EC994FE86D1F777CECFC0AB533</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>image annotation</term>
					<term>image labelling</term>
					<term>classification</term>
					<term>segmentation</term>
					<term>coral reef image annotation</term>
					<term>marine image annotation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding the composition of species in ecosystems on a large scale is key to developing effective solutions for marine conservation, hence there is a need to classify imagery automatically and rapidly. In 2019, ImageCLEF proposed for the first time the ImageCLEFcoral task. The task requires participants to automatically annotate and localize benthic substrate (such as hard coral, soft coral, algae and sponge) in a collection of images originating from a growing, large-scale dataset from coral reefs around the world as part of monitoring programmes. In its first edition, five groups participated submitting 20 runs using a variety of machine learning and deep learning approaches. Best runs achieved 0.24 in the annotation and localisation subtask and 0.04 on the pixelwise parsing subtask in terms of MAP 0.5 IoU scores which measures the Mean Average Precision (MAP) when using the performance measure of Intersection over Union (IoU) bigger to 0.5 of the ground truth.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEFcoral task described in this paper is part of the ImageCLEF<ref type="foot" coords="1,476.12,474.24,3.97,6.12" target="#foot_0">3</ref> benchmarking campaign <ref type="bibr" coords="1,247.06,487.77,9.96,8.74" target="#b0">[1]</ref>. ImageCLEF is part of CLEF <ref type="foot" coords="1,397.06,486.20,3.97,6.12" target="#foot_1">4</ref> (Cross Language Evaluation Forum) and it provides a framework where researchers can share their expertise and compare their methods based on the exact same data and evaluation methodology in an annual rhythm. In 2019, there were four tasks in ImageCLEF: ImageCLEFlifelog; ImageCLEFmedical; ImageCLEFcoral; and ImageCLEFsecurity.</p><p>In its first edition, ImageCLEFcoral follows the successful ImageCLEF annotation tasks (2012-2016) <ref type="bibr" coords="1,254.76,571.46,8.19,8.74" target="#b1">[2]</ref><ref type="bibr" coords="1,262.95,571.46,4.10,8.74" target="#b2">[3]</ref><ref type="bibr" coords="1,262.95,571.46,4.10,8.74" target="#b3">[4]</ref><ref type="bibr" coords="1,262.95,571.46,4.10,8.74" target="#b4">[5]</ref><ref type="bibr" coords="1,267.05,571.46,8.19,8.74" target="#b5">[6]</ref> and requires participants to automatically annotate and localize a collection of images with types of benthic substrate, such as hard coral and sponge.</p><p>The genesis of the ImageCLEFcoral task is from the ever-increasing amount of data being collected as part of marine conservation efforts. With the rise in popularity both of Scuba diving and underwater photography, high quality equipment to archive the underwater world has become inexpensive for marine biologists and conservation enthusiasts. The performance of so-called "action cameras" such as GoPros can yield high quality imagery with very little photographic expertise from the user. Such imagery can be used to assess the health of a coral reef, for measurements of individual species and wider benthic coverage of assemblages indicating phase shifts in the marine ecosystem <ref type="bibr" coords="2,406.03,214.64,9.96,8.74" target="#b6">[7]</ref>. However, this creates a volume of data that is too large to be annotated by human labellers, and this problem is substantially more difficult than the segmentation of other image types due to the complex morphologies of the objects <ref type="bibr" coords="2,400.08,250.50,9.96,8.74" target="#b7">[8]</ref>.</p><p>A typical computer vision application involves image capture, then segmenting regions of interest from the surroundings, and finally classifying them. There has been a long-standing use of machine learning in the last of these stages in particular, because alternative strategies such as rule-based classification are generally less effective. Classifying man-made objects (such as in previous Image-CLEF annotations tasks), which may have simple, well-defined shapes, is more straightforward than classifying biological objects because the shape of the latter varies from one instance to another; correct classification will often involve a combination of shape and texture.</p><p>In the same way, segmenting a natural shape from its surroundings is not necessarily straightforward. For this task for example, colour might be enough to segment some parts of a single coral from a rock lying behind it but other parts of the same coral might lie in front of a similarly-coloured sponge or fish -this makes the segmenting task much more difficult.</p><p>The difficulty in both the segmentation and classification stages explains why there are two sub-tasks within the coral reef exercise. The first is to identify the type of substrate present within a region without having to identify its outline precisely, while the second involves segmenting the coral correctly as well as classifying it.</p><p>The rest of the paper is organised as follows: Section 2 presents the 2019 ImageCLEFcoral task; Section 3 provides an overview and analysis of the collection; Section 4 details the evaluation methodology; Sections 5 and 6 present and discuss the results of the participants; and finally, Section 7 concludes the paper indicating possible new directions for the challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks</head><p>In its first edition, the ImageCLEFcoral task follows a similar format to 2015 and 2016 ImageCLEF annotation tasks <ref type="bibr" coords="2,309.36,599.29,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,321.53,599.29,7.75,8.74" target="#b2">3]</ref> and includes two subtasks:</p><p>Coral reef image annotation and localisation subtask For each image, participants produce a set of bounding boxes, predicting the benthic substrate for each bounding box in the images.</p><p>Coral reef image pixel-wise parsing subtask For each image, participants produce a set of polygons bounding each benthic substrate and predict the benthic substrate for each polygon in the image. This task aims to provide a more detailed segmentation of the substrate in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Collection</head><p>The annotated dataset comprises several sets of overlapping images, each set taken in an area of underwater terrain (see Section 3.1). Figure <ref type="figure" coords="3,430.27,220.25,4.98,8.74" target="#fig_0">1</ref> shows an example of an image in the collection. Each image was labelled by experts (see Section 3.2). The training set contains contains 240 images with 6670 substrate areas annotated and the test set contains 200 images with 5370 substrate areas annotated. Some types of benthic substrates are easy to detect with obvious features, others are much more difficult. For example, hard corals are typified by having a rigid skeleton unlike soft corals which often have a soft, fluffy appearance. The classification and descriptions of organic benthic substrate are as follows:</p><p>Hard Coral -Branching: Morphologies grow like a tree, with branches coming out from a centre point and continually branching (secondary branching). Hard Coral -Sub-Massive: Unlike branching colonies, these are columnar morphologies which do not have secondary branching. All branches come from the first column and typically they will only have columns.</p><p>Hard Coral -Boulder: Coral morphologies that grow in spherical shapes and are often called massive corals or brain corals (on some species polyp walls form ridges that look like a human brain). Hard Coral -Encrusting: Morphologies that form a layer over hard surfaces, and they follow the contours of the surface. These are more difficult to see than more 3D corals, but some of those genera that form 3D morphologies also form encrusting morphologies. Hard Coral -Table : Corals morphologies that look like a table, typically with a stalk connecting it with the substratum. Many of these colonies begin as branching colonies, but their branches form tight networks to create a flat surface with a table-like appearance. Hard Coral -Foliose: These morphologies are named after their leaf-like structure. They are sometimes referred to as cabbage corals. Hard Coral -Mushroom: These are single polyp corals. Their skeleton looks like that of an up-turned mushroom, with the ridges mimicking the "gills" of the underside of a mushroom. These can be found all over a reef, and are from only one family, the Fungidae. These single polyps are much larger than the polyps you find in colonies. Soft coral: Soft corals cover a wide variety of species that are distinguished by their apparent flexibility, texture and colour. They may have clearly distinguishable open polyps. This category covers all soft corals, except Gorgonian sea fans (see below). Gorgonian: Gorgonian sea fans are distinctive soft corals that grow as large branching planar structures that face into the prevailing current. They can resemble tree-like structures with a thick trunk and branches, as well as more uniformly branching structures that do not form complete plates. Sponge: These can have a varied morphology and can also be highly cryptic.</p><p>They differ from corals in that they do not have polyps but tiny holes, giving them a pitted appearance. This category covers all sponges, except barrel sponges (see below). Barrel sponge: A sponge with a highly distinctive, dark orange/brown barrel shape with a wide opening at the top. Barrel sponges were classified morphologically, not taxonomically, so any with this growth form were grouped. Fire coral (Millepora): This is not actually a coral but rather a colony of tiny animals called hydroids. They grow in a variety of shapes but typically a branching staghorn structure. They can also encrust rocks and other organic substrate such as gorgonian sea fans. Algae: There are several classes of algae but we are interested only in the large-leaved foliose macro-algae and not turf algae, crustose coraline algae, encrusting algae or maerl which are difficult to classify from imagery. This type of algae can vary from large, vivid green leaves to paler, fluffy bushes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data acquisition</head><p>The images for the ImageCLEFcoral task are a subset from a growing, largescale collection of images taken of coral reefs around the world as part of a coral reef monitoring project with the Marine Technology Research Unit (MTRU) at the University of Essex. The subset used was collected from several locations in the Wakatobi Marine Reserve in Sulawesi, Indonesia in July 2018. The data was collected using SJCAM5000 Elite action cameras in underwater housings with a red filter attached, held at an oblique angle to the reef. Most images have a tape measure running through a portion of the image because they are part of a monitoring collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image annotation</head><p>The images were manually annotated using a custom online polygon drawing tool (see Figure <ref type="figure" coords="5,206.39,252.49,4.98,8.74" target="#fig_1">2</ref> for a screenshot of the tool). Each image was hand-annotated by postgraduate coral biology students to identify benthic substrate and validated by an administrator. Using the online tool, areas of organic benthic substrate were identified. First a polygon area was created by clicking points on the image (see Figure <ref type="figure" coords="5,357.42,589.95,4.43,8.74" target="#fig_2">3</ref>) and then a substrate type was chosen and the label completed (see Figure <ref type="figure" coords="5,345.42,601.91,3.87,8.74" target="#fig_3">4</ref>).  Annotators were instructed to ensure the following rules:</p><p>-Click points must be in sequence around the object to avoid overlapping bounding boxes; -Click points and eventual bounding box should be inside the annotated object; -One bounding box can be used for multiple individuals (substrates) so long as they are all the same benthic substrate type; -Bounding boxes must not overlap. As a quality control to rank and train annotators, they were provided with five training images with a rich diversity of substrates, annotated previously to pixel level (see Figure <ref type="figure" coords="7,249.48,142.90,3.87,8.74" target="#fig_4">5</ref>). These training images are considered as "golden annotations" and used for the per-class agreement of the users and the groundtruth.</p><p>The agreement per class has been calculated, on a pixel level, with the intersection over union (IoU) metric: agreement = # of true positives # of false positives + # of false negatives + # of true positives .</p><p>After discarding the annotators with an agreement level smaller than 15% for quality control, the average annotation agreement and standard deviation was calculated per category of the benthic substrate type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Collection analysis</head><p>For the training set, the proportion of "background" pixels (i.e., pixels not annotated with any of substrates) was 76.44%. Excluding the background, the benthic substrate type pixel-distribution can be found in Figure <ref type="figure" coords="7,423.30,360.76,3.87,8.74" target="#fig_5">6</ref>. The figure shows a large imbalance towards soft-corals, whereas seven benthic substrate types have been underrepresented in less than 3% of the total non-background pixels ("Hard Coral -Submassive", "Hard Coral -Table ", "Hard Coral -Foliose", "Hard Coral -Mushroom", "Soft Coral -Gorgonian", "Fire Coral -Millepora", "Algae -Macro or Leaves").    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Methodology</head><p>The task was evaluated using the methodology of previous ImageCLEF annotation tasks <ref type="bibr" coords="8,191.77,426.02,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="8,203.95,426.02,8.92,8.74" target="#b2">3]</ref>,which follows a PASCAL style metric of IoU. We used the following three measures: M AP 0.5 IoU : the localised Mean Average Precision (MAP) for each submitted method using the performance measure of IoU &gt;=0.5 of the ground truth; R 0.5 IoU : the localised mean recall for each submitted method using the performance measure of IoU &gt;=0.5 of the ground truth; M AP 0 IoU : the image annotation average for each method in which the concept is detected in the image without any localisation.</p><p>In addition, to further analyse the results per types of benthic substrate, the measure accuracy per substrate was used, in which the segmentation accuracy for a substrate was assessed using the number of correctly labelled pixels of that substrate, divided by the number of pixels labelled with that class (in either the ground truth labelling or the inferred labelling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In 2019, 13 teams registered for the first edition of the ImageCLEFcoral task. Five individual teams submitted 20 runs. Table <ref type="table" coords="8,347.83,656.12,4.98,8.74" target="#tab_0">1</ref> gives an overview of all par-ticipants and their runs. There was a limit of at most 10 runs per team and subtask. ISEC <ref type="bibr" coords="9,160.61,203.04,9.73,7.86" target="#b8">[9]</ref> Coimbra Institute of Engineering, Portugal 1 0 VIT <ref type="bibr" coords="9,156.13,214.00,14.34,7.86" target="#b9">[10]</ref> Vellore Institute of Technology, India 5 0 HHUD <ref type="bibr" coords="9,167.00,224.96,14.34,7.86" target="#b10">[11]</ref> Heinrich-Heine-Universität Duesseldorf, Germany Tables <ref type="table" coords="9,165.10,344.42,4.98,8.74" target="#tab_1">2</ref> and<ref type="table" coords="9,190.77,344.42,4.98,8.74" target="#tab_2">3</ref> present the performance of the participants on the coral reef image annotation and localisation subtask. 15 runs were submitted in this subtask by 3 teams. The HHUD team <ref type="bibr" coords="9,226.41,644.16,15.50,8.74" target="#b10">[11]</ref> achieved the best results in terms of M AP 0.5 IoU by applying a state-of-the-art deep learning approach, YOLO. Unlike the regional convolutional neural network (R-CNN) approach adopted by <ref type="bibr" coords="10,401.87,118.99,14.60,8.74" target="#b9">[10]</ref>, YOLO works on the whole image at the same time, by dividing the entire image into square cells which are predicted to contain bounding boxes of substrate. This should mean that there are fewer background errors compared to R-CNNs because more context is taken into account. In addition, the authors devised an approach of their own, first locating and then classifying, and then utilising machine learning. The strategy for locating possible substrate is that they differ from background regions, so the authors partition the image into small "tiles" and construct a feature vector containing colour, shape and texture measures: normalized histograms, grey-level co-occurrence matrices and Hu moments, respectively. From these features, a binary classifier is trained to distinguish coral and non-coral tiles. For classification of the substrate types, both k-nearest neighbour and convolutional neural network (CNN) approaches were examined, the latter contrasting shallow and deep networks and utilised the popular pre-trained VGG19 with transfer learning.</p><p>The contribution from the ISEC <ref type="bibr" coords="10,293.14,321.80,10.52,8.74" target="#b8">[9]</ref> team took a traditional computer vision approach. The authors established that colour alone is not a suitable measure for classification and established a feature vector that encapsulated colour and texture information: the mean, standard deviation, entropy of a grey-scale version of the original colour image, plus a hue ratio which measures colour content. All of these were calculated in a 5 × 5 region around each pixel. Using this feature vector, the authors explored a variety of machine learning algorithms (e.g., nearest neighbour, decision trees, discriminant analysis and support vector machines). They found that the most effective machine learning algorithm was random forests.</p><p>The VIT submission <ref type="bibr" coords="10,241.07,464.84,15.50,8.74" target="#b9">[10]</ref> approaches the annotation task using CNNs, which have been applied to a wide range of image classification tasks in recent years. A standard CNN would fare badly on this task because it is tuned to the entire image containing a single instance of the object to be classified, so they have used a variant known as Faster R-CNN. It overcomes the issue by extracting "region proposals" from the image, then using an algorithm to combine them into those passed on to the CNN. They have explored layering this on top of three existing CNNs: NASnet, Inception V2, and Resnet101. All these models are provided with the Tensorflow object detection API and are provided pre-trained using the COCO dataset, which contains 300,000 images in 80 categories of natural and man-made objects. Using the ImageCLEF training images, the authors fine-tuned the ability of the various networks to perform the classification task. They also explored the effect of augmenting the networks' training dataset by adding noise, changing the brightness and contrast, and performing geometrical distortions such as shears, shifts, rotations and mirror imaging. CNNs currently represent the state-of-the-art in machine learning for image classification, and this use of well-known techniques provides a good benchmark. 27497 ISEC 0.0198 0.0 0.0007 0.0121 0 0.0 0.0 0.0079 0.0 0.0277 0.0 0 0.0 27421 HHUD 0.0007 0.0 0.0167 0.0048 0 0.0006 0.0106 0.0571 0.0 0.0072 0.0 0 0.0 27419 HHUD 0.0261 0.0 0.0315 0.0042 0 0.0023 0.0228 0.0423 0.0005 0.0088 0.0304 0 0.0012 27418 HHUD 0.0246 0.0 0.0321 0.0038 0 0.0 0.0269 0.0447 0.0005 0.0086 0.0318 0 0.0012 27417 HHUD 0.0356 0.0 0.033 0.0069 0 0.0 0.0406 0.0505 0.0008 0.0094 0.0213 0 0.0157 27416 HHUD 0.0346 0.0 0.0343 0.0064 0 0.0 0.0437 0.055 0.0008 0.0094 0.0222 0 0.0158 27415 HHUD 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0731 0.0 0.0 0.0 0 0.0 27414 HHUD 0.0089 0.0 0.016 0.0015 0 0.0 0.0 0.0562 0.0 0.0104 0.0054 0 0.0 27413 HHUD 0.0068 0.0021 0.0063 0.0014 0 0.0022 0.0 0.0523 0.0 0.0063 0.0 0 0.0 27398 HHUD 0.0013 0.0 0.0116 0.0 0 0.0 0.0 0.0702 0.0 0.002 0.0 0 0.0 27350 VIT 0.0597 0.0 0.0305 0.0141 0 0.0 0.0422 0.0808 0.0 0.0299 0.0598 0 0.0 27349 VIT 0.0637 0.0 0.1012 0.0195 0 0.0028 0.0758 0.0804 0.0 0.0329 0.0619 0 0.0004 27348 VIT 0.0548 0.0 0.0956 0.0171 0 0.0129 0.119 0.0782 0.0 0.0365 0.0579 0 0.0 27347 VIT 0.0456 0.0 0.0374 0.0055 0 0.0 0.0204 0.0918 0.0 0.0239 0.0498 0 0.0 27115 VIT 0.0436 0.0 0.0809 0.0168 0 0.0128 0.0664 0.0722 0.0 0.0349 0.0526 0 0.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Coral reef image pixel-wise parsing (subtask 2)</head><p>Tables <ref type="table" coords="11,165.88,416.38,4.98,8.74" target="#tab_3">4</ref> and<ref type="table" coords="11,193.14,416.38,4.98,8.74" target="#tab_4">5</ref> present the performance of the participants on the coral reef image pixel-wise parsing subtask. Five runs were submitted in this subtask by three teams. In this subtask, a team comprising of researchers at Filament working with the University of Essex <ref type="bibr" coords="11,256.14,608.30,15.50,8.74" target="#b11">[12]</ref> achieved best results in terms of M AP 0.5 IoU . They developed a classification system based around Deeplab V3, a deep CNN that reduces the amount of post-processing necessary to deliver a final, semantic segmentation and classification. Their post-processing involved connectedcomponent labelling, morphological opening and closing to delete small regions  and polygon approximation, all reasonably conventional image processing functions.</p><p>The SOTON team used a Keras implementation of Deeplab V3+, pre-trained on the well-known Pascal VOC and fine-tuned for each class using a one-versusall, pixel-wise classifier which was trained on the ImageCLEF dataset, with the loss weighted by the ratio of pixels belonging to the class versus not. As is common with deep networks, the training data were augmented with rotation, flipping, shearing and elastic distortions. The result was then passed through a conditional random field (CRF), which allows groups of similar entities to be assigned the same label.</p><p>The HHUD <ref type="bibr" coords="12,205.23,419.28,15.50,8.74" target="#b10">[11]</ref> team also participated in this subtask and used a similar approach from subtask 1 (see Section 5.1).</p><p>HHUD and SOTON submitted self-intersecting polygons, like the one in Figure <ref type="figure" coords="12,166.20,455.30,3.87,8.74" target="#fig_11">9</ref>, which were excluded from the evaluation. The ultimate goal of this ImageCLEF task is the reconstruction of 3D models of coral reefs from images and using measurements of complexity, surface area and volume from the reconstructed models for advances in marine conservation monitoring. The 3D reconstruction process is known as visual structure from motion (SfM) and requires only a set of uncalibrated images of the object to work from. This has been used for a wide variety of reconstructions ranging from small archaeological finds to large-scale environments with some success. In the context of this work, reconstructions range from individual coral specimens (from about 100 images) up to entire reefs, the latter using an innovative multi-camera capture system. To assess the health of a coral reef, marine biologists need to measure individual specimens and it has been established that reconstructions are accurate enough for this to be possible. However, this cannot be automated because there is no easy way to establish which reconstructed points belong to which substrate type; indeed, this problem is substantially more difficult than the segmentation of images. Hence, we have conjectured that, given annotated image regions in 2D, it should be possible to carry the assigned class labels forward through the SfM processing pipeline, resulting in a 3D model in which individual substrates are labelled <ref type="bibr" coords="13,234.69,345.28,14.61,8.74" target="#b12">[13]</ref>. The 2D labelling task is what this ImageCLEF task addresses.</p><p>As such the task is different from similar image classification and marine substrate classification tasks <ref type="bibr" coords="13,265.47,381.15,12.45,8.74" target="#b13">[14]</ref><ref type="bibr" coords="13,277.92,381.15,4.15,8.74" target="#b14">[15]</ref><ref type="bibr" coords="13,282.07,381.15,12.45,8.74" target="#b15">[16]</ref>. Firstly, the images were collected using low-cost action cameras (approx. $200 per camera) with a fixed lens and firing on a three second lapse. The effect of this on the imagery is that there is some blurring (in some images this is quite bad), the colour balance is not always correct (as the camera adjusts the white balance automatically based on changing environmental variables) and final image quality is lower than what could be achieved using high-end action cameras or DSLRs which are more typically used in this type of research. However, all of the images used in the task are used for building the 3D model and therefore have useful information in the pipeline. Low cost cameras were used to show this approach could be replicated affordably for marine conservation projects.</p><p>Additionally, the distance and angle the camera was facing the reef was unpredictable due to how they were placed on the multicamera array. This meant that some images were close-range and downward facing whilst other images were oblique across the reef. This has a big impact on the number of objects in the field of view and the ability of annotators to label them.</p><p>Tables <ref type="table" coords="13,180.94,572.43,4.98,8.74" target="#tab_2">3</ref> and<ref type="table" coords="13,208.41,572.43,4.98,8.74" target="#tab_4">5</ref> shows how the accuracy of the proposed approach varied between benthic substrate types. In fact, a marine dataset is difficult to annotate due to the wide variety of growth forms of benthic organisms that causes errors in image hand-annotation. Sponges are widely varied in their morphology and can often look like both hard and soft coral species, particularly encrusting varieties. Branching and submassive hard coral growth forms can be easily mistaken for each other due to their similar morphologies and the difficulty of noting secondary branching from images alone. Branching hard coral and lobed soft coral are also difficult to distinguish. However, some morphologies are easier to detect, such as gorgonian sea fans and barrel sponges, hence these were annotated as separate categories.</p><p>Despite the difficulties, the participants applied a variety of machine learning and deep learning approaches with promising results. We noticed that in the coral reef image pixel-wise parsing subtask many self-intersecting polygons were submitted and the evaluation approach excluded this type of polygon which could cause the low performance in the subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>The first edition of the ImageCLEFcoral task required participants to automatically annotate and localize benthic substrate (such as hard coral, soft coral, algae and sponge) in a collection of images used for marine conservation monitoring. Five groups participated in the task with a variety of machine learning and deep learning approaches.</p><p>We hope that future editions of this task will include images from different geographical areas, meaning that the visual features of the substrate classes will be different; however, it may be possible to employ a cross-learning technique from training data from other regions. Additionally, we hope to develop methods for evaluating the subtasks based on insitu evaluation and photogrammetric evaluation, giving participants a richer set of metadata to use within their computer vision approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,177.59,515.93,260.18,7.86;3,169.35,296.97,276.60,207.45"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: An image from the ImageCLEFcoral 2019 task collection.</figDesc><graphic coords="3,169.35,296.97,276.60,207.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,137.65,519.34,340.05,7.86;5,134.77,284.47,345.83,223.36"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: A screenshot from the polygon annotation tool used for the manual labelling.</figDesc><graphic coords="5,134.77,284.47,345.83,223.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,134.77,223.41,140.56,7.86;6,134.77,234.36,135.24,7.86;6,134.77,245.32,137.84,7.86;6,134.77,115.84,172.91,96.06"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: A cropped screenshot of the annotation interface showing how dots are drawn around the object.</figDesc><graphic coords="6,134.77,115.84,172.91,96.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,307.68,223.12,172.92,7.86;6,307.68,234.08,172.91,7.86;6,307.68,245.04,78.45,7.86;6,307.68,116.12,172.91,95.49"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: A cropped screenshot of the annotation interface showing a completed polygon classified by colour.</figDesc><graphic coords="6,307.68,116.12,172.91,95.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,134.77,642.55,345.83,7.86;6,134.77,653.51,125.50,7.86;6,169.35,423.55,276.66,207.49"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: An example of a training image used by ImageCLEFcoral annotators to ensure they were annotating correctly.</figDesc><graphic coords="6,169.35,423.55,276.66,207.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,134.77,642.55,345.82,7.86;7,134.77,653.51,141.78,7.86;7,203.93,454.77,207.49,176.27"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Benthic substrate types distribution (per-pixel), excluding background, in the ImageCLEFcoral 2019 training set.</figDesc><graphic coords="7,203.93,454.77,207.49,176.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,149.71,118.99,330.88,8.74;8,134.77,130.95,345.83,8.74;8,134.77,142.90,345.82,8.74;8,134.77,154.86,248.04,8.74;8,307.68,185.33,172.92,131.87"><head>Figure 7</head><label>7</label><figDesc>Figure 7 presents the histograms showing the number of objects (i.e., distinct polygons) per image and Figure 8 presents the number of points per object completed by the annotators in the training set. This shows the substrate type imbalance in the data making the task more challenging.</figDesc><graphic coords="8,307.68,185.33,172.92,131.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,134.77,327.35,137.25,7.86;8,134.77,338.30,107.37,7.86;8,134.77,186.69,172.91,129.14"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Histogram representing the number objects per image.</figDesc><graphic coords="8,134.77,186.69,172.91,129.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="8,307.68,328.71,172.91,7.86;8,307.68,339.67,114.32,7.86"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Histogram representing the number points (or clicks) per object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="9,406.68,224.96,4.61,7.86;9,455.21,224.96,4.61,7.86;9,136.16,246.88,33.02,7.86;9,184.90,246.88,128.83,7.86;9,406.68,246.88,4.61,7.86;9,455.21,246.88,4.61,7.86;9,136.16,257.84,247.16,7.86;9,184.89,268.80,42.22,7.86;9,406.68,257.84,4.61,7.86;9,455.21,257.84,4.61,7.86;9,134.77,325.26,308.94,8.77"><head>5. 1</head><label>1</label><figDesc>Coral reef image annotation and localisation (subtask 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="12,397.50,205.22,5.44,23.93;12,420.34,181.69,5.44,47.46;12,442.42,162.44,5.44,66.72;12,464.49,154.30,5.44,74.85;12,138.38,236.43,333.33,5.42;12,138.38,243.49,338.60,5.42;12,138.38,250.56,333.33,5.42;12,138.38,257.62,338.60,5.42;12,138.38,264.69,338.60,5.42"><head></head><label></label><figDesc>0958 0.0 0.1659 0.0446 0.0 0.0065 0.219 0.13 0.0186 0.0573 0.0889 0.0 0.0007 27343 SOTON 0.0235 0.0 0.023 0.0124 0.0 0.0012 0.048 0.1145 0.0 0.0108 0.0 0.0038 0.0 27324 SOTON 0.0262 0.0296 0.0183 0.0188 0.0138 0.0135 0.0025 0.0851 0.0254 0.0283 0.0135 0.0212 0.0167 27212 SOTON 0.0121 0.0047 0.0153 0.0188 0.0 0.0042 0.0021 0.0851 0.0 0.0 0.0156 0.0 0.0008</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="12,134.77,642.55,345.83,7.86;12,134.77,653.51,335.94,7.86;12,198.29,487.03,218.78,144.01"><head>Fig. 9 :</head><label>9</label><figDesc>Fig.9: An example of a self-intersecting polygon submitted to the ImageCLEFcoral coral reef image pixel-wise parsing subtask that were excluded from the evaluation.</figDesc><graphic coords="12,198.29,487.03,218.78,144.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,136.16,161.58,344.22,33.37"><head>Table 1 :</head><label>1</label><figDesc>Participating groups</figDesc><table coords="9,136.16,187.08,344.22,7.86"><row><cell>Team</cell><cell>Institution</cell><cell># Runs T1 # Runs T2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,134.77,398.96,345.82,213.71"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table coords="9,190.06,451.38,220.25,161.29"><row><cell>27417 HHUD</cell><cell>0.2427</cell><cell>0.1309</cell><cell>0.4877</cell></row><row><cell>27416 HHUD</cell><cell>0.2294</cell><cell>0.1307</cell><cell>0.5010</cell></row><row><cell>27419 HHUD</cell><cell>0.2199</cell><cell>0.1216</cell><cell>0.4421</cell></row><row><cell>27418 HHUD</cell><cell>0.2100</cell><cell>0.1216</cell><cell>0.4547</cell></row><row><cell>27349 VIT</cell><cell>0.1400</cell><cell>0.0682</cell><cell>0.4310</cell></row><row><cell>27348 VIT</cell><cell>0.1344</cell><cell>0.0723</cell><cell>0.4240</cell></row><row><cell>27115 VIT</cell><cell>0.0849</cell><cell>0.0456</cell><cell>0.4240</cell></row><row><cell>27350 VIT</cell><cell>0.0483</cell><cell>0.0287</cell><cell>0.2871</cell></row><row><cell>27347 VIT</cell><cell>0.0410</cell><cell>0.0274</cell><cell>0.2716</cell></row><row><cell>27421 HHUD</cell><cell>0.0026</cell><cell>0.0037</cell><cell>0.2051</cell></row><row><cell>27414 HHUD</cell><cell>0.0029</cell><cell>0.0043</cell><cell>0.2284</cell></row><row><cell>27415 HHUD</cell><cell>0.0027</cell><cell>0.0045</cell><cell>0.2910</cell></row><row><cell>27398 HHUD</cell><cell>0.0026</cell><cell>0.0043</cell><cell>0.2715</cell></row><row><cell>27413 HHUD</cell><cell>0.0021</cell><cell>0.0030</cell><cell>0.2028</cell></row><row><cell>27497 ISEC</cell><cell>0.0006</cell><cell>0.0006</cell><cell>0.0006</cell></row></table><note coords="9,172.91,398.96,307.68,7.86;9,134.77,409.92,264.16,7.86;9,190.06,435.40,234.27,7.89"><p>Coral reef image annotation and localisation performance in terms of the following scores: M AP 0.5 IoU ; R 0.5 IoU ; and M AP 0 IoU . Run id team M AP 0.5 IoU R 0.5 IoU M AP 0 IoU</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,134.77,117.38,345.82,123.43"><head>Table 3 :</head><label>3</label><figDesc>Coral reef image annotation and localisation performance in terms of the Intersection over Union (IoU) per benthic substrate.</figDesc><table coords="11,136.16,152.30,336.29,88.51"><row><cell>Run id Team</cell><cell>hard-coral-branching</cell><cell>hard-coral-submassive</cell><cell>hard-coral-boulder</cell><cell>hard-coral-encrusting</cell><cell>hard-coral-table</cell><cell>hard-coral-foliose</cell><cell>hard-coral-mushroom</cell><cell>soft-coral</cell><cell>soft-coral-gorgonian</cell><cell>sponge</cell><cell>sponge-barrel</cell><cell>fire-coral-millepora</cell><cell>algae-macro-or-leaves</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,134.77,473.74,345.82,104.12"><head>Table 4 :</head><label>4</label><figDesc>Coral reef image pixel-wise parsing performance in terms of the following scores: M AP 0.5 IoU ; R 0.5 IoU ; and M AP 0 IoU .</figDesc><table coords="11,187.43,510.17,239.52,67.69"><row><cell cols="4">Run id team M AP 0.5 IoU R 0.5 IoU M AP 0 IoU</cell></row><row><cell>27505 HHUD</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>27500 MTRU</cell><cell>0.0419</cell><cell>0.049</cell><cell>0.2398</cell></row><row><cell>27343 SOTON</cell><cell>0.0004</cell><cell>0.0015</cell><cell>0.0484</cell></row><row><cell>27324 SOTON</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0899</cell></row><row><cell>27212 SOTON</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0712</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,134.77,117.38,345.83,112.97"><head>Table 5 :</head><label>5</label><figDesc>Coral reef image pixel-wise parsing performance in terms of the Intersection over Union (IoU) per benthic substrate type.</figDesc><table coords="12,138.38,151.89,243.26,78.46"><row><cell>Run id team</cell><cell>hard-coral-branching</cell><cell>hard-coral-submassive</cell><cell>hard-coral-boulder</cell><cell>hard-coral-encrusting</cell><cell>hard-coral-table</cell><cell>hard-coral-foliose</cell><cell>hard-coral-mushroom</cell><cell>soft-coral</cell><cell>soft-coral-gorgonian</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="1,144.73,613.61,117.68,7.47"><p>http://www.imageclef.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="1,144.73,624.57,136.51,7.47;1,144.73,634.61,335.86,8.13;1,144.73,645.84,335.87,7.86;1,144.73,656.80,125.35,7.86"><p>http://www.clef-campaign.org/ Copyright c 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CLEF 2019, 9-12 September 2019, Lugano, Switzerland.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The work of <rs type="person">Antonio Campello</rs> was supported by <rs type="funder">Innovate UK</rs>, <rs type="programName">Knowledge Transfer Partnership</rs> project <rs type="grantNumber">KTP010993</rs>, and hosted at <rs type="institution">Filament Consultancy Group Limited</rs>. The data collection was funded by a <rs type="grantName">University of Essex Impact Acceleration Account</rs> grant <rs type="grantNumber">ES/M500537/1</rs> with support from <rs type="person">Professor David Smith</rs> and <rs type="funder">Operation Wallacea</rs>. We would also like to thank the annotators <rs type="person">Edward Longford</rs>, <rs type="person">Ekin Yagis</rs>, <rs type="person">Nida Sae Yong</rs>, <rs type="person">Abigail Wink</rs>, <rs type="person">Gareth Naylor</rs>, <rs type="person">Hollie Hubbard</rs>, <rs type="person">Nicholas Adamson</rs>, <rs type="person">Laura Macrina</rs>, <rs type="person">James Burford</rs>, <rs type="person">Duncan O'Brien</rs>, <rs type="person">Deanna Atkins</rs>, <rs type="person">Hollie Sams</rs> and <rs type="person">Olivia Beatty</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jYDgpfG">
					<idno type="grant-number">KTP010993</idno>
					<orgName type="grant-name">University of Essex Impact Acceleration Account</orgName>
					<orgName type="program" subtype="full">Knowledge Transfer Partnership</orgName>
				</org>
				<org type="funding" xml:id="_87TxBjE">
					<idno type="grant-number">ES/M500537/1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,142.96,591.04,337.64,7.86;14,151.52,602.00,329.07,7.86;14,151.52,612.96,329.07,7.86;14,151.52,623.92,329.07,7.86;14,151.52,634.88,329.07,7.86;14,151.52,645.84,329.07,7.86;14,151.52,656.80,329.07,7.86;15,151.52,119.67,329.07,7.86;15,151.52,130.63,329.07,7.86;15,151.52,141.59,304.03,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,343.21,645.84,137.38,7.86;14,151.52,656.80,207.96,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cuevas Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,384.78,656.80,95.81,7.86;15,151.52,119.67,329.07,7.86;15,151.52,130.63,234.98,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="15,151.52,141.59,168.93,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF 2019)<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="15,142.96,152.55,337.64,7.86;15,151.52,163.51,329.07,7.86;15,151.52,174.47,298.72,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,304.29,163.51,176.31,7.86;15,151.52,174.47,122.40,7.86">Overview of the ImageCLEF 2016 scalable concept image annotation task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,295.95,174.47,86.07,7.86">CLEF Working Notes</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="254" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,185.43,337.63,7.86;15,151.52,196.39,329.07,7.86;15,151.52,207.34,304.95,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,236.76,196.39,243.83,7.86;15,151.52,207.34,163.96,7.86">Overview of the ImageCLEF 2015 scalable image annotation, localization and sentence generation task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,337.49,207.34,86.07,7.86">CLEF Working Notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,218.30,337.63,7.86;15,151.52,229.26,273.71,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,254.17,218.30,226.42,7.86;15,151.52,229.26,61.98,7.86">Overview of the ImageCLEF 2014 scalable concept image annotation task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,235.53,229.26,86.07,7.86">CLEF Working Notes</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="308" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,240.22,337.64,7.86;15,151.52,251.18,277.27,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,309.53,240.22,171.06,7.86;15,151.52,251.18,135.99,7.86">Overview of the ImageCLEF 2013 scalable concept image annotation subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,309.82,251.18,86.07,7.86">CLEF Working Notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,262.14,337.64,7.86;15,151.52,273.10,202.97,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,260.51,262.14,220.08,7.86;15,151.52,273.10,61.98,7.86">Overview of the ImageCLEF 2012 scalable web image annotation task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,235.53,273.10,86.07,7.86">CLEF Working Notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,284.06,337.64,7.86;15,151.52,295.02,329.07,7.86;15,151.52,305.95,223.30,7.89" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,352.10,284.06,128.49,7.86;15,151.52,295.02,329.07,7.86;15,151.52,305.98,79.19,7.86">Cost and time-effective method for multi-scale measures of rugosity, fractal dimension, and vector dispersion from coral reef 3d models</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Exton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,239.16,305.98,47.73,7.86">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,316.93,337.64,7.86;15,151.52,327.89,329.07,7.86;15,151.52,338.83,300.12,7.89" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,328.67,316.93,151.93,7.86;15,151.52,327.89,325.18,7.86">Quality assurance in the identification of deep-sea taxa from video and image analysis: response to henry and roberts</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">D</forename><surname>Bullimore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">L</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,151.52,338.85,210.67,7.86">ICES Journal of Marine Science: Journal du Conseil</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="899" to="906" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,349.81,337.64,7.86;15,151.52,360.77,329.07,7.86;15,151.52,371.73,183.16,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="15,310.55,349.81,170.04,7.86;15,151.52,360.77,101.08,7.86">Automatic classification of coral images using color and textures</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M R</forename><surname>Caridade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R S</forename><surname>Marcal</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org&gt;" />
		<imprint>
			<date type="published" when="2019-12">September 9-12 2019</date>
			<biblScope unit="volume">2380</biblScope>
			<pubPlace>Lugano, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,382.69,337.98,7.86;15,151.52,393.65,329.07,7.86;15,151.52,404.61,183.16,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="15,348.71,382.69,131.88,7.86;15,151.52,393.65,106.15,7.86">Coral reef annotation and localization using faster r-cnns</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Jaisakthi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mirunalini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Aravindan</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org&gt;" />
		<imprint>
			<date type="published" when="2019-12">September 9-12 2019</date>
			<biblScope unit="volume">2380</biblScope>
			<pubPlace>Lugano, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,415.56,337.98,7.86;15,151.52,426.52,329.07,7.86;15,151.52,437.48,216.20,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="15,312.33,415.56,168.27,7.86;15,151.52,426.52,143.34,7.86">A two-stage approach for localization and classification of coral reef structures</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bogomasov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grawe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Conrad</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org&gt;" />
		<imprint>
			<date type="published" when="2019-12">September 9-12 2019</date>
			<biblScope unit="volume">2380</biblScope>
			<pubPlace>Lugano, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,448.44,337.97,7.86;15,151.52,459.40,329.07,7.86;15,151.52,470.36,329.07,7.86;15,151.52,481.32,34.30,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="15,417.24,448.44,63.35,7.86;15,151.52,459.40,290.28,7.86">Deep segmentation: Using deep convolutional networks for coral reef pixel-wise parsing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Steffens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ravenscroft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hagras</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org&gt;" />
		<imprint>
			<date type="published" when="2019-12">September 9-12 2019</date>
			<biblScope unit="volume">2380</biblScope>
			<pubPlace>Lugano, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,492.28,337.98,7.86;15,151.52,503.24,329.07,7.86;15,151.52,514.17,329.07,7.89;15,151.52,525.13,103.15,7.89" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,298.87,492.28,181.72,7.86;15,151.52,503.24,191.72,7.86">Semantic photogrammetry boosting imagebased 3d reconstruction with semantic labeling</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stathopoulou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Remondino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,353.08,503.24,127.51,7.86;15,151.52,514.17,329.07,7.89">ISPRS -International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences XLII</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="685" to="690" />
			<date type="published" when="2019-01">01 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,536.11,337.98,7.86;15,151.52,547.07,329.07,7.86;15,151.52,558.00,307.98,7.89" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,179.68,547.07,300.91,7.86;15,151.52,558.03,203.69,7.86">Semi-automated image analysis for the assessment of megafaunal densities at the Arctic deep-sea observatory HAUSGARTEN</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schoening</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Purser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dannheim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gutt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">W</forename><surname>Nattkemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,365.50,558.03,45.17,7.86">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,568.99,337.98,7.86;15,151.52,579.95,329.07,7.86;15,151.52,590.88,225.34,7.89" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,437.29,568.99,43.31,7.86;15,151.52,579.95,329.07,7.86;15,151.52,590.91,17.54,7.86">Do experts make mistakes? A comparison of human and machine identification of dinoflagellates</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Culverhouse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Reguera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Herry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>González-Gil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,176.66,590.91,126.44,7.86">Marine Ecology Progress Series</title>
		<imprint>
			<biblScope unit="volume">247</biblScope>
			<biblScope unit="page" from="17" to="25" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,601.87,337.98,7.86;15,151.52,612.82,329.07,7.86;15,151.52,623.78,329.07,7.86;15,151.52,634.74,75.18,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="15,435.28,601.87,45.31,7.86;15,151.52,612.82,152.23,7.86">Automated annotation of coral reef survey images</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Edmunds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">I</forename><surname>Kline</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">G</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,326.49,612.82,154.10,7.86;15,151.52,623.78,246.64,7.86">Proceedings of the 25th IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;12)</title>
		<meeting>the 25th IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;12)<address><addrLine>Providence, Rhode Island</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
