<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.66,115.96,334.04,12.62;1,254.51,133.89,106.33,12.62">Overview of the ImageCLEFmed 2019 Concept Detection Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,195.87,172.25,61.02,8.74"><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
							<email>obioma.pelka@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Diagnostic and Interventional Radiology and Neuroradiology</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.01,172.25,52.53,8.74;1,135.57,184.21,38.66,8.74"><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
							<email>christoph.friedrich@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.35,184.21,106.98,8.74"><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Essex</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,234.29,196.16,68.10,8.74"><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<email>henning.mueller@hevs.ch</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.66,115.96,334.04,12.62;1,254.51,133.89,106.33,12.62">Overview of the ImageCLEFmed 2019 Concept Detection Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">86E4E9E4EA9B947CDD863A46062D2A0F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Concept Detection</term>
					<term>Computer Vision</term>
					<term>ImageCLEF 2019</term>
					<term>Image Understanding</term>
					<term>Radiology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the ImageCLEF 2019 Concept Detection Task. This is the 3rd edition of the medical caption task, after it was first proposed in ImageCLEF 2017. Concept detection from medical images remains a challenging task. In 2019, the format changed to a single subtask and it is part of the medical tasks, alongside the tuberculosis and visual question and answering tasks. To reduce noisy labels and limit variety, the data set focuses solely on radiology images rather than biomedical figures, extracted from the biomedical open access literature (PubMed Central). The development data consists of 56,629 training and 14,157 validation images, with corresponding Unified Medical Language System (UMLS R ) concepts, extracted from the image captions. In 2019 the participation is higher, regarding the number of participating teams as well as the number of submitted runs. Several approaches were used by the teams, mostly deep learning techniques. Long short-term memory (LSTM) recurrent neural networks (RNN), adversarial auto-encoder, convolutional neural networks (CNN) image encoders and transfer learning-based multi-label classification models were the frequently used approaches. Evaluation uses F1-scores computed per image and averaged across all 10,000 test images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The concept detection task presented in this paper is part of the ImageCLEF 1 benchmarking campaign, that is part of the Cross Language Evaluation Forum 2 (CLEF). ImageCLEF was first held in 2003 and in 2004 a medical task was added that has been held every year <ref type="bibr" coords="2,300.49,178.34,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="2,317.65,178.34,7.01,8.74" target="#b5">6]</ref>. More information regarding other proposed tasks in 2019 can be found in <ref type="bibr" coords="2,308.75,190.30,9.96,8.74" target="#b4">[5]</ref>.</p><p>The caption task was first proposed in 2016 as a caption prediction task. In 2017, the caption task was split into two subtasks: concept detection and caption prediction and ran in that format at ImageCLEFcaption 2017 <ref type="bibr" coords="2,410.24,226.16,10.52,8.74" target="#b0">[1]</ref> and 2018 <ref type="bibr" coords="2,467.31,226.16,9.96,8.74" target="#b3">[4]</ref>. The format has slightly changed in 2019 with a single task.</p><p>The motivation for this task is that an increasing number of images has become available without metadata, so obtaining some metadata is essential to make the content usable. The objective is to develop systems capable of predicting concepts automatically for radiology images, or possibly for other clinical images. These predicted concepts enable order for unlabeled and unstructured radiology images and for data sets lacking metadata, as multi-modal approaches prove to obtain better results regarding image classification <ref type="bibr" coords="2,391.83,321.80,14.61,8.74" target="#b11">[12]</ref>. As the interpretation and summarization of knowledge from medical images such as radiology output is time-consuming, there is a considerable need for automatic methods that can approximate this mapping from visual information to condensed textual descriptions. The more image characteristics are known, the more structured are the radiology scans and hence, the more efficient are the radiologists regarding interpretation.</p><p>For development data, a subset of the Radiology Object in COntext data set (ROCO) <ref type="bibr" coords="2,190.95,417.44,15.50,8.74" target="#b10">[11]</ref> is used. ROCO contains radiology images originating from the PubMed Central (PMC) Open Access Subset 3 <ref type="bibr" coords="2,337.10,429.40,14.61,8.74" target="#b13">[14]</ref>, with several Unified Medical Language System (UMLS R ) Concept Unique Identifiers (CUIs) per image. The test set used for official evaluation was created in the same manner as proposed in Peltka et al. <ref type="bibr" coords="2,202.43,465.27,14.61,8.74" target="#b10">[11]</ref>.</p><p>This paper presents an overview of the ImageCLEFmed Concept Detection Task 2019 with task description and participating teams in Section 2, an exploratory analysis on the data set and ground truth described in Section 3 and the evaluation framework explained in Section 4. The approaches applied by the participating teams are listed in Section 5, which is followed by discussion and conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task and Participation</head><p>Succeeding the previous subtasks in ImageCLEFcaption 2017 <ref type="bibr" coords="2,415.17,591.93,10.52,8.74" target="#b0">[1]</ref> and Image-CLEFcaption 2018 <ref type="bibr" coords="2,219.58,603.88,9.96,8.74" target="#b3">[4]</ref>, a concept detection task with the objective of extracting UMLS R CUIs from radiology images was proposed. We work on the basis of a large-scale collection of figures from biomedical open access journal articles (PMC). All images in the training data are accompanied by UMLS R concepts extracted from the original image caption. An example of an image from the training set with the extracted concepts is shown in Figure <ref type="figure" coords="3,395.57,154.86,3.87,8.74" target="#fig_0">1</ref>. In comparison to the previous tasks, the following improvements were made:</p><p>-To reduce the variety of content and focus the scenario, the images in the distributed collection are limited to radiology images.</p><p>-The number of concepts was decreased by preprocessing the captions, prior to concept extraction. The proposed task is the first step towards automatic image captioning and scene understanding, by identifying the presence and location of relevant biomedical concepts (CUIs) in a large corpus of medical images. Based on the visual image content, this task provides the building blocks for the scene understanding step by identifying the individual components of which captions are composed. The concepts can be used for context-based image analysis and for information retrieval. The detected concepts per image are evaluated with precision and recall scores from the ground truth, as described in Section 4.</p><p>In Table <ref type="table" coords="3,190.23,596.34,3.87,8.74" target="#tab_1">2</ref>, the 11 participating teams of the ImageCLEFmed Concept Detection task are listed. There were 49 registered participants out of 99 teams, who downloaded the End-User-Agreement. Altogether, 77 runs were submitted for evaluation. Out of the 77 submitted runs, 60 were graded and 17 were faulty submission. The majority of the participating teams are new to the task, as only three groups participated in the previous years. The training and validation sets containing 56,629 and 14,157 images were subsets of the ROCO data set presented in Peltka et al. <ref type="bibr" coords="4,390.25,499.55,14.61,8.74" target="#b10">[11]</ref>. ROCO has two classes: Radiology and Out-Of-Class. The first contains 81,825 radiology images, which was used for the presented work. It includes several medical imaging modalities such as, Computed Tomography (CT), Ultrasound, X-Ray, Fluoroscopy, Positron Emission Tomography (PET), Mammography, Magnetic Resonance Imaging (MRI), Angiography and PET-CT, and can be seen in Figure <ref type="figure" coords="4,472.84,559.32,3.87,8.74" target="#fig_1">2</ref>.</p><p>From the PMC Open Access subset <ref type="bibr" coords="4,306.73,572.43,14.61,8.74" target="#b13">[14]</ref>, a total of 6,031,814 image -caption pairs were extracted. Compound figures, which are images with more than one subfigure, were removed using deep learning as proposed in Koitka et al. <ref type="bibr" coords="4,467.31,596.34,9.96,8.74" target="#b6">[7]</ref>. The non-compound images were further split into radiology and non-radiology, as the objective was solely on radiology. Semantic knowledge of object interplay present in the images were extracted in the form of UMLS R Concepts using the QuickUMLS library <ref type="bibr" coords="4,223.13,644.16,14.61,8.74" target="#b16">[17]</ref>. The image captions from the biomedical articles served as basis for the extraction of the concepts. The text pre-processing steps applied are described in Peltka et al. <ref type="bibr" coords="5,267.42,118.99,14.61,8.74" target="#b10">[11]</ref>. Figure <ref type="figure" coords="5,321.87,118.99,4.98,8.74" target="#fig_1">2</ref> displays example images from the training set, containing several radiology imaging modalities.  Examples of concepts in the training set are listed in descending order of occurence in Table <ref type="table" coords="6,219.41,130.95,3.87,8.74" target="#tab_1">2</ref>. A few concepts were labelled only once, as can be seen in Figure <ref type="figure" coords="6,166.20,142.90,3.87,8.74" target="#fig_2">3</ref>.</p><p>ROCO contains images from the PMC archive extracted in January 2018, which makes up the training set for the ImageCLEF Concept Detection Task.</p><p>To avoid an overlap with images distributed at previous ImageCLEF medical tasks, the test set for ImageCLEF 2019 was created with a subset of PMC Open Access (archiving date: 01.02.2018 -01.02.2019). The same procedures applied for the creation of the ROCO data set were applied for the test set as well.</p><p>Concepts with very high frequency (&gt;13,000), such as "Image", as well as redundant synonyms were removed. This lead to reduction of concepts per image in comparison to the previous years. All images in the training, validation and test sets have ,  and  concepts, respectively. 4 Evaluation Methodology UMLS R CUIs need to be automatically predicted by the participating teams for all 10,000 test images. As in previous editions <ref type="bibr" coords="7,338.05,165.37,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="7,350.24,165.37,7.01,8.74" target="#b3">4]</ref>, the balanced precision and recall trade-off in terms of F1-scores was measured. The default implementation of the Python scikit-learn (v0.17.1-2) library was applied to compute the F-scores per image and average them across all test images. As the training, validation and test set contain a maximum of 72, 77 and 34 concepts per image, the maximum number of concepts allowed in the submission runs was set to 100. Each participating group could submit altogether 10 valid and 7 faulty submission runs. Faulty submissions include:</p><p>-Same image id more than once -Wrong image id -Too many concepts -Same concept more than once -Not all test images included All submission runs were uploaded by the participating teams and evaluated with CrowdAI<ref type="foot" coords="7,197.24,372.32,3.97,6.12" target="#foot_0">4</ref> . The source code of the evaluation tool is available on the task Web page<ref type="foot" coords="7,177.38,384.27,3.97,6.12" target="#foot_1">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>This section details the results achieved by all 11 participating teams for the concept detection task. The best run per team is shown in Table <ref type="table" coords="7,434.74,474.61,3.87,8.74" target="#tab_2">3</ref>. Table <ref type="table" coords="7,475.61,474.61,4.98,8.74" target="#tab_3">4</ref> contains the complete list of all graded submission runs. There is an improvement compared to both previous editions, from 0.1583 in ImageCLEF 2017 <ref type="bibr" coords="7,449.84,498.52,10.52,8.74" target="#b0">[1]</ref> and 0.1108 in ImageCLEF 2018 <ref type="bibr" coords="7,256.94,510.47,10.52,8.74" target="#b3">[4]</ref> to 0.2823 this year in terms of F1-score.</p><p>Best results were achieved by the AUEB NLP Group [8] by applying convolutional neural network (CNN) image encoders that were combined either with image retrieval methods or feed-forward neural networks to predict the concepts for images in the test set. On the test set, this CheXNet-based system <ref type="bibr" coords="7,465.09,560.40,15.50,8.74" target="#b12">[13]</ref> achieved better results in terms of F1-score, while an ensemble of an k-NN image retrieval system with CheXNet performed better on the development data. AUEB NLP ranked 1st to 3rd place with 3 out of the 4 submitted runs. Damo <ref type="bibr" coords="8,179.80,447.77,15.50,8.74" target="#b18">[19]</ref> was the second ranked group with 9 runs and applied two distinct methods to address the concept detection task. The latest deep learning system ResNet-101 was used for a multi-label classification approach, as well as a CNN-RNN model framework with attention mechanisms. Due to the imbalanced concept distribution, the group applied several data filtering methods. This proved to be positive, as the best run was a combination of multi-label classification with a filtered and reduced data set.</p><p>A two-stage concept detection approach was presented by the third ranked group: ImageSem <ref type="bibr" coords="8,216.54,543.81,9.96,8.74" target="#b2">[3]</ref>. This included a medical image pre-classification and a transfer learning-based multi-label classification model. For the pre-classification step based on body parts, the semantic types of all CUIs from UMLS R were extracted to cluster the images into four body part related categories, including "chest", "abdomen", "head and neck" and "skeletal muscle". Prior to training of a multi-label classifier that was fine-tuned from the ImageNet data set, high frequency concepts were selected. The best run by ImageSem ranked 8 out of all submissions.</p><p>The pri2si17 team <ref type="bibr" coords="8,230.45,635.86,15.50,8.74" target="#b15">[16]</ref> participated for the first time in the concept detection task. They addressed the task as a multi label classification problem and limited the concepts to the most frequent 25 labels. UA.PT BioInformatics <ref type="bibr" coords="9,252.28,644.16,10.52,8.74" target="#b1">[2]</ref> was the overall fourth best team and ranked 16th with their best F1-score of 0.2058 out of all submissions. Two independent ap-proaches were applied to address the concept detection task. Image representations obtained with several feature extraction methods, such as color edge directivity descriptors (CEDD) and adversarial auto-encoder, as well as an endto-end approach using two deep learning architectures. The best score out of the 8 submitted runs was achieved with a simplenet A recurrent neural network (RNN) architecture was proposed by Sam Maksoud <ref type="bibr" coords="10,159.09,191.67,9.96,8.74" target="#b8">[9]</ref>. Soft attention and visual gating mechanisms are used to enable the network to dynamically regulate "where" and "when" to extract visual data for concept generation. Two runs were submitted for grading, with the score of 0.1749 ranked 22nd out of all submissions and the group was ranked 6th overall.</p><p>The 7th overall ranked group is AI600 with 7 graded submission runs. Multilabel classification based on Bag-of-Visual-Words model with color descriptors and logistic regression, using different SIFT (Scale-Invariant Feature Transform) descriptors as visual features were applied for the concept detection task. The best run with a combination of SIFT, C-SIFT, HSV-SIFT and RGB-SIFT visual descriptors achieved 0.1656261, which is the 26th out of all submissions.</p><p>MS-CSIRO <ref type="bibr" coords="10,203.29,313.10,15.50,8.74" target="#b14">[15]</ref> submitted 1 run for official evaluation. Relevant concepts were predicted with an approach based on a multi-label classification model using CNN. MS-CSIRO ranked as the 8th best team and their submitted run with the score 0.1435 ranked 36th.</p><p>Similar to team Damo, the deep learning system ResNet-101 was utilized as base model. pri2si17 are the ninth best ranked team. Three runs were submitted for grading, of which the best run achieved the score 0.0497 ranking 45th.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Conclusion</head><p>The results of the task in 2019 show that there is an improvement in the F1scores in this 3rd edition (best score 0.2823) in comparison to ImageCLEF 2017 and ImageCLEF 2018. In the previous years, the best scores were 0.1583 in 2017 and 0.1108 in 2018. There were several new teams participating for the 1st time, as well as 3 teams, who participated in all editions. In addition, an increased number of participating teams and submitted runs was noticed in 2019. This shows the interest in this challenging task.</p><p>Most submitted runs are based on deep learning techniques. Several methods such as concept filtering, data augmentation and image normalization were applied to optimize the input for the predicting systems. Long short-term memory (LSTM) recurrent neural networks (RNN), adversarial auto-encoder, CNN image encoders and transfer learning-based multi-label classification models were the frequently used approaches.</p><p>The focus this year was reduced from biomedical images to solely radiology images, which led to the reduction of extracted concepts from 111,155 to 5,528. However, there is still an unbalanced distribution of concepts, which shows to be challenging to most teams. This can be due to the different imaging modalities, as well as several body parts included in the data set. Medical data and diseases are also usually unbalanced with a few conditions happening very frequently and most being very rare.</p><p>In future work, an extensive review of the clinical relevance for the concepts in the development data should be explored. As the concepts originate from the natural language captions, not all concepts high clinical utility. Medical journals also have very different policies in terms of checking figure cations. We believe this will assist in creating more efficient systems for automated medical data analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,454.81,345.83,7.89;3,161.35,298.31,292.67,141.73"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of a radiology image with the corresponding extracted UMLS R CUIs.</figDesc><graphic coords="3,161.35,298.31,292.67,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,435.71,345.83,7.89;5,134.77,446.70,34.86,7.86;5,194.29,166.25,226.77,254.69"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of a radiology images displaying the broad content of the ROCO data set.</figDesc><graphic coords="5,194.29,166.25,226.77,254.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,633.11,345.83,7.89;5,136.73,644.10,343.86,7.86;5,134.77,655.05,215.51,7.86;5,170.51,504.96,274.33,113.38"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The frequency versus the number of UMLS R (Unified Medical Language System R ) Concept Unique Identifiers (CUIs) in the development data. For example, 416 concepts occurred 10-20 times in the training images.</figDesc><graphic coords="5,170.51,504.96,274.33,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,115.91,348.45,367.31"><head>Table 1 .</head><label>1</label><figDesc>Participating groups of the ImageCLEF 2019 Concept Detection Task. Teams with previous participation in 2018 are marked with an asterix.</figDesc><table coords="4,134.77,147.27,332.42,282.75"><row><cell>Team</cell><cell>Institution</cell><cell>Runs</cell></row><row><cell>AUEB NLP Group [8]</cell><cell>Department of Informatics</cell><cell>4</cell></row><row><cell></cell><cell>Athens University of Economics and Business</cell><cell></cell></row><row><cell>Damo [19]</cell><cell>Beihang University, Beijing, China</cell><cell>9</cell></row><row><cell>ImageSem* [3]</cell><cell>Institute of Medical Information</cell><cell>10</cell></row><row><cell></cell><cell>Chinese Academy of Medical Sciences</cell><cell></cell></row><row><cell cols="2">UA.PT Bioinformatics* [2] Biomedical Informatics Research Group</cell><cell>8</cell></row><row><cell></cell><cell>Universidade de Aveiro, Portugal</cell><cell></cell></row><row><cell>richard ycli</cell><cell>The Hong Kong University of Science and</cell><cell>5</cell></row><row><cell></cell><cell>Technology, Kowloon Hong Kong</cell><cell></cell></row><row><cell>Sam Maksoud [9]</cell><cell>The University of Queensland</cell><cell>2</cell></row><row><cell></cell><cell>Brisbane, Australia</cell><cell></cell></row><row><cell>AI600 [18]</cell><cell>University of International Business and</cell><cell>7</cell></row><row><cell></cell><cell>Economics, Beijing, China</cell><cell></cell></row><row><cell>MacUni-CSIRO [15]</cell><cell>Macquarie University, North Ryde</cell><cell>1</cell></row><row><cell></cell><cell>Sydney, Australia</cell><cell></cell></row><row><cell>pri2si17 [16]</cell><cell>Mentor Graphics LibreHealth</cell><cell>3</cell></row><row><cell></cell><cell>Uttar Pradesh, India</cell><cell></cell></row><row><cell>AILAB*</cell><cell>University of the Aegean</cell><cell>5</cell></row><row><cell></cell><cell>Samos, Greece</cell><cell></cell></row><row><cell>LIST</cell><cell>Faculty of Sciences and Techniques</cell><cell>6</cell></row><row><cell></cell><cell>Abdelmalek Essadi University, Morocco</cell><cell></cell></row><row><cell>3 Data Set</cell><cell></cell><cell></cell></row></table><note coords="4,134.77,450.57,348.45,8.74;4,134.77,462.53,345.83,8.74;4,134.77,474.48,88.78,8.74"><p>Equivalently to previous editions, the data set distributed for the ImageCLEFmed 2019 Concept Detection task originates from biomedical articles of the PMC Open Access subset.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,314.51,345.83,346.47"><head>Table 2 .</head><label>2</label><figDesc>UMLS R (An excerpt of Unified Medical Language System R ) Concept Unique Identifiers (CUIs) distributed for Tte ImageCLEF Concept Detection Task with their respective number of occurrence. The concepts were randomly chosen in a descending order. All listed concepts were distributed in the training set.</figDesc><table coords="6,151.82,367.78,305.80,293.19"><row><cell>CUI</cell><cell>Concept</cell><cell>Occurrence</cell></row><row><cell>C0441633</cell><cell>Scanning</cell><cell>6733</cell></row><row><cell>C0043299</cell><cell>Diagnostic radiologic examination</cell><cell>6321</cell></row><row><cell>C1962945</cell><cell>Radiographic imaging procedure</cell><cell>6318</cell></row><row><cell>C0040395</cell><cell>Tomography</cell><cell>6235</cell></row><row><cell>C0034579</cell><cell>Panoramic Radiography</cell><cell>6127</cell></row><row><cell>C0817096</cell><cell>Chest</cell><cell>5981</cell></row><row><cell>C0040405</cell><cell>X-Ray Computed Tomography</cell><cell>5801</cell></row><row><cell>C1548003</cell><cell>Diagnostic Service Section ID -Radiograph</cell><cell>5159</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>C0000726</cell><cell>Abdomen</cell><cell>2297</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>C2985765</cell><cell>Enhancement Description</cell><cell>1084</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>C0228391</cell><cell>Structure of habenulopeduncular tract</cell><cell>672</cell></row><row><cell>C0729233</cell><cell>Dissecting aneurysm of the thoracic aorta</cell><cell>652</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>C0771711</cell><cell>Pancreas extract</cell><cell>456</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>C1704302</cell><cell>Permanent premolar tooth</cell><cell>177</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>C0042070</cell><cell>Urography</cell><cell>67</cell></row><row><cell>C0085632</cell><cell>Apathy</cell><cell>67</cell></row><row><cell>C0267716</cell><cell>Incisional hernia</cell><cell>67</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>C0081923</cell><cell>Cardiocrome</cell><cell>1</cell></row><row><cell>C0193959</cell><cell>Tonsillectomy and adenoidectomy</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.77,115.91,345.82,300.89"><head>Table 3 .</head><label>3</label><figDesc>Performance of the participating teams at ImageCLEFmed 2019 Concept Detection Task. The best run per team is selected. Teams with previous participation in 2018 are marked with an asterix.</figDesc><table coords="8,136.16,156.49,339.81,260.32"><row><cell>Team</cell><cell>Institution</cell><cell>F1-Score</cell></row><row><cell>AUEB NLP Group [8]</cell><cell>Department of Informatics</cell><cell>0.2823094</cell></row><row><cell></cell><cell>Athens University of Economics and Business,</cell><cell></cell></row><row><cell></cell><cell>Greece</cell><cell></cell></row><row><cell>Damo [19]</cell><cell>Beihang University, Beijing, China</cell><cell>0.2655099</cell></row><row><cell>ImageSem* [3]</cell><cell>Institute of Medical Information</cell><cell></cell></row><row><cell></cell><cell>Chinese Academy of Medical Sciences, Beijing,</cell><cell></cell></row><row><cell></cell><cell>China</cell><cell></cell></row><row><cell cols="2">UA.PT Bioinformatics* [2] Biomedical Informatics Research Group</cell><cell>0.2058640</cell></row><row><cell></cell><cell>Universidade de Aveiro, Portugal</cell><cell></cell></row><row><cell>richard ycli</cell><cell>The Hong Kong University of Science and</cell><cell>0.1952310</cell></row><row><cell></cell><cell>Technology, Kowloon Hong Kong</cell><cell></cell></row><row><cell>Sam Maksoud [9]</cell><cell>The University of Queensland</cell><cell>0.1749349</cell></row><row><cell></cell><cell>Brisbane, Australia</cell><cell></cell></row><row><cell>AI600 [18]</cell><cell>University of International Business and</cell><cell>0.1656261</cell></row><row><cell></cell><cell>Economics, Beijing, China</cell><cell></cell></row><row><cell>MacUni-CSIRO [15]</cell><cell>Macquarie University, North Ryde</cell><cell>0.1435435</cell></row><row><cell></cell><cell>Sydney, Australia</cell><cell></cell></row><row><cell>pri2si17 [16]</cell><cell>Mentor Graphics LibreHealth</cell><cell>0.0496821</cell></row><row><cell></cell><cell>Uttar Pradesh, India</cell><cell></cell></row><row><cell>AILAB*</cell><cell>University of the Aegean</cell><cell>0.0202243</cell></row><row><cell></cell><cell>Samos, Greece</cell><cell></cell></row><row><cell>LIST</cell><cell>Faculty of Sciences and Techniques</cell><cell>0.0013269</cell></row><row><cell></cell><cell>Abdelmalek Essadi University, Morocco</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,154.65,121.03,294.73,499.63"><head>Table 4 :</head><label>4</label><figDesc>Concept detection performance in terms of F1-scores</figDesc><table coords="9,154.65,135.94,294.73,484.72"><row><cell>Group Name</cell><cell>Submission Run</cell><cell>F1-Score</cell></row><row><cell>AUEB NLP Group</cell><cell>s2 results.csv</cell><cell>0.2823094</cell></row><row><cell>AUEB NLP Group</cell><cell>ensemble avg.csv</cell><cell>0.2792511</cell></row><row><cell>AUEB NLP Group</cell><cell>s1 results.csv</cell><cell>0.2740204</cell></row><row><cell>Damo</cell><cell>test cat xi.txt</cell><cell>0.2655099</cell></row><row><cell>AUEB NLP Group</cell><cell>s3 results.csv</cell><cell>0.2639952</cell></row><row><cell>Damo</cell><cell>results.txt</cell><cell>0.2613895</cell></row><row><cell>Damo</cell><cell>first concepts detection result check.txt</cell><cell>0.2316484</cell></row><row><cell>ImageSem</cell><cell>F1TOP1.txt</cell><cell>0.2235690</cell></row><row><cell>ImageSem</cell><cell>F1TOP2.txt</cell><cell>0.2227917</cell></row><row><cell>ImageSem</cell><cell>F1TOP5 Pmax.txt</cell><cell>0.2216225</cell></row><row><cell>ImageSem</cell><cell>F1TOP3.txt</cell><cell>0.2190201</cell></row><row><cell>ImageSem</cell><cell>07Comb F1Top1.txt</cell><cell>0.2187337</cell></row><row><cell>ImageSem</cell><cell>F1TOP5 Rmax.txt</cell><cell>0.2147437</cell></row><row><cell>Damo</cell><cell>test tran all.txt</cell><cell>0.2134523</cell></row><row><cell>Damo</cell><cell>test cat.txt</cell><cell>0.2116252</cell></row><row><cell cols="2">UA.PT Bioinformatics simplenet.csv</cell><cell>0.2058640</cell></row><row><cell>richard ycli</cell><cell>testing result.txt</cell><cell>0.1952310</cell></row><row><cell>ImageSem</cell><cell>08Comb Pmax.txt</cell><cell>0.1912173</cell></row><row><cell cols="2">UA.PT Bioinformatics simplenet128x128.csv</cell><cell>0.1893430</cell></row><row><cell cols="2">UA.PT Bioinformatics mix-1100-o0-2019-05-06 1311.csv</cell><cell>0.1825418</cell></row><row><cell cols="2">UA.PT Bioinformatics aae-1100-o0-2019-05-02 1509.csv</cell><cell>0.1760092</cell></row><row><cell>Sam Maksoud</cell><cell>TRIAL 1.txt</cell><cell>0.1749349</cell></row><row><cell>richard ycli</cell><cell>testing result.txt</cell><cell>0.1737527</cell></row><row><cell cols="2">UA.PT Bioinformatics ae-1100-o0-2019-05-02 1453.csv</cell><cell>0.1715210</cell></row><row><cell cols="2">UA.PT Bioinformatics cedd-1100-o0-2019-05-03 0937-trim.csv</cell><cell>0.1667884</cell></row><row><cell>AI600</cell><cell>ai600 result weighing 1557061479.txt</cell><cell>0.1656261</cell></row><row><cell>Sam Maksoud</cell><cell>TRIAL 18.txt</cell><cell>0.1640647</cell></row><row><cell>richard ycli</cell><cell>testing result run4.txt</cell><cell>0.1633958</cell></row><row><cell>AI600</cell><cell>ai600 result weighing 1557059794.txt</cell><cell>0.1628424</cell></row><row><cell>richard ycli</cell><cell>testing result run3.txt</cell><cell>0.1605645</cell></row><row><cell>AI600</cell><cell>ai600 result weighing 1557107054.txt</cell><cell>0.1603341</cell></row><row><cell>AI600</cell><cell>ai600 result weighing 1557062212.txt</cell><cell>0.1588862</cell></row><row><cell>AI600</cell><cell>ai600 result weighing 1557062494.txt</cell><cell>0.1562828</cell></row><row><cell>AI600</cell><cell>ai600 result weighing 1557107838.txt</cell><cell>0.1511505</cell></row><row><cell>richard ycli</cell><cell>testing result run2.txt</cell><cell>0.1467212</cell></row><row><cell>MacUni-CSIRO</cell><cell>run1FinalOutput.txt</cell><cell>0.1435435</cell></row><row><cell>AI600</cell><cell>ai600 result rgb 1556989393.txt</cell><cell>0.1345022</cell></row><row><cell cols="2">UA.PT Bioinformatics simplenet64x64.csv</cell><cell>0.1279909</cell></row><row><cell cols="2">UA.PT Bioinformatics resnet19-cnn.csv</cell><cell>0.1269521</cell></row><row><cell>ImageSem</cell><cell>09Comb Rmax new.txt</cell><cell>0.1121941</cell></row><row><cell>Damo</cell><cell>test att 3 rl best.txt</cell><cell>0.0590448</cell></row><row><cell>Damo</cell><cell>test rl 5 result check.txt</cell><cell>0.0584684</cell></row><row><cell>Damo</cell><cell>test tran rl 5.txt</cell><cell>0.0567311</cell></row><row><cell>Damo</cell><cell>test tran 10.txt</cell><cell>0.0536554</cell></row><row><cell>pri2si17</cell><cell>submission 1.csv</cell><cell>0.0496821</cell></row><row><cell>AILAB</cell><cell>results v3.txt</cell><cell>0.0202243</cell></row><row><cell>AILAB</cell><cell>results v1.txt</cell><cell>0.0198960</cell></row><row><cell>AILAB</cell><cell>results v2.txt</cell><cell>0.0162458</cell></row><row><cell>pri2si17</cell><cell>submission 3.csv</cell><cell>0.0141422</cell></row><row><cell>AILAB</cell><cell>results v4.txt</cell><cell>0.0126845</cell></row><row><cell>LIST</cell><cell>denseNet pred all 0.55.txt</cell><cell>0.0013269</cell></row><row><cell>ImageSem</cell><cell>yu 1000 inception v3 top6.csv</cell><cell>0.0009450</cell></row><row><cell>ImageSem</cell><cell>yu 1000 resnet 152 top6.csv</cell><cell>0.0008925</cell></row><row><cell>LIST</cell><cell>denseNet pred all 0.6.txt</cell><cell>0.0003665</cell></row><row><cell>LIST</cell><cell>denseNet pred all.txt</cell><cell>0.0003400</cell></row><row><cell>LIST</cell><cell>predictionBR(LR).txt</cell><cell>0.0002705</cell></row><row><cell>LIST</cell><cell>denseNet pred all 0.6 50 0.04(max if null).txt</cell><cell>0.0002514</cell></row><row><cell>LIST</cell><cell>predictionCC(LR).txt</cell><cell>0.0002494</cell></row><row><cell>AILAB</cell><cell>results v0.txt</cell><cell>0</cell></row><row><cell>pri2si17</cell><cell>submission 2.csv</cell><cell>0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="7,144.73,623.92,319.12,7.86;7,144.73,634.88,259.22,7.86"><p>https://www.crowdai.org/challenges/imageclef-2019-caption-concept-detection-6812fec9-8c9e-40ad-9fb9-cc1721c94cc1 [last accessed: 02.06.2019]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="7,137.50,644.07,3.65,5.24;7,144.73,645.84,291.56,7.86;7,144.73,656.80,164.37,7.86"><p>5 https://www.imageclef.org/system/files/ImageCLEF-ConceptDetection-Evaluation.zip [last accessed: 02.06.2019]</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,260.82,337.64,7.86;11,151.52,271.78,329.07,7.86;11,151.52,282.74,329.07,7.86;11,151.52,293.70,329.07,7.86;11,151.52,304.66,100.60,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,389.79,260.82,90.81,7.86;11,151.52,271.78,329.07,7.86;11,151.52,282.74,25.50,7.86">Overview of imageclefcaption 2017 -image caption prediction and concept detection for biomedical images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1866/invitedpaper7.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,197.80,282.74,282.79,7.86;11,151.52,293.70,24.01,7.86">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,315.86,337.63,7.86;11,151.52,326.82,329.07,7.86;11,151.52,337.78,329.07,7.86;11,151.52,348.74,256.75,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,307.45,315.86,173.14,7.86;11,151.52,326.82,172.24,7.86">Informative and intriguing visual features: Ua.pt bioinformatics in imageclef caption</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Gonalves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pinho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Costa</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="11,369.10,326.82,111.49,7.86;11,151.52,337.78,123.39,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings</title>
		<title level="s" coord="11,289.13,337.78,34.70,7.86">CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-09">2019. September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,359.94,337.64,7.86;11,151.52,370.90,329.07,7.86;11,151.52,381.86,329.07,7.86;11,151.52,392.81,256.75,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,299.17,359.94,181.42,7.86;11,151.52,370.90,193.77,7.86">Imagesem at imageclefmed caption 2019 task: a two-stage medical concept detection strategy</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="11,369.06,370.90,111.53,7.86;11,151.52,381.86,123.39,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings</title>
		<title level="s" coord="11,289.13,381.86,34.70,7.86">CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,404.02,337.64,7.86;11,151.52,414.97,329.07,7.86;11,151.52,425.93,329.07,7.86;11,151.52,436.89,251.47,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,414.37,404.02,66.22,7.86;11,151.52,414.97,157.95,7.86">Overview of the imageclef 2018 caption prediction tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2125/invitedpaper4.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,330.87,414.97,149.73,7.86;11,151.52,425.93,175.01,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,448.09,337.63,7.86;11,151.52,459.05,329.07,7.86;11,151.52,470.01,329.07,7.86;11,151.52,480.97,329.07,7.86;11,151.52,491.93,329.07,7.86;11,151.52,502.89,329.07,7.86;11,151.52,513.85,329.07,7.86;11,151.52,524.81,329.07,7.86;11,151.52,535.76,329.07,7.86;11,151.52,546.72,216.27,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,268.29,502.89,212.30,7.86;11,151.52,513.85,124.23,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,297.67,513.85,182.92,7.86;11,151.52,524.81,329.07,7.86;11,151.52,535.76,122.46,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="11,305.55,535.76,171.07,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="11,142.96,557.93,337.63,7.86;11,151.52,568.88,329.07,7.86;11,151.52,579.84,329.07,7.86;11,151.52,590.80,329.07,7.86;11,151.52,601.73,13.17,7.89;11,181.40,601.76,23.04,7.86;11,221.14,601.76,28.16,7.86;11,266.00,601.76,214.59,7.86;11,151.52,612.72,212.03,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,323.31,568.88,157.28,7.86;11,151.52,579.84,329.07,7.86;11,151.52,590.80,164.06,7.86">Evaluating performance of biomedical image retrieval systems -an overview of the medical image retrieval task at imageclef 2004-2013</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compmedimag.2014.03.004</idno>
		<ptr target="https://doi.org/10.1016/j.compmedimag.2014.03.004" />
	</analytic>
	<monogr>
		<title level="j" coord="11,329.43,590.80,151.16,7.86">Comp. Med. Imag. and Graph</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="55" to="61" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,623.92,337.64,7.86;11,151.52,634.88,329.07,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,329.07,7.86;12,151.52,119.67,329.07,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,230.20,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,263.35,623.92,217.25,7.86;11,151.52,634.88,122.48,7.86">Optimized convolutional neural network ensembles for medical subfigure classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-65813-1</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-65813-1" />
	</analytic>
	<monogr>
		<title level="m" coord="11,385.42,645.84,95.17,7.86;11,151.52,656.80,329.07,7.86;12,151.52,119.67,99.67,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction at the 8th International Conference of the CLEF Association</title>
		<title level="s" coord="12,424.04,119.67,56.55,7.86;12,151.52,130.63,111.95,7.86">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lawless</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Kelly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Dublin, Ireland; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
			<biblScope unit="volume">10456</biblScope>
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,152.56,337.63,7.86;12,151.52,163.52,329.07,7.86;12,151.52,174.48,329.07,7.86;12,151.52,185.44,145.96,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,365.64,152.56,114.95,7.86;12,151.52,163.52,63.74,7.86">Aueb nlp group at imageclefmed caption</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulo</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="12,258.00,163.52,222.59,7.86;12,151.52,174.48,14.79,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-09">2019. September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,196.41,337.64,7.86;12,151.52,207.37,329.07,7.86;12,151.52,218.33,329.07,7.86;12,151.52,229.29,116.24,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,308.98,196.41,171.61,7.86;12,151.52,207.37,72.86,7.86">Recurrent attention networks for medical concept prediction</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maksoud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wiliem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lovell</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="12,245.35,207.37,231.02,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings</title>
		<title level="s" coord="12,156.48,218.33,34.70,7.86">CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,240.26,337.97,7.86;12,151.52,251.22,329.08,7.86;12,151.52,262.17,171.84,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="12,429.01,240.26,51.57,7.86;12,151.52,251.22,248.91,7.86">ImageCLEF, Experimental Evaluation in Visual Information Retrieval</title>
		<idno type="DOI">10.1007/978-3-642-15181-1</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-15181-1" />
		<editor>Müller, H., Clough, P.D., Deselaers, T., Caputo, B.</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,273.14,337.98,7.86;12,151.52,284.10,329.07,7.86;12,151.52,295.06,329.07,7.86;12,151.52,306.02,329.07,7.86;12,151.52,316.98,329.07,7.86;12,151.52,327.94,329.07,7.86;12,151.52,338.90,329.07,7.86;12,151.52,349.86,184.37,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,422.33,273.14,58.26,7.86;12,151.52,284.10,226.10,7.86;12,400.86,284.10,79.73,7.86;12,151.52,295.06,329.07,7.86;12,151.52,306.02,123.14,7.86">Intravascular Imaging and Computer Assisted Stenting -and -Large-Scale Annotation of Biomedical Data and Expert Label</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01364-620</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01364-620" />
	</analytic>
	<monogr>
		<title level="m" coord="12,280.77,306.02,199.82,7.86;12,151.52,316.98,230.85,7.86;12,461.90,316.98,18.69,7.86;12,151.52,327.94,153.75,7.86">Synthesis -7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop</title>
		<meeting><address><addrLine>LABELS; Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-16">2018. September 16, 2018. 2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Held in Conjunction with MICCAI 2018</note>
</biblStruct>

<biblStruct coords="12,142.62,360.83,337.98,7.86;12,151.52,371.79,329.07,7.86;12,151.52,382.74,329.07,7.86;12,151.52,393.70,329.07,7.86;12,151.52,404.66,306.99,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,305.14,360.83,175.46,7.86;12,151.52,371.79,196.43,7.86">Adopting semantic information of grayscale radiographs for image classification and retrieval</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.5220/0006732301790187</idno>
		<ptr target="https://doi.org/10.5220/0006732301790187" />
	</analytic>
	<monogr>
		<title level="m" coord="12,369.30,371.79,111.29,7.86;12,151.52,382.74,329.07,7.86;12,151.52,393.70,69.45,7.86">Proceedings of the 11th International Joint Conference on Biomedical Engineering Systems and Technologies (BIOSTEC 2018)</title>
		<meeting>the 11th International Joint Conference on Biomedical Engineering Systems and Technologies (BIOSTEC 2018)<address><addrLine>Funchal, Madeira, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>BIOIMAGING</publisher>
			<date type="published" when="2018">January 19-21, 2018. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,415.63,337.97,7.86;12,151.52,426.59,329.07,7.86;12,151.52,437.55,329.07,7.86;12,151.52,448.48,236.95,7.89" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="12,444.49,426.59,36.10,7.86;12,151.52,437.55,296.87,7.86">Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bagul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shpanskaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>CoRR abs/1711.05225</idno>
		<ptr target="http://arxiv.org/abs/1711.05225" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,459.48,337.98,7.86;12,151.52,470.44,329.07,7.86;12,151.52,481.37,265.56,7.89" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,212.82,459.48,242.65,7.86">PubMed Central: The GenBank of the published literature</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.98.2.381</idno>
		<ptr target="https://doi.org/10.1073/pnas.98.2.381" />
	</analytic>
	<monogr>
		<title level="j" coord="12,463.04,459.48,17.56,7.86;12,151.52,470.44,329.07,7.86">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="381" to="382" />
			<date type="published" when="2001-01">Jan 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,492.37,337.97,7.86;12,151.52,503.33,329.07,7.86;12,151.52,514.29,329.07,7.86;12,151.52,525.25,305.67,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,358.48,492.37,122.10,7.86;12,151.52,503.33,262.62,7.86">Biomedical concept detection in medical images: Mq-csiro at 2019 imageclefmed caption task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ho-Shon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hamey</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="12,437.46,503.33,43.13,7.86;12,151.52,514.29,184.04,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings</title>
		<title level="s" coord="12,347.55,514.29,34.70,7.86">CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,536.22,337.98,7.86;12,151.52,547.17,329.07,7.86;12,151.52,558.13,329.07,7.86;12,151.52,569.09,305.67,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,307.32,536.22,173.27,7.86;12,151.52,547.17,262.03,7.86">Full training versus fine tuning for radiology images concept detection task for the imageclef 2019 challenge</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Purkayastha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gichoya</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="12,437.46,547.17,43.13,7.86;12,151.52,558.13,184.04,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings</title>
		<title level="s" coord="12,347.55,558.13,34.70,7.86">CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,580.06,337.98,7.86;12,151.52,591.02,225.42,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,259.84,580.06,220.75,7.86;12,151.52,591.02,73.14,7.86">QuickUMLS: a fast, unsupervised approach for medical concept extraction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,245.75,591.02,102.52,7.86">MedIR Workshop, SIGIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,601.99,337.98,7.86;12,151.52,612.95,329.07,7.86;12,151.52,623.91,329.07,7.86;12,151.52,634.87,22.02,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,239.25,601.99,220.96,7.86">Ai600 lab at imageclef 2019 concept detection task</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,612.95,229.53,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings</title>
		<title level="s" coord="12,392.96,612.95,34.69,7.86">CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,645.84,337.97,7.86;12,151.52,656.80,329.07,7.86;13,151.52,119.67,329.07,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,116.24,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,419.04,645.84,61.55,7.86;12,151.52,656.80,329.07,7.86;13,151.52,119.67,48.83,7.86">Concept detection based on multi-label classification and image captioning approach -damo at imageclef</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="13,242.90,119.67,233.47,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings</title>
		<title level="s" coord="13,156.48,130.63,34.70,7.86">CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-09">2019. September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
