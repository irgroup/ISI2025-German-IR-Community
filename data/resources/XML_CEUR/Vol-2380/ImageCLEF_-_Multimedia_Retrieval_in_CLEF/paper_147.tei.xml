<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.62,152.67,311.73,12.64;1,248.57,170.67,98.04,12.64">Medical Visual Question Answering at Image CLEF 2019-VQA Med</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,176.42,209.82,52.24,8.96"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mohit.b.bansal@pwc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Pricewaterhouse Coopers US Advisory</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.49,209.82,60.86,8.96"><forename type="first">Tanmay</forename><surname>Gadgil</surname></persName>
							<email>tanmay.p.gadgil@pwc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Pricewaterhouse Coopers US Advisory</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.15,209.82,43.47,8.96"><forename type="first">Rishi</forename><surname>Shah</surname></persName>
							<email>rishi.s.shah@pwc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Pricewaterhouse Coopers US Advisory</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,366.16,209.82,52.38,8.96"><forename type="first">Parag</forename><surname>Verma</surname></persName>
							<email>parag.verma@pwc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Pricewaterhouse Coopers US Advisory</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.62,152.67,311.73,12.64;1,248.57,170.67,98.04,12.64">Medical Visual Question Answering at Image CLEF 2019-VQA Med</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0F0FFFD6D0B3C2BBBFEAF460DA9A6751</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>Sequence to Sequence Model</term>
					<term>Image CLEF 2019</term>
					<term>Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the submission created by PwC US-Advisory for the Medical Domain Visual Question Answering (VQA-Med) Task of Image CLEF 2019. The goal of the challenge was to create a Visual Question Answering System which uses medical images as context to generate answers. The VQA pipeline classifies the questions into two groups, the first group of questions involves giving answers from a fixed pool of predefined answer categories and the second group of questions involves generating answers based on the abnormality seen in the image. The first model uses question embeddings from the Universal Sentence Encoder and Image Embeddings from ResNet which are fed into an attention-based classifier to generate answers. The second model uses the same ResNet image embedding along with word embeddings from a Word2Vec model pre-trained on PubMed data which is used as an input to a sequence to sequence model which generates descriptions of abnormalities. This methodology helped us achieve reasonable results with a strict accuracy of 48% and a BLEU score of 53% on the challenge's test data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Technical advancements in the healthcare sector have helped in storing large number of health records electronically and this provides opportunities for the use of artificial intelligence (AI) in supporting clinical decision making and improving patient engagement. Visual Question answering (VQA) is one way in which the technical innovation in Artificial Intelligence can be used for the benefit of the healthcare sector. VQA is a relatively new approach that uses a combination of computer vision and natural language processing for designing the model. Given the image and question input, VQA technique processes both the visual and textual inputs to give relevant answer to the input question. Some interesting approaches have been published over the years on VQA, but for this challenge we have come up with a relatively simple architecture to best address the Image-CLEF 2019 problem. In this paper, we aim to explain the details about different models used for getting the image embeddings, question embeddings and answer embeddings and for finally training the model. In subsequent parts of this paper, we go on to discuss the possible applications of the VQA model in the industry and possible changes in model design for enhancing the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Literature Review</head><p>In the last few years, there have been tremendous advancements in the field of AI. From neural network to computer vision, the field has transformed through leaps and bounds <ref type="bibr" coords="2,203.48,369.43,11.68,9.94" target="#b0">[1]</ref>. Among other things, its ability to impact the value chain has been felt unequivocally <ref type="bibr" coords="2,248.20,383.95,11.77,9.94" target="#b1">[2]</ref>. Healthcare as a domain is very fragmented and the components in the value chain don't adhere to accepted standards <ref type="bibr" coords="2,328.71,413.13,11.69,9.94" target="#b2">[3]</ref>. Such discretization provides immense opportunity for consolidation and optimization in the Provider Market <ref type="bibr" coords="2,124.70,442.17,11.77,9.94" target="#b3">[4]</ref>. A physician scans scores of documents and images before arriving at a conclusion. Without having a heuristic process in place, he or she may have to often spend substantial amount of time sieving through the scans and vitals before arriving at a conclusion. Visual Question Answering (VQA) is a recent advancement in AI which tries to answer a set of questions about an image <ref type="bibr" coords="2,454.80,500.37,11.77,9.94" target="#b4">[5]</ref>. Tasks such as feature extraction, understanding question and generating answers are an important part of the VQA. Complex systems built to extract information out of images and scans need to incorporate VQA along with natural language processing to leverage the huge data generated at the patient and provider interface <ref type="bibr" coords="2,190.29,573.12,11.77,9.94" target="#b5">[6]</ref>. These will not only supplement the information at hand with the clinician to make better decisions but also enable better knowledge management.</p><p>Earlier work in VQA was concentrated mostly towards image captioning <ref type="bibr" coords="2,124.70,631.32,11.77,9.94" target="#b6">[7]</ref>. In most of them, deep learning methods such as CNN, LSTM, DMN had been used. The proposed model aims to find a link between the semantics of the text description and the features extracted from an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Description and Dataset</head><p>As part of the Image-CLEF 2019 VQA-Med Task <ref type="bibr" coords="3,355.39,176.80,18.44,9.94" target="#b11">[12]</ref> [13], the participants were given medical images accompanied by few clinically relevant questions, and were supposed to answer the questions based on the visual image content.</p><p>The training dataset consisted of 3200 medical images and 12,792 questionanswer pairs, while the validation set had 500 medical images and 2000 question-answer pairs. Finally, after developing the model framework, the participants were required to predict answers for a test set of 500 medical images with 500 questions  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Question Classifier Model</head><p>To classify the questions, question embeddings were passed through a dense connected layer. This deep representation was then projected using a final dense layer followed by a softmax activation to form a distribution over all possible tags. Question embeddings were created using the Universal Sentence Encoder as it gives strong transfer performance on a number of NLP tasks and surpasses the performance of transfer learning using word level embeddings alone. Also, it handled variable input text length seamlessly to give a 512-dimensional vector output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">VQA Classifier Model</head><p>Once the questions were classified in the question classifier, questions and images from the category of Modality, Plane and Organ System were fed to the VQA classifier. The VQA classifier model used a Universal Sentence Encoder to create Question embeddings. To encode the information of the category to which the question belongs, one-hot-encoded vector of the Question Classifier was concatenated to the 512-dimensional vector to create a final 516-dimensional question embedding vector. A pre-trained convolutional neural network (CNN) model based on ResNet50 architecture <ref type="bibr" coords="4,326.95,431.13,12.80,9.94" target="#b7">[8]</ref> was used to embed the image.</p><p>To compute attention over image features, we concatenated tiled LSTM state with image features over the depth dimension and passed it through a 1 × 1 dimensional convolution layer of depth 512 followed by ReLU nonlinearity. The output feature was passed through another 1 × 1 convolution of depth C= 2 followed by softmax over spatial dimensions to compute attention distributions. We used these distributions to compute two image glimpses by computing the weighted average of image features. We further concatenated the image glimpses with the state of the LSTM and passed it through a fully connected layer of size 1024 with ReLU nonlinearity <ref type="bibr" coords="4,317.69,562.05,17.01,9.94" target="#b9">[10]</ref>. The output was then fed to a linear layer of size M = 66 followed by softmax to produce probabilities over the different sub-categorical answers in the top 3 categories. We used dropout of 0.5 on input features of all layers including the LSTM, convolutions, and fully connected layers and optimized the model with Adam optimizer.  This is a custom encoder decoder architecture that was used to generate answer sequences. In the following section, this will be discussed in detail. The first component was a pre-trained convolutional neural network (CNN) model based on ResNet50 architecture that took the image as an input and extracted a vector representation for that image, while the second component was a word embedding layer that encoded the question into a vector representation which was passed through a LSTM network. The embeddings were created using pre-trained word2vec model on PubMed data. The decoder consisted of LSTM network that took the thought vector as initial state and 'Start of Sentence' &lt;SOS&gt; token as input in the first time step and tried to predict the answer tokens using softmax layer until 'End of Sentence' &lt;EOS&gt; was obtained <ref type="bibr" coords="6,221.21,346.99,16.90,9.94" target="#b10">[11]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Encoder</head><p>The encoder layer consists of 2 main components: the first is used to obtain image embeddings from a CNN architecture and the second is a LSTM network with a pre-trained word embedding layer to encode questions. In the first component, the image embedding input layer takes the vectorized image from ResNet50. The image is flattened and passed through a fully connected dense layer followed by a RELU activation to create a deep 512dimensional representation of the image. The main purpose of these two layers is to reshape and compress the feature vector dimension to match the hidden layer of the LSTM.</p><p>In the second component, the semantic meaning of the question is to be extracted with respect to the image. A 100-dimensional pre-trained word embedding layer is used to encode the word into a dense semantic space using a word2vec model trained on PubMed Data. This word representation is then fed to a LSTM network with 512 hidden nodes. LSTM is a special type of Recurrent Neural Network (RNN) that has been designed to solve the problem of vanishing gradient. The LSTM layer uses its memory cells to store the context information. LSTM has three gates (i.e. Input gate, forget gate and output gate) which will decide how the input will be handled. At any time step, inputs to the LSTM cell include the current word (x), previous hidden state (h-1) and previous memory state (c-1), and LSTM cell outputs are current hidden state (h) and current memory state (c). These states have 512 hidden nodes. At last time step in the sequence, the LSTM cell outputs its hidden state (h) and memory state (c). Both the hidden state and the memory state of the first LSTM layer is initialized with the image vector which helps LSTM layer learn the internal representations useful for extracting information relevant to answering the question. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Decoder</head><p>The decoder model is responsible for generating the answer from the input image and question. The Decoder LSTM uses three inputs to generate a token which is mapped to a dictionary. These include the token of the previous word, the hidden state and the memory state of the previous LSTM layer as shown in Fig 5 . At the first time step, LSTM cell takes 'Start of Sentence' &lt;SOS&gt; token, the hidden and memory state of the encoder model as the input and calculates the probability distribution of the target word using softmax layer. The word with the highest probability will be the first word of the answer; this word will be then passed to the second LSTM cell as input and predict the second word of the answer. The full answer will be generated by repeating this process until the model predicts 'End of Sentence' &lt;EOS&gt; token. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Training data and validation data for the challenge comprised of 3,200 images with 12,792 Question-Answer (QA) pairs &amp; 500 images with 2,000 QA pairs respectively. Test data contained 500 images and questions. We experimented with primarily 2 approaches for challenge submissions:</p><p>Approach 1: In the first experiment, we use the Question Classifier, followed by VQA Classifier on the top 3 categories and a second VQA Classifier model trained on Abnormality category train data only. We only consider top 657 most frequent answers in the abnormality classifier. Other answers are ignored and do not contribute to the loss during training. This covers 74% of the answers in the validation set of abnormality category in VQA dataset <ref type="bibr" coords="9,382.09,164.44,12.78,9.94" target="#b8">[9]</ref> Approach 2: In the second experiment, we use the proposed pipeline, Question Classifier, followed by VQA Classifier on the top 3 categories and VQA Seq-2-Seq Model on the Abnormality category questions</p><p>The question classifier model to classify the question category gave a 100% accuracy on the validation set. The VQA classifier trained on the first 3 categories (Modality, Plane and Organ Systems) delivered an overall 76.1% accuracy in predicting answers across 66 sub-categorical classes on validation data of the corresponding 3 categories. The second VQA classifier trained on the Abnormality category questions, used in Approach 1, delivered a 19.5% accuracy in predicting answers across 657 sub-categorical classes on the validation set of Abnormality category.</p><p>The VQA Seq-2-Seq model, used in Approach 2, generated answer sequences with an accuracy of 23.4% on validation set of Abnormality category. Evaluation of test data was conducted based on the following metrics:</p><p>1. Accuracy (Strict)-an adapted version of the accuracy metric from the general domain VQA task that considers exact matching of a participant provided answer and the ground truth answer.; 2. BLEU metric <ref type="bibr" coords="9,223.15,448.77,12.81,9.94" target="#b0">[1]</ref> to capture the similarity between a system-generated answer and the ground truth answer.; With different model hyper parameters, 5 valid submissions were given by our team; 2 from Approach 1 and 3 from Approach 2. The best results from both approaches were as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>VQA has great potential with respect to the automation of information gathering from images. The medical domain can be greatly benefited in several ways by these techniques, like providing the coherence required to simplify the diagnosis process, providing clinicians with a tool to check the validity of their diagnosis and providing patients with details which may otherwise be overlooked during a consultation. It can empower the clinician to ascertain that enough information is at hand before arriving at any conclusion. Specific models relevant for a particular disease can also be made by training the algorithm on specific data sets, and this can create a better scanning process. Supplementing the models with more advanced techniques like attention and Multitask learning can greatly enhance their accuracy. It can also create standards in terms of models used by different providers. Moreover, the cost benefits incurred from such simplification can be propagated through the entire value chain. Clinicians, patients and insurance providers can benefit greatly from the value created by VQA based diagnosis systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,124.70,312.97,6.00,10.80;3,153.02,312.97,184.51,10.80;3,124.70,338.23,345.82,9.94;3,124.70,352.75,345.75,9.94;3,124.70,367.39,346.00,9.94;3,124.70,381.91,345.89,9.94;3,124.70,396.43,345.72,9.94;3,124.70,410.97,345.70,9.94;3,124.70,425.49,346.00,9.94;3,124.70,440.13,345.63,9.94;3,124.70,454.65,345.84,9.94;3,124.70,469.17,345.71,9.94;3,124.70,483.69,345.91,9.94;3,124.70,498.33,24.20,9.94"><head>4</head><label></label><figDesc>Pipeline overview / Model StructureIn this section we discuss our model architecture, which primarily consists of 3 main components: Question Classifier Model, VQA Classifier Model, VQA Seq-2-Seq Model as shown in Fig.1. Due to the complex nature of the problem, we decided to break the problem into simpler tasks which could be handled independently. We sorted the questions into its respective categories namely Modality, Plane, Organ system &amp; Abnormality using the Question Classifier Model. The first group consisting of all questions on Modality, Plane &amp; Organ system categories along with their image pairs was fed to VQA Classifier Model to implement a multi-class classification model. The second group consisting of all the questions on Abnormality category along with their image pair was fed to the VQA Seq-2-Seq Model to generate answer sequences for predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,238.13,651.07,118.98,8.10;3,124.70,524.87,346.35,122.15"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Model Pipeline Overview</figDesc><graphic coords="3,124.70,524.87,346.35,122.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,446.45,620.28,23.99,9.94;4,124.70,634.80,158.31,9.94"><head></head><label></label><figDesc>Fig 2 below shows the model architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,232.85,685.03,129.46,8.10;5,150.70,147.40,293.90,533.60"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. VQA Classifier Architecture</figDesc><graphic coords="5,150.70,147.40,293.90,533.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,231.41,568.63,132.47,8.10;6,130.65,373.60,334.00,191.00"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. VQA Seq-2-Seq Architecture</figDesc><graphic coords="6,130.65,373.60,334.00,191.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,400.81,650.52,69.60,9.94;6,124.70,665.16,141.66,9.94"><head></head><label></label><figDesc>Fig 4 below describes the encoder architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,233.21,670.51,128.85,8.10;7,126.15,496.51,343.00,170.00"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Encoder Model Architecture</figDesc><graphic coords="7,126.15,496.51,343.00,170.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,232.85,489.52,129.44,8.10;8,124.70,344.50,337.00,141.00"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Decoder Model Architecture</figDesc><graphic coords="8,124.70,344.50,337.00,141.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,124.70,535.72,346.00,137.81"><head>Table 1 .</head><label>1</label><figDesc>Performance statistics of model on test data Seq models are not very expensive to train. It took 14 seconds and 12 seconds per epoch for the VQA-Classifier and VQA-Seq-2-Seq respectively on a Virtual Machine (VM) equipped with NVIDIA Tesla P4 GPU card with 8GB of RAM. The VM had Ubuntu OS with CUDA 10.1. For the implementation, we use Keras with TensorFlow 1.13.1 backend.</figDesc><table coords="9,130.22,568.56,340.48,90.46"><row><cell></cell><cell>Accuracy (Strict)</cell><cell>BLEU Score</cell></row><row><cell>Approach 1</cell><cell>0.484</cell><cell>0.531</cell></row><row><cell>Approach 2</cell><cell>0.488</cell><cell>0.534</cell></row><row><cell cols="3">It is worth mentioning that both the VQA Classifier and VQA Seq-2-</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,128.84,516.33,342.03,9.94;10,142.70,530.85,171.27,9.94" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,242.57,516.33,228.30,9.94;10,142.70,530.85,131.14,9.94">Advanced Applications of Neural Networks and Artificial Intelligence: A Review</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Koushal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gour</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.84,545.37,341.88,9.94;10,142.70,559.89,327.74,9.94;10,142.70,574.44,32.16,9.94" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,332.95,545.37,137.77,9.94;10,142.70,559.89,323.26,9.94">Review on Application of Artificial Neural Networks in Supply Chain Management and its Future</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soroush</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Nakhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bahreininejad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.84,589.08,341.78,9.94;10,142.70,603.60,64.59,9.94" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lawton</surname></persName>
		</author>
		<title level="m" coord="10,196.97,589.08,273.65,9.94;10,142.70,603.60,25.47,9.94">The Health Care Value Chain: Producers, Purchasers, and Providers</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.84,618.12,333.26,9.94" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,194.81,618.12,227.77,9.94">Examining the Impact of Health Care Consolidation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.84,632.64,341.90,9.94;10,142.70,647.16,278.09,9.94" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,233.56,647.16,147.14,9.94">VQA-Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aishwarya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jiasen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stanislaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Margaret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dhruv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Devi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.84,661.80,341.52,9.94;10,142.70,676.32,139.71,9.94" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,187.22,661.80,283.14,9.94;10,142.70,676.32,105.74,9.94">Pushing the Limits of Radiology with Joint Modeling of Visual and Textual Information</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sonit</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.84,149.80,341.92,9.94;11,142.70,164.44,43.80,9.94" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,197.93,149.80,229.91,9.94">Visual Question Answering Using Various Methods</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Shuhui</surname></persName>
		</author>
		<imprint>
			<date>94305</date>
			<pubPlace>California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.84,178.96,341.82,9.94;11,142.70,193.48,177.41,9.94" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,307.61,178.96,163.05,9.94;11,142.70,193.48,138.16,9.94">An Analysis of deep neural network models for practical application</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Alfredo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eugenio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.84,208.00,341.90,9.94;11,142.70,222.52,327.77,9.94;11,142.70,237.16,67.11,9.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,159.02,222.52,143.35,9.94">VQA: Visual question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,321.31,222.52,149.16,9.94;11,142.70,237.16,27.55,9.94">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,133.90,251.71,336.68,9.94;11,142.70,266.23,268.37,9.94" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="11,225.53,251.71,245.05,9.94;11,142.70,266.23,119.78,9.94">Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Vahid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ali</surname></persName>
		</author>
		<idno>CoRR, abs/1704.03162</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,133.90,280.75,336.59,9.94;11,142.70,295.39,327.82,9.94;11,142.70,309.91,131.53,9.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,277.37,280.75,193.12,9.94;11,142.70,295.39,38.35,9.94">Sequence to sequence learning with neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,201.32,295.39,269.20,9.94">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,133.90,324.43,336.80,9.94;11,142.70,337.03,327.71,9.94;11,142.70,349.75,327.98,9.94;11,142.70,362.35,328.26,9.94;11,142.70,375.07,195.35,9.94" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,443.11,324.43,27.59,9.94;11,142.70,337.03,327.71,9.94;11,142.70,349.75,66.48,9.94">VQA-Med: Overview of the Medical Visual Question Answering Task at Im-ageCLEF 2019</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Asma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Joey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Henning</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/,Lu-gano" />
	</analytic>
	<monogr>
		<title level="m" coord="11,225.97,349.75,118.80,9.94">CLEF 2019 Working Notes</title>
		<title level="s" coord="11,351.82,349.75,118.86,9.94;11,142.70,362.35,69.29,9.94">CEUR Workshop Proceedings (CEUR-WS</title>
		<meeting><address><addrLine>, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">September 09-12, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,133.90,387.67,336.57,9.94;11,142.70,402.21,327.80,9.94;11,142.70,416.73,327.95,9.94;11,142.70,431.37,327.72,9.94;11,142.70,445.89,327.93,9.94;11,142.70,460.41,328.00,9.94;11,142.70,474.93,328.02,9.94;11,142.70,489.57,224.48,9.94;11,367.39,488.40,5.40,6.26;11,376.63,489.57,94.09,9.94;11,142.70,504.09,327.82,9.94;11,142.70,518.61,328.12,9.94" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,188.64,460.41,282.06,9.94;11,142.70,474.93,328.02,9.94;11,142.70,489.57,117.76,9.94">ImageCLEF 2019: Multimedia Retrieval in Medicine, Lifelogging, Security and Nature In: Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bogdan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Renaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yashin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Vitali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Vassili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Dzmitri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Aleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Asma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Joey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Duc-Tien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Minh-Triet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cathal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Obioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Alba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Narciso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ergina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nikos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Adrian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Antonio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,268.37,489.57,98.81,9.94;11,367.39,488.40,5.40,6.26;11,376.63,489.57,94.09,9.94;11,142.70,504.09,194.22,9.94">Proceedings of the 10 th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="11,442.43,504.09,28.10,9.94;11,142.70,518.61,163.60,9.94">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10 th International Conference of the CLEF Association (CLEF 2019)<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">September 9-12, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
