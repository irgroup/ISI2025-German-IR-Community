<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.37,115.90,322.62,12.90">AUEB NLP Group at ImageCLEFmed Caption 2019</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,185.99,153.95,59.91,8.64"><forename type="first">Vasiliki</forename><surname>Kougia</surname></persName>
							<email>kouyiav@aueb.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.88,153.95,68.43,8.64"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.97,153.95,84.40,8.64"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.37,115.90,322.62,12.90">AUEB NLP Group at ImageCLEFmed Caption 2019</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A5B513BA58D4EF2719D71677CA2ED5B3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Images</term>
					<term>Concept Detection</term>
					<term>Image Retrieval</term>
					<term>Multi-label Classification</term>
					<term>Image Captioning</term>
					<term>Machine Learning</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the systems that AUEB's NLP Group used to participate in the ImageCLEFmed 2019 Caption task. The goal of this task is to automatically select medical concepts related to each image, as a first step towards generating image captions, medical reports, or to help in medical diagnosis. We participated with four systems, all using CNN image encoders. The encoder of each system is combined with an image retrieval method or a feed-forward neural network to predict concepts. Our systems were ranked 1st, 2nd, 3rd, and 5th.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning methods are being developed to automatically interpret biomedical images in order to help clinicians who examine large numbers of images daily <ref type="bibr" coords="1,443.13,390.56,15.27,8.64" target="#b9">[10]</ref>. The ImageCLEFmed Caption task <ref type="bibr" coords="1,257.35,402.51,16.60,8.64" target="#b11">[12]</ref> is part of ImageCLEF 2019 <ref type="bibr" coords="1,390.73,402.51,10.58,8.64" target="#b5">[6]</ref>. 1 Image CLEF is a campaign that suggests novel challenges and develops benchmarking resources for the evaluation of systems operating on images. The ImageCLEFmed Caption Task ran for the 3rd year in 2019. It included a Concept Detection sub-task, where the goal was to perform multi-label classification of medical images by automatically selecting medical concepts that should be assigned to each image. The concepts come from the Unified Medical Language System (UMLS). 2 Selecting the appropriate concepts per image can be a first step towards automatically generating image captions, longer medical reports, and can also assist, more generally, in computer-assisted diagnosis <ref type="bibr" coords="1,404.20,498.15,10.58,8.64" target="#b8">[9]</ref>. In the two previous years, ImageCLEFmed also included a Caption Prediction (generation) sub-task <ref type="bibr" coords="1,134.77,522.06,10.79,8.64" target="#b1">[2,</ref><ref type="bibr" coords="1,147.22,522.06,7.19,8.64" target="#b3">4]</ref>, which was not included this year.</p><p>This paper presents the four Concept Detection systems that AUEB's NLP Group used to participate in ImageCLEFmed 2019 Caption. The systems were ranked 1st, 2nd, 3rd, and 5th. The system that was ranked 3rd consists of a DenseNet-121 <ref type="bibr" coords="1,436.54,557.93,11.62,8.64" target="#b4">[5]</ref> Convolutional Neural Network (CNN) image encoder and a k-Nearest Neighbors (k-NN) retrieval component that uses the encoding of the image being classified to retrieve similar training images with known concepts; these are then used to assign concepts to the new image. The top-ranked system is a re-implementation of CheXNet <ref type="bibr" coords="2,400.67,119.31,15.27,8.64" target="#b13">[14]</ref>, with modifications for ImageCLEFmed Caption 2019. CheXnet also uses the DenseNet-121 encoder <ref type="bibr" coords="2,134.77,143.22,10.58,8.64" target="#b4">[5]</ref>, combined with a feed-forward neural network (FFNN) that performs multi-label classification. The second-best system is an ensemble combining concept probability scores obtained from the CheXNet-based system and image similarity scores produced by k-NN retrieval of similar training images. The system ranked 5th uses the VGG-19 image encoder <ref type="bibr" coords="2,195.74,191.04,15.27,8.64" target="#b14">[15]</ref>, which was also used by Jing et al. <ref type="bibr" coords="2,355.24,191.04,10.58,8.64" target="#b6">[7]</ref>, combined with a FFNN for multi-label classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>The ImageCLEFmed Caption 2019 dataset is a subset of the Radiology Objects in COntext (ROCO) dataset <ref type="bibr" coords="2,222.17,264.34,15.27,8.64" target="#b12">[13]</ref>. It consists of medical images extracted from open access biomedical journal articles of PubMed Central. <ref type="foot" coords="2,323.93,274.63,3.49,6.05" target="#foot_0">3</ref> Each image was extracted along with its caption. The caption was processed using QuickUMLS <ref type="bibr" coords="2,379.22,288.25,16.60,8.64" target="#b15">[16]</ref> to produce the gold UMLS concept unique identifiers (CUIs). An image can be associated with multiple CUIs (Figure <ref type="figure" coords="2,190.11,312.16,3.60,8.64" target="#fig_0">1</ref>). Each CUI is accompanied by its corresponding UMLS term. In ImageCLEFmed Caption 2017 <ref type="bibr" coords="2,273.78,627.07,11.62,8.64" target="#b1">[2]</ref> and 2018 <ref type="bibr" coords="2,329.33,627.07,10.58,8.64" target="#b3">[4]</ref>, the datasets were noisy. They included generic and compound images, covering a wide diversity of medical images; there was also a large total number of concepts (111,155) and some of them were too generic and did not appropriately describe the images <ref type="bibr" coords="3,350.63,131.27,15.27,8.64" target="#b17">[18]</ref>. In the ROCO dataset, compound and non-radiology images were filtered out using a CNN model. This led to 80,786 radiology images in total, of which 56,629 images were provided as the training set, 14,157 as the validation set, and the remaining 10,000 images were used for testing. In ImageCLEFmed Caption 2019, the total number of UMLS concepts was reduced to 5,528, with 6 concepts assigned to each training image on average. The minimum number of concepts per training image is 1, and the maximum is 72. Table <ref type="table" coords="3,424.44,203.00,4.98,8.64" target="#tab_0">1</ref> shows the 6 most frequent concepts of the training set and how many training images they were assigned to, according to the gold annotations. We note that 312 of the 5,528 total concepts are not assigned to any training image; and We randomly selected 20% of the training images and used them as our development set <ref type="bibr" coords="3,148.20,405.63,17.55,8.64" target="#b10">(11,</ref><ref type="bibr" coords="3,165.76,405.63,13.16,8.64">326</ref> images along with their gold concepts). The models we used to produce the submitted results were trained on the entire training set. The validation set was used for hyper-parameter tuning and early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>This section describes the four methods we developed for ImageCLEFmed Caption 2019.</p><p>3.1 System 1: DenseNet-121 Encoder + k-NN Image Retrieval (Ranked 3rd)</p><p>In this system, we followed a retrieval approach, extending the 1-NN baseline of our previous work on biomedical image captioning <ref type="bibr" coords="3,330.52,554.33,10.58,8.64" target="#b8">[9]</ref>. Given a test image, the previous 1-NN baseline returned the caption of the most similar training image, using a CNN encoder to map each image to a dense vector. For ImageCLEFmed Caption 2019, we retrieve the k-most similar training images and use their concepts, as described below.</p><p>We use the DenseNet-121 <ref type="bibr" coords="3,260.06,602.15,11.62,8.64" target="#b4">[5]</ref> image encoder, a CNN with 121 layers, where all layers are directly connected to each other improving information flow and avoiding vanishing gradients. We started with DenseNet-121 pre-trained on ImageNet <ref type="bibr" coords="3,451.14,626.06,11.62,8.64" target="#b0">[1]</ref> and fine-tuned it on ImageCLEFmed Caption 2019 training images. <ref type="foot" coords="3,392.48,636.35,3.49,6.05" target="#foot_1">4</ref> The fine-tuning was performed as when training DenseNet-121 in System 2, including data augmentation (Section 3.2). Without fine-tuning, the performance of the pre-trained encoder was worse. ImageCLEFmed Caption 2019 images were rescaled to 224×224 and normalized with the mean and standard deviation of ImageNet to match the requirements of DenseNet-121 and how it was pre-trained on ImageNet. Having fine-tuned DenseNet-121, we used it to obtain dense vector encodings, called image embeddings, of all training images. The image embeddings are extracted from the last average pooling layer of DenseNet-121. Given a test image (Fig. <ref type="figure" coords="4,294.06,203.00,3.60,8.64" target="#fig_1">2</ref>), we again use the fine-tuned DensNet-121 to obtain the image's embedding. We then retrieve the k training images with the highest cosine similarity (computed on image embeddings) to the test image, and return the r concepts that are most frequent among the concepts of the k images. We set r to the average number of concepts per image of the particular k retrieved images. We tuned the value of k in the range from 1 to 200 using the validation set, which led to k = 199. Further fine-tuning may improve performance further. This system ranked 3rd.  This system, which is based on CheXNet <ref type="bibr" coords="4,312.12,506.31,15.27,8.64" target="#b13">[14]</ref>, achieved the best results in Image-CLEFmed Caption 2019. In its original form, CheXNet maps X-rays of the ChestX-ray 14 dataset <ref type="bibr" coords="4,176.67,530.22,16.60,8.64" target="#b16">[17]</ref> to 14 labels. It uses DenseNet-121 <ref type="bibr" coords="4,332.46,530.22,11.62,8.64" target="#b4">[5]</ref> to encode images, adding a FFNN to assign one or more of the 14 labels (classes) to each image.</p><p>We re-implemented CheXNet in Keras <ref type="foot" coords="4,306.58,552.46,3.49,6.05" target="#foot_2">5</ref> and extended it for the many more labels (5,528 vs. 14) of ImageCLEFmed Caption 2019. The images of ImageCLEFmed Caption 2019 were again rescaled to 224×224 and normalized using the mean and standard deviation values of ImageNet. Also the training images of ImageCLEFmed Caption 2019 were augmented by applying random horizontal flip. Image embeddings are again extracted from the last average pooling layer of DenseNet-121. In this system, however, the image embeddings are then passed through a dense layer with 5,528 outputs and sigmoid activations to produce a probability per label. We trained the model by minimizing binary cross entropy loss. We used Adam <ref type="bibr" coords="5,306.69,119.31,11.62,8.64" target="#b7">[8]</ref> with its default hyper-parameters, early stopping on the validation set, and patience of 3 epochs. We also decayed the learning rate by a factor of 10 when the validation loss stopped improving.</p><p>At test time, we predict the concepts for each test image using their probabilities, as estimated by the trained model. For each concept (label), we assign it to the test image if the corresponding predicted probability exceeds a threshold t. We use the same t value for all 5,528 concepts. We tuned t on the validation set, which led to t = 0.16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">System 3: Based on Jing et al., VGG-19 Encoder + FFNN (Ranked 5th)</head><p>This system is based on the work of Jing et al. <ref type="bibr" coords="5,324.42,238.06,10.58,8.64" target="#b6">[7]</ref>, who presented an encoder-decoder model to generate tags and medical reports from medical images. Roughly speaking, the full model of Jing et al. uses a VGG-19 <ref type="bibr" coords="5,309.56,261.97,16.60,8.64" target="#b14">[15]</ref> image encoder, a multi-label classifier to produce tags (describing concepts) from the images, and a hierarchical LSTM that generates texts by attending on both image and tag embeddings; the top level of the LSTM generates sentence embeddings, and the bottom level generates the words of each sentence. We implemented in Keras a simplified version of the first part of Jing et al.'s model, the part that performs multi-label image classification.</p><p>Again, we rescale the ImageCLEFmed Caption 2019 images to 224×224 and normalize them using the mean and standard deviation of ImageNet. We feed the resulting images to the VGG-19 CNN, which has 19 layers and uses small kernels of size 3 × 3. We used VGG-19 pre-trained on ImageNet. <ref type="foot" coords="5,307.83,367.90,3.49,6.05" target="#foot_3">6</ref> We feed whole images to VGG-19, unlike Jing et al. <ref type="bibr" coords="5,176.98,381.53,10.58,8.64" target="#b6">[7]</ref>, who divide each image into regions and encode each region separately. The output of the last fully connected layer of VGG-19 is then given as input to a dense layer with a softmax activation to obtain a probability distribution over the concepts. The model is trained using categorical cross entropy, which is calculated as:</p><formula xml:id="formula_0" coords="5,244.72,436.71,235.87,31.18">E = - |C| i=1 y true,i log 2 (y pred,i )<label>(1)</label></formula><p>where C is the set of |C| = 5, 528 concepts, y true is the ground truth binary vector of a training image, and y pred is the predicted softmax probability distribution over the concepts C for the training image. Categorical cross entropy sums loss terms only for the gold concepts of the image, which have a value of 1 in y true . When using softmax and categorical cross-entropy, usually y true is a one-hot vector and the classes are mutually exclusive (single-label classification). To use softmax with categorical cross entropy for multi-label classification, where y true is binary but not necessarily one-hot, the loss is divided by the number of gold labels (true concepts) <ref type="bibr" coords="5,394.84,561.42,10.79,8.64" target="#b2">[3,</ref><ref type="bibr" coords="5,407.29,561.42,11.83,8.64" target="#b10">11]</ref>. Jing et al. <ref type="bibr" coords="5,468.97,561.42,11.62,8.64" target="#b6">[7]</ref> achieve this by dividing the ground truth binary vector y true by its L1 norm, which equals the number of gold labels. Hence, the categorical cross-entropy loss is computed as follows:</p><formula xml:id="formula_1" coords="5,183.95,614.59,296.64,31.18">E = - |C| i=1 y true,i y true 1 log 2 (y pred,i ) = - 1 M M j=1 log 2 (y pred,j )<label>(2)</label></formula><p>where M is the number of gold labels (true concepts) of the training image, which is different per training image. In this model, the loss of Eq. 2 achieved better results on the development set, compared to binary cross entropy with a sigmoid activation per concept. We used the Adam optimizer with initial learning rate 1e-5 and early stopping on the validation set with patience 3 epochs. Given a test image, we return the six concepts with the highest probability scores, since the average number of gold concepts per training image is 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">System 4: Ensemble, k-NN Image Retrieval + CheXNet (Ranked 2nd)</head><p>This method is an ensemble of System 1 (DenseNet-121 + k-NN Image Retrieval) and System 2 (CheXNet-based), where System 1 is modified to produce a score for each returned concept. Given a test image g, we use System 1 (Fig. <ref type="figure" coords="6,323.72,276.72,4.15,8.64" target="#fig_1">2</ref>) to retrieve the k most similar training images g 1 , . . . , g k , their gold concepts, and the cosine similarities s(g, g 1 ), . . . , s(g, g k ) between the test image g and each one of the k retrieved images. Let C be again the set of |C| = 5, 528 concepts. For each concept c j ∈ C, the modified System 1 assigns to c i the following score:</p><formula xml:id="formula_2" coords="6,242.29,345.22,238.30,30.32">v 1 (c j , g) = k i=1 s(g, g i ) δ(c j , g i )<label>(3)</label></formula><p>where δ(c j , g i ) = 1 if c j is a gold concept of the retrieved training image g i , and δ(c j , g i ) = 0 otherwise. In other words, the score of each concept c j is the sum of the cosine similarities of the retrieved documents where c j is a gold concept.</p><p>For the same test image g, we also obtain concept probabilities from System 2, i.e., a vector of 5,528 probabilities. Let v 2 (c j , g) be the probability of concept c j being correct for test image g according to System 2. For each c j ∈ C, the ensemble's score v(c j , g) of c j is simply the average of v 1 (c j , g) and v 2 (c j , g). The ensemble returns the six concepts with the highest v(c j , g) scores, as in System 3, on the grounds that the average number of gold concepts per training image is 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Systems were evaluated in ImageCLEFmed Caption 2019 by computing their F1 scores on each test image (in effect comparing the binary ground truth vector y true to the predicted concept probabilities y pred ) and then averaging over all test images <ref type="bibr" coords="6,434.20,562.48,10.58,8.64" target="#b5">[6]</ref>. Table <ref type="table" coords="6,475.61,562.48,4.98,8.64">2</ref> reports the evaluation results of our four systems on the development and test data, as well as their ranking among the approximately 60 systems that participated in the task. The ensemble (System 4) had the best results on development data, but the CheXNetbased system (System 2) had the best results on the test set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,605.89,345.83,8.12;2,134.77,330.54,345.83,261.00"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Two images from ImageCLEFmed Caption 2019, with their gold CUIs and UMLS terms.</figDesc><graphic coords="2,134.77,330.54,345.83,261.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,450.47,345.83,8.12;4,134.77,297.15,345.84,138.96"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of how System 1 (DenseNet-121 and k-NN image retrieval) works at test time.</figDesc><graphic coords="4,134.77,297.15,345.84,138.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,134.77,486.78,329.31,8.96"><head>3. 2</head><label>2</label><figDesc>System 2: CheXNet-based, DenseNet-121 Encoder + FFNN (Ranked 1st)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,238.86,345.82,142.12"><head>Table 1 .</head><label>1</label><figDesc>1,530 concepts are assigned to only one training image. The</figDesc><table coords="3,236.62,274.93,142.11,74.27"><row><cell>CUI</cell><cell>UMLS term</cell><cell>Images</cell></row><row><cell cols="3">C0441633 diagnostic scanning 6,733</cell></row><row><cell cols="2">C0043299 x-ray procedure</cell><cell>6,321</cell></row><row><cell>C1962945</cell><cell>radiogr</cell><cell>6,318</cell></row><row><cell>C0040395</cell><cell>tomogr</cell><cell>6,235</cell></row><row><cell>C0034579</cell><cell>pantomogr</cell><cell>6,127</cell></row><row><cell>C0817096</cell><cell>thoracics</cell><cell>5,981</cell></row></table><note coords="3,186.08,362.25,294.51,7.77;3,134.77,373.21,329.73,7.77"><p>6 most frequent concepts (CUIs) in the training set of ImageCLEFmed Caption 2019 and how many training images they are assigned to, according to the gold annotations.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,152.70,657.93,177.53,6.31"><p>https://www.ncbi.nlm.nih.gov/pmc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="3,152.70,657.08,326.14,7.77"><p>We used the implementation of https://keras.io/applications/#densenet.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="4,152.70,657.93,91.46,6.31"><p>https://keras.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="5,152.70,657.93,193.67,6.31"><p>https://keras.io/applications/#vgg19</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description</head><p>F1 Score Ranking Dev Test S1</p><p>DenseNet <ref type="bibr" coords="7,294.38,140.38,10.45,7.77" target="#b4">[5]</ref> + k-NN 0.2575244 0.2740204 3 S2 (CheXNet-based <ref type="bibr" coords="7,228.93,151.34,14.34,7.77" target="#b13">[14]</ref>) DenseNet <ref type="bibr" coords="7,294.38,151.34,10.45,7.77" target="#b4">[5]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We described the four systems that AUEB's NLP Group used to participate in Image-CLEFmed 2019 Caption. The four systems were ranked 1st, 2nd, 3rd, and 5th. Our top system was a re-implementation of CheXNet <ref type="bibr" coords="7,340.42,272.31,15.27,8.64" target="#b13">[14]</ref>, with modifications to handle the much larger label set of ImageCLEFmed 2019 Caption and data augmentation. The system that was ranked 3rd used DenseNet <ref type="bibr" coords="7,308.94,296.22,11.62,8.64" target="#b4">[5]</ref> to encode images and k-NN retrieval to return the concepts of the most similar training images. Our second-best system was an ensemble of the previous two (CheXNet-based and k-NN based), indicating that the two approaches are complementary. Our weakest system, which nevertheless was ranked 5th, was based on the multi-label classification part of the system of Jing et al. <ref type="bibr" coords="7,466.48,344.04,10.58,8.64" target="#b6">[7]</ref>, which aims to generate draft medical reports using an encoder-decoder approach.</p><p>In future work, we aim to experiment with, combine, and improve upon additional methods and datasets for medical image captioning. Towards that direction, we recently published a survey on medical image to text methods <ref type="bibr" coords="7,346.94,391.96,10.58,8.64" target="#b8">[9]</ref>, which we also plan to extend.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.61,448.38,337.98,7.77;7,150.95,459.34,329.64,7.77;7,150.95,470.30,161.39,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,361.44,448.38,119.15,7.77;7,150.95,459.34,80.03,7.77">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,249.84,459.34,226.91,7.77">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami Beach, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,481.35,337.98,7.77;7,150.95,492.31,329.64,7.77;7,150.95,503.27,329.64,7.77;7,150.95,513.94,227.16,8.06" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,361.66,481.35,118.94,7.77;7,150.95,492.31,329.64,7.77;7,150.95,503.27,39.66,7.77">Overview of ImageCLEFcaption 2017 -the Image Caption Prediction and Concept Extraction Tasks to Understand Biomedical Images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="7,209.81,503.27,255.95,7.77">CLEF2017 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,525.29,337.98,7.77;7,150.95,536.24,302.55,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,325.24,525.29,155.35,7.77;7,150.95,536.24,63.37,7.77">Deep Convolutional Ranking for Multilabel Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,232.64,536.24,194.72,7.77">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,547.30,337.98,7.77;7,150.95,558.26,329.64,7.77;7,150.95,568.93,308.03,8.06" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,376.04,547.30,104.55,7.77;7,150.95,558.26,110.12,7.77">Overview of the ImageCLEF 2018 Caption Prediction Tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="7,280.28,558.26,95.80,7.77">CLEF2018 Working Notes</title>
		<title level="s" coord="7,382.68,558.26,97.91,7.77;7,150.95,569.22,39.20,7.77">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,580.27,337.98,7.77;7,150.95,591.23,329.64,7.77;7,150.95,602.19,202.98,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,372.80,580.27,107.79,7.77;7,150.95,591.23,56.23,7.77">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,227.06,591.23,253.53,7.77;7,150.95,602.19,42.24,7.77">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,613.25,337.98,7.77;7,150.95,624.21,329.64,7.77;7,150.95,635.17,329.64,7.77;7,150.95,646.13,329.64,7.77;7,150.95,657.08,329.64,7.77;8,150.95,119.96,329.64,7.77;8,150.95,130.92,329.64,7.77;8,150.95,141.88,329.64,7.77;8,150.95,152.84,160.08,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,367.99,657.08,112.61,7.77;8,150.95,119.96,201.03,7.77">ImageCLEF 2019: Multimedia Retrieval in Medicine, Lifelogging, Security and Nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,370.85,119.96,109.75,7.77;8,150.95,130.92,329.64,7.77;8,150.95,141.88,136.03,7.77">Proceedings of the 10th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="8,292.68,141.88,149.56,7.77">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF 2019)<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,142.61,163.80,337.98,7.77;8,150.95,174.76,329.64,7.77;8,150.95,185.71,216.52,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,250.44,163.80,213.46,7.77">On the Automatic Generation of Medical Imaging Reports</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,150.95,174.76,329.64,7.77;8,150.95,185.71,49.53,7.77">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2577" to="2586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,196.67,337.98,7.77" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="8,227.17,196.67,161.18,7.77">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,207.63,337.98,7.77;8,150.95,218.59,329.64,7.77;8,150.95,229.55,329.64,7.77;8,150.95,240.51,112.32,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,331.33,207.63,149.26,7.77;8,150.95,218.59,10.28,7.77">A Survey on Biomedical Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,180.51,218.59,300.09,7.77;8,150.95,229.55,284.53,7.77">Workshop on Shortcomings in Vision and Language of the Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="26" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,251.47,338.35,7.77;8,150.95,262.43,329.64,7.77;8,150.95,273.04,187.77,8.12" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,304.90,262.43,175.69,7.77;8,150.95,273.39,29.89,7.77">A Survey on Deep Learning in Medical Image Analysis</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A V D</forename><surname>Laak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">V</forename><surname>Ginneken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,186.82,273.39,87.66,7.77">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,284.34,338.35,7.77;8,150.95,295.30,329.64,7.77;8,150.95,306.26,236.91,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,196.02,295.30,200.46,7.77">Exploring the Limits of Weakly Supervised Pretraining</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,415.67,295.30,64.93,7.77;8,150.95,306.26,89.94,7.77">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,317.22,338.35,7.77;8,150.95,328.18,329.64,7.77;8,150.95,338.85,329.64,8.06;8,150.95,350.10,132.73,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,361.14,317.22,119.45,7.77;8,150.95,328.18,113.61,7.77">Overview of the ImageCLEFmed 2019 Concept Prediction Task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="8,287.49,328.18,99.50,7.77">CLEF2019 Working Notes</title>
		<title level="s" coord="8,395.43,328.18,85.16,7.77;8,150.95,339.14,29.88,7.77">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,361.06,338.35,7.77;8,150.95,372.02,329.64,7.77;8,150.95,382.97,304.07,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,371.50,361.06,109.09,7.77;8,150.95,372.02,140.26,7.77">Radiology Objects in COntext (ROCO): A Multimodal Image Dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,310.08,372.02,170.51,7.77;8,150.95,382.97,167.43,7.77">MICCAI Workshop on Large-scale Annotation of Biomedical data and Expert Label Synthesis</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,393.93,338.35,7.77;8,150.95,404.89,307.83,7.77" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="8,376.58,393.93,104.01,7.77;8,150.95,404.89,210.32,7.77">CheXNet: Radiologist-Level Pneumonia Detection on Chest X-rays with Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mehta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05225</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,415.85,338.35,7.77;8,150.95,426.81,135.26,7.77" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="8,263.99,415.85,216.60,7.77;8,150.95,426.81,42.24,7.77">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,437.77,338.35,7.77;8,150.95,448.73,160.28,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,249.12,437.77,231.47,7.77;8,150.95,448.73,53.18,7.77">QuickUMLS: A Fast, Unsupervised Approach for Medical Concept Extraction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,222.17,448.73,62.92,7.77">MedIR workshop</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,459.69,338.35,7.77;8,150.95,470.65,329.64,7.77;8,150.95,481.60,329.64,7.77;8,150.95,492.56,291.96,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,395.86,459.69,84.73,7.77;8,150.95,470.65,329.64,7.77;8,150.95,481.60,142.84,7.77">ChestX-ray8: Hospitalscale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,313.17,481.60,167.43,7.77;8,150.95,492.56,131.22,7.77">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,503.52,338.35,7.77;8,150.95,514.48,329.64,7.77;8,150.95,525.44,105.18,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,287.37,503.52,193.22,7.77;8,150.95,514.48,113.10,7.77">ImageSem at ImageCLEF 2018 Caption Task: Image Retrieval and Transfer Learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,282.47,514.48,94.92,7.77">CLEF2018 Working Notes</title>
		<title level="s" coord="8,383.57,514.48,97.03,7.77;8,150.95,525.44,13.75,7.77">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
