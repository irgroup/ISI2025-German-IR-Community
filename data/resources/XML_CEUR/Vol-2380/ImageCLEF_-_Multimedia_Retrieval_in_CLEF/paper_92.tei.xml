<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.52,114.32,308.33,14.35;1,218.53,132.25,178.28,14.35;1,215.58,150.18,184.20,14.35">Informative and Intriguing Visual Features: UA.PT Bioinformatics in ImageCLEF Caption 2019</title>
				<funder>
					<orgName type="full">European Regional Development Fund</orgName>
				</funder>
				<funder ref="#_UhzWHPX">
					<orgName type="full">Portugal 2020</orgName>
				</funder>
				<funder ref="#_4jSZr6M">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,170.30,189.00,91.88,9.96;1,262.19,188.54,10.95,9.96"><forename type="first">Ana</forename><forename type="middle">Jorge</forename><surname>Gonçalves ⋆</surname></persName>
							<idno type="ORCID">0000-0002-9582-359X</idno>
							<affiliation key="aff0">
							</affiliation>
						</author>
						<author>
							<persName coords="1,282.82,189.00,66.20,9.96"><forename type="first">Eduardo</forename><surname>Pinho</surname></persName>
							<email>eduardopinho@ua.pt</email>
							<affiliation key="aff0">
							</affiliation>
						</author>
						<author>
							<persName coords="1,389.02,189.00,56.03,9.96"><forename type="first">Carlos</forename><surname>Costa</surname></persName>
							<email>carlos.costa@ua.pt</email>
							<affiliation key="aff0">
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.52,114.32,308.33,14.35;1,218.53,132.25,178.28,14.35;1,215.58,150.18,184.20,14.35">Informative and Intriguing Visual Features: UA.PT Bioinformatics in ImageCLEF Caption 2019</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CD93BEF97E1B14FC685A695B51FA32C9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>representation learning</term>
					<term>deep learning</term>
					<term>auto-encoders</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Digital medical imaging has opened new advances in clinical decision support and treatment procedures since its inception. This leads to the creation of huge amounts of data that are often not fully exploited. The development and evaluation of representation learning techniques for automatic detection of concepts in medical images can make way for improved indexing, processing and retrieval capabilities in medical imaging archives. This paper discloses several independent approaches for multi-label classification of biomedical concepts, in the context of the ImageCLEFmed Caption challenge of 2019. We emphasize the use of threshold tuning to optimize the quality of sample retrieval, as well as the differences between training a convolutional neural network end-to-end for supervised image classification, and training unsupervised learning models before linear classifiers. In the test results, the best mean F1-score of 0.206 was obtained with the supervised approach, albeit with images of a larger resolution than for the dual-stage approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical imaging modalities are an essential and well established medium, and as the amount of medical images is dramatically growing, automatic and semiautomatic algorithms are quite pertinent for the extraction of information from biomedical image data <ref type="bibr" coords="1,238.36,564.87,14.62,9.96" target="#b13">[14]</ref>. Therefore, deep learning techniques are becoming increasingly useful and necessary for this aim, posing as a valuable key for the development of representation learning techniques, and ultimately for improving the quality of systems in healthcare.</p><p>The process of annotating images with useful information in this context is time-consuming and usually requires medical expertise. The development of powerful representations of images could enable the automatic detection of biomedical concepts in a medical imaging data set. The ImageCLEFmed initiative, inserted in ImageCLEF <ref type="bibr" coords="2,242.45,165.59,9.97,9.96" target="#b3">[4]</ref>, has been focused on automatic concept detection, diagnosis, and question answering from medical images. In particular, the Im-ageCLEFmed Caption challenge of 2019 <ref type="bibr" coords="2,309.08,189.50,15.51,9.96" target="#b10">[11]</ref> has narrowed its scope into the task of concept detection, with the goal of recognizing biomedical concepts presented in medical images, using only the visual content.</p><p>This paper presents our solution proposal for the concept detection task, describing our methodology and evaluating its performance under the ImageCLEF 2019 challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>For this task, a data set with a total of 70,786 radiology images of several medical imaging modalities was provided from Radiology Objects in Context (ROCO) <ref type="bibr" coords="2,134.77,331.27,14.62,9.96" target="#b11">[12]</ref>. This global set was further split into training (56,629 images), validation (14,157 images) and test (10,000 images) sets by the organizers. Only the first two were accompanied with the list of concepts applicable to each image, whereas the testing set's ground truth was hidden from the participants.</p><p>The ImageCLEF Caption 2019 data set includes an overwhelming number of 5,216 unique concepts, not all of which can be reasonably considered due to the very small number of positive samples in the training and validation splits. In all of the methods described next, we have admitted only the 1,100 concepts with the highest number of samples with a positive occurrence of that concept (henceforth named positive samples). The label vectors were built based on a direct mapping from the UMLS concept unique identifier (CUI) to an index in the vector. The reverse mapping was kept for producing the textual list of concepts.</p><p>Also contributing to this decision, was the observed imbalance in the number of positives of each label, as discerned in Figure <ref type="figure" coords="2,341.81,499.06,3.88,9.96" target="#fig_0">1</ref>, making them difficult to train classifiers and evaluate them. By considering the 1,100 most frequent concepts, one could ensure, in the extreme case, a minimum number of 29 positive samples in the training set and 2 positive samples in the validation set. We admit that attempting to detect any less frequent concepts is unlikely to result in useful classifiers.</p><p>At ImageCLEF 2018, the highest mean F 1 -score was obtained through unsupervised methods, namely by using the features of an adversarial auto-encoder, followed by logistic regression <ref type="bibr" coords="2,269.13,594.91,14.62,9.96" target="#b12">[13]</ref>. However, in relation to last year challenge, the number of images in the training set was reduced by 34 % and since all the data was annotated, we felt inclined to compare our past approach with the training of purely supervised methods.</p><p>Convolutional neural networks (CNNs) are considered one of the best approaches for image classification <ref type="bibr" coords="2,278.92,654.89,14.62,9.96" target="#b14">[15]</ref>. Unlike a 2-stage approach, where the ex- tracted feature descriptors are served as input to a trainable classifier, the images themselves are used in the learning process of the CNN. This could lead to a more focused guidance of the feature learning process across the multiple layers of the network. It was of our interest to compare this common supervised image classification method with the 2-stage pipeline involving unsupervised learning methods and simple classifiers.</p><p>Therefore, we have addressed the concept detection task with multiple independent approaches, which can be divided in two major groups:</p><p>-Through image representations, obtained by the implementation of several feature extraction methods:</p><p>• Color and edge directivity descriptors, that were used as image descriptors. • An auto-encoder and an adversarial auto-encoder was trained and features were extracted from its bottleneck vector. • The ensemble of features obtained from the previous point was used for classification.</p><p>-An end-to-end approach, using two deep learning architectures:</p><p>• A simple convolutional neural network model was assumed.</p><p>• A residual neural network.</p><p>In every case, some form of optimum threshold tuning was employed, to overcome the classifier's focus on accuracy rather than F-measure. Further details are given in Sections 2.4 and 2.5.</p><p>Neural network training, feature extraction, and logistic regression were conducted using TensorFlow on one of the GPUs of an NVIDIA Tesla K80 graphics card in an Ubuntu server machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Color and Edge Directivity Descriptors</head><p>As traditional visual feature extraction algorithms are still very often considered in medical image recognition, these techniques contribute to a baseline, which we expect modern deep learning methods to surpass. For this purpose, we have extracted Color and Edge Directivity Descriptors (CEDDs) <ref type="bibr" coords="4,391.74,172.61,10.52,9.96" target="#b1">[2]</ref> from the images<ref type="foot" coords="4,473.36,171.40,3.97,6.97" target="#foot_0">1</ref> , after they were resized to a minimum size of 256 while keeping the aspect ratio. These low-level features accumulate color and texture information into a histogram of 144 bins per sample, and are known for their appealing accuracy in image retrieval tasks, when contrasted with their high compactness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adversarial Auto-encoder</head><p>For the unsupervised extraction of visual features from the medical images, an adversarial auto-encoder (AAE) <ref type="bibr" coords="4,277.72,280.29,10.52,9.96" target="#b8">[9]</ref> was trained on the given data set, with images resized to 64 pixels (64 × 64 × 3 inputs). While functioning as a typical auto-encoder, which seeks to minimize the information loss of passing samples through an information bottleneck (Equation <ref type="formula" coords="4,338.94,316.16,3.88,9.96">1</ref>), a discriminator D is also included. The purpose of D is to learn to distinguish latent codes produced by the encoder E from a prior code created by an arbitrary distribution p(z), whereas E seeks to fool the code discriminator by approximating its output distribution to that of p(z) (Equation <ref type="formula" coords="4,252.02,363.98,3.88,9.96" target="#formula_1">2</ref>). Based on the concept of Generative Adversarial Networks (GANs) <ref type="bibr" coords="4,218.03,375.94,9.97,9.96" target="#b2">[3]</ref>, this min-max game of adversarial components provides variational inference to the basic auto-encoder structure while leading the encoder to match the prior distribution, thus regularizing the encoder. In this work, we have sampled ϵ ∼ p(z) from a rectified unit-norm Gaussian distribution (as in, N (0, I) with all negative numbers replaced with zeros), which resulted in organically sparse latent codes.</p><formula xml:id="formula_0" coords="4,240.25,456.18,134.36,47.78">x ′ = G(E(x)) L rec (x, x ′ ) = 1 2N N ∑ i (x i -x ′ i ) 2</formula><p>(1)</p><formula xml:id="formula_1" coords="4,167.44,522.56,313.15,15.81">V (E, D) = min E max D E z∼pz [log D(z)] + E x∼p(x) [log (1 -D(E(x)))]<label>(2)</label></formula><p>Both encoder and decoder architecture are based on the ResNet19 <ref type="bibr" coords="4,444.69,552.89,9.97,9.96" target="#b5">[6]</ref>, each component comprising four 2-layer residual blocks. At the end of the encoder, the final layer was subjected to a ReLU activation and a very light L 1 activation regularization (of factor 10 -6 ), thus contributing to the features' sparsity without deviating from the established prior. The code discriminator, on the other hand, is composed of three 1024-channel wide dense layers, with layer normalization <ref type="bibr" coords="4,134.77,624.62,9.97,9.96" target="#b0">[1]</ref>, plus an output layer. Drop-out of rate 25% was also added before the output layer.</p><p>The AAE was trained for 30 epochs on the training set, with images resized to a minimum dimension of 72 pixels and then randomly cropped to a 64 x 64 square. All three RGB channels were kept with their values normalized to the [-1, 1] range with the formula x/127.5 -1. Each iteration is composed of three optimization steps: the code discriminator step, the encoder regularization step, and the reconstruction step. The components were trained in this order with the Adam optimizer <ref type="bibr" coords="5,226.29,189.50,9.97,9.96" target="#b4">[5]</ref>, a learning rate of 10 -5 , beta parameters β 1 = 0.5 and β 2 = 0.999, and a mini-batch size of 32.</p><p>Moreover, in order to better understand the influence of the adversarial autoencoder's regularization phase, a separate auto-encoder (AE) was trained with the same encoder and decoder characteristics as in the adversarial one, but without the adversarial loss. In this case, each iteration is only composed of the reconstruction step, mainly influenced by the loss L rec .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Early Fusion: Auto-encoder and Adversarial Auto-encoder</head><p>The combination of information from different techniques seems intuitively appealing for improving the performance of the learning process. More recently, there is an attempt of combining traditional image characteristics with high-level features <ref type="bibr" coords="5,172.19,344.49,9.97,9.96" target="#b6">[7]</ref>.</p><p>Although it is known that the combination of low-level features, such as color and texture in CEDD, is important to the quality of visual features, it is not as clear whether the combination of high-level features obtained from two autoencoders can benefit from an early fusion. Hence, we hereby took the features obtained from the AE and the AAE, and concatenated them to form a 1024dimensional feature set, for its subsequent use in the logistic regression step alongside the other feature sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Logistic Regression</head><p>For each of the previously described set of features, logistic regression classifiers were trained for the chosen labels, for a set of predefined operating point thresholds: 0.075, 0.1, 0.125, and 0.15. The linear classifiers were trained with the Adam optimizer <ref type="bibr" coords="5,227.45,511.43,9.97,9.96" target="#b4">[5]</ref>, with a mini-batch size of 128, until the best F 1 -score among the various thresholds would reach a plateau. Our experiments suggests that training in this phase with a very small learning rate, often 10 -5 in our experiments, for a large number of epochs (more than 500), helps the training process to find a more optimal solution.</p><p>Aware of the presence of concepts with a very low number of positive samples, one may wonder whether certain labels were not well trained or resulted in uninformative classifiers. To mitigate this, we calculated the area under the curve (AUC) of each binary classifier's receiver operating characteristic curve (ROC). Afterwards, we have tested whether ignoring the predictions of concepts where the AUC was lower that 0.5 would potentially improve the overall performance. Testing this hypothesis on the validation set, it is revealed that this would improve the mean F 1 -score in most cases, albeit only slightly. For the features of the AAE, as an example, this tweak has only increased the score by 5 × 10 -5 . With the features of the simple AE, the score was only improved by 7.3 × 10 -4 . We held this mechanism away from the classifiers trained with the CEDD feature set.</p><p>Probabilistic classifiers minimizing binary cross-entropy inherently optimize for the accuracy of predictions. However, accuracy is overoptimistic when labels have a very low number of positives, as is the case in this task, making a poor metric for the classifiers' usefulness. As recognized by past work in the scope of ImageCLEF concept detection, adjusting the operating point thresholds to optimize the F 1 -score provides significant improvements in the final metric values, in spite of the known implications of this practice <ref type="bibr" coords="6,364.02,238.96,9.97,9.96" target="#b7">[8]</ref>. In order to adjust the probabilistic threshold for optimizing the F 1 -score, the provided validation set was split in five folds. For each one, we used a granular sequential search (with a granularity of 0.01) to identify the threshold resulting in the highest F 1 -score and the calculated median of the five optimizing thresholds was used for the prediction over the testing set, using the trained classifiers.</p><p>In the event that a sample was predicted to have more than 100 concepts before submission, the list of concepts was trimmed by the less frequent concepts. In practice, this has only happened to the linear classifiers trained using CEDD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">End-to-end Convolutional Neural Network</head><p>A simple CNN was designed (Table <ref type="table" coords="6,292.09,402.48,4.43,9.96" target="#tab_0">1</ref>) and trained for multi-label classification, thus once again treating concepts as labels. Conv2D stands for 2D convolution layer, GAP for global average pooling and FC for fully connected layer. Training samples were augmented using random square random crops, experimented with different sized squares. In one approach, denoted as CNN-A-256px, we used 256 pixel-wide and excluded the layer Conv2D-5. In CNN-B, the full CNN was used with 128 (CNN-B-128px) and 64 (CNN-B-64px) pixel-wide images. Validation and test samples were simply resized to fit these dimensions, according to the training process. Moreover, to serve as a more intuitive means of comparison, the same architecture as the encoder in the AAE and AE, based on ResNet19 <ref type="bibr" coords="7,413.92,129.72,9.97,9.96" target="#b5">[6]</ref>, was trained for end-to-end classification. It is composed by five ResNet blocks, with the architectures depicted in Tables <ref type="table" coords="7,269.40,153.63,4.98,9.96" target="#tab_1">2</ref> and<ref type="table" coords="7,297.91,153.63,3.88,9.96" target="#tab_2">3</ref>. We employed the same process of data augmentation as in the previously described CNN, resulting in 64×64 images. In Table <ref type="table" coords="7,161.49,177.54,3.88,9.96" target="#tab_2">3</ref>, BN means batch normalization and c in and c out represent the input and output channels for the ResNet block, respectively. The Addition layer depicts the addition of the previous stages: the first, with one convolution layer and the second, with two convolution layers. </p><formula xml:id="formula_2" coords="7,300.14,289.33,180.66,74.28">3 × 3 / 2 h/2 × w/2 × cout BN, ReLU - h × w × cin Conv2D 3 × 3 / 1 h × w × cout BN, ReLU - h × w × cout Conv2D, BN 3 × 3 / 2 h/2 × w/2 × cout Addition - h/2 × w/2 × cout</formula><p>Both end-to-end deep learning models were trained with the AMSGrad optimizer <ref type="bibr" coords="7,169.27,407.83,14.62,9.96" target="#b15">[16]</ref>, with a batch size of 32 and a learning rate of 10 -4 with a decay of 2x10 -5 over each update and parameters β 1 = 0.9 and β 2 = 0.999. For each model, threshold fine tuning was performed by evaluating the F 1 -score performance for multiple thresholds, using the validation data set. Thereafter, we determined the threshold which would yield the optimal mean F 1 -score on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head><p>Alongside with the metrics obtained from our submissions, we also present a brief qualitative analysis as part of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Qualitative Feature Analysis</head><p>The visualizations of the features for five of our models were obtained by training dimensionality reduction algorithms, namely principal component analysis (PCA) and uniform manifold approximation and projection (UMAP) <ref type="bibr" coords="7,441.65,601.21,14.62,9.96" target="#b9">[10]</ref>. The visualizations are presented in Figures <ref type="figure" coords="7,308.27,613.16,4.98,9.96" target="#fig_1">2</ref> and<ref type="figure" coords="7,337.32,613.16,3.88,9.96" target="#fig_2">3</ref>, respectively, using a stratified portion of 5 % of the training set, where the extreme outliers were removed from the figures. The official and open implementation in Python of UMAP 2 was used and the algorithm was configured with 15 as the number of neighbors and 0.1 as the minimum distance.</p><p>For the CNN, the features were extracted at the GAP layer, whereas in the remaining the visualizations depict the features extracted before the classification process. The points associated with the concepts C0441633 (diagnostic scanning) , C0817096 (thoracics) and C0935598 (sagittal planes set) are labeled in red, green and blue, respectively, each painted in an additive fashion. Comparing the two types of dimensionality reduction algorithms, we notice that the representations obtained with PCA have more outliers. In a good representation, the samples will be linearly separable based on their associated concepts. In both types of representations, we can identify regions in the man- ifold in which points of one of the chosen labels are mostly gathered, with this being more perceptible in the representations obtained with UMAP and for the CNN trained end-to-end. In fact, the clustering of representations with common labels is highly expected, even more so for the CNN, since the feature learning in this case was uniquely guided by the target labels. In general, these observations are a rough approximation of the effective performance of each method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quantitative Results</head><p>The metrics on the validation and test set for each submission are depicted in Table <ref type="table" coords="9,162.48,630.98,3.88,9.96" target="#tab_3">4</ref>. Both Val F 1 -score and Test F 1 -score represent the F 1 -scores averaged sample-wise. When applied to the validation set, all concepts of the ground truth were considered, even though the predictions were made only assuming the 1,100 most frequent concepts on the training set, and so always predicting "negative" for the remaining concepts. The scores obtained from end-to-end CNN models (CNN-A-256px, CNN-B-128px, CNN-B-64px) was highly varied, which demonstrates the impact of the input shape, as well as neural network architecture, in the performance of the model. With an image resolution of 64×64, this approach did not perform better than any of the 2-stage procedures. On the other hand, higher resolutions have contributed to significantly better scores.</p><p>Concerning the unsupervised methods, the mean F 1 -score obtained with CEDDs (CEDD) was lower than with the deep learning architectures, probably because they lack representation ability for high-level problems, an effect that was also observed in prior work <ref type="bibr" coords="10,303.66,466.45,14.62,9.96" target="#b12">[13]</ref>. Even with a smaller data set than the previous edition of the concept detection task, unsupervised methods have pushed the performance limits within the initially proposed input shape.</p><p>The early fusion of the features obtained from the two auto-encoders (AE + AAE) was also beneficial, resulting in a higher score than any of the two forms independently(AE and AAE), suggesting that this aggregation was not entirely redundant, thus providing another useful distribution.</p><p>It is also worth noting that, much unlike in our previous participations in the same task, the instance-wise mean F 1 -scores on the testing set were higher than on the validation set. This effect is even more noteworthy, since these methods relied on the validation set for threshold optimization, and as such the classifiers were fit for both the training and validation sets. This consistent discrepancy is due to the fact that the test set did not include any new concepts that were not present in the overall data set provided at the beginning of the challenge, whereas the validation set contained some concepts which were not present in the training set.</p><p>In the context of the ImageCLEFmed Caption challenge, we did an assessment of feature learning techniques for concept detection from radiology images of several medical imaging modalities. The extraction of informative -and intriguingvisual features can yield great potential for multiple use cases in medical imaging systems, including automated image labelling and content-based retrieval.</p><p>We had confirmed the greater potential of deeper architectures for the construction of more powerful representations, in comparison with low-level feature extraction algorithms. With the data set size being significantly smaller in this edition of the challenge, this was seen as an opportunity to compare end-to-end classification models with the use of unsupervised learning methods. The outperforming CNN model had a larger image size as input, making this factor a counterbalance to obtain a better F 1 -score than with the unsupervised models.</p><p>In fact, at a late stage of these experiments, we have identified that the attempted resolution of 64 × 64 is insufficient to attain better results. In the end, a simple CNN with a higher resolution showed the best performance among these submissions. Time constraints have not enabled us to combine the two ideas together in our submissions.</p><p>The quality of the results obtained in this edition may also be attributed to the use of threshold tuning to optimize the F 1 -score. Without an adjustment of the classifiers' operating point, these methods would have a focus towards the highest accuracy of a prediction, which is not as useful in the context of information retrieval. When the number of positive samples is low, the potential retrieval of less relevant entries is compensated by a significantly greater chance of receiving relevant images. Nevertheless, we understand that the focus of a single metric can distort the perception of quality among multiple methods in the challenge, such that a change of performance metric could result in different rankings <ref type="bibr" coords="11,175.53,463.03,9.97,9.96" target="#b7">[8]</ref>. Therefore, it may be insightful for future editions to also present other metrics alongside the main metric, such as the mean precision and recall on the testing set.</p><p>This year presented an increase in participants engagement in the challenge, which might echo the interest in solving timely situations in medical information retrieval and automated medical data analysis. We believe that further investment in the challenge, both from participants and organizers, will enable the implementation of these solutions in real-world scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,261.82,345.82,8.97;3,134.77,272.78,345.83,8.97;3,134.77,283.74,345.83,8.97;3,208.47,115.84,198.43,132.28"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Number of positives of each label, for the 1,100 more frequents ones. The concepts C0441633 (diagnostic scanning), C1533810(placed), C0237058 (no hydronephrosis), C0205160 (ruled out), C4282132 (malignancy) and C0181209 (hooks) are marked.</figDesc><graphic coords="3,208.47,115.84,198.43,132.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,145.51,585.81,324.33,8.97;8,225.43,458.99,170.08,113.39"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The 2D projections of the features, obtained with PCA, for each model.</figDesc><graphic coords="8,225.43,458.99,170.08,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,141.55,479.79,332.26,8.97;9,225.43,352.97,170.08,113.39"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The 2D projections of the features, obtained with UMAP, for each model.</figDesc><graphic coords="9,225.43,352.97,170.08,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,173.64,533.26,268.07,125.60"><head>Table 1 .</head><label>1</label><figDesc>The specification of the CNN used.</figDesc><table coords="6,173.64,557.22,268.07,101.64"><row><cell cols="4">Layer Type Kernel/Stride Output Shape Details</cell></row><row><cell>Conv2D-1</cell><cell>5 x 5 / 2</cell><cell>64 × 64 × 64</cell><cell>ReLU activation</cell></row><row><cell>Conv2D-2</cell><cell>3 x 3 / 2</cell><cell>32 × 32 × 128</cell><cell>ReLU activation</cell></row><row><cell>Conv2D-3</cell><cell>3 x 3 / 2</cell><cell>16 × 16 × 256</cell><cell>ReLU activation</cell></row><row><cell>Conv2D-4</cell><cell>3 x 3 / 2</cell><cell>8 × 8 × 512</cell><cell>ReLU activation</cell></row><row><cell>Conv2D-5</cell><cell>3 x 3 / 2</cell><cell>4 × 4 × 512</cell><cell>ReLU activation</cell></row><row><cell>GAP</cell><cell>-</cell><cell>512</cell><cell>-</cell></row><row><cell>Dropout</cell><cell>-</cell><cell>-</cell><cell>50 %</cell></row><row><cell>FC</cell><cell>-</cell><cell>1100</cell><cell>sigmoid activation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,147.86,244.17,125.98,124.06"><head>Table 2 .</head><label>2</label><figDesc>ResNet architecture.</figDesc><table coords="7,153.90,266.59,113.89,101.64"><row><cell cols="2">Layer Type Output Shape</cell></row><row><cell>ResBlock</cell><cell>64 × 64 × 64</cell></row><row><cell>ResBlock</cell><cell>32 × 32 × 128</cell></row><row><cell>ResBlock</cell><cell>16 × 16 × 256</cell></row><row><cell>ResBlock</cell><cell>8 × 8 × 256</cell></row><row><cell>ResBlock</cell><cell>4 × 4 × 512</cell></row><row><cell cols="2">GAP, ReLU 512</cell></row><row><cell>Dropout</cell><cell>512</cell></row><row><cell>FC</cell><cell>1100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,300.14,249.40,169.73,48.89"><head>Table 3 .</head><label>3</label><figDesc>Residual block specification.</figDesc><table coords="7,300.14,273.36,169.73,24.93"><row><cell>Layer Type Kernel/Stride Output Shape</cell></row><row><cell>Conv2D</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,134.77,170.69,345.82,145.88"><head>Table 4 .</head><label>4</label><figDesc>The final results obtained in the concept detection task, ranked by the list of all valid submissions.</figDesc><table coords="10,409.17,203.97,60.37,8.97"><row><cell>Val</cell><cell>Test</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.73,655.70,229.65,9.22"><p>Available on GitHub: https://github.com/Enet4/ACEDD</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,144.73,655.70,239.06,9.22"><p>Available on GitHub: https://github.com/lmcinnes/umap</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="programName">Integrated Programme of SR&amp;TD "SOCA"</rs> (Ref. <rs type="grantNumber">CENTRO-01-0145-FEDER-000010</rs>), co-funded by <rs type="programName">Centro 2020 program</rs>, <rs type="funder">Portugal 2020</rs>, <rs type="funder">European Union</rs>, through the <rs type="funder">European Regional Development Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4jSZr6M">
					<idno type="grant-number">CENTRO-01-0145-FEDER-000010</idno>
					<orgName type="program" subtype="full">Integrated Programme of SR&amp;TD &quot;SOCA&quot;</orgName>
				</org>
				<org type="funding" xml:id="_UhzWHPX">
					<orgName type="program" subtype="full">Centro 2020 program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,144.46,337.63,8.97;12,151.53,155.42,300.43,9.22" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="12,334.31,144.46,88.89,8.97">Layer Normalization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14236</idno>
		<ptr target="http://arxiv.org/abs/1607.06450" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,166.97,337.63,8.97;12,151.53,177.93,329.06,8.97;12,151.53,188.89,188.03,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="12,291.32,166.97,189.26,8.97;12,151.53,177.93,226.06,8.97">CEDD: Color and edge directivity descriptor: A compact descriptor for image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Boutalis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-79547-6_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-79547-6_30" />
		<imprint>
			<date type="published" when="2008-01">01 2008</date>
			<biblScope unit="page" from="312" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,200.44,337.63,8.97;12,151.53,211.40,280.79,8.97" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m" coord="12,257.21,211.40,113.65,8.97">Generative Adversarial Nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,222.95,337.63,8.97;12,151.53,233.91,329.07,8.97;12,151.53,244.87,329.07,8.97;12,151.53,255.83,329.07,8.97;12,151.53,266.79,329.07,8.97;12,151.53,277.75,329.06,8.97;12,151.53,288.70,329.07,8.97;12,151.53,299.66,329.06,8.97;12,151.53,310.62,329.06,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,442.95,266.79,37.65,8.97;12,151.53,277.75,236.74,8.97">Overview of ImageCLEF 2019: Challenges, datasets and evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,411.96,277.75,68.63,8.97;12,151.53,288.70,329.07,8.97;12,151.53,299.66,238.43,8.97">Proceedings of the Tenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="12,422.04,299.66,58.54,8.97;12,151.53,310.62,105.69,8.97">LNCS Lecture Notes in Computer Science</title>
		<meeting>the Tenth International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 09-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="12,142.96,322.18,337.63,8.97;12,151.53,333.13,329.06,9.22;12,151.53,344.34,61.20,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,249.41,322.18,178.06,8.97">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1412.6980.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="12,448.31,322.18,32.29,8.97;12,151.53,333.13,190.41,8.97">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,355.65,337.63,8.97;12,151.53,366.61,329.06,8.97;12,151.53,377.56,177.18,9.22" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="12,393.86,355.65,86.72,8.97;12,151.53,366.61,222.09,8.97">The GAN landscape: Losses, architectures, regularization, and normalization</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno>CoRR abs/1807.04720</idno>
		<ptr target="http://arxiv.org/abs/1807.04720" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,389.12,337.63,8.97;12,151.53,400.08,329.06,8.97;12,151.53,411.04,329.06,8.97;12,151.53,421.99,154.98,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,237.67,389.12,242.92,8.97;12,151.53,400.08,329.06,8.97;12,151.53,411.04,15.59,8.97">Medical image classification based on deep features extracted by deep model and statistic feature fusion with multilayer perceptron</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1155/2018/2061516</idno>
		<ptr target="https://doi.org/10.1155/2018/2061516" />
	</analytic>
	<monogr>
		<title level="j" coord="12,178.14,411.04,194.30,8.97">Computational Intelligence and Neuroscience</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018-09">2018. 09 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,433.55,337.63,8.97;12,151.53,444.51,329.06,8.97;12,151.53,455.46,204.48,9.22" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,332.83,433.55,147.76,8.97;12,151.53,444.51,33.61,8.97">Thresholding Classifiers to Maximize F1 Score</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1402.1892" />
	</analytic>
	<monogr>
		<title level="m" coord="12,192.06,444.51,232.80,8.97">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2014-02">feb 2014</date>
			<biblScope unit="volume">8725</biblScope>
			<biblScope unit="page" from="225" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,467.02,337.63,8.97;12,151.53,477.98,223.07,9.22" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.05644" />
		<title level="m" coord="12,401.13,467.02,79.46,8.97;12,151.53,477.98,25.41,8.97">Adversarial autoencoders</title>
		<imprint>
			<date type="published" when="2015-11">nov 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,489.53,337.97,8.97;12,151.53,500.49,325.66,8.97" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="12,293.96,489.53,186.63,8.97;12,151.53,500.49,143.60,8.97">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018-02">Feb 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct coords="12,142.62,512.04,337.98,8.97;12,151.53,523.00,329.07,8.97;12,151.53,533.96,329.08,8.97;12,151.53,544.92,181.46,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,442.95,512.04,37.65,8.97;12,151.53,523.00,221.47,8.97">Overview of the ImageCLEFmed 2019 concept prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,398.22,523.00,82.37,8.97;12,151.53,533.96,144.83,8.97">CLEF2019 Working Notes. CEUR Workshop Proceedings</title>
		<title level="s" coord="12,308.17,533.96,38.81,8.97">CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,556.47,337.97,8.97;12,151.53,567.43,329.06,8.97;12,151.53,578.39,329.06,8.97;12,151.53,589.35,329.06,8.97;12,151.53,600.31,329.06,8.97;12,151.53,611.27,329.06,8.97;12,151.53,622.22,290.20,8.97" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="12,408.36,556.47,72.22,8.97;12,151.53,567.43,203.30,8.97;12,317.16,600.31,163.43,8.97;12,151.53,611.27,329.06,8.97;12,151.53,622.22,36.03,8.97">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Balocco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zahnd</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Demirci</surname></persName>
		</author>
		<editor>S., Albarqouni, S., Lee, S.L., Moriconi, S., Cheplygina, V., Mateus, D., Trucco, E., Granger, E., Jannin, P.</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="180" to="189" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Radiology objects in context (ROCO): A multimodal image dataset</note>
</biblStruct>

<biblStruct coords="12,142.62,633.78,337.97,8.97;12,151.53,644.74,329.06,8.97;12,151.53,655.70,76.16,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,236.90,633.78,243.69,8.97;12,151.53,644.74,228.94,8.97">Feature learning with adversarial networks for concept detection in medical images: Ua.pt bioinformatics at imageclef</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pinho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,420.89,644.74,59.70,8.97;12,151.53,655.70,35.18,8.97">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2018-09">2018. 09 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,118.57,337.97,8.97;13,151.53,129.53,329.06,8.97;13,151.53,140.49,146.79,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,262.70,118.57,217.88,8.97;13,151.53,129.53,181.30,8.97">Unsupervised learning for concept detection in medical images: A comparative analysis</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pinho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Costa</surname></persName>
		</author>
		<idno type="DOI">10.3390/app8081213</idno>
		<ptr target="https://doi.org/10.3390/app8081213" />
	</analytic>
	<monogr>
		<title level="j" coord="13,345.21,129.53,72.92,8.97">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,151.45,337.97,8.97;13,151.53,162.41,329.06,8.97;13,151.53,173.37,159.08,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,247.81,151.45,232.78,8.97;13,151.53,162.41,120.78,8.97">Deep convolutional neural networks for image classification: A comprehensive review</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco_a_00990</idno>
		<ptr target="https://doi.org/10.1162/neco_a_00990" />
	</analytic>
	<monogr>
		<title level="j" coord="13,280.58,162.41,84.36,8.97">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2352" to="2449" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,184.33,337.97,8.97;13,151.53,195.28,329.06,9.22;13,151.53,206.49,104.06,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,282.98,184.33,165.28,8.97">On the Convergence of Adam and Beyond</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryQu7f-RZ" />
	</analytic>
	<monogr>
		<title level="m" coord="13,476.75,184.33,3.84,8.97;13,151.53,195.28,206.84,8.97">ternational Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
