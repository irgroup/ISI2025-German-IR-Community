<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.78,115.96,311.80,12.62;1,224.47,133.89,166.43,12.62">Multimedia Lab @ ImageCLEF 2019 Lifelog Moment Retrieval Task</title>
				<funder ref="#_evj56Js">
					<orgName type="full">Ministry of Innovation and Research, UEFIS-CDI</orgName>
				</funder>
				<funder ref="#_HA8cVnj">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.29,171.61,63.26,8.74"><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
							<email>mdogariu@imag.pub.ro</email>
							<affiliation key="aff0">
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.25,171.61,68.82,8.74"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
							<email>bionescu@imag.pub.ro</email>
							<affiliation key="aff0">
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.78,115.96,311.80,12.62;1,224.47,133.89,166.43,12.62">Multimedia Lab @ ImageCLEF 2019 Lifelog Moment Retrieval Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F06C5C54491D448179E03BB6495961CA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lifelog</term>
					<term>Information Retrieval</term>
					<term>Visual Concepts</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the participation of the Multimedia Lab to the 2019 ImageCLEF Lifelog Moment Retrieval task. Given 10 topics in natural language description, participants are expected to retrieve 50 images for each topic that best correspond to its description. Our method uses the data provided by the organizers, without adding any further annotations. We first remove severely blurred images. Then, according to a list of constraints concerning the images' metadata, we remove uninformative images. Finally, we compute a relevance score based on the detection scores provided by the organizers and select the 50 highest ranked images for submission as these should best match the search query.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Wearable devices have become popular in recent years with technological advances helping in reducing their dimensions and improving their performance. Also, people have become more accustomed to interacting with gadgets, be they smart phones, smart watches, fitness bracelets, wearable cameras, etc. Moreover, lately there has been a growing exposure to multimedia content via every communication channel (e.g., TV, radio, Internet browsing, ads) up to the point where every person has had contact with or heard of wearable devices. By combining these two social trends, lifelogging emerges as a promising research field, where multimodal information is harvested and processed.</p><p>The ImageCLEF 2019 lifelog task <ref type="bibr" coords="1,297.24,531.17,10.52,8.74" target="#b2">[3]</ref> is at its 3 rd edition and has gained traction over the past few years <ref type="bibr" coords="1,255.41,543.12,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="1,267.58,543.12,7.01,8.74" target="#b3">4]</ref>. It has attracted many teams in an information retrieval benchmarking competition, as part of the more general ImageCLEF 2019 campaign <ref type="bibr" coords="1,202.71,567.04,14.61,8.74" target="#b9">[10]</ref>. The purpose of the Lifelog Moment Retrieval Task is to be able to retrieve 50 images from the given dataset that correspond to a given topic (e.g., "Find the moment when u1 was using smartphone when he was walking or standing outside. To be considered relevant, u1 must be clearly using a smartphone and the location is outside."). There are 10 such topics, with different aspects that need to be taken into consideration, such as time, location, number of objects, etc. The extracted images need to be both relevant and diverse with the official metric for the competition being the F 1@10 measure. This metric is the harmonic mean between the precision and recall taken for the first 10 (out of 50) retrieval results for each topic.</p><p>We organize the paper as follows. In Section 2 we explore the state of the art for lifelog retrieval tasks, in Section 3 we explain our approach. Section 4 covers the experimental part and in Section 5 we draw the conclusions and discuss the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous lifelogging competitions <ref type="bibr" coords="2,280.11,303.42,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,292.29,303.42,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="2,301.70,303.42,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="2,311.12,303.42,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="2,320.52,303.42,7.75,8.74" target="#b8">9]</ref> have attracted numerous teams for lifelog retrieval events. Usually, teams shared a common approach to processing the input data and extract relevant information. In 2017, Molino et al. <ref type="bibr" coords="2,465.10,327.33,15.50,8.74" target="#b13">[14]</ref> won the competition with a system which filtered out blurry images and images with low color diversity and ran several CNNs on the remaining images in order to detect objects, concepts, places and persons. Afterwards, they computed a weighted relevance score for each image and selected the highest ranked ones from several clusters. In 2018, Kavallieratou et al. <ref type="bibr" coords="2,351.39,387.11,15.50,8.74" target="#b10">[11]</ref> won the competition with a system which splits the images into clusters based on location and activity and applied several CNNs to the remaining ones in order to classify them in 2,10 and 11 classes, respectively. It is worth mentioning that both of these two approaches implied the manually labeling of a part of the data.</p><p>Another relevant work is of Abadallah et al. <ref type="bibr" coords="2,348.68,449.88,10.52,8.74" target="#b0">[1]</ref> who used CNNs to extract features. Their approach also included a natural language processing component, which proves to be critical for this task. This was used to match concepts and queries, together with an LSTM. Tran et al. <ref type="bibr" coords="2,328.35,485.75,15.50,8.74" target="#b15">[16]</ref> proposed a method where they extract scene features, objects and actions. In addition textual descriptions of the images are created and then combined in an inverted index for retrieval. Tang et. al <ref type="bibr" coords="2,184.85,521.61,15.50,8.74" target="#b14">[15]</ref> and Dogariu and Ionescu <ref type="bibr" coords="2,315.32,521.61,10.52,8.74" target="#b6">[7]</ref> proposed similar techniques, where they applied a blur detection system as a first step, then extracted several types of features such as concepts, places, objects and combined them with textual knowledge.</p><p>As a general trend, most teams tried to first exclude non-informative images from the dataset and then extract several types of features, most notably objects, concepts and places. Another common aspect is that most teams needed further information for running their system, therefore they have manually annotated a part of the training data or used some sort of manual input method that would calibrate their systems. We relied on visual information and metadata parsing to solve this year's challenge, with no additional annotations and a minimum user input. The method is presented in the following section.</p><p>Our approach focuses on excluding uninformative images as a first step and then compute a relevance score for the remaining subset of images. From previous experience, we noticed that being more strict with the criteria for excluding uninformative images leads to better results. The architecture of our system can be seen in Figure <ref type="figure" coords="3,213.03,192.38,3.87,8.74" target="#fig_0">1</ref>. We start our pipeline by running a blur detection system, computing the variance of the Laplacian kernel for each image. This type of computation captures both motion blur and large homogeneous areas, when the camera might have been facing a wall or it could have been blocked by the wearer's arm. The images that do not meet a certain threshold are removed from the pipeline.</p><p>Next, the metadata of the images is checked to correspond to the restrictions imposed by the queried topic. Information about the user's id number, location, time, action, time zone are then used to remove another part of the remaining images. In some cases, this selection of metadata can suffer modifications from one topic to another, as it is described in Section 4.</p><p>At this point, the set of images on which we compute the relevance score has drastically diminished in comparison to the original dataset. Blurry images are not considered relevant in the retrieval process, but there is no metric to asses this parameter. This is a compromise, because some of the images which are part of the correct retrieval results, but have a small amount of blur, may be excluded from the processing pipeline due to our hard decision. Then, we run the remaining set of images through the relevancy score computation process.</p><p>The development dataset contained information regarding the detected attributes, categories and concepts in each image. The attributes refer to different aspects of the scenery, concepts represent the output of an object detector trained on MSCOCO <ref type="bibr" coords="3,195.15,644.16,15.50,8.74" target="#b11">[12]</ref> and the categories are the output of an image classifier trained on Imagenet <ref type="bibr" coords="3,192.03,656.12,9.96,8.74" target="#b4">[5]</ref>. We created a list of unique attributes, categories and concepts that are present in the entire set. Then, we retained the detection confidence for each of these features under the form of sparse vectors for each image. We also kept track of the number of detections of each object in every image, since each object could trigger multiple detections in the same image. For the final part of our algorithm, we manually select several attributes, categories and concepts which are relevant to the queried topic. These can mean that they must either be present or not present in the image's detection results, depending on the topic. More details on this are given in Section 4.</p><p>Having all the needed information available, we proceed to computing the relevance score as the sum of the confidences of features that need to be detected in the image. Mathematically, the score S for a single image is expressed as follows:</p><formula xml:id="formula_0" coords="4,142.14,281.78,338.45,31.18">S = |A| i=1 {s(a i )|a i ∈ L A } + |C| i=1 {s(c i )|c i ∈ L C } + |O| i=1 |Oi| j=1 {s(o i,j )|o i ∈ L O }, (1)</formula><p>where A is the set of all attributes, a i is one attribute from this set, L A is the subset of attributes that we selected as being relevant for the query and they must be present in the image's detection results and s(a i ) is the confidence score of the respective attribute, a i . Similar reasoning is applied for the categories set, C. We denoted the concepts set with O, since these concepts are, in fact, objects from the MSCOCO dataset. As each object can appear several times in a single image, we denote the subset of its detections with |O j |. s(o i,j ) refers to the confidence of the j th detection of object of index i. As opposed to <ref type="bibr" coords="4,451.97,408.76,9.96,8.74" target="#b6">[7]</ref>, we do not use a weighted sum because the weights have to be manually tuned for each individual query and it would stray too much from the idea of automatic processing.</p><p>This score is computed for each image and we rank them in descending order according to it. From previous experience, we saw that many events targeted by topics have a low level of recurrence. In other words, they are isolated events which occur only once in the dataset. Therefore, we decided to improve precision, rather than cluster recall, so we did not apply any diversification method. In the end, we submitted the 50 best ranked images per topic, according to the relevance score.</p><p>We note that the user has to manually select the parameters for both the metadata restrictions and the list of items that drive the relevance score and this is the only manual input required from the user. As far as we know, there has yet to be developed a clear method on how this parameter tuning process could be completely automatized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The development dataset and the assigned topics proved to be challenging, as in previous editions. There was a plethora of data concerning each image that had to be stored under a homogeneous format. Moreover, since there is no fixed template for the conditions imposed by different queries, it is important to have as much information as possible about individual images. Therefore, we stored all the available metadata for all images, even if we only used just a part of it in the retrieval process. To give an idea regarding the magnitude of this effort, we stored 31 different data fields for each of the 82k images in the dataset. We note that 4 of the 31 data fields represented feature vectors of lengths 77, 77, 99 and 302, respectively.</p><p>Our approach involved a common part to all 10 queries, namely the removal of images with a high amount of blur. As mentioned in Section 3, we computed the variance of the Laplacian for each image. We imposed a threshold of 90 for the blur filter which is more aggressive than in previous works as we observed that many images from the final list were previously still suffering from motion blur. The dataset is thus reduced from about 82k images down to roughly 35k images. This is a major reduction in the number of candidate images for the next part of the pipeline.</p><p>In the metadata restrictions that we imposed on the images we used 6 types of parameters: the user's id, the action that was being performed, the local time, the location, the time zone and the person count. We used different combinations of these metadata, with the notable exception of user id and action, which were present in all combinations. We relied on the detection result of the object detector to account for the number of persons in the scene. In Table <ref type="table" coords="5,436.44,371.85,4.98,8.74" target="#tab_0">1</ref> we summarized the types of information that were used for each topic, marking with an 'X' the type of parameter that was used for the respective topic. A complete description of the topics can be found in the competition's overview paper <ref type="bibr" coords="5,462.39,407.72,9.96,8.74" target="#b2">[3]</ref>. </p><formula xml:id="formula_1" coords="5,174.23,484.68,267.29,106.49">× × × × - - 2 × × × × - - 3 × × × - - - 4 × × × - - - 5 × × × - - - 6 × × × × - - 7 × × × × - × 8 × × - - - - 9 × × - - - × 10 × × - - × ×</formula><p>It is important to have a detailed discussion on how and why these parameters were chosen, depending on the queried topic's description. All but one topic, T4, required retrieving images corresponding to u1. However, during submission of the proposed list of images for user 2, on T4, we encountered a problem from the submission platform, which rendered all our proposed images corresponding to u2 as erroneous. Therefore, we eliminated all images from user 2 from our submission. For the activity field, only topic T2 asked for the user to be driving, therefore we imposed the restriction that images should have the "transportation" activity. For all other images we imposed that the user should have any other activity than "transportation".</p><p>The location where the images were taken was also helpful as we had to identify different sequences where the user was at home, in a cafe or leaving from work. We did not impose drastic restrictions regarding the metadata, since we did not want to eliminate potentially relevant images. We also tried to find the moments when the user was in a toy shop for T1 based on the location, but noticed that the location was not synchronized with the respective images, having a gap of about 1 hour in between the images that were taken inside the shop and the images that were annotated as being inside a toy shop. Therefore, location was not decisive for this topic, as we expected.</p><p>Time constraints were imposed only regarding the working hours of regular shops, for T1, the user's working hours, for T2 and T7 and the time interval imposed by T6's topic description. We also used the time zone of the images in order to locate the set of images when the user travelled to China. Lastly, the person count from the concept/object detector was used in topics which specifically asked for a certain number of people to be present in the image.</p><p>Following this step, we compiled a list of categories, concepts and attributes that might fit each topic, individually. One case in which this technique was somewhat successful is for T1, when we were asked to find the moments when the user was looking at various toys, such as electronic trains, model kits and board games. Here, we searched for attributes such as {"playing", "shopping", "gaming", "plastic", "cluttered", "supermarket"}, all of which could be related to a toy shop. These helped us create the attributes list, L A from eq. 1. For the categories list, L C , we selected {"store", "shop", "toy", "train", "arcade"}. We also searched for objects such as {"board", "game", "train", "toy", "model", "kit", "bus"} in the image. These represented the objects list, L O . We applied similar reasoning for the rest of the topics. The length and variety of items that could fit in either of the 3 aforementioned lists changed from one topic to another. For some of them it worked well, whereas for others it gave us disappointing results. The full set of attributes, categories and objects that we used for each topic can be seen in Table <ref type="table" coords="6,252.15,545.75,3.87,8.74" target="#tab_1">2</ref>.</p><p>A somewhat different approach was for topic T2, "Driving home", where the participants were asked to retrieve images when u1 was driving home from the office. Any other departure or arrival point than the ones mentioned in the description render the image irrelevant. Here, we considered that this event can happen at most once each day. Then, we took all the images from the afternoon (time interval between 16 and 20 o'clock) and kept only the ones that had the "transport" label for their activity. This should reduce the set of images to only the ones when the user was driving. Afterwards, we checked to see if there was any pause between successive images, when the user was not driving anymore.</p><p>Since we noticed that the "transport" action is continuous throughout an entire car drive interval, having a pause in this interval would mean that the user had an intermediate stop on his way home and would remove the image from the list. The official results of our run can be seen in Table <ref type="table" coords="7,370.56,606.46,3.87,8.74" target="#tab_2">3</ref>. The obtained precision rate was lower than expected. Moreover, high contrast to the cluster recall on several topics also affected the overall F1 measure.</p><p>We submitted 2 runs, but the second one followed the same algorithm, with the only exception being that it excluded pictures taken by the user with his We also present the final state of the learderboard, at the end of the competition in Table <ref type="table" coords="8,192.52,343.43,3.87,8.74" target="#tab_3">4</ref>. Our approach ranked 8 th out of 10 teams. The entries field refers to the number of times that each team tried to submit a run. This accounts for both valid and wrong submissions. Therefore, it is not to be confused with the number of different runs. This year there were a total of 10 teams competing in the LMRT task, the most that have been recorded in Image CLEF Lifelog competitions since they began. We can see that the leader stands far from the rest, while the rest of the ranking remains quite balanced. This proves that lifelogging is gaining traction and draws the attention of more and more research teams in a highly complex challenge.</p><p>The algorithm that we proposed is composed of a selection of the images based on the amount of blur, the metadata that is associated to them and then computing a relevance score in accordance with several manually built lists of items that best describe each particular topic query. We took into consideration an automatic selection of these parameters, but, as reported in <ref type="bibr" coords="9,345.86,189.71,9.96,8.74" target="#b5">[6]</ref>, this is not a trivial task. We also took into consideration using a word2vec model <ref type="bibr" coords="9,361.97,201.66,14.61,8.74" target="#b12">[13]</ref>, but not having enough documents relevant for our task made it difficult to extract relevant meanings for the words inside the topics' descriptions. Therefore, being able to automatically go from a natural language description of the topic to a list of accurately defined terms which best describe the relevant images still remains an open problem.</p><p>Another important aspect is that given the large variety of aspects that are searched for in the set of images, it is difficult to propose a unique system that would solve all 10 queries. Several tuning mechanisms are in order to help the overall architecture adapt to particular tasks. Additionally, training neural nets on the available data, without supplementary annotations, could lead to overfitting, since the provided groundtruth is very scarce in comparison to what neural nets need for being robust enough.</p><p>Once the groundtruth data will be available we plan to run ablation studies to figure out the way each part of our system had an impact on the overall performance, both positive and negative. In conclusion, lifelog moment retrieval remains a challenging task in which it is crucial to understand the provided data and the limitations of current state of the art. Many efforts have been made in this direction due to the Image CLEF Lifelog campaigns, encouraging researchers to take part in this benchmarking competition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,174.57,369.62,266.22,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Pipeline execution of proposed approach on a given query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,441.56,345.83,50.98"><head>Table 1 .</head><label>1</label><figDesc>Metadata combinations used for each topic -× indicates that the respective type of information was used</figDesc><table coords="5,150.77,473.32,313.21,19.22"><row><cell>Topic number User id Activity Location Time</cell><cell>Time zone Person count</cell></row><row><cell>1</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,136.25,188.09,342.87,385.24"><head>Table 2 .</head><label>2</label><figDesc>Attributes, categories and objects lists used for each topic</figDesc><table coords="7,136.25,209.02,342.87,364.31"><row><cell>Topic</cell><cell>Attributes, categories and objects lists</cell></row><row><cell></cell><cell>L A = ['playing', 'shopping', 'gaming', 'plastic', 'cluttered', 'supermarket']</cell></row><row><cell>1</cell><cell>L C = ['store', 'shop', 'toy', 'train', 'arcade']</cell></row><row><cell></cell><cell>L O = ['board', 'game', 'train', 'toy', 'model', 'kit', 'bus']</cell></row><row><cell></cell><cell>L A = ['driving', 'glass', 'metal', 'matte', 'glossy', 'transporting']</cell></row><row><cell>2</cell><cell>L C = ['car interior', 'bus interior', 'cockpit']</cell></row><row><cell></cell><cell>L O = ['car', 'bus']</cell></row><row><cell></cell><cell>L A = ['indoor', 'eating', 'plastic', 'cluttered', 'shopping', 'vegetation']</cell></row><row><cell>3</cell><cell>L C = ['kitchen', 'shop', 'ice', 'living room']</cell></row><row><cell></cell><cell>L O = ['pizza', 'bottle', 'broccoli', 'refrigerator', 'sandwich', 'cup']</cell></row><row><cell></cell><cell>L A = ['indoor', 'cluttered space', 'enclosed area', 'sports', 'spectating']</cell></row><row><cell>4</cell><cell>L C = ['living room', 'soccer field', 'soccer', 'football', 'television']</cell></row><row><cell></cell><cell>L O = ['tv', 'sports ball', 'remote']</cell></row><row><cell></cell><cell>L A = ['indoor', 'socializing']</cell></row><row><cell>5</cell><cell>L C = ['coffee shop', 'cafeteria', 'bar', 'restaurant', 'lobby']</cell></row><row><cell></cell><cell>L O = ['cup', 'person']</cell></row><row><cell></cell><cell>L A = ['indoor', 'eating', 'cluttered space', 'enclosed area', 'plastic']</cell></row><row><cell>6</cell><cell>L C = ['kitchen', 'living room', 'dining room', 'food', 'pizzeria', 'picnic']</cell></row><row><cell></cell><cell>L O = ['pizza', 'bottle', 'sandwich', 'cup', 'dining table', 'cake', 'toaster']</cell></row><row><cell></cell><cell>L A = ['indoor', 'socializing', 'cloth']</cell></row><row><cell>7</cell><cell>L C = ['coffee shop', 'cafeteria', 'bar', 'restaurant', 'lobby']</cell></row><row><cell></cell><cell>L O = ['cup', 'person']</cell></row><row><cell></cell><cell>L A = ['natural', 'pavement', 'concrete', 'vegetation', 'trees', 'sunny']</cell></row><row><cell>8</cell><cell>L C = ['outdoor', 'phone booth', 'park', 'street', 'garden']</cell></row><row><cell></cell><cell>L O = ['cell phone', 'cell phone', 'cell phone', 'car', 'bus', 'traffic light']</cell></row><row><cell></cell><cell>L A = ['glass', 'glossy', 'natural light', 'indoor', 'manmade']</cell></row><row><cell>9</cell><cell>L C = ['indoor']</cell></row><row><cell></cell><cell>L O = ['person']</cell></row><row><cell></cell><cell>L A = ['business', 'indoor lighting', 'man-made', 'paper', 'research']</cell></row><row><cell>10</cell><cell>L C = ['indoor', 'restaurant', 'conference', 'classroom', 'lobby']</cell></row><row><cell></cell><cell>L O = ['person', 'suitcase', 'bottle', 'chair', 'dining table']</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.77,115.91,345.82,207.14"><head>Table 3 .</head><label>3</label><figDesc>Official results of our submitted run.</figDesc><table coords="8,134.77,134.97,345.82,188.09"><row><cell cols="4">Topic number P@10 CR@10 F1@10</cell></row><row><cell>1</cell><cell>0.2</cell><cell>0.5</cell><cell>0.285</cell></row><row><cell>2</cell><cell>0.3</cell><cell>0.047</cell><cell>0.082</cell></row><row><cell>3</cell><cell>0.1</cell><cell>0.055</cell><cell>0.071</cell></row><row><cell>4</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>5</cell><cell>0.7</cell><cell>0.22</cell><cell>0.33</cell></row><row><cell>6</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>7</cell><cell>0.1</cell><cell>1</cell><cell>0.181</cell></row><row><cell>8</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>9</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>10</cell><cell>0.3</cell><cell>0.33</cell><cell>0.31</cell></row><row><cell>Mean</cell><cell>0.17</cell><cell>0.215</cell><cell>0.127</cell></row><row><cell cols="4">mobile phone. However, it obtained the same exact result so we decided to only</cell></row><row><cell>present the results of the first run.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,206.24,419.26,202.87,136.91"><head>Table 4 .</head><label>4</label><figDesc>Leaderboard    </figDesc><table coords="8,206.24,438.31,202.87,117.85"><row><cell>Position</cell><cell>Team name</cell><cell cols="2">F1@10 Entries</cell></row><row><cell>1</cell><cell>HCMUS</cell><cell>0.61</cell><cell>4</cell></row><row><cell>2</cell><cell>ZJUT</cell><cell>0.44</cell><cell>8</cell></row><row><cell>3</cell><cell>NICT</cell><cell>0.367</cell><cell>3</cell></row><row><cell>4</cell><cell>Baseline</cell><cell>0.289</cell><cell></cell></row><row><cell>5</cell><cell>ATS</cell><cell>0.255</cell><cell>20</cell></row><row><cell>6</cell><cell>DCU</cell><cell>0.238</cell><cell>5</cell></row><row><cell>7</cell><cell>Regim Lab</cell><cell>0.188</cell><cell>10</cell></row><row><cell>8</cell><cell cols="2">Multimedia Lab (ours) 0.127</cell><cell>5</cell></row><row><cell>9</cell><cell>TU Chemnitz</cell><cell>0.117</cell><cell>16</cell></row><row><cell>10</cell><cell>University of Aveiro</cell><cell>0.057</cell><cell>7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was supported by the <rs type="funder">Ministry of Innovation and Research, UEFIS-CDI</rs>, project <rs type="projectName">SPIA-VA</rs>, agreement <rs type="grantNumber">2SOL/2017</rs>, grant <rs type="grantNumber">PN-III-P2-2.1-SOL-2016-02-0002</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_evj56Js">
					<idno type="grant-number">2SOL/2017</idno>
					<orgName type="project" subtype="full">SPIA-VA</orgName>
				</org>
				<org type="funding" xml:id="_HA8cVnj">
					<idno type="grant-number">PN-III-P2-2.1-SOL-2016-02-0002</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,547.72,337.63,7.86;9,151.52,558.65,311.39,7.89" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,417.34,547.72,63.25,7.86;9,151.52,558.68,166.76,7.86">Regim lab team at imageclef lifelog moment retrieval task</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">B</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Feki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ezzarka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2125. September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,569.38,337.64,7.86;9,151.52,580.34,329.07,7.86;9,151.52,591.30,329.07,7.86;9,151.52,602.26,329.07,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,151.52,580.34,308.83,7.86">Overview of ImageCLEFlifelog 2017: Lifelog Retrieval and Summarization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org/Vol-1866/" />
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,591.30,109.35,7.86">CLEF2017 Working Notes</title>
		<title level="s" coord="9,270.59,591.30,124.28,7.86">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
			<biblScope unit="volume">1866</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,612.96,337.63,7.86;9,151.52,623.92,329.07,7.86;9,151.52,634.88,329.07,7.86;9,151.52,645.84,329.07,7.86;9,151.52,656.80,46.58,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,151.52,623.92,329.07,7.86;9,151.52,634.88,34.93,7.86">Overview of ImageCLEFlifelog 2019: Solve my life puzzle and Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="9,206.63,634.88,103.66,7.86">CLEF2019 Working Notes</title>
		<title level="s" coord="9,317.16,634.88,118.59,7.86">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,119.67,337.64,7.86;10,151.52,130.63,329.07,7.86;10,151.52,141.59,329.07,7.86;10,151.52,152.55,329.07,7.86;10,151.52,163.51,34.31,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,442.93,119.67,37.66,7.86;10,151.52,130.63,329.07,7.86;10,151.52,141.59,24.44,7.86">Overview of ImageCLEFlifelog 2018: Daily Living Understanding and Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org/Vol-2125/" />
	</analytic>
	<monogr>
		<title level="m" coord="10,198.17,141.59,106.07,7.86">CLEF2018 Working Notes</title>
		<title level="s" coord="10,312.32,141.59,121.01,7.86">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
			<biblScope unit="volume">2125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,174.47,337.63,7.86;10,151.52,185.43,329.07,7.86;10,151.52,196.39,198.96,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,403.66,174.47,76.93,7.86;10,151.52,185.43,134.02,7.86">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,330.56,185.43,150.03,7.86;10,151.52,196.39,93.91,7.86">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,207.34,337.64,7.86;10,151.52,218.30,329.07,7.86;10,151.52,229.26,329.07,7.86;10,151.52,240.22,231.99,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,254.84,207.34,225.75,7.86;10,151.52,218.30,43.31,7.86">A textual filtering of hog-based hierarchical clustering of lifelog data</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="10,446.53,218.30,34.06,7.86;10,151.52,229.26,289.87,7.86">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017</date>
			<biblScope unit="page">1866</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,251.18,337.63,7.86;10,151.52,262.14,329.07,7.86;10,151.52,273.10,238.40,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,253.40,251.18,227.19,7.86;10,151.52,262.14,15.40,7.86">Multimedia lab @ imageclef 2018 lifelog moment retrieval task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,188.91,262.14,291.68,7.86;10,151.52,273.10,24.01,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
			<biblScope unit="volume">2125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,284.06,337.64,7.86;10,151.52,295.02,189.91,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,398.34,284.06,82.26,7.86;10,151.52,295.02,42.29,7.86">Overview of ntcir-12 lifelog task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Albatal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,214.81,295.02,27.41,7.86">NTCIR</title>
		<meeting><address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07">July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,305.98,337.64,7.86;10,151.52,316.93,255.70,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,206.05,316.93,135.25,7.86">Overview of ntcir-13 lifelog-2 task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Albatal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,348.22,316.93,30.33,7.86">NTCIR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,327.89,337.98,7.86;10,151.52,338.85,329.07,7.86;10,151.52,349.81,329.07,7.86;10,151.52,360.77,329.07,7.86;10,151.52,371.73,329.07,7.86;10,151.52,382.69,329.07,7.86;10,151.52,393.65,329.07,7.86;10,151.52,404.61,329.07,7.86;10,151.52,415.56,329.07,7.86;10,151.52,426.52,251.10,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,268.29,382.69,212.30,7.86;10,151.52,393.65,124.23,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,297.67,393.65,182.92,7.86;10,151.52,404.61,329.07,7.86;10,151.52,415.56,143.86,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="10,343.38,415.56,137.22,7.86;10,151.52,426.52,27.78,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF 2019)<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-12">September 9-12 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="10,142.62,437.48,337.98,7.86;10,151.52,448.41,167.04,7.89" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,398.91,437.48,81.69,7.86;10,151.52,448.41,68.20,7.89">Retrieving events in life logging 2125</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cuevas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>García</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,459.40,337.97,7.86;10,151.52,470.36,329.07,7.86;10,151.52,481.32,199.02,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,207.08,470.36,170.78,7.86">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,397.66,470.36,82.93,7.86;10,151.52,481.32,75.98,7.86">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,492.28,337.97,7.86;10,151.52,503.24,329.07,7.86;10,151.52,514.19,217.12,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,407.41,492.28,73.17,7.86;10,151.52,503.24,232.55,7.86">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,404.48,503.24,76.12,7.86;10,151.52,514.19,123.83,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="3111" to="3119" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,525.15,337.98,7.86;10,151.52,536.11,329.07,7.86;10,151.52,547.04,182.59,7.89" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,168.13,536.11,312.46,7.86;10,151.52,547.07,59.46,7.86">Vc-i2r@ imageclef2017: Ensemble of deep learned features for lifelog video summarization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G D</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Subbaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
			<biblScope unit="volume">1866</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,558.03,337.98,7.86;10,151.52,568.99,329.07,7.86;10,151.52,579.92,191.79,7.89" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,406.73,558.03,73.86,7.86;10,151.52,568.99,329.07,7.86;10,151.52,579.95,68.66,7.86">Visual concept selection with textual knowledge for understanding activities of daily living and life moment retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2125. September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,590.91,337.98,7.86;10,151.52,601.87,329.07,7.86;10,151.52,612.80,120.06,7.89" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,151.52,601.87,329.07,7.86">Lifelog moment retrieval with visual concept fusion and text-based query expansion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dinh-Duy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">D</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">K</forename><surname>Vo-Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">A</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2125. September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
