<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.07,115.96,311.22,12.62;1,159.51,133.89,296.33,12.62;1,145.79,151.82,323.78,12.62">UA.PT Bioinformatics at ImageCLEF 2019: Lifelog Moment Retrieval based on Image Annotation and Natural Language Processing</title>
				<funder>
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_C8XX5rQ">
					<orgName type="full">of SR&amp;TD SOCA</orgName>
				</funder>
				<funder ref="#_gYeUMVm">
					<orgName type="full">Portugal 2020</orgName>
				</funder>
				<funder>
					<orgName type="full">European Regional Development Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,174.30,189.49,67.66,8.74"><forename type="first">Ricardo</forename><surname>Ribeiro</surname></persName>
							<email>rfribeiro@ua.pt</email>
							<affiliation key="aff0">
								<orgName type="department">IEETA/DETI</orgName>
								<orgName type="institution">University of Aveiro</orgName>
								<address>
									<postCode>3810-193</postCode>
									<settlement>Aveiro</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.63,189.49,86.16,8.74"><forename type="first">António</forename><forename type="middle">J R</forename><surname>Neves</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IEETA/DETI</orgName>
								<orgName type="institution">University of Aveiro</orgName>
								<address>
									<postCode>3810-193</postCode>
									<settlement>Aveiro</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.15,189.49,77.90,8.74"><forename type="first">José</forename><forename type="middle">Luis</forename><surname>Oliveira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IEETA/DETI</orgName>
								<orgName type="institution">University of Aveiro</orgName>
								<address>
									<postCode>3810-193</postCode>
									<settlement>Aveiro</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.07,115.96,311.22,12.62;1,159.51,133.89,296.33,12.62;1,145.79,151.82,323.78,12.62">UA.PT Bioinformatics at ImageCLEF 2019: Lifelog Moment Retrieval based on Image Annotation and Natural Language Processing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7838AE11C477386E04BD4F31E46AC080</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>lifelog</term>
					<term>moment retrieval</term>
					<term>image processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The increasing number of mobile and wearable devices is dramatically changing the way we can collect data about a person's life. These devices allow recording our daily activities and behavior in the form of images, video, biometric data, location and other data. This paper describes the participation of the Bioinformatics group of the Institute of Electronics and Engineering Informatics of University of Aveiro in the ImageCLEF lifelog task, more specifically in the Lifelog Moment Retrieval sub-task. The approach to solve this sub-task is divided into three stages. The first one is the pre-processing of the lifelog dataset for a selection of the images that contain relevant information in order to reduce the amount of images to be processed and obtain additional visual information and concepts from the ones to be considered. In the second step, the query topics are analyzed using Natural Languages Processing tools to extract relevant words to retrieve the desired moment. This words are compared with the visual concepts words, obtained in the pre-processing step using a pre-trained word2vec model, to compute a confidence score for each processed image. An additional step is used in the last two runs, in order to include the images not processed in the first step and improve the results of our approach. A total of 6 runs were submitted and the results obtained show an evolution with each submission. Although the results are not yet competitive with other teams, this challenge is a good starting point for our research work. We pretend to continue the development of a lifelogging application in the context of a research project, so we expect to participate in the next year in the ImageCLEFlifelog task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past few years, with the increase of wearable and smart technologies, the term lifelogging has received significant attention from both research and commercial communities. There is no general definition for lifelogging, but an appropriate definition is given by Dodge and Kitchin <ref type="bibr" coords="2,367.35,130.95,10.52,8.74" target="#b2">[3]</ref> as "a form of pervasive computing, consisting of a unified digital record of the totality of an individuals experiences, captured multi-modally through digital sensors and stored permanently as a personal multimedia archive" <ref type="bibr" coords="2,317.73,166.81,9.96,8.74" target="#b3">[4]</ref>. In a simple way, lifelogging is the process of tracking and record personal data created by our activities and behaviour.</p><p>The number of workshops and tasks for research has increased over the last few years and among them are the main tasks of ImageCLEF 2019 lab <ref type="bibr" coords="2,467.31,214.84,9.96,8.74" target="#b5">[6]</ref>: lifelogging, medicine, nature, and security. The lifelogging task aims to bring the attention of lifelogging to an as wide as possible audience and to promote research into some of the key challenges of the coming years <ref type="bibr" coords="2,400.19,250.71,9.96,8.74" target="#b1">[2]</ref>.</p><p>Our motivation for this work is the great potential that personal lifelogs have in numerous applications, including memory and moments retrieval, daily living understanding, diet monitoring, or disease diagnosis, among others. For example: in Alzheimer's disease, people have memory problems and using a lifelog application the person with the disease can be followed by a specialist or can help the person to remember certain moments or activities of her last days or months.</p><p>This paper is organized as follows: the paper starts with an introductory section. Section 2 provides a brief introduction to the ImageCLEF lifelog and the sub-task Lifelog Moment Retrieval. The proposed approach used in our best run is described in Section 3. In Section 4, the results of all submitted runs obtained in the LMRT sub-task are presented and described the differences of each run compared to the implementation of our best result. Finally, a summary of the work presented in this paper, concluding remarks, and the future work are presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>The ImageCLEFlifelog 2019 task <ref type="bibr" coords="2,282.31,488.33,10.52,8.74" target="#b0">[1]</ref> is divided into two different sub-tasks: the Lifelog moment retrieval (LMRT) and the Puzzle sub-task. In this work, we only addressed the LMRT sub-task, as a starting point for a research work that we intend to develop with the aim of helping in some problems that exist around the world.</p><p>In the LMRT sub-task, the participants have to retrieve a number of specific predefined activities in a lifelogger's life. For example, they should return the relevant moments for the query "Find the moment(s) when I was shopping". Particular attention should be paid to the diversification of the selected moments with respect to the target scenario. The ground truth for this sub-task was created using manual annotation <ref type="bibr" coords="2,281.24,608.09,9.96,8.74" target="#b0">[1]</ref>.</p><p>ImageCLEFlifelog dataset is a completely new rich multimodal dataset which consists of 29 days of data from one lifelogger, namely: images (1,500-2,500 per day from wearable cameras), visual concepts (automatically extracted visual concepts with varying rates of accuracy), semantic content (semantic locations, semantic activities) based on sensor readings (via the Moves App) on mobile devices, biometrics information (heart rate, galvanic skin response, calorie burn, steps, continual blood glucose, etc.), music listening history, computer usage (frequency of typed words via the keyboard and information consumed on the computer via ASR of on-screen activity on a per-minute basis) <ref type="bibr" coords="3,411.93,166.81,9.96,8.74" target="#b0">[1]</ref>. However, In this work we use the images, the visual concepts and the semantic content of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this sub-task, we submitted 6 runs. Although our results have a lower F1measure@10 in the test topics, this sub-task has become a good starting point for our research work in lifelog. In this section, we present the proposed approcah used in the last submission (run 6 of Table <ref type="table" coords="3,321.03,283.46,4.43,8.74">1</ref>) that was our best result. However, some more details about the other runs are mentioned in Section 3.</p><p>The final approach used to the LMRT task is divided into three stages, respectively:</p><p>-Pre-Processing: The large amount of data (images) are analyzed and some images images are excluded based on a low-level image analysis algorithm proposed by the authors in order to reduce the search time that will be needed to analyze each topic. The images that are considered to be valuable are then processed using several state-of-art algorithms to extract information from the images. -Retrieval: The relevant words of the query topic are extracted using tools of Natural Language Processing (NLP). These words are compared with the information obtained from the images in the pre-processing stage, through a state-of-art model used to produce word embedding. Finally, we assign a score to each analyzed image for the query topic. -Post-Processing: Images that were skipped in the pre-processing step are reused depending on a defined distance from the images selected in the retrieval step in order to fulfill with the goal of the sub-task, where all the images was used and annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-Processing</head><p>We consider the pre-processing of the image dataset in a lifelogging application a very important stage, in order to select the relevant images and reduce the processing time and the errors in the annotation, extracting only relevant information from the lifelog images. In this step, we proposed a method for automatic selection of the lifelog images that contain relevant information using a blur/focus measure operator, called modified Laplacian. We use this method to extract low-level features and machine learning algorithms, namely k-nearest neighbors, to classify these features and decide if an image is valuable in this context. Figure <ref type="figure" coords="3,412.13,656.12,4.98,8.74" target="#fig_0">1</ref> shows a block diagram presenting the steps used in the proposed method. This proposal is described in a manuscript submitted by the authors but not yet published.</p><p>Images that are not selected in this step are not processed in the retrieval step, and can be reused in the post-processing step to fulfill with the sub-task expected results.</p><p>In a lifelogging application, the most important characteristics that we can extract from images are the objects and the elements that a certain environment contains. Some content of the selected images were extracted using the label detection of Google Cloud Vision API, YoloV3 <ref type="bibr" coords="4,339.83,361.56,10.52,8.74" target="#b7">[8]</ref> and the information provided by the organizers (location, activity and visual concepts). The data associated with each image is stored into JSON files of each day of the lifelogger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Retrieval</head><p>In the retrieval step the images are selected according to the query topic entered for the desired moment search. We use the SpaCy library <ref type="bibr" coords="4,420.90,451.94,10.52,8.74" target="#b4">[5]</ref> to analyze the topic narrative and extract relevant words. These words are divided into five categories, among them "activities", "locations", "relevant things", "other things", and "other words".</p><p>In order to assign words to each category we define some linguistic rules, such as semantic and syntactic rules. Semantic rules build the meaning of the sentence from its words and how words combine syntactically. Syntax refers to the rules that govern the ways in which words combine to form phrases, and sentences. For example: if the sentence has an auxiliary verb, the main verb usually corresponds to an activity and the words that follow the main verb may be things or locations involved in this activity. Figure <ref type="figure" coords="4,394.21,571.96,4.98,8.74">2</ref> presents linguistic annotations generated by SpaCy library for topic number 10 narrative of the test topics. The words extracted from this topic are "attending", "meeting" and "China", and then divided into the categories "activities", "relevant things" and "locations", respectively.</p><p>A comparison is made between the words extracted from the narrative of the query topic and the concept words of each image selected in pre-processing, using a word2vec pre-trained model. We used a model trained on part of Google Fig. <ref type="figure" coords="5,202.12,221.20,4.13,7.89">2</ref>. Linguistic annotations generated by SpaCy library <ref type="bibr" coords="5,420.59,221.22,9.22,7.86" target="#b4">[5]</ref>.</p><p>News dataset (about 100 billion words). This model contains 300-dimensional vectors for 3 million words and phrases. The gensim libary <ref type="bibr" coords="5,393.88,266.62,10.52,8.74" target="#b8">[9]</ref> allows us to load the word2vec model and compute the cosine similarity between words.</p><p>For each category defined previously we have a similarity (values from 0 to 1). As the concepts that we have for each image are not very large and accurate to decide if the image correspond to the query topic or not, we use the sequence of images in a certain distance of the image that is being analyzed and their similarities to assign a score to the category of those words. These categories have different weights associated (the sum of all categories weight is equal to 1), therefore, the confidence score of each image is computed using these weights and the scores of categories.</p><p>Finally, we determine a general threshold to select the images for the query topics. Images with confidence score above the threshold are selected for the query topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Post-Processing</head><p>In the last step, some images that were not analyzed in the pre-processing step are reused to increase the performance of the result images for each topic. Thus, the images between two selected images in the retrieval step, are re-selected if the sequential distance between these two images doesn't exceed a certain threshold obtained experimentally.</p><p>Figure <ref type="figure" coords="5,180.08,518.16,4.98,8.74" target="#fig_2">3</ref> shows three images selected for query topic number 10 as final output of the proposed approach. The image (b) was rejected in the pre-processing stage and the images (a) and (b) were selected in the retrieval stage. Then, in the postprocessing stage, since image (b) is sequentially between the images (a) and (b), image (b) was reselected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We submitted 6 runs on the LMRT sub-task. In this sub-task, the final score is computed as an arithmetic mean of all queries. The ranking metrics was the F1-measure@10, which gives equal importance to diversity (via CR@10) and relevance (via P@10), Cluster Recall and Precision at top 10 results, respectively.  We describe the last submission (run 6) in Section 3 and the other submissions follow the same pre-processing approach. However, we made changes in the retrieval step and added the post-processing step in the implementation of the other submissions. The post-processing step was only implemented in the two last submissions (run 5 and run 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Run 1</head><p>In the first submission (run 1), the query topic is analyzed using the title and narrative. From the title, stop words have been removed using the scikit-learn tools <ref type="bibr" coords="6,159.87,570.43,9.96,8.74" target="#b6">[7]</ref>. From the narrative, the nouns and the main verbs associated to an auxiliary verb were extracted using the SpaCy library <ref type="bibr" coords="6,376.31,582.39,10.52,8.74" target="#b4">[5]</ref> without rules. Then, we used the same word2vec model, described for run 6, to obtain the similarity of the topic words and concept words extracted in pre-processing step.</p><p>In this run, the topic words were not divided into categories. We check if the topic words had a similarity above a threshold and if two or more topic words score above that threshold. In this case, the sequence of images in a certain distance is selected. The confidence score is the same for all selected images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Run 2</head><p>In the second submission (run 2), we used the same approach of the run 1, however in the query topic analysis we only used the title.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Run 3</head><p>In this submission we only analyze the narrative of the query topic and we define linguistic rules to extract relevant words. The words were divided into four categories, that is, the same categories of run 6 but without the "other words" category. We check if the words in the categories had a similarity above a threshold and if two or more scores of the categories are above that threshold. In this case, the sequence of images is selected. The confidence score is assigned by the number of categories that had the score above that threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Run 4</head><p>This submission follows the same approach of run 3, however some thresholds were adjusted and it is used the fifth category described in the run 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Run 5</head><p>In this submission, we reorganize the implementation of the run 4 and we defined different weights for each category of words in order to calculate the confidence score of each image. Then, we added the post-processing step to our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">UA.PT Bioinformatics Results</head><p>The results obtained are shown in Table <ref type="table" coords="7,320.01,476.49,3.87,8.74">1</ref>, along with the best result in this task, for comparison. The results of all participating team can be found in <ref type="bibr" coords="7,467.31,488.44,9.96,8.74" target="#b0">[1]</ref>. We can observe that our results are still far from the best ones on this task but we consider that our first participation in the ImageCLEF lifelog 2019 was an excellent starting point for our research work. Moreover, the best team already participated in the past.</p><p>One of the main problems in our approaches is the low information and visual concepts extracted from the images of the lifelog data, for example the word "toyshop" never appears associated to an image. For this reason, half of the analyzed query topics did not obtain any result in the evaluation metrics. Using other state-of-art algorithms and API's to obtain a more rich description of the images may increase the performance. Some visual concepts of the images are in form of bigrams or trigrams, for example "electronic device", "ice cream", "cell phone", "car interior", among others. As in our approach we only compute cosine similarity between two words, some of the visual concepts are lost and the result of our approaches decrease.</p><p>Table <ref type="table" coords="8,164.13,115.91,4.13,7.89">1</ref>. F1-measure@10 of each run submitted by us and the best team run in the LMRT task.</p><p>Team Run Name F1-measure@10 (%) In order to solve this problem, the identification of bigrams and trigrams is one of the future implementations for our application. Another way to increase the performance of our work is the development of new linguistic rules, in order to analyze the description of the query topic and obtain more information for the retrieval step. For example: identify negative sentences and exclude some objects or environments that are not relevant.</p><note type="other">Our</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>The Lifelog Moment Retrieval (LMRT) sub-task of ImageCLEF lifelog 2019 was an excellent starting point for our research work in lifelogging. Although our results have a low score in this sub-task, we observe an evolution in each submitted run. After these results, our goal is to continue the development and improvement of our implementation.</p><p>For future work, we intend to do more state-of-art research for the recognition of visual concepts and text mining methods. In order to develop an efficient application, we are going to create ontologies for daily activities and create hierarchical relationships for the words that can appear in the visual concepts.</p><p>Developing a user interface is also one of our priorities for user interaction and visualization of search results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,148.42,230.31,318.53,7.89;4,134.77,115.84,345.82,99.71"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Block diagram of the main steps implemented in the proposed method.</figDesc><graphic coords="4,134.77,115.84,345.82,99.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,185.61,391.17,244.13,7.89;6,229.87,248.48,155.61,116.71"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Some selected images for the query topic number 10.</figDesc><graphic coords="6,229.87,248.48,155.61,116.71" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgments</head><p>Supported by the <rs type="programName">Integrated Programme</rs> <rs type="funder">of SR&amp;TD SOCA</rs> (Ref. <rs type="grantNumber">CENTRO-01-0145-FEDER-000010</rs>), co-funded by <rs type="programName">Centro 2020 program</rs>, <rs type="funder">Portugal 2020</rs>, <rs type="funder">European Union</rs>, through the <rs type="funder">European Regional Development Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_C8XX5rQ">
					<idno type="grant-number">CENTRO-01-0145-FEDER-000010</idno>
					<orgName type="program" subtype="full">Integrated Programme</orgName>
				</org>
				<org type="funding" xml:id="_gYeUMVm">
					<orgName type="program" subtype="full">Centro 2020 program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,612.96,342.24,7.86;8,146.91,623.92,333.68,7.86;8,146.91,634.88,333.67,7.86;8,146.91,645.84,333.68,7.86;8,146.91,656.80,62.74,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,246.29,623.92,234.31,7.86;8,146.91,634.88,123.15,7.86">Overview of ImageCLEFlifelog 2019: Solve my life puzzle and Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="8,293.67,634.88,186.92,7.86;8,146.91,645.84,97.23,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,119.67,342.24,7.86;9,146.91,130.63,333.68,7.86;9,146.91,141.59,333.68,7.86;9,146.91,152.55,55.45,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,442.93,119.67,37.65,7.86;9,146.91,130.63,330.11,7.86">Overview of imagecleflifelog 2018: daily living understanding and lifelog moment retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,160.85,141.59,230.56,7.86">CLEF2018 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>CEURWS</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,163.51,342.24,7.86;9,146.91,174.47,333.68,7.86;9,146.91,185.40,117.15,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,249.37,163.51,231.21,7.86;9,146.91,174.47,143.65,7.86">outlines of a world coming into existence&apos;: pervasive computing and the ethics of forgetting</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kitchin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,298.65,174.47,181.94,7.86;9,146.91,185.43,25.14,7.86">Environment and planning B: planning and design</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="445" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,196.39,342.24,7.86;9,146.91,207.32,285.27,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,354.89,196.39,121.61,7.86">Lifelogging: Personal big data</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Doherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,146.91,207.34,207.78,7.86">Foundations and Trends R in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="125" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,218.30,342.24,7.86;9,146.91,229.26,333.68,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,253.65,218.30,226.94,7.86;9,146.91,229.26,258.13,7.86">spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Montani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="9,138.35,240.22,342.24,7.86;9,146.91,251.18,333.68,7.86;9,146.91,262.14,333.68,7.86;9,146.91,273.10,333.67,7.86;9,146.91,284.06,333.68,7.86;9,146.91,295.02,333.68,7.86;9,146.91,305.98,333.68,7.86;9,146.91,316.93,333.68,7.86;9,146.91,327.89,333.68,7.86;9,146.91,338.85,141.36,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,221.90,295.02,258.69,7.86;9,146.91,305.98,75.92,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,243.10,305.98,237.49,7.86;9,146.91,316.93,333.68,7.86;9,146.91,327.89,76.05,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="9,230.46,327.89,170.59,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF 2019)<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="9,138.35,349.81,342.24,7.86;9,146.91,360.77,333.68,7.86;9,146.91,371.73,333.68,7.86;9,146.91,382.66,325.87,7.89" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,393.75,371.73,86.84,7.86;9,146.91,382.69,73.64,7.86">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,228.21,382.69,155.11,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,393.65,342.25,7.86;9,146.91,404.61,97.80,7.86" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m" coord="9,257.70,393.65,153.47,7.86">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,138.35,413.30,342.25,10.13;9,146.91,426.52,333.68,7.86;9,146.91,437.48,333.68,7.86;9,146.91,448.44,164.39,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,253.14,415.56,227.46,7.86;9,146.91,426.52,31.17,7.86">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Řehůřek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<ptr target="http://is.muni.cz/publication/884893/en" />
	</analytic>
	<monogr>
		<title level="m" coord="9,210.58,426.52,270.01,7.86;9,146.91,437.48,117.79,7.86">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
