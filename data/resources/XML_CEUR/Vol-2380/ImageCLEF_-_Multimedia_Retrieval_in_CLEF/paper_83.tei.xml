<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.18,115.96,332.99,12.62;1,158.00,133.89,299.34,12.62">Automated Lifelog Moment Retrieval based on Image Segmentation and Similarity Scores</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.21,171.61,65.62,8.74"><forename type="first">Stefan</forename><surname>Taubert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair Media Informatics</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.38,171.61,51.76,8.74"><forename type="first">Stefan</forename><surname>Kahl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair Media Informatics</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.69,171.61,69.63,8.74"><forename type="first">Danny</forename><surname>Kowerko</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Junior Professorship Media Computing</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,389.25,171.61,70.43,8.74"><forename type="first">Maximilian</forename><surname>Eibl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair Media Informatics</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>D-09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.18,115.96,332.99,12.62;1,158.00,133.89,299.34,12.62">Automated Lifelog Moment Retrieval based on Image Segmentation and Similarity Scores</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6438052DD810AF26A5A8D171CE938DCF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lifelog Moment Retrieval</term>
					<term>Visual Concept</term>
					<term>Image Segmentation</term>
					<term>XGBoost</term>
					<term>Wordembeddings</term>
					<term>YOLO</term>
					<term>Detectron</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In our working notes we discuss our proposed strategies for the ImageCLEFlifelog LMRT task. We used word vectors to calculate similarity scores between queries and images. To extract important moments and reduce the image amount we used image segmentation based on histograms. We enriched the given data with concepts from pretrained models and got twelve concept types for which similarity scores were calculated and accounted. Furthermore, we used tree boosting as a predictive approach. Our highest F1@10 on the training queries was 27.41% and for the test queries we obtained a maximal F1@10 of 11.70%. All of our models were applicable to generic queries in a fully automated manner.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A lifelog is a collection of structured data from the life of a person <ref type="bibr" coords="1,440.93,507.24,14.61,8.74" target="#b11">[12]</ref>. The data is collected through sensors, which take images or measure for example heart rate, electrodermal activity, blood sugar levels or geographic coordinates. These sensors can be found in mobile phones, wearable cameras or smart watches. Lifelogs can greatly differ from person to person because of different behaviours.</p><p>In most cases, a lifelog contains big amounts of data and can be used for multiple use cases. One example is lifelogging as memory extension for patients who suffer mild cognitive impairment and episodic memory impairment <ref type="bibr" coords="1,462.32,590.98,14.61,8.74" target="#b21">[25]</ref>. Furthermore, they are used to analyse people's behaviour, review important moments of the day or to find lost items <ref type="bibr" coords="1,314.28,614.89,14.61,8.74" target="#b27">[31]</ref>.</p><p>As processing of the lifelogs by hand takes enormous effort, computer systems are deployed. The goal of the LMRT task is to create a system which processes shot images and meta data from the life of a person with predefined queries and return appropriate images <ref type="bibr" coords="2,275.12,154.86,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="2,287.30,154.86,11.62,8.74">14]</ref>. For this task lifelogs of two people are given which contain images recorded from a wearable camera and meta data like geographical coordinates and heartrate of each minute of the day.</p><p>In this paper we will briefly discuss existing solutions and will afterwards present our proposed strategies. We also review our experiments which we will summarize at the end. We decided to only focus on queries given for user 1 because the training queries only contain queries to user 1 and in the test queries only one query is on behalf of user 2.</p><p>In the last years, we worked with deep convolutional networks [23, <ref type="bibr" coords="2,453.47,251.05,8.49,8.74" target="#b15">16</ref>, 24] and we participated in TRECVID challenges <ref type="bibr" coords="2,332.55,263.00,15.50,8.74" target="#b26">[30,</ref><ref type="bibr" coords="2,349.71,263.00,12.73,8.74" target="#b17">18,</ref><ref type="bibr" coords="2,364.10,263.00,12.73,8.74" target="#b16">17,</ref><ref type="bibr" coords="2,378.49,263.00,12.73,8.74" target="#b29">33]</ref> and LifeCLEF challenges <ref type="bibr" coords="2,164.15,274.96,15.50,8.74" target="#b14">[15]</ref> (BirdCLEF <ref type="bibr" coords="2,236.14,274.96,15.50,8.74" target="#b18">[19,</ref><ref type="bibr" coords="2,253.30,274.96,12.73,8.74">21,</ref><ref type="bibr" coords="2,267.68,274.96,12.73,8.74" target="#b10">11,</ref><ref type="bibr" coords="2,282.08,274.96,11.62,8.74">20]</ref>, GeoLifeCLEF <ref type="bibr" coords="2,364.62,274.96,15.50,8.74" target="#b28">[32]</ref> and PlantCLEF <ref type="bibr" coords="2,456.49,274.96,14.76,8.74" target="#b12">[13]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">State of the Art</head><p>Lately the topic of lifelog moment retrieval gained increasing attention. In the last year's challenge six teams in total participated whereas in the year 2017 it was only one team <ref type="bibr" coords="2,218.90,357.39,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="2,231.08,357.39,7.01,8.74" target="#b6">7]</ref>. The difficulty in this sort of task lies in the extraction of information from the big amount of data and comparing these with main concepts from the queries, whereby the detection of concepts in the images works really reliable <ref type="bibr" coords="2,197.09,393.26,15.50,8.74" target="#b24">[28,</ref><ref type="bibr" coords="2,214.24,393.26,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="2,228.64,393.26,12.73,8.74" target="#b31">35,</ref><ref type="bibr" coords="2,243.03,393.26,11.62,8.74" target="#b25">29]</ref>.</p><p>In such tasks the metrics cluster recall, precision and F1 play a great role because as many different situations as relevant as possible needed to be found. Therefore, methods were found which work automated and some of them used human interaction with relevance feedback. The latter performs generally better than the automated one <ref type="bibr" coords="2,243.24,453.31,15.50,8.74" target="#b32">[36,</ref><ref type="bibr" coords="2,260.41,453.31,11.62,8.74" target="#b19">22]</ref>.</p><p>The usage of image segmentation before the further query processing can result in better scores <ref type="bibr" coords="2,230.64,477.49,14.61,8.74" target="#b32">[36]</ref>. Methods for image segmentation are based on frames characterisation, event segmentation and key frame selection <ref type="bibr" coords="2,408.18,489.45,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,420.36,489.45,12.73,8.74" target="#b22">26,</ref><ref type="bibr" coords="2,434.74,489.45,7.01,8.74" target="#b8">9]</ref>. Frames were characterised through concepts predicted with CNNs <ref type="bibr" coords="2,390.69,501.40,9.96,8.74" target="#b7">[8]</ref>. Then agglomerative clustering were used but in regard with time coherence <ref type="bibr" coords="2,397.09,513.36,14.61,8.74" target="#b22">[26]</ref>.</p><p>Furthermore, NLP approaches with word embeddings and LSTMs had been successfully used to compare query and image concepts <ref type="bibr" coords="2,379.99,537.54,15.50,8.74" target="#b30">[34,</ref><ref type="bibr" coords="2,397.15,537.54,7.01,8.74" target="#b0">1]</ref>. Vectors and relevance scores were used for the final comparison <ref type="bibr" coords="2,348.46,549.50,15.50,8.74" target="#b30">[34,</ref><ref type="bibr" coords="2,365.62,549.50,7.01,8.74" target="#b7">8]</ref>.</p><p>Our approach combines segmentation, NLP methods and similarity scores without human in the loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Strategies</head><p>Our main strategy was to account multiple similarity scores which were calculated between images and queries. Therefore, we extracted concepts which describe different areas out of a lifelog and compared these with each token from a query. Furthermore, we include an image segmentation for grouping similar moments and reducing the image amount by removing segments containing only one image. We also present a model based on supervised machine leaning which we trained with similarity scores to process the test queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Segmentation</head><p>We read all images which were shot with the wearable camera of a person. To recognise differences between two images we extracted the colour histograms of the images by the day. Then we perform agglomerative clustering by comparing the euclidean distances of the histograms. To remove the hierarchical structure of the clusters, we flattened them afterwards by merging clusters which had a distance lower than a certain threshold.</p><p>After those steps we had clusters which did not necessarily contained images from only one moment, for instance the moments user 1 is driving to work and back were in the same cluster. Because of that we separated the clusters into single segments out of coherent images. To reduce the total image amount, we removed segments which contained only one image because these images were mostly blurry or represented changes of the situation.</p><p>In the last step we combined segments cluster-wise which were maximum 15 images apart because they were most likely similar and represented the same moment. Finally, we took the segments out of the clusters because we did not needed the clusters anymore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We performed experiments where we tried different thresholds and found a value with which we were able to extract 4302 segments containing in total 21643 out of the 63696 images of user 1. This corresponded to a reduction of 66%. Afterwards we took one image of each segment as representative and calculated the cluster recall for those 4302 images. As result we attained 88.50% cluster recall by anew reduction of 80%. The selection of the representative image did not decrease the cluster recall in comparison to taking all images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Extracted Labels</head><p>We extracted multiple label types to describe moments from different views. Therefore, we used many of the given data but performed own detections with pretrained models as well. We used following types:</p><p>-Categories: We used the given categories which contained Places365 labels predicted through Place CNN. Some of the categories include information about indoor and outdoor, for example museum indoor so we decided to extract two extra label types from the categories: Base Categories and Indoor/Outdoor Labels. Those labels described the location of a moment. -Attributes: We used the given attributes which contained labels from the SUNattribute dataset predicted on Place CNN. With those labels we described the environment of a moment.</p><p>-COCO Concepts 1: We used the given concepts which contained labels from the COCO dataset predicted on Faster R CNN. We used them to identify objects in the images. -COCO Concepts 2: Besides the given concepts we used a pretrained YOLOv3 model to detect COCO labels. -COCO Concepts 3: Because COCO labels describe common situations very well we extracted COCO labels a third time but with a pretrained Detetron model. -Open Images Concepts: We used a pretrained model on YOLOv3 again for the detection of labels from the Open Images dataset. -Image Net Concepts: We used a pretrained model on YOLOv3 again for the prediction of labels from the ImageNet dataset. These concepts were classification labels, they did not identify multiple objects in images. -Daytimes: We mapped the recording times of the images into four daytimes: morning, afternoon, evening and night. -Locations: We generalised the names of the locations by defining common locations like store, bar or bakery. -Activities: We used the activities transport and walking.</p><p>-Cities: We extracted from the given time zones the city part.</p><p>Besides the raw labels we also calculated the IDF of each label to weight them later in the comparison. For each label type with scores a threshold could be defined at which the label is seen as accurate for a certain image. In those cases all other labels were ignored and the IDFs were adjusted. For labels which were not in the word vectors we tried to find a valid representation and if we could not find any, we ignored the label.</p><p>In the following table all label types are shown with their amount of valid labels and if they were generated from the given data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image Processing</head><p>For every image/segment a vector is created for each label type whose size is equal to the amount of labels. There were two possible ways to create these vectors, one for the images and one for the segments.</p><p>Image based Vectors For the creation of an image vector we looked a every label of a label type if it occurred in the image. If this was the case, the score of the label was written into the vector. For label types without scores an one was written into the vector if the label occurred in the image. Labels which did not occur were represented by a zero.</p><p>Segment based Vectors For the segment based vector creation we either selected the first image of a segment and produced the vector the way described above or we combined the image vectors of all images of a segment by taking the maximum of each label value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Query Processing</head><p>For the query processing we first had to define the query source which could be either the query description or query title. Then we removed punctuation from the query, lowercased and tokenized it. Afterwards we removed stop words as well as query typical tokens like find, moment or u1 and duplicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results</head><p>In the following tables the results of our tokenization process are shown. In Table <ref type="table" coords="5,162.20,456.78,4.98,8.74" target="#tab_1">2</ref> it is visible that the description token contain token which do not have a clear representation in any of the label types like view, taking, using or beside. The tokenization for the test queries shown in Table <ref type="table" coords="6,383.41,118.99,4.98,8.74" target="#tab_2">3</ref> also returned tokens which had no clear representations like using, two, items, plaid or red. We recognized differences in the query types comparing training and test queries, for example test query 10 contains a location but in the training queries no locations occurred. Furthermore, test query 7 asked for two persons but a special number of objects were not asked in any of the training queries.</p><p>Token Vectors For each query token a vector with the same size as the image vector is created. The entries of the vector were the cosine similarities between the token and each of the labels retrieved from our word vectors. These similarities were inside the interval -1 and 1, whereby 1 represented the highest similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Vector Comparison</head><p>After we retrieved both image/segment vectors (I) and token vectors we calculated a similarity score between them. Therefore, we first transformed the co-domain of the token vectors into the interval [0, 1] because we wanted to obtain scores in that range. We called the resulting vector T . Then we calculated the norm of the difference vector of I and T and divided it by the square root of the amount of predicted labels n. We obtained a value between 0 and 1 which we subtract from 1 to get the similarity score:</p><formula xml:id="formula_0" coords="6,238.98,600.79,129.63,22.38">Similarity(I, T ) = 1 - I -T √ n</formula><p>We implemented an option called ceiling which replaces the entries (scores) of the image/segment vector with 1. We also include a factor p which potentiate the difference of the vectors and an option use idf which defines if an IDF vector should be included in the calculation. It contained the IDF of the label if the difference of the vectors was greater than 0.5 and otherwise the reciprocal of the IDF. This way we could boost similarities and punish dissimilarities of rare labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Accounting of the similarity scores</head><p>After we calculated similarity scores between all images/segments and tokens for each label type we needed to account them in the next step. Therefore, we created a similarity matrix M k for each image/segment k whose rows represented each label type j and whose columns represented each token of a query i.</p><p>We also included weights v ij for each token and weights w j for each label type. Weights v ij were the maximum cosine similarity that a token i could have with any label of label type j. If the maximum was negative the weight was set to zero. Weights w j were set manually.</p><formula xml:id="formula_1" coords="7,227.32,306.40,160.72,9.68">(M k ) ij = v ij • w j • Similarity(I k , T ij )</formula><p>Based on this matrix we calculated two different similarity scores. The first method was to calculate the mean of the matrix and the second method was to take the maximum similarities of each label type and calculate the mean. We called the first method mean and the second method labelmax. We also tried to take the maximum of the token and calculate the mean but the result on the training queries was always worse than that of both other methods.</p><p>Token Clustering As extension to the described accounting method we implemented a clustering of the query tokens to group similar tokens. We merged tokens which had a cosine distance lower than 0.5 because they were very similar.</p><p>Then we calculated the similarity score as described above but for each cluster. The resulting cluster scores were taken and the mean was computed.</p><p>The idea behind this clustering was that if a query contains different things all of them should be considered equally. For example the token icecream and sea were in different clusters whereby food and restaurant were in the same cluster. If the token beach would be added to the first query it would be added to the second cluster. Without clustering the images with the labels icecream and sea would be taken less into account than images with all tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Postprocessing</head><p>After we calculated the similarity scores, they were sorted descending for each topic. From the segments we only extracted the first image because it was a valid representative for all other images as shown in 3.1.</p><p>To improve the cluster recall we added the option sort submission which defines if only one image per day should be selected for the top submission entries per topic because many of the test queries contained situations which were likely occur only once per day like driving home from office or looking for items in a toyshop.</p><p>We only used resources which were open source. Our word vectors were pretrained GloVe vectors from Common Crawl which had 300 dimensions and a vocabulary of 2.2 million tokens <ref type="bibr" coords="8,277.37,167.48,14.61,8.74" target="#b23">[27]</ref>.</p><p>Furthermore, we used real-time object detection system YOLOv3 in combination with pretrained models to detect labels from the Open Images dataset, ImageNet 1000 dataset and COCO dataset <ref type="bibr" coords="8,325.45,203.48,14.61,8.74" target="#b24">[28]</ref>. We also used Detectron with a pretrained Mask R CNN to predict labels from the COCO dataset <ref type="bibr" coords="8,428.52,215.44,14.61,8.74" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Source Code</head><p>We published our source code on GitHub<ref type="foot" coords="8,317.75,265.01,3.97,6.12" target="#foot_0">3</ref> . It was written in Python and uses different third party libraries. The project page provides instructions on how to execute the code. We used an Intel Core in combination with a NVIDIA GeForce GTX 1070 Mobile to run our code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section we will present our experiments which all based on one base model which combined multiple label types and calculated similarity scores regarding to them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Optimization of label types</head><p>As we had many parameters we first optimized each label type for itself by running a grid search for different parameter settings. The best comparing settings of a label type were these which achieved the highest F1@10 on the training queries. We tried the following parameters and ran our model without segmentation, with mean comparing, unsorted, without label optimization and queries based on the description. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We obtained the results shown in Table <ref type="table" coords="9,345.91,118.99,3.87,8.74" target="#tab_4">5</ref>. We found out that the Places Labels achieved a maximal F1 of 23.21%. The label types from the minute based table on the other hand achieved really low scores. The best score of the 3 COCO Concepts was achieved by Detectron at a threshold of 0.95. The labels from ImageNet performed best using a threshold of 0.99. The IDF values were only useful for the Open Images labels which was really unexpected. The ceiling for the Places related labels was useful which was expected because the scores of the places labels were really low in comparison to the other label types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Category Model</head><p>Because the category labels achieved such high F1 we decided to process the test queries with that model. Table <ref type="table" coords="9,163.89,552.90,4.13,7.89">6</ref>. The results of run 1 showed that the categories worked much better on the training queries than on the test queries.</p><p>Run Segmentation Sorted F1@10 train. set in % F1@10 test set in % 1 no no 23.21 3.30</p><p>The resulting score was 3.3% which was much lower than expected. The reasons could be that the difference between the training and test queries were to big.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Visual Concept Model</head><p>Our second model was based on all visual concept labels: Attributes, Places and COCO Concepts 1. We ran four different settings. Run 5 was accidentally submitted with the same submission of run 4. The results showed that the score increased after sorting the submissions for the training queries but the test queries only benefited from it in the runs with segmentation. For the segmentation variants we considered all images of a segment. It was noticeable that the run with segmentation achieved almost three times better scores than the normal runs. One reason could be the decreased amount of possible images which increased the probability of taking relevant images from different clusters. The difference between the training and test scores was this time lower than in run 1 but still high especially for run 2 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Predictive Model</head><p>We decided to create a model which uses tree boosting as predictive approach. We used the similarity scores from run 2 and 4 for training a classifier. Therefore, we took 20% of the irrelevant images and 20% of the images of each cluster for the evaluation set and the rest for the training set. Then we trained our model with XGBoost and binary logistic regression and depth three for the best log loss <ref type="bibr" coords="10,153.69,511.34,9.96,8.74" target="#b3">[4]</ref>. Afterwards we predicted the images for each test topic. Results of run 7 and 8 showed that the predictive approach was likely not suitable for such a task because the similarities simply were memorized and could not be used to predict unknown queries. In run 7 no relevant images were found and in run 8 the found images could be just random images because in total there were just 4302 images to select from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Visual Concept Metadata Model</head><p>The Visual Concept Metadata Model was an attempt to improve the Visual Concept model by adding the given metadata. We did not sort the submissions in these runs and switched over to the labelmax comparing method and optimized the labels for all following runs. We also included the weights for each token. For the segmentation variant we considered only the first image of a segment. The scores were really low for both test and training queries. The main reason could be that the given metadata pulled the scores down because they were really unsuitable as found out in the label optimization experiments. We obtained mostly better scores on the training queries with the labelmax comparing method which was why we used this method. It could be guessed that an adjusting of weights for the metadata could be resulted in higher scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Extended Visual Concept Model</head><p>As the extension with metadata did not increased the score, we tried to extend the Visual Concept Model with the self predicted concepts COCO Concepts 2 &amp; 3, Open Images and Image Net Concepts. This time, we decided to take the titles as queries to update the previous model. Using this model we achieved our highest score with 11.70% and we also had the highest score for the training queries with 26.16%. The segmentation decreased the score for both training and test queries. We had an increase of 7.8% F1 by taking the extra label types into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Extended Visual Concept Metadata Model</head><p>We combined the two last mentioned models into one big model which contained twelve label types. We also used token clustering in our models and added the common location costa coffee because two topics were asking for coffee. We obtained the results shown in Table <ref type="table" coords="12,293.81,179.28,8.49,8.74" target="#tab_9">11</ref>. From the results we could read out that the score could be increased by weighting the label types. The segmentation caused a decrease in the score again like in the previous model. A new highest score could not be accomplished. The following images in Fig. <ref type="figure" coords="12,241.50,350.96,4.98,8.74" target="#fig_0">1</ref> show the returned images for this run. For topic 1 correct images from a toyshop were found. All top ten images for topic 2 showed images of the same day when u1 was driving to work but not back home. Three relevant images were returned for topic 3 but none were found for topic 4 and 5. The reason for the latter may be that the uninformative token time was still in the query after the tokenization process.</p><p>Almost all selected images for topic 6 were relevant but were taken on the same day which led to a small cluster recall. For topic 7 we got two different relevant moments and for topic 8 half of the images were relevant. For the 9th topic we only got an image of u1 wearing not a plaid but a striped shirt and the images returned for the last topic contained the airport in Shanghai with many people but not a meeting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We tried different strategies for the lifelog task. The best results could be achieved without using segmentation. One reason for that could be, that too much of the relevant images were removed or that the histogram based approach was not precise enough. Sorting of the submissions was only useful in the models with few label types. In our evaluation, we found out that the weighting of the tokens improved the scores immense, so did the use of thresholds for the label types. We could also achieve an improvement in the score by considering only the highest similarity score of all tokens per label type. Token clustering led to an increase of the score, too.</p><p>Our predictive model showed that this approach may not be suitable for such tasks. The big differences between the training and test queries led to great differences in the F1 scores. We found out that the usage of the title did improve the scores because less uninformative words were included.</p><p>We could have done more experiments which would have shown that some parameters had more influences for the resulting scores. Weighting of the label types might improve our best run but because of this great amount of parameters it was difficult to find the best experiments.</p><p>To improve our model, we could include dissimilarity scores and find labels which may not occur in the images even if the similarity scores were high. We also could include more of the metadata like the heart rate and more external data, for example poses from open pose <ref type="bibr" coords="13,318.25,499.79,10.52,8.74" target="#b2">[3]</ref> to detect hands in the images to prevent the detection of hands as persons.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="12,134.77,574.71,345.83,7.89;12,134.77,585.69,345.83,7.86;12,134.77,596.65,293.10,7.86;12,136.16,483.89,86.46,64.84"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The returned images from run 15 showed that our model had found some relevant images at top positions. Topics 1, 3, 6, 7 and 8 contained at least one relevant image whereby for topics 2, 4, 5, 9, 10 no relevant images were returned.</figDesc><graphic coords="12,136.16,483.89,86.46,64.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.76,467.30,345.83,171.53"><head>Table 1 .</head><label>1</label><figDesc>Overview of used label types including if they were generated from given data and their total amount of labels. Overall we got a maximum of 1191 labels.</figDesc><table coords="4,213.91,499.06,187.53,139.77"><row><cell>Label type</cell><cell cols="2">Given data Count of labels</cell></row><row><cell>Categories</cell><cell>yes</cell><cell>360</cell></row><row><cell>-Base Categories</cell><cell>yes</cell><cell>347</cell></row><row><cell>-Indoor/Outdoor</cell><cell>yes</cell><cell>2</cell></row><row><cell>COCO Concepts 1</cell><cell>yes</cell><cell>76</cell></row><row><cell>COCO Concepts 2</cell><cell>no</cell><cell>72</cell></row><row><cell>COCO Concepts 3</cell><cell>no</cell><cell>79</cell></row><row><cell>Open Images</cell><cell>no</cell><cell>53</cell></row><row><cell>Image Net</cell><cell>no</cell><cell>423</cell></row><row><cell>Daytimes</cell><cell>yes</cell><cell>4</cell></row><row><cell>Locations</cell><cell>yes</cell><cell>16</cell></row><row><cell>Activities</cell><cell>yes</cell><cell>2</cell></row><row><cell>Cities</cell><cell>yes</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,500.41,345.83,160.57"><head>Table 2 .</head><label>2</label><figDesc>The results of the tokenized training queries showed that the description token contains multiple token which do not have a clear representation in any of the label types like view or taking.</figDesc><table coords="5,166.79,543.12,281.78,117.85"><row><cell>Topic Tokens from description</cell><cell>Tokens from title</cell></row><row><cell>1 beside, eating, icecream, sea</cell><cell>icecream, sea</cell></row><row><cell>2 drinking, eating, food, restaurant</cell><cell>food, restaurant</cell></row><row><cell cols="2">3 devices, digital, using, video, watching videos, watching</cell></row><row><cell>4 bridge, photo, taking</cell><cell>bridge, photograph</cell></row><row><cell>5 food, grocery, shop, shopping</cell><cell>grocery, shopping</cell></row><row><cell>6 guitar, man, playing, view</cell><cell>guitar, playing</cell></row><row><cell>7 cooking, food</cell><cell>cooking</cell></row><row><cell>8 car, sales, showroom</cell><cell>car, sales, showroom</cell></row><row><cell cols="2">9 countries, public, taking, transportation public, transportation</cell></row><row><cell>10 book, paper, reading</cell><cell>book, paper, reviewing</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,162.91,345.82,149.61"><head>Table 3 .</head><label>3</label><figDesc>The results of the tokenized test queries showed that even the title tokens contained tokens which do not had clear representations like using or two.</figDesc><table coords="6,146.98,194.67,321.39,117.85"><row><cell>Topic Tokens from description</cell><cell>Tokens from title</cell></row><row><cell>1 items, looking, toyshop</cell><cell>toyshop</cell></row><row><cell>2 driving, home, office</cell><cell>driving, home</cell></row><row><cell>3 home, inside, looking, refrigerator</cell><cell>food, fridge, seeking</cell></row><row><cell>4 either, football, tv, watching</cell><cell>football, watching</cell></row><row><cell>5 cafe, coffee</cell><cell>coffee, time</cell></row><row><cell>6 breakfast, home</cell><cell>breakfast, home</cell></row><row><cell>7 coffee, person, two</cell><cell>coffee, person, two</cell></row><row><cell cols="2">8 outside, smartphone, standing, using, walking outside, smartphone, using</cell></row><row><cell>9 plaid, red, shirt, wearing</cell><cell>plaid, red, shirt, wearing</cell></row><row><cell>10 attending, china, meeting</cell><cell>china, meeting</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.77,526.18,345.82,94.81"><head>Table 4 .</head><label>4</label><figDesc>To optimize each label type, we ran a grid search with following parameter variations on a model based only on one label type.</figDesc><table coords="8,207.50,557.94,200.36,63.06"><row><cell cols="2">Parameter Description</cell><cell>Variations</cell></row><row><cell>t</cell><cell>thresholds Places</cell><cell>0, 0.1, 0.05, 0.01</cell></row><row><cell></cell><cell>other label types</cell><cell>0, 0.9, 0.95, 0.99</cell></row><row><cell>idf</cell><cell>use IDF</cell><cell>no, yes</cell></row><row><cell>p</cell><cell>exponentiation factor</cell><cell>1, 2, 3</cell></row><row><cell>c</cell><cell>use ceiling</cell><cell>no, yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,134.77,174.89,345.83,193.44"><head>Table 5 .</head><label>5</label><figDesc>This table shows our obtained results for the label optimization experiments. We found out that the Places Labels achieved a maximal F1 of 23.21%. The label types from the minute based table on the other hand achieved really low scores.</figDesc><table coords="9,171.10,217.61,273.15,150.73"><row><cell>Label type</cell><cell cols="3">t idf p c max. F1@10 in % Count of labels</cell></row><row><cell>Places</cell><cell>0 no 1 yes</cell><cell>23.21</cell><cell>360</cell></row><row><cell>Raw Places</cell><cell>0 no 1 yes</cell><cell>21.16</cell><cell>347</cell></row><row><cell>Indoor/Outdoor</cell><cell>0 no 3 no</cell><cell>1.67</cell><cell>2</cell></row><row><cell>Open Images</cell><cell>0 yes 2 yes</cell><cell>6.50</cell><cell>53</cell></row><row><cell cols="2">COCO Concepts 1 0.9 no 1 no</cell><cell>10.89</cell><cell>72</cell></row><row><cell cols="2">COCO Concepts 2 0.9 no 1 no</cell><cell>11.57</cell><cell>64</cell></row><row><cell cols="2">COCO Concepts 3 0.95 no 1 no</cell><cell>14.69</cell><cell>68</cell></row><row><cell>Image Net</cell><cell>0.99 no 1 no</cell><cell>12.04</cell><cell>75</cell></row><row><cell>Attributes</cell><cell>-no 1 no</cell><cell>7.98</cell><cell>97</cell></row><row><cell>Daytimes</cell><cell>-no 1 no</cell><cell>3.00</cell><cell>4</cell></row><row><cell>Locations</cell><cell>-no 1 no</cell><cell>0</cell><cell>16</cell></row><row><cell>Activities</cell><cell>-no 1 no</cell><cell>0</cell><cell>2</cell></row><row><cell>Timezones</cell><cell>-no 1 no</cell><cell>0</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,134.77,231.23,345.83,93.07"><head>Table 7 .</head><label>7</label><figDesc>Results of run 2-6 showed that segmentation increased the test scores enormously. A reason could be that the limited amount of possible images increased the cluster recall.</figDesc><table coords="10,168.02,272.21,279.32,52.10"><row><cell cols="5">Run Segmentation Sorted F1@10 train. set in % F1@10 test set in %</cell></row><row><cell>2</cell><cell>no</cell><cell>no</cell><cell>12.81</cell><cell>3.90</cell></row><row><cell>3</cell><cell>no</cell><cell>yes</cell><cell>17.20</cell><cell>3.00</cell></row><row><cell>4, 5</cell><cell>yes</cell><cell>no</cell><cell>12.25</cell><cell>8.60</cell></row><row><cell>6</cell><cell>yes</cell><cell>yes</cell><cell>12.67</cell><cell>9.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,134.77,543.68,345.83,72.90"><head>Table 8 .</head><label>8</label><figDesc>Runs 7 and 8 showed that the predictive approach was not suitable for such a task because the similarities were simply memorized and could not be used to predict unknown queries.</figDesc><table coords="10,168.02,586.40,279.32,30.18"><row><cell cols="5">Run Segmentation Sorted F1@10 train. set in % F1@10 test set in %</cell></row><row><cell>7</cell><cell>no</cell><cell>no</cell><cell>19.43</cell><cell>0.00</cell></row><row><cell>8</cell><cell>yes</cell><cell>no</cell><cell>11.78</cell><cell>4.60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,134.77,268.38,345.82,61.94"><head>Table 9 .</head><label>9</label><figDesc>The results of run 9 and 10 were really bad for both test and training queries. The main reason could be that the given metadata worsened the scores.</figDesc><table coords="11,168.02,300.13,279.32,30.18"><row><cell cols="5">Run Segmentation Sorted F1@10 train. set in % F1@10 test set in %</cell></row><row><cell>9</cell><cell>no</cell><cell>no</cell><cell>12.27</cell><cell>1.70</cell></row><row><cell>10</cell><cell>yes</cell><cell>no</cell><cell>8.15</cell><cell>1.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,134.77,531.21,345.82,61.94"><head>Table 10 .</head><label>10</label><figDesc>With run 11 we achieved our highest score with 11.70%. Run 12 showed that the segmentation decreased both scores.</figDesc><table coords="11,168.02,562.97,279.32,30.18"><row><cell cols="5">Run Segmentation Sorted F1@10 train. set in % F1@10 test set in %</cell></row><row><cell>11</cell><cell>no</cell><cell>no</cell><cell>26.16</cell><cell>11.70</cell></row><row><cell>12</cell><cell>yes</cell><cell>yes</cell><cell>14.25</cell><cell>4.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="12,134.77,210.63,345.82,72.90"><head>Table 11 .</head><label>11</label><figDesc>Runs 13, 14 and 15  showed that the extension with the metadata did not improve the score but worsened it.</figDesc><table coords="12,138.82,242.39,337.73,41.14"><row><cell cols="6">Run Segmentation Sorted Label weights F1@10 train. set in % F1@10 test set in %</cell></row><row><cell>13</cell><cell>no</cell><cell>no</cell><cell>no</cell><cell>21.62</cell><cell>6.20</cell></row><row><cell>14</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>13.01</cell><cell>1.10</cell></row><row><cell>15</cell><cell>no</cell><cell>no</cell><cell>yes</cell><cell>27.41</cell><cell>8.70</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="8,144.73,656.80,225.66,7.86"><p>https://github.com/stefantaubert/imageclef-lifelog-2019</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.96,568.81,337.63,7.86;13,151.52,579.77,188.27,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="13,417.34,568.81,63.25,7.86;13,151.52,579.77,166.76,7.86">Regim lab team at imageclef lifelog moment retrieval task</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">B</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Feki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ezzarka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,590.89,337.63,7.86;13,151.52,601.84,329.07,7.86;13,151.52,612.80,329.07,7.86;13,151.52,623.76,25.60,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,432.07,590.89,48.52,7.86;13,151.52,601.84,241.11,7.86">Visual summary of egocentric photostreams by representative keyframes</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bolanos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Talavera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Giró-I Nieto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,412.69,601.84,67.91,7.86;13,151.52,612.80,263.70,7.86">2015 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,634.88,337.63,7.86;13,151.52,645.84,329.07,7.86;13,151.52,656.80,97.80,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="13,399.60,634.88,80.99,7.86;13,151.52,645.84,245.76,7.86">OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,142.96,119.67,337.63,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,137.46,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,247.50,119.67,164.04,7.86">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,432.52,119.67,48.07,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,25.89,7.86">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,152.56,337.63,7.86;14,151.52,163.52,329.07,7.86;14,151.52,174.48,329.07,7.86;14,151.52,185.44,254.98,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,167.90,163.52,308.16,7.86">Overview of ImageCLEFlifelog 2017: Lifelog Retrieval and Summarization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="14,168.00,174.48,296.33,7.86">CLEF2017 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,196.41,337.63,7.86;14,151.52,207.37,329.07,7.86;14,151.52,218.33,329.07,7.86;14,151.52,229.29,329.07,7.86;14,151.52,240.24,289.13,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,249.22,207.37,231.37,7.86;14,151.52,218.33,121.61,7.86">Overview of ImageCLEFlifelog 2019: Solve my life puzzle and Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="14,295.72,218.33,184.87,7.86;14,151.52,229.29,46.41,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,251.21,337.64,7.86;14,151.52,262.17,329.07,7.86;14,151.52,273.13,329.07,7.86;14,151.52,284.09,292.63,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,442.93,251.21,37.66,7.86;14,151.52,262.17,329.07,7.86;14,151.52,273.13,24.44,7.86">Overview of ImageCLEFlifelog 2018: Daily Living Understanding and Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="14,200.54,273.13,108.44,7.86">CLEF2018 Working Notes</title>
		<title level="s" coord="14,318.26,273.13,162.34,7.86;14,151.52,284.09,14.99,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,295.06,337.63,7.86;14,151.52,306.02,16.69,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="14,254.35,295.06,226.24,7.86;14,151.52,306.02,16.69,7.86">Multimedia lab@ imageclef 2018 lifelog moment retrieval task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,316.99,337.63,7.86;14,151.52,327.95,329.07,7.86;14,151.52,338.91,329.07,7.86;14,151.52,349.87,194.78,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,365.32,316.99,115.27,7.86;14,151.52,327.95,43.11,7.86">Multimodal segmentation of lifelog data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,215.55,327.95,242.97,7.86">Large Scale Semantic Access to Content (Text, Image, Video</title>
		<imprint>
			<publisher>LE CENTRE DE HAUTES ETUDES INTERNATIONALES D&apos;INFORMATIQUE DOCUMENTAIRE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="21" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,360.84,337.97,7.86;14,151.52,371.80,221.09,7.86" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<title level="m" coord="14,438.20,360.84,38.15,7.86">Detectron</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,382.77,337.97,7.86;14,151.52,393.73,329.07,7.86;14,151.52,404.69,25.60,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="14,432.73,382.77,47.86,7.86;14,151.52,393.73,239.02,7.86">Overview of birdclef 2018: monophone vs. soundscape bird identification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">CLEF working notes</note>
</biblStruct>

<biblStruct coords="14,142.62,415.66,337.98,7.86;14,151.52,426.59,281.32,7.89" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,356.14,415.66,120.36,7.86">Lifelogging: Personal big data</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Doherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,151.52,426.61,203.83,7.86">Foundations and Trends R in information retrieval</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="125" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,437.59,337.98,7.86;14,151.52,448.54,141.92,7.86;14,134.77,459.51,11.78,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="14,332.49,437.59,148.11,7.86;14,151.52,448.54,141.92,7.86">Large-scale plant classification using deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Haupt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,151.52,459.51,329.07,7.86;14,151.52,470.47,329.07,7.86;14,151.52,481.43,329.07,7.86;14,151.52,492.39,329.07,7.86;14,151.52,503.35,329.07,7.86;14,151.52,514.31,329.07,7.86;14,151.52,525.27,329.07,7.86;14,151.52,536.23,329.07,7.86;14,151.52,547.19,329.07,7.86;14,151.52,558.14,216.27,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,268.29,514.31,212.30,7.86;14,151.52,525.27,124.23,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,297.67,525.27,182.92,7.86;14,151.52,536.23,329.07,7.86;14,151.52,547.19,122.46,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="14,305.55,547.19,171.07,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="14,142.62,569.11,337.97,7.86;14,151.52,580.07,329.07,7.86;14,151.52,591.03,329.07,7.86;14,151.52,601.99,329.07,7.86;14,151.52,612.95,320.12,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,151.52,591.03,264.37,7.86">Lifeclef 2019: Biodiversity identification and prediction challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Poupard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,406.79,601.99,73.80,7.86;14,151.52,612.95,66.16,7.86">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Mayr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Hauff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,623.92,337.97,7.86;14,151.52,634.88,329.07,7.86;14,151.52,645.84,329.07,7.86;14,151.52,656.80,98.44,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,193.25,634.88,267.55,7.86">Acoustic event classification using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schlohauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Thangaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,268.48,645.84,82.15,7.86">INFORMATIK 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Eibl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Gaedke</surname></persName>
		</editor>
		<meeting><address><addrLine>Bonn</addrLine></address></meeting>
		<imprint>
			<publisher>Gesellschaft fr Informatik</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2177" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,119.67,337.98,7.86;15,151.52,130.63,329.07,7.86;15,151.52,141.59,46.41,7.86" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="15,168.00,130.63,312.59,7.86;15,151.52,141.59,24.90,7.86">Technische universitat chemnitz and hochschule mittweida at trecvid instance search</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Roschke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heinzig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,151.87,337.98,7.86;15,151.52,162.83,329.07,7.86;15,151.52,173.79,165.52,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="15,353.35,162.83,127.23,7.86;15,151.52,173.79,103.06,7.86">Technische universitat chemnitz at trecvid instance search</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Roschke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rickert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zywietz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manthey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heinzig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,184.07,337.97,7.86;15,151.52,195.03,308.99,7.86;15,134.77,205.32,345.82,7.86;15,151.52,216.27,218.92,7.86;15,134.77,226.56,345.83,7.86;15,151.52,237.52,181.25,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="15,168.16,195.03,292.35,7.86;15,134.77,205.32,7.85,7.86">Large-scale bird sound classification using convolutional neural networks 20</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,422.14,205.32,58.45,7.86;15,151.52,216.27,144.07,7.86;15,409.23,226.56,71.36,7.86;15,151.52,237.52,181.25,7.86">Recognizing birds from sound-the 2018 birdclef baseline system</title>
		<imprint/>
	</monogr>
	<note>A baseline for largescale bird species identification</note>
</biblStruct>

<biblStruct coords="15,142.62,247.80,337.98,7.86;15,151.52,258.76,43.78,7.86;15,134.77,269.04,345.83,7.86;15,151.52,280.00,329.07,7.86;15,151.52,290.96,78.97,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="15,398.91,247.80,81.69,7.86;15,151.52,258.76,43.78,7.86;15,134.77,269.04,7.85,7.86;15,249.08,269.04,212.21,7.86">Ws34 -deep learning in heterogenen datenbestnden</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cuevas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,269.35,280.00,82.33,7.86">INFORMATIK 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Eibl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Gaedke</surname></persName>
		</editor>
		<meeting><address><addrLine>Bonn</addrLine></address></meeting>
		<imprint>
			<publisher>Gesellschaft fr Informatik</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">2141</biblScope>
		</imprint>
	</monogr>
	<note>Retrieving events in life logging 23</note>
</biblStruct>

<biblStruct coords="15,142.62,301.24,337.97,7.86;15,151.52,312.20,329.07,7.86;15,151.52,323.16,329.07,7.86;15,151.52,334.12,158.80,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="15,459.73,301.24,20.85,7.86;15,151.52,312.20,329.07,7.86;15,151.52,323.16,34.67,7.86">Evaluation of cnn-based algorithms for human pose analysis of persons in red carpet scenarios</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heinzig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Helmert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Brunnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,328.64,323.16,82.63,7.86">INFORMATIK 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Eibl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Gaedke</surname></persName>
		</editor>
		<meeting><address><addrLine>Bonn</addrLine></address></meeting>
		<imprint>
			<publisher>Gesellschaft fr Informatik</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2201" to="2209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,344.40,337.97,7.86;15,151.52,355.36,329.07,7.86;15,151.52,366.32,329.07,7.86;15,151.52,377.28,329.07,7.86;15,151.52,388.24,182.03,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="15,265.91,344.40,214.68,7.86;15,151.52,355.36,125.58,7.86">Lifelogging memory appliance for people with episodic memory impairment</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<idno type="DOI">10.1145/1409635.1409643</idno>
		<ptr target="http://doi.acm.org/10.1145/1409635.1409643" />
	</analytic>
	<monogr>
		<title level="m" coord="15,309.08,355.36,171.51,7.86;15,151.52,366.32,166.33,7.86;15,386.38,366.32,57.86,7.86">Proceedings of the 10th International Conference on Ubiquitous Computing</title>
		<meeting>the 10th International Conference on Ubiquitous Computing<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="44" to="53" />
		</imprint>
	</monogr>
	<note>UbiComp &apos;08</note>
</biblStruct>

<biblStruct coords="15,142.62,398.52,337.97,7.86;15,151.52,409.48,329.07,7.86;15,151.52,420.44,329.07,7.86;15,151.52,431.40,86.19,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="15,268.27,398.52,212.32,7.86;15,151.52,409.48,148.00,7.86">Structuring continuous video recordings of everyday life using time-constrained clustering</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,320.98,409.48,159.61,7.86;15,151.52,420.44,81.19,7.86">Multimedia Content Analysis, Management, and Retrieval</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">6073</biblScope>
			<biblScope unit="page">60730D</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,441.68,337.97,7.86;15,151.52,452.64,329.07,7.86;15,151.52,463.60,255.88,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="15,329.82,441.68,150.77,7.86;15,151.52,452.64,35.30,7.86">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m" coord="15,208.20,452.64,250.94,7.86">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,473.88,319.38,7.86" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="15,255.09,473.88,148.05,7.86">Yolov3: An incremental improvement</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,484.16,337.97,7.86;15,151.52,495.12,329.07,7.86;15,151.52,506.08,329.07,7.86;15,151.52,517.04,262.72,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="15,353.73,484.16,126.86,7.86;15,151.52,495.12,245.61,7.86">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="15,442.66,506.08,37.93,7.86;15,151.52,517.04,188.88,7.86">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,549.24,337.98,7.86;15,151.52,560.20,329.07,7.86;15,151.52,571.16,251.48,7.86" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="15,151.52,560.20,238.11,7.86">Technische universität chemnitz at trecvid instance search</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heinzig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Herms</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manthey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,432.51,560.20,48.08,7.86;15,151.52,571.16,95.27,7.86">Proceedings of TRECVID Workshop</title>
		<meeting>TRECVID Workshop<address><addrLine>Orlando, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,581.44,337.98,7.86;15,151.52,592.40,169.77,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="15,257.15,581.44,223.45,7.86;15,151.52,592.40,15.56,7.86">Beyond total capture: a constructive critique of lifelogging</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sellen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Whittaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,174.05,592.40,118.57,7.86">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,602.68,337.97,7.86;15,151.52,613.64,329.07,7.86;15,151.52,624.60,65.07,7.86" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="15,407.90,602.68,72.69,7.86;15,151.52,613.64,271.93,7.86">Species prediction based on environmental variables using machine learning techniques</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Taubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mauermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">CLEF working notes</note>
</biblStruct>

<biblStruct coords="15,142.62,634.88,337.98,7.86;15,151.52,645.84,329.07,7.86;15,151.52,656.80,259.93,7.86" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="15,363.47,645.84,117.13,7.86;15,151.52,656.80,238.43,7.86">University of applied sciences mittweida and chemnitz university of technology at trecvid</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Thomanek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Roschke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manthey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Platte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rolletschke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heinzig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vodel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Zimmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,119.67,337.98,7.86;16,151.52,130.63,329.07,7.86;16,151.52,141.59,329.07,7.86;16,151.52,152.55,198.15,7.86" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="16,420.37,119.67,60.22,7.86;16,151.52,130.63,329.07,7.86;16,151.52,141.59,24.44,7.86">Visual concept selection with textual knowledge for activities of daily living and life moment retrieval</title>
		<author>
			<persName coords=""><forename type="first">Tsun-Hsien</forename><surname>Tang 12</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min-Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H H K T C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename></persName>
		</author>
		<ptr target="CEUR-WS" />
	</analytic>
	<monogr>
		<title level="m" coord="16,197.33,141.59,283.26,7.86;16,151.52,152.55,24.01,7.86">19th Working Notes of CLEF Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,163.51,337.97,7.86;16,151.52,174.47,329.07,7.86;16,151.52,185.43,329.07,7.86;16,151.52,196.39,264.77,7.86" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="16,420.32,163.51,60.26,7.86;16,151.52,174.47,229.22,7.86">Learning deep features for scene recognition using places database</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="16,442.65,185.43,37.94,7.86;16,151.52,196.39,184.78,7.86">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,229.26,337.97,7.86;16,151.52,240.22,329.07,7.86;16,151.52,251.18,106.05,7.86" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<title level="m" coord="16,151.52,240.22,329.07,7.86;16,151.52,251.18,77.37,7.86">Organizer team at imagecleflifelog 2017: baseline approaches for lifelog retrieval and summarization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
