<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.22,115.96,292.91,12.62;1,166.38,133.89,282.60,12.62">MIT Manipal at ImageCLEF 2019 Visual Question Answering in Medical Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,192.43,171.89,75.05,8.74"><forename type="first">Abhishek</forename><surname>Thanki</surname></persName>
							<email>abhishek.harish@learner.manipal.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Manipal Institute of Technology</orgName>
								<address>
									<addrLine>Manipal -Udupi District</addrLine>
									<postCode>576104</postCode>
									<region>Karnataka</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,294.64,171.89,123.81,8.74"><forename type="first">Krishnamoorthi</forename><surname>Makkithaya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Manipal Institute of Technology</orgName>
								<address>
									<addrLine>Manipal -Udupi District</addrLine>
									<postCode>576104</postCode>
									<region>Karnataka</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.22,115.96,292.91,12.62;1,166.38,133.89,282.60,12.62">MIT Manipal at ImageCLEF 2019 Visual Question Answering in Medical Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EC30841D65D6D75F02C9CD9993227ED8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>CNN</term>
					<term>Word2Vec</term>
					<term>LSTM</term>
					<term>Encoder-Decoder</term>
					<term>BLEU</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of MIT, Manipal in the ImageCLEF 2019 VQA-Med task. The goal of the task was to build a system that takes as input a medical image and a clinically relevant question, and generates a clinically relevant answer to the question by using the medical image. We explored a different approach compared to most VQA systems and focused on the answer generation part. We used a encoder-decoder architecture based on deep learning where a pretrained CNN on ImageNet was used to extract visual features from input image, a combination of pre-trained word embedding on pub-med articles along with a 2-layer LSTM was used to extract textual features from the question. Both visual and textual features were integrated using a simple element-wise multiplication technique. The integrated features were then passed into a LSTM decoder which then generated a natural language answer. We submitted a total of 8 runs for this task and the best model achieved a BLEU score of 0.462.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual Question Answering (VQA) is task which consists of building a AI system which takes as input a image and a question in natural language, and the system is expected to produce a correct answer to the question by using both the visual and the textual information. This problem intersects the two important fields of computer science, Computer Vision (CV) and Natural Language Processing (NLP). The answers can be as simple as a single word, a simple yes/no, true/false or consists of multiple words.</p><p>VQA task has so far made great progress in the general domain due to the increasing advancements in the field of computer vision and natural language processing. But this problem is relatively new in the medical domain. Image-CLEF conducts many tasks related to multimedia retrieval in many domains such as medicine, security, lifelogging, and nature <ref type="bibr" coords="2,355.08,118.99,9.96,8.74" target="#b0">[1]</ref>. Visual Question Answering in Medical domain is one such task and this is the second year, the VQA-Med task <ref type="bibr" coords="2,155.27,142.90,10.52,8.74" target="#b1">[2]</ref> has been introduced after last years success. Given a medical image and a clinically relevant question in natural language about the image, the task was to build a system that would produce a clinically relevant natural language answer to the question by using the image.</p><p>In this paper, we discuss our approach to build such a system which was inspired by VQA research in general domain <ref type="bibr" coords="2,329.50,202.68,10.52,8.74" target="#b2">[3]</ref> and sequence generation task in the natural language processing field <ref type="bibr" coords="2,293.59,214.64,9.96,8.74" target="#b3">[4]</ref>. We built a encoder-decoder architecture using the recent advancements in the field of deep learning. The model consists of a pre-trained Convolutional Neural Network (CNN) on ImageNet, a pre-trained word2vec model trained on pud-med articles <ref type="bibr" coords="2,326.92,250.50,10.52,8.74" target="#b4">[5]</ref> to extract word embeddings, and two Long Short Term Memory (LSTM) models. Image features were extracted using a pre-trained CNN on ImageNet. We tested with two architectures, VGG19 and DenseNet201 <ref type="bibr" coords="2,215.15,286.37,20.40,8.74">[6][7]</ref>. Question features were extracted by using pre-trained word2vec model and a 2-layer LSTM network. Both visual and textual features were integrated by using element-wise multiplication and resulting features were fed to a LSTM sequence generating network to produce the output answers.</p><p>This paper is organized in the following manner: Section 2 provides a information regarding the dataset provided for this challenge. Section 3 presents related work done which inspired us our model architecture. Section 4 describes our method of using a encoder-decoder architecture. Section 5 describes our experiments and corresponding results our model achieved. Finally, we conclude the paper in Section 6 by discussing the task, our method, and future improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset Description</head><p>In VQA-Med 2019 challenge, three datasets were provided:</p><p>The training set consisted of 3,200 medical images with 12,792 questionanswer pairs. The validation set consisted of 500 medical images with 2,000 questionanswer pairs. The test set consisted of 500 medical images with 500 question-answer pairs. Furthermore, the data in the dataset can be divided into four main categories as follows:</p><p>1. Modality: This category includes questions based on images of structural or functional parts of the body. For example: ultrasound, CT, etc. 2. Plane: Questions in this category consists about the plane of the medical image. This is important because different projections allow for depicting different tissues. For example: axial, sagittal, etc. 3. Organ System: Questions in this category consists on the different organs in the human body. For example: breast, skull and contents, etc. 4. Abnormality: Questions in this category consists of detecting any abnormality present in the input image and identifying the type of abnormality.</p><p>VQA in general domain is not a new problem and the data available is much more compared to VQA in medical domain. Due to these reasons a lot of work has been done in the general domain. Our work in this paper takes inspiration from various resources. First, we are inspired by the simplicity in the baseline model from <ref type="bibr" coords="3,158.15,191.58,10.52,8.74" target="#b2">[3]</ref> which still achieved good accuracy. Second, since the words used in the medical domain are quite different compared to general English this means that a word2vec trained on English language does not produce vectors that can best encode the questions and hence we used a word2vec model trained on pub-med articles for encoding the question tokens. Third, while a lot of models developed on VQA general domain use multi-class classification to generate answers, we chose a different approach of using sequence generation to generate the answers since it made more intuitive sense to us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>Our system consists of two main components: encoder and decoder. The encoder part consists of 3 sub-components: transfer learning to extract features from images, word2vec + 2-layer LSTM to extract features from questions, and element-wise multiplication to fuse the visual and textual features. The decoder part consists of a sequence generating LSTM network which generates the output answers to the input question and image. Fig. <ref type="figure" coords="3,332.76,392.54,4.98,8.74" target="#fig_0">1</ref> shows the high-level architecture of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoder</head><p>The encoder part of the model consists of:</p><p>-Pre-trained CNN on ImageNet: Deep CNNs trained on large-scale datasets such as ImageNet have demonstrated to be excellent at the task of transfer learning and this is why we chose transfer learning using a pre-trained CNN to extract visual features. For this purpose we experimented with VGG-19 <ref type="bibr" coords="3,166.07,524.44,10.52,8.74" target="#b5">[6]</ref> and DenseNet-201 <ref type="bibr" coords="3,265.81,524.44,10.52,8.74" target="#b6">[7]</ref> architectures. For VGG-19, we extracted the output features from its last hidden layer while in case of DenseNet201, we extracted the output features from conv5 block32 concat layer output. These extracted features were then passed through a dense layer which was trainable to get the final output visual features in various dimensions such as 128, 256, and 512. -Pre-trained word2vec model + 2-layer LSTM network: To extract features from the input question, we pre-processed by building a custom function which cleans the input sentence and outputs a list of tokens. These tokens were then converted to vector form by using a pre-trained word2vec model <ref type="bibr" coords="3,151.70,644.16,10.72,8.74" target="#b7">[8]</ref>[9] <ref type="bibr" coords="3,173.14,644.16,14.29,8.74" target="#b9">[10]</ref>. These vectors were then passed through a 2-layer LSTM network to produce the output textual features. The reason why we chose LSTM </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decoder</head><p>The decoder part of the model consists of a LSTM network. This network takes as input the output features from the decoder part as well as the state of the second LSTM. The sequence generation step is started by providing as input a special token &lt;SOS&gt;. Subsequent output tokens produced by the model are fed back into the model to produce the next token. This process is continued until a certain number of tokens are produced or a special token called &lt;EOS&gt; is predicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Result</head><p>We submitted eight runs to ImageLCEF 2019 VQA-Med:</p><p>1. VGG19-N128: This run used a VGG-19 for transfer learning and the number of neurons set in the encoder LSTM networks, the 2 dense layers, and the decoder LSTM network was 128. This network was trained for 100 epochs. 2. VGG19-N256: This was same as run number one except that the number of neurons were 256 and it was trained for 200 epochs.</p><p>3. VGG19-N256-Dropout: This run was same as run number two except that a dropout of 0.2 was used in the dense layers and it was trained for 150 epochs. 4. DenseNet201-N256: This run used a DenseNet-201 for transfer learning and the number of neurons set in the encoder LSTM networks, the 2 dense layers, and the decoder LSTM network was 256. This network was trained for 150 epochs. 5. DenseNet201-N256-D400: This run was similar to run five except that it used the embedding dimension used was 400 instead of 200 which was used in all the previous experiments. 6. DenseNet201-N256: This run was similar to run five except that the network was trained for 200 epochs. 7. DenseNet201-N128: This run was similar to run five except that the number of neurons were 256. 8. VGG19-N128: This run was identical to the first run. The VGG19-N128 model achieves the best BLEU score while DenseNet201-N256-Dropout achieves the best strict accuracy. Table <ref type="table" coords="5,366.51,476.33,4.98,8.74" target="#tab_0">1</ref> shows the result achieved by all models on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper describes our participation in the ImageCLEF 2019 VQA-Med challenge. We used a pre-trained CNN on ImageNet dataset to extract textual features, a word2vec + 2-layer LSTM network to extract textual features, and a sequence generating LSTM network to generate the output answer tokens. Our approach was different and instead focused on using sequence generation to generate the answers while using a simple element-wise multiplication technique to integrate the visual and textual features. While we would have liked to try out attention based techniques to integrate visual and textual features but we weren't able to do so due to the timing limitation. This is something which we will explore in the future to improve the model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,205.99,337.91,203.38,7.89;4,134.77,115.83,345.82,207.31"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Higher level architecture of the best model</figDesc><graphic coords="4,134.77,115.83,345.82,207.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,196.76,315.94,221.84,117.78"><head>Table 1 .</head><label>1</label><figDesc>Result of all the runs</figDesc><table coords="5,196.76,334.99,221.84,98.72"><row><cell>No. Model</cell><cell>BLEU Strict accuracy</cell></row><row><cell>1 VGG19-N128</cell><cell>0.462 0.15</cell></row><row><cell>2 VGG19-N256</cell><cell>0.433 0.126</cell></row><row><cell>3 VGG19-N256-Dropout</cell><cell>0.453 0.142</cell></row><row><cell>4 DenseNet201-N256</cell><cell>0.455 0.158</cell></row><row><cell cols="2">5 DenseNet201-N256-Dropout 0.453 0.16</cell></row><row><cell>6 DenseNet201-N256-D400</cell><cell>0.447 0.15</cell></row><row><cell>7 DenseNet201-N256</cell><cell>0.301 0.098</cell></row><row><cell>8 VGG19-N128</cell><cell>0.462 0.15</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,142.59,337.63,7.86;6,151.52,153.55,329.07,7.86;6,151.52,164.51,329.07,7.86;6,151.52,175.46,329.07,7.86;6,151.52,186.42,329.07,7.86;6,151.52,197.38,329.07,7.86;6,151.52,208.34,329.07,7.86;6,151.52,219.30,329.07,7.86;6,151.52,230.26,329.07,7.86;6,151.52,241.22,216.27,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,268.29,197.38,212.30,7.86;6,151.52,208.34,124.23,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,297.67,208.34,182.92,7.86;6,151.52,219.30,329.07,7.86;6,151.52,230.26,122.46,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="6,305.55,230.26,171.07,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="6,142.96,252.18,337.64,7.86;6,151.52,263.14,329.07,7.86;6,151.52,274.09,329.07,7.86;6,151.52,285.05,329.07,7.86;6,151.52,296.01,91.16,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,167.31,263.14,313.29,7.86;6,151.52,274.09,33.63,7.86">Vqa-med: Overview of the medical visual question answering task at imageclef 2019</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="6,206.89,274.09,108.17,7.86">CLEF 2019 Working Notes</title>
		<title level="s" coord="6,322.55,274.09,158.03,7.86;6,151.52,285.05,27.09,7.86">CEUR Workshop Proceedings (CEUR-WS.org</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,306.97,337.63,7.86;6,151.52,317.93,329.07,7.86;6,151.52,328.89,78.38,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,151.52,317.93,134.59,7.86">VQA: Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,308.15,317.93,172.44,7.86;6,151.52,328.89,49.71,7.86">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,339.85,337.63,7.86;6,151.52,350.81,329.07,7.86;6,151.52,361.77,281.85,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,219.48,350.81,261.11,7.86;6,151.52,361.77,120.48,7.86">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,142.96,372.72,337.63,7.86;6,151.52,383.68,329.07,7.86;6,151.52,394.64,329.07,7.86;6,151.52,405.60,329.07,7.86;6,151.52,416.56,183.74,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,363.96,372.72,116.63,7.86;6,151.52,383.68,176.29,7.86">Deep relevance ranking using enhanced document-query interactions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Brokos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1211" />
	</analytic>
	<monogr>
		<title level="m" coord="6,353.21,383.68,127.38,7.86;6,151.52,394.64,259.61,7.86">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">Oct-Nov 2018</date>
			<biblScope unit="page" from="1849" to="1860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,427.52,337.63,7.86;6,151.52,438.48,231.27,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,278.92,427.52,201.67,7.86;6,151.52,438.48,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,142.96,449.44,337.64,7.86;6,151.52,460.40,329.07,7.86;6,151.52,471.35,186.91,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,405.15,449.44,75.45,7.86;6,151.52,460.40,89.79,7.86">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,261.20,460.40,219.39,7.86;6,151.52,471.35,93.91,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,482.31,337.63,7.86;6,151.52,493.27,263.42,7.86" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m" coord="6,341.33,482.31,139.26,7.86;6,151.52,493.27,101.88,7.86">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,142.96,504.23,337.63,7.86;6,151.52,515.19,329.07,7.86;6,151.52,526.15,217.12,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,407.41,504.23,73.17,7.86;6,151.52,515.19,232.55,7.86">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,404.48,515.19,76.12,7.86;6,151.52,526.15,123.83,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="3111" to="3119" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,537.11,337.98,7.86;6,151.52,548.07,329.07,7.86;6,151.52,559.03,329.07,7.86;6,151.52,569.98,114.22,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,290.52,537.11,190.07,7.86;6,151.52,548.07,59.42,7.86">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,234.09,548.07,246.50,7.86;6,151.52,559.03,329.07,7.86;6,151.52,569.98,30.54,7.86">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.62,580.94,337.97,7.86;6,151.52,591.88,92.85,7.89" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,288.76,580.94,100.72,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,398.70,580.94,81.89,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,602.86,337.97,7.86;6,151.52,613.82,285.15,7.86" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<title level="m" coord="6,414.31,602.86,66.28,7.86;6,151.52,613.82,256.48,7.86">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
