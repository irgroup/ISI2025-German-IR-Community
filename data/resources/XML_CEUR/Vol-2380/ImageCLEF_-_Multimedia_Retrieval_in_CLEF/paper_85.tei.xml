<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.66,115.96,336.03,12.62;1,152.39,133.89,310.59,12.62">Zhejiang University at ImageCLEF 2019 Visual Question Answering in the Medical Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,180.16,171.97,36.25,8.74"><forename type="first">Xin</forename><surname>Yan</surname></persName>
							<email>yanxin@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.97,171.97,26.85,8.74"><forename type="first">Lin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.37,171.97,47.05,8.74"><forename type="first">Chulin</forename><surname>Xie</surname></persName>
							<email>chulinxie@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.98,171.97,39.72,8.74"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
							<email>junx@cs.zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,391.62,171.97,31.20,8.74;1,426.14,170.39,2.14,6.12"><forename type="first">Lin</forename><surname>Gu</surname></persName>
							<email>ling@nii.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Informatics (NII)</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.66,115.96,336.03,12.62;1,152.39,133.89,310.59,12.62">Zhejiang University at ImageCLEF 2019 Visual Question Answering in the Medical Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1EC429D009EBD392F9075C667FF170EC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>VGG Network</term>
					<term>Global Average Pooling</term>
					<term>BERT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the submission of Zhejiang University for Visual Question Answering task in medical domain (VQA-Med) of ImageCLEF 2019 <ref type="bibr" coords="1,231.19,314.32,11.52,7.86" target="#b1">[2]</ref>. We propose a novel convolutional neural network (CNN) based on VGG16 network and Global Average Pooling strategy to extract visual features. Our proposed CNN is able to effectively capture the medical image features under small training set. The semantic features of the raised question is encoded by a BERT model. We then leverage a co-attention mechanism to fuse these two features enhanced with jointly learned attention. These vectors then are then fed to a decoder to predict the answer in a manner of classification. Our model achieves the score with 0.624 in accuracy and 0.644 in BLEU, which ranked first among all participating groups in the ImageCLEF 2019 VQA-Med task <ref type="bibr" coords="1,243.64,423.91,10.87,7.86" target="#b1">[2]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual Question Answering (VQA) is a multidisciplinary task involves both Computer Vision (CV) and Natural Language Processing (NLP) techniques. As illsutrated in Fig. <ref type="figure" coords="1,191.91,541.85,4.13,8.74" target="#fig_0">1</ref>, presented with an image, the VQA system is expected to answer the raised natural language question about it.</p><p>In recent years, VQA has been successful in the general domain with a number of effective models and large-scale datasets. With the development of medical digitization, VQA in medical domain is drawing attention because it could not only serve as a supplementary reference for clinical decision, but also help patients better and faster understand their conditions from medical images. To promote the development, ImageCLEF 2019 organises 2nd edition of the Medical Domain Visual Question Answering Task <ref type="bibr" coords="2,326.35,130.95,12.68,8.74" target="#b1">[2]</ref>.</p><p>However, the VQA is very challenging on the medical task. On one hand, valid medical data for training are limited compared to those in the general domain. On the other hand, the content and focus of medical images are distinct from the general images. The glossary and the presentation of sentences in medical report are also different from language in every-day's discussion.</p><p>In this paper, we propose a system to effectively solve the Med-VQA problem in ImageCLEF 2019 challenge <ref type="bibr" coords="2,264.65,215.05,12.20,8.74" target="#b7">[8]</ref>. As illustrated in Fig. <ref type="figure" coords="2,377.60,215.05,4.13,8.74" target="#fig_1">2</ref>, the proposed system comprises the following steps: 1. With a novel Convolutional Neural Network (CNN), visual features are extracted from the input image. 2. BERT <ref type="bibr" coords="2,428.02,238.96,15.31,8.74" target="#b3">[4]</ref>, a NLP network is applied to capture syntactic patterns of question and encode it into context features. 3. To extenuate the effect of irrelevant or noisy information, attention mechanism is introduced to focus on particular image regions based on language. 4. Feature fusion mechanism is used to integrate the visual and textural feature vectors to generate a jointed representation. 5. These vectors then are then fed to a decoder to predict the answer in a manner of classification.</p><p>Our main contribution can be concluded as follows: Firstly, a novel CNN based on VGG16 <ref type="bibr" coords="2,204.51,334.81,21.46,8.74" target="#b11">[12]</ref> network and Global Average Pooling <ref type="bibr" coords="2,391.37,334.81,17.56,8.74" target="#b9">[10]</ref> strategy is proposed to extract visual features under limited training set. Secondly, we use Multi-modal Factorized Bilinear Pooling (MFB) <ref type="bibr" coords="2,344.61,358.72,20.42,8.74" target="#b17">[18]</ref> with co-attention to fuse these two features enhanced with jointly learned attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>According to recent work on VQA problems in the general domain, the performance of VQA is particularly sensitive to feature fusion strategy of textual and visual information.</p><p>Deep Convolution Neural Networks(CNNs), such as VGGNet <ref type="bibr" coords="2,407.11,464.63,20.68,8.74" target="#b11">[12]</ref>, ResNet <ref type="bibr" coords="2,462.69,464.63,13.42,8.74" target="#b5">[6]</ref>, Inception, pretrained on large dataset in the general domain has been successfully explored to extract image feature in recent years. When encoding the question, the majority of research use Recurrent Neural Networks(RNNs) and such as long short-term memory (LSTM) <ref type="bibr" coords="2,273.50,512.45,14.74,8.74" target="#b6">[7]</ref>, grated recurrent units(GRU) <ref type="bibr" coords="2,418.17,512.45,14.37,8.74" target="#b2">[3]</ref> to capture syntactic patterns.</p><p>For fine-grained image and question representation, attention mechanisms are effective in extracting the localized image or language features, while global features may bring irrelevant or noisy information. Attention mechanisms have been successfully employed in image captioning <ref type="bibr" coords="2,338.68,572.43,15.50,8.74" target="#b15">[16]</ref> and machine translation <ref type="bibr" coords="2,462.32,572.43,14.61,8.74" target="#b14">[15]</ref>, <ref type="bibr" coords="2,134.77,584.39,9.96,8.74" target="#b0">[1]</ref>. For VQA task, <ref type="bibr" coords="2,222.46,584.39,15.50,8.74" target="#b16">[17]</ref> developed a multiple-layer stacked attention networks (SANs) to query an image multiple times to progressively infer the answer. <ref type="bibr" coords="2,134.77,608.30,15.50,8.74" target="#b13">[14]</ref> used image features from bottom-up attention to provide region-specific features. <ref type="bibr" coords="2,174.83,620.25,15.50,8.74" target="#b12">[13]</ref> built upon previous VQA models by developing thirteen attention mechanisms and introducing a simplified classifier to the model. <ref type="bibr" coords="2,411.54,632.21,15.50,8.74" target="#b10">[11]</ref> put forward a novel "co-attention" mechanism that jointly reasons about visual attention and question attention.</p><p>With respect to multi-modal feature fusion, <ref type="bibr" coords="3,343.02,118.99,15.50,8.74" target="#b10">[11]</ref> presented a hierarchical coattention model (Hie+CoAtt) which combines the co-attention multi-modal features using element-wise summations, concatenation, and fully connected layers. <ref type="bibr" coords="3,134.77,154.86,10.52,8.74" target="#b4">[5]</ref> proposed to rely on Multimodal Compact Bilinear pooling (MCB) which computes the outer product between two vectors. <ref type="bibr" coords="3,352.02,166.81,10.52,8.74" target="#b8">[9]</ref> employed the Multi-modal Low-rank Bilinear (MLB) pooling model to get a joint representation based on the Hadamard product of two feature vectors. <ref type="bibr" coords="3,359.54,190.72,15.50,8.74" target="#b17">[18]</ref> developed Multi-modal Factorized Bilinear pooling (MFB) method and combined it with co-attention learning. Our proposed VQA model in medical domain derives inspiration from that architecture (MFB+CoAtt).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Description</head><p>Fig. <ref type="figure" coords="3,156.29,343.24,4.98,8.74" target="#fig_0">1</ref> shows two examples in VQA-Med dataset <ref type="bibr" coords="3,350.38,343.24,12.24,8.74" target="#b1">[2]</ref>. In the ImageCLEF 2019 VQA-Med task <ref type="bibr" coords="3,198.59,355.19,11.75,8.74" target="#b1">[2]</ref>, the dataset are divided into three subsets:</p><p>-The training set contains 12792 question-answer pairs associated with 3200 training images.</p><p>-The validation set contains 2000 question-answer pairs associated with 500 training images.</p><p>-The test set contains 500 question-answer pairs associated with 500 training images.</p><p>There are 4 categories of questions: abnormality, modality, organ system and plane.</p><p>-Abnormality: the questions are mainly in two forms: 1. inquiry on the existence of abnormalities in the picture, 2. inquiry on the abnormal type.</p><p>-Modality, inquiry on the types of medical images such as mri, CT images.</p><p>-Organ, inquiry on what organ is shown in the image.</p><p>-Plane, inquiry on the captured plane such as vertical or horizontal.  In open-domain VQA, the convolutional network like VGGNet <ref type="bibr" coords="4,401.62,632.21,21.64,8.74" target="#b11">[12]</ref> or ResNet <ref type="bibr" coords="4,466.60,632.21,14.00,8.74" target="#b5">[6]</ref> are usually used to extract image feature map which represents the visual content of the image. In order to extract the feature of medical image, we proposed a new convolution network that based on VGG16 network(pretrained on ImageNet) and Global Average Pooling <ref type="bibr" coords="5,254.41,130.95,17.56,8.74" target="#b9">[10]</ref> strategy. We remove all the fully-connected layers in the VGG16 network and the convolution outputs of different feature scales are concatenated after global average pooling to form a 1984-dimensional vector to represent the image. The architecture is shown in Fig. <ref type="figure" coords="5,374.24,166.81,4.13,8.74" target="#fig_2">3</ref>. Our experiments show that the new network structure could effectively avoid over-fitting and improve the accuracy of the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Question semantic encoder</head><p>We propose a question encoder based on the bidirectional encoder representation from transformers(BERT) <ref type="bibr" coords="5,245.13,572.43,14.36,8.74" target="#b3">[4]</ref> to get the semantic feature of question. BERT <ref type="bibr" coords="5,464.28,572.43,16.31,8.74" target="#b3">[4]</ref> is a pre-trained language representation model proposed by Google. Unlike the context-free model such as Glove which generates a "word embedding" for each word, BERT <ref type="bibr" coords="5,184.54,608.30,16.31,8.74" target="#b3">[4]</ref> emphasizes more on the relationships between a word and the other words in a sentence that can effectively avoid polysemy. The model we used is a basic version of BERT <ref type="bibr" coords="5,264.72,632.21,16.31,8.74" target="#b3">[4]</ref> which includes 12 layers, 768 hidden variables with a total of 110M parameters. To represent each sentence, we average the last and penultimate layer to obtain a 768-d question feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Feature fuse with co-attention mechanism</head><p>The strategy to combine visual and semantic feature plays an important role in improving performance of VQA task. Co-attention mechanism assigns weight of importance to features from different regions to avoid the irrelevant information. We therefore use multi-modal factorized bilinear pooling (MFB) <ref type="bibr" coords="6,424.74,180.58,15.50,8.74" target="#b17">[18]</ref> with coattention to fuse the two modalities of features. The network is shown in Fig. <ref type="figure" coords="6,472.34,192.54,4.13,8.74" target="#fig_3">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visual Feature Network</head><p>As discussed above, the convolution network with global average pooling <ref type="bibr" coords="6,463.28,547.36,17.31,8.74" target="#b9">[10]</ref> could effectively avoid the over-fitting on the small dataset. As shown in Fig. <ref type="figure" coords="6,472.33,559.32,4.13,8.74" target="#fig_4">5</ref>.</p><p>Severe over-fitting has occurred in the model without GAP (the left one). As the training progresses, the loss on the validation set decreases and then increases. This did not happen in the model with GAP (the right one) and the model achieves higher accuracy on validation set. Based on the performance on the validation set, the parameters are set as follows. We use the ADAM optimizer with initial learning rate 1e-4. The regularization coefficient is 1e-3. The dropout coecient in MFB is 0.3 and the batch size is 32. We train the model for 300 epoch on one GTX1080Ti for 1 hours. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation</head><p>The VQA-Med competition <ref type="bibr" coords="7,252.40,312.23,13.34,8.74" target="#b1">[2]</ref> adopted two evaluation indexes, accuracy(strict) and BLEU. Accuracy was used to measure the ratio between the number of correctly classified and the total number of test data set. BLEU measures the similarity between the predicted answer and the actual answer. Based on the above architecture, we submitted six valid runs, among which "VGG16(GAP)+BERT+MFB" achieved the best accuracy score of 0.624 and the best BLEU score of 0.644. The result of the competition is shown in the Figure .6 with the team ID: Hanlin. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,317.73,345.83,7.89;4,134.77,328.71,182.72,7.86;4,134.77,115.84,345.83,187.12"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Two examples of medical image and the associated question-answer pair from the ImageCLEF 2019 VQA-Med training set.</figDesc><graphic coords="4,134.77,115.84,345.83,187.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,245.06,573.87,125.23,7.89;4,134.77,461.95,345.83,97.15"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Our model architecture</figDesc><graphic coords="4,134.77,461.95,345.83,97.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,242.64,467.27,130.08,7.89;5,134.77,226.06,345.82,226.44"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Image feature extraction</figDesc><graphic coords="5,134.77,226.06,345.82,226.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,245.48,431.26,124.39,7.89;6,134.77,226.28,345.82,190.21"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. MFB with co-attention</figDesc><graphic coords="6,134.77,226.28,345.82,190.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,214.94,259.93,185.48,7.89;7,134.77,115.83,345.82,129.33"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Contrast whether GAP is used or not.</figDesc><graphic coords="7,134.77,115.83,345.82,129.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,134.77,633.47,345.83,7.89;7,134.77,414.27,345.82,204.43"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Official Results of ImageCLEF 2019 VQA-Med. The ID of our team is Hanlin.</figDesc><graphic coords="7,134.77,414.27,345.82,204.43" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper,we describes the model we submitted in ImageCLEF 2019 VQA-Med task. Our proposed model VGG16(GAP)+BERT+MFB could effectively suppress over-fitting on small data sets. We have achieved the score with 0.624 in accuracy and 0.644 in BLEU on the test set. This performance ranks the first among all participating groups. In futurewe will continue improving the accuracy of our model and evaluating it on more datasets.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,306.07,337.63,7.86;8,151.52,317.03,329.07,7.86;8,151.52,327.99,87.07,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,385.15,306.07,95.44,7.86;8,151.52,317.03,178.23,7.86">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName coords=""><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,336.78,317.03,143.81,7.86;8,151.52,327.99,59.08,7.86">international conference on learning representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,338.95,337.64,7.86;8,151.52,349.91,329.07,7.86;8,151.52,360.87,329.07,7.86;8,151.52,371.83,329.07,7.86;8,151.52,382.79,91.48,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,277.47,349.91,203.12,7.86;8,151.52,360.87,134.70,7.86">VQA-Med: Overview of the medical visual question answering task at imageclef 2019</title>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="8,309.86,360.87,170.73,7.86;8,151.52,371.83,69.64,7.86">CLEF2019 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,393.75,337.64,7.86;8,151.52,404.71,329.07,7.86;8,151.52,415.67,329.07,7.86;8,151.52,426.63,92.39,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,368.97,404.71,111.62,7.86;8,151.52,415.67,264.16,7.86">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.96,437.59,337.63,7.86;8,151.52,448.55,329.07,7.86;8,151.52,459.51,132.38,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,439.96,437.59,40.63,7.86;8,151.52,448.55,293.36,7.86">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.96,470.48,337.64,7.86;8,151.52,481.44,329.07,7.86;8,151.52,492.40,329.07,7.86;8,151.52,503.35,252.96,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,232.44,481.44,248.15,7.86;8,151.52,492.40,115.07,7.86">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName coords=""><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,285.55,492.40,195.04,7.86;8,151.52,503.35,161.85,7.86">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,514.32,337.64,7.86;8,151.52,525.28,219.48,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,391.23,514.32,89.37,7.86;8,151.52,525.28,83.92,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,254.98,525.28,23.16,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,536.24,337.64,7.86;8,151.52,547.20,125.83,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,325.43,536.24,97.75,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,431.76,536.24,48.84,7.86;8,151.52,547.20,31.76,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,558.17,337.63,7.86;8,151.52,569.13,329.07,7.86;8,151.52,580.09,329.07,7.86;8,151.52,591.04,329.07,7.86;8,151.52,602.00,329.07,7.86;8,151.52,612.96,329.07,7.86;8,151.52,623.92,329.07,7.86;8,151.52,634.88,329.07,7.86;8,151.52,645.84,329.07,7.86;8,151.52,656.80,329.07,7.86;9,151.52,119.67,329.07,7.86;9,151.52,130.63,170.89,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,249.53,634.88,231.06,7.86;8,151.52,645.84,106.09,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitri</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aleh</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Narciso</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ergina</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Roberto Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Cuevas Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikos</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Konstantinos</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,275.07,645.84,205.52,7.86;8,151.52,656.80,329.07,7.86;9,151.52,119.67,100.74,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 10th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="9,436.22,119.67,44.37,7.86;9,151.52,130.63,127.46,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,141.59,337.64,7.86;9,151.52,152.55,329.07,7.86;9,151.52,163.51,258.55,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,319.99,152.55,160.60,7.86;9,151.52,163.51,167.32,7.86">Multimodal residual learning for visual qa. neural information processing systems</title>
		<author>
			<persName coords=""><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dong-Hyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,174.47,337.98,7.86;9,151.52,185.43,92.39,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,332.94,174.47,79.11,7.86">Network in network</title>
		<author>
			<persName coords=""><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.62,196.39,337.98,7.86;9,151.52,207.34,281.74,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="9,381.37,196.39,99.23,7.86;9,151.52,207.34,115.26,7.86">Hierarchical co-attention for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00061</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.62,218.30,337.97,7.86;9,151.52,229.26,275.91,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="9,327.39,218.30,153.20,7.86;9,151.52,229.26,114.45,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.62,240.22,337.98,7.86;9,151.52,251.18,329.07,7.86;9,151.52,262.14,20.99,7.86" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jasdeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Nutkiewicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07724</idno>
		<title level="m" coord="9,368.65,240.22,111.94,7.86;9,151.52,251.18,186.68,7.86">Attention on attention: Architectures for visual question answering (vqa)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.62,273.10,337.98,7.86;9,151.52,284.06,329.07,7.86;9,151.52,295.02,329.07,7.86;9,151.52,305.98,68.09,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,462.62,273.10,17.97,7.86;9,151.52,284.06,310.24,7.86">Tips and tricks for visual question answering: Learnings from the 2017 challenge</title>
		<author>
			<persName coords=""><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,295.02,300.02,7.86">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4223" to="4232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,316.93,337.97,7.86;9,151.52,327.89,329.07,7.86;9,151.52,338.85,251.65,7.86" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="9,377.20,327.89,103.40,7.86;9,151.52,338.85,151.20,7.86">Attention is all you need. neural information processing systems</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,349.81,337.98,7.86;9,151.52,360.77,329.07,7.86;9,151.52,371.73,329.07,7.86;9,151.52,382.69,130.98,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,339.58,360.77,141.01,7.86;9,151.52,371.73,166.26,7.86">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,330.07,371.73,150.52,7.86;9,151.52,382.69,30.82,7.86">international conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,393.65,337.98,7.86;9,151.52,404.61,311.04,7.86" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="9,151.52,404.61,227.93,7.86">Stacked attention networks for image question answering</title>
		<author>
			<persName coords=""><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,415.56,337.97,7.86;9,151.52,426.52,329.07,7.86;9,151.52,437.48,319.00,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,356.12,415.56,124.47,7.86;9,151.52,426.52,261.13,7.86">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,435.51,426.52,45.09,7.86;9,151.52,437.48,217.89,7.86">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1839" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
