<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.69,115.90,315.97,12.90;1,275.93,133.83,63.50,12.90">Recurrent Attention Networks for Medical Concept Prediction</title>
				<funder>
					<orgName type="full">Sullivan Nicolaides Pathology</orgName>
				</funder>
				<funder ref="#_nP7YZW9">
					<orgName type="full">Australian Research Council</orgName>
					<orgName type="abbreviated">ARC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,210.34,171.95,54.83,8.64"><forename type="first">Sam</forename><surname>Maksoud</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Electrical Engineering</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<settlement>Brisbane QLD</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.61,171.95,58.13,8.64"><forename type="first">Arnold</forename><surname>Wiliem</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Electrical Engineering</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<settlement>Brisbane QLD</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,354.67,171.95,50.34,8.64"><forename type="first">Brian</forename><surname>Lovell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Electrical Engineering</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<settlement>Brisbane QLD</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.69,115.90,315.97,12.90;1,275.93,133.83,63.50,12.90">Recurrent Attention Networks for Medical Concept Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">118EF449233A3054DC2FF746B36C9EF1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the working notes for the CRADLE group's participation in the ImageCLEF2019 medical competition. Our group focused on the concept detection task which challenged participants to approximate the mapping from radiology images to concept labels. Traditionally, such a task is often modelled as an image tagging or image retrieval problem. However, we empirically discovered that many concept labels had weak visual connotations; hence, image features alone are insufficient for this task. To this end, we utilize a recurrent neural network architecture which enables our model to capture the relational dependencies among concepts in a label set to supplement visual grounding when their association to image features is weak or unclear. We also exploit soft attention and visual gating mechanisms to enable our network to dynamically regulate "where" and "when" to extract visual data for concept generation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In 2019, ImageCLEF <ref type="bibr" coords="1,224.17,424.15,11.62,8.64" target="#b7">[8]</ref> hosted its 3rd edition of the medical image captioning task. The participants of this task were challenged to develop a method for generating Concept Unique Identifiers (CUI) to describe the contents of a radiology image <ref type="bibr" coords="1,449.61,448.06,15.27,8.64" target="#b12">[13]</ref>. In contrast to natural language captions, CUIs parse out standardized concept terms from the medical texts. Resolving captions into key concepts alleviates the constraint of modelling the syntactic structures of free text. Removing the language modelling component results in a task akin to image tagging i.e. identifying the presence of a label (CUI) by its most distinguishable visual features.</p><p>However, a considerable number of CUI terms in the supplied subset of the ROCO dataset <ref type="bibr" coords="1,165.33,543.70,16.60,8.64" target="#b13">[14]</ref> have no obvious association to visual features. This is due to the fact that the CUIs were extracted automatically from the figure captions using only natural language processing tools; there was no constraint for the CUIs to be associated with visual features. Consequently, concepts with weak visual connotations such as "study , "rehab" and "supplement" are abundant throughout the ROCO dataset <ref type="bibr" coords="1,425.38,591.52,15.49,8.64" target="#b13">[14]</ref>; it is unreasonable to assume that a model can learn general visual features to reliably identify such concepts. While it is true that in isolation, accurately identifying these non-visual concepts is unlikely or impossible, their relevance to an image can be indirectly estimated by modelling relational dependencies to other CUIs in the set of concept labels. This is because all CUIs in a set of concepts are derived from a common source: the original figure caption.</p><p>Under these conditions, our group concluded it would be best to model the problem as an image to sequence translation task; emphasizing the need to map an image to a set of concepts, rather than mapping individual CUIs directly to image features. Thus, we design our model as a recurrent neural network (RNN) given their unrivalled performance in capturing the long term dependencies in sequential data <ref type="bibr" coords="2,403.36,226.91,15.27,8.64" target="#b10">[11]</ref>. Our proposed RNN is conditioned on features from both the image and CUI labels. We utilize a soft attention mechanism <ref type="bibr" coords="2,219.41,250.82,16.60,8.64" target="#b17">[18]</ref> which dynamically attends to different regions of an image in order to select the most distinguishable visual features for each CUI. In situations where a CUI has weak visual connotations, a visual feature gating mechanism <ref type="bibr" coords="2,421.12,274.73,16.60,8.64" target="#b17">[18]</ref> allows the model to focus on textual features as they are likely to provide greater discriminatory power in such contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset Challenges</head><p>In order to design an appropriate model for the task, our group carried out an extensive investigation of the supplied subset of the ROCO dataset <ref type="bibr" coords="2,361.06,389.14,15.27,8.64" target="#b13">[14]</ref>. During this investigation we identified several challenges that would complicate the task of mapping text to visual features. These challenges pertain to incidences of redundant, inconsistent, and/or nonsensical assignment of CUIs to an image. We describe these challenges and how they influenced our approach to this task in detail below. First and foremost, we identified that a majority of concepts redundantly describe generic radiology images. In Table . 1 we list the top 10 most frequent concepts in the training dataset. Eight out of the top 10 concepts (all but "alesion" and "thoracics") could arguably describe most of the radiology images in the dataset. The ROCO dataset <ref type="bibr" coords="3,463.99,143.22,16.60,8.64" target="#b13">[14]</ref> exclusively contains radiological images; a concept such as "radiograph" would be appropriate for all of the images. However, since the umbrella concept of "radiograph" can be expressed using a variety of different CUIs, we are forced to find arbitrary features to distinguish these cognate identifiers. The CUIs C1548003 and C1962945 describe "radiograph" as a diagnostic procedure and a diagnostic service ID respectively. While distinguishing these different types of "radiograph" is trivial in natural language contexts, identifying discriminating visual features is an extremely dubious pursuit. As such, any model tasked with learning the haphazard distribution of these semantically interchangeable (and often universal) concepts in the ROCO dataset <ref type="bibr" coords="3,407.05,250.82,16.60,8.64" target="#b13">[14]</ref> is expected to have limited generalizability.</p><p>This property of the dataset has implications on the F1 score used to evaluate this task. The F1 score will penalize models for misidentifying the arbitrary instances or absences of these CUIs in the test data. This is because of the inherent stochasticity of the ROCO dataset <ref type="bibr" coords="3,210.02,322.55,15.49,8.64" target="#b13">[14]</ref>; where unobservable variations in source figure captions determine the CUIs assigned to a sample.</p><p>In the supplied subset of the ROCO dataset <ref type="bibr" coords="3,308.12,358.42,15.27,8.64" target="#b13">[14]</ref>, we observe recurring patterns of these semantically similar CUIs. For example C0043299 and C1962945 occur frequently as a pair but they also regularly occur as a tripartite alongside C1548003. An RNN architecture enables our model to exploit the statistical co-occurrence of these concepts when modelling probability distributions for a set of CUIs <ref type="bibr" coords="3,374.63,406.24,10.79,8.64" target="#b6">[7,</ref><ref type="bibr" coords="3,387.14,406.24,11.83,8.64" target="#b10">11]</ref>. To achieve a competitive F1 score, a model must not only learn "what" visual features best represent a CUI, but also "when" that CUI is most likely to occur in a given set of labels. Since all CUIs in a set of labels are derived from the same figure caption, modelling their interdependencies will ensure our model is more robust to the unobservable variations in the original figure caption. This enables our model to more reliably predict "when" a label is assigned to an image based on the learned co-occurrence statistics with previously generated concepts.</p><p>Another challenge encountered in this task is the assignment of nonsensical CUIs by the quickUMLS system <ref type="bibr" coords="3,216.05,536.89,16.60,8.64" target="#b15">[16]</ref> used to create the ROCO dataset <ref type="bibr" coords="3,362.83,536.89,15.27,8.64" target="#b13">[14]</ref>. The quickUMLS system utilizes the CPMerge algorithm for dictionary mapping <ref type="bibr" coords="3,360.70,548.84,15.27,8.64" target="#b15">[16]</ref>. CPMerge uses character trigrams as features and maps terms to a dictionary based on overlapping features <ref type="bibr" coords="3,461.50,560.80,15.27,8.64" target="#b15">[16]</ref>. This method introduces a significant source of error resulting in random and nonsensical CUIs being extracted from a medical figure caption. Table . 3 showcases examples of when trigram feature matching resulted in nonsensical or redundant CUIs being assigned to an image. This presents a major obstacle for multi-modal retrieval as minor changes in descriptive syntax results in significant and erratic variations of the CUIs extracted from the figure caption. For example, one could rephrase the last sentence for ROCO CLEF ID 25756 in Table . 3 as "Visualization of the proximal ACL is poor, suggesting an ACL rupture". The arbitrary decision to remove the word "substance" would no longer produce the erroneous CUI describing 11-Deoxycortisol (C0075414) based on its common name "Reichsteins Substance S".</p><p>To satisfy our scientific curiosity, we compared CUIs extracted using quickUMLS to those extracted by MetaMap <ref type="bibr" coords="4,250.90,413.67,10.79,8.64" target="#b0">[1]</ref>; as MetaMap is a commonly used alternative for automatic concept extraction. In the particular instance shown in Table . 2, the CUIs produced by MetaMap are undoubtedly higher quality than those produced by quick-UMLS. This is likely due to the fact that MetaMap does not use trigram character matching <ref type="bibr" coords="4,175.67,461.49,11.62,8.64" target="#b0">[1]</ref> and so it accurately captures C0007133 (Papillary Carcinoma) instead of C0226964 (Papilla of tongue). In the original paper describing quickUMLS <ref type="bibr" coords="4,461.50,473.45,15.27,8.64" target="#b15">[16]</ref>, the authors claim the quickUMLS system could outperform MetaMap in certain tasks. However, an important caveat of this claim is that they use SpaCy models to pre-process texts instead of the MetaMaps inbuilt pre-processing tools <ref type="bibr" coords="4,368.09,509.31,14.94,8.64" target="#b15">[16]</ref>). SpaCy pre-processing models are trained on a general text corpus whereas MetaMap utilizes the SPECIAL-IST lexicon <ref type="bibr" coords="4,183.74,533.22,10.58,8.64" target="#b0">[1]</ref>. Medical terms are highly featured in the SPECIALIST lexicon <ref type="bibr" coords="4,451.61,533.22,10.79,8.64" target="#b2">[3]</ref>; the lexems are likely to be more representative of those seen in radiology figure captions. Thus, substituting MetaMap's pre-processing tools with SpaCy's may not accurately reflect the performance of the end-to-end MetaMap system.</p><p>Furthermore, we empirically discovered that certain semantic types were more prone to erroneous assignment; the majority of nonsensical CUIs encountered were chemical names and abbreviations. In light of this issue, it may be worthwhile to investigate semantic types more prone to error and identify those which have the strongest visual connotations. This could assist our multi-modal retrieval models in determining how to weigh the importance of visual features and CUI relational dependencies based on the CUI's semantic type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Methodology</head><p>To overcome the challenges described in Section. 2 we seek to construct a model that satisfies the following requirements:</p><p>1. It must be able to identify the most distinguishable visual characteristics for a CUI; 2. It must capture interdependences among CUIs in a set of labels and; 3. It must be able to regulate the weight of visual features based on the variable strength of a CUI's visual connotation.</p><p>To this end, the proposed methodology borrows many features from the works of Xu et al <ref type="bibr" coords="5,145.08,560.80,15.27,8.64" target="#b17">[18]</ref>. Although their architecture was originally designed for use in image captioning tasks, the dynamic soft attention mechanism, recurrent inductive bias of long short term memory networks (LSTM) <ref type="bibr" coords="5,266.56,584.71,11.62,8.64" target="#b6">[7]</ref> and deterministic visual gating mechanism can be exploited to satisfy requirements (1), ( <ref type="formula" coords="5,287.58,596.66,3.87,8.64" target="#formula_1">2</ref>) and (3) respectively. We describe our methodology in detail below.</p><p>Firstly, we resize all images to 244x244x3 pixels in order to exploit a VGG16 <ref type="bibr" coords="5,463.99,632.53,16.60,8.64" target="#b14">[15]</ref> convolutional neural network (CNN) pre-trained on ImageNet <ref type="bibr" coords="5,385.12,644.48,10.58,8.64" target="#b5">[6]</ref>. Although the distribution of images in the ROCO dataset <ref type="bibr" coords="5,286.22,656.44,16.60,8.64" target="#b13">[14]</ref> differs greatly to the ImageNet dataset; there is empirical evidence to suggest that ImageNet-trained CNNs produce state-of-the-art results when transfer learning techniques are applied to smaller datasets. <ref type="bibr" coords="6,434.20,131.27,15.27,8.64" target="#b11">[12]</ref>. Given that the ROCO dataset <ref type="bibr" coords="6,227.04,143.22,16.60,8.64" target="#b13">[14]</ref> is over 200x smaller than ImageNet, we exploit ImageNettrained VGG16 models to benefit from this effect of transfer learning. Thus, we use the Keras <ref type="bibr" coords="6,160.80,167.13,11.62,8.64" target="#b3">[4]</ref> implementation of a VGG16 model with pre-trained ImageNet weights and extract the 14x14x512 vector from the "block-4 max-pooling" intermediary layer to represent the image features. We keep the weights of the CNN fixed during training to limit the number of trainable parameters; hence reducing the complexity of our model. We pre-process each label set such that each CUI is represented by its unique index in the concept vocabulary V = 5531. To represent concept features, we train an embedding space E ∈ R V xd ; where d is the concept vector dimensions. At the beginning of every time step, the CUI index at position t is used to retrieve its vector representation X t = 1xd from the embedding. Meanwhile, the attention mechanism takes the LSTM hidden state vector h t to construct a probability distribution, A t over the 14x14 spatial dimensions for the image; we multiply the feature vector by A t and average the spatial dimensions to produce a visual context vector with C t = 1x512 dimensions as per <ref type="bibr" coords="6,461.50,418.19,15.27,8.64" target="#b17">[18]</ref>. A visual sentinel learns to estimate a gating scalar S ∈ [0, 1] from h t to dynamically assign an attention weighting to C t ; this process is described in depth in <ref type="bibr" coords="6,433.38,442.10,15.27,8.64" target="#b17">[18]</ref>. As C t and S t are both produced as a function of h t , the network learns "where" to look for discriminatory visual features and how important those visual features are in generating the CUI at time t + 1.</p><p>Once we multiply gating scalar S t to context vector C t , we concatenate the image features with CUI feature vector X t along the last dimension to produce the 512 + d dimensional input to the LSTM network. A fully connected layer with relu activation reduces the D dimensional output of the LSTM network into a vector with d dimensions; residual connections to previous CUI are added by adding X t-1 to the output. We then multiply the resulting vector by P = R dxV and apply a softmax function to construct a probability distribution over all the concepts in the vocabulary. At t = 0, the LSTM is initialized on global image feature vector G. To produce G, image features are averaged along their 14x14 spatial dimensions and pushed through a fully connected layer to create a vector with the same dimensions as the LSTM input i.e. 512 + d.</p><p>The protocol described above represents the general framework for all 6 model variants used in this task. The learning rate lr = 0.0001 and batch size n = 125 were fixed across all variant training protocols and their performance was evaluated on the validation dataset after 20 epochs. We now describe each model variant in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model A</head><p>Model A is the standard implementation of our model. We set the dimensions D and d to 1024 and 512 respectively. The loss at each time step is calculated as the cross entropy between the estimated probability distribution and the ground truth concept label. In addition to using cross entropy loss, we use an alpha regularizing strategy described in <ref type="bibr" coords="7,134.77,253.64,16.60,8.64" target="#b9">[10]</ref> to regulate the outputs of the attention mechanism. When no constraints are placed on an attention network, a neural network can output nonsensical weights to optimize performance on training data. To ensure the attention mechanism produces attention salient weights we first construct an attention matrix α ∈ R 196xT from the probability distributions over the 14x14 spatial dimensions for each time step. As described in <ref type="bibr" coords="7,463.99,301.46,16.60,8.64" target="#b9">[10]</ref> we calculate the alpha regularizing term, L alpha from α as follows;</p><formula xml:id="formula_0" coords="7,256.53,334.19,224.07,30.32">L xu = N i (1 - C t α ti ) 2<label>(1)</label></formula><formula xml:id="formula_1" coords="7,218.80,373.70,261.79,30.20">L SAL = 1 C C t=0 ( max i (α ti ) -mean i (α ti ) mean i (α ti ) )<label>(2)</label></formula><formula xml:id="formula_2" coords="7,248.01,413.10,232.58,30.32">L T D = 1 N N i=0 ( std t (α ti ) mean t (α ti ) )<label>(3)</label></formula><p>Where L xu is the alpha regularising term in <ref type="bibr" coords="7,309.59,450.61,15.27,8.64" target="#b17">[18]</ref>, t represents the time axis, i represents the probability distribution axis, max i is the maximum value, mean i is the mean value along the column axis, std t is the standard deviation and mean t is the mean along the row axis of α ti . L xu ensures all image regions receive attention over the course generating each CUI, L SAL ensures attention mechanism produces salient attention maps at each time step and L T D ensures that the attention mechanism is not biased to any particular image region over the course of generation. The final alpha term can thus be written as;</p><formula xml:id="formula_3" coords="7,202.07,554.18,278.52,23.22">L alpha = λ 1 C xu + λ 2 max(δ, C SAL ) + λ 3 max(δ, C T D )<label>(4)</label></formula><p>Where λ 1 , λ 2 and λ 3 are hyper-parameters to scale the representation of each term. δ is used to avoid zero division and exploding gradients in the initial training steps. This loss term is then added to the total cross entropy loss and we perform standard backpropagation with ADAM optimisation <ref type="bibr" coords="7,289.84,620.57,11.62,8.64" target="#b8">[9]</ref> During training, we implement a teacher forcing training protocol <ref type="bibr" coords="7,402.39,644.48,16.60,8.64" target="#b16">[17]</ref> where we feed the ground truth CUI to the LSTM at every time step. During inference, the "START" token is fed into the LSTM network to get the probability distribution for the first CUI; the index with the highest probability estimate is used to generate the CUI for that time step. This process is repeated until a terminal "END" sequence token is produced or maximum time steps T have passed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model B</head><p>Model B is a standard implementation of our model. The protocol is identical to A except we restrict the dimension of the CUI feature vectors to d = 300. This was due to concerns that an embedding size of 512 may over-fit to the training distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model C</head><p>Model C is a standard implementation of our model. The protocol is identical to A except we restrict the dimension of the concept vectors to D = 512. This was due to concerns that an LSTM hidden state size of 1024 may over-fit to the training distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model D</head><p>Model D seeks to address the problem of cumulative error resulting in a bias towards learning samples with longer CUI sequences. In the standard implementation of our model, the maximum error for each sample is constrained by the number of CUIs in the set. This is because cross entropy error is calculated on a per concept basis (at each time step), not a per sample basis. To ensure each sample has equal weighting in the objective function, we divide the error at every time step by the total number of CUIs for each sample and multiply the result by the maximum number of CUIs (19). This ensures that every sample has the same theoretical maximum error and that the error incurred for each incorrect concept is relative to the total number of concepts in the set. Aside from the new weighted cross entropy loss function, Model D is otherwise identical to Model A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model E</head><p>Model E assesses the performance of our standard implementation without any constraints on our attention mechanism. Here, we use the standard implementation described in Model A except only the cross entropy error is used to train the network. This was done to ensure the alpha regularisation strategy is appropriate for this task and not over regulating our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Model F</head><p>Model F assesses the performance of our standard implementation without the visual sentinel. Here, we use a similar implementation to that described in Model A; however, we remove the step of estimating the visual gating scalar S t and allow the LSTM to be conditioned on the unscaled C t vector. This can be interpreted equally representing features from X t and C t at every time step; meaning that the network no longer has the capability of dynamically assessing the importance of visual features for each CUI. This was done to ensure that the gating scalars produced by the network in Model A actually resulted in improved outcomes with regards to performance on the validation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This section provides the results for our own internal evaluations on the validation dataset supplied for this task; these are tabulated in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Works</head><p>The performance of the proposed methods placed the CRADLE group 6 th out of 12 participating teams in the ImageCLEF 2019 medical concept detection task. The baseline architecture "Model A" achieved the highest performance. "Model B" and "Model C" did not improve the F1 score which suggests that the proposed dimensionality of hidden state and word embedding vectors in "Model A" is not resulting in over-fitting to the training distribution.</p><p>As evident by the reduced performance of "Model D", resolving disparities in concept distributions by normalising per-sample error has an adverse effect on training. This contrary to what was hypothesized in Section 3.4. In retrospect, normalizing persample error in fact forms a bias towards samples with fewer concepts. This is because the "disproportionate" increase in per-sample error for longer concept sequences would occur at time steps where operations are exclusive to those longer sequences. Once the "END" token is generated for a sample, the error at latter time steps for this sample should in fact be zero. The normalization methods described in Section 3.4 would unfairly disadvantage longer sequences by reducing the relative error at each time step. Subduing error in operations common to all samples to resolve disparities in total error due to exclusive operations in longer samples is counter-productive and is likely to explain the reduced performance of "Model D".</p><p>The reduced performance of 'Model E" confirms that unregulated attention mechanisms result in reduced performance and that the general constraints described in Section 3.1 are capable of improving attention and overall performance. "Model F" achieved one of the lowest F1 scores, highlighting the importance of regulating the weight of visual features depending on the visual connotations of each CUI. Future work will attempt to address the challenges described in Section 2 by studying the association of CUI semantic type to visual connotation. This will be achieved by retrieving CUI meta-data from the UMLS metathesaurus <ref type="bibr" coords="10,260.94,310.60,10.58,8.64" target="#b1">[2]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,179.31,477.79,256.73,149.04"><head>Table 1 .</head><label>1</label><figDesc>Top 10 most frequent concepts in the training data.</figDesc><table coords="2,179.31,509.07,256.73,117.76"><row><cell>RANK</cell><cell>CUI</cell><cell>FREQUENCY</cell><cell>CONCEPT</cell></row><row><cell>1</cell><cell>C0040395</cell><cell>6033</cell><cell>tomogr</cell></row><row><cell>2</cell><cell>C0034579</cell><cell>6002</cell><cell>pantomogr</cell></row><row><cell>3</cell><cell>C0043299</cell><cell>5830</cell><cell>x-ray procedure</cell></row><row><cell>4</cell><cell>C0441633</cell><cell>5283</cell><cell>diagnostic scanning</cell></row><row><cell>5</cell><cell>C1548003</cell><cell>5045</cell><cell>radiograph</cell></row><row><cell>6</cell><cell>C1962945</cell><cell>5044</cell><cell>radiogr</cell></row><row><cell>7</cell><cell>C0817096</cell><cell>4794</cell><cell>thoracics</cell></row><row><cell>8</cell><cell>C0772294</cell><cell>4372</cell><cell>alesion</cell></row><row><cell>9</cell><cell>C0040405</cell><cell>3113</cell><cell>x-ray computer assisted tomography</cell></row><row><cell cols="2">10 C0009924</cell><cell>2771</cell><cell>materials/contrast media</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,134.77,115.83,345.83,214.80"><head>Table 2 .</head><label>2</label><figDesc>This table compares CUIs produced by MetaMap<ref type="bibr" coords="4,346.14,116.18,10.45,7.77" target="#b0">[1]</ref> compared to the quickUMLS<ref type="bibr" coords="4,465.65,116.18,14.94,7.77" target="#b15">[16]</ref> for the following medical text: "Intensity modulated radiotherapy (IMRT) planning axial CT post-contrast showing a residual post-operative cystic nodal metastasis from papillary carcinoma (arrow). The patient underwent a further neck dissection to remove the node before IMRT was performed." (source caption for ROCO CLEF ID 42724).</figDesc><table coords="4,163.00,190.94,289.37,139.68"><row><cell>quickUMLS</cell><cell>MetaMap</cell></row><row><cell>C0034619 (radiother)</cell><cell>C1512814 (Intensity modulated radiotherapy)</cell></row><row><cell>C1522449 (radiation therapy)</cell><cell>C0032074 (Planning)</cell></row><row><cell>C2939420 (mets)</cell><cell>C0205131 (Axial)</cell></row><row><cell>C0027530 (collum)</cell><cell>C3888140 (CT)</cell></row><row><cell>C0012737 (dissection procedure)</cell><cell>C1609982 (Residual)</cell></row><row><cell>C0226964 (papillae linguales)</cell><cell>C0032790 (Postoperative Period)</cell></row><row><cell>C0935624 (capillaris)</cell><cell>C0205207 (Cystic)</cell></row><row><cell>C0746922 (noded)</cell><cell>C0443268 (Nodal)</cell></row><row><cell>C1328685 (metastat)</cell><cell>C0027627 (Metastasis)</cell></row><row><cell>C0227296 (papilla)</cell><cell>C0007133 (Carcinoma, Papillary)</cell></row><row><cell>C0027627 (spreading of cancer)</cell><cell>-</cell></row><row><cell>C0006901 (smallest blood vessel)</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,134.77,115.83,345.82,251.33"><head>Table 3 .</head><label>3</label><figDesc>This table shows examples of erroneous CUI assignment. To retrieve the original caption, each image was used to query the original figure caption pair using the Openi image search engine<ref type="bibr" coords="5,160.91,138.09,9.52,7.77" target="#b4">[5]</ref>.</figDesc><table coords="5,142.22,168.23,332.90,198.92"><row><cell>ROCO CLEF ID</cell><cell>ORIGINAL CAPTION</cell><cell>ERRONEOUS CUIs</cell><cell>REASON FOR ERROR</cell></row><row><cell>24120</cell><cell>Tc99m pertechnetate thyroid scan did not show any tracer concentration by the thyroid gland</cell><cell>C0004268 (concentration) C0086045 (attention concentration) C3827302 (i can concentrate well)</cell><cell>The caption refers to the chemistry definition of concentration however is has also been mistakenly interpreted as a verb</cell></row><row><cell>25756</cell><cell>A twenty five year old female suffering from internal derangement of the left knee. The MRI report described ACL rupture due to poor visualization of the ACL substance.</cell><cell>C0075414 (Reichstein's Substance)</cell><cell>Poor understanding of sentence semantics has resulted in the word substance triggering the assignment of chemical Reichstein's Substance to the image.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Another example of chemical</cell></row><row><cell>22356</cell><cell>CECT abdomen showing the lesion</cell><cell>C0772294 (alesion)</cell><cell>drug name Alesion (antihistamine) being mistakenly matched to a</cell></row><row><cell></cell><cell></cell><cell></cell><cell>commonly used term (lesion).</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Poor trigram matching has</cell></row><row><cell></cell><cell></cell><cell></cell><cell>produced CUIs for Teration (a</cell></row><row><cell>24120</cell><cell>Severe Bilateral secretion and concentration alterations</cell><cell>C0076106 (Teration) C1306232 (Sever)</cell><cell>type of Organothiophosphate) and Sever (verb) from</cell></row><row><cell></cell><cell></cell><cell></cell><cell>alteration and severe</cell></row><row><cell></cell><cell></cell><cell></cell><cell>respectively.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,134.77,226.91,345.83,92.32"><head></head><label></label><figDesc>The image features are then passed into a recurrent network where each CUI is processed one at a time until maximum time T has passed. The unconstrained maximum number of CUIs in the training data is 72; however, we observe that we can reduce the number of time steps by 74% and retain 99% of the training data if we constrain the maximum number of CUIs to 19. Hence, to maximize efficiency, we exclude samples with CUIs greater than 19. We add "START" and "END" tokens respectfully to the beginning and end of each label set; NULL tokens are added to sets with fewer than 19 CUIs to attain a fixed length time sequence T = 21.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,134.77,242.56,345.82,68.41"><head></head><label></label><figDesc>Table. 4. We submitted Model A for evaluation on test data as it achieved the highest F1 score, as shown in Table. 4. We decided to submit Model D as well as it achieved a competitive result with a surprisingly small average concepts per sample; we were curious to see the performance of a more conservative model on the test distribution. Model A and Model D achieved F1 scores of 0.1749349 (rank 22) and 0.1640647 (rank 27) on the test dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,134.77,332.19,345.83,138.08"><head>Table 4 .</head><label>4</label><figDesc>This table compares the quantitative performance of each of our models on the validation dataset. F1 refers to the average F1 score on the validation dataset. MIN, MAX, and MEAN respectively refer to the minimum, maximum and mean number of concepts generated for each example in the validation set. The highest F1 score is highlighted in bold font.</figDesc><table coords="9,249.38,396.34,116.60,73.92"><row><cell cols="3">Model F1 MIN MAX MEAN</cell></row><row><cell>A 0.16 1</cell><cell>16</cell><cell>4.3</cell></row><row><cell>B 0.15 1</cell><cell>11</cell><cell>4.2</cell></row><row><cell>C 0.13 0</cell><cell>14</cell><cell>3.7</cell></row><row><cell>D 0.15 0</cell><cell>14</cell><cell>0.4</cell></row><row><cell>E 0.12 1</cell><cell>9</cell><cell>2.5</cell></row><row><cell>F 0.12 1</cell><cell>9</cell><cell>2.9</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>This project has been funded by <rs type="funder">Sullivan Nicolaides Pathology</rs> and the <rs type="funder">Australian Research Council (ARC)</rs> Linkage Project [Grant number <rs type="grantNumber">LP160101797</rs>].</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_nP7YZW9">
					<idno type="grant-number">LP160101797</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.61,436.84,337.98,7.77;10,150.95,447.80,329.64,7.77;10,150.95,458.76,104.59,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,213.09,436.84,267.50,7.77;10,150.95,447.80,63.90,7.77">Effective mapping of biomedical text to the umls metathesaurus: the metamap program</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,233.86,447.80,136.68,7.77">Proceedings of the AMIA Symposium</title>
		<meeting>the AMIA Symposium</meeting>
		<imprint>
			<publisher>American Medical Informatics Association</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,469.87,337.98,7.77;10,150.95,480.83,228.75,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,212.63,469.87,267.96,7.77;10,150.95,480.83,22.77,7.77">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,179.76,480.83,80.66,7.77">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">suppl 1</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,491.94,337.98,7.77;10,150.95,502.90,165.22,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,321.10,491.94,79.63,7.77">The specialist lexicon</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">T</forename><surname>Mccray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="10,407.98,491.94,72.61,7.77;10,150.95,502.90,100.97,7.77">National Library of Medicine Technical Reports</title>
		<imprint>
			<biblScope unit="page" from="18" to="21" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,514.01,209.78,7.77" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<title level="m" coord="10,215.00,514.01,19.11,7.77">Keras</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,525.12,337.98,7.77;10,150.95,536.08,329.64,7.77;10,150.95,547.04,329.64,7.77;10,150.95,558.00,57.53,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,293.19,536.08,187.41,7.77;10,150.95,547.04,88.42,7.77">Preparing a collection of radiology examinations for distribution and retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Rosenman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Shooshan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,245.43,547.04,210.60,7.77">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="304" to="310" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,569.11,337.98,7.77;10,150.95,580.07,213.66,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,368.21,569.11,112.38,7.77;10,150.95,580.07,77.24,7.77">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,246.32,580.07,25.66,7.77">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,591.18,337.98,7.77;10,150.95,602.14,279.70,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,257.14,591.18,223.45,7.77;10,150.95,602.14,121.47,7.77">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,278.10,602.14,61.41,7.77">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,613.25,337.98,7.77;10,150.95,624.21,329.64,7.77;10,150.95,635.17,329.64,7.77;10,150.95,646.13,329.64,7.77;10,150.95,657.08,329.64,7.77;11,150.95,119.96,329.64,7.77;11,150.95,130.92,329.64,7.77;11,150.95,141.88,329.64,7.77;11,150.95,152.84,160.08,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,367.99,657.08,112.61,7.77;11,150.95,119.96,191.39,7.77">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,361.11,119.96,119.49,7.77;11,150.95,130.92,329.64,7.77;11,150.95,141.88,108.14,7.77">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="11,287.60,141.88,153.79,7.77">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="11,142.61,163.80,337.98,7.77;11,150.95,174.76,86.94,7.77" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,241.93,163.80,175.30,7.77">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.24,185.71,338.35,7.77;11,150.95,196.67,329.64,7.77;11,150.95,207.63,266.41,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,410.09,185.71,70.50,7.77;11,150.95,196.67,222.32,7.77">Coral8: Concurrent object regression for area localization in medical image panels</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maksoud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wiliem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,391.24,196.67,89.35,7.77;11,150.95,207.63,240.26,7.77">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,216.51,338.36,9.85;11,150.95,229.55,329.64,7.77;11,150.95,240.51,97.12,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,389.55,218.59,91.05,7.77;11,150.95,229.55,78.69,7.77">Recurrent neural network based language model</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Černockỳ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,249.06,229.55,231.53,7.77;11,150.95,240.51,70.98,7.77">Eleventh annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,251.47,338.35,7.77;11,150.95,262.43,329.64,7.77;11,150.95,273.39,227.87,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,305.56,251.47,175.03,7.77;11,150.95,262.43,167.87,7.77">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,337.80,262.43,142.79,7.77;11,150.95,273.39,142.10,7.77">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,284.34,338.35,7.77;11,150.95,295.30,329.64,7.77;11,150.95,306.26,271.74,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,394.48,284.34,86.10,7.77;11,150.95,295.30,144.38,7.77">Overview of the Image-CLEFmed 2019 concept prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,314.38,295.30,166.22,7.77;11,150.95,306.26,100.22,7.77">CLEF2019 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,317.22,338.35,7.77;11,150.95,328.18,329.64,7.77;11,150.95,339.14,329.64,7.77;11,150.95,350.10,74.96,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,376.10,317.22,104.49,7.77;11,150.95,328.18,125.70,7.77">Radiology objects in context (roco): A multimodal image dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,293.91,328.18,186.68,7.77;11,150.95,339.14,291.34,7.77">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,361.06,338.35,7.77;11,150.95,372.02,165.41,7.77" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m" coord="11,258.48,361.06,222.11,7.77;11,150.95,372.02,19.86,7.77">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.24,382.97,338.35,7.77;11,150.95,393.93,161.90,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,251.83,382.97,228.76,7.77;11,150.95,393.93,34.51,7.77">Quickumls: a fast, unsupervised approach for medical concept extraction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,203.36,393.93,60.99,7.77">MedIR workshop</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,404.89,338.35,7.77;11,150.95,415.85,187.93,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,245.10,404.89,235.49,7.77;11,150.95,415.85,31.13,7.77">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,188.22,415.85,71.47,7.77">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,426.81,338.35,7.77;11,150.95,437.77,329.64,7.77;11,150.95,448.73,66.49,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,150.95,437.77,273.15,7.77">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,442.30,437.77,24.66,7.77">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
