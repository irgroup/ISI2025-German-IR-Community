<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,201.27,115.90,212.82,12.68;1,185.37,133.83,244.61,12.68;1,145.94,151.77,323.46,12.68">Lifelog Moment Retrieval with Advanced Semantic Extraction and Flexible Moment Visualization for Exploration</title>
				<funder>
					<orgName type="full">Software Engineering Laboratory, University of Science, Vietnam National University -Ho Chi Minh City</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,219.24,189.59,78.98,8.80"><forename type="first">Nguyen-Khang</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">University of Science</orgName>
								<orgName type="institution">VNU-HCM</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.78,189.59,80.10,8.80"><forename type="first">Dieu-Hien</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName coords="1,214.23,201.55,80.79,8.80"><forename type="first">Vinh-Tiep</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">University of Science</orgName>
								<orgName type="institution">VNU-HCM</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">University of Information Technology</orgName>
								<orgName type="institution">VNU-HCM</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.95,201.55,71.71,8.80"><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">University of Science</orgName>
								<orgName type="institution">VNU-HCM</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,201.27,115.90,212.82,12.68;1,185.37,133.83,244.61,12.68;1,145.94,151.77,323.46,12.68">Lifelog Moment Retrieval with Advanced Semantic Extraction and Flexible Moment Visualization for Exploration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D58354171232C4606389D6CC3972F67A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lifelog Retrieval</term>
					<term>Object Color Detection</term>
					<term>User Interaction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the rise of technology over the last decade, the number of smart wearable devices, low-cost sensors as well as inexpensive data storage technologies have been increasing rapidly, making it easy for anyone to use them to capture the details of their everyday's life and create an enormous dataset which can include photos, videos, biometric and GPS information. These kinds of activities can be referred to as Lifelogging, which is becoming a popular trend in the research community. One of the most important tasks of processing lifelog data is to retrieve the moments of interest from the lifelog, which is also referred to as lifelog semantic access task. Our proposed system provides a novel way to extract semantic from the lifelog data using scene classification and object detection, including object color detection. In addition, we also design a user interface which can efficiently visualize moments in the lifelog. Using our solution, we achieve the first rank in Lifelog Moment Retrieval task of ImageCLEF Lifelog 2019 with F1@10 score of 0.61.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In a formal definition, a lifelog is the phenomenon whereby people record their own daily life in a varying amount of detail, for a variety of purposes. The record contains a more or less comprehensive dataset of a human's life and activities.</p><p>The concept of "Lifelog" has been around for a long time. A diarist named Robert Shields, who manually recorded 25 years of his life from 1972 to 1997 at 5-minute intervals in his diary, is probably the first ever lifelogger in the world.</p><p>He spent hours a day in the office recording data such as his body temperature, blood pressure, medications, .etc. In addition, he only slept for two hours at a time so he could be able to write about his dreams. His work consists of 37-million word diary which is also considered to be the longest ever written.</p><p>From an information science perspective, lifelogging provides us with huge archives of personal data. However, these are usually raw data which have no annotations, no semantic descriptions or even contain errors. Therefore, to make this data usable, it is a challenge to build a system that can understand the data semantically.</p><p>Nowadays, due to the dramatic increase of smart wearable devices. It is easier for anyone to use these devices to capture the details of their everyday's life and create an enormous dataset which can include photos, videos, biometric and GPS information. This type of dataset is commonly referred to as lifelog.</p><p>Lifelog analysis has a lot of benefits in research and applications, it can help to give a better intuition about human activities on a regular basis as well as improving their own wellness. Particularly, lifelogging analysis that aims <ref type="bibr" coords="2,457.48,301.09,10.51,8.80" target="#b4">[5]</ref> to retrieve the moments of interest from lifelog data can help people to revive memories <ref type="bibr" coords="2,179.91,325.00,14.61,8.80" target="#b11">[12]</ref>, verify events, find entities, or analyze people's social traits <ref type="bibr" coords="2,467.31,325.00,9.96,8.80" target="#b2">[3]</ref>. There are many other challenging tasks in lifelog analysis that also have great potential in research and application. In this paper, we will focus on solving the Lifelog Semantic Access Task, whose mission is to retrieve moments of interest from the lifelog data.</p><p>We view this problem as two separate subproblems. The first subproblem aims to preprocess the data and annotate each data with appropriate metadata. We propose a way to extract basic and advanced concepts from the lifelog dataset. The second subproblem aims to design and provide a friendly user interface that enables novice users to interact with the queries and visualize the moment in a way that they can easily solve the search topic.</p><p>Comparing to our previous lifelog retrieval systems <ref type="bibr" coords="2,372.84,458.39,14.61,8.80" target="#b9">[10]</ref>, the improvement on this system focus on more advanced concepts and more efficient user interaction. With this system, we efficiently solved the 10 test topics of ImageCLEF 2019 Lifelog (LMRT) and achieve the best result comparing to other runs.</p><p>In Section 2, we discuss some recent challenges and achievements in Lifelog research. We propose our methods in Section 3 where we focus on the offline data processing and user interaction. In Section 4, we give an example of how our system assists a novice user to retrieve the moments of interest from the lifelog. The conclusion and a discussion of what can be done in the future work are presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Comparing the performance of information access and retrieval systems that operate on lifelog data is among the interesting topics for researchers worldwide recently. One of the first significant conferences that focus on known-item search and activity understanding applied over lifelog data was NTCIR-12 which happened in 2016 <ref type="bibr" coords="3,201.11,130.89,9.96,8.80" target="#b3">[4]</ref>. The lifelog data used in this conference is collected from 3 different volunteers wearing cameras to record visual daily life data for a month. Furthermore, the conference also provides a concept detector to support the participating teams. Many different analytic approaches and applications are discussed in the conference due to the enormous amount of data in the lifelog.</p><p>In ImageCLEFlifelog 2017, more information is added to the lifelog dataset, some are semantic locations such as coffee shops and restaurants, others are physical activities such as walking, cycling and running. The tasks on this dataset include a retrieval task which includes the evaluation of result image correctness, and a task in which the dataset is summarized by a specific requirement.</p><p>Lifelog is one of the four main tasks in ImageCLEF 2019 <ref type="bibr" coords="3,393.62,251.12,9.96,8.80" target="#b6">[7]</ref>. The main goal is to promote the evaluation of technologies for annotation, indexing and retrieval of visual data. The task aims to provide information access to large collections of images.</p><p>The annual Lifelog Search Challenge (LSC) takes more effort on the evaluation of interactive lifelog retrieval systems. In the LSC 2018 <ref type="bibr" coords="3,418.90,311.23,9.96,8.80" target="#b0">[1]</ref>, associated with the dataset was 6 testing and 16 evaluation multimodal topics representing challenging real-world information needs.</p><p>In the previous version of our system <ref type="bibr" coords="3,320.71,347.44,14.61,8.80" target="#b9">[10]</ref>, we proposed a retrieval system that is able to detect basic concepts. In our other work <ref type="bibr" coords="3,383.61,359.39,9.96,8.80" target="#b8">[9]</ref>, we also conducted experiments on our system where a novice user uses the system to perform retrieval tasks.</p><p>Taking advantage of our previous works, we build a more efficient system that aims to detect more advanced concepts and improve the performance by applying modern techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Retrieval System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Retrieval System Overview</head><p>To solve the retrieval task, we first evaluate the dataset to figure what kind of places and concepts we should focus on. After the evaluation, we find that the accurate and relevant information about the place and objects appearing on the lifelog data combined with a user interface that allows users to traverse back and forth from a specific moment will be sufficient to retrieve the moments of interest. Therefore, we break down the problem into the offline data processing problem and user interaction problem. We then process to solve these problems separately. The platform has two main processes (Figure <ref type="figure" coords="3,385.12,583.99,3.87,8.80">1</ref>):</p><p>1. The offline data processing aims to annotate each moment in the dataset with metadata that the user of the system can later use to retrieve the moment of interest. This process employs machine learning methods and vision-based algorithm to extract information about the visual scene, the concepts appearing on the image, etc., and use this information to annotate the image.</p><p>Fig. <ref type="figure" coords="4,153.44,368.83,3.87,8.80">1</ref>: Two main steps in our system: Offline Data Processing and Online Retrieval Process 2. The online retrieval process aims to provide a friendly user interface for the retrieval step which involves human interaction.</p><p>Our system architecture separate the back-end and the front-end, this will give us benefits in development, deployment and future improvement. For the back-end, we develop a RESTFul web service that performs retrieval tasks and provides data for its clients. The architecture of the back-end is based on the traditional multi-tier architecture. We employ three layer: the handler layer that handles requests and responses, the service layer that performs all the logic of our application, and data access layer which is in charge of reading data from and writing data to files and databases. The overview of the system architecture is illustrated in Figure <ref type="figure" coords="4,235.50,550.81,4.98,8.80">2</ref> 3.2 Retrieval System's Components Main components of our system are illustrated in Figure <ref type="figure" coords="4,410.37,608.24,3.87,8.80">3</ref>. In the offline data processing step, we have two main goals. First, we aim to annotate each image in the lifelog with the metadata that consists of the information about scene's category, scene's attributes, and appearing concepts. Second, we provide a method to index the dataset based on these metadata for fast retrieval. Fig. <ref type="figure" coords="5,212.99,434.73,3.87,8.80">2</ref>: Overview of our retrieval system architecture For object detection, we employ a basic object detector trained on MS COCO 2014 dataset <ref type="bibr" coords="5,191.77,487.53,15.49,8.80" target="#b10">[11]</ref> and our habit-based detectors that take advantage of the Open Image V4 dataset <ref type="bibr" coords="5,214.72,499.48,9.96,8.80" target="#b7">[8]</ref>. Furthermore, a classifying model is trained to predict the scene's category and scene's attributes. In addition, we develop an object color detector to further improve our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Scene classification</head><p>To classify scenes in the lifelog dataset, we train the Residual Network (ResNet) on Places365-Standard dataset <ref type="bibr" coords="5,314.91,608.24,14.61,8.80" target="#b12">[13]</ref>. In this paper, ResNet152 has the highest top-5 accuracy when tested on the validation set and the test set compare to other three popular CNN architectures, AlexNet, GoogLeNet, and VGG 16 convolutional-layer CNN. With this model, we can annotate each image in the lifelog dataset with about 102 scene attributes and 365 scene categories. Fig. <ref type="figure" coords="6,156.83,342.76,3.87,8.80">3</ref>: Main components of the proposed system (Lifelog retrieval system V2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">COCO object detector</head><p>To detect the concepts appearing on the images, we use the Faster R-CNN with a backbone of the 101-layer residual net (ResNet-101) and train our model on the MS COCO 2014 dataset (1) <ref type="bibr" coords="6,288.98,426.47,14.61,8.80" target="#b10">[11]</ref>. With this concept detector, we are able to detect 80 categories of concept. Furthermore, we group these categories into 11 super-categories as in the MS COCO <ref type="bibr" coords="6,313.16,450.38,14.61,8.80" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Habit-based concept detector</head><p>We find that the lifeloggers, as well as any person, have several habits in their lives, they usually eat certain kinds of food, enjoy certain kinds of drink, and participate in certain activities. Because of this observation and our evaluation of the lifelog dataset, we prepare a number of detectors which aim to detect the concepts that appear many times in the daily life of the lifelogger in particular, and the people in his/her country in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Object color detection</head><p>Some search topic contains specific information about color of the object. Being able to detect an object with specific color in the lifelog dataset will improve our system performance a lot. Because of this reason, we develop our object color detector by applying Mask R-CNN <ref type="bibr" coords="6,345.29,656.06,10.51,8.80" target="#b5">[6]</ref> and K-means clustering.</p><p>To detect the color of the object, we propose a method to find the object's dominant colors through clustering. We use "K-means Clustering" as the clustering technique. After applying the mask of the object on the image, we cluster pixels of the image by three channels: Red, Green, Blue. After the clustering process, the color at each cluster center will represent the color of that cluster, these colors will determine the dominant colors of the object.</p><p>Before applying K-means clustering method, we first standardize the variables (the values of red, green, blue channels) by dividing each data point by the standard deviation. This is for ensuring that the variations in each variable will affect the clusters equally.</p><p>We then perform clustering and experiment with several numbers of clusters to find the relevant number of clusters. We decide to have three clusters, representing three dominant colors of the object. Finally, we annotate the image in the dataset with this object colors.</p><p>Figure <ref type="figure" coords="7,180.03,286.31,4.98,8.80">4</ref> demonstrates object detection and segmentation on the lifelog dataset. Taking the person wearing a black cloth and a blue jacket in Figure <ref type="figure" coords="7,431.22,298.26,4.98,8.80">4</ref> for example, after applying the mask to the image, Figure <ref type="figure" coords="7,355.73,310.22,4.98,8.80">5</ref> and Figure <ref type="figure" coords="7,415.72,310.22,4.98,8.80">6</ref> illustrate the dominant colors of this object with 7 and 3 clusters respectively Fig. <ref type="figure" coords="7,153.44,527.45,3.87,8.80">4</ref>: An example of detection and segmentation of objects in the lifelog dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">User interaction</head><p>A friendly user interface is one of the most important aspects of our system. This user interface design must meet these two goals:</p><p>1. The novice user can easily query the images from the dataset with the desired attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The novice user can traverse back and forth from a specific result moment,</head><p>and choose what images are the correct ones. We design a friendly user interface that meets these goals and develop a web application applying this design. In our web application, we provide the user with 3 main views, the user can easily switch between these views:Search mode, Result mode, and Semi mode.</p><p>Moreover, a view for reviewing the answers is also provided. In this view, the user can view all of his/her chosen images for a specific topic, remove incorrect ones and change the images' order.</p><p>Our retrieval system supports the user to automatically fill in input fields based on a description of the event in natural language. To build this feature, we first split the description into words. For each word, we find its position in the sentence (Noun, Verb, Adjective, Adverb, etc.), we only collect words which are nouns and verbs. After this step, we have a list of collected words, we then enhance this list by adding words that are in the thesaurus (synonyms and related concepts) of these words. The complete process is shown in Figure <ref type="figure" coords="9,461.98,274.35,3.87,8.80">7</ref>. A demonstration of this feature is shown in Figure <ref type="figure" coords="9,348.25,286.31,3.87,8.80">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 7: A complete process of extracting keywords from the event description to automatically fill in input fields</head><p>We employ the pagination strategy to visualize the resulting events to the user. This feature allows the user to visualize the result in pages, the user can navigate through each page and see the corresponding results. Also, the user can choose the number of results per page. The pagination process is conducted on the server side. Because the cost of resources for making a retrieval process is expensive, we do not execute this process whenever the user navigates through pages. Instead of that, the results of the retrieval are cached on the memory of the server. We use key-value caching, we use the information of the retrieval Fig. <ref type="figure" coords="10,153.44,465.07,3.87,8.80">8</ref>: A demonstration of the system's Automatic Input feature, keywords extracting from the search topic are automatically filled in criteria (represented in a configured format) as keys and the retrieval results as values. Whenever the user executes a retrieval, the results are computed and cached. After that, when the user navigates through each page, we only perform the pagination on the cached data and return the results to the user. Finally, when the user makes the next retrieval with different criteria, we latest set of results on cache is cleared. Our cache keeps up to three sets of retrieval results. The complete flow is illustrated in Figure <ref type="figure" coords="10,318.78,656.06,3.87,8.80" target="#fig_1">9</ref>. In this section, we present how our system performs in practice where it is used to retrieve the moments in the lifelog which corresponds to a given search topic. The system automatically generates some of the input fields for the user and allows the user to modify them to get the correct result. The system provides a flexible way and multiple tools for the user to do the task, but the user also needs to picture the moments and decide what needs to be in the inputs in order to get a more precise result. Although our system's user interface is user-friendly and self-explained itself, many tooltips and pop up instructions are supported to guide the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task and Dataset</head><p>The detail of the dataset gathering process as well as the task's desciption is described in <ref type="bibr" coords="12,191.36,153.52,9.96,8.80" target="#b1">[2]</ref>. The task is split into two related subtasks using a completely new rich multimodal dataset which consists of 29 days of data from one lifelogger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Search topic</head><p>Find the moment when user 1 was having breakfast at home. Note: User 1 was having breakfast at home and the breakfast time must be from 5:00 AM until 9:00 AM.</p><p>Time is one of the most important aspects of this topic. The correct moments need to be in the range of time from 5:00 AM to 9:00 AM. Our system provides an easy way for the user to retrieve moments in an exact range of time in the day. Moreover, our food detector performs efficiently in the lifelog data and is able to detect moments when the lifelogger was having a meal. By inputting 5:00 AM, 9:00 AM as the time range, food as the concept appearing, and "Home" as the location's name, the user can retrieve every moment when the lifelogger was having breakfast at home in the morning. Find the moment when u1 was looking at items in a toyshop. Note: To be considered relevant, u1 must be clearly in a toyshop. Various toys are being examined, such as electronic trains, model kits, and board games. Being in an electronics store, or a supermarket, are not considered to be relevant.</p><p>The only relevant scene's category is toyshop, which is one of the categories in Place365 Standard dataset. Our system was able to retrieve all the moments when the lifelogger was shopping in a toyshop. There were two moments retrieved for this topic. Using image sequence view, we chose every relevant image which our system retrieved. We then finalized our result (Figure <ref type="figure" coords="13,389.78,166.75,8.30,8.80">11</ref>). Fig. <ref type="figure" coords="13,153.44,389.17,8.48,8.80">11</ref>: Reviewing answer for topic: "Find the moment when u1 was looking at items in a toyshop."</p><p>Through the mentioned example queries, we aim to demonstrate the possible strategies for users to use our retrieval system in different scenarios. Depending on specific needs to query for a certain moment, a user can begin to retrieve related scenes based on scene's category, scene's attributes, or objects existing in images. Then the user can expand the sequence of images from a single image to further evaluate the context of the moment.</p><p>Find the moment when u1 was having coffee with two person. Note: Find the moment when u1 was having coffee with two person. One was wearing blue shirt and the other one was wearing white cloth. Gender is not relevant.</p><p>In this search topic, there are some advanced concepts such as blue shirt and white cloth, which indicate not only the objects but also their colors. Our system supports the user to search in the lifelog dataset for objects with basic colors. By search for concept "person" and the color "blue", the user of our system can efficiently retrieve the relevant moments (Figure <ref type="figure" coords="13,348.10,602.24,8.30,8.80">12</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Result in ImageCLEF Lifelog 2019</head><p>We participate in the ImageCLEF Lifelog 2019 -Lifelog Moment Retrieval task with the team name "HCMUS". In the ImageCLEF Lifelog 2019 <ref type="bibr" coords="13,448.59,656.06,9.96,8.80" target="#b1">[2]</ref>, F1-Fig. <ref type="figure" coords="14,153.44,304.93,8.48,8.80">12</ref>: Reviewing answer for topic: "Find the moment when u1 was having coffee with two person." measure at 10 is used to evaluate the performance of participating systems. Figure <ref type="figure" coords="14,165.90,363.25,9.96,8.80">13</ref> demonstrates the result of our team (HCMUS) in comparing with the results of other teams in Lifelog Moment Retrieval task of ImageCLEF 2019 Lifelog. According to the result, our system has the highest F1-measure at 10. The detail of our system performance is illustrated in Table <ref type="table" coords="14,412.47,399.12,3.87,8.80" target="#tab_0">1</ref>, with Cluster Recall at 10 (CR@10), Precision at 10 (P@10), and F1-measure at 10 (F1@10) Query P@10 CR@10 F1@10 Although our system can achieve very promising results comparing to other systems, we still need to further improve our system to better represent unfamiliar concepts and integrate various interaction modalities to assist users in exploring lifelog data. Fig. <ref type="figure" coords="15,153.44,514.04,8.48,8.80">13</ref>: ImageCLEF 2019 Lifelog -LMRT Leaderboard. The result is evaluated by F1-measure at 10. Our system has the highest result comparing to other runs Our system supports the user to retrieve the moments of interest from the lifelog by 2 main steps: offline processing of the data (including annotating each image with metadata and structure the data for better performance and scalability), and optimize the user interaction (including a user-friendly design web application which supports flexible ways of searching and selecting results).</p><p>However, there are still some aspects that our system needs to improve. The user still needs to picture the moments to decide what scene category the images should be, and what concepts should be in the images.</p><p>In the future works, we will look into the aspect of natural language semantics to give our system the ability to understand the topic search and suggest more relevant inputs for the user.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,164.33,337.67,286.69,8.80;8,134.77,153.11,345.82,173.09"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Fig. 5: Dominant colors extracted from the object using 7 clusters</figDesc><graphic coords="8,134.77,153.11,345.82,173.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="11,134.77,429.81,345.83,8.80;11,134.77,441.77,30.18,8.80;11,134.77,115.83,345.84,302.51"><head>Fig. 9 :</head><label>9</label><figDesc>Fig.9: A caching method to improve the performance of the system's pagination feature</figDesc><graphic coords="11,134.77,115.83,345.84,302.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="12,134.77,570.33,345.83,8.80;12,134.77,582.29,86.40,8.80;12,134.77,373.90,345.80,184.97"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Result moments for topic: "Find the moment when user 1 was having breakfast at home."</figDesc><graphic coords="12,134.77,373.90,345.80,184.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,134.77,115.83,345.84,241.53"><head></head><label></label><figDesc></figDesc><graphic coords="4,134.77,115.83,345.84,241.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,134.77,115.83,345.86,307.43"><head></head><label></label><figDesc></figDesc><graphic coords="5,134.77,115.83,345.86,307.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,134.77,115.84,345.83,215.46"><head></head><label></label><figDesc></figDesc><graphic coords="6,134.77,115.84,345.83,215.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,134.77,352.59,345.83,163.40"><head></head><label></label><figDesc></figDesc><graphic coords="7,134.77,352.59,345.83,163.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,134.77,316.74,345.84,196.97"><head></head><label></label><figDesc></figDesc><graphic coords="9,134.77,316.74,345.84,196.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="13,134.77,196.80,345.82,180.91"><head></head><label></label><figDesc></figDesc><graphic coords="13,134.77,196.80,345.82,180.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="14,134.77,115.84,345.80,177.62"><head></head><label></label><figDesc></figDesc><graphic coords="14,134.77,115.84,345.80,177.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="15,134.77,242.31,345.82,260.27"><head></head><label></label><figDesc></figDesc><graphic coords="15,134.77,242.31,345.82,260.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="14,134.77,455.45,345.82,128.76"><head>Table 1 :</head><label>1</label><figDesc>Detail of our system performance in ImageCLEF Lifelog 2019 test topics</figDesc><table coords="14,145.98,455.45,325.99,106.55"><row><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>2</cell><cell>0.6</cell><cell>0.1</cell><cell>0.16</cell></row><row><cell>3</cell><cell>0.5</cell><cell>0.28</cell><cell>0.36</cell></row><row><cell>4</cell><cell>1</cell><cell>0.75</cell><cell>0.86</cell></row><row><cell>5</cell><cell>0.8</cell><cell>0.56</cell><cell>0.66</cell></row><row><cell>6</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>7</cell><cell>0.7</cell><cell>1</cell><cell>0.82</cell></row><row><cell>8</cell><cell>0.5</cell><cell>0.33</cell><cell>0.4</cell></row><row><cell>9</cell><cell>0.9</cell><cell>0.57</cell><cell>0.7</cell></row><row><cell>10</cell><cell>1</cell><cell>1</cell><cell>1</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="institution">AIOZ Pte Ltd</rs> for supporting our research team. This research is partially supported by the research funding for the <rs type="funder">Software Engineering Laboratory, University of Science, Vietnam National University -Ho Chi Minh City</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="16,142.95,405.77,337.64,7.92;16,151.52,416.73,140.00,7.92" xml:id="b0">
	<monogr>
		<title level="m" coord="16,151.52,405.77,324.88,7.92">LSC &apos;18: Proceedings of the 2018 ACM Workshop on The Lifelog Search Challenge</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.95,427.50,337.64,7.92;16,151.52,438.46,329.07,7.92;16,151.52,449.42,329.07,7.92;16,151.52,460.38,329.07,7.92;16,151.52,471.34,145.93,7.92" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,275.51,438.46,205.08,7.92;16,151.52,449.42,145.14,7.92">Overview of ImageCLEFlifelog 2019: Solve my life puzzle and Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="CEUR-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="16,316.55,449.42,103.35,7.92">CLEF2019 Working Notes</title>
		<title level="s" coord="16,426.63,449.42,53.96,7.92;16,151.52,460.38,69.01,7.92">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.95,482.12,337.64,7.92;16,151.52,493.08,329.07,7.92;16,151.52,504.04,329.08,7.92;16,151.52,515.00,283.24,7.92" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,290.57,482.12,190.03,7.92;16,151.52,493.08,166.45,7.92">Social relation trait discovery from visual lifelog data with facial multi-attribute framework</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">D</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,337.90,493.08,142.69,7.92;16,151.52,504.04,256.38,7.92">Proceedings of the 7th International Conference on Pattern Recognition Applications and Methods</title>
		<meeting>the 7th International Conference on Pattern Recognition Applications and Methods<address><addrLine>ICPRAM; Funchal, Madeira -Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-01-16">2018. January 16-18, 2018. 2018</date>
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.95,525.78,337.64,7.92;16,151.52,536.74,72.23,7.92" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="16,398.35,525.78,82.25,7.92;16,151.52,536.74,43.56,7.92">Overview of ntcir-12 lifelog task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Albatal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.95,547.51,337.63,7.92;16,151.52,558.46,329.07,7.93;16,151.52,569.43,154.60,7.92" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,326.37,547.51,119.11,7.92">Lifelogging: Personal big data</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Doherty</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000033</idno>
		<ptr target="http://dx.doi.org/10.1561/1500000033" />
	</analytic>
	<monogr>
		<title level="j" coord="16,452.83,547.51,27.76,7.92;16,151.52,558.47,71.37,7.92">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="125" />
			<date type="published" when="2014-06">Jun 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.95,580.21,337.64,7.92;16,151.52,591.17,329.07,7.92;16,151.52,602.13,110.05,7.92" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,347.84,580.21,52.18,7.92">Mask R-CNN</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,422.96,580.21,57.64,7.92;16,151.52,591.17,205.66,7.92">IEEE International Conference on Computer Vision, ICCV 2017</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">October 22-29, 2017. 2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.95,612.91,337.63,7.92;16,151.52,623.87,329.07,7.92;16,151.52,634.83,329.07,7.92;16,151.52,645.79,329.07,7.92;16,151.52,656.74,329.07,7.92;17,151.52,119.62,329.07,7.92;17,151.52,130.58,329.07,7.92;17,151.52,141.54,329.08,7.92;17,151.52,152.50,329.07,7.92;17,151.52,163.46,216.23,7.92" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,268.31,119.62,212.28,7.92;17,151.52,130.58,124.23,7.92">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,297.68,130.58,182.91,7.92;17,151.52,141.54,329.08,7.92;17,151.52,152.50,122.46,7.92">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="17,305.56,152.50,171.06,7.92">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="17,142.95,174.42,337.64,7.92;17,151.52,185.37,329.07,7.92;17,151.52,196.33,329.07,7.92;17,151.52,207.29,329.08,7.92;17,151.52,218.25,329.08,7.92;17,151.52,229.21,271.52,7.92" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://storage.googleapis.com/openimages/web/index.html" />
		<title level="m" coord="17,313.64,207.29,166.96,7.92;17,151.52,218.25,223.10,7.92">Openimages: A public dataset for largescale multi-label and multi-class image classification</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.95,240.17,337.64,7.92;17,151.52,251.13,329.07,7.92;17,151.52,262.09,329.07,7.92;17,151.52,273.05,224.63,7.92" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="17,359.51,251.13,121.08,7.92;17,151.52,262.09,15.40,7.92">Hcmus at the ntcir-14 lifelog-3 task</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">D</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">A</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">K</forename><surname>Vo-Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,188.37,262.09,292.22,7.92;17,151.52,273.05,87.71,7.92">14th NTCIR Conference on Evaluation of Information Access Technologies (NTCIR-14 2019)</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">June 10-13 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,284.00,337.98,7.92;17,151.52,294.96,329.07,7.92;17,151.52,305.92,329.08,7.92;17,151.52,316.88,188.74,7.92" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="17,374.54,294.96,106.05,7.92;17,151.52,305.92,230.00,7.92">Smart lifelog retrieval system with habit-based concepts and moment visualization</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">D</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">A</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">K</forename><surname>Vo-Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,402.69,305.92,77.90,7.92;17,151.52,316.88,16.79,7.92">LSC 2019 @ ICMR 2019</title>
		<meeting><address><addrLine>Ottawa ON, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">June 10 -13 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,327.84,337.97,7.92;17,151.52,338.80,329.07,7.92;17,151.52,349.76,329.07,7.92;17,151.52,360.72,193.21,7.92" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,209.68,338.80,177.28,7.92">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,298.43,349.76,124.81,7.92">Computer Vision -ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,371.68,337.98,7.92;17,151.52,382.63,329.07,7.92;17,151.52,393.59,329.07,7.92;17,151.52,404.55,184.27,7.92" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="17,351.86,371.68,128.74,7.92;17,151.52,382.63,234.41,7.92">Nowandthen: A social networkbased photo recommendation tool supporting reminiscence</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fjeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,406.67,382.63,73.92,7.92;17,151.52,393.59,272.11,7.92;17,151.52,404.55,37.62,7.92">Proceedings of the 15th International Conference on Mobile and Ubiquitous Multimedia</title>
		<meeting>the 15th International Conference on Mobile and Ubiquitous Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
	<note>MUM &apos;16</note>
</biblStruct>

<biblStruct coords="17,142.61,415.51,337.97,7.92;17,151.52,426.47,329.07,7.92;17,151.52,437.43,111.08,7.92" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="17,399.52,415.51,81.07,7.92;17,151.52,426.47,145.66,7.92">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,303.99,426.47,176.60,7.92;17,151.52,437.43,82.42,7.92">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
