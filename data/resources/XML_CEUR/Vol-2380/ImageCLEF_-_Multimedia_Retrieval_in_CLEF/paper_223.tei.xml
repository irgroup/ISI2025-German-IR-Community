<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,178.28,115.96,258.79,12.62;1,217.31,133.89,180.74,12.62;1,217.22,151.82,180.91,12.62">Overview of ImageCLEFlifelog 2019: Solve My Life Puzzle and Lifelog Moment Retrieval</title>
				<funder ref="#_8hSeZfm">
					<orgName type="full">Irish Research Council</orgName>
					<orgName type="abbreviated">IRC</orgName>
				</funder>
				<funder ref="#_K2da7S6 #_yfcd4df">
					<orgName type="full">Science Foundation Ireland</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,153.40,189.49,103.51,8.74"><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bergen</orgName>
								<address>
									<settlement>Bergen</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.45,189.49,46.86,8.74"><forename type="first">Luca</forename><surname>Piras</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Pluribus One &amp; University of Cagliari</orgName>
								<address>
									<settlement>Cagliari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.86,189.49,67.69,8.74"><forename type="first">Michael</forename><surname>Riegler</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Simula Metropolitan Center for Digital Engineering</orgName>
								<address>
									<settlement>Oslo</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,403.11,189.49,51.61,8.74"><forename type="first">Liting</forename><surname>Zhou</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,157.41,201.45,55.54,8.74"><forename type="first">Mathias</forename><surname>Lux</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">ITEC</orgName>
								<orgName type="institution">Klagenfurt University</orgName>
								<address>
									<settlement>Klagenfurt</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.50,201.45,71.73,8.74"><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">University of Science</orgName>
								<orgName type="institution">VNU-HCM</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.79,201.45,57.98,8.74"><forename type="first">Tu-Khiem</forename><surname>Le</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,374.33,201.45,57.01,8.74"><forename type="first">Van-Tu</forename><surname>Ninh</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.39,213.40,62.11,8.74"><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,178.28,115.96,258.79,12.62;1,217.31,133.89,180.74,12.62;1,217.22,151.82,180.91,12.62">Overview of ImageCLEFlifelog 2019: Solve My Life Puzzle and Lifelog Moment Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">893E2606C8B7DC306AEC8F82A7AADD89</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes ImageCLEFlifelog 2019, the third edition of the Lifelog task. In this edition, the task was composed of two subtasks (challenges): the Lifelog Moments Retrieval (LMRT) challenge that followed the same format as in the previous edition, and the Solve My Life Puzzle (Puzzle), a brand new task that focused on rearranging lifelog moments in temporal order. ImageCLEFlifelog 2019 received noticeably higher submissions than the previous editions, with ten teams participating resulting in a total number of 109 runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since 2016 with the first lifelog initiative, the NTCIR-12 -Lifelog task <ref type="bibr" coords="1,467.30,460.58,9.96,8.74" target="#b6">[7]</ref>, research in lifelogging, 'a form of pervasive computing, consisting of a unified digital record of the totality of an individual's experiences, captured multimodally through digital sensors and stored permanently as a personal multimedia archive' <ref type="bibr" coords="1,172.49,508.40,9.96,8.74" target="#b4">[5]</ref>, is getting more attention, especially within the multimedia information retrieval community. However, given the huge volume of data that a lifelog would generate, along with the complex patterns of the data, we are just at the starting point of lifelog data organisation. We need to advance the next step of development and to unlock the potential of such data. There is a need new ways of organising, annotating, indexing and interacting with lifelog data, and that is the key motivation of the Lifelog task at ImageCLEF.</p><p>The ImageCLEFlifeLog2019 task at ImageCLEF 2019 <ref type="bibr" coords="1,396.97,592.09,14.61,8.74" target="#b9">[10]</ref>, was the third edition of the task, with previous editions in 2017 <ref type="bibr" coords="1,357.79,604.05,10.52,8.74" target="#b1">[2]</ref> and 2018 <ref type="bibr" coords="1,415.18,604.05,9.96,8.74" target="#b2">[3]</ref>, which were inspired by the fundamental image annotation and retrieval tasks of ImageCLEF since 2003. This year, the task continued to follow the general evolution of Image-CLEF, by applying the advanced deep learning methods and extending the focus to multi-modal approaches instead of only working just with image retrieval.</p><p>Comparing to the previous editions, in this third edition we merged the previous two sub-tasks (challenges): Activities of Daily Living understanding (ADLT) and Lifelog Moment Retrieval (LMRT) into a single challenge (LMRT) and proposed a brand new one: Solve My Life Puzzle (Puzzle), which focused on the new ways of organising lifelog data, in particular in rearranging lifelog moments.</p><p>The details of this year two challenges will be provided in section 2. This includes also the descriptions of data and resources. For the rest of the paper, in Section 3, submissions and results are presented and discussed and in the final section 4 the paper is concluded and final remarks and future work are discussed.</p><p>2 Overview of the Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation and Objectives</head><p>An increasingly wide range of personal devices, such as smartphones, video cameras as well as wearable devices that allow capturing pictures, videos, and audio clips for every moment of our lives are becoming available. Considering the huge volume of data created, there is a need for systems that can automatically analyse the data in order to categorize, summarize and also query to retrieve the information the user may need.</p><p>Despite the increasing number of successful related workshops and panels, lifelogging has seldom been the subject of a rigorous comparative benchmarking exercise as, for example, the new lifelog evaluation task at NTCIR-13 <ref type="bibr" coords="2,440.79,437.10,10.52,8.74" target="#b7">[8]</ref> or the last editions of the ImageCLEFlifelog task <ref type="bibr" coords="2,329.12,449.06,10.20,8.74" target="#b1">[2]</ref> <ref type="bibr" coords="2,339.32,449.06,10.20,8.74" target="#b2">[3]</ref>. In this edition we aimed to bring the attention of lifelogging to a wide audience and to promote research into some of the key challenges of the coming years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenge Description</head><p>Lifelog Moment Retrieval Task (LMRT) In this task, the participants are required to retrieve a number of specific moments in a lifeloggers life. Moments are defined as semantic events, or activities that happened throughout the day. For example, a participant would have been required to find and return relevant moments for the query "Find the moment(s) when the user1 is cooking in the kitchen". In this edition, particular attention was to be paid to the diversification of the selected moments with respect to the target scenario. The ground truth for this subtask was created using a manual annotation process. Figure <ref type="figure" coords="2,252.29,620.25,4.98,8.74" target="#fig_0">1</ref> illustrates some examples of the moments when the lifelogger was having coffee with friends". In addition, listings 1 and 2 list all the queries used in the challenge. Solve my Life Puzzle Task (Puzzle) Given a set of lifelog images with associated metadata (e.g., biometrics, location, etc.), but no timestamps, the participants needed to analyse these images and rearrange them in chronological order and predict the correct day (e.g. Monday or Sunday) and part of the day (e.g. morning, afternoon, or evening). Figure <ref type="figure" coords="5,475.61,388.56,4.98,8.74" target="#fig_1">2</ref> illustrates an example of this challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset</head><p>The data was a medium-sized collection of multimodal lifelog data over 42 days by two lifeloggers. The contribution of this dataset over previously released datasets was the inclusion of additional biometric data, a manual diet log and the inclusion of conventional photos. In most cases the activities of the lifeloggers were separate and they did not meet. However on a small number of occasions the lifeloggers appeared in data of each other. The data consists of:</p><p>-Multimedia Content. Wearable camera images captured at a rate of about two images per minute and worn from breakfast to sleep. Accompanying this image data was a time-stamped record of music listening activities sourced from Last.FM<ref type="foot" coords="5,213.05,557.73,3.97,6.12" target="#foot_0">1</ref> and an archive of all conventional (active-capture) digital photos taken by the lifelogger. -Biometrics Data. Using the FitBit fitness trackers<ref type="foot" coords="5,382.56,581.14,3.97,6.12" target="#foot_1">2</ref> , the lifeloggers gathered 24 × 7 heart rate, calorie burn and steps. In addition, continuous blood glucose monitoring captured readings every 15 minutes using the Freestyle Libre wearable sensor<ref type="foot" coords="5,245.85,617.00,3.97,6.12" target="#foot_2">3</ref> . -Human Activity Data. The daily activities of the lifeloggers were captured in terms of the semantic locations visited, physical activities (e.g. walking, running, standing) from the Moves app <ref type="foot" coords="6,320.06,473.71,3.97,6.12" target="#foot_3">4</ref> , along with a time-stamped diet-log of all food and drink consumed. -Enhancements to the Data. The wearable camera images were annotated with the outputs of a visual concept detector, which provided three types of outputs (Attributes, Categories and Concepts). Two visual concepts which include attributes and categories of the place in the image are extracted using PlacesCNN <ref type="bibr" coords="6,231.93,547.12,14.61,8.74" target="#b17">[18]</ref>. The remaining one is detected object category and its bounding box extracted by using Faster R-CNN <ref type="bibr" coords="6,371.72,559.07,15.50,8.74" target="#b13">[14]</ref> trained on MSCOCO dataset <ref type="bibr" coords="6,186.62,571.03,14.61,8.74" target="#b11">[12]</ref>.</p><p>Format of the metadata. The metadata was stored in a .csv files, which was called the minute-based table. The precise structured of it is described in Table <ref type="table" coords="6,163.17,615.16,3.87,8.74" target="#tab_1">2</ref>. Additionally, extra metadata was included, such as visual categories and concepts descriptors. The format of the extra metadata could be found in Table <ref type="table" coords="6,162.16,639.08,3.87,8.74" target="#tab_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Performance Measures</head><p>LMRT For assessing performance, classic metrics were deployed. These metrics were:</p><p>-Cluster Recall at X (CR@X) -a metric that assesses how many different clusters from the ground truth are represented among the top X results; -Precision at X (P@X) -measures the number of relevant photos among the top X results; -F1-measure at X (F1@X) -the harmonic mean of the previous two.</p><p>Various cut off points were considered, e.g., X=5, 10, 20, 30, 40, 50. Official ranking metrics were the F1-measure@10, which gives equal importance to diversity (via CR@10) and relevance (via P@10).</p><p>Participants were allowed to undertake the sub-tasks in an interactive or automatic manner. For interactive submissions, a maximum of five minutes of search time was allowed per topic. In particular, methods that allowed interaction with real users (via Relevance Feedback (RF), for example), i.e., beside of the best performance, the way of interaction (like number of iterations using RF), or innovation level of the method (for example, new way to interact with real users) were encouraged.</p><p>Puzzle For the Puzzle task, we used Kendall's Tau score to measure the similarity of the temporal order between the participant's temporal arrangement and ground-truth for each query. The formula of Kendall's Tau is as follows:</p><formula xml:id="formula_0" coords="7,260.69,538.53,219.90,22.31">τ = max 0, C -D C + D<label>(1)</label></formula><p>where C and D are the number of concordant pairs and discordant pairs correspondingly between the participant's submission order and the one of groundtruth. In the original Kendall's Tau formula, the range of the formula is from [-1, 1], however, we choose to narrow the range of the score to [0, 1]. It means that if the number of opposite ranking-direction pairs are greater than the quantity of the same ranking-direction pairs, participants would get nothing in Kendall's Tau score. The accuracy of part-of-day prediction was computed simply by dividing the number of correct predictions by the total number of predictions. Finally, the primary score was computed as the average of Kendall's Tau score and accuracy of part-of-day prediction for all queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Ground Truth Format</head><p>LMRT Task. The ground truth for the LMRT task was provided in two individual txt files: one file for the cluster ground truth and one file for the relevant image ground truth.</p><p>In the cluster ground-truth file, each line corresponded to a cluster where the first value was the topic id, followed by cluster id number, followed by the cluster user tag separated by comma. Lines were separated by an end-of-line character (carriage return). An example is presented below: In the relevant ground-truth file, the first value on each line was the topic id, followed by a unique photo id, and then followed by the cluster id number (that corresponded to the values in the cluster ground-truth file) separated by comma. Each line corresponded to the ground truth of one image and lines were separated by an end-of-line character (carriage return). An example is presented below:</p><formula xml:id="formula_1" coords="10,140.99,207.81,126.63,169.40">-1, u1 20180528 1816 i00, 1 -1, u1 20180528 1816 i02, 1 -1, u1 20180528 1816 i01, 1 -1, u1 20180528 1817 i01, 1 -1, u1 20180528 1817 i00, 1 -1, u1 20180528 1818 i02, 1 -... -2, u1 20180508 1110 i00, 1 -2, u1 20180508 1110 i01, 1 -2, u1 20180508 1111 i00, 1 -2, u1 20180508 1111 i01, 1 -2, u1 20180508 1112 i01, 1 -...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Puzzle Task</head><p>The ground truth was provided in only one individual .csv file. For each line in this file, first value was the id of the query, followed by the id of image provided by the organisers, followed by the order that the image should be arranged in this query in temporal manner, followed by the part-of-day prediction. Values in each line were separated by comma. Lines were separated by and end-of-line character (carriage return). The values in ground-truth file were sorted by query id, and then image id column in ascending order. An example is shown below:</p><p>-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>Run P@10 CR@10 F1@10 Team Run P@10 CR@10 F1@10</p><p>Organiser Notes: * submissions from the organizer teams are just for reference.</p><p>† submissions submitted after the official competition.</p><p>3 Evaluation Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Participating Groups and Runs Submitted</head><p>This year the number of participants as well as the number of submissions was considerably higher with respect to 2018: we received in total 50 valid submissions (46 official and 4 additional) for LMRT, and 21 (all are official) for Puzzle, from 10 teams representing over 10 countries. The submitted runs and their results are summarised in Tables <ref type="table" coords="11,280.96,490.30,4.98,8.74" target="#tab_4">4</ref> and<ref type="table" coords="11,308.64,490.30,3.87,8.74" target="#tab_6">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>In this section we provide a short description of all submitted approaches followed by the official result of the task. The Organiser team <ref type="bibr" coords="11,243.74,560.48,15.50,8.74" target="#b12">[13]</ref> provided a baseline approach to the LMRT task with a web-based interactive search engine called LIFER 2.0, which was based on a previous system <ref type="bibr" coords="11,230.94,584.39,15.50,8.74" target="#b18">[19]</ref> used at the LSC 2018 Lifelog Search Challenge. The authors submitted two runs which were obtained by letting two novice users perform interactive moment retrieval on the search engine for all ten queries. For the Puzzle task, the team proposed an activity mining approach which utilised Bag-of-Visual-Words (BOVW) methods. For each image in a query, the authors find the most relevant images in the training data based on the L2 distance of BOVW vectors to predict the part-of-day and chronological index of the images. The REGIM-Lab <ref type="bibr" coords="12,228.98,496.64,10.52,8.74" target="#b0">[1]</ref> focused on LMRT task by improving the system from their last year participation with NoSQL which offers distributed database and framework to handle huge data. They employed the ground-truth of development set to improve the fine-tuning phase for concept extraction. In addition, CQL Query was used to exploit complicated metadata. For the query analysis, the authors trained a LSTM classifier to enhance the query with relevant concepts.</p><p>The UPB team <ref type="bibr" coords="12,219.33,570.40,10.52,8.74" target="#b5">[6]</ref> proposed the algorithm to eliminate blurry images which contain less information using a blur detection system. Following that, a metadata restriction filter, which was created manually by users, was applied to the dataset to further remove uninformative images. The remaining images was then computed a relevance score based on given metadata description for query answering.</p><p>The UAPTBioinformatics (UAPT) team <ref type="bibr" coords="12,337.63,644.16,15.50,8.74" target="#b14">[15]</ref> proposed an automatic approach for LMRT task. The images are pre-processed through an automatic selection step to eliminate images with irrelevant information to the topics (feature extraction, machine learning algorithm, k-nearest neighbors, etc.) and more visual concepts were generated using various state-of-the-art models (Google Cloud Vision API, YOLOv3). Then they extracted relevant words from topics' titles and narratives, dividing them into five categories, and finally matching them with the annotation concepts of lifelog images using a word embedding model trained on Google News dataset. Moreover, an extra step to reuse unselected images from the pre-processing steps for image similarity matching was proposed to increase the performance of their system.</p><p>The TUC MI team <ref type="bibr" coords="13,239.75,234.27,15.50,8.74" target="#b15">[16]</ref> proposed an automatic approach for LMRT task. They firstly extracted twelve types of concept from different pre-trained models to increase the annotation information for lifelog data (1191 labels in total). For image processing, two methods were introduced to transform images into vectors: image-based vectors and segment-based vectors. For query processing, they processed the query with Natural Language Processing techniques and introduced a token vector which has the same dimension as image/segment vector. Finally, they defined a formula to compare the similarity between image/segment and token vector and conducted an ablation study to find the best model that achieved the highest score in this task.</p><p>The HCMUS team <ref type="bibr" coords="13,236.18,361.51,15.50,8.74" target="#b10">[11]</ref> proposed to extract semantic concepts from images to adapt to lifelogger's habits and behaviours in daily life. They firstly identified a list of concepts manually, then trained object detectors to extract extra visual concepts automatically. Moreover, they also utilised object' region of interest to infer its color by K-Means clustering. To further understand the temporal relationship between events, they also integrated the visualization function for an event sequence in their retrieval system. For the Puzzle task, the HCMUS team <ref type="bibr" coords="13,160.72,445.20,10.52,8.74" target="#b8">[9]</ref> utilised a BOVW approach to retrieve the visually similar moments with reference to lifelog data to infer the probability of the time and order of the images. Before applying the proposed remedy, they tried to cluster the images into groups based on the provided concepts extracted from PlacesCNN, GPS location, and user activities.</p><p>The BIDAL team <ref type="bibr" coords="13,230.70,512.66,10.52,8.74" target="#b3">[4]</ref> participated in both Puzzle and LMRT tasks. For the LMRT task, they introduced their interactive system with two main stages. For stage 1, they generated many atomic clusters from the dataset based on rough concepts and utilising text annotation to create Bag-of-Words (BOW) vectors for each image. In stage 2, they generated the BOW vector for query texts and found similar images that suited the context and content of the query. They then used the output for result expansion by adding more images which were in the same cluster. Finally, an end-user chooses appropriate images for the query. For the Puzzle task, they proposed to use visual feature matching via two similarity functions between each image in both train data and test data as the initial step to filter out dissimilarity associative pairs (train-test image pairs). Finally, the remaining images are grouped based on temporal order of train data so that it forms the full set as test data to rearrange the images.</p><p>The ZJUTCVR team <ref type="bibr" coords="14,249.00,118.99,15.50,8.74" target="#b19">[20]</ref> pre-processed the images with blur/cover filters to eliminate the blurred and occluded images. Then, they proposed three approaches to handle the remaining lifelog images: the two-class approach, the eleven-class approach, and the clustering approach. For two-class approach, the authors divided query topics into directories and ran a test on each directory with a fine-tuned CNN. After that, the results are classified into two classes based on the relevance to topic description. The eleven-class approach shared the same process with the previous method, but the results are split into 11 classes, where 10 classes are corresponding to 10 query topics and the 11th class contains irrelevant images to all 10 topics. With the clustering approach, the team inherited the procedure of two-class approach with a modification after the first-round retrieval by clustering images with LVQ algorithm.</p><p>The ATS team <ref type="bibr" coords="14,216.53,270.14,15.50,8.74" target="#b16">[17]</ref> approached the LMRT task with 11 automatics runs and 1 interactive run. All automatic runs shared the same process with 4 components: Interpretation, Subset selection, Scoring and Refinement, but differed in the configuration of selecting the approach of each component. The interpretation state provided keywords and synonyms approaches which utilised WordNet and Word2Vec to diversify the results. The choice of subset is highly reliant on the configuration to use partial match or entire dataset to test. A scoring process was used to produce final ranking with three settings: label counting, topic similarity and SVM. The Refinement step offered multiple approaches: weighting, thresholding, visual clustering and temporal clustering. Finally, the team conducted ablation study to find the best configuration. The interactive run was done by letting user filter the subset of dataset and choose automatic approach of each component to complete the query.</p><p>The official results are summarised in Tables <ref type="table" coords="14,348.34,433.24,4.98,8.74" target="#tab_4">4</ref> and<ref type="table" coords="14,376.14,433.24,3.87,8.74" target="#tab_6">5</ref>. For LMRT (Table <ref type="table" coords="14,468.97,433.24,3.87,8.74" target="#tab_4">4</ref>), eight teams participated and the highest F1@10 was submitted by HCMUS <ref type="bibr" coords="14,465.09,445.20,15.50,8.74" target="#b10">[11]</ref> at their second run (RUN2) with a score of 0.61 which is considerably higher than the results obtained by novice human (the Organiser team results). For this task, the common approach was to enrich visual concepts from images through different CNNs, transform everything data to vector through BOW, Word2Vec, etc., cluster/segment sequential images, and apply different feature similarity search methods to find images with suitable context and concepts. The best score was achieved by building object color detectors to extract visual concepts which adapt to each user's daily life.</p><p>In the Puzzle task, four teams have participated and the highest score was obtained at the value of 0.55 by two runs (RUN03ME and RUN04ME), also from the HCMUS team. With the exception of the BIDAL team that utilised different functions and visual features to evaluate the similarity between each train-test pairs and proposed new grouping algorithm to decide query arrangement, most teams used BOVW to retrieve images from training data, in order to rearrange the test images based on the temporal order of the retrieved results. The highest score was achieved by applying BOVW with one million clusters, which would be a promising approach to conduct further research and improvement.</p><p>The submitted approaches in this year confirmed the trend from last year: all approaches are exploiting multi-modal instead of using only visual information. We also confirmed the importance of deep neural networks in solving these challenges: all ten participants are using directly tailored-built deep networks or exploiting the semantic concepts extracted by using deep learning methods. Unlike previous editions, we received more semi-automatic approaches, which combine human knowledge with state-of-the-art multi-modal information retrieval. Regarding the number of the signed-up teams and the submitted runs, the task keeps growing, with the highest number of registrations and participated teams. It is also a great successful that team retention rate is high with two third nonorganiser teams from last year continued to participate in this year. This again confirms how interesting and challenging lifelogging is. As next steps, we do not plan to enrich the dataset but rather provide richer and better concepts, improve the quality of the queries and narrow down the application of the challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,298.08,345.83,7.89;5,134.77,309.06,53.04,7.86;5,142.26,202.90,107.20,80.40"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples from the results of the query: 'Show all moment I was having coffee with friends.'</figDesc><graphic coords="5,142.26,202.90,107.20,80.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,166.04,417.44,283.27,7.89;6,137.07,310.21,110.65,82.99"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Sample images from the Puzzle task and the predicted results.</figDesc><graphic coords="6,137.07,310.21,110.65,82.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,140.99,596.09,120.60,8.77;9,140.99,608.09,91.64,8.77;9,140.99,620.09,19.01,8.77;9,140.99,632.09,89.51,8.77;9,140.99,644.09,89.51,8.77;9,140.99,656.09,19.01,8.77"><head>- 1 , 1 ,</head><label>11</label><figDesc>Icecream by the Sea -2, 1, DCU canteen -... -2, 8, Restaurant 5 -2, 9, Restaurant 6 -...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,181.23,115.91,239.81,109.98"><head>Table 1 .</head><label>1</label><figDesc>Statistics of ImageCLEFlifelog 2019 Data</figDesc><table coords="7,181.23,136.29,239.81,89.60"><row><cell>Characters</cell><cell>Size</cell></row><row><cell>Number of Lifeloggers</cell><cell>2</cell></row><row><cell>Number of Days</cell><cell>43 days</cell></row><row><cell>Size of the Collection</cell><cell>14 GB</cell></row><row><cell>Number of Images</cell><cell>81,474 images</cell></row><row><cell>Number of Locations</cell><cell>61 semantic locations</cell></row><row><cell>Number of Puzzle Queries</cell><cell>20 queries</cell></row><row><cell>Number of LMRT Queries</cell><cell>20 queries</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,140.74,115.91,342.96,517.55"><head>Table 2 .</head><label>2</label><figDesc>Structure of Minute-based table.</figDesc><table coords="8,140.74,138.38,342.96,495.08"><row><cell>Field name</cell><cell>Meaning</cell><cell>Example</cell></row><row><cell>minute ID</cell><cell>Identity field for every minute,</cell><cell>u1 20180503 0000</cell></row><row><cell></cell><cell>unique for every volunteer</cell><cell></cell></row><row><cell>utc time</cell><cell>UTC Time with format:</cell><cell>20180503 0000 UTC</cell></row><row><cell></cell><cell>YYYYMMDD HHMM UTC</cell><cell></cell></row><row><cell>local time</cell><cell>Local time in volunteers timezone</cell><cell>20180503 0100</cell></row><row><cell></cell><cell>(from volunteers smart phone):</cell><cell></cell></row><row><cell></cell><cell>YYYYMMDD HHMM</cell><cell></cell></row><row><cell>time zone</cell><cell>The name of volunteers timezone</cell><cell>Europe/Dublin</cell></row><row><cell>lat</cell><cell>Latitude of volunteers position</cell><cell>53.386881</cell></row><row><cell>lon</cell><cell>Longitude of volunteers position</cell><cell>-6.15843</cell></row><row><cell>name</cell><cell>The name of the place</cell><cell>Home</cell></row><row><cell></cell><cell>corresponding to volunteers</cell><cell></cell></row><row><cell></cell><cell>position</cell><cell></cell></row><row><cell>song</cell><cell>The name of the song was playing</cell><cell>walking, transport</cell></row><row><cell></cell><cell>at that time activity The activity</cell><cell></cell></row><row><cell></cell><cell>that volunteer was doing at that</cell><cell></cell></row><row><cell></cell><cell>time</cell><cell></cell></row><row><cell>steps</cell><cell>The number of volunteers steps</cell><cell>14</cell></row><row><cell></cell><cell>collected by wearable devices</cell><cell></cell></row><row><cell>calories</cell><cell>The number of calories collected</cell><cell>1.17349994</cell></row><row><cell></cell><cell>by wearable devices</cell><cell></cell></row><row><cell>historic glucose</cell><cell>The historic glucose index</cell><cell>4.3</cell></row><row><cell>(mmol/L)</cell><cell>collected by wearable devices,</cell><cell></cell></row><row><cell></cell><cell>measured in mmol/L</cell><cell></cell></row><row><cell>scan glucose</cell><cell>The scan glucose index collected</cell><cell>4.8</cell></row><row><cell>(mmol/L)</cell><cell>by wearable devices, measured in</cell><cell></cell></row><row><cell></cell><cell>mmol/L</cell><cell></cell></row><row><cell>heart rate</cell><cell>The heart rate of volunteer at that</cell><cell>73</cell></row><row><cell></cell><cell>time collected by wearable devices</cell><cell></cell></row><row><cell>distance</cell><cell>The distance collected by wearable</cell><cell></cell></row><row><cell></cell><cell>devices</cell><cell></cell></row><row><cell>img00 id to</cell><cell>The image ID captured by the</cell><cell>u1 20180503 1627 i01</cell></row><row><cell>img 19 id</cell><cell>wearable camera at that time</cell><cell></cell></row><row><cell>cam00 id to</cell><cell>The image ID captured by</cell><cell>u1 20180503 1625 cam i00</cell></row><row><cell>cam14 id</cell><cell>volunteers smart phone at that</cell><cell></cell></row><row><cell></cell><cell>time</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,151.16,115.91,314.75,293.10"><head>Table 3 .</head><label>3</label><figDesc>Structure of the Visual Concepts table.</figDesc><table coords="9,151.16,138.59,314.75,270.43"><row><cell>Field name</cell><cell>Meaning</cell><cell>Example</cell></row><row><cell>image id</cell><cell>Identity field for every</cell><cell>u1 20180503 0617 i00</cell></row><row><cell></cell><cell>image, including images</cell><cell></cell></row><row><cell></cell><cell>from wearable camera and</cell><cell></cell></row><row><cell></cell><cell>smart phone camera</cell><cell></cell></row><row><cell>image path</cell><cell>Image path to the</cell><cell></cell></row><row><cell></cell><cell>corresponding image</cell><cell></cell></row><row><cell>attribute top1 to</cell><cell>The top 10 attributes</cell><cell>no horizon,</cell></row><row><cell>attribute top10 (top</cell><cell>predicted by using the</cell><cell>man-made, metal,</cell></row><row><cell>10 attributes)</cell><cell>PlaceCNN, trained on</cell><cell>indoor lighting</cell></row><row><cell></cell><cell>SUNattribute dataset</cell><cell></cell></row><row><cell>category topXX,</cell><cell>The top 05 categories and</cell><cell>chemistry lab, 0.082</cell></row><row><cell>category topXX score</cell><cell>their scores predicted by</cell><cell></cell></row><row><cell>(top 05 categories)</cell><cell>using the PlaceCNN,</cell><cell></cell></row><row><cell></cell><cell>trained on Place 365</cell><cell></cell></row><row><cell></cell><cell>dataset.</cell><cell></cell></row><row><cell>concept class topXX,</cell><cell>Class name, bounding box</cell><cell>person, 0.987673</cell></row><row><cell>concept score topXX,</cell><cell>and score of the top 25</cell><cell>508.568878</cell></row><row><cell>concept bbox topXX,</cell><cell>objects with the highest</cell><cell>171.124496</cell></row><row><cell>(top 25 concepts)</cell><cell>score in each image. They</cell><cell>513.541748</cell></row><row><cell></cell><cell>are predicted by using</cell><cell>395.073303</cell></row><row><cell></cell><cell>Faster R-CNN, trained on</cell><cell></cell></row><row><cell></cell><cell>the COCO dataset</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,167.81,115.91,279.73,7.89"><head>Table 4 .</head><label>4</label><figDesc>Official Results of the ImageCLEFlifelog 2019 LMRT Task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="12,143.31,115.91,328.74,350.82"><head>Table 5 .</head><label>5</label><figDesc>Official Results of the ImageCLEFlifelog 2019 Puzzle Task.</figDesc><table coords="12,143.31,138.59,328.74,328.15"><row><cell>Team</cell><cell>Run</cell><cell cols="3">Kendall's Tau Part of Day Final Score</cell></row><row><cell cols="2">Organiser [13] RUN1 *</cell><cell>0.06</cell><cell>0.31</cell><cell>0.18</cell></row><row><cell></cell><cell>RUN2 *</cell><cell>0.03</cell><cell>0.35</cell><cell>0.19</cell></row><row><cell></cell><cell>RUN3 *</cell><cell>0.03</cell><cell>0.34</cell><cell>0.18</cell></row><row><cell></cell><cell>RUN4 *</cell><cell>0.05</cell><cell>0.49</cell><cell>0.27</cell></row><row><cell>BIDAL [4]</cell><cell>RUN1</cell><cell>0.12</cell><cell>0.30</cell><cell>0.21</cell></row><row><cell></cell><cell>RUN2</cell><cell>0.08</cell><cell>0.31</cell><cell>0.20</cell></row><row><cell></cell><cell>RUN3</cell><cell>0.06</cell><cell>0.28</cell><cell>0.17</cell></row><row><cell></cell><cell>RUN4</cell><cell>0.12</cell><cell>0.38</cell><cell>0.25</cell></row><row><cell></cell><cell>RUN5</cell><cell>0.10</cell><cell>0.30</cell><cell>0.20</cell></row><row><cell></cell><cell>RUN6</cell><cell>0.09</cell><cell>0.29</cell><cell>0.19</cell></row><row><cell></cell><cell>RUN7</cell><cell>0.15</cell><cell>0.26</cell><cell>0.21</cell></row><row><cell></cell><cell>RUN8</cell><cell>0.07</cell><cell>0.30</cell><cell>0.19</cell></row><row><cell></cell><cell>RUN9</cell><cell>0.19</cell><cell>0.55</cell><cell>0.37</cell></row><row><cell></cell><cell>RUN10</cell><cell>0.17</cell><cell>0.50</cell><cell>0.33</cell></row><row><cell></cell><cell>RUN11</cell><cell>0.10</cell><cell>0.49</cell><cell>0.29</cell></row><row><cell>DAMILAB</cell><cell>RUN6</cell><cell>0.02</cell><cell>0.40</cell><cell>0.21</cell></row><row><cell></cell><cell>RUN7</cell><cell>0.02</cell><cell>0.47</cell><cell>0.25</cell></row><row><cell>HCMUS [9]</cell><cell>RUN03ME</cell><cell>0.40</cell><cell>0.70</cell><cell>0.55</cell></row><row><cell></cell><cell>RUN3</cell><cell>0.40</cell><cell>0.66</cell><cell>0.53</cell></row><row><cell></cell><cell>RUN04ME</cell><cell>0.40</cell><cell>0.70</cell><cell>0.55</cell></row><row><cell></cell><cell>RUN4</cell><cell>0.40</cell><cell>0.66</cell><cell>0.53</cell></row><row><cell>Notes:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="12,198.00,457.10,3.65,5.24;12,205.23,458.87,240.59,7.86"><p>* submissions from the organizer teams are just for reference.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,144.73,634.88,269.19,7.86"><p>Last.FM Music Tracker and Recommender -https://www.last.fm/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,144.73,645.84,251.90,7.86"><p>Fitbit Fitness Tracker (FitBit Versa) -https://www.fitbit.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,144.73,656.80,293.73,7.86"><p>Freestyle Libre wearable glucose monitor -https://www.freestylelibre.ie/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,144.73,656.80,257.92,7.86"><p>Moves App for Android and iOS -http://www.moves-app.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>This publication has emanated from research supported in party by research grants from <rs type="funder">Irish Research Council (IRC)</rs> under Grant Number <rs type="grantNumber">GOIPG/2016/741</rs> and <rs type="funder">Science Foundation Ireland</rs> under grant numbers <rs type="grantNumber">SFI/12/RC/2289</rs> and <rs type="grantNumber">SFI/13/RC/2106</rs>.</p><p>The authors thank to <rs type="person">Thanh-An Nguyen</rs>, <rs type="person">Trung-Hieu Hoang</rs>, and the annotation team of <rs type="institution">Software Engineering Laboratory (SELab), University of Science, VNU-HCM</rs> for supporting in building the data collection as well as giving valuable discussions for the Lifelog moment retrieval (LMRT) task.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8hSeZfm">
					<idno type="grant-number">GOIPG/2016/741</idno>
				</org>
				<org type="funding" xml:id="_K2da7S6">
					<idno type="grant-number">SFI/12/RC/2289</idno>
				</org>
				<org type="funding" xml:id="_yfcd4df">
					<idno type="grant-number">SFI/13/RC/2106</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="15,142.96,503.61,337.64,7.86;15,151.52,514.56,329.07,7.86;15,151.52,525.52,234.03,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,376.72,503.61,103.88,7.86;15,151.52,514.56,119.28,7.86">Big Data For Lifelog Moments Retrieval Improvement</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">B</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Feki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="15,294.67,514.56,106.58,7.86">CLEF2019 Working Notes</title>
		<title level="s" coord="15,409.60,514.56,70.99,7.86;15,151.52,525.52,75.40,7.86">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,536.40,337.64,7.86;15,151.52,547.36,329.07,7.86;15,151.52,558.32,329.07,7.86;15,151.52,569.28,117.59,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,151.52,547.36,308.83,7.86">Overview of ImageCLEFlifelog 2017: Lifelog Retrieval and Summarization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="15,151.52,558.32,132.57,7.86">CLEF 2017 Labs Working Notes</title>
		<title level="s" coord="15,292.10,558.32,150.53,7.86">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,580.16,337.64,7.86;15,151.52,591.12,329.07,7.86;15,151.52,602.08,329.07,7.86;15,151.52,613.04,150.63,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,442.93,580.16,37.66,7.86;15,151.52,591.12,329.07,7.86;15,151.52,602.08,24.44,7.86">Overview of ImageCLEFlifelog 2018: Daily Living Understanding and Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="15,200.54,602.08,108.44,7.86">CLEF2018 Working Notes</title>
		<title level="s" coord="15,318.26,602.08,156.40,7.86">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,623.92,337.63,7.86;15,151.52,634.88,329.07,7.86;15,151.52,645.84,329.07,7.86;15,151.52,656.80,117.59,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,351.07,623.92,129.52,7.86;15,151.52,634.88,329.07,7.86;15,151.52,645.84,11.56,7.86">BIDAL@imageCLEFlifelog2019: The Role of Content and Context of Daily Activities in Insights from Lifelogs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">D</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zettsu</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="15,183.66,645.84,104.08,7.86">CLEF2019 Working Notes</title>
		<title level="s" coord="15,294.83,645.84,147.79,7.86">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,119.67,337.64,7.86;16,151.52,130.63,329.07,7.86;16,151.52,141.59,135.60,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,259.63,119.67,220.97,7.86;16,151.52,130.63,157.96,7.86">Outlines of a world coming into existence&apos;: Pervasive computing and the ethics of forgetting</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kitchin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,317.14,130.63,163.45,7.86;16,151.52,141.59,44.98,7.86">Environment and Planning B: Planning and Design</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="445" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,153.48,337.63,7.86;16,151.52,164.43,329.07,7.86;16,151.52,175.39,150.63,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,253.42,153.48,227.17,7.86;16,151.52,164.43,44.98,7.86">Multimedia Lab @ ImageCLEF 2019 Lifelog Moment Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="16,216.57,164.43,103.10,7.86">CLEF2019 Working Notes</title>
		<title level="s" coord="16,326.28,164.43,148.38,7.86">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,187.28,337.64,7.86;16,151.52,198.24,90.03,7.86" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Albatal</surname></persName>
		</author>
		<title level="m" coord="16,396.33,187.28,84.26,7.86;16,151.52,198.24,61.36,7.86">Overview of NTCIR-12 Lifelog Task</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,210.12,337.64,7.86;16,151.52,221.08,329.07,7.86;16,151.52,232.04,310.21,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,212.01,221.08,151.81,7.86">Overview of NTCIR-13 Lifelog-2 Task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Albatal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,385.24,221.08,95.35,7.86;16,151.52,232.04,281.53,7.86">Proceedings of the 13th NTCIR Conference on Evaluation of Information Access Technologies</title>
		<meeting>the 13th NTCIR Conference on Evaluation of Information Access Technologies</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,243.92,337.63,7.86;16,151.52,254.88,329.07,7.86;16,151.52,265.84,307.06,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,367.84,243.92,112.75,7.86;16,151.52,254.88,201.96,7.86">Solving Life Puzzle with Visual Context-based Clustering and Habit Reference</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="16,373.31,254.88,103.05,7.86">CLEF2019 Working Notes</title>
		<title level="s" coord="16,151.52,265.84,148.42,7.86">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,277.72,337.98,7.86;16,151.52,288.68,329.07,7.86;16,151.52,299.64,329.07,7.86;16,151.52,310.60,329.07,7.86;16,151.52,321.56,329.07,7.86;16,151.52,332.52,329.07,7.86;16,151.52,343.48,329.07,7.86;16,151.52,354.43,329.07,7.86;16,151.52,365.39,329.07,7.86;16,151.52,376.35,62.50,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,268.29,332.52,212.30,7.86;16,151.52,343.48,124.23,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,297.67,343.48,182.92,7.86;16,151.52,354.43,329.07,7.86;16,151.52,365.39,122.46,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="16,305.55,365.39,171.07,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="16,142.62,388.24,337.97,7.86;16,151.52,399.19,329.07,7.86;16,151.52,410.15,329.07,7.86;16,151.52,421.11,150.63,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,375.08,388.24,105.51,7.86;16,151.52,399.19,329.07,7.86;16,151.52,410.15,35.04,7.86">Lifelog Moment Retrieval with Advanced Semantic Extraction and Flexible Moment Visualization for Exploration</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="16,208.79,410.15,105.69,7.86">CLEF2019 Working Notes</title>
		<title level="s" coord="16,322.39,410.15,152.26,7.86">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,433.00,337.98,7.86;16,151.52,443.95,329.07,7.86;16,151.52,454.91,296.14,8.12" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="16,328.62,443.95,151.96,7.86;16,151.52,454.91,28.22,7.86">Microsoft COCO: common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>CoRR abs/1405.0312</idno>
		<ptr target="http://arxiv.org/abs/1405.0312" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,466.80,337.97,7.86;16,151.52,477.76,329.07,7.86;16,151.52,488.72,329.07,7.86;16,151.52,499.67,234.03,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,251.75,477.76,228.85,7.86;16,151.52,488.72,118.12,7.86">LIFER 2.0: Discover Personal Lifelog Insight by Interactive Lifelog Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="16,293.81,488.72,107.02,7.86">CLEF2019 Working Notes</title>
		<title level="s" coord="16,409.38,488.72,71.21,7.86;16,151.52,499.67,75.40,7.86">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,511.56,337.98,7.86;16,151.52,522.52,329.07,8.12;16,151.52,534.12,112.98,7.47" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="16,317.58,511.56,163.01,7.86;16,151.52,522.52,162.18,7.86">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR abs/1506.01497</idno>
		<ptr target="http://arxiv.org/abs/1506.01497" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,545.36,337.97,7.86;16,151.52,556.32,329.07,7.86;16,151.52,567.28,329.07,7.86;16,151.52,578.24,25.60,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,323.74,545.36,156.85,7.86;16,151.52,556.32,235.30,7.86">UAPTBioinformatics working notes at ImageCLEF 2019 Lifelog Moment Retrieval (LMRT) task</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J R</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="16,408.97,556.32,71.63,7.86;16,151.52,567.28,37.66,7.86">CLEF2019 Working Notes</title>
		<title level="s" coord="16,197.63,567.28,151.86,7.86">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,590.12,337.97,7.86;16,151.52,601.08,329.07,7.86;16,151.52,612.04,234.03,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,242.07,590.12,238.51,7.86;16,151.52,601.08,127.38,7.86">Automated Lifelog Moment Retrieval based on Image Segmentation and Similarity Scores</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Taubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="16,299.62,601.08,104.11,7.86">CLEF2019 Working Notes</title>
		<title level="s" coord="16,410.84,601.08,69.76,7.86;16,151.52,612.04,75.40,7.86">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,623.92,337.97,7.86;16,151.52,634.88,329.07,7.86;16,151.52,645.84,329.07,7.86;16,151.52,656.80,25.60,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,473.68,623.92,6.91,7.86;16,151.52,634.88,238.46,7.86">A Multimedia Modular Approach to Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tournadre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Cheikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lmami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Ginsca</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="16,410.04,634.88,70.55,7.86;16,151.52,645.84,37.66,7.86">CLEF2019 Working Notes</title>
		<title level="s" coord="16,197.63,645.84,151.86,7.86">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.62,119.67,337.97,7.86;17,151.52,130.63,329.07,7.86;17,151.52,141.59,111.11,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="17,399.52,119.67,81.07,7.86;17,151.52,130.63,145.67,7.86">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,303.98,130.63,176.62,7.86;17,151.52,141.59,82.44,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.62,152.55,337.98,7.86;17,151.52,163.51,329.07,7.86;17,151.52,174.47,329.07,8.12;17,151.52,186.07,127.10,7.47" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="17,373.21,152.55,107.39,7.86;17,151.52,163.51,61.10,7.86">Lifer: An interactive lifelog retrieval system</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hinbarji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3210539.3210542</idno>
		<ptr target="http://doi.acm.org/10.1145/3210539.3210542" />
	</analytic>
	<monogr>
		<title level="m" coord="17,232.45,163.51,248.14,7.86;17,151.52,174.47,37.79,7.86;17,233.73,174.47,30.43,7.86">Proceedings of the 2018 ACM Workshop on The Lifelog Search Challenge</title>
		<meeting>the 2018 ACM Workshop on The Lifelog Search Challenge<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
	<note>LSC &apos;18</note>
</biblStruct>

<biblStruct coords="17,142.62,196.39,337.98,7.86;17,151.52,207.34,329.07,7.86;17,151.52,218.30,180.33,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="17,257.02,196.39,223.57,7.86;17,151.52,207.34,78.15,7.86">ZJUTCVR Team at ImageCLEFlifelog2019 Lifelog Moment Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="17,249.41,207.34,102.80,7.86">CLEF2019 Working Notes</title>
		<title level="s" coord="17,358.64,207.34,121.95,7.86;17,151.52,218.30,21.70,7.86">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
