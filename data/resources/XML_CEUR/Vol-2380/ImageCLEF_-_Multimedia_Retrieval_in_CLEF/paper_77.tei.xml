<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,173.11,115.96,269.14,12.62;1,146.38,133.89,322.60,12.62;1,222.60,151.82,170.16,12.62">Estimating Severity from CT Scans of Tuberculosis Patients using 3D Convolutional Nets and Slice Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,180.07,189.58,56.81,8.74"><forename type="first">Hasib</forename><surname>Zunair</surname></persName>
							<email>hasibzunair@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">North South University</orgName>
								<address>
									<postCode>1229</postCode>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.44,189.58,69.05,8.74"><forename type="first">Aimon</forename><surname>Rahman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">North South University</orgName>
								<address>
									<postCode>1229</postCode>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,346.42,189.58,84.40,8.74"><forename type="first">Nabeel</forename><surname>Mohammed</surname></persName>
							<email>nabeel.mohammed@northsouth.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">North South University</orgName>
								<address>
									<postCode>1229</postCode>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,173.11,115.96,269.14,12.62;1,146.38,133.89,322.60,12.62;1,222.60,151.82,170.16,12.62">Estimating Severity from CT Scans of Tuberculosis Patients using 3D Convolutional Nets and Slice Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6B20AB966014FFB4B6FCD02B10049DF6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>Convolutional Neural Networks</term>
					<term>Image Analysis</term>
					<term>Computed Tomography</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose a 16-layer 3D convolutional neural network with a slice selection technique employed in the task of 3D Computed Tomography Image data of Tuberculosis(TB) patients which attained 10 th place in the ImageCLEF 2019 Tuberculosis -Severity scoring challenge. The goal is aimed at estimating TB severity based on the CT image. The best result reported in this work is Area Under the ROC Curve (AUC) of 0.61 and a binary accuracy of 61.5%. Codes for this work can be found at URL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tuberculosis is a potentially serious infectious disease that affects lungs, and sometimes other parts of the body. Mycobacterium tuberculosis, is the bacteria that causes the infection and can spread from one person to another via cough, spit or sneezes. Most infections do not cause any symptoms, which is refered as latent tuberculosis and 10% of them turns to potentially fatal active diseases. People with HIV/AIDS and smokers are more vulnerable to active TB. The symptoms of the infections are cough with blood-containing mucus, night sweats, fever, chills, loss of appetite and severe weight-loss. About onequarter of the world population has been infected with the disease. In 2010, 1.2 -1.45 million deaths have occurred due to the disease, mostly in developing countries which makes it the second most common cause of death from infectious disease <ref type="bibr" coords="1,191.17,591.81,9.96,8.74" target="#b0">[1]</ref>. Tuberculosis is diagnosed by conducting chest X-ray and microscopic examination of bodily fluids. Computed tomography (CT) scan provides more detailed information about the infection than X-ray images <ref type="bibr" coords="2,431.83,118.99,9.96,8.74" target="#b1">[2]</ref>. Several techniques have been used to automate the detection of Mycobacterium infection using different machine learning techniques in chest radiographs and CT scan images. Deep learning <ref type="bibr" coords="2,260.34,154.86,15.50,8.74" target="#b13">[14]</ref> has also been used to diagnose lung diseases in CT scan images. <ref type="bibr" coords="2,223.88,166.81,10.52,8.74" target="#b3">[4]</ref> used deep belief network and convolutional neural network to classify lung nodule in 3D tomography images, where deep beleif network performed better than convolutional neural network and SIFT. <ref type="bibr" coords="2,441.07,190.72,10.52,8.74" target="#b2">[3]</ref> shows an ensemble of AlexNet and GoogleNet DCNNs performed best with the AUC score of 0.99 on radiograph images to detect pulmonary tuberculosis. Prognosis of diseases such as chronic obstructive pulmonary disease and acute respiratory disease on smokers lungs has also been predicted using convolutional neural network in computed tomography images <ref type="bibr" coords="2,348.13,250.50,9.96,8.74" target="#b4">[5]</ref>. Three different deep learning techniques have also been applied to detect lung cancer from CT scan images using convolutional neural networks(CNN), Deep neural networks(DNN) and Sparse Autoencoders(SAE), where CNN performed best with an accuracy of 84.15%, sensitivity of 83.96%, and specificity of 84.32% <ref type="bibr" coords="2,396.42,298.32,9.96,8.74" target="#b5">[6]</ref>. Most computer aided diagnosis has been done for tuberculosis which uses radiographic images <ref type="bibr" coords="2,134.77,322.23,10.70,8.74" target="#b6">[7]</ref>- <ref type="bibr" coords="2,149.03,322.23,14.27,8.74" target="#b9">[10]</ref>. A challenging part of working with CT scans is the fact that the data points comprises of depth information, which make it not only three dimensional but also computationally expensive to process for images with have large depth size. Several works have been done on detecting 3D objects, for example by integrating volumetric grid representation with a 3D convolutional neural network <ref type="bibr" coords="2,188.88,382.01,15.50,8.74" target="#b10">[11]</ref> introduced a network named VoxNet, which is then validated in LiDAR, RGBD, and CAD data. <ref type="bibr" coords="2,294.72,393.96,15.50,8.74" target="#b11">[12]</ref> on the other hand, uses 2D representation from various angles of a 3D object and trained a multi view convolutional neural network classifier with view pooling method which performed better than 3D CNN. They show the use of transfer learning <ref type="bibr" coords="2,410.66,429.83,15.50,8.74" target="#b12">[13]</ref> which is an effective technique where a model is used which previously learnt a set of features to solve a visual recognition task as a starting point to solve another task.</p><p>Training 3D Convolutional Nets on volumetric data is computationally expensive due to depth information which requires additional feature learning and hence results in an increased number of learnable paramters. Moreover, all the data samples do not have same depth size which complicates training. To address this problem, we introduce a novel data partitioning technique which makes the training method not only effective but most importantly feasible. We show, using partial depth information from the each volumetric data point, it is possible to achieve good AUC and accuracy values. We show our approach which achieves 10 th place among a total of 100 participants in the ImageCLEF 2019 Tuberculosis -Severity scoring challenge <ref type="bibr" coords="2,314.07,585.53,14.61,8.74" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental Overview</head><p>This section provides details of the setup used in the different experiments. A brief description of the data set used for the experimentation will be given, followed by description of the network architecture. Moreover, details of the training regiment with the metrics are provided.</p><p>For replication of this work, it is relevant to mention that all of the experiments were performed on a machine with Windows system with Intel Core(TM) i7-7700 CPU @3.60GHz processor, 32 GB RAM, a single CUDA-enabled NVIDIA GTX 1050 4GB graphical processing unit (GPU), Python 3.6.7, Keras 2.2.4 with Tensorflow 1.12.0 backend, and CUDA compilation tools, release 10.0, V10.0.130 dependencies for GPU acceleration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset Description</head><p>The dataset for the severity scoring(SVR) task is provided by ImageCLEF Tuberculosis 2019 <ref type="bibr" coords="3,217.09,280.11,30.01,8.74">[15][16]</ref>. The dataset consists of a total 335 chest CT scans of Tuberculosis patients in addition with clinically relevant metadata. From the dataset, 218 data points are used for training and the remaining 117 are hold out for the final evaluation. The selected metadata includes the following binary measures: disability, relapse, symptoms of TB, comorbidity, bacillary, drug resistance, higher education, ex-prisoner, alcoholic, smoking. The 3D CT images which were provided have a slice size of 512×512 pixels and a number of slices varying from 50 to 400. All the CT images are stored in NIFTI file format. This file format stores raw voxel intensities in Hounsfield units (HU) as well the corresponding image metadata such as image dimensions, voxel size in physical units, slice thickness, etc. Figure <ref type="figure" coords="3,279.89,399.67,4.98,8.74" target="#fig_0">1</ref> shows an instance of this. The original severity score which is assigned by a medical doctor is included as training meta-data which are annotated in a scale from 1("critical/very bad") to 5("very good). This grade is converted to "LOW" (scores 4 and 5) and "HIGH" (scores 1, 2 and 3) thereby reducing the task to a binary classification problem as per task descriptions provided by ImageCLEF Tuberculosis 2019 <ref type="bibr" coords="3,430.43,632.21,16.52,8.74" target="#b14">[15]</ref> <ref type="bibr" coords="3,446.95,632.21,16.52,8.74" target="#b15">[16]</ref>. In addition to these requirements, the 117 test set labels are hidden, which make it more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Prerocessing -Slice Selection Technique</head><p>In order to prepare the data for training, we first resize individual slices of the 3D input volume to 128×128 with bicubic interpolation. This is followed by a slice selection technique that we introduce in this work, where a depth size of 32 and 16 -which results in two experimental settings discussed in 1 -are chosen for our final submissions. In Figure <ref type="figure" coords="4,193.54,548.52,3.87,8.74" target="#fig_1">2</ref>, we show a visual representation of the slice selection technique. For a given input volume the first 4 slices, the middle 8 slices and the last 4 slices are extracted. The middle slice of the input volume is obtained by taking the half of the input volume depth. These three sub components are then stacked to reconstruct the desired input volume where, in this case, is a depth size of 16. In other words, the input volume consists of a total of 16 slices. The main motivation behind the proposed technique eliminates the problem of GPU exhaustion during optimization. Since the default input volume consist of large number of slices, it was entire impossible to allocate tensors for computation in our experimental setup discussed at the end of Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Configuration of the proposed Convolutional Neural Network</head><p>The network used in this work was inspired by the architecture used for real time object recognition by integrating a volumetric occupancy grid representation with a supervised convolutional net named VoxNet <ref type="bibr" coords="5,384.64,164.68,14.61,8.74" target="#b10">[11]</ref>. All the 3D input volumes were transformed to 128 × 128 × depth, where depth is 32 and 16 for the two different settings, with cubic interpolation for the network input. The network consists of three convolutional layers with 32 filters of size 2 × 2 × 2 . After every convolutional layer, a maxpooling layer is added with a stride of 2. Maxpooling layers reduces its size of input to half by taking maximum values from a window of size 2×2. Recitified Linear Units(ReLu) was used as the activation function for both convolutional and fully connected layers. The activation function is governed by Equation <ref type="formula" coords="5,282.49,260.32,3.87,8.74" target="#formula_0">1</ref>:</p><formula xml:id="formula_0" coords="5,277.12,285.72,203.47,8.74">a = max(0, x)<label>(1)</label></formula><p>where, a is the output of the activation for a given input x.</p><p>The convolutional blocks in Figure <ref type="figure" coords="5,302.97,317.46,9.96,8.74" target="#fig_2">3a</ref> is then followed by a batch normalization layer. The deep learning community has quickly adopted the use of batch normalization as it introduces a form of regularization which restrains the network form simply memorizing the training dataset, which means the network is expected to generalize better on unseen data with use of batch normalization. The output from the batch normalization layer is flattened and passed to a series of fully connected layers with two dense layers having 1028 neurons, one with 512 neurons. Each of the dense layers were connected to a dropout layer which drops the neuron connection with a probability of 40%. The output from the final two dropout layers was followed by a dense layer of 2 neurons. The network architecture is shown in Figure <ref type="figure" coords="5,273.19,437.01,3.87,8.74" target="#fig_2">3</ref>.</p><p>Softmax activation shown in equation 2 was applied on the last layer to get the probability results for the binary classification problem. The output of the softmax function is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true. This enables better performance of the model.</p><formula xml:id="formula_1" coords="5,262.97,518.87,213.38,28.14">σ(z j ) = e (zj ) K k=1 e (z k ) . (<label>2</label></formula><formula xml:id="formula_2" coords="5,476.35,527.18,4.24,8.74">)</formula><p>where z is a vector of the inputs to the output layer (if you have 10 output units, then there are 10 elements in z). And again, j indexes the output units, so j = 1, 2, ..., K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training Regiment</head><p>Stochastic Gradient descent was used to optimize the weights of the network via backpropagation with a learning rate of 10 -4 with a momentum of 0.9. Cross-entropy error between the predicted and ground truth was used as the  <ref type="formula" coords="6,279.32,365.44,4.98,8.74" target="#formula_3">3</ref>and the weights were updated using minibatches for every iteration. The initialization of the weights was done at random and the biases were initialized as zero.</p><formula xml:id="formula_3" coords="6,211.81,423.28,268.78,30.32">L(y, ŷ) = - 1 N N i=1 C c=1 1 yi Cc log p model [y i Cc ]<label>(3)</label></formula><p>In equation 3, the double sum is over the observations i, whose number is N , and the categories c, whose number is C. The term 1 yi in C c is the indicator function of the i th observation belonging to the cth category. The p model is the probability predicted by the model for the ith observation to belong to the c th category. When there are more than two categories, the neural network outputs a vector of C probabilities, each giving the probability that the network input should be classified as belonging to the respective category. When the number of categories is just two, the neural network outputs a single probability with the other one being 1 minus the output.</p><p>Training was continued for 300 epochs on layers L1 to L16 in Figure <ref type="figure" coords="6,445.53,584.39,4.98,8.74" target="#fig_2">3</ref> with a validation split of 0.1 and the final evaluation was done on the test set. For the final submissions, we only make change in input volume depth size which also required us to change the batch size which results two different settings shown in Table <ref type="table" coords="6,161.85,632.21,3.87,8.74" target="#tab_0">1</ref>. All the other remaining parameters were kept same in the two settings. It is noteworthy that, in both the configurations, the learnable parameters are 23,808,378. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Metrics</head><p>The task is evaluated as binary classification problem, including measures such as Area Under the ROC Curve (AUC) and accuracy. An AUC of 1 represents a perfect classification system where True positive rate is 1 and False positive rate is 0. Since the ranking of the techniques will be first based on the AUC and then by the accuracy, AUC is the optimizing metric and the binary accuracy is the satisfying metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Result and Discussion</head><p>From Table <ref type="table" coords="7,205.96,335.47,4.98,8.74" target="#tab_1">2</ref> it can be seen that CFG-A achieves the highest AUC and accuracy values in the experiments conducted in this work. CFG-A, where the network is trained with an input volume of 128×128×32 , this achieves an average test AUC of 0.611 and test accuracy of 61.5%. Also to note, the batch size for this configuration was set to 4, since any larger values resulted in GPU memory exhaustion. This still led to achieve the best result in the set of experiments conducted which is surprising. CFG-B which is trained and evaluated with an input volume of 128 × 128 × 16. From the experiments it can be said that the degradation in CFG-B is due to lower number of slices in the input volume which results in information loss. Even though the batch size was 16 in this setting, the information loss outweighs the impact on the overall performance. In Figures <ref type="figure" coords="7,195.89,584.39,4.98,8.74" target="#fig_3">4</ref> and<ref type="figure" coords="7,221.91,584.39,4.98,8.74" target="#fig_4">5</ref> we show the training logs for both the configurations. Both CFG-A and CFG-B are trained for 300 epochs. In Figure <ref type="figure" coords="7,387.62,596.34,3.87,8.74" target="#fig_3">4</ref>, which portrays the training log for CFG-A, it can be seen that the network starts to overfit only after 25 epochs. CFG-A achieves a highest validation accuracy of 68%, where in the test set an accuracy of 61.5% is achieved which results in test-val accuracy margin of 6.5% between validation and testing set even when the batch size is set to 4. In the case of CFG-B which is shown in Figure <ref type="figure" coords="8,358.59,304.45,4.98,8.74" target="#fig_4">5</ref> the network starts overfitting after 60 epochs and achieves a highest validation accuracy of 82.5%. It is surprising that this configuration achieves only a test accuracy of 53.8% which results in a test-val accuracy of 28.7%. From this behaviour we can say that the preprocessing technique employed in CFG-B with depth size of 16 results the test set to not be representative of the validation set. This setup also causes information loss which results in significantly lower performance than CFG-A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We demonstrate a 3D convolutional neural network with a newly proposed preprocessing technique, slice selection from volumetric data, used in the task to estimate severity based on CT Image of Tuberculosis patients. This work achieved 10 th place with a test AUC of 0.611 and test accuracy of 61.5% in the ImageCLEF 2019 Tuberculosis -Severity Scoring challenge <ref type="bibr" coords="9,402.82,118.99,15.01,8.74" target="#b14">[15]</ref> <ref type="bibr" coords="9,417.82,118.99,15.01,8.74" target="#b15">[16]</ref>. We show that even without using all the slices from the training set, via slice selection technique, it is possible to achieve certain rather good AUC and accuracy values in the final test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Works</head><p>In future works, the results will be further analyzed to gain a better understanding of the reasons behind the results. In addition, various networks architectures will be experimented and further improvements in the proposed slice selection technique will be made, in an attempt to build a robust deep learning model to estimate severity of TB patients.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,536.79,345.83,7.89;3,134.77,547.78,193.85,7.86;3,134.77,433.93,345.84,88.09"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Slices of 2D image from an arbitrary 3D CT image. The slices are started off from the lower chest progressing to upper chest.</figDesc><graphic coords="3,134.77,433.93,345.84,88.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,192.84,506.99,229.68,7.89;4,134.77,239.50,345.72,252.71"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Slice Selection Technique with a depth size of 16.</figDesc><graphic coords="4,134.77,239.50,345.72,252.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,230.69,325.13,153.98,7.89;6,134.77,115.83,345.82,194.52"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. 16-layer 3D Convolutional Net</figDesc><graphic coords="6,134.77,115.83,345.82,194.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,232.96,268.29,149.42,7.89;8,134.77,115.84,345.83,137.68"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. CFG-A Accuracy/Loss graph</figDesc><graphic coords="8,134.77,115.84,345.83,137.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,233.16,558.30,149.04,7.89;8,134.77,408.54,345.84,135.00"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. CFG-B Accuracy/Loss graph</figDesc><graphic coords="8,134.77,408.54,345.84,135.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,175.79,115.91,224.76,62.34"><head>Table 1 .</head><label>1</label><figDesc>Different experimental settings</figDesc><table coords="7,175.79,136.68,224.76,41.56"><row><cell>Name</cell><cell>Input</cell><cell>Volume</cell><cell>Batch Size</cell></row><row><cell></cell><cell cols="2">Depth Size</cell><cell></cell></row><row><cell>CFG-A</cell><cell>32</cell><cell></cell><cell>4</cell></row><row><cell>CFG-B</cell><cell>16</cell><cell></cell><cell>16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,158.83,485.76,281.11,71.55"><head>Table 2 .</head><label>2</label><figDesc>Performance on final test set</figDesc><table coords="7,158.83,504.78,281.11,52.52"><row><cell>Name</cell><cell>Area Under</cell><cell>Test Set Ac-</cell><cell>Best</cell><cell>Val-</cell><cell>Test-Val</cell></row><row><cell></cell><cell>ROC Curve</cell><cell>curacy</cell><cell>idation</cell><cell></cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Accuracy</cell><cell>Margin</cell></row><row><cell>CFG-A</cell><cell>0.611</cell><cell>61.5</cell><cell>68</cell><cell></cell><cell>6.5</cell></row><row><cell>CFG-B</cell><cell>0.57</cell><cell>53.8</cell><cell>82.5</cell><cell></cell><cell>28.7</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.35,323.66,310.27,8.12;9,146.91,334.62,166.34,8.12" xml:id="b0">
	<monogr>
		<ptr target="https://www.who.int/en/news-room/fact-sheets/detail/tuberculosis" />
		<title level="m" coord="9,146.91,323.66,48.80,7.86">Tuberculosis</title>
		<imprint>
			<date type="published" when="2018-09-08">8 Sept 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,346.06,342.24,7.86;9,146.91,357.02,163.81,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,327.52,346.06,93.58,7.86">Imaging in tuberculosis</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Skoura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zumla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bomanji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,428.35,346.06,52.24,7.86;9,146.91,357.02,117.84,7.86">International Journal of Infectious Diseases</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="87" to="93" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,368.47,342.25,7.86;9,146.91,379.43,333.68,7.86;9,146.91,390.38,183.74,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,321.97,368.47,158.63,7.86;9,146.91,379.43,333.68,7.86;9,146.91,390.38,21.62,7.86">Deep learning at chest radiography: automated classification of pulmonary tuberculosis by using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Paras</forename><surname>Lakhani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baskaran</forename><surname>Sundaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,180.26,390.38,40.32,7.86">Radiology</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="574" to="582" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,401.83,342.24,7.86;9,146.91,412.79,333.68,7.86;9,146.91,423.75,318.66,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,209.78,412.79,270.81,7.86;9,146.91,423.75,165.64,7.86">Computer-aided classification of lung nodules on computed tomography images via deep learning technique</title>
		<author>
			<persName coords=""><forename type="first">Kai</forename><forename type="middle">-</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Che-Hao</forename><surname>Lung</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chusnul</forename><surname>Shintami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen-Huang</forename><surname>Hidayati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu-Jen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,323.91,423.75,102.75,7.86">OncoTargets and therapy</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,435.19,342.25,7.86;9,146.91,446.15,333.68,7.86;9,146.91,457.11,333.68,7.86;9,146.91,468.07,275.37,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,402.79,446.15,77.80,7.86;9,146.91,457.11,287.58,7.86">Disease staging and prognosis in smokers using deep learning in chest computed tomography</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gonzlez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Y</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Vegas-Snchez-Ferrero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Onieva Onieva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">N</forename><surname>Rahaghi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Daz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>San Jos Estpar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Washko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,441.92,457.11,38.68,7.86;9,146.91,468.07,190.78,7.86">American journal of respiratory and critical care medicine</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="203" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,479.51,342.25,7.86;9,146.91,490.47,333.68,7.86;9,146.91,501.43,142.89,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,407.77,479.51,72.82,7.86;9,146.91,490.47,280.99,7.86">Using deep learning for classification of lung nodules on computed tomography images</title>
		<author>
			<persName coords=""><forename type="first">Qingzeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xingke</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuechen</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,439.65,490.47,40.94,7.86;9,146.91,501.43,90.16,7.86">Journal of healthcare engineering</title>
		<imprint>
			<biblScope unit="page">2017</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,512.87,342.24,7.86;9,146.91,523.83,333.68,7.86;9,146.91,534.79,333.68,7.86;9,146.91,545.75,185.58,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,433.92,512.87,46.68,7.86;9,146.91,523.83,313.65,7.86">A novel approach for tuberculosis screening based on deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Sangheum</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hyo-Eun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jihoon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hee-Jin</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,146.91,534.79,67.33,7.86">Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">9785</biblScope>
			<biblScope unit="page">97852</biblScope>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,557.20,342.24,7.86;9,146.91,568.16,333.68,7.86;9,146.91,579.11,76.79,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,312.45,557.20,168.14,7.86;9,146.91,568.16,183.15,7.86">Pre-trained convolutional neural networks as feature extractors for tuberculosis detection</title>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">K</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joo</forename><forename type="middle">Francisco</forename><surname>Valiati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,340.39,568.16,140.20,7.86">Computers in biology and medicine</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="135" to="143" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,590.56,342.24,7.86;9,146.91,601.52,333.67,7.86;9,146.91,612.48,158.23,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,399.30,590.56,81.29,7.86;9,146.91,601.52,277.63,7.86">Computer-aided detection in chest radiography based on artificial intelligence: a survey</title>
		<author>
			<persName coords=""><forename type="first">Chunli</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Demin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yonghong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhijian</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,436.17,601.52,44.42,7.86;9,146.91,612.48,80.52,7.86">Biomedical engineering online</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">113</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,623.92,337.98,7.86;9,146.91,634.88,333.68,7.86;9,146.91,645.84,333.68,7.86;9,146.91,656.80,119.70,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,382.52,634.88,98.07,7.86;9,146.91,645.84,243.29,7.86">Feature selection for automatic tuberculosis screening in frontal chest radiographs</title>
		<author>
			<persName coords=""><forename type="first">Szilrd</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandros</forename><surname>Karargyris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">C</forename><surname>Santosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sema</forename><surname>Candemir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyun</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sameer</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,403.08,645.84,77.51,7.86;9,146.91,656.80,31.13,7.86">Journal of medical systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,119.67,337.97,7.86;10,146.91,130.63,333.68,7.86;10,146.91,141.59,277.77,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,325.32,119.67,155.27,7.86;10,146.91,130.63,145.61,7.86">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,314.24,130.63,166.35,7.86;10,146.91,141.59,169.16,7.86">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,152.55,337.98,7.86;10,146.91,163.51,333.68,7.86;10,146.91,174.47,306.93,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,151.29,163.51,260.51,7.86">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName coords=""><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,432.51,163.51,48.08,7.86;10,146.91,174.47,226.94,7.86">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,185.43,337.98,7.86;10,146.91,196.39,208.41,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,268.73,185.43,116.29,7.86">A survey on transfer learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,391.82,185.43,88.78,7.86;10,146.91,196.39,127.60,7.86">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,207.34,337.98,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,326.60,207.34,54.26,7.86">Deep learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,387.67,207.34,24.60,7.86">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,218.30,337.98,7.86;10,146.91,229.26,333.68,7.86;10,146.91,240.22,333.68,7.86;10,146.91,251.18,333.68,7.86;10,146.91,262.14,121.17,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,248.12,229.26,232.48,7.86;10,146.91,240.22,277.87,7.86">Overview of ImageCLEFtuberculosis 2019 -Automatic CT-based Report Generation and Tuberculosis Severity Assessment</title>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitri</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aleh</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="10,433.32,240.22,47.27,7.86;10,146.91,251.18,58.59,7.86">CLEF 2019 Working Notes</title>
		<title level="s" coord="10,213.10,251.18,170.71,7.86">CEUR Workshop Proceedings (CEUR-WS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,273.10,337.97,7.86;10,146.91,284.06,333.68,7.86;10,146.91,295.02,333.68,7.86;10,146.91,305.98,333.68,7.86;10,146.91,316.93,333.68,7.86;10,146.91,327.89,333.68,7.86;10,146.91,338.85,333.68,7.86;10,146.91,349.81,333.68,7.86;10,146.91,360.77,333.68,7.86;10,146.91,371.73,333.68,7.86;10,146.91,382.69,333.68,7.86;10,146.91,393.65,128.06,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,192.03,349.81,49.02,7.86;10,270.86,349.81,209.73,7.86;10,146.91,360.77,333.68,7.86;10,146.91,371.73,42.73,7.86">Multimedia Retrieval in Medicine, Lifelogging, Security and Nature In: Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitri</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aleh</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Narciso</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ergina</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Roberto Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Cuevas Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikos</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Konstantinos</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,196.12,371.73,284.47,7.86;10,146.91,382.69,69.70,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="10,310.38,382.69,166.24,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF 2019)<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>ImageCLEF</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
