<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.46,115.96,314.45,12.62;1,156.09,133.89,303.17,12.62;1,231.52,151.82,152.33,12.62">BIDAL@imageCLEFlifelog2019: The Role of Content and Context of Daily Activities in Insights from Lifelogs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,157.35,189.49,63.23,8.74"><forename type="first">Minh-Son</forename><surname>Dao</surname></persName>
							<email>dao@nict.go.jp</email>
							<affiliation key="aff0">
								<orgName type="department">of Information and Communications Technology</orgName>
								<orgName type="laboratory">Big Data Analytics Laboratory</orgName>
								<orgName type="institution">National Institute</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.14,189.49,60.05,8.74"><forename type="first">Anh-Khoa</forename><surname>Vo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Information Technology University of Science</orgName>
								<orgName type="institution">VNU-HCMC</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.75,189.49,71.70,8.74"><forename type="first">Trong-Dat</forename><surname>Phan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Information Technology University of Science</orgName>
								<orgName type="institution">VNU-HCMC</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,403.39,189.49,50.14,8.74"><forename type="first">Koji</forename><surname>Zettsu</surname></persName>
							<email>zettsu@nict.go.jp</email>
							<affiliation key="aff0">
								<orgName type="department">of Information and Communications Technology</orgName>
								<orgName type="laboratory">Big Data Analytics Laboratory</orgName>
								<orgName type="institution">National Institute</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.46,115.96,314.45,12.62;1,156.09,133.89,303.17,12.62;1,231.52,151.82,152.33,12.62">BIDAL@imageCLEFlifelog2019: The Role of Content and Context of Daily Activities in Insights from Lifelogs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">70B084591E341F2F04A31D99F5BC4228</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>lifelog</term>
					<term>content and context</term>
					<term>watershed</term>
					<term>image retrieval</term>
					<term>image rearrange</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>imageCLEFlifelog2019 introduces two exciting challenges of getting insights from lifelogs. Both challenges aim to have a memory assistant that can accurately bring a memory back to a human when necessary. In this paper, two new methods to tackle these challenges by leveraging the content and context of daily activities are introduced. Two backbone hypotheses are built based on observations: (1) under the same context (e.g., a particular activity), one image should have at least one associative image (e.g., same content, same concepts) taken from different moments. Thus, a given set of images can be rearranged chronologically by ordering their associative images whose orders are known precisely, and (2) a sequence of images taken during a specific period can share the same context and content. Thus, if a set of images can be clustered into sequential atomic clusters, given an image, it is possible to automatically find all images sharing the same content and context by first finding the atomic cluster sharing the same content, then watershed reward and forward to find other clusters sharing the same context. The proposed methods are evaluated on the imageCLEFlifelog 2019 dataset and compared to participants joined this event. The experimental results confirm the high productivity of the proposed method in both stable and accuracy aspects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction generated by lifelogging. The small sizes and affordable prices of wearable sensors, high bandwidth of the Internet, and flexible cloud storages encourage people to lifelogging more frequent than before. That leads to the fact that people can have more opportunities to understand their lives thoroughly due to daily recording data whose content conceal both cognitive and physiological information. One of the most exciting topics when trying to get insights from lifelogs is to understand human activities from the first-person perspective <ref type="bibr" coords="2,413.14,190.72,12.10,8.74" target="#b5">[6]</ref> <ref type="bibr" coords="2,425.24,190.72,12.10,8.74" target="#b6">[7]</ref>. Another interesting topic is to augment human memory towards improving human capacity to remember <ref type="bibr" coords="2,216.31,214.64,18.92,8.74" target="#b9">[10]</ref>. The former aims to understand how people act daily towards having effective and efficient support to improve the qualification of living both in social and physical activities. The latter tries to create a memory assistant that can accurately and quickly bring a memory back to a human when necessary.</p><p>In order to encourage people to pay more attention to the topics above, several events have been organized <ref type="bibr" coords="2,267.06,286.37,11.78,8.74" target="#b7">[8]</ref> <ref type="bibr" coords="2,278.84,286.37,11.78,8.74" target="#b8">[9]</ref>[2] <ref type="bibr" coords="2,302.39,286.37,11.78,8.74">[4]</ref>. These events offer a large annotated lifelog collected from various sensors such as physiology (e.g., heartbeat, step counts), images (e.g., lifelog camera), location (e.g., GPS), users tags, smartphone logs, and computer logs. A series of tasks introduced throughout these events started attracting people leading to an increase in the number of participants. Unfortunately, the results of proposed solutions from participants are far from expectation. It means that lifelogging still a mystical land that needs to be discovered more both in what kind of insights people can extract from lifelogs and how the accuracy of these insights are.</p><p>Along this direction, imageCLEFlifelog2019 -a session of imageCLEF2019 <ref type="bibr" coords="2,460.89,393.96,20.61,8.74" target="#b10">[11]</ref> -is organized with two tasks <ref type="bibr" coords="2,260.62,405.92,11.75,8.74" target="#b2">[3]</ref>: <ref type="bibr" coords="2,280.26,405.92,12.73,8.74" target="#b0">(1)</ref> solve my puzzle: the new task introduced this time, and (2) lifelog moment retrieval: the old one with some modification to make it more difficult and enjoyable than before. In this paper, two new methods are introduced to tackle two tasks mentioned. The basement of the proposed methods is built by utilizing the association of contents and contexts of daily activities. Two backbone hypotheses are built based on this basement:</p><p>1. Under the same context (e.g., a particular activity), one image should have at least one associative image (e.g., same content, same concepts) taken from different moments. Thus, a given set of images can be rearranged chronologically by ordering their associative images whose orders are known precisely, and 2. A sequence of images taken during a specific period can share the same context and content. Thus, if a set of images can be clustered into sequential atomic clusters, given an image, it is possible to automatically find all images sharing the same content and context by first finding the atomic cluster sharing the same content, then watershed reward and forward to find other clusters sharing the same context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task 1: Solve my puzzle</head><p>In this section, we introduce the proposed method, as well as its detail explanation, algorithms, and examples related to the Task 1. Task 1: solve my life puzzle is stated as: Given a set of lifelogging images with associated metadata such as biometrics and location, but no timestamps, these images are required to be rearranged in chronological order and predict the correct day (e.g., Monday or Sunday) and part of the day (morning, afternoon, or evening) <ref type="bibr" coords="3,181.16,217.46,12.58,8.74" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">From Chaos to Order</head><p>We build our solution based on one of the characteristics of lifelogs: activities of daily living (ADLs) <ref type="bibr" coords="3,228.77,283.94,13.88,8.74" target="#b4">[5]</ref>. Some common contents and contexts people live in every day can be utilized to associate unordered time images to ordered time images. For example, one always prepares and has breakfast in a kitchen every morning from 6 am to 8 am, except for a weekend. Hence, all record r d i recorded from 6 am to 8 am in the kitchen are shared the same context (i.e., in a kitchen, chronological order of sequential concepts) and content (e.g., objects in a kitchen, foods). If we can associate one-by-one each image of a set of unordered time images r q i to a subset of ordered time images r d i , r q i can totally be ordered by the order of r d i . This above observation brings the following useful hints:</p><p>-If one record=(image, metadata) r q captured in the scope of ADLs, there probably is a record r d sharing the same content and context. -If we can find all associative pairs (r d i , r q i ), we can rearrange r q i by rearranging r d i utilizing metadata of r d i , especially timestamps and locations.</p><p>We call r q and r d a query record/image and associative record/image (of the query record/image), respectively, if similary(r d , r q ) &gt; θ (the predefined threshold). We call a pair of (r d , r q ) an associative pair if it satisfies the condition similary(r d , r q ) &gt; θ. Hence, we propose a solution as follows: Given sets of unordered images Q = {r q i } and ordered images D = r d i , we</p><p>1. Find all associative pairs (r d , r q ) by using the similarity function (described in subsection 3.2). The output of this stage is a set of associative pairs ordering by timestamps of D, call P . Next, P is refined to generate P T OU to guarantee that there is only one associate pair remained with the highest similarity score among consecutive associative pairs of the same query image. Algorithm 1 models this task. Fig. <ref type="figure" coords="3,308.52,595.87,3.87,8.74">1</ref>, the first and second rows, illustrates how this task works. 2. Extract all patterns pat of associative pair to create P U T OU . The pattern is defined as the set of associative pairs so that the query images set is precisely the same as the images of Q. Algorithm 2 models this task. Figure <ref type="figure" coords="3,454.98,644.16,3.87,8.74">1</ref>, the third row 27 denotes the process of extracting patterns.</p><p>3. Find the best pattern P BEST that has the highest average similarity score comparing to the rest. Algorithm 3 models this task. Fig. <ref type="figure" coords="4,410.24,130.95,3.87,8.74">1</ref>, the third row denotes the process of selecting the best pattern. 4. The unordered images Q is ordered by order of associative images of P BEST .</p><p>Fig. <ref type="figure" coords="4,172.04,166.71,3.87,8.74">1</ref>, the red rectangle at the left-bottom corner illustrates this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Similarity Functions</head><p>The similarity function to measure the similarity between a query record and its potentially associative record has two different versions built as follows:</p><p>1. Color histogram vector and Chi-square: The method introduced by Adrian Rosebrock<ref type="foot" coords="4,230.34,259.28,3.97,6.12" target="#foot_1">3</ref> is utilized to build the histogram vector. As mentioned above, the environment of images taken by lifelogging can be repeated daily, both indoor and outdoor. Hence, the color information probably is the primary cue to find similar images captured under the same context. 2. Category vector and FAISS: The category vector is concerned as a deep learning feature to overcome problems that handcraft features cannot do. The ResNet18 of the pre-trained model PlaceCNN<ref type="foot" coords="4,372.04,330.91,3.97,6.12" target="#foot_2">4</ref> is utilized to create the category vector. The 512-dimension vector extracted from the "avgpool" layer of the ResNet18 is used. The reason such a vector is used is to search images sharing the same context (spatial dimension). The FAISS (Facebook AI Similarity Search) <ref type="foot" coords="4,244.92,378.73,3.97,6.12" target="#foot_3">5</ref> is utilized to push the speed of searching for similar images due to its high productivity of similarity searching and clustering dense vectors. </p><formula xml:id="formula_0" coords="5,140.99,367.17,273.40,24.70">-∀m : time(p[m].iD) &lt; time(p[m + 1].iD) (i.e., time-ordered) -1 k k i=1 (similarity(d[p[i].iD], q[p[i].iQ]</formula><p>)) ⇒ M AX (i.e., associative) Let θ, α, and β denote the similarity threshold, the number of days in the data set, and the number of clusters within one day (i.e. parts of the day), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task 2: Lifelog Moment Retrieval</head><p>In this section, we introduce the proposed method, as well as its detail explanation, algorithms, and examples related to Task 2.</p><p>Task 2: Lifelog moment retrieval is stated as <ref type="bibr" coords="5,339.19,500.75,11.10,8.74" target="#b2">[3]</ref>: Retrieve a number of specific predefined activities in a lifelogger's life.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Interactive-Watershed-based for Lifelog Moment Retrieval</head><p>The proposed method is built based on the following observation: A sequence of images taken during a specific period can share the same context and content. Thus, if a set of images can be clustered into sequential atomic clusters, given an image, we can automatically find all images sharing the same content and context by first finding the atomic cluster sharing the same content, then watershed reward and forward to find other clusters sharing the same context. The terminology atomic cluster is understood that all images inside should share the similarity higher than the predefined threshold and must share the same context (e.g., location, time). Based on the above discussion, the Algorithm 1 is built to find a Lifelog moment from a dataset defined be a query events text. Following is the description to explain the Algorithm 1.</p><p>-Stage 1 (Offline): This stage aims to cluster a dataset into the atomic clusters. The category vector V c and attribute vector V a extracted from images are utilized to build the similarity function. Besides, each image is analyzed by using I2BoW function to extract its BoW contains concepts and attributes reserved for later processing. -Stage 2 (Online): This stage targets to find all clusters satisfied a given query. Since the given query is described by text, a text-based query method must be used for finding related images. Thus, we create Pos tag, Google-SearchAPI, and Filter functions to find the best BoWs that represent the taxonomy of the querys context and content. In order to prune the output of these functions, we utilize the Interactive function to select the best taxonomy. First, images queried by using BoW (i.e., seeds) are utilized for finding all clusters, namely LMRT1, that contain these images. These clusters are then hidden for the next step. Next, these seeds are used to query on the rest clusters (i.e., unhidden clusters) to find the second set of seeds. These second set of seeds are used to find all clusters, namely LRMT2, that contain these seeds. The final output is the union of C1 and C2. At this step, we might apply Interactive function to refine the output (e.g., select clusters manually). Consequently, all lifelog moment satisfied the query are found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Functions</head><p>In this subsection, the significant functions utilized in the proposed method are introduced, as follows:</p><p>-Pos tag: processes a sequence of words tokenized from a given set of sentences, and attaches a part of speech tag (e.g., noun, verb, adjective, adverb) to each word. The library NLTK<ref type="foot" coords="7,294.44,248.80,3.97,6.12" target="#foot_4">6</ref> is utilized to build this function.</p><p>-featureExtractor: analyzes an image and return a pair of vectors v c (category vector, 512 dimension) and v a (attribute vector, 102 dimension). The former is extracted from the "avgpool" layer of the RestNet18 of the pretrained model PlaceCNN <ref type="bibr" coords="7,263.25,298.32,14.61,8.74" target="#b12">[13]</ref>. The latter is calculated by using the equation</p><formula xml:id="formula_1" coords="7,151.70,308.70,131.18,12.19">v a = W T a v c , introduced in [12]</formula><p>, where W a is the weight of Learnable Transformation Matrix.</p><p>-similarity: measures the similarity between two vectors using the cosine similarity. -I2BoW: converts an image into a bag of words using the method introduced in <ref type="bibr" coords="7,162.73,370.32,9.96,8.74" target="#b0">[1]</ref>. The detector developed by the authors return concepts, attribute, and relation vocab. Nevertheless, only a pair of attribute and concept is used for building I2BoW. -GoogleSearchAPI: enriches a given set of words by using Google Search API <ref type="foot" coords="7,172.30,416.70,3.97,6.12" target="#foot_5">7</ref> . The output of this function is the set of words that could be probably similar to the queried words under certain concepts. -Interactive: allows users to interfere with refining the results generated by related functions. -Query: finds all items of the searching dataset that are similar to queried items. This function can adjust its similarity function depending on the type of input data. -cluster: clusters images into sequential clusters so that all images of one cluster must share the highest similarity comparing to its neighbors. All images are sorted by time before being clustered. Algorithm 2 describes this function in detail. Figure <ref type="figure" coords="7,264.22,538.22,4.24,8.74" target="#fig_3">3</ref>.1 illustrates how this function works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameter Definitions</head><p>Let I = {I i } i=1..N denote the set of given images (e.g., dataset). Let F = {(V c i , V a i )} i=1..N denote the set of feature vectors extracted from I.</p><p>Let C = {C k } denote a set of atomic clusters.</p><p>Let {BoW Ii } i=1..N denote the set of BoWs; each of them is a BoW built by using the I2BoW function.</p><p>Let {BoW QT } , BoW N oun QT , BoW Aug QT denote the Bag of Words extracted from the query, the NOUN part of BoW QT , and the augmented part of BoW QT , respectively.</p><p>Let Seed i j and LM RT k denote a set of seeds and lifelog moments, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section, datasets and evaluation metrics used to evaluate the proposed solution are introduced. Besides, the comparison of our results with others from different participants is also discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>We use the dataset released by the imageCLEFlifelog 2019 challenge <ref type="bibr" coords="8,442.56,322.62,12.20,8.74" target="#b2">[3]</ref>. The challenge introduces an entirely new rich multimodal dataset which consists of 29 days of data from one lifeloggers. The dataset contains images (1,500-2,500 per day from wearable cameras), visual concepts (automatically extracted visual concepts with varying rates of accuracy), semantic content (semantic locations, semantic activities) based on sensor readings (via the Moves App) on mobile devices, biometrics information (heart rate, galvanic skin response, calorie burn, steps, continual blood glucose, etc.), music listening history, computer usage (frequency of typed words via the keyboard and information consumed on the computer via ASR of on-screen activity on a per-minute basis).</p><p>Generally, the organizers of the challenge do not release the ground truth for the test set. Instead, participants must send their arrangement to the organizers and get back their evaluation.</p><p>Task 1: Solve my puzzle Two training and testing query sets are given. Each of them has a total of 10 sets of images. Each set has 25 unordered images need to be rearranged. The training set has its ground truth to let participants can evaluate their solution.</p><p>For evaluating, the Kendall rank correlation coefficient is utilized to evaluate the similarity between the arrangement and the ground truth. Then, the mean of the accuracy of the prediction of which part of the day the image belongs to and the Kendall's Tau coefficient is calculated to have the final score.  <ref type="table" coords="8,443.08,620.25,4.98,8.74" target="#tab_2">3</ref> and<ref type="table" coords="8,472.85,620.25,3.87,8.74" target="#tab_4">4</ref>, respectively. Besides, more descriptions and constraints are also listed to guide participants on how to understand queries precisely.</p><p>The evaluation metrics are defined by imageCLEFlifelog 2019 as follows:</p><p>-Cluster Recall at X (CR@X): a metric that assesses how many different clusters from the ground truth are represented among the top X results, -Precision at X (P@X): measures the number of relevant photos among the top X results, -F1-measure at X (F1@X): the harmonic mean of the previous two.</p><p>X is chosen as 10 for evaluation and comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation and Comparison</head><p>Task 1: Solve my puzzle Although eleven runs were submitted to evaluate, only the best five runs are introduced and discussed in this paper: (1) only color histogram vector, (2) color histogram vector and the constraint of timezone (e.g., only places in Dublin, Ireland) (3) object search (i.e., using visual concepts of metadata), and category vector plus the constraint of timezone, (4) timezone is strictly narrowed into only working days and inside Dublin, Ireland, and (5) only category vector, and the constraint of timezone (i.e., only places in Dublin, Ireland).</p><p>The reason we reluctantly integrate places information to the similarity function due to the uncertainty of metadata given by the organizer. Records that have location data (e.g., GPS, place names) occupy only 14.90%. Besides, most of the location data give wrong information that can lead to similarity scores coverage in a wrong optimal peak.</p><p>As described in Table <ref type="table" coords="9,249.67,404.33,3.87,8.74" target="#tab_0">1</ref>, the similarity function using only color histogram gave the worst results compared to those who use the category vector. Since users are living in Dublin, Ireland, the same daily activities should share the same context and content. Nevertheless, if users go abroad or other cities, this context and content will be changed totally. Hence, the timezone could be seen as the majority factor that can improve the final accuracy since it constraints the context of searching scope. As mentioned above, the metadata does not usually contain useful data that can be leveraged the accuracy of searching. Thus, when utilizing object concepts as a factor for measuring the similarity, it can pull down the accuracy comparing to not using them at all. Distinguish between working day and weekend activities depending totally on types of activities. In this case, the number of activities that happen every day is more significant than those that happen only on working days. That explains the higher accuracy of ignoring the working day/weekend factor. In general, the similarity funtion integrated timezone (i.e., daily activities context) and category vector (i.e., rich content) gives the best accuracy compared to others. It should be noted that the training query sets are built by picking up exactly images from dataset while the testing query sets not appear in the dataset at all. That explains why the accuracy of the proposed method is perfect when running on training query sets.</p><p>Four participants submitted their outputs for evaluating (1) DAMILAB, (2) HCMUS (Vietnam National University in HCM city), ( <ref type="formula" coords="9,382.76,644.16,4.24,8.74">3</ref>) BIDAL (ourselves), and ( <ref type="formula" coords="9,158.84,656.12,4.24,8.74">4</ref>) DCU (Dublin City University, Ireland). Table <ref type="table" coords="9,376.49,656.12,4.98,8.74" target="#tab_1">2</ref>  in primary scores between results generated by methods proposed by these participants and the proposed method. Although the final score of the proposed method is lower than of HCMUS, the proposed method might be more stable than others. In other words, the variance of accuracy scores when rearranging ten different queries of the proposed method is less than others. Hence, the proposed method can cope with underfitting, overfitting and bias problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>denotes the difference</head><p>Task 2: Lifelog Moment Retrieval Although three runs were submitted to evaluate, two best runs are chosen to introduce and discuss in this paper: (1) run 1: interactive mode. In this run, interactive functions are activated to let users interfere and manually get rid of those images that do not relevant to a query, (2) run 2: automatic mode. In this run, a program runs without any interfere from users. Table <ref type="table" coords="10,178.43,608.30,4.98,8.74" target="#tab_2">3</ref> and 4 show the results running on the training and testing sets, respectively. Opposite to the results of Task 1, the results of Task 2 do not have much difference between training and testing stages. That could lead to the conclusion that the proposed method probably is stable and robust enough to cope with different types of queries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event</head><p>Run 1 Run 2 P@10 CR@10 F1@10 P@10 CR@10 F1@10 01. Ice cream by the Sea In all cases, the P@10 results are very high. It proves that the approach used for querying seeds of the proposed method is useful and precise. Moreover, the watershed-based stage after finding seeds can help not only to decrease the complexity of querying related images but also to increase the accuracy of event boundaries. Unfortunately, the CR@10 results are less accuracy comparing to P@10. The reason could come from merging clusters. Currently, clusters gained after running watershed are not merged and rearranged. That could lead to low accuracy when evaluating CR@X. This issue is investigated thoroughly in the future.</p><p>Nevertheless, misunderstanding context and content of queries sometimes lead to the worst results. For example, both runs failed in query 2 "driving home" and query 9 "wearing a red plaid shirt." The former was understood as "driving from office to home regardless of how many times stop at in-middle places," and the latter was distracted by synonym words of "plaid shirt" when leveraging GoogeSearchAPI to augmented the BoW. The first case should be understood that "driving home from the last stop before home," and the second case should focus on only "plaid shirt" not "sweater" nor "fannel." After fixing these mistakes, both runs have higher scores on query 2 and query 9, as described in Table <ref type="table" coords="11,217.39,543.25,3.87,8.74" target="#tab_4">4</ref>. The second rows of event 2, 9, and the average score show the results after correcting the mentioned misunderstanding. Hence, building a flexible mechanism to automatically build a useful taxonomy from a given query to avoid these mistakes is built in the future.</p><p>Nine teams participated to Task 2 included (1) HCMUS, (2) ZJUTCVR, (3) BIDAL (ourselves), (4) DCU, (5) ATS, (6) REGIMLAB, (7) TUCMI, (8) UAPT, and ( <ref type="formula" coords="11,194.47,620.25,4.24,8.74">9</ref>) UPB. The detail information of these teams could be referred to in <ref type="bibr" coords="11,160.33,632.21,9.96,8.74" target="#b2">[3]</ref>. Table <ref type="table" coords="11,206.10,632.21,4.98,8.74" target="#tab_6">5</ref> denotes the comparison among these teams. We are ranked in the third position. In general, the proposed method can find all events with acceptance accuracy (i.e., no event with zero F1@10 scores comparing to others </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event</head><p>Run 1 Run 2 P@10 CR@10 F1@10 P@10 CR@10 F1@10 01. In a Toyshop those have at least one event with zero F1@10 scores). That confirms again the stability and anti-bias of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We introduce two methods for tackling two tasks challenged by imageCLE-Flifelog 2019: (1) solve my puzzle, and (2) lifelog moment retrieval. Although each method is built differently, the backbone of these methods is the same: considering the association of content and context of daily activities. The first method is built for augmenting human memory when the chaos of memory snapshots must be rearranged chronologically to give a whole picture of a users life moment. The second method is constructed to visualize peoples memories from their explanation: start from their pinpoints of memory and watershed to get all image clusters around those pinpoints. The proposed method is thoroughly evaluated by the benchmark dataset provided by the imageCLEFlifelog 2019, and compared with other solutions coming from different teams. The final results show that the proposed method is developed in the right direction even though it needs more improvement to reach the expected targets. Many issues are raised during the experimental results and need to be investigated further for better results. In general, two issues should be investigated more in the future: (1) Similarity functions and watershed boundaries, and (2) A taxonomy generated from queries. </p><formula xml:id="formula_2" coords="15,134.77,171.55,277.98,62.68">Input: Q = {q[k]}, D = {d[l]}, PT OU [i][j], α (#days), β (#clusters) Output: PUT OU [i][j][z] 1: slidingW indow = Q 2: for i = 1..α do 3: z = 1, N = PT OU [i][j] 4:</formula><p>for j = 1..β do 5:</p><p>for n = 1.  for i = 1..α do 4:</p><formula xml:id="formula_3" coords="15,138.66,503.97,193.59,18.82">CoverSet[i] ⇐ CoverSet ∪ PST OAU [i][j] 5:</formula><p>end for 6:</p><p>Find the most common pattern pat from all patterns contained in CoverSet 7:</p><p>Delete all subsets of CoverSet that do not match pat 8:</p><p>For each remained subset of CoverSet, calculate the average similarity with Q 9:</p><p>Find the subset CoverSet largest that has the largest average similarity with Q 10:</p><p>PBEST CLU ST ER <ref type="bibr" coords="15,223.73,572.76,12.68,5.24">[j]</ref> ⇐ CoverSet largest 11: end for 12: Find the subset PBEST of PBEST CLU ST ER <ref type="bibr" coords="15,317.06,594.68,12.68,5.24">[j]</ref> that has the largest average similarity with Q 13: return PBEST  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,323.04,345.83,8.31;5,134.77,334.02,182.36,8.35"><head>2. 3 Fig. 1 .</head><label>31</label><figDesc>Fig. 1. Task 1: An example of finding PT OU , PUT OU , and PBEST where Q = {A, B, C}, {ai, bj, c k } ∈ D are ordered by time, θ = 0.24</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,134.77,389.48,345.83,7.89;6,134.77,400.47,95.53,7.86;6,134.77,115.84,345.83,258.87"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An Overview of the Proposed Method for Task 2: Interactive watershed-based lifelog moment retrieval</figDesc><graphic coords="6,134.77,115.84,345.83,258.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,134.77,608.27,345.83,8.77;8,134.77,620.25,303.95,8.74"><head>Task 2 :</head><label>2</label><figDesc>Lifelog Moment Retrieval The training and testing set stored with the JSON format, have ten queries whose titles are listed in Tables</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="15,134.77,435.42,224.64,8.77;15,134.77,449.15,293.42,8.31;15,134.77,460.11,68.08,7.89;15,138.66,471.07,74.00,7.89;15,138.66,482.05,79.60,7.86;15,138.66,493.01,7.17,7.86"><head>Algorithm 3</head><label>3</label><figDesc>Find the BEST set of UTOU imagesInput: Q = {q[k]}, D = {d[l]}, PUT OU [i][j][z], α (#days), β (#clusters) Output: PBEST 1: for j = 1..β do 2:CoverSet ⇐ ∅ 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="16,134.77,121.52,313.15,8.77;16,134.77,135.25,127.05,7.89;16,261.81,140.05,22.23,5.24;16,134.77,146.21,70.01,7.89;16,150.43,157.19,50.68,7.86;16,138.66,168.15,40.24,7.86;16,178.90,173.13,2.66,4.37;16,182.55,168.15,4.61,7.86;16,187.16,172.95,22.23,5.24;16,213.09,168.15,16.38,7.86;16,138.66,179.11,62.29,7.86;16,200.95,183.89,22.23,5.24;16,226.88,179.11,16.38,7.86;16,138.66,190.07,82.45,7.86;16,221.10,195.05,2.66,4.37;16,227.32,190.07,57.33,7.86;16,138.66,201.03,201.86,7.86;16,138.66,211.99,97.44,7.86;16,236.10,216.77,22.23,5.24;16,259.46,211.99,81.29,7.86;16,150.43,222.94,45.56,7.86;16,138.66,233.90,32.36,7.86;16,172.29,232.14,19.10,5.24;16,171.01,233.90,114.68,9.15;16,138.66,246.27,32.36,7.86;16,172.29,244.06,13.56,5.24;16,171.01,250.90,10.15,5.24;16,189.13,246.27,111.80,7.86;16,302.21,244.50,19.10,5.24;16,300.93,246.27,24.46,9.15;16,138.66,259.35,32.36,7.86;16,172.29,257.58,19.10,5.24;16,171.01,259.35,84.62,9.15;16,256.91,257.14,13.56,5.24;16,255.63,263.98,10.15,5.24;16,273.23,259.35,63.86,7.86;16,138.66,271.99,129.23,7.86;16,269.16,270.23,19.10,5.24;16,267.88,271.99,81.81,9.15;16,134.77,284.24,41.68,7.86;16,176.44,282.47,3.65,5.24;16,176.44,284.24,139.10,9.03;16,150.43,295.20,28.46,7.86;16,178.90,300.18,2.66,4.37;16,182.55,295.20,4.61,7.86;16,187.16,300.00,22.23,5.24;16,210.53,295.20,7.17,7.86;16,134.77,307.17,44.94,7.86;16,180.98,305.40,3.65,5.24;16,187.69,307.17,82.79,8.35;16,270.48,305.40,3.65,5.24;16,270.48,307.17,40.61,9.03;16,311.10,305.40,3.65,5.24;16,311.10,307.17,35.78,9.03;16,134.77,319.56,27.57,7.86;16,162.97,317.80,13.72,5.24;16,162.33,319.56,96.44,9.22;16,260.06,317.80,3.65,5.24;16,134.77,330.52,41.68,7.86;16,176.44,328.76,3.65,5.24;16,176.44,330.52,79.31,9.03;16,255.75,328.76,3.65,5.24;16,255.75,330.52,26.78,9.03;16,283.17,328.76,13.72,5.24;16,282.53,330.52,23.05,9.22;16,134.77,343.47,44.94,7.86;16,180.98,341.71,3.65,5.24;16,187.69,343.47,81.12,8.35;16,268.81,341.71,3.65,5.24;16,268.81,343.47,40.61,9.03;16,309.42,341.71,3.65,5.24;16,309.42,343.47,26.60,9.03;16,336.66,341.71,13.72,5.24;16,336.02,343.47,19.46,9.22;16,134.76,355.87,89.11,7.86;16,225.15,354.10,3.65,5.24;16,231.35,355.87,36.74,7.86;16,269.37,354.10,3.65,5.24;16,134.76,366.83,149.19,7.86;16,134.76,377.76,80.62,7.89;16,134.77,404.60,93.30,8.77;16,134.77,418.33,66.79,7.89;16,201.55,423.14,22.23,5.24;16,226.46,418.36,74.06,7.86;16,300.52,423.14,22.23,5.24;16,325.43,418.36,24.63,8.35;16,134.77,429.29,61.83,8.37;16,138.66,440.28,41.41,7.86;16,138.66,451.24,76.22,8.35;16,138.66,462.20,15.80,7.86;16,155.16,460.43,16.75,5.24;16,174.97,462.20,84.10,8.35;16,138.66,473.13,41.36,7.89;16,138.66,484.11,54.74,7.86;16,194.10,482.35,16.75,5.24;16,211.35,484.11,33.29,7.86;16,138.66,495.07,54.05,7.86;16,193.41,493.30,16.75,5.24;16,210.66,495.07,32.40,7.86;16,138.66,506.03,63.19,7.86;16,202.55,504.26,16.75,5.24;16,219.79,506.03,17.02,7.86;16,138.66,516.99,25.77,7.86;16,165.12,515.22,16.75,5.24;16,184.93,516.99,15.80,7.86;16,201.44,515.22,16.75,5.24;16,220.73,516.99,13.24,7.86;16,234.68,515.22,16.75,5.24;16,251.93,516.99,9.73,7.86;16,138.66,527.92,53.30,7.89"><head>Algorithm 4 A 6 : 8 :⇐ 1 j,Seed 2 j⇐ 2 j</head><label>468122</label><figDesc>Interactive Watershed-based Lifelog Moment Retrieval Input: QT , BoWConcepts, {Ii} i=1..N Output: LM RT {OFFLINE} 1: {BoWIi } i=1..N ⇐ ∅ 2: {(V ci, V ai)} i=1..N ⇐ ∅ 3: ∀i ∈ [1..N ], BoWI i ⇐ I2BoW (Ii) 4: ∀i ∈ [1..N ], (V ci, V ai) ⇐ f eatureExtractor(Ii) 5: {Cm} ⇐ cluster({Ii} i=1..N ) using {(V ci, V ai)} {ONLINE} BoW N oun QT ⇐ P os tag(QT ).N oun 7: BoW Aug QT ⇐ GoogleSearchAP I(BoW N oun QT ) BoW N oun QT ⇐ F ilter(BoW Aug QT ∩ BoWConcepts) 9: BoWQT ⇐ Interactive(BoW N oun QT , P os tag(QT )) 10: Seed 1 j Interactive(Query(BoWQT , {BoWI i } i=1..N )) 11: LM RT 1 ⇐ C k |∀j ∈ Seed Seed 1 j ∈ {C k } 12: {C rem l } ⇐ {Cm} -LM RT 1 13: Query( Seed 1 j , {C rem l }) 14: LM RT 2 ⇐ C l |∀j ∈ Seed LM RT ⇐ LM RT 1 ∪ LM RT2 16: LM RT ⇐ Intearactive(LM RT ) 17: return LM RT Algorithm 5 cluster Input: I = {Ii} i=1..N , F = {(V ci, V ai)} i=1..N , θa, θ b Output: {C k } 1: k ←-0 2: C = {C k } ←-∅ 3: I temp ⇐ SORT bytime (I, F ) 4: repeat 5: va ←-I temp .F.V a[0] 6: vc ←-I temp .F.V c[0] 7: C[0] ⇐ ∪I temp .I[0] 8: I temp ⇐ I temp -I temp [0] 9:for i=1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,134.77,115.84,345.83,192.43"><head></head><label></label><figDesc></figDesc><graphic coords="5,134.77,115.84,345.83,192.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,146.24,116.41,322.87,96.91"><head>Table 1 .</head><label>1</label><figDesc>Task 1: An Evaluation of Training and Testing Sets (β ⇐ 4, θ ⇐ 0.24)</figDesc><table coords="10,178.68,137.68,257.84,75.64"><row><cell>Run</cell><cell>Parameters</cell><cell cols="2">Primary Score Training Testing</cell></row><row><cell>1</cell><cell>color hist</cell><cell>1.000</cell><cell>0.208</cell></row><row><cell>2</cell><cell>time zone+color hist</cell><cell>1.000</cell><cell>0.248</cell></row><row><cell cols="3">3 time zone+object serach+category vec 1.000</cell><cell>0.294</cell></row><row><cell cols="2">4 time zone+working day+category vec</cell><cell>1.000</cell><cell>0.335</cell></row><row><cell>5</cell><cell>time zone+category vec</cell><cell>1.000</cell><cell>0.372</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,199.54,227.97,216.28,178.50"><head>Table 2 .</head><label>2</label><figDesc>Task 1: A Comparison with Other Methods</figDesc><table coords="10,204.13,258.70,207.09,147.77"><row><cell>Query ID</cell><cell cols="4">Primary score BIDAL DAMILAB DCU HCMUS</cell></row><row><cell>1</cell><cell>0.490</cell><cell>0.220</cell><cell cols="2">0.377 0.673</cell></row><row><cell>2</cell><cell>0.260</cell><cell>0.200</cell><cell cols="2">0.240 0.380</cell></row><row><cell>3</cell><cell>0.310</cell><cell>0.153</cell><cell cols="2">0.300 0.530</cell></row><row><cell>4</cell><cell>0.230</cell><cell>0.220</cell><cell cols="2">0.233 0.453</cell></row><row><cell>5</cell><cell>0.453</cell><cell>0.200</cell><cell cols="2">0.240 0.617</cell></row><row><cell>6</cell><cell>0.447</cell><cell>0.320</cell><cell cols="2">0.240 0.817</cell></row><row><cell>7</cell><cell>0.437</cell><cell>0.293</cell><cell cols="2">0.360 0.877</cell></row><row><cell>8</cell><cell>0.300</cell><cell>0.200</cell><cell cols="2">0.300 0.883</cell></row><row><cell>9</cell><cell>0.447</cell><cell>0.300</cell><cell>0.270</cell><cell>0.280</cell></row><row><cell>10</cell><cell>0.347</cell><cell>0.357</cell><cell>0.120</cell><cell>0.020</cell></row><row><cell>Average</cell><cell>0.372</cell><cell>0.246</cell><cell cols="2">0.268 0.553</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,185.36,115.91,244.62,7.89"><head>Table 3 .</head><label>3</label><figDesc>Task 2: Evaluation of all Runs on the Training Set</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,194.54,115.91,226.29,7.89"><head>Table 4 .</head><label>4</label><figDesc>Task 2: Evaluation of all Runs on the Test set</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,140.84,116.41,333.67,220.59"><head>Table 5 .</head><label>5</label><figDesc>Task 2: Comparison to Others (Avg: descending order from left to right)</figDesc><table coords="13,140.99,137.68,332.14,199.32"><row><cell>Event ID</cell><cell>HCMUS</cell><cell>ZJUTCVR</cell><cell>BIDAL</cell><cell>DCU</cell><cell>F1@10 ATS REGIMLAB</cell><cell>TUC MI</cell><cell>UAPT</cell><cell>UPB</cell></row><row><cell>01. In a Toyshop</cell><cell cols="8">1.00 0.95 0.67 0.57 0.46 0.44 0.46 0.00 0.29</cell></row><row><cell>02. Driving home</cell><cell cols="8">0.42 0.17 0.09 0.09 0.09 0.06 0.13 0.00 0.08</cell></row><row><cell>03. Seeking Food in a Fridge</cell><cell cols="8">0.36 0.40 0.36 0.29 0.36 0.43 0.00 0.07 0.07</cell></row><row><cell>04. Watching Football</cell><cell cols="8">0.86 0.37 0.40 0.67 0.00 0.40 0.00 0.00 0.00</cell></row><row><cell>05. Coffee time</cell><cell cols="8">0.68 0.47 0.20 0.36 0.36 0.54 0.00 0.11 0.34</cell></row><row><cell>06. Having breakfast at home</cell><cell cols="8">0.00 0.35 0.19 0.17 0.26 0.00 0.18 0.14 0.00</cell></row><row><cell cols="9">07. Having coffee with two person 0.89 1.00 0.95 0.75 0.33 0.00 0.00 0.00 0.18</cell></row><row><cell>08. Using smartphone outside</cell><cell cols="8">0.32 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.00</cell></row><row><cell>09. Wearing a red plaid shirt</cell><cell cols="8">0.58 0.44 0.25 0.00 0.12 0.00 0.00 0.00 0.00</cell></row><row><cell>10. Having a meeting in China</cell><cell cols="8">1.00 0.25 0.45 0.00 0.57 0.00 0.40 0.25 0.32</cell></row><row><cell>Average Score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="15,134.76,237.31,345.83,106.94"><head></head><label></label><figDesc>.(N -slidingW indow) do 6: substr ⇐ PT OU [i][j][m] m=n..(n+slidingW indow) 7: if (∀m = n : substr[m].iQ = substr[n].iQ and substr == slidingW indow) then 8: PUT OU [i][j][z] ⇐ substr, z ⇐ z + 1</figDesc><table coords="15,134.76,292.10,71.48,51.73"><row><cell>9:</cell><cell>end if</cell></row><row><cell>10:</cell><cell>end for</cell></row><row><cell>11:</cell><cell>end for</cell></row><row><cell cols="2">12: end for</cell></row><row><cell cols="2">13: return PUT</cell></row></table><note coords="15,203.71,335.96,39.40,8.28"><p>OU [i][j][z]</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="16,134.76,526.18,345.33,131.44"><head></head><label></label><figDesc>. I temp do 10: if (similarity(va, I temp .F.V a[i]) &gt; θa and similarity(vc, I temp .F.V c[i]) &gt; θc then 11: C[k] ⇐ ∪I temp .I[i] 12: I temp ⇐ I temp -I temp [i]</figDesc><table coords="16,134.76,583.98,98.07,73.64"><row><cell>13:</cell><cell>else</cell></row><row><cell>14:</cell><cell>k ←-k + 1</cell></row><row><cell>15:</cell><cell>Break</cell></row><row><cell>16:</cell><cell>end if</cell></row><row><cell>17:</cell><cell>end for</cell></row><row><cell cols="2">18: until I temp == 0</cell></row><row><cell cols="2">19: return C</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="2,149.71,620.25,330.88,8.74;2,134.77,632.21,345.83,8.74;2,134.77,644.16,345.82,8.74;2,134.77,656.12,207.05,8.74"><p>The paper is organized as follows: Section 2 and 3 introduce the solutions for Task1 and Task 2, respectively. Section 4 describes the experimental results gotten by applying two proposed methods as well as related discussions. The last Section gives conclusions and future works.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,144.73,612.96,313.46,7.86;4,144.73,623.92,116.04,7.86"><p>https://www.pyimagesearch.com/2014/12/01/complete-guide-building-imagesearch-engine-python-opencv</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="4,137.50,633.11,3.65,5.24;4,144.73,634.88,136.90,7.86"><p>  4  https://github.com/CSAILVision/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="4,281.63,634.88,39.75,7.86;4,137.50,644.07,3.65,5.24;4,144.73,645.84,312.71,7.86;4,144.73,656.80,24.90,7.86"><p>places365 5 https://code.fb.com/data-infrastructure/faiss-a-library-for-efficient-similaritysearch</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="7,144.73,645.84,154.94,7.86"><p>https://www.nltk.org/book/ch05.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="7,144.73,656.80,197.23,7.86"><p>https://github.com/abenassi/Google-Search-API</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,645.84,337.64,7.86;12,151.52,656.80,329.07,7.86;13,282.29,329.11,192.08,7.89;13,151.52,366.75,114.75,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,166.09,656.80,314.50,7.86;13,282.29,329.11,4.72,7.89">Bottom-up and top-down attention for image captioning and visual question 0</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno>25 0.19 0.12 0.06 0.13 answering</idno>
	</analytic>
	<monogr>
		<title level="m" coord="13,210.98,366.75,26.62,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">61</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,378.34,337.64,7.86;13,151.52,389.30,329.07,7.86;13,151.52,400.26,95.81,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,151.52,389.30,282.34,7.86">Overview of imagecleflifelog 2017: Lifelog retrieval and summarization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,455.90,389.30,24.69,7.86;13,151.52,400.26,67.14,7.86">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,411.86,337.63,7.86;13,151.52,422.82,329.07,7.86;13,151.52,433.78,329.07,7.86;13,151.52,444.74,329.07,7.86;13,151.52,455.70,62.74,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,249.22,422.82,231.37,7.86;13,151.52,433.78,121.61,7.86">Overview of ImageCLEFlifelog 2019: Solve my life puzzle and Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="13,295.72,433.78,184.87,7.86;13,151.52,444.74,96.31,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,467.30,337.64,7.86;13,151.52,478.25,329.07,7.86;13,151.52,489.21,329.07,7.86;13,151.52,500.17,182.51,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,442.93,467.30,37.66,7.86;13,151.52,478.25,325.51,7.86">Overview of imagecleflifelog 2018: daily living understanding and lifelog moment retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org¿" />
	</analytic>
	<monogr>
		<title level="m" coord="13,166.42,489.21,289.05,7.86">CLEF2018 Working Notes (CEUR Workshop Proceedings). CEUR-WS</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,511.77,337.63,7.86;13,151.52,522.73,329.07,7.86;13,151.52,533.69,329.07,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,325.26,511.77,155.33,7.86;13,151.52,522.73,141.05,7.86">Leveraging content and context in understanding activities of daily living</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kasem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S H</forename><surname>Nazmudeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,311.78,522.73,168.81,7.86;13,151.52,533.69,132.42,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,545.29,337.64,7.86;13,151.52,556.25,185.51,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="13,390.88,545.29,89.72,7.86;13,151.52,556.25,156.84,7.86">Smart lifelogging: recognizing human activities using phasor</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,567.85,337.64,7.86;13,151.52,578.81,329.07,7.86;13,151.52,589.76,139.46,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,314.50,567.85,166.10,7.86;13,151.52,578.81,154.05,7.86">Activity recognition from visual lifelogs: State of the art and future challenges</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dimiccoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cartas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,328.11,578.81,152.49,7.86;13,151.52,589.76,17.82,7.86">Multimodal Behavior Analysis in the Wild</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="121" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,601.36,337.64,7.86;13,151.52,612.32,329.07,7.86;13,151.52,623.28,329.07,7.86;13,151.52,634.24,114.22,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,398.34,601.36,82.26,7.86;13,151.52,612.32,41.92,7.86">Overview of ntcir-12 lifelog task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Albatal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,445.37,612.32,35.22,7.86;13,151.52,623.28,329.07,7.86;13,151.52,634.24,30.54,7.86">Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Kando</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Kishida</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Kato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Yamamoto</surname></persName>
		</editor>
		<meeting>the 12th NTCIR Conference on Evaluation of Information Access Technologies</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="354" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,645.84,337.64,7.86;13,151.52,656.80,255.70,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,206.05,656.80,135.25,7.86">Overview of ntcir-13 lifelog-2 task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Albatal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,348.22,656.80,30.33,7.86">NTCIR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,119.67,40.16,7.86;14,203.38,119.67,13.57,7.86;14,237.54,119.67,54.68,7.86;14,312.82,119.67,13.57,7.86;14,346.99,119.67,24.59,7.86;14,392.19,119.67,12.35,7.86;14,425.14,119.67,55.45,7.86;14,151.52,130.63,31.76,7.86;14,203.22,130.63,43.27,7.86;14,266.41,130.63,37.57,7.86;14,323.92,130.63,20.73,7.86;14,364.59,130.63,35.33,7.86;14,419.84,130.61,26.99,7.89;14,466.77,130.63,13.82,7.86;14,151.52,141.59,9.22,7.86;14,193.94,141.59,19.22,7.86;14,246.37,141.59,24.58,7.86;14,304.15,141.59,176.44,7.86;14,151.52,152.55,182.79,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,425.14,119.67,55.45,7.86;14,151.52,130.63,31.76,7.86;14,203.22,130.63,39.66,7.86">Remembering through lifelogging</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Langheinrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ward</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.pmcj.2015.12.002</idno>
		<ptr target="http://dx.doi.org/10.1016/j.pmcj.2015.12.002" />
	</analytic>
	<monogr>
		<title level="j" coord="14,266.41,130.63,37.57,7.86;14,323.92,130.63,20.73,7.86;14,364.59,130.63,35.33,7.86">Pervasive Mob. Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="14" to="26" />
			<date type="published" when="2016-04">Apr 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,162.59,337.98,7.86;14,151.52,173.55,329.07,7.86;14,151.52,184.51,329.07,7.86;14,151.52,195.47,329.07,7.86;14,151.52,206.42,329.07,7.86;14,151.52,217.38,329.07,7.86;14,151.52,228.34,329.07,7.86;14,151.52,239.30,329.07,7.86;14,151.52,250.26,329.07,7.86;14,151.52,261.22,216.27,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,268.29,217.38,212.30,7.86;14,151.52,228.34,124.23,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,297.67,228.34,182.92,7.86;14,151.52,239.30,329.07,7.86;14,151.52,250.26,122.46,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="14,305.55,250.26,171.07,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="14,142.62,271.26,337.98,7.86;14,151.52,282.22,329.07,7.86;14,151.52,293.15,329.07,7.89;14,151.52,304.13,180.80,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,352.94,271.26,127.66,7.86;14,151.52,282.22,223.49,7.86">The sun attribute database: Beyond categories for deeper scene understanding</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-013-0695-z</idno>
		<ptr target="http://dx.doi.org/10.1007/s11263-013-0695-z" />
	</analytic>
	<monogr>
		<title level="j" coord="14,387.25,282.22,93.35,7.86;14,151.52,293.18,15.92,7.86">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="59" to="81" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,314.17,337.97,7.86;14,151.52,325.13,329.07,7.86;14,151.52,336.09,111.11,7.86;14,134.77,374.08,303.01,8.77;14,134.77,388.36,60.46,7.89" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,399.52,314.17,81.07,7.86;14,151.52,325.13,145.67,7.86">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,134.77,374.08,303.01,8.77;14,134.77,388.36,60.46,7.89">Algorithm 1 Create a set of time-ordered-associative (TOU) images Input: Q = {q</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,199.00,388.39,177.49,7.86;14,134.77,399.32,67.56,8.31;14,150.43,410.31,31.99,8.28;14,142.24,421.24,63.83,7.89;14,138.66,432.20,77.38,7.89" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="14,247.82,388.39,128.66,7.86;14,134.77,399.32,67.56,8.31;14,150.43,410.31,31.99,8.28;14,142.24,421.24,36.87,7.89">l}, θ, α (#days), β (#clusters) Output: PT OU [PT OU [i]: for i=1</title>
		<author>
			<persName coords=""><forename type="first">D =</forename><surname>{d</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>: for j=1..β do</note>
</biblStruct>

<biblStruct coords="14,142.24,443.18,3.58,7.86;14,170.36,443.18,280.38,7.86;14,138.66,454.14,7.17,7.86;14,170.36,454.14,126.07,7.86;14,320.55,454.14,108.09,7.86;14,216.27,465.10,16.70,7.86;14,287.36,465.10,8.16,7.86" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="14,206.78,443.18,243.95,7.86;14,138.66,454.14,7.17,7.86;14,170.36,454.14,112.42,7.86">so that only images taken on day i within cluster j is enable 5: Establish P = {p(iD, iQ)</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Filter</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>m]that ∀m &lt; n : time(d[.iD</note>
</biblStruct>

<biblStruct coords="14,315.91,465.10,29.92,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="14,337.84,465.10,4.00,7.86">q</title>
		<author>
			<orgName type="collaboration" coords="14,319.99,465.10,8.16,7.86">iD</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,361.83,465.10,20.00,7.86;14,411.24,465.08,48.82,7.89;14,478.03,465.10,2.56,7.86;14,174.71,476.06,37.12,7.86;14,250.63,476.06,18.23,7.86;14,180.32,497.98,33.46,7.86;14,138.66,508.94,7.17,7.86;14,180.32,508.91,104.99,7.89;14,203.63,519.90,51.29,7.86;14,278.60,519.90,21.08,7.86;14,278.54,530.86,20.06,7.86;14,142.62,552.77,3.93,7.86;14,191.00,552.75,25.92,7.89;14,134.77,563.73,11.78,7.86;14,191.00,563.73,101.29,7.86;14,134.77,574.69,11.78,7.86;14,181.04,574.67,25.92,7.89;14,134.77,585.65,11.78,7.86;14,171.07,585.62,134.95,7.89;14,171.07,596.61,26.10,8.28" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="14,411.24,465.08,48.82,7.89;14,478.03,465.10,2.56,7.86;14,174.71,476.06,37.12,7.86;14,250.63,476.06,18.23,7.86;14,180.32,497.98,33.46,7.86;14,138.66,508.94,7.17,7.86;14,180.32,508.91,6.19,7.89">and ∀m = : (iD, iQ)[, iQ)∀n ≤ P 8: if</title>
		<author>
			<orgName type="collaboration" coords="14,365.83,465.10,8.00,7.86">iQ</orgName>
		</author>
		<imprint/>
	</monogr>
	<note>iQ == p[n+1].) (similarity(diD], qiD], q: end if 12: Rearrange the index of P 13: end if 14: until cannot delete any item of p PT OU</note>
</biblStruct>

<biblStruct coords="14,226.23,596.61,143.09,7.86;14,384.83,596.61,23.74,7.86;14,134.77,607.54,59.00,7.89;14,134.77,618.50,49.03,7.89;14,134.77,629.46,78.16,8.31" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,340.46,596.61,28.87,7.86;14,384.83,596.61,3.96,7.86">iQ = p[1</title>
		<author>
			<persName coords=""><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,241.20,596.61,71.37,7.86;14,396.70,596.61,11.87,7.86">P must satisfy ∀n</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
	<note>iQ). end for 17: end for 18: return PT OU</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
