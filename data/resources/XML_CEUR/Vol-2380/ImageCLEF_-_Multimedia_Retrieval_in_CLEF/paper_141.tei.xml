<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,168.85,115.96,277.66,12.62;1,145.16,133.89,325.04,12.62;1,208.22,151.82,198.91,12.62">Concept detection based on multi-label classification and image captioning approach -DAMO at ImageCLEF 2019</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,188.09,189.53,34.73,8.74"><forename type="first">Jing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,233.38,189.53,34.45,8.74"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group DAMO Academy AI Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.39,189.53,40.55,8.74"><forename type="first">Chao</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group DAMO Academy AI Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,329.49,189.53,40.41,8.74"><forename type="first">Yu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group DAMO Academy AI Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.45,189.53,39.58,8.74"><forename type="first">Ying</forename><surname>Chi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group DAMO Academy AI Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.77,201.48,60.93,8.74"><forename type="first">Xuansong</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group DAMO Academy AI Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,317.64,201.48,66.47,8.74"><forename type="first">Xiansheng</forename><surname>Hua</surname></persName>
							<email>xiansheng.hxs@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group DAMO Academy AI Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,168.85,115.96,277.66,12.62;1,145.16,133.89,325.04,12.62;1,208.22,151.82,198.91,12.62">Concept detection based on multi-label classification and image captioning approach -DAMO at ImageCLEF 2019</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B35A0E8F4695A522CA22826B4644D421</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Radiology</term>
					<term>Image caption</term>
					<term>Concept detection</term>
					<term>Multi-label classification</term>
					<term>Encoder-decoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical image captioning is an important and challenging task, which covers computer vision and natural language processing. This ImageCLEF 2019 [6] Caption competition is dedicated to research this field. The purpose of this year challenge is using radiological images to detect the concepts representing the key information. In this paper, we illustrate the proposed method to address the issue, based on multilabel classification model and CNN-LSTM architecture with attention mechanism. We also perform a detailed analysis and processing for the overall dataset and demonstrate performance with the baseline in the caption prediction task. In final evaluation, we completed 9 submissions and ranked second among 12 participants with our best mean F1-score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical images, such as radiological images, are widely used in hospital diagnosis and disease treatment. The reading and summarization of medical images is usually performed by experienced medical professionals, and obtaining information from radiological medical images is a time-consuming and laborious task. Therefore, it is essential to automatically and efficiently extract vital information from medical images. ImageCLEF 2019 Caption <ref type="bibr" coords="1,365.49,567.66,15.50,8.74" target="#b10">[11]</ref> is the third year of the challenge, starting in 2017, to analyze and solve the problem of medical image caption. The organizing committee provided a large corpus of medical radiology images and UMLS (Unified Medical Language System) <ref type="bibr" coords="1,381.49,603.52,10.52,8.74" target="#b0">[1]</ref> concepts pairs, and the purpose of this task is to detect the relevant concepts based on the visual radiology images. Evaluation criteria is conducted in terms of F1-score between concepts predicted and ground truth concepts.</p><p>Inspired by the recent successes of convolutional architectures on other endto-end frameworks <ref type="bibr" coords="2,217.94,156.94,12.33,8.74" target="#b2">[3,</ref><ref type="bibr" coords="2,230.27,156.94,8.22,8.74" target="#b4">5,</ref><ref type="bibr" coords="2,238.48,156.94,12.33,8.74" target="#b13">14,</ref><ref type="bibr" coords="2,250.81,156.94,12.33,8.74" target="#b15">16]</ref>, we study convolutional architectures for the task of image concept detection. Specifically, we handle each concept sequence corresponding to each radiological image as a set of labels, and attempt to build a multi-label classification network to solve the task. Furthermore, increasing research has been devoted to image captioning, and almost all of the current proposed methods are under the framework of CNN+RNN <ref type="bibr" coords="2,406.60,216.71,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="2,419.26,216.71,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="2,434.13,216.71,12.73,8.74" target="#b14">15,</ref><ref type="bibr" coords="2,449.02,216.71,11.62,8.74" target="#b16">17]</ref>. To imitate the human visual attention mechanism, the attention module has been applied. Hence, we adopt the encoder-decoder network, in which a basic CNN is used for the vision feature extractor, and an LSTM is employed to generate sentences due to the ability of learning long term dependencies through a memory cell.</p><p>The paper is organized as follow: Section 2 describes the analysis of the overall data, Section 3 introduces the method for the concept detection task, Section 4 demonstrates the details of the experiments and results, and Section 5 discusses and concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data analysis</head><p>In the ImageCLEF 2019 Concept Detection Task, the overall dataset contains 70,786 radiology images of several medical imaging modalities. The images are collected from open access biomedical journal articles (PubMed Central) <ref type="bibr" coords="2,462.33,426.89,14.61,8.74" target="#b12">[13]</ref>, and the corresponding UMLS concepts that totals 5,528 are extracted from the original image caption. Training dataset includes 56,629 images, and the number of associated concepts is 5216. Validation dataset includes 14,157 images, and the concepts related is 3233. It is worth mentioning that the sequences of the training dataset does not include the total concepts, and 312 concepts appear only in the validation dataset.</p><p>To further understand the datasets, we performed statistical analysis to reveal the overall data distribution. The Top-10 concepts descriptions, and the statistics of the length of concept sequence corresponding to each image and the distribution of concept frequency are shown in Table <ref type="table" coords="2,368.37,548.52,4.98,8.74" target="#tab_0">1</ref> and Fig. <ref type="figure" coords="2,416.54,548.52,3.87,8.74" target="#fig_1">1</ref>, respectively. From Table <ref type="table" coords="2,190.22,560.48,3.87,8.74" target="#tab_0">1</ref>, we can see that among the 10 concepts of high frequency, the C0043299 and C1962945, or the C0040395 and C0040405 have similar meanings. Thus, we conducted correlation analysis for all pairs of concepts. See section 4.1 for details. We counted the length of the concept sequences corresponding to each images in dataset. Only one or two concepts of the sequence account for 17.09% of the overall sample. From Fig. <ref type="figure" coords="2,316.90,620.25,3.87,8.74" target="#fig_1">1</ref>, the total number of concepts with a frequency less than 3 is 2,293, accounting for 41.48% of the entire concept dictionary, while there are only 15 concepts with a frequency of more than 3,000 times.   shows the frequency distribution of all concepts, the horizontal axis represents the word frequency interval, and the word frequency less than 10 times accounts for 64.47%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We design two distinct methods to address the concept detection issue, one is to transform the issue into a multi-label classification problem, the other is to treat it as an image captioning task, using the encoder-decoder network to generate the concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-label classification approach</head><p>Since there is no strong contextual correlation between the concepts of an image, we transform this task into a multi-label classification problem. That is, an image has several labels. Let l i be the label of i-th image, as follows:</p><formula xml:id="formula_0" coords="4,244.71,140.54,231.63,9.65">l i = [c i,1 , c i,2 , . . . , c i,j , . . . c i,n ] (<label>1</label></formula><formula xml:id="formula_1" coords="4,476.34,140.54,4.24,8.74">)</formula><p>where n is the total number of labels. If the i-th image has the j-th label, the c i,j is set to 1, else 0. We utilize the latest deep learning method to solve this problem, which has achieved great success on the field of image processing, such as classification, captioning. Empirically the deeper the network is, the richer the features extracted on different levels. While the drawbacks of gradient vanishing and explosion make it difficult to converge. To overcome this problem, He et al. proposed ResNet <ref type="bibr" coords="4,467.31,233.81,9.96,8.74" target="#b2">[3]</ref>, which reformulates the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions, and had won the first prize on ImageNet competitions. We choose the pre-trained ResNet-101 model on the ImageNet dataset <ref type="bibr" coords="4,245.84,281.63,10.52,8.74" target="#b7">[8]</ref> as backbone in our multi-label classification experiment. The overall process is shown in Fig. <ref type="figure" coords="4,328.33,293.59,3.87,8.74" target="#fig_2">2</ref>. An image is firstly preprocessed to adapt to the input of the net, and feed forward to the net to get the output feature vectors. Then passing by a fully connection layer with sigmoid activation function to calculate the probability of each class. If the probability is greater than 0.5, we assert the input image belongs to that class. Finally the predicted labels obtained, which can be reflected back to the original concepts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Medical image captioning approach</head><p>Considering that the concept detection task is to generate text information from the corresponding radiology images, we attempt to address it with the CNN-RNN model framework with attention mechanism. Typically, the model that generates a sequence of concepts will use an encoder to encode the input into a fixed form and use a decoder to decode it into a sequence verbatim.</p><p>In our approach, the encoder built upon the pre-trained ResNet-101 is first applied to extract visual features from the input images. We resize the input images normalized by the mean and standard deviation to 224×224 for uniformity, and then fine-tuned the convolutional blocks on the given medical dataset with a smaller learning rate. We utilize the visual features captured by the conv 5 convolution block in the ResNet to better describe the local information. Meanwhile, the model combine soft attention mechanism <ref type="bibr" coords="5,368.78,154.86,15.50,8.74" target="#b16">[17]</ref> to dynamically select spatial characteristic of the input image.</p><p>In decoder, We apply a long short-term memory (LSTM) network <ref type="bibr" coords="5,448.15,179.13,10.52,8.74" target="#b3">[4]</ref> that produces a caption by generating one word at every time step conditioned on a context vector capturing the visual information, the previous hidden state and the previously generated concepts. After extracting visual features in CNN, we transform the encoded image to create the initial hidden state h and cell state c for the LSTM decoder. At each decode step, the encoded image and the previous hidden state is used to generate weights for each pixel in the attention network. Finally, the previous generated concept and the weighted average of the encoded image are fed to the LSTM decoder to generate the next concept with the highest score. In addition, we also perform beam search with different beam sizes instead of sampling the maximum probability words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data preprocessing</head><p>Concept association mining It has been found that some concepts have a certain relevance as they often appear simultaneously in different radiological images. Thus, we filter out the high-correlation concept combinations from the high-frequency concepts in training dataset. First, we utilize association rule mining to search for relationships between all concepts defined as a set of items C = {c 1 , c 2 , c 3 , . . . c M }, and I = {i 1 , i 2 , i 3 , . . . i N } represents a collection of all training samples, where i m is the concept sets corresponding to each image. Obviously, i N ⊂ C. The form of association rule for the concept sets X and Y can be written as: X → Y , where X ⊂ C, Y ⊂ C, and X ∩ Y = ∅. The support is the fraction of training set that contain both X and Y , and the conf idence represents the measure that how often concepts in Y appear in sample sets that contain X. We suppose σ represents the frequency of occurrence of an item-set. Specifically,</p><formula xml:id="formula_2" coords="5,234.12,534.76,246.47,52.50">support(X → Y ) = σ(X ∪ Y ) N (2) conf idence(X → Y ) = σ(X ∪ Y ) σ(X)<label>(3)</label></formula><p>Second, the concept subsets divided with support &gt; 0.02 is a total of 99, and from which we select the combinations contain with the most elements with conf idence &gt; 0.9. Finally, we define 9 different concept combinations as 9 new concepts, called concept grouping, as shown in Table <ref type="table" coords="5,380.05,632.21,3.87,8.74" target="#tab_1">2</ref>. During the training process, we replace the concepts involved with the 9 new concepts and define the dataset changed as C g , and then map them in predicted results. Data filtering It is obvious that the dataset is extremely unbalanced through the statistics above. The low-frequency concepts would not only not be learned, but bring great bias to the model. Therefore, we filter out the concepts which are indeed rare. Firstly, we pick out all the concepts which only occurs once and get the corresponding images. Then checking all the related concepts on each image one by one, if the frequencies of all related concepts are once either, the image would be moved out of the dataset. The filtered dataset denotes as D f,1 with 163 concepts and 98 images omitted. At the same time, we also roughly filter out the concepts with frequency less than 3 or 5 times defined D f,3 and D f,5 , to avoid these noises affecting the overall dataset distribution.</p><p>Data redivision Since the pre-divided training dataset provided by organizer dose not contain all the concepts need to be learned, we re-divided all the data as follows:</p><p>(a) Picking out the images form validation dataset, as mentioned above, which has the concepts that are not occurred in training dataset.</p><p>(b) Changing these images slightly by random transformation, such as mirroring, rotation, etc.</p><p>(c) Appending these transformed images to the training dataset, while the original validation dataset keeps unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image preprocess and normalization</head><p>In order to make full use of the provided data, several data augmentation operations are introduced in our experiment. Specifically, the image is firstly random flipped horizontally or vertically with a probability of 0.5, and then resized to different scale ranging from 0.6 to 1.2 with bilinear interpolation. Finally, the transformed image is random cropped into the size of 224 × 224 before input into the backbone net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training parameters</head><p>Multi-label classification method Parameters of the last fully connection layer are initialized by MSRA method <ref type="bibr" coords="7,311.06,150.39,10.52,8.74" target="#b1">[2]</ref> and the F1-score is utilized as the criteria. The batch size and the max iteration epochs are set to 64 and 100 respectively. We apply the Adam <ref type="bibr" coords="7,285.03,174.30,10.52,8.74" target="#b6">[7]</ref> optimizer to fine-tune the model with an initial learning rate of 0.001. The training procedure is shown in the Fig. <ref type="figure" coords="7,456.17,186.26,3.87,8.74">3</ref>. Fig. <ref type="figure" coords="7,200.97,380.92,7.75,8.74">3:</ref> The training procedure of multi-label classification CNN-RNN with attention mechanism method With fine-tuning the encoder, the model was trained with cross entropy loss for 30 epochs, batch size of 20 and dropout rate of 0.5. In concept generation, we set the dimensions of all hidden states and word embeddings as 512. We used the Adam optimizer and the learning rates for the CNN and the LSTM were 1e -4 and 4e -4 respectively. Early stopping was used to prevent over-fitting when performance on a validation dataset started to degrade. The best model saved was used to predict the sequence of concepts in the test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Specific description in each run</head><p>We completed a total of 9 graded submissions before the deadline, the evaluation results for our submitted runs is shown in Table <ref type="table" coords="7,351.17,578.89,4.98,8.74" target="#tab_2">3</ref> and the specific method for each run is as follows:</p><p>Run ID 27103: In this run, we applied multi-label classification model introduced in Section 3.1 and the dataset trained was filtered dataset D f,1 . We chose the pre-trained ResNet-101 model as backbone in our experiment and performed concept grouping C g in data preprocessing. Meanwhile, we applied the Adam Run ID 26786: This run we utilized the CNN-RNN architecture with attention mechanism, based on pre-trained ResNet-101 and LSTM. We used the Adam optimizer and the learning rates for the CNN and the LSTM were 1e -4 and 4e -4 respectively. In the training dataset D f,3 , concepts occurring less frequently than 3 was ignored. Early stopping was used, and the best model saved was used to predict the sequence of concepts in the test images.</p><p>Run ID 27107: We combined the predicted results of ID 27103 and ID 26786, that is, the final results in test dataset was the union of two methods for each sample.</p><p>Run ID 27106: Similarly, the final results in this run was the union of the predicted results of ID 27184 and ID 26786.</p><p>Run ID 27188&amp;26877&amp;27111&amp;27158: These process were based on ID 26786, the pre-trained ResNet-101 was used for the vision model, and an LSTM was employed to generate sentences. The dataset trained was filtered dataset D f,5 . Otherwise, we made an attempt to apply reinforcement learning <ref type="bibr" coords="8,396.16,632.21,15.50,8.74" target="#b11">[12]</ref> in decoder, and the experimental results performed well on validation dataset but were poorly effective on the test dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>The evaluation for the caption detection task is conducted using the mean F1score. As shown in Table <ref type="table" coords="9,246.27,279.63,3.87,8.74" target="#tab_2">3</ref>, among the 61 results submitted by all participants, Run ID 27103 based multi-label classification model has achieved the better performance with the mean F1-score of 0.2655. We mitigate the impact of extreme data imbalance on the model by setting threshold culling noise data, and utilize association rule mining to search for the high-correlation concept combinations. For CNN-LSTM network method, the model did not perform well on the test dataset. Since in the sequences corresponding to the radiology images, the concept exists independently, although some frequent concepts have slight correlation.</p><p>Overall, we have completed this challenge in the medical image concept detection task and our group rank second among 12 participants (see Table <ref type="table" coords="9,468.97,399.30,3.87,8.74" target="#tab_3">4</ref>). The method adopted has achieved preliminary results and we will further investigate the medical image captioning task based on higher quality datasets and advanced deep learning algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,439.37,345.82,8.74;3,134.77,451.33,345.82,8.74;3,134.77,463.28,345.82,8.74;3,134.77,475.24,345.83,8.74;3,134.77,487.19,345.83,8.74;3,134.77,499.15,49.01,8.74"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: (a) We count the length of the concept sequences corresponding to each images in overall dataset, only one or two concepts of the sequence account for 17.09% and the maximum length is 77 and only appears once. (b) The figure shows the frequency distribution of all concepts, the horizontal axis represents the word frequency interval, and the word frequency less than 10 times accounts for 64.47%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,190.96,499.04,233.43,8.74;4,151.77,384.63,311.82,102.88"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The overall process of multi-label classification</figDesc><graphic coords="4,151.77,384.63,311.82,102.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,165.66,127.36,261.34,275.30"><head>Table 1 :</head><label>1</label><figDesc>Top-10 Concept description</figDesc><table coords="3,165.66,142.89,261.34,259.77"><row><cell></cell><cell></cell><cell cols="5">Concept ID Number</cell><cell>Concept</cell></row><row><cell></cell><cell></cell><cell cols="2">C0441633</cell><cell></cell><cell>8425</cell><cell></cell><cell>diagnostic scanning</cell></row><row><cell></cell><cell></cell><cell cols="2">C0043299</cell><cell></cell><cell>7906</cell><cell></cell><cell>x-ray procedure</cell></row><row><cell></cell><cell></cell><cell cols="2">C1962945</cell><cell></cell><cell>7902</cell><cell></cell><cell>radiogr</cell></row><row><cell></cell><cell></cell><cell cols="2">C0040395</cell><cell></cell><cell>7697</cell><cell></cell><cell>tomogr</cell></row><row><cell></cell><cell></cell><cell cols="2">C0034579</cell><cell></cell><cell>7564</cell><cell></cell><cell>pantomogr</cell></row><row><cell></cell><cell></cell><cell cols="2">C0817096</cell><cell></cell><cell>7470</cell><cell></cell><cell>thoracics</cell></row><row><cell></cell><cell></cell><cell cols="2">C0040405</cell><cell></cell><cell cols="3">7164 x-ray computer assisted tomography</cell></row><row><cell></cell><cell></cell><cell cols="2">C1548003</cell><cell></cell><cell>6428</cell><cell></cell><cell>radiograph</cell></row><row><cell></cell><cell></cell><cell cols="2">C0221198</cell><cell></cell><cell>5679</cell><cell></cell><cell>visible lesion</cell></row><row><cell></cell><cell></cell><cell cols="2">C0772294</cell><cell></cell><cell>5677</cell><cell></cell><cell>alesion</cell></row><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>[3,5]</cell><cell>[6,10]</cell><cell>[11,15]</cell><cell>[15,77]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,179.00,127.36,254.30,127.02"><head>Table 2 :</head><label>2</label><figDesc>Associated concept combination</figDesc><table coords="6,179.00,142.89,254.30,111.50"><row><cell>Concept ID</cell><cell>Concept sets</cell></row><row><cell>C1</cell><cell>C0034579;C0040405;C0040395</cell></row><row><cell>C2</cell><cell>C0043299;C1962945;C1548003</cell></row><row><cell>C3</cell><cell>C0221198;C0772294</cell></row><row><cell>C4</cell><cell>C0009924;C0449900</cell></row><row><cell>C5</cell><cell>C0817096;C0024109</cell></row><row><cell>C6</cell><cell>C0412555;C0041618</cell></row><row><cell>C7</cell><cell>C0007876;C1552858;C0728940;C0184905;C0015252</cell></row><row><cell>C8</cell><cell>C0013516;C0183129</cell></row><row><cell>C9</cell><cell>C0003842;C0002978</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.77,127.36,345.83,250.17"><head>Table 3 :</head><label>3</label><figDesc>The results of our submitted runs This process is similar to ID 27103. We chose the pre-trained ResNet-101 model and the learning rate is 1e -3 . The multi-label classification model was trained with filtered dataset D f,1 except for the concept grouping strategy and the max epoch was 60 in training procedure.</figDesc><table coords="8,134.77,142.89,345.82,198.78"><row><cell>Run ID</cell><cell>Method</cell><cell>Mean F1-score</cell><cell>Rank</cell></row><row><cell>27103</cell><cell>MLC+D f,1 +Cg</cell><cell>0.2655</cell><cell>4</cell></row><row><cell>27184</cell><cell>MLC+D f,1</cell><cell>0.2614</cell><cell>6</cell></row><row><cell cols="2">26786 CNN+RNN+att+D f,3</cell><cell>0.2316</cell><cell>7</cell></row><row><cell>27107</cell><cell>27103 26786</cell><cell>0.2135</cell><cell>14</cell></row><row><cell>27106</cell><cell>27184 26786</cell><cell>0.2116</cell><cell>15</cell></row><row><cell cols="2">27188 CNN+RNN+att+RL</cell><cell>0.0590</cell><cell>41</cell></row><row><cell cols="2">26877 CNN+RNN+att+RL</cell><cell>0.0585</cell><cell>42</cell></row><row><cell cols="2">27111 CNN+RNN+att+RL</cell><cell>0.0567</cell><cell>43</cell></row><row><cell cols="2">27158 CNN+RNN+att+RL</cell><cell>0.0537</cell><cell>44</cell></row><row><cell cols="4">optimizer to fine-tune the model with an initial learning rate of 1e -3 , and the</cell></row><row><cell cols="2">max epoch was 100 in training procedure.</cell><cell></cell><cell></cell></row><row><cell>Run ID 27184:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,199.75,127.36,215.85,83.18"><head>Table 4 :</head><label>4</label><figDesc>Top-5 groups in Concept Detection Task</figDesc><table coords="9,206.09,142.89,203.18,67.66"><row><cell>Gruop name</cell><cell>Mean F1-score</cell><cell>Rank</cell></row><row><cell>AUEB NLP Group</cell><cell>0.2823</cell><cell>1</cell></row><row><cell>damo(ours)</cell><cell>0.2655</cell><cell>2</cell></row><row><cell>ImageSem</cell><cell>0.2236</cell><cell>3</cell></row><row><cell>UA.PT Bioinformatics</cell><cell>0.2059</cell><cell>4</cell></row><row><cell>richard ycli</cell><cell>0.1952</cell><cell>5</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,491.82,337.63,7.86;9,151.52,502.75,288.82,7.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,220.54,491.82,260.05,7.86;9,151.52,502.78,62.44,7.86">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,221.17,502.78,88.22,7.86">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">suppl 1</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,513.86,337.64,7.86;9,151.52,524.82,329.07,7.86;9,151.52,535.78,239.71,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,293.02,513.86,187.57,7.86;9,151.52,524.82,175.13,7.86">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,347.02,524.82,133.57,7.86;9,151.52,535.78,146.92,7.86">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,546.85,337.63,7.86;9,151.52,557.81,329.07,7.86;9,151.52,568.77,76.80,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,290.95,546.85,172.55,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,557.81,325.14,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,579.85,337.64,7.86;9,151.52,590.78,92.85,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,288.76,579.85,100.73,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,398.69,579.85,81.90,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,601.88,337.64,7.86;9,151.52,612.84,329.07,7.86;9,151.52,623.80,186.91,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,405.15,601.88,75.45,7.86;9,151.52,612.84,89.79,7.86">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,261.20,612.84,219.39,7.86;9,151.52,623.80,93.91,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,634.88,337.63,7.86;9,151.52,645.84,329.07,7.86;9,151.52,656.80,329.07,7.86;10,151.52,119.67,329.07,7.86;10,151.52,130.63,329.07,7.86;10,151.52,141.59,329.07,7.86;10,151.52,152.55,329.07,7.86;10,151.52,163.51,329.07,7.86;10,151.52,174.47,329.07,7.86;10,151.52,185.43,216.27,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,268.29,141.59,212.30,7.86;10,151.52,152.55,124.23,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,297.67,152.55,182.92,7.86;10,151.52,163.51,329.07,7.86;10,151.52,174.47,122.46,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="10,305.55,174.47,171.07,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="10,142.96,196.39,337.63,7.86;10,151.52,207.34,93.19,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,239.72,196.39,176.61,7.86">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.96,218.30,337.63,7.86;10,151.52,229.26,329.07,7.86;10,151.52,240.22,86.01,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,328.09,218.30,152.50,7.86;10,151.52,229.26,103.94,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,275.64,229.26,200.74,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,251.18,337.63,7.86;10,151.52,262.14,329.07,7.86;10,151.52,273.07,329.07,7.89;10,151.52,284.06,47.10,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,201.25,262.14,275.47,7.86">Babytalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,151.52,273.10,268.73,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2891" to="2903" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,295.02,337.97,7.86;10,151.52,305.98,329.07,7.86;10,151.52,316.93,257.90,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,312.54,295.02,168.05,7.86;10,151.52,305.98,163.70,7.86">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,336.32,305.98,144.27,7.86;10,151.52,316.93,174.10,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="375" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,327.89,337.97,7.86;10,151.52,338.85,329.07,7.86;10,151.52,349.81,329.07,7.86;10,151.52,360.77,46.58,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,431.64,327.89,48.95,7.86;10,151.52,338.85,199.23,7.86">Overview of the ImageCLEFmed 2019 concept prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,371.81,338.85,108.79,7.86;10,151.52,349.81,186.41,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,371.73,337.97,7.86;10,151.52,382.69,329.07,7.86;10,151.52,393.65,244.05,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,396.88,371.73,83.71,7.86;10,151.52,382.69,117.93,7.86">Self-critical sequence training for image captioning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,291.90,382.69,188.69,7.86;10,151.52,393.65,150.78,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,404.61,327.31,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,211.11,404.61,230.14,7.86">Pubmed central: The genbank of the published literature</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,415.56,337.97,7.86;10,151.52,426.52,231.27,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,278.92,415.56,201.67,7.86;10,151.52,426.52,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.62,437.48,337.98,7.86;10,151.52,448.44,329.07,7.86;10,151.52,459.40,169.00,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,352.45,437.48,128.15,7.86;10,151.52,448.44,68.85,7.86">Show and tell: A neural image caption generator</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,240.86,448.44,239.73,7.86;10,151.52,459.40,75.99,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,470.36,337.98,7.86;10,151.52,481.32,329.07,7.86;10,151.52,492.28,213.33,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,335.74,470.36,144.85,7.86;10,151.52,481.32,100.94,7.86">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,275.97,481.32,204.62,7.86;10,151.52,492.28,120.32,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,503.24,337.98,7.86;10,151.52,514.19,329.07,7.86;10,151.52,525.15,201.29,7.86" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m" coord="10,201.52,514.19,279.07,7.86;10,151.52,525.15,35.25,7.86">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
