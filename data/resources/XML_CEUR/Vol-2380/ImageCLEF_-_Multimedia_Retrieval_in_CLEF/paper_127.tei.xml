<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.29,115.96,324.76,12.62;1,186.38,133.89,242.59,12.62">An Xception-GRU Model for Visual Question Answering in the Medical Domain</title>
				<funder ref="#_HgsxfsX">
					<orgName type="full">Natural Science Foundations of China</orgName>
				</funder>
				<funder ref="#_VDQaTJ6">
					<orgName type="full">NSF of Yunnan Province</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,146.53,177.62,57.80,8.74"><forename type="first">Shengyan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650091</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.98,177.62,46.95,8.74"><forename type="first">Xiaozhi</forename><surname>Ou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650091</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.61,177.62,36.12,8.74"><forename type="first">Jiao</forename><surname>Che</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650091</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,312.03,177.62,64.48,8.74"><forename type="first">Xiaobing</forename><surname>Zhou</surname></persName>
							<email>zhouxb@ynu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650091</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,413.89,177.62,54.93,8.74"><forename type="first">Haiyan</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650091</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.29,115.96,324.76,12.62;1,186.38,133.89,242.59,12.62">An Xception-GRU Model for Visual Question Answering in the Medical Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">76A82481DC2C30EE909058893CCC7DD0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>VQA-Med</term>
					<term>Xception</term>
					<term>GRU</term>
					<term>Attention Mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces an Xception-GRU model for Image-CLEF 2019 Medical Domain Visual Question Answering (VQA-Med) Task. First, we enhance the images and remove extraneous words from the questions and convert the questions to vectors. Then, we employ pretrained Xception model to extract image features and use GRU model to encode the questions. To generate the output, we combine these two models with the attention mechanism. Our Xception-GRU model achieves the accuracy score of 0.21 and BLEU score of 0.393.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the extensive application of deep learning in Computer Vision (CV) and Natural Language Processing (NLP), the powerful feature learning ability of deep learning greatly promotes the research in CV and NLP. Various deep networks represented by Convolutional Neural Network (CNN) emerge endlessly in CV, which can learn image features end-to-end without relying on the features of manual design. Through feature extraction layer by layer, CNN combines images from simple edges, corners, and other low-level features into higher-level features layer by layer. CNN's powerful feature extraction ability makes it more efficient to extract and compress image information. Recurrent Neural Network (RNN) model also shows its power in the field of NLP, especially in speech recognition, machine translation, language model and text generation. Visual Question Answer (VQA) consists of CV and NLP content, which inputs an image and an arbitrary form of natural language question about the image, and finally outputs a natural language answer. Medical VQA can help doctors improve their confidence in diagnosis and help patients better understand their conditions through the automatic system. In this paper, we propose an Xception-GRU model for ImageCLEF Medical Visual Question Answering (Med-VQA) 2019 Task <ref type="bibr" coords="2,183.74,119.00,10.52,8.74" target="#b1">[2]</ref>  <ref type="bibr" coords="2,198.51,119.00,9.96,8.74" target="#b6">[7]</ref>. The model takes an image and a question as an input and outputs the answer to this question based on features combined both image and question features with an attention mechanism.</p><p>The rest of this paper is structured as follows. The next section will provide a brief overview of the work involved. The dataset provided in the rang of the VQA-Med challenge is described in Section 3. The deep learning networks that we proposed for VQA in the medical domain is presented in Section 4. The submitted runs is described in Section 5. Finally, Section 6 is the summary of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Because VQA involves the two domains of CV and NLP, a natural VQA solution is to combine CNN and RNN, which have been very successful in CV and NLP, respectively, to construct a combination model. The VQA model, which is composed of deep CNN and LSTM network structure, is a relatively good model in visual question answering. Among them, some of the superior VQA models are introduced below.</p><p>Deeper LSTM Q + norm I model <ref type="bibr" coords="2,322.44,344.66,9.96,8.74" target="#b0">[1]</ref>. This model is proposed by Aishwarya Agrawal et al. In which, "I" refers to the extracted image features, and "norm I" refers to L2 normalization of image semantic information vector (1024 dimension) extracted by CNN. CNN extracts image semantic information and LSTM extracts text semantic information contained in the problem, and then the two information are fused so that the model can learn the meaning of the problem. Finally, the answer output is generated in a multi-layer MLP with Softmax as the output layer.</p><p>VIS+LSTM model <ref type="bibr" coords="2,246.33,440.51,14.61,8.74" target="#b10">[11]</ref>. This model is proposed by Mengye Ren et al. The basic structure of the model is to extract image information with CNN at first, and then connect LSTM to generate prediction results.</p><p>Neural-Image-QA model <ref type="bibr" coords="2,278.08,476.59,9.96,8.74" target="#b8">[9]</ref>. This model is proposed by Mateusz Malinowski et al. Based on CNN and LSTM, a model with length-variable prediction result is designed. In this model, visual question answering task is regarded as an auxiliary sequence to sequence task combined with image information.</p><p>mQA model <ref type="bibr" coords="2,215.61,524.62,9.96,8.74" target="#b4">[5]</ref>. This model is proposed by Gao H et al. In their paper, the understanding of visual question and answer task is that this model needs to give an answer to the question of the free form of an image, and the answer can be a sentence, a phrase or a word. The mQA model consists of four sub-modules, the first module encodes natural statements into a dense word vector feature by a LSTM network, i.e., extracts the information contained in the problem, called the problem LSTM network; The second module extracts image features from a deep CNN; The third module is another different LSTM network, which is used to code the characteristic information of the current word and some previous words in the answer, called the answer LSTM network; The last module fuses the information generated by the previous three models to predict the next word to be generated in the answer.</p><p>Most of the work discussed in this section cannot be directly applied to the VQA-Med for two reasons. The first one is obvious, this task mainly focuses on the medical domain, which gives this problem its unique set of challenges. As for the other one, it is related to how the sentences of the answers are constructed in VQA-Med, which is different from existing VQA datasets, such as DAtaset for QUestion Answering on Realworld images (DAQUAR) <ref type="bibr" coords="3,371.54,178.77,9.96,8.74" target="#b7">[8]</ref>, Visual7W <ref type="bibr" coords="3,432.43,178.77,14.61,8.74" target="#b13">[14]</ref>, Visual Madlibs <ref type="bibr" coords="3,172.39,190.73,14.61,8.74" target="#b12">[13]</ref>, COCO-QA <ref type="bibr" coords="3,245.04,190.73,14.61,8.74" target="#b10">[11]</ref>, Freestyle Multilingual Image Question Answering dataset (FM-IQA) <ref type="bibr" coords="3,218.54,202.68,9.96,8.74" target="#b4">[5]</ref>, Visual Question Answering (VQA) <ref type="bibr" coords="3,390.32,202.68,9.96,8.74" target="#b0">[1]</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Description</head><p>This dataset of ImageCLEF 2019 VQA-Med <ref type="bibr" coords="3,351.27,256.74,10.52,8.74" target="#b1">[2]</ref> differs from previous data sets in that it divides the problems into four categories based on modality, plane, organ system, and abnormality. The purpose is to generate a more focused set of problems for the evaluation of the results. The dataset contains 12,792 QA pairs, 3,200 medical images for training sets, and 2,000 QA pairs, 500 medical images for validation sets, and 500 medical images with 500 questions for test sets. Fig. <ref type="figure" coords="3,175.57,328.47,4.40,8.74" target="#fig_0">1</ref> shows an example of a medical image and the associated question and answer from the training set of VQA-Med 2019 dataset. Table <ref type="table" coords="3,436.49,340.43,4.98,8.74" target="#tab_0">1</ref> lists the statistics of VQA-Med 2019 dataset. Examples of these four categories are shown below: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Xception-GRU Model</head><p>Although at present the research in the VQA field has made some achievements, there's still a challenging problem that the overall accuracy of the answer by using existing models to realize the visual question and answer the problem is not high. The existing VQA models are relatively simple in structure, the content and form of answers are relatively simple, and more priori knowledge is needed for slightly complex problems, so simple reasoning cannot make correct answers. The reason is that, in addition to the image information of CNN, the knowledge source of LSTM in the learning process is only focused on the training question and answer pairs, with simple knowledge structure and lack of information. After comparing the characteristics of each pre-training CNN model, we propose the following model for our participation in VQA-Med 2019.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Main Model</head><p>The model we proposed is as Fig. <ref type="figure" coords="4,295.65,476.74,4.13,8.74" target="#fig_1">2</ref>: We use Xception <ref type="bibr" coords="5,226.49,119.00,10.52,8.74" target="#b3">[4]</ref> to extract image features and GRU to extract question features. Since the number of image features is much larger than question features, the question features should be repeated to make the number of the two features equal, and then an attention mechanism <ref type="bibr" coords="5,351.32,154.86,15.50,8.74" target="#b11">[12]</ref> is added before fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image Representation</head><p>In recent years, many CNN models have been proposed, such as AlexNet, VGGNet, Inception, Xception, ResNet, etc. In this paper, we use Xception to extract image features. Xception was proposed by Francois Chollet, author of Keras, in 2017. It is another improvement of Inception-v3 proposed by Google after Inception. The advantage of Xception is that it can improve the efficiency of network, as well as in the case of a number of equal participation. On large data sets, the effect is better than Inception-v3. This also provides another idea of "lightweight": increasing network efficiency and performance as much as possible in the case of given hardware resources, which can also be understood as making full use of hardware resources. The architecture of Xception is shown in Fig. <ref type="figure" coords="5,467.49,305.62,4.13,8.74">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3. The architecture of Xception module</head><p>First, the image goes through a convolution kernel of 1*1, the function of 1*1 convolution is to reduce dimension, and because each convolution kernel convolves only with the corresponding channel, the network uses separate convolution kernels. Finally, the features of each channel are joined together. The benefit about this method is that we get features that are independent of each other, without too much redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Question Representation</head><p>Gated Recurrent Unit(GRU) <ref type="bibr" coords="5,276.87,644.17,10.52,8.74" target="#b2">[3]</ref> was proposed by Cho van Merrienboer, Bahdanau and Bengio in 2014. By introducing the concept of gate, the calculation method of hidden state in the cyclic neural network is modified, which includes reset gate, update gate, candidate hidden state and hidden state. GRU is one gate less than LSTM. We can think of the GRU as an optimization or variation of the LSTM. Resetting the gate helps capture short-term dependencies in the time series. Update gates help capture long-term dependencies in time series, but the experimental results are quite similar: When we train a GRU network, the input of the output layer is:</p><formula xml:id="formula_0" coords="6,149.71,450.83,182.01,42.55">y i t = W 0 h. The output is: y o t = σ(y i t ).</formula><p>The loss function at a certain moment is:</p><formula xml:id="formula_1" coords="6,268.74,511.64,77.88,22.31">E t = 1 2 (y d -y o t ) 2 .</formula><p>In this part, we use the GRU model to extract the features of questions after preprocessing them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation and Results</head><p>Before running the evaluation metrics on the answers, the following preprocessing are performed:</p><p>1. The capitals in each answer are converted to lowercases, 2. All punctuations are deleted and each answer is tokenized by single words.</p><p>The evaluation can be conducted based on the following metrics: Accuracy (Strict) Accuracy is our most common evaluation indicator, and it is easy to understand. For a given test dataset, the number of correct samples in the test task is divided by the number of all samples. Generally speaking, the higher the accuracy, the better the classifier. BLEU How to measure the similarity between the generated statement and the reference statement is an important issue. In 2002, Kishore Papineni et al proposed a classic measure standard BiLingual Evaluation Understudy (BLEU) <ref type="bibr" coords="7,134.76,223.46,14.61,8.74" target="#b9">[10]</ref>. BLEU is an auxiliary method to evaluate the quality of Bilingual translation. This method is simple, short, fast and easy to understand. Because the effect is reasonable, it has been widely migrated to various assessment tasks of natural language processing. It is used to determine how similar machinetranslated sentences are to human-translated sentences. BLEU calculates the ratio of similarity between two sentences by counting the frequency of words appearing together, using the n-gram matching rule. BLEU evaluations are fast and close to human ratings. So the performance of a VQA model can be judged with the BLEU score. The higher the score, the better performance of a VQA model.</p><p>Three experiments are conducted to evaluate our model. The parameters are set as follows, the size of dictionary is 1000, the length of sequences is 9, the hidden size of GRU is 128, and the batch size of training is 256. We set the epoch to 54.</p><p>The experiments are described as follows.</p><p>1. In the first experiment, we run our proposed model (Xception-GRU) without date enhancement. 2. In the second experiment, we use Bi-LSTM instead of GRU to extract text features. Obviously, bidirectional LSTM is less effective than GRU. 3. In the last experiment, we run our proposed model (Xception-GRU) with date enhancement, the rest of the architecture stays the same.</p><p>The following table shows the results obtained on the test set: As shown in Table <ref type="table" coords="7,235.49,620.26,3.87,8.74" target="#tab_2">2</ref>, our proposed Xception-GRU model without data enhancement achieves good results in term of BLEU metric (0.393) and accuracy (0.21). When we try Bi-LSTM model <ref type="bibr" coords="7,299.73,644.17,10.52,8.74" target="#b5">[6]</ref> to extract question features and Xception to extract image features without data enhancement, the effect is reduced a little. Then we remain Xception-GRU architecture, and introduce data enhancement, the effect is reduced a lot. The reason is that due to the high performance of Xception model and the depth of feature extraction, it is easy to overfit. Therefore, Xception without image enhancement produces better results, while with image enhancement produces worse results.</p><p>In this regard, we still need to make some improvements on the mechanism to prevent overfitting. However, since there are no medical imaging professionals who can provide suggestions for the improvement of our process, the results may differ from the actual situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present our contribution to the visual question answering task in the field of medicine in view of the very meaningful but challenging VQAMed Task of ImageCLEF 2019. Our Xception-GRU model achieves the accuracy score of 0.21 and BLEU score of 0.393.</p><p>Our future work will focus on making the answers more readable and accurate. We consider that there is an essential semantic gap between the regional visual features and the source of the problem text representation. Due to the great success of the attention-based model in VQA task <ref type="bibr" coords="8,385.18,350.74,14.61,8.74" target="#b11">[12]</ref>, we want to work on features, namely how to extract visual information more effectively and apply the attention mechanism better. We will improve our visual model by using attention feature enhancement techniques to further make regional semantic representations more relevant to the problem. Our future work also includes training on multiple data sets, improving model performance, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.76,555.38,345.82,7.89;3,134.76,566.37,193.22,7.86;3,169.35,384.42,276.62,156.19"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example of a medical image and the associated question and answer from the training set of ImageCLEF 2019 VQA-Med.</figDesc><graphic coords="3,169.35,384.42,276.62,156.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,205.81,655.03,203.73,7.89;4,152.06,507.33,311.28,132.93"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The architecture of Xception-GRU module</figDesc><graphic coords="4,152.06,507.33,311.28,132.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,134.76,369.75,345.82,7.89;6,134.76,380.73,345.82,7.86;6,134.76,391.69,345.83,7.86;6,134.76,402.65,176.65,7.86;6,134.76,209.59,345.79,145.38"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. illustration of (a) LSTM and (b) gated recurrent units. (a) i, f and o are the input, forgetand output gates, respectively. c and c denote the memory cell and the new memory cell content. (b) r and z are the reset and update gates, and h and h are the activation and the candidate activation.</figDesc><graphic coords="6,134.76,209.59,345.79,145.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,138.97,620.47,341.63,44.39"><head>1 .</head><label>1</label><figDesc>Modality, e.g. What kind of image is this? Was IV contrast given to the patient? 2. Plane, e.g. What plane is the image acquired in? In what plane is this image oriented? 3. Organ System, e.g. What organ system is primarily present in this image? What organ system is shown in this CT scan? 4. Abnormality, e.g. What is abnormal in the CT scan? What abnormality is seen in the image?</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,162.08,188.57,291.19,61.93"><head>Table 1 .</head><label>1</label><figDesc>Statistics of VQA-Med 2019 dataset.</figDesc><table coords="4,162.08,209.37,291.19,41.13"><row><cell></cell><cell>Training</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>Images</cell><cell>3200</cell><cell>500</cell><cell>500</cell></row><row><cell>Questions</cell><cell>12792</cell><cell>2000</cell><cell>500</cell></row><row><cell>Answers</cell><cell>12792</cell><cell>2000</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,156.84,529.34,301.68,61.94"><head>Table 2 .</head><label>2</label><figDesc>Results of our proposed model on Test set.</figDesc><table coords="7,156.84,550.14,301.68,41.14"><row><cell>Model</cell><cell>Accuracy</cell><cell>BLEU</cell></row><row><cell>Xception + GRU without enhancement</cell><cell>0.21</cell><cell>0.393</cell></row><row><cell>Xception + Bi-LSTM without enhancement</cell><cell>0.2</cell><cell>0.31</cell></row><row><cell>Xception + GRU with enhancement</cell><cell>0.178</cell><cell>0.27</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">Natural Science Foundations of China</rs> under Grants <rs type="grantNumber">61463050</rs>, the <rs type="funder">NSF of Yunnan Province</rs> under Grant <rs type="grantNumber">2015FB113</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HgsxfsX">
					<idno type="grant-number">61463050</idno>
				</org>
				<org type="funding" xml:id="_VDQaTJ6">
					<idno type="grant-number">2015FB113</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.34,526.56,342.25,7.86;8,146.91,537.52,333.67,7.86;8,146.91,548.48,199.76,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,161.93,537.52,124.59,7.86">Vqa: Visual question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,307.47,537.52,173.11,7.86;8,146.91,548.48,106.99,7.86">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.34,559.02,342.25,7.86;8,146.91,569.98,333.67,7.86;8,146.91,580.93,333.67,7.86;8,146.91,591.89,254.95,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,161.69,569.98,318.89,7.86">VQA-Med: Overview of the medical visual question answering task at imageclef</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="8,183.77,580.93,105.66,7.86">CLEF 2019 Working Notes</title>
		<title level="s" coord="8,296.11,580.93,146.51,7.86">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.34,602.43,342.24,7.86;8,146.91,613.39,333.67,7.86;8,146.91,624.35,270.01,7.86" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m" coord="8,210.40,613.39,270.18,7.86;8,146.91,624.35,108.66,7.86">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,138.34,634.88,342.24,7.86;8,146.91,645.84,333.67,7.86;8,146.91,656.80,86.01,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,198.87,634.88,262.13,7.86">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,146.91,645.84,329.75,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.34,119.68,342.24,7.86;9,146.91,130.64,333.67,7.86;9,146.91,141.60,86.01,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,361.31,119.68,119.28,7.86;9,146.91,130.64,253.80,7.86">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,407.92,130.64,72.65,7.86">Computer Science</title>
		<imprint>
			<biblScope unit="page" from="2296" to="2304" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.34,152.55,342.24,7.86;9,146.91,163.48,333.68,7.89;9,146.91,174.47,25.60,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,268.80,152.55,211.79,7.86;9,146.91,163.51,182.87,7.86">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,338.06,163.51,68.79,7.86">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.34,185.43,342.23,7.86;9,146.91,196.39,333.67,7.86;9,146.91,207.35,259.38,7.86;9,441.77,207.35,38.82,7.86;9,146.91,218.31,333.67,7.86;9,146.91,229.27,333.67,7.86;9,146.91,240.23,333.67,7.86;9,146.91,251.19,333.67,7.86;9,146.91,262.14,333.67,7.86;9,146.91,273.10,333.67,7.86;9,146.91,284.06,75.79,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,221.89,240.23,258.69,7.86;9,146.91,251.19,75.91,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,243.09,251.19,237.48,7.86;9,146.91,262.14,333.67,7.86;9,146.91,273.10,76.04,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="9,230.45,273.10,170.59,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF 2019)<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="9,138.34,295.02,342.24,7.86;9,146.91,305.98,200.66,7.86" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<title level="m" coord="9,260.41,295.02,220.17,7.86;9,146.91,305.98,171.99,7.86">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.34,316.94,342.23,7.86;9,146.91,327.90,333.67,7.86;9,146.91,338.86,97.60,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,322.73,316.94,157.84,7.86;9,146.91,327.90,175.42,7.86">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,343.42,327.90,137.16,7.86;9,146.91,338.86,68.93,7.86">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,349.82,337.97,7.86;9,146.91,360.77,333.67,7.86;9,146.91,371.73,333.67,7.86;9,146.91,382.69,98.01,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,348.36,349.82,132.22,7.86;9,146.91,360.77,131.97,7.86">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,301.80,360.77,178.78,7.86;9,146.91,371.73,164.19,7.86">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,393.65,337.97,7.86;9,146.91,404.61,333.67,7.86;9,146.91,415.57,25.60,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,285.29,393.65,195.29,7.86;9,146.91,404.61,38.09,7.86">Exploring models and data for image question answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,206.34,404.61,274.24,7.86">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,426.53,337.97,7.86;9,146.91,437.49,333.67,7.86;9,146.91,448.45,201.28,7.86" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m" coord="9,197.75,437.49,282.82,7.86;9,146.91,448.45,35.26,7.86">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.61,459.40,337.97,7.86;9,146.91,470.36,333.67,7.86;9,146.91,481.32,54.06,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,320.80,459.40,159.78,7.86;9,146.91,470.36,135.54,7.86">Visual madlibs: Fill in the blank image generation and question answering</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,302.35,470.36,178.23,7.86;9,146.91,481.32,25.39,7.86">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,492.28,337.98,7.86;9,146.91,503.24,81.97,7.86" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<title level="m" coord="9,328.64,492.28,151.94,7.86;9,146.91,503.24,53.30,7.86">Visual7w: Grounded question answering in images</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
