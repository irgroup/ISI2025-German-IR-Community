<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.11,115.90,317.14,12.68;1,165.79,133.83,283.77,12.68">Deep Segmentation: Using deep convolutional networks for coral reef pixel-wise parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,139.24,172.97,73.71,8.80"><forename type="first">Aljoscha</forename><surname>Steffens</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Filament AI</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.50,172.97,79.41,8.80"><forename type="first">Antonio</forename><surname>Campello</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Filament AI</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Essex</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.79,172.97,81.21,8.80"><forename type="first">James</forename><surname>Ravenscroft</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Filament AI</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,411.55,172.97,57.33,8.80"><forename type="first">Adrian</forename><surname>Clark</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Essex</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.97,184.92,54.32,8.80"><forename type="first">Hani</forename><surname>Hagras</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Essex</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.11,115.90,317.14,12.68;1,165.79,133.83,283.77,12.68">Deep Segmentation: Using deep convolutional networks for coral reef pixel-wise parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C08CC2FDA5C8B3FA26CC173B38A9213E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe a deep-convolutional network based method to segment coral reef images into different types of substrates. The method described in the paper includes data preparation, model summary, specific techniques to deal with class imbalance, and downstream post-processing computer vision tasks, such as morphological operations and polygon generation from pixel segmentation. We present the results of our method in the ImageCLEFcoral pixel-wise parsing task, evaluated across the different classes of substrate.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation models have received significant attention in computer vision due to their applicability in medical imagining, autonomous driving, and full-scene understanding. In this paper, we consider the ImageCLEF 2019 pixelwise parsing competition <ref type="bibr" coords="1,243.77,452.82,9.96,8.80" target="#b2">[4]</ref>, <ref type="bibr" coords="1,259.49,452.82,9.96,8.80" target="#b7">[9]</ref>, which consists of segmenting pictures from coral reefs into 13 different substrates. We are particularly interested in evaluating the applicability of deep convolutional neural networks (DCNNs) to the coral images, taken under real conditions in the ocean. A model that performs well on the task of automatic segmentation of corals could be beneficial to the conservation of reefs by measuring the amounts of different corals, their condition and other characteristics.</p><p>This paper is organised as follows. In section 2 we explore the ImageCLEFcoral dataset <ref type="bibr" coords="1,183.26,549.92,9.96,8.80" target="#b2">[4]</ref>, how to split it into training and validation set in order to keep the distributions balanced, as well as a data augmentation approach. In section 3, we describe DeeplabV3 <ref type="bibr" coords="1,223.91,573.83,9.96,8.80" target="#b4">[6]</ref>, a DCNN designed for semantic segmentation, alongside with our pipeline. Our method includes post-processing tasks such as morphological operations and polygon filling. Training, bootstrapping and inference are also described. In section 4 we discuss possible routes to improve the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>The data that was provided consisted of 240 images of size 4032 × 3024 × 3 as well as a text file containing the polygons in the images for the following classes/substrates: We mapped the 13 substrates to integers {0, 1, . . . 13} (the background corresponding to 0) and created a 4032 × 3024 integer matrix for each image defined as</p><formula xml:id="formula_0" coords="2,205.74,386.76,274.85,12.69">M k ij = c if pixel i, j corresponds to substrate c,<label>(1)</label></formula><p>with ties broken arbitrarily. The corresponding matrix acts as a per-class "mask" for each substrate, an example can be seen in figure <ref type="figure" coords="2,364.10,420.34,3.87,8.80" target="#fig_0">1</ref>. The class-distribution of the pixel reflects the naturally occurring composition of corals in the photographed area and is thus highly imbalanced, see figure <ref type="figure" coords="3,472.84,142.84,3.87,8.80" target="#fig_1">2</ref>. Also, the class-distributions between two images can vary strongly as only a limited selection of coral types will be present in each image. Both the overall class-imbalance and the difference in inter-image class-distributions need to be taken into account and are discussed in section 3.2 and 2.1 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Split</head><p>The data was split into a training and validation set with 204 images (85%) for training and 36 for validation. Due to reasons discussed in section 2.2, we chose to split the data on a per-image basis. This posed a problem given the difference in inter-image class-distributions and the relatively small number of images: for a random split it would be highly likely that the overall, training, and validation class-distribution would differ strongly which would have an effect on the evaluation of the model performance.</p><p>In order to achieve balanced distributions for the training and validation set, we created N training and validation sets of constant size, with randomly selected images and compared the resulting training/validation distributions by using a cosine distance that is weighted by the overall class-distribution.</p><formula xml:id="formula_1" coords="3,201.92,382.99,278.68,33.14">dist(v 1 , v 2 , w) = d i=1 w i 2 × v 1 i × v 2 i d i=1 w i × v 1 i d i=1 w i × v 2 i<label>(2)</label></formula><p>From those N splits, we chose the split that resulted in the lowest distance. Afterwards, we selected one item from each set that, if swapped, resulted in the biggest decrease in the weighted cosine distance. Swaps were performed until there was no further decrease possible by swapping individual items. The whole procedure -N random splits, choosing the best split, optimise by swappingwas performed several times in order to increase the chance of finding an good final split.</p><p>While the given approach does not result in an optimal solution, it is fast and did achieve satisfying result for the given task. The three distributions (overall, training, validation) can be seen in figure <ref type="figure" coords="3,318.03,532.47,3.87,8.80" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Augmentation</head><p>Splitting the data on image level was mainly done due to how we prepared the data before feeding it into the Neural Network. With their large spatial dimensions that resulted in both rich details and many different coral types per image, it seemed sensible to use random cropping as a main preparation step. For each image that was loaded into memory during training, 16 random crops with square sizes between 400 × 400 and 1400 × 1400 were performed. The crops were then bi-linearly scaled to 256 × 256 and randomly flipped in vertical and horizontal direction before they were fed into the network. Both the flipping and the random cropping served as a data augmentation method that ensured that the network was exposed to a variety of relative coral sizes, orientations, and image-compositions. This way, the network never saw the exact same image twice which reduced overfitting.</p><p>For the validation data, random crops and re-scaling were done prior to training the network and thus always the same. This was done to make the metrics that were computed after each epoch comparable. We used DeeplabV3 <ref type="bibr" coords="4,224.54,536.51,9.96,8.80" target="#b4">[6]</ref>, a deep convolutional neural network that improves over existing networks. In particular, DeeplabV3 extends Deeplab <ref type="bibr" coords="4,403.36,548.46,10.51,8.80" target="#b3">[5]</ref> and avoids the need of a post-processing machine learning model (such as conditional random fields). Nevertheless, since the challenge is evaluated over polygons, we needed to apply image processing techniques (in particular, morphological transformations and region-filling algorithms) in order to generate the final file. Note that the polygon-filling operations have more degrees-of-freedom than the training preprocessing, therefore adding extra parameters to the model. These operations are described in more details in 3.5. A high-level operational diagram of the model and evaluation can be found below. More details will be described in the next sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DeeplabV3</head><p>Deeplab V3 is a Deep Convolutional Neural Network (DCNN) for semantic image segmentation proposed by Chen et al in <ref type="bibr" coords="5,332.36,428.91,10.51,8.80" target="#b4">[6]</ref> in 2017. It is a state-of-the-art network architecture that, with pretraining on the ImageNet <ref type="bibr" coords="5,401.50,440.86,10.51,8.80" target="#b5">[7]</ref> and JFT-300M <ref type="bibr" coords="5,134.77,452.82,15.49,8.80" target="#b8">[10]</ref> dataset resulted in a mIOU of 86.9% on the PASCAL VOC 2012 test set <ref type="bibr" coords="5,467.31,452.82,9.96,8.80" target="#b4">[6]</ref>. The model consist of two parts: The first part is a feature extracting backbone that is not strictly limited to be of a given type -in the paper the authors use a ResNet-50 and ResNet-101 but other architectures can be employed as well. The second part is where the novelty happens with an extensive use of atrous convolutions. An atrous convolution filter kernel layout is defined by the normal size of say 3x3 and in addition to that, an atrous rate that specifies how many 0-values are between the individual filter entries along the spatial dimensions. With 0-values between, the atrous convolution would be the same as a normal convolution; inserting one zero between two neighbouring values in a 3x3 convolution would make it have the same receptive field as a 5x5 convolution, while only employing 9 weights instead of 25. Deeplab V3 uses atrous convolutions for constructing feature pyramids. Feature pyramids are used to combine features from different scales into one feature map, which is done by using atrous convolutions with different rates on the same feature map and concatenating the individual outputs to a new feature map. For our submission we used a PyTorch implementation of DeepLab V3 found from <ref type="bibr" coords="5,329.27,644.10,10.51,8.80" target="#b1">[3]</ref> with a ResNet101 backbone <ref type="bibr" coords="5,470.08,644.10,10.51,8.80" target="#b6">[8]</ref> and an output-stride of 16</p><p>Prior to polygon-filling post-processing, the model outputs, for every pixel, a probability that such pixel belongs to a class, or more formally:</p><formula xml:id="formula_2" coords="6,171.55,150.82,309.04,12.69">f k ij (c) = probability that pixel ij on image k belongs to class c (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Class imbalance and weighted loss function</head><p>As discussed in section 2, the class-distribution of the data is highly skewed. This is a problem, as the model will emphasize more on classifying frequent classes correctly to achieve lower errors if no counter-measures are in place. One approach that is often used -and that was used here as well -is to weigh the loss function based on the class distribution. We used a pixel-wise cross-entropy loss and weighted the individual components with following weights:</p><formula xml:id="formula_3" coords="6,261.97,282.06,218.62,22.31">w(c) = 1 log(α + p(c))<label>(4)</label></formula><p>with p(c) being the relative occurrence of class c and α being a hyper-parameter that scales the weights (in our submission α = 1.025 yielded good results). This was done as there are orders of magnitudes between the individual relative occurrences, and the model would over-emphasize on the infrequent classes if we just used the reciprocal. The final cross-entropy loss is as follows:</p><formula xml:id="formula_4" coords="6,135.96,403.16,344.63,42.23">1 N × height × width × N k=1 height i=1 width j=1 C c=1 -y k ij (c) × w(c) × log exp(f k ij (c)) exp(f k ij (c)) ,<label>(5)</label></formula><p>where f k ij (c) and y k ij (c) describe the predicted confidence (f ) and ground truth (y) for image k at position ij and class c respectively and N is the number of images that are included in the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Bootstrapping</head><p>We trained the Neural Network for 50 epochs, with a batch size of 32 (2 images per batch, 16 crops per image) on a Nvidia GeForce GTX 1080 Ti. After the training, we used the network to predict the training images and cropped out areas where the network was particularly bad. The network was then trained on those images for another 30 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference</head><p>In order to predict a full-sized image at inference time, we used a sliding window approach. With window-sizes of 500 × 500, 1000 × 1000, and 1500 × 1500 corresponding step-sizes of 400, 800, and 1200 we cut each 4032 × 3024 image into 112 partially overlapping sections. Each section was then scaled to 256×256 and fed into the Neural Network. The results were scaled to their original size and added at their respective position to a 4032 × 3024 × 14 confidence matrix C. For each position C i,j the number of votes were stored (meaning how often a given pixel was predicted) so that the average confidence could be calculated subsequently. The final classification for pixel i, j was then given by</p><formula xml:id="formula_5" coords="7,262.33,190.66,218.26,17.05">c = argmax k∈{0,...,13} C i,j (k)<label>(6)</label></formula><p>By using sliding windows with different window-sizes we made sure that each pixel was predicted at several different resolutions and thus also with different amounts of context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Post-processing</head><p>After calculating the classification mask for a predicted image, we used several basic computer vision algorithm for post-processing and transforming the data into the given submission format.</p><p>1. Find connected components.</p><p>2. Morphological opening with kernel size = (31, 31) 3. Morphological closing with kernel size = (31, 31) 4. Flood fill 5. Polygon approximation using Douglas-Peucker algorithm <ref type="bibr" coords="7,399.60,367.09,9.96,8.80" target="#b0">[2]</ref>, with maximum distance to correct output ε equals 0.1% of the contour arc-length for that connected component. We used the OpenCV 3.4.2 implementations [1] of the corresponding algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Final results and further work</head><p>The average intersection over union for all classes, as reported in the official ImageCLEF 2019 Coral competition, over the test set, is described in Table <ref type="table" coords="7,472.85,482.49,3.87,8.80">2</ref>. Soft corals and hard corals (boulder) performed relatively well in comparison to the other classes, which was expected, anticipated by the class abundance, as shown in figure <ref type="figure" coords="7,205.17,518.36,3.87,8.80" target="#fig_1">2</ref>. Surprisingly, mushroom pixels have been correctly identified 21%, in spite of the small number of samples. A full analysis and visualisation of the results per class is left for future investigation.</p><p>There are a number of areas where our pipeline can potentially be improved in the future and things that can be investigated:</p><p>-Increasing the input size to the model -Increase the batch size -Test different model backbones -Tuning hyper-parameters (both for training and for post-processing) -Investigate the impact of crop sizes -Investigate the impact of bootstrapping -Try different methods to counteract the class-imbalance</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,166.00,652.47,283.36,8.80;2,145.11,452.94,325.15,188.06"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Substrate mask and image for id 2018_0714_112417_024</figDesc><graphic coords="2,145.11,452.94,325.15,188.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,156.35,458.90,302.65,8.80;4,145.68,233.59,324.00,213.84"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Class distributions overall and for training and validation split</figDesc><graphic coords="4,145.68,233.59,324.00,213.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,230.09,356.90,155.19,8.80;5,152.06,242.83,308.40,102.60"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Training and inference flows.</figDesc><graphic coords="5,152.06,242.83,308.40,102.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,206.36,204.21,201.90,96.48"><head>Table 1 :</head><label>1</label><figDesc>Substrate names</figDesc><table coords="2,206.36,204.21,201.90,73.67"><row><cell cols="2">Hard Coral -Branching Soft Coral</cell></row><row><cell cols="2">Hard Coral -Submassive Soft Coral -Gorgonian</cell></row><row><cell>Hard Coral -Boulder</cell><cell>Sponge</cell></row><row><cell cols="2">Hard Coral -Encrusting Sponge -Barrel</cell></row><row><cell>Hard Coral -Table</cell><cell>Fire Coral -Millepora</cell></row><row><cell>Hard Coral -Foliose</cell><cell>Algae -Macro or Leaves</cell></row><row><cell>Hard Coral -Mushroom</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.95,364.80,337.64,8.17;8,151.52,375.76,139.56,8.17" xml:id="b0">
	<monogr>
		<ptr target="https://docs.opencv.org/3.1.0/dd/d49/tutorial_py_contour_features.html" />
		<title level="m" coord="8,151.52,364.80,100.48,7.92">Opencv contour features</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,386.16,112.50,7.92;8,344.11,386.86,136.48,7.47;8,151.52,397.12,140.56,8.17" xml:id="b1">
	<monogr>
		<ptr target="https://github.com/jfzhang95/pytorch-deeplab-xception" />
		<title level="m" coord="8,151.52,386.16,99.78,7.92">pytorch-deeplab-xception</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,407.51,337.63,7.92;8,151.52,418.47,329.08,7.92;8,151.52,429.43,236.57,7.92" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,221.98,418.47,158.91,7.92">Overview of ImageCLEFcoral 2019 task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">G</forename><surname>Clift</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,400.35,418.47,80.25,7.92;8,151.52,429.43,20.96,7.92">CLEF2019 Working Notes</title>
		<title level="s" coord="8,243.47,429.43,116.53,7.92">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,439.82,337.64,7.92;8,151.52,450.78,329.07,7.92;8,151.52,461.74,329.08,7.92;8,151.52,472.70,20.99,7.92" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,445.40,439.82,35.19,7.92;8,151.52,450.78,329.07,7.92;8,151.52,461.74,94.45,7.92">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,252.94,461.74,163.18,7.92">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,483.09,337.64,7.92;8,151.52,494.05,257.53,7.92" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,359.18,483.09,121.41,7.92;8,151.52,494.05,130.22,7.92">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>CoRR, abs/1706.05587</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,504.45,337.64,7.92;8,151.52,515.41,220.01,7.92" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,400.34,504.45,80.25,7.92;8,151.52,515.41,137.59,7.92">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,309.13,515.41,32.87,7.92">CVPR09</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,525.80,337.64,7.92;8,151.52,536.76,329.07,7.92;8,151.52,547.72,106.14,7.92" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,302.10,525.80,174.57,7.92">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,162.98,536.76,312.40,7.92">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,558.11,337.64,7.92;8,151.52,569.07,329.07,7.92;8,151.52,580.03,329.07,7.92;8,151.52,590.99,329.07,7.92;8,151.52,601.95,329.07,7.92;8,151.52,612.91,329.07,7.92;8,151.52,623.87,329.07,7.92;8,151.52,634.83,329.07,7.92;8,151.52,645.79,329.07,7.92;8,151.52,656.74,151.38,7.92" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,209.29,612.91,271.30,7.92;8,151.52,623.87,69.88,7.92">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,242.56,623.87,238.03,7.92;8,151.52,634.83,329.07,7.92;8,151.52,645.79,82.96,7.92">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 10th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="8,422.26,645.79,58.33,7.92;8,151.52,656.74,107.95,7.92">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,119.62,337.98,7.92;9,151.52,130.58,202.60,7.92" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,360.15,119.62,120.44,7.92;9,151.52,130.58,144.62,7.92">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta1</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,303.95,130.58,20.89,7.92">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
