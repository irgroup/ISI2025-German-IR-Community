<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.90,152.79,307.10,12.64;1,224.21,170.79,142.37,12.64;1,366.43,169.19,4.50,8.10">ZJUTCVR Team at ImageCLEFlifelog2019 Lifelog Moment Retrieval Task *</title>
				<funder ref="#_Adbkq8D">
					<orgName type="full">Zhejiang Provincial Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_XhFPuJW">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,221.45,209.94,51.79,8.96"><forename type="first">Pengfei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University of Technology</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,280.59,209.94,36.93,8.96"><forename type="first">Cong</forename><surname>Bai</surname></persName>
							<email>congbai@zjut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University of Technology</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.63,209.94,28.05,8.96"><forename type="first">Jie</forename><surname>Xia</surname></persName>
							<email>jiexiaxx@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University of Technology</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.90,152.79,307.10,12.64;1,224.21,170.79,142.37,12.64;1,366.43,169.19,4.50,8.10">ZJUTCVR Team at ImageCLEFlifelog2019 Lifelog Moment Retrieval Task *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2FFF0E6DE127521B6722497D92F502B4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lifelog Moment Retrieval</term>
					<term>cross-modal retrieval</term>
					<term>Convolutional neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our approach to the task of ImageCLEFlifelog 2019. Totally five runs are presented here, which contribute to Lifelog Moment Retrieval (LMRT) task. We use several different methods to provide a flexible retrieval system based on the huge amounts of multi-modal dataset. The work proposes a supervised learning approach with high precision and two exploratory approaches. The first run based on pre-trained Alexnet is the only run we summit through the ImageCLEFlifelog 2019 evaluation system, which reaches the second rank with F1-measure@10=0.44. We then improve our pipeline and get better results of retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lifelog is described as a phenomenon whereby people can digitally record their own daily lives in varying amounts of detail, for a variety of purposes <ref type="bibr" coords="1,393.84,498.95,11.00,8.96" target="#b0">[1]</ref>. With the rapid development of Internet of things (IOT) and the increasing popularity of sensors and wearable devices that can sense and record biological characteristics <ref type="bibr" coords="1,402.67,522.95,10.69,8.96" target="#b1">[2]</ref>, data is ready to be captured and combined with more and more personal information in the form of a digital diary. Individuals can now use digital technology to track the details of their daily activities, such as eating, commuting, exercising, working and sleeping. Lifelog data also includes all kinds of data created in daily interactions between individuals and mobile phones and PCs, such as shopping records and music listening records <ref type="bibr" coords="1,438.91,582.95,10.71,8.96" target="#b2">[3]</ref>.</p><p>As part of the ImageCLEF 2019 evaluation campaign <ref type="bibr" coords="1,366.43,594.95,10.66,8.96" target="#b3">[4]</ref>, The ImageCLEFlife-log2019 task <ref type="bibr" coords="1,178.50,606.95,11.69,8.96" target="#b4">[5]</ref> aims to automatically analyze the data in order to categorize, summarize and also to retrieve the information as the users' need.</p><p>The task is divided into two subtasks: Solve my life puzzle (Puzzle), Lifelog moment retrieval (LMRT). The main demand of LMRT subtask is to retrieve a number of specific predefined moments in a lifelogger's normal life. For example, the retrieval result of the query "find my breakfast time" should be images that show the moments of user having breakfast at home in the morning.</p><p>And the results of retrieval should not only be relevant, the diversification should also be taken into account. The definition of diversification is that the results of retrieval should cover different proper moments as much as possible. And multiply methods can be used to diversify the retrieval results. To be specific, implementing cluster on textual or visual properties can improve the diversification of the selected moments with respect to the target scenario.</p><p>In this paper, we present three approaches to LMRT challenge. Two rely on visual concepts using respectively fine-tuned Googlenet <ref type="bibr" coords="2,329.51,270.45,11.69,8.96" target="#b5">[6]</ref> and Alexnet <ref type="bibr" coords="2,397.56,270.45,10.66,8.96" target="#b6">[7]</ref>. One relies on both the segmentation and visual concepts with fine-tuned Resnet18 <ref type="bibr" coords="2,396.98,282.45,10.66,8.96" target="#b7">[8]</ref>. Related works are discussed in section 2. The proposed methods are described in section 3. In section 4 we analyze the results of our experiments. And we conclude this paper in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related works</head><p>In this section, we briefly discuss recent works on lifelog retrieval. The researchers in this field have proposed different strategies and models to explore inherent law. What's more, the personal devices provided more personal data that can be applied to lifelog retrieval could help users to find the need of multimedia data more efficiently. Liting et al. <ref type="bibr" coords="2,184.94,411.45,11.69,8.96" target="#b8">[9]</ref> proposed retrieval system LIFER, an interactive life record retrieval system developed by ImageCLEFlifelog2018 organizing team <ref type="bibr" coords="2,376.94,423.45,15.57,8.96" target="#b9">[10]</ref>, in the spirit of the MyLifeBits <ref type="bibr" coords="2,173.78,435.45,16.72,8.96" target="#b10">[11]</ref> seminal lifelog database. It provides effective interface with users according to different requirements. The method is to segment dataset based on time and concepts of metadata, and the pipeline is summarized into query, retrieval, filtering and diversification through hierarchical clustering. Minh-Triet et al. <ref type="bibr" coords="2,382.59,471.47,16.72,8.96" target="#b11">[12]</ref> proposed a novel method using conception coded feature augmentation to generate text descriptions to exploit further semantics of images; Bernd et al. <ref type="bibr" coords="2,317.90,495.47,16.61,8.96" target="#b12">[13]</ref> developed LifeXplore search system, a search and discovery tool that serves Lifelog domain researchers.</p><p>Ergina et al. <ref type="bibr" coords="2,187.19,519.47,16.72,8.96" target="#b13">[14]</ref> presented a method only based on visual concept using fine-tuned CNN with human-in-the-loop, and get a pretty good performance. The approaches basically consider the tasks as the problem of classification. In their methods, the training procedure are able be simplified with proper preprocessing methods. So, we propose three approaches of preprocessing. What's more, we attempt to present a new method of clustering for classification strategy to improve the performance of diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>Proposed method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The basic pipeline is described in Figure <ref type="figure" coords="2,287.55,656.54,3.76,8.96" target="#fig_0">1</ref>. The three approaches follow the same pipeline in earlier stage, but different in the methods of diversification. With the pipeline and the proper threshold, the output of the system is formed by generating a list of numbered images , which are both relevant and diversity to the query.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preprocessing</head><p>We apply three filtering metrics over the dataset. Two of them filter the blur images, another one filter images that are covered by any objects.</p><p>Blur filtering. The first filter is the Laplacian filter (3x3 kernel) with OpenCV implementation to calculate the blur as the variance of convolution result and set the threshold as 30 to avoid misjudging and removing the true positive images <ref type="bibr" coords="3,385.99,457.91,15.43,8.96" target="#b14">[15]</ref>. After first filtering, we demand a more refined metric to cut down the amount of images. Then a Fast Fourier Transform is applied to images. Once this step is completed, the average value in the transformed image is obtained and then scaled according to the size of the image to compensate for the tearing effect. The average value is then used for thresholding the image with the larger value representing the focused image and the lower value representing the blurred image.</p><p>Covered filtering. To detect if an image is covered by something or facing the ceiling or wall, we use detectors with maximum connected area calculator to calculate the proportion of subjects in the image. Then we remove the images that have a subject's size over 90% of the whole area. The method used is described as follows:</p><p>Step 1 Convert the images into grayscale images.</p><p>Step 2 Convert grayscale images into binary images.</p><p>Step 3 Convert the binary images into matrixes.</p><p>Step 4 Find the largest pattern in matrixes and calculate the proportion of it.</p><p>Step 5 Remove the images according to the result of matrix calculate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3</head><p>The two-class approach</p><p>In this approach, we consider each query topic independently to retrieve lifelog image according to the defined topic. For each topic, we get a two-class classifier that can classify the True images and False images after training. The topics defined by organizers are listed in Table 1 <ref type="bibr" coords="4,228.13,206.46,15.31,8.96" target="#b15">[16]</ref>. Find the moment when u1 was using smartphone when he was walking or standing outside.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>009</head><p>Wearing a red plaid shirt Find the moment when U1 was wearing a red plaid shirt 010 Having a meeting in China Find all moments when u1 was attending a meeting in China.</p><p>The performance of classifiers depends on the preprocessing methods and the training of CNNs. We propose to use supervised learning methods based on Alexnet or Googlenet, for what a pre-trained CNN has a better performance rather than training a CNN from the beginning. The details of the pipeline are described as follows:</p><p>1.Preprocessing the dataset as mentioned before. After all the blur filtering and covered filtering, it leaves us with 51.8K true images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.Directories choosing and concept filtering.</head><p>For each query topic, we divide them into directories based on different specific categories. Then we make the corresponding directories for each query by exerting the constraint on the specific topic, and manually select the proper directories by imposing restriction on the concepts. Besides the visual concept, we adapt the time stamp into concept that represent the period of a day and also the binary concept that shows if the music is played in specific moment <ref type="bibr" coords="4,431.46,639.62,15.41,8.96" target="#b16">[17]</ref>.</p><p>The directories we discussed are shown in table 2, the nulls in table mean that the constraint is not considered for avoid misjudging. The topic 009 is an exception corresponding to all images with no constraint. Besides, several state-of-art tools is used to filter the irrelevant images and the redundant images. For example, the relevant score is calculated by the concepts based on the referenced directories <ref type="bibr" coords="5,231.24,425.49,15.39,8.96" target="#b17">[18]</ref>. We apply VisiPics as a trick to remove the duplicate images for improving the diversification and the generalization performance <ref type="bibr" coords="5,434.35,437.49,15.39,8.96" target="#b18">[19]</ref>, and also lightening the load. All these tricks are implemented for a better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.Manual choosing several images as true.</head><p>We manually select 8 to 20 True images for each queries topic from the corresponding directory. The choosing procedure via the categories that predicted by using the Place CNN <ref type="bibr" coords="5,348.59,485.51,15.39,8.96" target="#b19">[20]</ref>, and the number of True images depends on the official preprocessing clustering result of meta dataset. 4.Trainning by implementing pre-trained CNN. Pre-trained convolutional neural network Alexnet or Googlenet (trained on ImageNet) is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.Testing on the chosen directory by fine-tuned CNN.</head><p>After training, we use the finetuned CNN to test on the corresponding directory. And the results are used as the training dataset for retrained fine-tuning CNN. 6.Splitting the results into two classes by relevance to the query topic. We move the results into two directories by images batching processing (we notice that the true images also have principle of locality). One of the directories is True, then the other is False.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.</head><p>Training by implementing the same pre-trained CNN. We use the same trained CNN that we used in step2 as well. 8.Testing on all data. The retrained CNN is applied to all images of entire dataset. 9.Saving the results in required format. The procedure is designed to automatically transfer the results as the collecting into CSV format in MATLAB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The eleven-class approach</head><p>With the basic pipeline proposed before, we apply the entire pipeline to all ten topics at once. We classify the True images of each topic into 10 classes. And the images which do not belong to these ten classes are classified as False. The 11 classes are 10 True classes of each topic and the only one False class. The pipeline before merging is the same as the former two-class approach, and the procedures after merging are presented below:</p><p>1.Trainning by implementing pre-trained CNN. The Alexnet or Googlenet is trained on the eleven classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.Testing on all data.</head><p>The retrained CNN is applied to all images of entire dataset. 3.Saving the results in the required format. The results are converted into required format automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">The clustering approach</head><p>The approach is quite same to the former approaches, the difference is that we propose a procedure of clustering right after the first-round retrieval. In clustering approach, we just merge the True images of each topic into 10 classes in spite of the False class for training. The pipeline before merging is the same as the Two-class approach, and the procedures after merging are presented below:</p><p>1.Trainning by implementing pre-trained CNN. The Alexnet or Googlenet is trained on the ten classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Testing on all data. The retrained CNN is applied to all images of entire dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.Clustering by implementing LVQ algorithm.</head><p>In this process of work, learning vector quantization (LVQ) algorithm is used for clustering. The main steps of algorithm include: initializing the prototype vector; iterative optimization, update the prototype vector. The details of algorithm are introduced below (A set of prototype vectors is initialized by randomly selecting a sample labeled ùë° q from the q cluster firstly):</p><formula xml:id="formula_0" coords="6,130.10,533.51,116.29,32.48">Algorithm 1 LVQ algorithm 1: Repeat 2:</formula><p>Pick a random sample 3:</p><p>Calculate the Euclidean distance from the sample to each prototype vector 4:</p><p>Find the shortest distance 5:</p><p>If y j = t i (same class) 6:</p><p>p' = p j + a (x j -p j ) (reduce the distance so that p become closer to the sample point) 7: Else 8:</p><p>p' = p j -a (x j -p j ) (increase the distance so that p become farther from the sample point) 9: p j = p'(update) 10: Until the condition to end is met.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Saving the results in the required format. The results are converted into required format automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results discussion</head><p>Due to time limit and other force majeure factors, we just submit one run follow basic two-class approach during the competition, however, we send our new results to the organizers after the deadline of submission and get an evaluation of our whole three retrieval approaches. The only run that we submitted during the competition follows the pipeline described in Section 3.</p><p>To evaluate the performance, the metrics used are F1@X (X=10), which measure the harmonic mean between Precision at X (P@X) and Cluster recall at X (CR@X), with X representing the top X results are taken into consideration in evaluation. So the diversity (via CR@10) and relevance (via P@10) are both taken into account. The results of our runs are displayed in Table3. The runs with * are submitted after the competition. The run 1 (with the details in table <ref type="table" coords="7,277.29,488.03,4.17,8.96" target="#tab_4">4</ref>) is the only run we submit through the evaluation system, while the run 5 has the best performance in diversification (CR@X), which is one of the most notable performance in lifelog moment retrieval. The details of last 4 runs are shown in Figure <ref type="figure" coords="7,238.33,524.03,3.76,8.96">2</ref>. Compared with run 3 and run 4, run 2 and run 5 have better performance. Which apparently shows that the eleven-class approach needs to be improved in precision.</p><p>From the charts we summarize that the query 8(Using smartphone outside) is difficult to retrieve, however, when we manually verify the retrieval results in query 8, we find that most of retrieval results are related to phone using and outdoor. So, we consider whether the problem is about the definition of outside? Using smartphone in the yards probably is not affirmed as true. Besides, the vital signs devices that included in dataset are noise for smartphone retrieval.</p><p>In the proposed pipeline, the manual operation is taken into consideration, so the results are fluctuating in a certain range. And to summarize from the results, the method of clustering is a significant influence factor, a better method of clustering improves the result rapidly in the performance in diversity, and also can improve the performance of relevance within limits.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Resources</head><p>Our approach is implemented using Intel(R) Xeon(R) CPU @3.50Ghz with 16G RAM. We work on Ubuntu16.04 using MATLAB 2019a. We use Neural Network Toolbox with GPU coder which generates CUDA from MATLAB code for deep learning. The OpenCV and PyTorch are also used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper presents our proposal for lifelog retrieval system on Lifelog moment retrieval (LMRT)subtask, which is corresponding to different types of queries, such as location, time, activity, and additional biometric data. As for procedure of preprocessing, we present three different methods of preprocessing to adapt the meta dataset to dataset which is appropriate in quantity and quality. During the retrieval process, we implement three different methods to compare the relevant and diversified performance of retrieval results. Experimental results show that our proposal is of high precision and reactiveness in official ranking metric F1-measure, while the relevant score is far better (P@10) than the diversified score (CR@10). Therefore, the ability of diversifying the results could be improved further, which helps to achieve a comprehensive and complete view of the query. The clustering method of our system is able to be updated to improve the performance of entire system.</p><p>As for future work, we will improve the pipeline of our proposal using the natural language processing approach such as RNN and LSTM to automatically match the query topics with visual concept. What's more, we will develop a user-friendly graphics interface for our proposal.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,252.29,348.74,90.88,8.10"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Proposed pipeline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,234.77,547.96,125.72,8.10;8,135.40,319.95,324.49,212.95"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Detail results for last 4 runs</figDesc><graphic coords="8,135.40,319.95,324.49,212.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,136.22,230.39,323.14,196.88"><head>Table 1 .</head><label>1</label><figDesc>The topics of LMRT</figDesc><table coords="4,136.22,248.30,323.14,178.97"><row><cell>Topic ID</cell><cell>Topic Title</cell><cell>Topic Description</cell></row><row><cell>001</cell><cell>In a Toyshop</cell><cell>Find the moment when u1 was looking at items in a toyshop</cell></row><row><cell>002</cell><cell>Driving home</cell><cell>Find any moment when u1 was driving home from the office</cell></row><row><cell>003</cell><cell>Seeking Food in a Fridge</cell><cell>Find the moments when u1 was looking in-side a refrigerator at home</cell></row><row><cell>004</cell><cell>Watching Football</cell><cell>Find the moments when either u1 or u2 was watching football on the TV</cell></row><row><cell>005</cell><cell>Coffee time</cell><cell>Find the moment when u1 was having cof-fee in a cafe</cell></row><row><cell>006</cell><cell>Having breakfast at home</cell><cell>Find the moment when u1 was having breakfast at home.</cell></row><row><cell>007</cell><cell>Having coffee with two person</cell><cell>Find the moment when u1 was having cof-fee with two person.</cell></row><row><cell>008</cell><cell>Using smartphone outside</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,135.38,150.35,323.39,235.64"><head>Table 2 .</head><label>2</label><figDesc>Selected Images per topic.</figDesc><table coords="5,135.38,168.74,323.39,217.25"><row><cell cols="2">Topic ID Location</cell><cell>Activity</cell><cell>Time</cell><cell>Concept</cell></row><row><cell>001</cell><cell>Not DCU</cell><cell></cell><cell></cell><cell>Not outdoor</cell></row><row><cell></cell><cell>Not Restaurant</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Not Home</cell><cell></cell><cell></cell></row><row><cell>002</cell><cell></cell><cell>transport</cell><cell>Not morning</cell></row><row><cell>003</cell><cell>Home</cell><cell></cell><cell></cell></row><row><cell>004</cell><cell>Not Park</cell><cell>Not walking</cell><cell></cell><cell>TV, unknown, No music</cell></row><row><cell></cell><cell></cell><cell>Not transport</cell><cell></cell></row><row><cell>005</cell><cell>Cafe,</cell><cell>Not transport</cell><cell></cell></row><row><cell></cell><cell>Unknown</cell><cell></cell><cell></cell></row><row><cell>006</cell><cell>Home</cell><cell>Not transport</cell><cell></cell><cell>Morning</cell></row><row><cell>007</cell><cell>Not DCU</cell><cell>Not walking</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Not transport</cell><cell></cell></row><row><cell>008</cell><cell>Not Home</cell><cell>Not transport</cell><cell></cell><cell>Not enclosed area</cell></row><row><cell>009</cell><cell></cell><cell></cell><cell></cell></row><row><cell>010</cell><cell>Not DCU</cell><cell></cell><cell></cell><cell>enclosed area</cell></row><row><cell></cell><cell>Not Home</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Not Work</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,133.82,383.42,318.69,89.11"><head>Table 3 .</head><label>3</label><figDesc>Result on ImageCLEFlifelog 2019 -LMRT challenge.</figDesc><table coords="7,133.82,401.81,318.69,70.72"><row><cell>Run ID</cell><cell>Description</cell><cell>P@10</cell><cell>CR@10</cell><cell>F1@10</cell></row><row><cell>Run 1</cell><cell>Two-class approach with Alexnet</cell><cell>0.71</cell><cell>0.380</cell><cell>0.440</cell></row><row><cell>Run 2*</cell><cell>Two-class approach with Googlenet</cell><cell>0.74</cell><cell>0.342</cell><cell>0.428</cell></row><row><cell>Run 3*</cell><cell>Eleven-class approach with Alexnet</cell><cell>0.41</cell><cell>0.305</cell><cell>0.330</cell></row><row><cell>Run 4*</cell><cell>Eleven-class approach with Googlenet</cell><cell>0.48</cell><cell>0.348</cell><cell>0.363</cell></row><row><cell>Run 5*</cell><cell>Clustering approach</cell><cell>0.59</cell><cell>0.498</cell><cell>0.481</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,132.98,150.35,328.04,160.34"><head>Table 4 .</head><label>4</label><figDesc>Detail results for run1</figDesc><table coords="8,132.98,169.17,328.04,141.52"><row><cell>RUN1</cell><cell>P@5</cell><cell>CR@ 5</cell><cell>F1@ 5</cell><cell>P@10</cell><cell>CR@ 10</cell><cell>F1@ 10</cell><cell>P@50</cell><cell>CR@ 50</cell><cell>F1@ 50</cell></row><row><cell>Query 1</cell><cell>0.8</cell><cell>0.5</cell><cell>0.615</cell><cell>0.9</cell><cell>1</cell><cell>0.947</cell><cell>0.24</cell><cell>1</cell><cell>0.387</cell></row><row><cell>Query 2</cell><cell>1</cell><cell>0.095</cell><cell>0.174</cell><cell>1</cell><cell>0.095</cell><cell>0.174</cell><cell>1</cell><cell>0.143</cell><cell>0.25</cell></row><row><cell>Query 3</cell><cell>1</cell><cell>0.167</cell><cell>0.286</cell><cell>0.7</cell><cell>0.278</cell><cell>0.398</cell><cell>0.56</cell><cell>0.778</cell><cell>0.651</cell></row><row><cell>Query 4</cell><cell>0.8</cell><cell>0.25</cell><cell>0.381</cell><cell>0.7</cell><cell>0.25</cell><cell>0.368</cell><cell>0.72</cell><cell>0.5</cell><cell>0.590</cell></row><row><cell>Query 5</cell><cell>0.8</cell><cell>0.333</cell><cell>0.471</cell><cell>0.8</cell><cell>0.333</cell><cell>0.471</cell><cell>0.74</cell><cell>0.333</cell><cell>0.46</cell></row><row><cell>Query 6</cell><cell>0.8</cell><cell>0.222</cell><cell>0.348</cell><cell>0.8</cell><cell>0.222</cell><cell>0.348</cell><cell>0.68</cell><cell>0.444</cell><cell>0.538</cell></row><row><cell>Query 7</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0.54</cell><cell>1</cell><cell>0.701</cell></row><row><cell>Query 8</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.08</cell><cell>0.333</cell><cell>0.129</cell></row><row><cell>Query 9</cell><cell>1</cell><cell>0.286</cell><cell>0.444</cell><cell>1</cell><cell>0.286</cell><cell>0.444</cell><cell>1</cell><cell>0.286</cell><cell>0.444</cell></row><row><cell>Query 10</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.2</cell><cell>0.333</cell><cell>0.25</cell><cell>0.52</cell><cell>1</cell><cell>0.684</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement:</head><p>This work is supported by <rs type="funder">Zhejiang Provincial Natural Science Foundation of China</rs> under Grant No. <rs type="grantNumber">LY18F020032</rs> and <rs type="funder">Natural Science Foundation of China</rs> under Grant No. <rs type="grantNumber">61502424</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Adbkq8D">
					<idno type="grant-number">LY18F020032</idno>
				</org>
				<org type="funding" xml:id="_XhFPuJW">
					<idno type="grant-number">61502424</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,132.67,507.40,338.10,8.10;9,141.74,518.44,203.09,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,310.81,507.40,109.11,8.10">Lifelogging: Personal big data</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Doherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,425.90,507.40,44.86,8.10;9,141.74,518.44,128.46,8.10">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="125" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,529.36,337.75,8.10;9,141.74,540.40,328.97,8.10;9,141.74,551.44,125.53,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,141.74,540.40,198.09,8.10">Building a Disclosed Lifelog Dataset: Challenges</title>
		<author>
			<persName coords=""><forename type="first">Duc</forename><surname>Tien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dang</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rashmi</forename><surname>Litingzhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Andcathalgurrin</surname></persName>
		</author>
		<idno type="DOI">1-6.10.1145/3095713.3095736</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,350.26,540.40,95.67,8.10">Principles and Processes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,562.36,337.70,8.10;9,141.74,573.40,329.02,8.10;9,141.74,584.44,328.97,8.10;9,141.74,595.36,328.62,8.10;9,141.74,606.31,158.33,8.19" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,409.33,562.36,61.03,8.10;9,141.74,573.40,329.02,8.10;9,141.74,584.44,8.81,8.10">Multimodal Segmentation of Lifelog Data Centre for Digital Video Proessing &amp; Adaptive Information Cluster</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Aiden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keansub</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,141.74,595.36,328.62,8.10;9,141.74,606.31,41.00,8.18">Procs. Large Scale Semantic Access to Content (Text, Image, Video, and Sound) (RIAO &apos;07)</title>
		<meeting>s. Large Scale Semantic Access to Content (Text, Image, Video, and Sound) (RIAO &apos;07)<address><addrLine>New York, USA; Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="21" to="38" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University, Ireland LabROSA, Columbia University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,617.44,337.81,8.10;9,141.74,628.39,328.83,8.10;9,141.74,639.43,328.84,8.10;9,141.74,650.47,328.84,8.10;9,141.74,661.39,328.81,8.10;9,141.74,672.43,328.62,8.10;9,141.74,683.47,328.64,8.10;10,141.74,150.35,328.74,8.10;10,141.74,161.39,328.54,8.10;10,141.74,172.31,328.63,8.10;10,141.74,183.35,149.29,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,453.06,650.47,17.52,8.10;9,141.74,661.39,127.29,8.10;9,315.14,683.47,44.21,8.10;9,384.22,683.47,86.16,8.10;10,141.74,150.35,328.74,8.10;10,141.74,161.39,112.21,8.10">Multimedia Retrieval in Medicine, Lifelogging, Security and Nature In: Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>P√© Teri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitri</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aleh</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M ; Narciso</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ergina</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Roberto Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikos</forename><surname>Cuevas Rodr√≠ Guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Konstantinos</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,260.88,161.39,209.39,8.10;10,141.74,172.31,116.83,8.10">Proceedings of the 10th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="10,345.28,172.31,125.08,8.10;10,141.74,183.35,26.42,8.10">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF 2019)<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Friedrich, Alba Garc√≠ a Seco de Herrera</note>
</biblStruct>

<biblStruct coords="10,132.67,194.39,338.06,8.10;10,141.74,205.31,328.66,8.10;10,141.74,216.35,328.86,8.10;10,141.74,227.39,329.07,8.10;10,141.74,238.31,211.95,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,404.55,205.31,65.84,8.10;10,141.74,216.35,268.07,8.10">Overview of Im-ageCLEFlifelog 2019: Solve my life puzzle and Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tu-Khiem</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Van-Tu</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="10,428.34,216.35,42.26,8.10;10,141.74,227.39,248.83,8.10">CLEF 2019 Working Notes. CEUR Workshop Proceedings (CEUR-WS.org</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,249.26,337.77,8.19;10,141.74,260.39,328.84,8.10;10,141.74,271.25,328.78,8.19;10,141.74,282.38,24.09,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,299.79,249.26,170.65,8.18;10,141.74,260.39,55.47,8.10">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,214.56,260.39,256.02,8.10;10,141.74,271.34,78.39,8.10;10,326.41,271.25,29.57,8.18">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>NIPS&apos;12</note>
</biblStruct>

<biblStruct coords="10,132.67,293.42,337.49,8.10;10,141.74,304.34,328.88,8.10;10,141.74,315.38,102.05,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,219.53,304.34,119.32,8.10">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,360.49,304.34,110.13,8.10;10,141.74,315.38,75.78,8.10">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,326.42,337.80,8.10;10,141.74,337.34,328.71,8.10;10,141.74,348.38,106.07,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,353.60,326.42,116.87,8.10;10,141.74,337.34,39.65,8.10">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,195.94,337.34,274.51,8.10;10,141.74,348.38,20.01,8.10">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,359.42,338.00,8.10;10,141.74,370.34,328.75,8.10;10,141.74,381.38,76.71,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,176.30,370.34,294.19,8.10;10,141.74,381.38,10.33,8.10">An Interactive Lifelog Retrieval System for Activities of Daily Living Understanding</title>
		<author>
			<persName coords=""><forename type="first">Liting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cath-Algurrin</forename><surname>Duc-Tien Dang-Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,170.04,381.38,21.91,8.10">CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,392.42,338.29,8.10;10,141.74,403.34,279.04,8.10" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,389.82,392.42,80.87,8.10;10,141.74,403.34,88.44,8.10">LIFER: An Interactive Lifelog Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">Liting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zaher</forename><surname>Hinbarji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathalgurrin</forename></persName>
		</author>
		<idno type="DOI">9-14.10.1145/3210539.3210542</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICMR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,414.38,338.21,8.10;10,141.74,425.42,312.44,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,281.78,414.38,113.03,8.10">Telling Stories with Mylifebits</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gemmell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lueder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,412.51,414.38,58.10,8.10;10,141.74,425.42,17.06,8.10;10,187.77,425.42,168.50,8.10">ICME 2005. IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="1536" to="1539" />
		</imprint>
	</monogr>
	<note>Multimedia and Expo</note>
</biblStruct>

<biblStruct coords="10,132.40,436.25,338.27,8.19;10,141.74,447.40,328.94,8.10;10,141.74,458.44,130.06,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,214.67,447.40,256.01,8.10;10,141.74,458.44,61.02,8.10">Lifelog Moment Retrieval with Visual Concept Fusion and Text-based Query Expansion</title>
		<author>
			<persName coords=""><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tung</forename><surname>Dinh-Duy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thanh-Dat</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Viet-Khoa</forename><surname>Vo-Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qu·ªëc</forename><surname>L∆∞∆°ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vinh-Tiep</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,221.31,458.44,21.91,8.10">CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,469.36,338.15,8.10;10,141.74,480.40,328.83,8.10;10,141.74,491.44,24.09,8.10" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,167.51,480.40,159.03,8.10">lifeXplore at the Lifelog Search Challenge</title>
		<author>
			<persName coords=""><forename type="first">Bernd</forename><surname>M√ºnzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Leibetseder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sabrina</forename><surname>Kletz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manfred</forename><surname>Primus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus</forename><surname>Schoeffmann</surname></persName>
		</author>
		<idno type="DOI">3-8.10.1145/3210539.3210541</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,502.36,338.26,8.10;10,141.74,513.40,148.86,8.10" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,432.55,502.36,38.11,8.10;10,141.74,513.40,81.82,8.10">Retrieving Events in Life Logging</title>
		<author>
			<persName coords=""><forename type="first">Ergina</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><forename type="middle">R</forename><surname>Del-Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Cuevas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Narciso</forename><surname>Garc√≠</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,524.44,338.14,8.10;10,141.74,535.36,328.97,8.10;10,141.74,546.40,137.89,8.10" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,284.32,524.44,161.56,8.10">Image partial blur detection and classification</title>
		<author>
			<persName coords=""><forename type="first">Renting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhaorong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">1-8.10.1109/CVPR.2008.4587465</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,451.01,524.44,19.53,8.10;10,141.74,535.36,298.28,8.10">IEEE International Conference on Computer Vision Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,557.44,338.39,8.10;10,141.74,568.36,328.79,8.10;10,141.74,579.40,160.50,8.10" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,176.66,568.36,293.87,8.10;10,141.74,579.40,93.71,8.10">Organizer Team at ImageCLEFlifelog 2017: Baseline Approaches for Lifelog Retrieval and Summarization</title>
		<author>
			<persName coords=""><forename type="first">Liting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giulia</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cath-Algurrin</forename><surname>Duc-Tien Dang-Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,254.06,579.40,21.78,8.10">CLEF</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,590.44,338.31,8.10;10,141.74,601.36,328.62,8.10;10,141.74,612.40,189.54,8.10" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,141.74,601.36,328.62,8.10;10,141.74,612.40,123.18,8.10">Visual Concept Selection with Textual Knowledge for Understanding Activities of Daily Living and Life Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Tsun-Hsien</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min-Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hen-Hsen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kuan-Ta</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,282.97,612.40,21.91,8.10">CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,623.47,338.10,8.10;10,141.74,634.39,180.95,8.10" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="10,248.17,623.47,222.33,8.10;10,141.74,634.39,150.45,8.10">Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">Ruth</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,645.43,337.90,8.10;10,141.74,656.47,328.69,8.10;10,141.74,667.39,236.56,8.10" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,250.87,645.43,219.44,8.10;10,141.74,656.47,197.78,8.10">Using High Performance Computing for Detecting Duplicate, Similar and Related Images in a Large Data Collection</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Trelogan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,412.69,656.47,57.73,8.10;10,141.74,667.39,144.36,8.10">Conquering Big Data with High Performance Computing</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Arora</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham.</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.28,150.35,338.25,8.10;11,141.74,161.39,328.97,8.10;11,141.74,172.31,24.12,8.10" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,364.99,150.35,105.54,8.10;11,141.74,161.39,80.49,8.10">A 10 million Image Database for Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,229.13,161.39,238.13,8.10">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
