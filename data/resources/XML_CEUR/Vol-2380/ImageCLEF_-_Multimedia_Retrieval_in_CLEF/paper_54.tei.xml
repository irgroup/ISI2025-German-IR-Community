<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,177.30,115.96,260.75,12.62;1,193.13,133.89,229.10,12.62">ImageCLEF 2019: Deep Learning for Tuberculosis CT Image Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.71,171.60,86.93,8.74"><forename type="first">Abdelkader</forename><surname>Hamadi</surname></persName>
							<email>abdelkader.hamadi@univ-mosta.dznoreddine.belhadjcheikh@univ-mosta.dzzouatineyamina@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Exact Sciences and Computer Science Mathematics and Computer Science Department Mostaganem</orgName>
								<orgName type="institution">University of Abdelhamid Ibn Badis Mostaganem</orgName>
								<address>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.41,170.02,47.06,6.12;1,322.05,171.60,81.26,8.74"><forename type="first">Noreddine</forename><forename type="middle">Belhadj</forename><surname>-9990-332x]</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Exact Sciences and Computer Science Mathematics and Computer Science Department Mostaganem</orgName>
								<orgName type="institution">University of Abdelhamid Ibn Badis Mostaganem</orgName>
								<address>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,406.63,171.60,70.02,8.74"><forename type="first">Yamina</forename><surname>Cheikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Exact Sciences and Computer Science Mathematics and Computer Science Department Mostaganem</orgName>
								<orgName type="institution">University of Abdelhamid Ibn Badis Mostaganem</orgName>
								<address>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,154.14,183.55,52.58,8.74"><forename type="first">Si</forename><surname>Zouatine</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Exact Sciences and Computer Science Mathematics and Computer Science Department Mostaganem</orgName>
								<orgName type="institution">University of Abdelhamid Ibn Badis Mostaganem</orgName>
								<address>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.05,183.55,75.40,8.74"><forename type="first">Bekkai</forename><surname>Mohamed</surname></persName>
							<email>bekkai.menad@univ-mosta.dzredha.djebbara@univ-mosta.dz</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Exact Sciences and Computer Science Mathematics and Computer Science Department Mostaganem</orgName>
								<orgName type="institution">University of Abdelhamid Ibn Badis Mostaganem</orgName>
								<address>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.78,183.55,26.98,8.74;1,343.85,183.55,74.04,8.74"><forename type="first">Mohamed</forename><forename type="middle">Redha</forename><surname>Menad</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Exact Sciences and Computer Science Mathematics and Computer Science Department Mostaganem</orgName>
								<orgName type="institution">University of Abdelhamid Ibn Badis Mostaganem</orgName>
								<address>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,421.20,183.55,40.02,8.74"><surname>Djebbara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Exact Sciences and Computer Science Mathematics and Computer Science Department Mostaganem</orgName>
								<orgName type="institution">University of Abdelhamid Ibn Badis Mostaganem</orgName>
								<address>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,177.30,115.96,260.75,12.62;1,193.13,133.89,229.10,12.62">ImageCLEF 2019: Deep Learning for Tuberculosis CT Image Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0D1DC4B0473FB9EB4F5AFB7A3803D3D8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>Tuberculosis Task</term>
					<term>Deep Learning</term>
					<term>CT Image</term>
					<term>Tuberculosis CT Image Classification</term>
					<term>Tuberculosis Severity Scoring</term>
					<term>CT Report</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this article, we present the methodologies used in our participation in the two subtasks of the ImageCLEF 2019 Tuberculosis Task (SVR and CTR). Our contributions are essentially based on deep learning and other machine learning techniques. In addition to the use of deep learners, semantic descriptors are tested to represent patients CT scans. These features are extracted after a first learning step. Our submissions on the test corpus reached AUC value of about 65% in the SVR task and an average AUC value of about 63% in CTR. These results offered us the seventh and the eighth places in SVR and CTR, respectively. We believe that our contributions could be further improved and might give better results if they applied properly and in an optimized way.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tuberculosis (TB) is a deadly disease. Its early diagnosis can give the necessary treatment and prevent the death of patients. The technological advancement especially in the field of artificial intelligence and precisely supervised learning opens the door for researchers to study the possibility of an automatic diagnosis. This would speed up the process and lower its cost. Several researchers have invested their efforts in recent years, especially within the medical image analysis community. In fact, a task dedicated to this disease had been adopted as part of the ImageCLEF evaluation campaign in its editions of the three last years <ref type="bibr" coords="2,460.67,130.95,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="2,472.84,130.95,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="2,134.77,142.90,7.01,8.74" target="#b6">7]</ref>. In this task, the objective is to analyze automatically the 3D CT images of TB patients to detect semantic information: the type of Tuberculosis, the degree of severity of the disease, information related to the state of the lungs, etc. In ImageCLEF 2019 two sub-tasks of the main task, "ImageCLEFmed Tuberculosis" are considered: Severity Scoring (SVR) and CT Report (CTR). In the first task, the goal is to deduce automatically from a CT image whether a TB case is severe or not. In the second one, the problematic consists of generating an automatic report that includes the following information in binary form (0 or 1): Left lung affected, right lung affected, presence of calcifications, presence of caverns, pleurisy, lung capacity decrease. based solely on the CT image. We can summarize the objectives of the Tuberculosis task through the following points:</p><p>-Helping medical doctors in the diagnosis and determining the state of the patient through image processing techniques; -Predicting quickly the TB severity degree to make quick decisions and give effective treatments; -Assist doctors and medical officers to have accurate details about the patient's lung condition by providing a report summarizing information describing the state of the lungs.</p><p>We present in the following section our work that had been made in the context of our participation in the two sub-tasks of ImageCLEF 2019 Tuberculosis task: Tuberculosis Severity Scoring (SVR) and Tuberculosis CT Report (CTR) <ref type="bibr" coords="2,167.55,423.42,9.96,8.74" target="#b6">[7]</ref>.</p><p>The remainder of this article is organized as follows. Section 2 describes the two tasks in which we had participated. In section 3, we present our contribution by detailing the system deployed to perform our submissions. Section 4 details our experimental protocols we used to generate our predictions. We present and analyze in the same section the results obtained. We make our conclusions in the last section by presenting potential perspectives and future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Participation to ImageCLEF 2019</head><p>ImageCLEF 2019 <ref type="bibr" coords="2,213.83,572.43,15.50,8.74" target="#b10">[11]</ref> is an evaluation campaign that is being organized as part of the CLEF initiative labs. This campaign offers several research tasks that welcome participation from teams around the world. For the 2019 edition, Image-CLEF organises four main tasks: ImageCLEFcoral, ImageCLEFlifelog, Image-CLEFmedical and ImageCLEFsecurity. In this work, we focus on the Tuberculosis task that takes part in the ImageCLEFmedical challenge. ImageCLEFmed Tuberculosis task includes two sub-tasks: Severity Scoring (SVR) and CT Report (CTR) that we describe in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SVR and CTR Tasks description</head><p>In this paper, we focus on our participation in the SVR and the CTR sub-tasks. The main objective of these two challenges is the automatic analysis of Tuberculosis CT scans. In both tasks, the same dataset is used, one corpus for training and another one for testing. The data is provided as 3D CT scans. All the CT images are stored in NIFTI file format with "nii.gz" extension file (gzipped .nii files). For each of the three dimensions of the CT image, we find a number of slices varying according to the dimension considered (512 images for the Y and X dimensions, from 40 to 250 images for the Z dimension). Each slice has a size of about 512×128 pixels for the X and Y dimensions and 512×512 pixels for the Z dimension.</p><p>A training collection is provided at the beginning of the task with its groundtruth (labels of samples). Participants prepare and train their systems on this dataset. A test collection is provided at a later date. Participants interrogate their system and submit their predictions to the organizers' committee. An evaluation is performed by the latter to compare the performance of the participants' predictions submissions.</p><p>SVR task aims to predict the degree of severity of TB cases. Given a CT scan of TB patient, the main goal is to predict the severity of his illness based on his 3D CT scan. The degree of severity is modeled according to 5 discrete values: from 1 ("critical/very bad") to 5 ("very good"). The score value is simplified so that values 1, 2 and 3 correspond to "high severity" class, and values 4 and 5 correspond to "low severity".</p><p>The classification problem is evaluated using two measures: 1) Area Under ROC-curve (AUC) and 2) Accuracy.</p><p>CT Report task has as objective to automatically generate a report based on the patient's CT scan. This report should include the following six pieces of information in the binary form (0 or 1):</p><p>1. Is the left lung affected? 2. Is the right lung affected? 3. The presence of calcifications; 4. The presence of caverns; 5. The presence of pleurisy; 6. The lung capacity decrease. This task is considered as a multilabel classification problem (6 binary findings). The ranking of this task is done first by average AUC and then by min AUC (both over the 6 CT findings).</p><p>We proposed to use the system presented in Figure <ref type="figure" coords="4,364.40,139.45,3.87,8.74" target="#fig_0">1</ref>. The latter goes through two essential steps: input data pre-processing and training a classification model. A third optional step is added in order to improve the performance of the first learning step. The latter includes a second learning stage by using a recurrent neural network (LSTM) or by generating semantic features and exploiting them through a learner or a deep learner. We will detail our proposed system in the following. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Learner</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slices, labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input data pre-processing</head><p>We remind that in both tasks, 3D CT scans are provided in compressed Nifti format. Firstly, we decompressed the files and extracted the slices. In the end, we got three sets of slices corresponding to the three dimensions of the 3D image. For each dimension and for each Nifti image we obtained a number of slices ranging according to the dimension considered (512 images for the Y and X dimensions, from 40 to 250 images for the Z dimension).</p><p>The visual content of the images that were extracted from the different dimensions is not similar. Indeed, the images of each dimension are taken from a different angle of view. We noticed from our experiments that the slices of the -Z-dimension give better results compared to the two others (X and Y). This remark concerns our proposed approaches. This is why we used in our work the Z-dimension. However, all steps can be applied to slices of any of the three dimensions.</p><p>After choosing the dimension to consider, we propose to filter the slices of each patient. Indeed, we can notice that many slices do not necessarily contain relevant information that could help to classify the samples. This is why we added a step to filter and select a number of slices per patient. For this, we propose two filtering approaches: Automatic supervised filtering: In this approach, we select a set of patients from each of the considered classes (the five degrees of severity for the SVR task). Then, a professional radiologist selects for each patient, the slices likely to contain relevant information indicating the presence of Tuberculosis. The resulting set of slices constitutes a filtering group. Given a new patient, we compare each of its slices to the filtering group by calculating a distance measure: a weighted sum of distances between the slice and those of the filtering group. This comparison can be done through a direct pixel-wise comparison. In our experiments we used the "Structural Similarity" as distance <ref type="bibr" coords="5,386.95,277.54,9.96,8.74" target="#b3">[4]</ref>. Unfortunately, we could not do a thorough to choose a better distance for lack of time. We selected at the end N slices that are judged to be the most similar to the filtering group. So, at the end, each patient is represented by the N filtered slices instead of all its extracted images. We think that this would reduce the noise introduced by the consideration of all slices. We tested in our contributions the value N=10. Automatic unsupervised filtering: We noticed that there is usually a maximum of 50/60 slices visually informative. Since the slices are ordered, the most informative slices are usually at the center of the list. We propose then to keep only the N middle ones. This is not optimal but we opted for this choice for a fully automatic and unsupervised approach. This choice can be improved by performing manual filtering with the intervention of a human expert, preferably with medical skills on TB disease.</p><p>Figure <ref type="figure" coords="5,181.14,656.12,4.98,8.74" target="#fig_1">2</ref> summarizes the pre-processing steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep learning model for CT image classification (first learning step)</head><p>As a deep learner, we chose to use Resnet-50 architecture because of its good results in the context of the same problematic in last Tuberculosis task editions <ref type="bibr" coords="6,159.04,174.78,14.61,8.74" target="#b12">[13]</ref>. On the other hand, we developed a model that we called "LungNet". We present more details about this deep learner in the following section. The outputs of the deep learners deployed are considered as initial results. We exploited then these outputs to generate: 1) semantic features of a patient that are used to reclassify the samples, and 2) features of slices organized in a sequence format that are fed to LSTM input as described in section 3.5.</p><p>LungNet Deep Learner: We proposed and developed our deep learner architecture for CT Image Analysis that we called "LungNet". The input to the latter is an RGB image of size 119x119, followed by five convolutional layers and two fully connected layers. Initially, input data were in nifty format. Slices of the CT scans are 1-channel gray-level images. However, we extracted the slices using med2image tool [1]. This software converts the slices to jpeg format. To avoid introducing noise by using this extraction method, we can do better by reading directly image pixels values using Niftilib library for python that was suggested by the task organizers. The idea behind using med2image to extract slices is that we planned to filter the slices by a medical expert intervention. This process required the slices to be in a format easily visible to the expert.</p><p>After each convolutional layer "relu" activation is applied followed by a local normalization and MaxPooling. The first, the second and the third convolution blocks have dropout layers to reduce overfitting. The sigmoid activation function is applied to the output layer in order to predict values in the range of 0 to 1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semantic Features extraction</head><p>We implemented the method of semantic descriptors extraction described in <ref type="bibr" coords="7,470.08,464.84,10.52,8.74" target="#b8">[9]</ref> with slight differences. After slices extraction and filtering, we generated a single descriptor per patient to exploit it through a transfer learning process. The results of SGEast <ref type="bibr" coords="7,204.94,500.70,15.50,8.74" target="#b12">[13]</ref> and even other teams in the same task of ImageCLEF 2017 proved the efficiency of this approach <ref type="bibr" coords="7,300.92,512.66,15.50,8.74" target="#b12">[13,</ref><ref type="bibr" coords="7,318.08,512.66,7.01,8.74" target="#b8">9]</ref>. So, we chose to exploit the probabilities that were predicted by a deep learner trained on a set of slices. If K is the number of classes considered, these predictions typically correspond to the K predicted probability values for the K classes (For SVR Task, K = 5: the five severity degrees). We obtain then for each slice K values corresponding to the probabilities of the K considered classes. Furthermore, K sub-descriptors are generated: D 1 , D 2 , D 3 , D 4 , ... D k . Each sub-descriptor D i contains the predicted probabilities for the class i for all the slices of the patient. A final semantic descriptor is constructed by concatenating the K sub-descriptors. Figure <ref type="figure" coords="7,263.87,644.16,4.98,8.74" target="#fig_4">4</ref> details the whole process of the semantic features extraction for one patient. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slices, labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Learner</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning a classification model based on semantic features (second learning step)</head><p>We propose to exploit the semantic descriptors of patients described previously. Any approach of supervised classification can be applied as shown in figure <ref type="figure" coords="8,472.84,435.44,3.87,8.74" target="#fig_5">5</ref>. We tested in our experiments SVM as supervised classifier. However, Random Forests and bagging of Random Forests have shown good results in the context of the same problematic <ref type="bibr" coords="8,243.05,471.31,9.96,8.74" target="#b8">[9]</ref>.</p><p>We recommend some ideas for this step:</p><p>-To use a deep learner having as input the semantic descriptors of patients and the labels of patients. As an alternative, it would be interesting to use a bagging method that collaborates several learners and sub-samples the train collection. This would lead to better results as presented in <ref type="bibr" coords="8,413.58,551.01,9.96,8.74" target="#b8">[9]</ref>; -To apply samples selection and data augmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">LSTM as classification model</head><p>As each patient is described by a sequence of slices, it is interesting to test the LSTM (Long Short-Term Memory) <ref type="bibr" coords="9,289.04,423.48,15.50,8.74" target="#b9">[10]</ref> recurrent neural network that is suitable for such data type. However, it is not recommended to apply LSTM on slices as input. Extracting features from slices using deep learner and pushing them to LSTM seems to be a good alternative. We propose to describe each slice by a feature of size equal to the number of the considered classes (five classes for SVR task). This feature is composed of the five values corresponding to the probabilities of the considered classes. These values are obtained through a deep learning stage. After generating these features, they are fed to an LSTM neural network by considering the ordered set of slices of each patient as a sequence. Figure <ref type="figure" coords="9,166.20,531.08,4.98,8.74">6</ref> describes the whole process. The same dataset is given for the CTR task. The samples are labeled regarding to seven main target classes:</p><p>1. Target classes for SVR Task:</p><p>(a) SVR severity (binary class: HIGH and LOW). Another label called md Severity is given (Five discrete values ranging from 1 to 5). We remind that values of md Severity (1, 2 and 3) belong to the "HIGH" Severity case. The other two values (4 and 5) correspond to the "LOW" Severity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SVR task</head><p>Experimental protocol: We used the train collection provided by the organizers and we split it into two sub-collections: training and validation sets. We finally submitted three main runs. The other submissions concern some tested approaches that we could not optimize and finalize correctly because of lack of time:</p><p>-SVR FSEI resnet50 run3: results of ResNet-50 trained on 50% of training data. Each patient was represented by 50 slices filtered using the automatic unsupervised filtering approach that was described in section 3.1. The slices were adapted by resizing them directly using the Python Imaging Library (PIL). The input images of Resnet50 are of size 199 × 199;</p><p>-SVR FSEI lungnet run2: results of LungNet deep learner trained on 80% of data. Each patient was represented by 10 slices filtered using the automatic supervised filtering approach that was described in section 3.1;</p><p>-SVR FSEI lstm run8: results of LSTM exploiting outputs of Lungnet deep learner. Each patient was represented by 50 slices filtered using the automatic unsupervised filtering approach. So, a sequence for the LSTM learner is composed of the 50 features representing the 50 slices of the patient.</p><p>We considered for each run a hierarchical classification problem. Firstly, we classified the samples in the 5 classes corresponding to the five degrees of severity. Secondly, We deduced for each patient its predicted class using a majority vote on the predicted labels of all slices. Finally, the class predicted in the previous step is transformed to a binary value corresponding to the SVR Severity class (HIGH if predicted class ∈ {1, 2, 3} and LOW if not).</p><p>Our tools and scripts used in our experiments are accessible in <ref type="bibr" coords="12,426.42,202.68,9.96,8.74" target="#b1">[2]</ref>.</p><p>Results: Table <ref type="table" coords="12,209.75,232.57,4.98,8.74" target="#tab_1">2</ref> shows the results in terms of AUC and accuracy obtained by our runs on the evaluation performed by the ImageCLEF committee on test collection. We can see that SVR FSEI resnet50 run3 got the best performance followed by SVR FSEI lstm run8. These two runs were ranked 22th and 25th out of 54 submissions.</p><p>We note that for SVR FSEI lungnet run2 patients were represented by 10 slices (50 slices for the two other runs), it would be interesting to see the performance of the Lungnet model after training it on 50 slices per patient in order to make a detailed comparison with the other two runs.</p><p>Figures <ref type="figure" coords="12,185.47,483.60,4.98,8.74">7</ref> and<ref type="figure" coords="12,213.94,483.60,4.98,8.74" target="#fig_7">8</ref> describe the results and ranking of all submissions of SVR task in terms of AUC and accuracy, respectively.</p><p>Although the results achieved by our submissions are not well ranked compared to those of the top of the list, we can notice that several runs belong to the same teams that had good results, and they probably do not differ too much. On the other hand, We believe that our models could give better results after a more advanced data preprocessing including the use of masks, samples selection and data augmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CTR task</head><p>Experimental protocol: We trained in a first step our deep models (Resnet and Lungnet). Secondly, we generated the semantic descriptors following the approach described in section 3. We treated the problematic as a multilabel classification problem in the first learning stage and as a binary classification problem in the second learning stage. We used in the latter SVM as a binary classifier. We optimized its parameters independently for each target class.</p><p>We submitted three main runs:</p><p>1. CTR FSEI run1: results of LungNet trained on 50% of training data. Each patient was represented by 10 slices filtered using the automatic supervised filtering approach that was described in section 3.1;</p><p>2. CTR FSEI run2 : results of LungNet trained on 70% of training data. Each patient was represented by 50 slices filtered using the automatic unsupervised filtering approach that was described in section 3.1;</p><p>3. CTR FSEI run5: SVM using semantic features that are extracted using Resnet-50. Each patient was represented by 10 slices filtered using the automatic supervised filtering approach that was described in section 3.1.</p><p>Our tools and scripts used in our experiments are accessible in <ref type="bibr" coords="14,426.42,345.07,9.96,8.74" target="#b1">[2]</ref>.</p><p>Results: Table <ref type="table" coords="14,207.93,378.41,4.98,8.74" target="#tab_2">3</ref> shows the results (in terms of Average-AUC and Min-AUC) and ranking obtained by our runs on the evaluation performed by the Image-CLEF committee on test collection. We can see that our best results were obtained by CTR FSEI run1 followed by CTR FSEI run2. However, we should mention here that we used the same sub-division of the corpus in two sub-parts (train and validation) for all CTR target classes, which is not optimal since the distribution of class values is not the same for the six target classes. This explains the disadvantage of the run CTR FSEI run5 compared to the other two and also the low value of Min-AUC for the three runs. We believe that the semantic descriptors approach might perform better by making more efforts to optimize parameters or by testing another learner like the Bagging of Random Forests as presented in <ref type="bibr" coords="14,146.47,632.21,9.96,8.74" target="#b8">[9]</ref>. Considering a multi-label classifier constitutes also an interesting idea to test.</p><p>Figure <ref type="figure" coords="15,181.15,118.99,4.98,8.74" target="#fig_8">9</ref> describes the results (in terms of Average-AUC) and ranking of all submissions of CTR task. Our best two runs were ranked 14th and 17th out of 35 submissions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and future works</head><p>We have described in this article our contributions to the SVR and CTR subtasks of ImageCLEFmed 2019 Tuberculosis task. We proposed to use after a data preprocessing step, a deep learner to classify samples to the target classes. We used for that, ResNet-50 and proposed our LungNet architecture. Moreover, we proposed to extract a single semantic descriptor for each CT image/patient instead of considering all the slices as separate samples. We tested also LSTM as another alternative. Although our proposals had not been the best, the obtained results showed that these approaches could be much more efficient and might give more interesting results if they are applied in an optimized way.</p><p>As perspectives, we plan to adopt data augmentation strategies and learning samples selection. In addition, we noticed during the sub-sampling of our data that the deletion or addition of some samples had an impact on the results. On the other hand, filtering slices in an optimized way is a key idea that could further improve the system performance. Moreover, we noticed in our experiments that there is a difference of precision for each severity class studied which arises the hypothesis of the classes having varying difficulties to be identified by the model. Indeed, some classes are more difficult to identify than others. It is also an interesting track to study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,195.95,425.73,223.46,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The architecture of the overall proposed system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,233.70,515.38,147.97,7.89"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pre-processing of input data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,149.71,449.75,255.09,8.74"><head>Figure 3</head><label>3</label><figDesc>Figure 3 illustrates the architecture of the Lungnet model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,197.75,414.07,219.85,7.89;7,166.43,143.08,282.69,255.87"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The architecture of the LungNet Deep Learner</figDesc><graphic coords="7,166.43,143.08,282.69,255.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,210.94,357.38,193.49,7.89"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Semantic features extraction process<ref type="bibr" coords="8,392.13,357.41,9.22,7.86" target="#b8">[9]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,152.69,357.38,309.98,7.89"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Learning a classification model based on the semantic descriptors [9].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,138.97,295.41,210.66,8.74;11,150.93,318.63,97.00,8.74;11,150.37,330.24,104.19,8.74;11,151.48,341.85,127.86,8.74;11,150.37,353.46,105.46,8.74;11,151.48,365.08,106.58,8.74;11,152.09,376.69,120.27,8.74"><head>2 .</head><label>2</label><figDesc>Target classes for CTR Task (binary classes): (a) Left lung affected; (b) Right lung affected; (c) Presence of calcifications; (d) Presence of caverns; (e) Presence of pleurisy; (f) Lung capacity decrease.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="13,153.16,590.45,309.05,7.89"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Results and ranking in terms of Accuracy on test data for SVR Task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="15,137.16,413.14,341.03,7.89"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Results and ranking in terms of Mean-AUC on test collection for CTR Task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,173.05,116.41,269.25,40.12"><head>Table 1 .</head><label>1</label><figDesc>Dataset given for Tuberculosis SVR and CTR tasks<ref type="bibr" coords="11,425.41,116.44,13.52,7.86" target="#b10">[11]</ref>.</figDesc><table coords="11,202.80,137.31,209.76,19.22"><row><cell></cell><cell cols="2">Train Collection Test Collection</cell></row><row><cell>Number of patients</cell><cell>218</cell><cell>117</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,199.70,286.04,215.95,60.19"><head>Table 2 .</head><label>2</label><figDesc>Results on test set for SVR task.</figDesc><table coords="12,199.70,305.09,215.95,41.14"><row><cell>Runs</cell><cell cols="2">AUC Accuracy Rank</cell></row><row><cell cols="2">SVR FSEI resnet50 run3 0.6510 0.6154</cell><cell>22</cell></row><row><cell cols="2">SVR FSEI lungnet run2 0.6103 0.5983</cell><cell>33</cell></row><row><cell>SVR FSEI lstm run8</cell><cell>0.6475 0.6068</cell><cell>25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="14,214.37,433.26,186.61,60.19"><head>Table 3 .</head><label>3</label><figDesc>Results on test set for CTR task.</figDesc><table coords="14,214.37,452.32,186.61,41.14"><row><cell>Runs</cell><cell cols="3">Mean AUC Min AUC Rank</cell></row><row><cell>CTR FSEI run1</cell><cell>0.6273</cell><cell>0.4877</cell><cell>14</cell></row><row><cell>CTR FSEI run2</cell><cell>0.6061</cell><cell>0.4471</cell><cell>17</cell></row><row><cell>CTR FSEI run5</cell><cell>0.5064</cell><cell>0.4134</cell><cell>32</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Learner</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and results</head><p>We describe in the following sections our main runs submitted to the SVR and CTR tasks.</p><p>We used in our experimental work the following tools:</p><p>-med2image [1] for the conversion of nifti medical images to the classic Jpeg format; -Tensorflow frawework <ref type="bibr" coords="10,248.97,494.72,10.52,8.74" target="#b2">[3]</ref> and Keras library <ref type="bibr" coords="10,342.93,494.72,10.52,8.74" target="#b4">[5]</ref> for deep learning; scikit-learn <ref type="bibr" coords="10,202.98,506.68,15.50,8.74" target="#b11">[12]</ref> library for testing several machine learning techniques.</p><p>We chose to use slices of the -Z-dimension because our experiments showed that they are more suitable than those of the two other ones and got better results.</p><p>Dataset: The dataset used in the SVR task includes chest CT scans of TB patients along with some metadata that describe a set of 19 classes. 2 classes concern the SVR task, six other classes concern CTR task. The other values are considered as additional information regarding to the patients. They could be used as contextual information.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,183.54,480.51,3.27,67.97;13,190.25,480.44,10.04,49.35;13,203.73,480.51,9.98,63.01;13,217.16,480.52,9.98,32.79;13,230.58,480.52,9.98,65.76;13,244.00,480.52,9.98,76.29;13,257.43,480.48,30.12,61.44;13,290.99,480.44,3.27,41.71;13,297.70,476.15,3.27,89.81;13,304.41,480.48,3.27,40.76;13,311.12,480.49,3.27,48.84;13,317.83,480.59,9.98,57.06;13,331.26,480.44,3.27,32.10;13,337.97,480.44,3.27,68.58;13,344.68,480.49,3.27,87.99;13,351.39,480.52,3.27,83.46;13,358.10,480.44,3.27,39.20;13,364.81,476.15,3.27,90.35;13,371.53,480.44,3.27,47.67;13,378.24,480.34,3.27,23.81;13,384.95,476.15,3.27,90.27;13,391.66,480.34,3.27,23.81;13,398.37,480.44,3.27,47.67;13,405.14,480.50,3.27,60.28;13,411.85,480.34,23.41,23.89;13,438.70,480.44,3.27,47.67;13,445.41,480.44,3.27,39.20;13,452.13,480.34,3.27,23.81;13,458.84,480.44,3.27,47.67;13,465.55,476.15,3.27,90.65;16,134.77,184.74,62.94,10.52" xml:id="b0">
	<monogr>
		<idno>-4.txt SVR-SVM-axis-mode-8.txt SVR-MC-4.txt SVR-MC-8.txt SVR_HHU_DBS2_run01</idno>
		<ptr target="80_1…run_6.csvSVRtest-model3.txtSVR_GNN_node2vec.csvrun_4.csvrun_7.csvSVRab.txtrun_5.csvSVRtest-model2.txtSVRfree-text.txtrun_3.csvSVRtest-model4" />
		<title level="m" coord="13,183.54,480.51,2.77,67.97;13,190.25,480.44,10.04,49.35;13,203.73,480.51,2.42,63.01">SRV_run1_linear.txt subm_SVR_Severity SVR-SVM-axis-mode</title>
		<imprint/>
	</monogr>
	<note>SRV_run2_less_features. SVR_HHU_DBS2_run02.txt SVR_From_Meta_Report1c.csv SVR_From_Meta_Report1c.. SVR_FSEI_run8_lstm_5_55_sD_lun… SVRtest-model1.txt run_8.csv SVR_FSEI_run2_lungnet_train. .txt SVR_FSEI_run9_oneSVM_desSem_… References</note>
</biblStruct>

<biblStruct coords="16,142.96,219.79,337.63,7.86;16,151.52,230.75,329.07,7.86;16,151.52,241.71,318.86,7.86" xml:id="b1">
	<monogr>
		<ptr target="https://github.com/anouar1991/imageCLEFfsei/tree/master/tools/application" />
		<title level="m" coord="16,151.53,219.79,329.06,7.86;16,151.52,230.75,324.82,7.86">Resnet-50 and lungnet for tuberculosis severity scoring. mostaganem university at imageclefmed 2019 : tools to run experiments</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,252.52,337.64,7.86;16,151.52,263.48,329.07,7.86;16,151.52,274.44,329.07,7.86;16,151.52,285.40,329.07,7.86;16,151.52,296.35,329.07,7.86;16,151.52,307.31,329.07,7.86;16,151.52,318.27,329.07,7.86;16,151.52,329.23,247.31,7.86" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/,softwareavailablefromtensorflow.org" />
		<title level="m" coord="16,167.61,318.27,280.76,7.86">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,340.04,337.63,7.86;16,151.52,350.97,329.07,7.89;16,151.52,361.96,225.99,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,298.86,340.04,181.73,7.86;16,151.52,351.00,84.59,7.86">On the mathematical properties of the structural similarity index</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Brunet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">R</forename><surname>Vrscay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2011.2173206</idno>
		<ptr target="https://doi.org/10.1109/TIP.2011.2173206" />
	</analytic>
	<monogr>
		<title level="j" coord="16,243.62,351.00,163.54,7.86">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1488" to="1499" />
			<date type="published" when="2012-04">April 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,372.77,277.43,7.86" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<title level="m" coord="16,226.66,372.77,21.39,7.86">Keras</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,383.58,337.64,7.86;16,151.52,394.53,329.07,7.86;16,151.52,405.49,329.07,7.86;16,151.52,416.45,288.03,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,442.93,383.58,37.66,7.86;16,151.52,394.53,329.07,7.86;16,151.52,405.49,23.74,7.86">Overview of ImageCLEFtuberculosis 2017 -predicting tuberculosis type and drug resistances</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kalinovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="16,200.37,405.49,108.50,7.86">CLEF2017 Working Notes</title>
		<title level="s" coord="16,318.18,405.49,162.42,7.86;16,151.52,416.45,14.99,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,427.26,337.63,7.86;16,151.52,438.22,329.07,7.86;16,151.52,449.18,329.07,7.86;16,151.52,460.14,329.07,7.86;16,151.52,471.10,111.64,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,151.52,438.22,329.07,7.86;16,151.52,449.18,166.17,7.86">Overview of ImageCLEFtuberculosis 2019 -automatic ct-based report generation and tuberculosis severity assessment</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="16,340.28,449.18,105.76,7.86">CLEF2019 Working Notes</title>
		<title level="s" coord="16,453.98,449.18,26.62,7.86;16,151.52,460.14,143.03,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,481.91,337.64,7.86;16,151.52,492.86,329.07,7.86;16,151.52,503.82,329.07,7.86;16,151.52,514.78,329.07,7.86;16,151.52,525.74,34.31,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,378.22,481.91,102.37,7.86;16,151.52,492.86,329.07,7.86;16,151.52,503.82,108.19,7.86">Overview of ImageCLEFtuberculosis 2018 -detecting multi-drug resistance, classifying tuberculosis type, and assessing severity score</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="16,279.94,503.82,103.81,7.86">CLEF2018 Working Notes</title>
		<title level="s" coord="16,390.71,503.82,89.89,7.86;16,151.52,514.78,82.70,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,536.55,337.63,7.86;16,151.52,547.51,329.07,7.86;16,151.52,558.47,329.07,7.86;16,151.52,569.43,169.82,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,268.16,536.55,212.43,7.86;16,151.52,547.51,108.56,7.86">Imageclef 2018: Semantic descriptors for tuberculosis CT image classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hamadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Yagoub</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2125/paper82.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="16,282.71,547.51,197.88,7.86;16,151.52,558.47,121.19,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,580.24,337.98,7.86;16,151.52,591.17,92.85,7.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,288.76,580.24,100.72,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,398.70,580.24,81.90,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.62,602.00,337.98,7.86;16,151.52,612.96,329.07,7.86;16,151.52,623.92,329.07,7.86;16,151.52,634.88,329.07,7.86;16,151.52,645.84,329.07,7.86;16,151.52,656.80,329.07,7.86;17,151.52,119.67,329.07,7.86;17,151.52,130.63,329.07,7.86;17,151.52,141.59,329.07,7.86;17,151.52,152.55,216.27,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,268.29,656.80,212.30,7.86;17,151.52,119.67,124.23,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,297.67,119.67,182.92,7.86;17,151.52,130.63,329.07,7.86;17,151.52,141.59,122.46,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="17,305.55,141.59,171.07,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="17,142.62,163.51,337.98,7.86;17,151.52,174.47,329.07,7.86;17,151.52,185.43,329.07,7.86;17,151.52,196.36,325.87,7.89" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="17,394.26,185.43,86.33,7.86;17,151.52,196.39,73.64,7.86">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,232.82,196.39,155.11,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.62,207.34,287.72,7.86;17,151.52,218.30,329.07,7.86;17,151.52,229.26,329.07,7.86;17,151.52,240.22,174.43,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="17,329.02,207.34,101.31,7.86;17,151.52,218.30,112.73,7.86">Imageclef 2017: Imageclef task -the sgeast submission</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">X M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1866/paper130.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="17,286.45,218.30,194.14,7.86;17,151.52,229.26,123.03,7.86">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
