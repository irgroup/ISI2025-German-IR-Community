<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.08,115.96,295.20,12.62;1,148.04,133.89,319.28,12.62;1,260.55,151.82,94.25,12.62">Biomedical Concept Detection in Medical Images: MQ-CSIRO at 2019 ImageCLEFmed Caption Task</title>
				<funder>
					<orgName type="full">CSIRO</orgName>
				</funder>
				<funder>
					<orgName type="full">Australian Government</orgName>
				</funder>
				<funder ref="#_epqp5sH">
					<orgName type="full">Macquarie University</orgName>
				</funder>
				<funder>
					<orgName type="full">National Computational Infrastructure</orgName>
					<orgName type="abbreviated">NCI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,159.18,189.79,50.37,8.74"><forename type="first">Sonit</forename><surname>Singh</surname></persName>
							<email>sonit.singh@hdr.mq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Macquarie University</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">DATA61</orgName>
								<orgName type="institution">CSIRO</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.44,189.79,68.42,8.74"><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DATA61</orgName>
								<orgName type="institution">CSIRO</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.41,189.79,66.43,8.74"><forename type="first">Kevin</forename><surname>Ho-Shon</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Macquarie University Health Sciences Centre</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.76,189.79,49.95,8.74"><forename type="first">Len</forename><surname>Hamey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Macquarie University</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.08,115.96,295.20,12.62;1,148.04,133.89,319.28,12.62;1,260.55,151.82,94.25,12.62">Biomedical Concept Detection in Medical Images: MQ-CSIRO at 2019 ImageCLEFmed Caption Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">444090BC4D3F77928F085884A4DE5ABC</idno>
					<note type="submission">letter C0560453: jump C1550043: identified C0034975: registry</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Imaging</term>
					<term>Concept Detection</term>
					<term>Caption Prediction</term>
					<term>Computer Vision</term>
					<term>Convolutional Neural Network</term>
					<term>Multi-label classification C0019066: non-traumatic hemoperitoneum C0162868: false aneurysm C0037993: lien C0607422: abdoman C0025474: mesenteric membrane C0009924: materials C0441633: diagnostic scanning C0003842: arteri C0449900: contrasting Concepts present: C0015252: surgical removal procedure C0007876: caesarean section (c-section) delivery C0542560: degrees C0021815: discus intervertebralis C0056663: cyanmethaeglobin C1552858: section C1318154: root [a body part] C0546660: methemoglobin (methb) level test C0965970: et combination C0728940: excisional C0251244: alexanian protocol C0442106: intervertebral C0052142: ap combination C0549207: bone tissue of vertebra C0005847: blood vessel structure C0184905: bisection C0003842: arteri Concepts present: C0086972: separated status C0022646: nephros C0227665: kidneys bilateral C0030797: region medication C0871472: t-test C0969625: methodology C0038435: stressed</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our concept detection system submitted for the ImageCLEFmed Caption task, part of the ImageCLEF 2019 challenge. The advancements in imaging technologies has improved the ability of clinicians to detect, diagnose, and treat diseases. Radiologists routinely interpret medical images and summarise their findings in the form of radiology reports. The mapping of visual information present in medical images to the condensed textual description is a tedious, time-consuming, expensive, and error-prone task. The development of methods that can automatically detect the presence and location of medical concepts in medical images can improve the efficiency of radiologists, reduce the burden of manual interpretation, and also help in reducing diagnostic errors. We propose a Convolutional Neural Network based multi-label image classifier to predict relevant concepts present in medical images. The proposed method achieved an F1-score of 0.1435 on the held-out test set of the 2019 ImageCLEFmed Caption Task. We present our proposed system with data analysis, experimental results, comparison, and discussion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical images contain rich semantic information in the form of concepts, attributes, and their interaction. Modelling the rich semantic information and its dependencies is essential for understanding medical images. Due to the rapid increase in big data, continuous evolution of medical imaging technologies, and the rise of electronic health records, medical imaging data is accumulating at a very fast pace. Automated understanding of medical images is highly beneficial for clinicians to provide useful insights and reduce the significant burden of the overall clinical workflow. Motivated by this need of automated image understanding methods in the healthcare domain, ImageCLEF<ref type="foot" coords="2,398.51,153.28,3.97,6.12" target="#foot_0">4</ref>  <ref type="bibr" coords="2,406.48,154.86,15.50,8.74" target="#b15">[16]</ref> organised its first concept detection and caption prediction tasks in 2017. The main objective of the concept detection task is to automatically find relevant clinical concepts present in medical images. Concept detection is also important for improving various downstream tasks such as knowledge discovery, medical report generation, question answering, and clinical decision making. Figure <ref type="figure" coords="2,412.36,214.64,4.98,8.74">1</ref> shows sample images and their corresponding relevant clinical concepts present in the training set provided by the challenge organisers.</p><p>ImageCLEF is an evaluation campaign organised as a part of the Conference and Labs of the Evaluation Forum (CLEF) initiative. In 2019, the Im-ageCLEFmedical proposed three tasks namely, Visual Question Answering <ref type="bibr" coords="2,467.31,274.41,9.96,8.74" target="#b2">[3]</ref>, Caption Analysis <ref type="bibr" coords="2,215.96,286.37,14.61,8.74" target="#b20">[21]</ref>, and tuberculosis <ref type="bibr" coords="2,315.04,286.37,9.96,8.74" target="#b8">[9]</ref>. This paper describes the participation of the MQ-CSIRO (Macquarie University and CSIRO, Sydney) team participation in the 3 rd edition of ImageCLEFmed Caption task 2019. The task consists of identifying the UMLS (Unified Medical Language System) Concept Unique Identifiers (CUI) <ref type="bibr" coords="2,245.40,334.19,10.52,8.74" target="#b4">[5]</ref> present in each sample image. Each medical image can be annotated with multiple concepts, making it a multi-label image classification task. Compared to single-label classification where an image is associated with a single label from a finite set of disjoint labels, multi-label classification associates a single image with multiple labels which may have semantic dependencies between them. We identified the relevant concepts present in medical images based on a multi-label classification model using Convolutional Neural Network (CNN). In section 2, we describe work in multi-label image classification. Section 3 describes building blocks of a convolutional neural network. In section 4, we describe our data exploration, experimental settings, and analysis of results. Finally, section 5 provides conclusion and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-label image classification is a fundamental task towards general visual understanding. Both medical images and natural images contain diverse semantic content that need multiple visual concepts to classify <ref type="bibr" coords="2,370.58,530.76,14.61,8.74" target="#b18">[19]</ref>. Compared to singlelabel classification, multi-label image classification is more challenging due to the association of concepts with semantic regions and capturing the semantic dependencies among concepts. In the following subsections, we explore work related to multi-label image classification in natural and medical images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-label image classification</head><p>The performance of image classification has recently experienced a rapid progress due to the establishment of large-scale hand-labeled datasets such as ImageNet <ref type="bibr" coords="2,471.14,637.66,15.50,8.74" target="#b23">[24]</ref> Fig. <ref type="figure" coords="3,160.11,666.49,3.87,8.74">1</ref>: Sample medical images and their corresponding relevant concepts <ref type="bibr" coords="3,460.65,666.49,9.96,8.74" target="#b7">[8]</ref>. and MS-COCO <ref type="bibr" coords="4,207.84,118.99,14.61,8.74" target="#b17">[18]</ref>, and the fast development of deep Convolutional Neural Networks <ref type="bibr" coords="4,179.64,130.95,15.50,8.74" target="#b24">[25,</ref><ref type="bibr" coords="4,195.14,130.95,11.62,8.74" target="#b13">14]</ref>. Due to their great success on binary and multi-class image classification, research has been towards extending deep convolutional networks for multi-label image classification. Multi-label image classification is a fundamental and practical task in Computer Vision where the aim is to identify the set of objects present in an image.</p><p>A simple approach for multi-label image classification is to train independent binary classifiers for each label or class. However, this method does not consider the relationship among labels, and the number of predicted label combinations will grow exponentially as the number of categories increase. For instance, if a dataset contains 20 labels, then the number of predicted label combination could be more than 1 million (i.e., 2 20 ). Besides, this baseline method ignores the topology structure among labels, which can be an important regulariser for the co-occurrence patterns of objects. For example, the combination of sand, trees, sky, boats, and clouds is plausible to appear in the physical world, but some combinations of labels are almost impossible such as glacier, rain forest, and sun. There is a possibility that artificial or partly artificial images can violate such natural dependencies.</p><p>In order to regularise the prediction space, many researchers have attempted to capture label dependencies. Gong et al. <ref type="bibr" coords="4,314.74,353.54,15.50,8.74" target="#b11">[12]</ref> proposed three multi-label ranking losses to adapt convolutional neural networks for the multi-label problem. These losses were namely, softmax, pairwise ranking, and weighted approximate ranking (WARP). They found that the WARP loss function performs significantly better than the other two loss functions. Wang et al. <ref type="bibr" coords="4,340.93,401.36,15.50,8.74" target="#b27">[28]</ref> proposed a joint framework combining a convolutional neural network and a recurrent neural network in order to learn the semantic dependencies among labels. Zhu et al. <ref type="bibr" coords="4,422.87,425.27,15.50,8.74" target="#b32">[33]</ref> proposed a unified framework that captures both semantic and spatial relations of labels using a Spatial Regularisation Network (SRN). The network learns an attention map for each label, which associates relevant image regions to each label. By learning convolutions on the attention maps of all labels, the SRN captures the underlying semantic and spatial relations between labels and acts as a spatial regularisation for multi-label output. In order to use object detection methods to provide region proposals, Wei et al. <ref type="bibr" coords="4,292.56,508.96,15.50,8.74" target="#b29">[30]</ref> proposed the Hypothesis-CNN-Pooling (HCP) network, it first finds region proposals using object detection techniques such as Edge Boxes <ref type="bibr" coords="4,219.95,532.87,15.50,8.74" target="#b33">[34]</ref> to produce a set of candidates. These selected hypothesis are fed to a shared CNN to compute confidence vectors. The confidence vectors are combined through a fusion layer with max-pooling to generate the final multilabel predictions. Wang et al. <ref type="bibr" coords="4,273.09,568.73,15.50,8.74" target="#b28">[29]</ref> proposed a recurrent memorised-attention module that combines a spatial transformer layer and an LSTM to capture global contextual dependencies among labels and to avoid the additional computational cost of predicting region proposals.</p><p>Recently, Durand et al. <ref type="bibr" coords="4,253.13,620.25,15.50,8.74" target="#b10">[11]</ref> proposed a partial binary cross-entropy (partial-BCE) loss function and used curriculum learning to train a multi-label image classification model with partial labels, which reduces the cost of annotating all labels in each image. To improve the performance by capturing and exploring la-bel dependencies, Chen et al <ref type="bibr" coords="5,260.06,118.99,10.52,8.74" target="#b5">[6]</ref> proposed a Graph Convolutional Network which learned to map the label graph into a set of inter-dependent object classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Concept Detection in Medical Images</head><p>The goal of concept detection is to find relevant clinical concepts in medical images. Automatic identification of relevant medical concepts in medical images is vital for indexing and retrieval, report generation, and clinical decision support systems <ref type="bibr" coords="5,171.74,213.78,14.61,8.74" target="#b25">[26]</ref>. Concept detection can be solved as a classification problem where a mapping function is learned between low-level visual features and high level semantic concepts based on the annotated training data.</p><p>Dimitris and Ergina <ref type="bibr" coords="5,238.34,249.64,15.50,8.74" target="#b9">[10]</ref> proposed the use of the ResNet50 <ref type="bibr" coords="5,401.98,249.64,15.50,8.74" target="#b13">[14]</ref> model for predicting biomedical concepts for the ImageCLEF 2017 caption prediction task. Abacha et al. <ref type="bibr" coords="5,198.30,273.55,10.52,8.74" target="#b0">[1]</ref> used CNN and Binary Relevance <ref type="bibr" coords="5,363.18,273.55,15.50,8.74" target="#b30">[31]</ref> Decision Tree for concept detection. Since the distribution of concepts is uneven with large number of concepts present in only a few images, they build two different training subsets targeting the most frequent concepts having frequency greater than 400 and 1500, respectively. The Binary Relevance approach has limitations in terms of computational cost since a different classifier is trained for each concept present in the dataset. Hasan et al. <ref type="bibr" coords="5,261.06,345.28,15.50,8.74" target="#b12">[13]</ref> proposed an attention based encoder-decoder framework for concept detection for ImageCLEF 2017 caption prediction. The encoder is a VGG-19 <ref type="bibr" coords="5,228.31,369.19,15.50,8.74" target="#b24">[25]</ref> model and the decoder is a Long-Short Term Memory (LSTM) <ref type="bibr" coords="5,174.18,381.15,15.50,8.74" target="#b14">[15]</ref> network with a soft attention mechanism. The dependencies have been captured by hidden states of the LSTM. This approach treated concept detection as a sequence generation task which lacks in identifying the dependency of different concepts. Because concepts are not inherently ordered into a sequence, capturing dependencies by the hidden states presents a problem.</p><p>Pinho and Costa <ref type="bibr" coords="5,228.43,440.92,15.50,8.74" target="#b22">[23]</ref> proposed an adversarial network for feature learning and training a multi-label classifier using the extracted features to predict medical concepts. They showed that the use of deep learning methods outperformed more traditional representations. Valanavis and Kalamboukis <ref type="bibr" coords="5,412.07,476.79,15.50,8.74" target="#b26">[27]</ref> proposed a k-Nearest Neighbour (kNN) based approach for concept detection. Images are represented using two models namely, Bag of Visual Words (BoVW) and generalised Bag of Colours (QBoC). Using the extracted image visual representation, for each test image, training images are sorted based on their similarity score and the concepts of the top matched image are assigned to the test image. In an another approach Zhang et al. <ref type="bibr" coords="5,267.81,548.52,15.50,8.74" target="#b31">[32]</ref> proposed retrieval and topic-modelling based methods for concept detection in the ImageCLEF 2018 challenge. They used Lucene Image Retrieval (LIRE) <ref type="bibr" coords="5,276.70,572.43,15.50,8.74" target="#b19">[20]</ref> for retrieving the most similar images and their corresponding clinical concepts from the training set to assign concepts to the test images. Also, Latent Dirichlet Allocation (LDA) <ref type="bibr" coords="5,382.34,596.34,10.52,8.74" target="#b3">[4]</ref> was used to analyse the topic distribution of clinical concepts present in the retrieved similar images from the training set. Although, the above approaches were simple, they suffer from computational complexity and lack novelty in identifying concepts in unseen images. Singh et al. <ref type="bibr" coords="5,246.73,644.16,15.50,8.74" target="#b25">[26]</ref> also did similar study in classifying the modality of images and finding relevant medical concepts on a publicly available dataset, and found that convolutional neural networks are better for feature extraction when compared to the traditional approaches. Motivated by the success of Convolutional Neural Networks (CNNs) for various computer vision task, we use a CNN model for finding relevant medical concepts present in an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Convolutional Neural Network</head><p>With the rapid collection of large-scale datasets and rapid development of high performance computing devices, Convolutional Neural Networks (CNNs) are increasingly drawing attention from both research and industry <ref type="bibr" coords="6,415.30,238.37,15.90,8.74" target="#b24">[25,</ref><ref type="bibr" coords="6,431.20,238.37,11.93,8.74" target="#b13">14,</ref><ref type="bibr" coords="6,443.12,238.37,11.93,8.74" target="#b27">28]</ref>. The common building blocks of Convolutional Neural Networks are Convolutional layer, activation layer, pooling layer, flattening layer, and fully-connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Layer</head><p>This is the main building block of Convolutional Neural Networks. The main role of the convolutional layer is to detect features by applying an affine filter (or kernel) over the image pixels. The early convolutional layers in a CNN extract low-level features whereas the later convolutional layers are responsible for extracting higher level semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activation layer</head><p>The goal of an activation layer is to pass the output of the convolutional layer through an activation function. This layer is also called a non-linearity layer because we pass the output through some non-linear function such as sigmoid, tanh, or ReLU to get feature maps. The activation layer does not change the dimensions of the feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling layer</head><p>The main functionality of a pooling layer is to reduce the spatial dimensions of the feature maps and provide some spatial invariance to distortions and translations. Apart from this, pooling layers are also responsible for reducing the number of parameters and computation in the network. Various pooling operations include: max pooling, average pooling, or L2-norm pooling. Pooling helps reduce overfitting, which would occur if the CNN is given too much information, especially if that information is not relevant to classify an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flattening layer</head><p>The goal of a flattening layer is to transform the entire pooled feature map matrix into a single column which is then fed to the neural network for processing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully-connected layer</head><p>After flattening, output of the network is fed through fully connected layers similar to an ordinary neural network. With the fully connected layers, we combine the extracted features together to create a model which performs high-level reasoning. After the final layer, we apply an activation function such as softmax or sigmoid to produce the classifier output.</p><p>4 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Notation</head><p>Concept detection in medical images can be formulated as a multi-label image classification problem where each class corresponds to a concept label. The multilabel classification aims at associating a given instance x i ∈ X with a set of labels Y i = y i1 , y i2 , . . . , y iN . For medical concept detection, x i is a given medical image, Y i refers to a set of clinical concepts relevant to the medical image, and N refers to number of concepts relevant to that particular image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset</head><p>The dataset provided in the ImageCLEFmed Caption task is collected from the PubMed<ref type="foot" coords="7,175.03,537.34,3.97,6.12" target="#foot_1">5</ref> Open Access subset containing 1, 828, 575 archives, having a total of 6, 031, 814 image-caption pairs. Automatic filtering using deep learning and manual revisions have been applied to focus on radiology images and non-compound figures, giving a reduced dataset of 70, 786 radiology images of various medical imaging modalities. The official split of data in the form of training, validation, and test is provided by the challenge organisers. Table <ref type="table" coords="7,376.34,598.69,4.98,8.74" target="#tab_0">1</ref> shows the statistics of the datasets. The ground-truth concepts are provided for the training and validation set, whereas only images are provided for the test set in order to provide a fair evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data Exploration</head><p>The dataset in the ImageCLEFmed caption task has huge diversity. Figure <ref type="figure" coords="8,475.61,523.77,4.98,8.74" target="#fig_0">2</ref> shows sample data highlighting various modalities such as X-ray, MRI, ultrasound, and PET, and different anatomies such as hands, feet, brain, chest, and teeth. Apart from this, the images differ in terms of contrast, pixel dimensions, and resolution. A data analysis shows that there are in total 5216 unique clinical concepts present in the training set. The validation set has a total of 3233 unique clinical concepts present. We found that there are 312 concepts that are present in the validation set but not present in the training set. So, to train our model on all the concepts, we combine the data of the training and validation sets, having a total of 5528 unique clinical concepts present in the dataset. Figure <ref type="figure" coords="8,475.61,644.16,4.98,8.74" target="#fig_1">3</ref> shows the distribution of concepts present in the entire dataset. There are 2, 655  clinical concepts that occur in less than four images in the dataset. Out of 5528 concepts, 5441 concepts occur less than or equal to 1000 times in the dataset whereas only 87 concepts are present in more than 1000 images. Given that a deep learning model needs at least 1000 samples per class to perform adequately, the distribution of concepts shows the difficulty in training such a model on rare concepts present in the dataset. Top 20 clinical concepts present in the dataset in terms of their occurrence is show in Table <ref type="table" coords="9,207.55,504.31,3.87,8.74">2</ref>. We can clearly see that the top 10 concepts refer to the type of imaging study undertaken. Table <ref type="table" coords="9,296.78,516.27,4.98,8.74">3</ref> shows example of clinical concepts that are found in the dataset but are not visually represented in the images, making it challenging for the model to learn to predict these concepts.</p><formula xml:id="formula_0" coords="9,178.96,225.84,244.58,110.65">1 - 3 4 - 1 0 1 1 - 3 0 3 1 - 1 0 0 1 0 1 - 3 0 0 3 0 1 - 1 0 0 0 1 0 0 1 - 3 0 0 0 3 0 0 1 - 1 0 0 0 0 0 500 1,000</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Metrics</head><p>The challenge organisers provide code for evaluating the performance of the model in terms of F1 scores, which is the official evaluation metric to rate submissions from different teams. The F1 score is the weighted average of the precision and recall, where an F1 score of 0 indicates the worst score and 1 indicates the best score. As the task is multi-label classification, the final F1 score is the average of the F1 scores of each class with binary weighting method.</p><p>Table <ref type="table" coords="10,201.19,127.36,3.87,8.74">2</ref>: Top 20 concepts with their count in the training set. Table <ref type="table" coords="10,162.55,277.62,3.87,8.74">3</ref>: Some of the CUI clinical concepts present in the dataset that are not represented in the medical images, making it difficult for the model to predict directly from the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experimental settings</head><p>We build our Convolutional Neural Network for multi-label image classification model in Python using Keras <ref type="bibr" coords="10,265.70,476.79,10.52,8.74" target="#b6">[7]</ref> with a Tensorflow backend <ref type="bibr" coords="10,398.93,476.79,9.96,8.74" target="#b1">[2]</ref>. Figure <ref type="figure" coords="10,446.98,476.79,4.98,8.74" target="#fig_2">4</ref> shows the architecture of the CNN used in this study. The input to the network is given as a 400 × 400 × 3 representing the Red, Green, and Blue (RGB) values of the input image. The input unsigned byte pixels are normalised by dividing them by 255. The first convolutional layer uses a local receptive field (or kernel) of size 5 × 5 with a stride of 1 pixel to extract 16 feature maps, followed by a max-pooling operation conducted over 2 × 2 regions. The second, third, and fourth convolutional layers produce 32, 64, and 128 feature maps respectively. All convolutional layers use Rectified Linear Units (ReLUs) as the activation function. After each convolutional layer, max-pooling with size of 2 × 2 and dropout of 0.25 is applied to avoid overfitting of the model. After four blocks of Convolution, max-pooling, and dropout, we flatten the activation map, and apply the fully connected layers. The final fully connected layer consists of 5528 neurons corresponding to the total number of concepts in our dataset. We use the sigmoid activation function instead of softmax at the output layer of the network to get the probability of each class c j as Bernoulli distribution. The motivation is to get the probability of each concept independent of the other concept probabilities so that by using a threshold θ we can predict whether a particular clinical concept is present in a medical image or not.</p><p>The network was trained with the stochastic gradient descent (SGD) algorithm, namely Adam <ref type="bibr" coords="11,242.42,477.26,15.50,8.74" target="#b16">[17]</ref> with a binary-crossentropy loss function. We use binary-crossentropy loss instead of categorical-crossentropy to penalise each output node independently. Deep neural networks are highly sensitive to hyperparameters, so we tune our model hyper-parameters by selecting a range of value for each parameter and tuning in a coarse to fine search. The batch size (BS) is set to 32 and the initial learning rate (η) is set to 0.0001 with a decay of 1 × e -6 . The model is trained for 10 epochs and the best model based on the accuracy score is saved as the final model. In order to predict concepts on the test data, we set a threshold (θ) of 0.1 based on the performance of the model on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results and Discussion</head><p>The proposed method convolutional neural network is trained in an end-toend manner to predict relevant medical concepts on the test set images. Although, three different runs are evaluated internally, only the best run is sub-Table <ref type="table" coords="12,161.93,127.36,3.87,8.74">4</ref>: Performance of our proposed method compared to other teams at 2019 ImageCLEFmed Caption task. The results of the best run by each team is selected for comparison as provided by the organisers on the challenge web page. Source: https://www.imageclef.org/2019/medical/caption/. mitted to the evaluation server for the challenge. Table <ref type="table" coords="12,374.19,351.77,4.98,8.74">4</ref> shows the performance of our proposed approach under the name MacUni-CSIRO with the run name run1FinalOutput.txt having F1 score of 0.1435435. We performed an error analysis on the validation set to figure out the reasons for the low performance of the model. As highlighted in Figure <ref type="figure" coords="12,298.98,399.59,4.98,8.74" target="#fig_1">3</ref> that majority of concepts are rare and are not present in at least 1000 instances (or data points) which makes the task quite challenging. When comparing the results of the multi-label classification model on generic datasets and the ImageCLEFmed caption dataset, we found that the low performance is also attributable to the large number of medical concepts (5528 in the ImageCLEFmed caption task) and the difficulty of obtaining a bounding box annotation for each medical concept present in the medical image.</p><p>Although the ImageCLEFmed caption 2019 dataset is of a smaller size and is focused on radiology images only (compared to the previous version of the challenge), there is still a huge diversity in images in terms of modality, anatomy, and contrast. Further, during data exploration we found that there are many concepts that do not correspond to any visual data present in the medical images, making the task more difficult. Finally, we feel the need to have a more robust evaluation metric so that partial correct concepts predicted by the model can be considered since current evaluation metric don't take into account of the partial correct concepts predicted by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper presents our experiments for detecting concepts in medical images submitted for the 2019 ImageCLEFmed caption task. The proposed convolutional neural network as a multi-label classifier achieved an F1 score of 0.1435435.</p><p>No external resources are used in our experiments. The best model achieved an F1 score of 0.2823094 which is still far from the required performance for these systems to be deployed in a real-world setting. In future, we aim to incorporate domain knowledge so that the performance of these systems can further be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Animal and Human Research Ethics</head><p>The de-identified dataset in the form of medical images and their relevant medical concepts is provided by the challenge organisers <ref type="bibr" coords="13,365.70,234.16,14.61,8.74" target="#b15">[16]</ref>. The dataset provided is also a subset of the Radiology Objects in COntext (ROCO) dataset <ref type="bibr" coords="13,442.03,246.11,14.60,8.74" target="#b21">[22]</ref>. The detailed description about how the original dataset is given in <ref type="bibr" coords="13,408.40,258.07,14.61,8.74" target="#b20">[21]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,134.77,449.77,345.82,8.74;8,134.77,461.73,345.67,8.74;8,273.43,363.37,62.36,56.69"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Diversity in terms of different modalities and anatomy present in the ImageCLEFmed Caption dataset. * in the image names denotes ROCO CLEF.</figDesc><graphic coords="8,273.43,363.37,62.36,56.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,136.11,382.50,343.13,8.74"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Number of concepts versus frequency of their occurrence in the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,134.77,382.49,345.83,8.74;11,134.77,394.44,57.67,8.74;11,134.77,115.83,368.50,255.13"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Schematic of the proposed Convolutional Neural Network for multi-label classification.</figDesc><graphic coords="11,134.77,115.83,368.50,255.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,193.09,127.36,229.16,77.23"><head>Table 1 :</head><label>1</label><figDesc>Statistics of ImageCLEFmed Caption Task.</figDesc><table coords="7,243.69,142.86,124.90,61.73"><row><cell cols="2">Data Set No. of images</cell></row><row><cell>Training set</cell><cell>56629</cell></row><row><cell>Validation set</cell><cell>14157</cell></row><row><cell>Test set</cell><cell>10000</cell></row><row><cell>Total</cell><cell>80786</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,657.44,122.39,7.47"><p>https://www.imageclef.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="7,144.73,657.44,164.76,7.47"><p>https://www.ncbi.nlm.nih.gov/pubmed</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work is supported by an international <rs type="funder">Macquarie University</rs> <rs type="grantName">Research Excellence Scholarship</rs> and the <rs type="grantNumber">DATA61</rs> <rs type="funder">CSIRO</rs> top-up scholarship. This research is undertaken with the assistance of resources and services form the <rs type="funder">National Computational Infrastructure (NCI)</rs>, supported by the <rs type="funder">Australian Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_epqp5sH">
					<idno type="grant-number">DATA61</idno>
					<orgName type="grant-name">Research Excellence Scholarship</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Conflicting Interest</head><p>The Authors declare that there is no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.96,459.74,337.64,7.86;13,151.52,470.70,329.07,7.86;13,151.52,481.66,329.07,7.86;13,151.52,492.61,46.58,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,463.44,459.74,17.15,7.86;13,151.52,470.70,123.20,7.86">Nlm at imageclef 2017 caption task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S D</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="13,296.76,470.70,183.83,7.86;13,151.52,481.66,98.56,7.86">CLEF2017 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,503.52,337.63,7.86;13,151.52,514.48,329.07,7.86;13,151.52,525.44,329.07,7.86;13,151.52,536.40,329.07,7.86;13,151.52,547.36,329.07,7.86;13,151.52,558.32,274.35,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,231.25,536.40,230.43,7.86">TensorFlow: A System for Large-scale Machine Learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,151.52,547.36,329.07,7.86;13,151.52,558.32,61.64,7.86;13,274.40,558.32,32.36,7.86">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation<address><addrLine>Savannah, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
	<note>OSDI&apos;16</note>
</biblStruct>

<biblStruct coords="13,142.96,569.23,337.64,7.86;13,151.52,580.19,329.07,7.86;13,151.52,591.14,329.07,7.86;13,151.52,602.10,217.02,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,166.84,580.19,313.75,7.86;13,151.52,591.14,34.11,7.86">VQA-Med: Overview of the medical visual question answering task at imageclef 2019</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,208.32,591.14,272.27,7.86;13,151.52,602.10,27.88,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,613.01,337.63,7.86;13,151.52,623.95,173.60,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,294.40,613.01,102.82,7.86">Latent dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,403.68,613.01,76.91,7.86;13,151.52,623.97,74.34,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-03">Mar 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,634.88,337.64,7.86;13,151.52,645.81,329.07,7.89;13,151.52,656.80,43.13,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,225.65,634.88,254.94,7.86;13,151.52,645.84,93.03,7.86">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,253.15,645.84,91.01,7.86">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004-01">Jan 2004</date>
		</imprint>
	</monogr>
	<note>Database issue</note>
</biblStruct>

<biblStruct coords="14,142.96,119.67,337.64,7.86;14,151.52,130.61,230.02,7.89" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="14,305.97,119.67,174.63,7.86;14,151.52,130.63,94.69,7.86">Multi-Label Image Recognition with Graph Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<idno>CoRR abs/1904.03582</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,140.76,216.43,8.12" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<title level="m" coord="14,226.66,140.76,21.39,7.86">Keras</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,150.89,337.63,7.86;14,151.52,161.84,329.07,7.86;14,151.52,172.80,329.07,7.86;14,151.52,183.74,186.90,7.89" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,348.23,161.84,132.36,7.86;14,151.52,172.80,188.50,7.86">Preparing a collection of radiology examinations for distribution and retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Rosenman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Shooshan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,346.84,172.80,133.76,7.86;14,151.52,183.76,94.89,7.86">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="304" to="310" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,193.89,337.63,7.86;14,151.52,204.85,329.07,7.86;14,151.52,215.81,329.07,7.86;14,151.52,226.77,329.07,7.86;14,151.52,237.72,111.64,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,151.52,204.85,329.07,7.86;14,151.52,215.81,166.17,7.86">Overview of ImageCLEFtuberculosis 2019 -automatic ct-based report generation and tuberculosis severity assessment</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="14,340.28,215.81,105.76,7.86">CLEF2019 Working Notes</title>
		<title level="s" coord="14,453.98,215.81,26.62,7.86;14,151.52,226.77,143.03,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,247.85,337.97,7.86;14,151.52,258.81,329.07,7.86;14,151.52,269.77,317.71,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,252.41,247.85,228.18,7.86;14,151.52,258.81,66.42,7.86">Concept detection on medical images using deep residual learning network</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Dimitris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ergina</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="14,241.05,258.81,239.55,7.86;14,151.52,269.77,43.39,7.86">CLEF2017 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,279.90,337.98,7.86;14,151.52,290.83,267.49,7.89" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="14,306.41,279.90,174.19,7.86;14,151.52,290.85,132.53,7.86">Learning a Deep ConvNet for Multi-label Classification with Partial Labels</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mehrasa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<idno>CoRR abs/1902.09720</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,300.98,337.98,7.86;14,151.52,311.94,329.07,7.86;14,151.52,322.90,56.36,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,356.22,300.98,124.37,7.86;14,151.52,311.94,112.54,7.86">Deep convolutional ranking for multilabel image annotation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,285.90,311.94,194.69,7.86;14,151.52,322.90,27.70,7.86">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,333.02,337.98,7.86;14,151.52,343.98,329.07,7.86;14,151.52,354.94,329.07,7.86;14,151.52,365.90,329.07,7.86;14,151.52,376.86,22.02,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,315.73,343.98,164.86,7.86;14,151.52,354.94,109.22,7.86">Prna at imageclef 2017 caption prediction and concept detection tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sreenivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Swisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="14,280.71,354.94,103.50,7.86">CLEF2017 Working Notes</title>
		<title level="s" coord="14,391.01,354.94,89.58,7.86;14,151.52,365.90,81.94,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,386.99,337.98,7.86;14,151.52,397.95,329.07,7.86;14,151.52,408.90,25.60,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,299.05,386.99,177.61,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,166.05,397.95,258.26,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,419.03,337.97,7.86;14,151.52,429.96,112.05,7.89" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,284.81,419.03,103.97,7.86">Long Short-Term Memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,397.12,419.03,83.47,7.86">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">Nov 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,440.12,337.98,7.86;14,151.52,451.08,329.07,7.86;14,151.52,462.03,329.07,7.86;14,151.52,472.99,329.07,7.86;14,151.52,483.95,329.07,7.86;14,151.52,494.91,329.07,7.86;14,151.52,505.87,329.07,7.86;14,151.52,516.83,329.07,7.86;14,151.52,527.79,329.07,7.86;14,151.52,538.75,216.27,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,268.29,494.91,212.30,7.86;14,151.52,505.87,124.23,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,297.67,505.87,182.92,7.86;14,151.52,516.83,329.07,7.86;14,151.52,527.79,122.46,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="14,305.55,527.79,171.07,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="14,142.62,548.87,337.98,7.86;14,151.52,559.83,329.07,7.86;14,151.52,570.79,53.30,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,247.55,548.87,186.40,7.86">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,458.05,548.87,22.55,7.86;14,151.52,559.83,199.81,7.86">International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, California, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,580.92,337.97,7.86;14,151.52,591.88,329.07,7.86;14,151.52,602.84,329.07,7.86;14,151.52,613.79,163.57,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,206.94,591.88,185.21,7.86">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,298.43,602.84,124.82,7.86">Computer Vision -ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,623.92,337.97,7.86;14,151.52,634.88,329.07,7.86;14,151.52,645.84,329.07,7.86;14,151.52,656.80,25.60,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,377.60,623.92,102.99,7.86;14,151.52,634.88,261.51,7.86">Multi-label image classification via knowledge distillation from weakly-supervised detection</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,432.52,634.88,48.07,7.86;14,151.52,645.84,234.63,7.86">Proceedings of the 26th ACM International Conference on Multimedia</title>
		<meeting>the 26th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,119.67,337.97,7.86;15,151.52,130.63,64.51,7.86" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="15,243.19,119.67,198.82,7.86">Visual Information Retrieval using Java and LIRE</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Marques</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Morgan Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,140.96,337.97,7.86;15,151.52,151.92,329.07,7.86;15,151.52,162.87,329.07,7.86;15,151.52,173.83,46.58,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="15,431.64,140.96,48.95,7.86;15,151.52,151.92,199.23,7.86">Overview of the ImageCLEFmed 2019 concept prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,371.81,151.92,108.79,7.86;15,151.52,162.87,186.41,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,184.16,337.98,7.86;15,151.52,195.12,329.07,7.86;15,151.52,206.07,329.07,7.86;15,151.52,217.03,329.07,7.86;15,151.52,227.99,329.07,7.86;15,151.52,238.95,329.07,7.86;15,151.52,249.91,260.43,7.86" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="15,405.76,184.16,74.83,7.86;15,151.52,195.12,215.49,7.86;15,317.14,227.99,163.45,7.86;15,151.52,238.95,329.07,7.86;15,151.52,249.91,36.03,7.86">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Balocco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zahnd</surname></persName>
		</author>
		<editor>., Demirci, S., Albarqouni, S., Lee, S.L., Moriconi, S., Cheplygina, V., Mateus, D., Trucco, E., Granger, E., Jannin, P.</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Radiology Objects in COntext (ROCO): A Multimodal Image Dataset</note>
</biblStruct>

<biblStruct coords="15,142.62,260.23,337.97,7.86;15,151.52,271.19,329.07,7.86;15,151.52,282.15,329.07,7.86;15,151.52,293.11,259.59,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="15,249.14,260.23,231.45,7.86;15,151.52,271.19,303.90,7.86">Feature Learning with Adversarial Networks for Concept Detection in Medical Images: UA.PT Bioinformatics at ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pinho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Costa</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="15,168.00,282.15,109.31,7.86">CLEF2018 Working Notes</title>
		<title level="s" coord="15,287.01,282.15,177.32,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-10">2018. September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,303.43,337.97,7.86;15,151.52,314.39,329.07,7.86;15,151.52,325.35,329.07,7.86;15,151.52,336.29,112.56,7.89" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="15,415.77,314.39,64.82,7.86;15,151.52,325.35,145.50,7.86">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,305.92,325.35,174.68,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015-12">Dec 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,346.64,337.98,7.86;15,151.52,357.59,105.60,7.86" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="15,274.05,346.64,206.54,7.86;15,151.52,357.59,73.50,7.86">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,367.92,337.97,7.86;15,151.52,378.88,329.07,7.86;15,151.52,389.84,298.56,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="15,350.02,367.92,130.57,7.86;15,151.52,378.88,253.73,7.86">Modality classification and concept detection in medical images using deep transfer learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ho-Shon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hamey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,428.34,378.88,52.25,7.86;15,151.52,389.84,232.84,7.86">International Conference on Image and Vision Computing New Zealand</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,400.16,337.98,7.86;15,151.52,411.12,329.07,7.86;15,151.52,422.08,322.32,7.86" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="15,282.86,400.16,197.74,7.86;15,151.52,411.12,77.88,7.86">IPL at ImageCLEF 2018: A kNN-based Concept Detection Approach</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Valavanis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kalamboukis</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="15,249.44,411.12,231.16,7.86;15,151.52,422.08,43.39,7.86">CLEF2018 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,432.40,337.98,7.86;15,151.52,443.36,329.07,7.86;15,151.52,454.32,219.45,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="15,392.84,432.40,87.75,7.86;15,151.52,443.36,186.07,7.86">CNN-RNN: A Unified Framework for Multi-label Image Classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,356.97,443.36,123.62,7.86;15,151.52,454.32,126.18,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,464.64,337.98,7.86;15,151.52,475.60,329.07,7.86;15,151.52,486.56,164.16,7.86" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="15,344.72,464.64,135.87,7.86;15,151.52,475.60,165.89,7.86">Multi-label image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,357.42,475.60,123.17,7.86;15,151.52,486.56,80.30,7.86">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,496.88,337.98,7.86;15,151.52,507.84,329.07,7.86;15,151.52,518.78,289.64,7.89" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="15,451.86,496.88,28.74,7.86;15,151.52,507.84,232.70,7.86">Hcp: A flexible cnn framework for multi-label image classification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,391.07,507.84,89.52,7.86;15,151.52,518.80,171.00,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1901" to="1907" />
			<date type="published" when="2016-09">Sep 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,529.13,337.98,7.86;15,151.52,540.06,272.18,7.89" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="15,238.83,529.13,174.73,7.86">A review on multi-label learning algorithms</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,420.74,529.13,59.85,7.86;15,151.52,540.09,170.82,7.86">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,550.41,337.98,7.86;15,151.52,561.37,329.07,7.86;15,151.52,572.33,329.07,7.86;15,151.52,583.29,95.77,7.86" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="15,296.38,550.41,184.22,7.86;15,151.52,561.37,161.28,7.86">ImageSem at ImageCLEF 2018 Caption TaskL Image Retrieval and Transfer Learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="15,337.38,561.37,143.22,7.86;15,151.52,572.33,143.85,7.86">CLEF2018 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,593.61,337.98,7.86;15,151.52,604.57,329.07,7.86;15,151.52,615.53,275.79,7.86" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="15,338.36,593.61,142.23,7.86;15,151.52,604.57,238.00,7.86">Learning spatial regularization with image-level supervisions for multi-label image classification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,410.79,604.57,69.80,7.86;15,151.52,615.53,182.52,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2027" to="2036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,625.85,337.97,7.86;15,151.52,636.81,329.07,7.86;15,151.52,647.77,241.38,7.86" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="15,256.31,625.85,205.30,7.86">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,374.15,636.81,106.44,7.86;15,151.52,647.77,16.79,7.86">Computer Vision -ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
