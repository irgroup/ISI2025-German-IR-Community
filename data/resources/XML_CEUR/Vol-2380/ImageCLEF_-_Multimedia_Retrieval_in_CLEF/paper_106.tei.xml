<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.10,115.96,313.16,12.62;1,156.41,133.89,302.54,12.62;1,259.84,151.82,95.67,12.62">A two-staged Approach for Localization and Classification of Coral Reef Structures and Compositions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.48,189.49,72.79,8.74"><forename type="first">Kirill</forename><surname>Bogomasov</surname></persName>
							<email>bogomasov@hhu.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Heinrich Heine University</orgName>
								<address>
									<addrLine>Universitätsstraße 1</addrLine>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.79,189.49,60.46,8.74"><forename type="first">Philipp</forename><surname>Grawe</surname></persName>
							<email>grawe@hhu.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Heinrich Heine University</orgName>
								<address>
									<addrLine>Universitätsstraße 1</addrLine>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.03,189.49,62.85,8.74"><forename type="first">Stefan</forename><surname>Conrad</surname></persName>
							<email>stefan.conrad@hhu.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Heinrich Heine University</orgName>
								<address>
									<addrLine>Universitätsstraße 1</addrLine>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.10,115.96,313.16,12.62;1,156.41,133.89,302.54,12.62;1,259.84,151.82,95.67,12.62">A two-staged Approach for Localization and Classification of Coral Reef Structures and Compositions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2CF703A7CD743EFA497088D953E0D8B1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Segmentation</term>
					<term>Image Classification</term>
					<term>Object Localization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present the approaches that achieved the first place in this years ImageCLEFcoral challenge. The task of the challenge was the localization and classification of corals within images of sea ground. Therefore we had to extract bounding boxes for each coral and labeling them with the specific type of substrate. We applied a state-of-the-art deep learning approach (YOLO) and also developed a two-staged approach, using a grid along with two classifiers. One that classifies the tiles of the grid, the other that classifies the found boxes. We had moderate results using YOLO and discovered that locating the corals is the most challenging part. Furthermore class imbalance and intersecting boxes, made the problem even harder.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Climate change is one of the major problems of the 21st century. Its impact is growing every year and therefore researched a lot. Since corals are a significant part of the maritime environment they are affected by the climate change in many ways <ref type="bibr" coords="1,198.65,520.00,9.96,8.74" target="#b5">[6]</ref>. Corals have their own self-contained and over many decades developed ecosystem, which is why the influence of damage to coral reefs can have serious consequences for every maritime organism. Every year the danger of complete destruction of coral reefs becomes more realistic. To sophisticatedly plan protective procedures, coverage of current stocks are required. For this purpose, images of the sea ground are currently viewed and annotated manually, which is nearly impossible for the whole considered surface area. This raises the question of whether an automatic localication and annotation of the coral is feasible. We will address this question in this paper. Therefore we use this year's ImageCLEFcoral dataset <ref type="bibr" coords="2,247.90,118.99,10.52,8.74" target="#b1">[2]</ref> as the base for our research and also participated in their challenge, which is part of the ImageCLEF 2019 <ref type="bibr" coords="2,390.91,130.95,9.96,8.74" target="#b7">[8]</ref>. The task can be divided into two logical subtasks, localization and classification of objects. This is a wide spread research field of computer science, with many fields of application. The automotive industry seems to be an obvious field of research <ref type="bibr" coords="2,470.08,166.81,10.52,8.74" target="#b2">[3]</ref> and is commercially relevant. Today, driver assistance systems are ubiquitous. Recognition of road signs is a part of it. At first images of the road are taken via vehicle camera while driving. Second road signs are searched and classified in these recordings. The greatest results in such application scenarios are achieved by artificial neural networks. YOLO <ref type="bibr" coords="2,293.61,226.59,15.50,8.74" target="#b10">[11]</ref> showed one of the best results. The application scenario can be transferred very well. The localization and labeling of corals is similar, because the images are taken automatically and contain corals in unknown areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>The training set, which we define as dataset (A), contains 240 images with 6670 annotated substrates. Generally there is a differentiation between 13 substrate types. Which are: "Hard Coral -Branching, Hard Coral -Submassive, Hard Coral -Boulder, Hard Coral -Encrusting, Hard Coral -Table, Hard Coral -Foliose, Hard Coral -Mushroom, Soft Coral, Soft Coral -Gorgonian, Sponge, Sponge -Barrel, Fire Coral -Millepora and Algae -Macro or Leaves" <ref type="bibr" coords="2,449.18,384.01,9.96,8.74" target="#b1">[2]</ref>. For the submitted runs a test set containing 200 raw images is used, which correct labels and boxes were not available at the time of the publication.   The substrate types have an unbalanced distribution, as shown in table <ref type="table" coords="2,451.01,644.16,3.87,8.74">1</ref>. Furthermore does the quality of the images vary, as well as the resolution. Some of the images contain a measurement white line, which is an obstacle while image processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Investigating the Dataset</head><p>When investigating the dataset, the problem of overlapping boxes appeared to us. Many of these bounding boxes fully contained or intersected with other boxes. To be more specific, only 2672 of 6670 bounding boxes do neither overlap or are contained in a bigger one. For this reason, we had started to investigate whether the substrates differ from each other at all, why we searched for meaningful features. These features were extracted from extracted bounding boxes. We applied a classical approach using SIFT <ref type="bibr" coords="3,295.42,251.65,9.96,8.74" target="#b8">[9]</ref>. Furthermore we calculated structure <ref type="bibr" coords="3,464.38,251.65,12.16,8.74" target="#b4">[5]</ref>, texture <ref type="bibr" coords="3,164.04,263.60,12.54,8.74" target="#b6">[7]</ref> and color histograms in another approach. We used the calculated features to train a k-Nearest Neighbors classifier. The considered neighborhood k was set to <ref type="bibr" coords="3,184.79,287.51,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="3,196.97,287.51,12.73,8.74">25]</ref> = {k ∈ N|3 ≤ k ≤ 25}. Setting k to a higher value would lead to a strong dominatation of the neighborhood by frequent classes. With a train-/validation split of 80 : 20 we got our best results on a combination of texture, structure and color features. The following values show that the rare substrates are basically not found. In this way, we did not succeed in improving these values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Augmentation</head><p>The amount of given data is remarkably low. Usually even a pre-trained neural network requires a larger data set, why we decided to use data augmentation.</p><p>To generate new data, we used the following methods: noise and blur <ref type="bibr" coords="3,436.14,420.17,11.63,8.74" target="#b0">[1]</ref>. Other augmentation methods did not seem practical, since it would change bounding boxes. Therefore, we generated a second dataset (B) and could triple the data set size. Within the new dataset, which consists of substrate bounding boxes, we kept the class distribution, due to the probability of finding a frequently represented substrate type is significantly higher than that of a rare one. Also because balancing the dataset would require to cut frequent substrate types, which did not seem appropriate regarding the low number of annotated corals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sharpening</head><p>The images vary in quality and many of them are out of focus or blurry. To counter this and to create an improved dataset (C), we increased the contrast of entire images and highlighted the details. For this purpose, each pixel value was replaced by the weighted average of its 3 × 3 neighborhood. The following matrix shows the filter:</p><formula xml:id="formula_0" coords="3,279.15,620.40,57.05,34.21">  -1 -1 -1 -1 12 -1 -1 -1 -1  </formula><p>The challenge of the annotation and localization task is to find corals within images of sea ground, define bounding boxes for each coral and label them with their specific type of substrate. We applied one state-of-the-art deep learning approach and additionally developed an own one. These two are presented in the following subsections, whereas the focus lies on explaining our own approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">YOLO</head><p>In contrast to comparable neural networks, like "fast R-CNN" <ref type="bibr" coords="4,409.74,259.04,9.96,8.74" target="#b3">[4]</ref>, which locate and classify objects multiple times for various regions of an image, the YOLO architecture passes the whole input image at once. That is achieved by dividing each image into square cells inside of which bounding boxes are predicted. In our work we scaled input images to a size of 608 x 608 pixels, because of the many corals contained in each image. This is the largest resolution we tested on our GPU and was the most promising. The classification process is basically a regression problem, which leads from image pixel values to bounding boxes with their class probabilities in one go. Part of the training is the optimization of predicted class probabilities, which defines the bounding boxes. In doing so, the calculation of each box considers features of the entire image. Therefore YOLO has the advantage of making less background errors as R-CNN, because more context information is taken into account. YOLO also outputs a confidence, which is calculated as the product of the precision of an object and its intersection over union (IoU). In a later step, this is multiplied by the conditional class probability of an object. Finally an output confidence is obtained, which describes how probable the particular class of the box is and how well the predicted bounding box fits this particular object.</p><p>Limitations However, there are some limitations. On the one hand each cell of the grid predicts only two boxes, which share the same class label. This is an algorithmic limitation on the number of objects with different labels, if the objects are close to each other. On the other hand the authors of YOLO mention that they treat errors in small bounding boxes the same way they treat large bounding box errors. Because of that, errors in small boxes have a larger impact on IoU, which leads to incorrect localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Own Developments</head><p>We developed a two-staged approach that first locates and then labels the substrates. Both of these steps make use of machine learning, to be more precise classification algorithms. This leaves room to improve the classification task, e.g. by evaluating different classification algorithms. One advantage of this two-staged approach is, that the two stages are independent from each other, which makes it possible to combine different algorithms and approaches together. The algorithm of our approach is shown in figure <ref type="figure" coords="5,472.84,465.58,3.87,8.74" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Locating Substrates</head><p>The main idea behind locating substrates is based on the assumption, that the coral images have coral and non-coral areas. Such non-coral areas should look relatively similar for all coral types. This is quite different for images showing objects like cars or birds. Following our assumption, we segmented an image in coral and non-coral areas. First the image is divided in a grid, small enough to predict all boxes. To classify these areas we used a grid and then extract features from the tiles of this grid. We used a square of a fixed size for the tiles, which is based on the size of the smallest boxes in the training set, i.e. the integer average of the smallest width and height. Based on the training set, we recommend to use a tile size of 12 × 12, so that the smallest box can be located completely without background. To ensure that all tiles of an image have the same size, i.e. image size is the whole multiple of the tile size, the image is scaled to fit.</p><p>Next we extracted features for every tile of each of the training set images.</p><p>For this purpose we used concatenated feature vectors consisting of features describing the color, texture and shape. For color, normalized histograms are used which describe the characteristics regarding the color <ref type="bibr" coords="6,401.69,154.86,14.61,8.74" target="#b9">[10]</ref>. The textural features of the tiles are modeled by Haralick texture features <ref type="bibr" coords="6,405.20,166.81,9.96,8.74" target="#b4">[5]</ref>, which applies co-occurrence matrices on the gray scale level. Lastly the shape is represented by Hu moments <ref type="bibr" coords="6,205.63,190.72,9.96,8.74" target="#b6">[7]</ref>. Hu moments are invariant to translation, rotation and scale.</p><p>All of these characteristics are useful for the domain of coral images. These features are used to train a binary classifier, which classifies whether a tile is a coral area or a non-coral area. An area of a training set image is considered a coral area, if more than 50% of its area intersects with a bounding box area of the ground truth. To classify areas of images that should be predicted, this image is also divided into tiles of the same previously defined size. Now the labels were obtained by feeding the features into the learned classifier. We decided to use K-Nearest-Neighbor with k = 15 to classify the tiles, because k = 15 performed best on our validation split.</p><p>After each tile of the grid was classified, we got an black and white image with 12 × 12 pixel large tiles. There are multiple strategies to extract boxes out of the resulting picture. We used a relatively naive approach with the application of connected-component labeling. Since we discovered a large amount of single, not connected tiles, we only kept components, that consisted of more than ten tiles. This counters a less beneficial performance of our classifier. Each unique component is now bordered with a bounding box, that borders the outside tiles of the component. Figure <ref type="figure" coords="6,308.16,393.96,4.98,8.74">2</ref> is showing the different stages of the location process (b -d), as well as the ground truth (a).</p><p>Labeling Found Boxes The found bounding boxes were classified on previously mentioned features 2.1 using a k-Nearest Neighbors Classifier. In addition to this already presented classification approaches, we studied whether the features can also be classified using a convolutional neural network. For the research, we subdivided the training data into a training and validation set in a ratio of 80:20 as previously. For comparability of the results we scaled the input data to the size of our grid. In consideration of the low amount of image data, we begun our work with a correspondingly small CNN, which we call baseline. The given CNN consists of one convolutional layer with maxpooling and rectified linear activation. We use dropout to prevent overfitting. The deactivation of neurons happens with a 20% probability. Subsequently, the data is handed to a flattening layer which serves as connection between convolutional and following dense layers. The result first enters a dense layer with RELU as the activation function and is then passed on to a density layer with softmax as activation function. This leads us to a confidence for each bounding box to belong to one of our 13 classes.</p><p>Considering that such a simple architecture may not be able to "remember" all relevant features of coral images, we extended our baseline architecture. Therefore we enlarged the existing architecture with two additional convolutional hid- Fig. <ref type="figure" coords="7,153.45,433.13,3.87,8.74">2</ref>: Visualized process of localization corals with our approach. The raw picture is taken from the ImageCLEFcoral dataset <ref type="bibr" coords="7,345.98,445.09,9.96,8.74" target="#b1">[2]</ref>.</p><p>den layers.</p><p>Finally we looked for an extra deep architecture for comparison. All networks were trained with a batch size of 100 and with up to 1000 epochs. We decided to use VGG19 <ref type="bibr" coords="7,200.38,517.00,15.50,8.74" target="#b11">[12]</ref> and trained it on our data via transfer learning, since it has been proven to be gold standard in recent years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation and Results</head><p>The following section discusses the submitted runs at ImageCLEF 2019. For a better understanding of the results of the approaches, we evaluated the localization and labeling separately. The results show that YOLO is considered state-of-the-art for a reason. Besides presenting our results of the submissions, we also discuss the limitations and potentials of our approach as well.</p><p>In table 2 we present the results of our submissions. Our own approach is marked with I, and YOLO based submissions with II. MAP 0.5 stands for the localised mean average precision for each submitted method with an IoU ≥ 0.5 of the ground truth and R 0.5 for the recall value, respectively MAP 0 represents the image annotation average without any localization. The results for I show, that CNNs and k-NN deliver comparable results. Sharpening has not led to better results, perhaps because it accentuates noise. YOLO combined with statistical probability distribution provides best results with an precision of 0.243 and a recall of 0.131.</p><p>Table <ref type="table" coords="8,161.55,214.64,4.98,8.74" target="#tab_3">2</ref> shows that we worked only on data set (A) and (C). We did not use data set (B) for our run submissions, since it did not lead to any kind of improvement. All approaches we used have some limitations and therefore leave space for improvement. Some of which we will describe in the following.</p><p>YOLO The weakness of YOLO is evident on rather smaller bounding boxes. Predictions on the validation data set showed that small coral substrates are either not found or subsequently labeled incorrectly. This results in the low recall value of 0.131. Corals that are found however, are mostly labeled as "c soft coral". Nevertheless, even on larger corals, YOLO shows rather moderate results. In a quarter of images it did not find boxes at all, that is why we used the found boxes from our other approach I to complete the results.</p><p>Our Approach Not only the performance (see Table <ref type="table" coords="8,384.53,596.34,4.43,8.74" target="#tab_3">2</ref>) shows flaws in our approach, but also some obvious conclusions do. Since we got an accuracy of 0.534 on labeling boxes, which was evaluated on a 80 : 20 split of the training set, we assume that our approach fails to locate corals correctly. We also tested using SIFT features which had an accuracy of 0.4744. One problem of the two-staged approach is the assumption, features of coral and non-coral tiles are distinct enough. This leaves room for further evaluation and research, regarding the choice of features and labels. It might be beneficial to use more than two labels, i.e. more than just coral and non-coral. This could e.g. be water in the background, because we discovered that water in the background is often "false positive" classified, i.e. as coral area. An additional label would also need an additional annotation. The question arises, whether 14 (13 substrate classes + background) labels could be used. This approach would only need one, instead of two classifiers. Regarding the tile classification, the usage of CNNs to classify the tiles sounds promising because of the high number of tiles. Another issue with our approach is the size of the tiles. Big tiles prevent small boxes from getting found and increase the chance of two corals in one tile. From a design perspective, boxes should be as small as possible to be as precise as possible. But if tiles are chosen relatively small, not only does the computational time extend, but features contain less information. This could lead e.g. to forms not getting recognized. We encountered the problem of an enormous computational time, because of that we increased the size to 24 × 24. Also we reduced the training set of tiles by 80%, which decreases the performance not significantly as seen in table <ref type="table" coords="9,263.59,334.19,3.87,8.74" target="#tab_4">3</ref>. An approach of using a sliding window should also be considered in future work. Lastly using connected components as the method to extract boxes from the tile images, could be not sophisticated enough. Firstly with a perfect labeling of coral and non-coral tiles, it would not be able to recognize inlying boxes. And secondly it only considers two labels as features. The use of density-based clustering, working on more than just the predicted labels could lead to better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Overall, our approaches show moderate results. The idea to use neural networks proved to be promising. However, afterwards we can assert that YOLO was not the best choice. It completely fails to find smaller bounding boxes. The concept of using feature engineering and searching for features or feature constellations, which are able to describe and represent different types of benthic substrate, still seems to be useful regarding the small amount of given data. But there is a lot of room for improvement.</p><p>Beside of that there are multiple images in the data set that show the same sea ground and contain for the most part the same corals. This kind of information can be used locally to improve the bounding boxes of corals, since their position can be tracked.</p><p>With regard to our approach 2b of labeling coral and non-coral areas, we can make the conclusion that the chosen features are not working properly. Probably we need a kind of back propagation to mark wrong labeled areas and process images multiple times. Additionally we could investigate the set of our features for a more performant subset using boosting. We would also stick to the deep learning approach and try another, maybe more time consuming but also more precise neural network, like an R-CNN.</p><p>Finally, the concept of combining deep learning and classic feature engineering is where we see the most potential.</p><p>Besides that, another point of potential improvement is the correction and balancing of the data set itself. Currently, seven of 13 coral type classes have a relative ratio of less than two percent, six out of them even less than one percent. The quality of the pictures is very variable too. Some of the images do not even seem to be completely annotated.</p><p>For future approaches, we would recommend publishing a larger and more balanced data set, in which each class has almost the same number of representatives.</p><p>To address the initial question whether an automatic localization and annotation of corals is feasible, we see good chances for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,391.36,345.83,8.77;5,134.77,403.35,267.39,8.74"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Flow diagram of our approach. Red clouds are input data and green ones output data. The blue boxes are steps described in text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,173.53,247.85,93.09,7.86;7,312.29,247.85,166.00,7.86;7,312.29,258.81,30.71,7.86;7,146.70,401.24,146.75,7.86;7,312.29,401.24,165.99,7.86;7,312.29,412.20,23.86,7.86"><head>( a )</head><label>a</label><figDesc>Grund truth boxes. (b) Inside (white) and outside tiles (black). (c) Refined inside and outside areas. (d) Bounding boxes of connected components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,143.25,452.06,24.08,8.74"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="2,174.52,452.03,297.59,163.09"><head>:</head><label></label><figDesc>Substrate types with their relative frequency in the training set.</figDesc><table coords="2,220.62,464.40,174.11,150.73"><row><cell>Class label</cell><cell>Relative frequency</cell></row><row><cell cols="2">c algae macro or leaves 0.0046</cell></row><row><cell>c fire coral millepora</cell><cell>0.0015</cell></row><row><cell>c hard coral boulder</cell><cell>0.1549</cell></row><row><cell cols="2">c hard coral branching 0.1280</cell></row><row><cell cols="2">c hard coral encrusting 0.0528</cell></row><row><cell>c hard coral foliose</cell><cell>0.0082</cell></row><row><cell cols="2">c hard coral mushroom 0.0258</cell></row><row><cell cols="2">c hard coral submassive 0.0031</cell></row><row><cell>c hard coral table</cell><cell>0.0009</cell></row><row><cell>c soft coral</cell><cell>0.5223</cell></row><row><cell cols="2">c soft coral gorgonian 0.0024</cell></row><row><cell>c sponge</cell><cell>0.0808</cell></row><row><cell>c sponge barrel</cell><cell>0.0145</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.77,270.06,345.83,155.52"><head>Table 2 :</head><label>2</label><figDesc>Results of our submitted runs at ImageCLEF 2019. Comparison of the results of the different approaches in our submissions. The methods used in I are our developed two-staged approach, whereas II are approaches using YOLO.</figDesc><table coords="8,194.28,318.29,229.87,107.29"><row><cell></cell><cell>Approach</cell><cell cols="3">Dataset MAP 0.5 R 0.5 MAP 0</cell></row><row><cell></cell><cell>k-NN, k = 13</cell><cell>A</cell><cell>0.003</cell><cell>0.004 0.272</cell></row><row><cell></cell><cell cols="2">Statistical labeling A</cell><cell>0.002</cell><cell>0.003 0.203</cell></row><row><cell>I</cell><cell>Baseline CNN</cell><cell>A</cell><cell>0.003</cell><cell>0.004 0.228</cell></row><row><cell></cell><cell cols="2">Transfer Learning A</cell><cell>0.003</cell><cell>0.004 0.291</cell></row><row><cell></cell><cell>3-Layer CNN</cell><cell>A</cell><cell>0.003</cell><cell>0.004 0.205</cell></row><row><cell></cell><cell>YOLO + k-NN</cell><cell>A</cell><cell>0.229</cell><cell>0.131 0.500</cell></row><row><cell>II</cell><cell cols="2">YOLO + Statistical A YOLO + k-NN C</cell><cell>0.243 0.210</cell><cell>0.131 0.488 0.122 0.455</cell></row><row><cell></cell><cell cols="2">YOLO + Statistical C</cell><cell>0.220</cell><cell>0.122 0.442</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,145.72,386.75,323.92,54.27"><head>Table 3 :</head><label>3</label><figDesc>Performance of 20% of the training set tiles compared to all tiles.</figDesc><table coords="9,164.35,399.09,286.65,41.94"><row><cell>Amount of dataset</cell><cell cols="2">Precision</cell><cell cols="2">Recall</cell><cell>F1 Score</cell></row><row><cell></cell><cell cols="5">Non-coral Coral Non-coral Coral Non-coral Coral</cell></row><row><cell>1.0</cell><cell>0.6916</cell><cell cols="2">0.4950 0.7664</cell><cell cols="2">0.4013 0.7271</cell><cell>0.4433</cell></row><row><cell>0.2</cell><cell>0.6907</cell><cell cols="2">0.4900 0.7603</cell><cell cols="2">0.4034 0.7238</cell><cell>0.4425</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,437.58,337.64,7.86;10,151.52,448.53,272.02,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,321.27,437.58,159.32,7.86;10,151.52,448.53,106.03,7.86">Augmentor: an image augmentation library for machine learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Bloice</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stocker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04680</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.96,459.50,337.63,7.86;10,151.52,470.46,329.07,7.86;10,151.52,481.42,243.53,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,206.53,470.46,167.18,7.86">Overview of ImageCLEFcoral 2019 task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">G</forename><surname>Clift</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="10,398.45,470.46,82.14,7.86;10,151.52,481.42,147.90,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,492.38,337.64,7.86;10,151.52,503.34,25.60,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,337.95,492.38,142.65,7.86">Pedestrian detection: A benchmark</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,514.31,337.64,7.86;10,151.52,525.27,155.97,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,205.97,514.31,40.00,7.86">Fast r-cnn</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,267.47,514.31,213.12,7.86;10,151.52,525.27,63.18,7.86">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,536.23,337.64,7.86;10,151.52,547.19,296.40,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,313.97,536.23,163.07,7.86">Textural features for image classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,151.52,547.19,215.00,7.86">IEEE Transactions on systems, man, and cybernetics</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,558.16,337.64,7.86;10,151.52,569.11,329.07,7.86;10,151.52,580.05,329.07,7.89;10,151.52,591.03,47.10,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,458.56,569.11,22.04,7.86;10,151.52,580.07,222.65,7.86">Coral reefs under rapid climate change and ocean acidification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Hoegh-Guldberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Mumby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Hooten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Steneck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Greenfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Harvell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Sale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Caldeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,380.47,580.07,27.71,7.86">science</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="issue">5857</biblScope>
			<biblScope unit="page" from="1737" to="1742" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,602.00,337.63,7.86;10,151.52,612.93,162.65,7.89" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,195.93,602.00,196.21,7.86">Visual pattern recognition by moment invariants</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,399.20,602.00,81.39,7.86;10,151.52,612.96,75.81,7.86">IRE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="187" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,623.92,337.63,7.86;10,151.52,634.88,329.07,7.86;10,151.52,645.84,329.07,7.86;10,151.52,656.80,221.65,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,333.28,634.88,147.31,7.86;10,151.52,645.84,222.66,7.86">Imageclef 2019: Multimedia retrieval in lifelogging, medical, nature, and security applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,394.68,645.84,85.91,7.86;10,151.52,656.80,98.43,7.86">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,119.67,337.64,7.86;11,151.52,130.61,151.45,7.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,201.56,119.67,223.93,7.86">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,432.03,119.67,48.56,7.86;11,151.52,130.63,44.84,7.86">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004-11">Nov 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,141.59,337.98,7.86;11,151.52,152.55,227.65,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,280.17,141.59,196.57,7.86">Comparing images using color coherence vectors</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,165.60,152.55,68.35,7.86">ACM multimedia</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="65" to="73" />
			<date type="published" when="1996">1996</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,163.51,337.97,7.86;11,151.52,174.47,329.07,7.86;11,151.52,185.43,204.12,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,363.83,163.51,116.76,7.86;11,151.52,174.47,103.81,7.86">You only look once: Unified, real-time object detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,277.90,174.47,202.69,7.86;11,151.52,185.43,120.32,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,196.39,337.97,7.86;11,151.52,207.34,231.27,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="11,278.92,196.39,201.67,7.86;11,151.52,207.34,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
