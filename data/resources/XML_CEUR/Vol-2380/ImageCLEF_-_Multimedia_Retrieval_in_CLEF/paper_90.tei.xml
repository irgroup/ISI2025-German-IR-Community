<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.18,115.96,324.99,12.62;1,256.21,133.89,102.93,12.62">Coral Reef Annotation and Localization using Faster R-CNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,174.13,171.56,60.53,8.74"><forename type="first">S</forename><forename type="middle">M</forename><surname>Jaisakthi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vellore Institute of Technology</orgName>
								<address>
									<settlement>Vellore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,245.21,171.56,55.79,8.74"><forename type="first">P</forename><surname>Mirunalini</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">SSN College of Engineeing. Kalavakkam</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,330.94,171.56,105.82,8.74"><forename type="first">Chandrabose</forename><surname>Aravindan</surname></persName>
							<email>aravindanc@ssn.edu.in</email>
							<affiliation key="aff1">
								<orgName type="institution">SSN College of Engineeing. Kalavakkam</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.18,115.96,324.99,12.62;1,256.21,133.89,102.93,12.62">Coral Reef Annotation and Localization using Faster R-CNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5A57173237D496996E85C778BD07398B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Coal Reef</term>
					<term>Object Detection</term>
					<term>Faster R-CNN</term>
					<term>Convolutional Neural Network (CNN)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Coral reefs are the most diverse and valuable ecosystems in the world. It is also called as rainforests of the sea as they are so diverse. Coral reefs are important since it provide shelter and food to many marine species and also act as the source of nitrogen and other essential nutrients for marine food chains. Recent studies show that coral reefs ecosystems are extremely threatened due to pollution, sedimentation, unviable fishing practices, and climate change. So, coral reefs should be protected and monitored to save marine ecosystem. Hence, to monitor coral reef a task was introduced in ImageCLEF 2019, to automatically identify and label different types of benthic substrate with bounding boxes in a given image. This paper presents a Convolutional Neural Network (CNN) based method to locate and detect different types of benthic substrate. We have used faster RCNN architecture to detect the substrate since this method is much faster and accurate in detecting the objects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coral reefs <ref type="bibr" coords="1,186.48,496.35,9.96,8.74" target="#b0">[1]</ref>, <ref type="bibr" coords="1,203.85,496.35,10.52,8.74" target="#b2">[3]</ref> are large underwater structures which are composed of the skeletons of colonial marine invertebrates called coral. These colonies are groups of individual animals called polyps. The reef structures are formed by the polyps secretion, upon which they live, which is made up of a substance called calcium carbonate. These coral make significant contributions to the well-being of people, animals, and plants in marine and coastal environments. They protect the coastal land from erosion that is caused by waves and storms. Coral reefs are not only important in terms of worldwide tourism, but it also serve as an important indicator to evaluate the health of our planet. In addition, they are an essential source of food and protein for millions of people throughout the world and also provide medical benefits to us. But today, we are the ones threatening reefs.</p><p>Around the world, roughly 50 percent of coral reefs have died in just the past few decades. The Great Barrier Reef was even declared dead last year. Coral reefs should be protected and many organisations are working hard to protect coral reefs. So an automatic system to locate and detect the coral reef in the sea will be helpful to conserve coral reefs. In the ImageCLEFcoral 2019 task, coral reefs are localised and annotated automatically using the CNN based object detection methods in an given image.</p><p>Predicting the location of the object along with the class label is called as object detection. This can be achieved with deep learning or computer vision techniques by localizing the objects along with image classification in each image. The traditional object detection methods involve detection based on block-wise orientation of histogram features. These methods use low level characteristics of the object features and hence, not able to discriminate objects of different labels well. But deep learning based methods construct a representation in hierarchical manner using low to high level features extracted from neural networks which improves the detection accuracy much better.</p><p>In deep learning object detection problem can be considered as a classification problem by classifying the image patches extracted from the images. In general the CNNs used for classification were too slow and computationally expensive because of running on so many patches generated by sliding window detector. This problem can be solved using R-CNN, which uses selective search that reduces number of bounding boxes to the classifier. Selective search uses local cues like texture, intensity, color and/or a measure of insideness etc. to generate all the possible locations of the objects. The selected objects regions are wrapped to a fixed size pixels and are fed to a classifier which gives the individual probability of the region belonging to background and classes. So to locate and detect the coral reefs in an input image we have used faster R-CNN technique to achieve good accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Methodology</head><p>Our method for substrate detection is based on the Faster R-CNN <ref type="bibr" coords="2,440.94,524.61,10.52,8.74" target="#b7">[8]</ref> architecture. Faster R-CNN uses Region Proposal Network (RPN) using CNN to generate region proposals. This architecture consist of 3 layers, namely convolutional layer to extract feature maps, RPN to obtain region proposals with the help of anchor boxes and detection network for predicting object classes and bounding boxes. The proposed method consists of 3 stages namely preprocessing, training and substrate detection. In this work we have used five variants of pretrained COCO object detection model namely (1)Faster RCNN with NasNet (with augmentation), (2) Faster RCNN with NasNet (without augmentation) (3) Faster RCNN with inception V2 (with augmentation) (4) Faster RCNN with inception V2 (without augmentation) and (5) Faster RCNN with resnet101 (with augmentation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset</head><p>The dataset for this task is taken from coral reefs around the world as part of a coral reef monitoring project with the Marine Technology Research Unit at the University of Essex. The images contains the following 13 types of substrates: Hard Coral -Branching, Hard Coral -Submassive, Hard Coral -Boulder, Hard Coral -Encrusting, Hard Coral -Table, Hard Coral -Foliose, Hard Coral -Mushroom, Soft Coral, Soft Coral -Gorgonian, Sponge, Sponge -Barrel, Fire Coral -Millepora and Algae -Macro or Leaves. The data set contains 240 training images with 6670 annotated substrates along with ground truth annotations as bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Preprocessing</head><p>To reduce the computational complexity we have scaled down the input images. To build a strong object detector we have applied image augmentation <ref type="bibr" coords="3,456.97,301.98,10.52,8.74" target="#b6">[7]</ref> to improve the accuracy. We have created more training images using augmentation by applying horizontal and vertical flips, rotating by 90 degrees and randomly adjusting the contrast and brightness of the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>We have used five variants of Faster R-CNN architecture that comes with the Tensorflow Object Detection API <ref type="bibr" coords="3,285.42,403.39,9.96,8.74" target="#b3">[4]</ref>. The architectures were pre-trained using the COCO dataset <ref type="bibr" coords="3,219.95,415.34,10.52,8.74" target="#b5">[6]</ref> that contains 300k images from 80 categories of animals, furniture, vehicles, etc. for general object detection. In order to make the pretrained models to learn the characteristics of benthic substrates we have finetuned it using the data set provided by the task ImageCLEFcoral 2019 <ref type="bibr" coords="3,448.39,451.21,10.52,8.74" target="#b1">[2]</ref> [5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Coral Reef Image Annotation and Localisation</head><p>To localize the coral reef we have trained the models using the dataset provided by the organizers using the hyper-parameters recommended in the Tensorflow Object Detection API. The different models used in this task are discussed in the following sections.</p><p>Faster RCNN with NasNet In this model we have trained Faster R-CNN with NasNet as backbone. To study the performance of this model we have conducted two different experiments, one with image augmentation and the other without augmentation. This architecture used NasNet to extract the features in the first stage with l2 regularizer. Since NasNet utilizes very large memory space we experienced resource allocation problem during the training phase. So we have down scaled the input images size as 300 × 300 and trained the architecture using dataset for 120000 epochs.</p><p>Faster R-CNN with Inception V2 Faster R-CNN with inception V2 model extracts the features from the input images using inception resnet v2 during the first stage. To reduce the computational complexity the input images are reduced to the size of 600*1024. The model is trained for 100000 epochs with l2 regularizer and truncated normal initializer in the first stage. Anchors are generated for 4 scales with 3 different aspect ratio. For box predictor, the model is trained with l2 regularizer and variance scale initializer is used. The model performance is evaluated and analysed with both with and without image augmentation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Faster R-CNN with Resnet101</head><p>In this architecture we have used Faster RCNN Resnet101 to extract features in the first stage along with the image augmentation technique. The model is trained with the coral reef dataset for 150000 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Results and Discussion</head><p>The proposed methods were evaluated using intersection over union (IoU), the area of intersection between the foreground in the output segmentation and the foreground in the ground-truth segmentation, divided by the area of their union. The final results were calculated using average performance over all images of all concepts, and also per concept performance over all images. The following table shows the result of the proposed method presented in this paper. MAP 50 is the localised Mean average precision (MAP) for each submitted method for using the performance measure of IoU &gt;= 50 of the ground truth, R 50 is the localised mean recall for each submitted method for using the performance measure of IoU &gt;= 50 of the ground truth and MAP 0 is the image annotation average for each method with success if the concept is simply detected in the image without any localisation. From the above table it is clear that all the methods with image augmentation techniques produced a good mean average precision when compared with the other methods trained without augmentation. But mean average recall is slightly higher for Faster R-CNN with NasNet without augmentation when compared to NasNet with augmentation. This variation may be due to which we have downscaled the input images too much. So further we need to conduct experiments by increasing the size of the input images. It is also found that among three different architectures Faster R-CNN with NasNet produced a good result in terms of both precision and recall.</p><p>In terms of per substrate accuracy, Faster R-CNN with NasNet produced a good accuracy when compared to the methods presented by the other participants. Table <ref type="table" coords="5,196.33,178.77,4.98,8.74" target="#tab_1">2</ref> shows the results of per substrate accuracy presented by other teams. From Table <ref type="table" coords="5,209.51,432.24,4.98,8.74" target="#tab_1">2</ref> it is evident that our method produced better accuracy in identifying many substrate types.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,139.01,416.92,337.54,80.60"><head>Table 1 .</head><label>1</label><figDesc>Results of the proposed Method</figDesc><table coords="4,139.01,437.62,337.54,59.90"><row><cell>Method</cell><cell>MAP 50 R 50</cell><cell>MAP 0</cell></row><row><cell>Faster R-CNN with NasNet (with augmentation)</cell><cell cols="2">0.139962 0.068156 0.43097514</cell></row><row><cell>Faster R-CNN with NasNet (without augmentation)</cell><cell cols="2">0.134396 0.072253 0.42396952</cell></row><row><cell>Faster R-CNN with inception V2 (with augmentation)</cell><cell cols="2">0.084863 0.045624 0.42396952</cell></row><row><cell cols="3">Faster R-CNN with inception V2 (without augmentation) 0.048321 0.028678 0.28710386</cell></row><row><cell>Faster R-CNN with resnet101 (with augmentation)</cell><cell cols="2">0.040993 0.027374 0.27161182</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,138.11,220.28,347.16,182.67"><head>Table 2 .</head><label>2</label><figDesc>Accuracy per substrate obtained by all the participating teams</figDesc><table coords="5,138.11,240.59,347.16,162.36"><row><cell>Run</cell><cell>Group</cell><cell>hard coral branch-ing</cell><cell>hard coral sub-mas-sive</cell><cell>hard coral boul-der</cell><cell>hard coral en-crust-ing</cell><cell>hard coral table</cell><cell>hard coral foliose</cell><cell>hard coral mush-room</cell><cell>soft coral</cell><cell>soft coral gor-gonian</cell><cell>sponge</cell><cell>sponge barrel</cell><cell>fire coral mille-pora</cell><cell>algae macro or leaves</cell></row><row><cell cols="2">27115 VIT</cell><cell>0.0436</cell><cell>0</cell><cell cols="2">0.0809 0.0168</cell><cell>0</cell><cell cols="3">0.0128 0.0664 0.0722</cell><cell>0</cell><cell cols="2">0.0349 0.0526</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">27347 VIT</cell><cell>0.0456</cell><cell>0</cell><cell cols="2">0.0374 0.0055</cell><cell>0</cell><cell>0</cell><cell cols="2">0.0204 0.0918</cell><cell>0</cell><cell cols="2">0.0239 0.0498</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">27348 VIT</cell><cell>0.0548</cell><cell>0</cell><cell cols="2">0.0956 0.0171</cell><cell>0</cell><cell cols="3">0.0129 0.119 0.0782</cell><cell>0</cell><cell cols="2">0.0365 0.0579</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">27349 VIT</cell><cell>0.0637</cell><cell>0</cell><cell cols="2">0.1012 0.0195</cell><cell>0</cell><cell cols="3">0.0028 0.0758 0.0804</cell><cell>0</cell><cell cols="2">0.0329 0.0619</cell><cell>0</cell><cell>0.0004</cell></row><row><cell cols="2">27350 VIT</cell><cell>0.0597</cell><cell>0</cell><cell cols="2">0.0305 0.0141</cell><cell>0</cell><cell>0</cell><cell cols="2">0.0422 0.0808</cell><cell>0</cell><cell cols="2">0.0299 0.0598</cell><cell>0</cell><cell>0</cell></row><row><cell cols="3">27398 HHUD 0.0013</cell><cell>0</cell><cell>0.0116</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.0702</cell><cell>0</cell><cell>0.002</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="6">27413 HHUD 0.0068 0.0021 0.0063 0.0014</cell><cell>0</cell><cell>0.0022</cell><cell>0</cell><cell>0.0523</cell><cell>0</cell><cell>0.0063</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="3">27414 HHUD 0.0089</cell><cell>0</cell><cell cols="2">0.016 0.0015</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.0562</cell><cell>0</cell><cell cols="2">0.0104 0.0054</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">27415 HHUD</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.0731</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="3">27416 HHUD 0.0346</cell><cell>0</cell><cell cols="2">0.0343 0.0064</cell><cell>0</cell><cell>0</cell><cell cols="5">0.0437 0.055 0.0008 0.0094 0.0222</cell><cell>0</cell><cell>0.0158</cell></row><row><cell cols="3">27417 HHUD 0.0356</cell><cell>0</cell><cell cols="2">0.033 0.0069</cell><cell>0</cell><cell>0</cell><cell cols="5">0.0406 0.0505 0.0008 0.0094 0.0213</cell><cell>0</cell><cell>0.0157</cell></row><row><cell cols="3">27418 HHUD 0.0246</cell><cell>0</cell><cell cols="2">0.0321 0.0038</cell><cell>0</cell><cell>0</cell><cell cols="5">0.0269 0.0447 0.0005 0.0086 0.0318</cell><cell>0</cell><cell>0.0012</cell></row><row><cell cols="3">27419 HHUD 0.0261</cell><cell>0</cell><cell cols="2">0.0315 0.0042</cell><cell>0</cell><cell cols="6">0.0023 0.0228 0.0423 0.0005 0.0088 0.0304</cell><cell>0</cell><cell>0.0012</cell></row><row><cell cols="3">27421 HHUD 0.0007</cell><cell>0</cell><cell cols="2">0.0167 0.0048</cell><cell>0</cell><cell cols="3">0.0006 0.0106 0.0571</cell><cell>0</cell><cell>0.0072</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="3">27497 ISEC 0.0198</cell><cell>0</cell><cell cols="2">0.0007 0.0121</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.0079</cell><cell>0</cell><cell>0.0277</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="3">Acknowledgements</head><p>The authors would like to thank the management of <rs type="institution">VIT university, Vellore, India and SSN College of Engineering, Chennai India</rs> for funding the respective research labs where the research work is being carried out. One of the authors <rs type="person">S M Jaisakthi</rs> would like to thank <rs type="institution">NVIDIA</rs> for providing a GPU grant in support of this research work and similarly <rs type="person">P Mirunalini</rs> and <rs type="person">Chandrabose Aravindan</rs> would like to thank the management for providing the GPU machine where this research is carried out.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,270.11,59.01,7.86;6,213.43,270.11,8.19,7.86;6,237.68,270.11,22.04,7.86;6,275.78,270.11,23.99,7.86;6,315.84,270.76,164.76,7.47;6,146.91,281.71,128.60,7.47" xml:id="b0">
	<monogr>
		<ptr target="http://www.deepbluediscoveries.com/introduction-to-coral-reefs" />
		<title level="m" coord="6,146.91,270.11,50.45,7.86;6,213.43,270.11,8.19,7.86;6,237.68,270.11,22.04,7.86;6,275.78,270.11,19.99,7.86">Introduction to Coral Reefs</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,292.03,342.24,7.86;6,146.91,302.99,333.68,7.86;6,146.91,313.95,215.06,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,196.78,302.99,156.88,7.86">Overview of ImageCLEFcoral 2019 task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">G</forename><surname>Clift</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="6,373.26,302.99,107.33,7.86;6,146.91,313.95,119.44,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,324.90,342.24,8.12;6,146.91,335.86,161.97,8.12" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gray</surname></persName>
		</author>
		<ptr target="https://www.edgeofexistence.org/blog/coral-reefs-an-introduction/" />
		<title level="m" coord="6,185.73,324.90,114.07,7.86">Coral Reefs: An introduction</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,346.82,342.24,7.86;6,146.91,357.78,333.68,7.86;6,146.91,368.71,333.68,8.14;6,146.91,380.34,112.98,7.47" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,360.74,357.78,119.85,7.86;6,146.91,368.74,156.21,7.86">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno>CoRR abs/1611.10012</idno>
		<ptr target="http://arxiv.org/abs/1611.10012" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,390.66,342.24,7.86;6,146.91,401.62,333.68,7.86;6,146.91,412.58,333.68,7.86;6,146.91,423.53,333.67,7.86;6,146.91,434.49,333.68,7.86;6,146.91,445.45,333.68,7.86;6,146.91,456.41,333.68,7.86;6,146.91,467.37,333.68,7.86;6,146.91,478.33,333.68,7.86;6,146.91,489.29,141.36,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,221.90,445.45,258.69,7.86;6,146.91,456.41,75.92,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,243.10,456.41,237.49,7.86;6,146.91,467.37,333.68,7.86;6,146.91,478.33,76.05,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="6,230.46,478.33,170.59,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF 2019)<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="6,138.35,500.25,342.24,7.86;6,146.91,511.21,333.68,7.86;6,146.91,522.16,199.02,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,203.40,511.21,173.09,7.86">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,397.19,511.21,83.40,7.86;6,146.91,522.16,75.98,7.86">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,533.12,342.24,7.86;6,146.91,544.08,333.68,7.86;6,146.91,555.04,333.68,7.86;6,146.91,566.00,191.80,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,319.41,533.12,161.18,7.86;6,146.91,544.08,216.12,7.86">Data augmentation for improving deep learning in image classification problem</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miko Lajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grochowski</surname></persName>
		</author>
		<idno type="DOI">10.1109/IIPHDW.2018.8388338</idno>
		<ptr target="https://doi.org/10.1109/IIPHDW.2018.8388338" />
	</analytic>
	<monogr>
		<title level="m" coord="6,399.43,544.08,81.16,7.86;6,146.91,555.04,195.62,7.86">2018 International Interdisciplinary PhD Workshop (IIPhDW)</title>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
			<biblScope unit="page" from="117" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,576.96,342.25,7.86;6,146.91,587.89,333.68,8.14;6,146.91,599.52,122.39,7.47" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,316.05,576.96,164.54,7.86;6,146.91,587.92,164.57,7.86">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR abs/1506.01497</idno>
		<ptr target="http://arxiv.org/abs/1506.01497" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
