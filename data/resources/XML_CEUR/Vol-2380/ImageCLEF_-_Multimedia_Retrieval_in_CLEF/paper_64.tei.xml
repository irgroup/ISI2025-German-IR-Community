<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.48,115.96,286.41,12.62;1,136.80,133.89,341.75,12.62;1,170.06,151.82,275.24,12.62">Ensemble of Streamlined Bilinear Visual Question Answering Models for the ImageCLEF 2019 Challenge in the Medical Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,146.92,189.89,52.03,8.74"><forename type="first">Minh</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
							<email>minh.vu@umu.se</email>
							<affiliation key="aff0">
								<orgName type="institution">Umeå University</orgName>
								<address>
									<postCode>901 87</postCode>
									<settlement>Umeå</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,209.50,189.89,79.85,8.74"><forename type="first">Raphael</forename><surname>Sznitman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">ARTORG Center</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.90,189.89,61.99,8.74"><forename type="first">Tufve</forename><surname>Nyholm</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Umeå University</orgName>
								<address>
									<postCode>901 87</postCode>
									<settlement>Umeå</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,391.83,189.89,72.14,8.74"><forename type="first">Tommy</forename><surname>Löfstedt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Umeå University</orgName>
								<address>
									<postCode>901 87</postCode>
									<settlement>Umeå</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,164.48,115.96,286.41,12.62;1,136.80,133.89,341.75,12.62;1,170.06,151.82,275.24,12.62">Ensemble of Streamlined Bilinear Visual Question Answering Models for the ImageCLEF 2019 Challenge in the Medical Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8D7857E12BD309CB68A2ADEF4173C9C9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the contribution by participants from Umeå University, Sweden, in collaboration with the University of Bern, Switzerland, for the Medical Domain Visual Question Answering challenge hosted by ImageCLEF 2019. We proposed a novel Visual Question Answering approach that leverages a bilinear model to aggregate and synthesize extracted image and question features. While we did not make use of any additional training data, our model used an attention scheme to focus on the relevant input context and was further boosted by using an ensemble of trained models. We show here that the proposed approach performs at state-of-the-art levels, and provides an improvement over several existing methods. The proposed method was ranked 3rd in the Medical Domain Visual Question Answering challenge of Im-ageCLEF 2019.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning (DL) has dramatically reshaped the state-of-the-art in computer vision, natural language processing (NLP), and many other domains. This is the case within medical image analysis as well. With exceptional outcomes for various diagnostic and prognostic tasks, DL has attracted the attention of the medical community. The hope is that DL will improve results or provide automated tools that can support clinical decision making, for example in the Visual Question Answering (VQA) task.</p><p>VQA is a complex multimodal task that aims at answering a question about an image. Here, a system needs to fathom both the image and question in order to correctly answer the question. Most recent VQA methods consists of artificial neural networks trained to answer a question regarding a given image <ref type="bibr" coords="1,443.12,589.55,9.96,8.74" target="#b8">[9]</ref>. Such models incorporate: (1) a question model encoding the question input, (2) an image model extracting visual features from the input image, (3) a fusion scheme that combines the image and question features, and (4) a classifier that uses the combined features to select the most likely answer.</p><p>ImageCLEF <ref type="bibr" coords="2,205.31,330.43,10.52,8.74" target="#b7">[8]</ref> aims to support the need of the global community of reusable resources for benchmarking the cross-language annotation and retrieval of images. In 2019, ImageCLEF had four main tasks: lifelogging, medicine, nature, and security. With the purpose of providing a "second opinion" for clinicians on complex medical images and offering patients an economical way to monitor their disease status, ImageCLEF organizes a medical domain VQA challenge, called ImageCLEF-VQA-Med <ref type="bibr" coords="2,267.46,402.16,10.52,8.74" target="#b0">[1]</ref> (see examples in Figure <ref type="figure" coords="2,387.29,402.16,3.87,8.74" target="#fig_0">1</ref>).</p><p>In the present work, we describe the model that we developed for the ImageCLEF-VQA-Med 2019 challenge. First, we present a novel fusion scheme for questions and images. Second, we introduce an image preprocessing step that suppresses unwanted distortions to enhance the quality of the ImageCLEF-VQA-Med images before they are fed into a Convolutional Neural Network (CNN) for image feature extraction. Third, we propose to utilize a pre-trained Bidirectional Encoder Representations from Transformers (BERT) model <ref type="bibr" coords="2,383.10,485.84,10.52,8.74" target="#b4">[5]</ref> to extract the question features. Last, we present an ensemble of VQA models that gave a large boost in the evaluation metrics on both validation and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Since most existing VQA methods use standard embedding models for text <ref type="bibr" coords="2,465.09,560.48,15.50,8.74" target="#b11">[12]</ref> and standard CNNs to extract image features <ref type="bibr" coords="2,336.60,572.43,9.96,8.74" target="#b5">[6]</ref>, the research focus has largely been on fusion strategies that combine information from both input sources <ref type="bibr" coords="2,456.17,584.39,11.25,8.74" target="#b5">[6,</ref><ref type="bibr" coords="2,467.43,584.39,11.25,8.74" target="#b9">10,</ref><ref type="bibr" coords="2,478.68,584.39,7.50,8.74" target="#b2">3]</ref>. Recently, attention schemes have also been introduced in VQA models in order to focus the trained models towards question-guided evaluations. The review paper of Kafle et al. <ref type="bibr" coords="2,225.31,620.25,10.52,8.74" target="#b8">[9]</ref> offers a comprehensive overview of recent VQA models.</p><p>An image model is used to extract visual features from the input images. Most recent VQA models use CNNs, often ones that are pre-trained on e.g. the Ima-geNet dataset <ref type="bibr" coords="2,197.79,656.12,14.61,8.74" target="#b12">[13]</ref>. Popular choices for the image model includes: VGGNet <ref type="bibr" coords="2,462.33,656.12,14.61,8.74" target="#b14">[15]</ref>, GoogLeNet <ref type="bibr" coords="3,187.34,118.99,14.61,8.74" target="#b15">[16]</ref>, and ResNet <ref type="bibr" coords="3,262.45,118.99,11.25,8.74" target="#b5">[6,</ref><ref type="bibr" coords="3,273.70,118.99,11.25,8.74" target="#b9">10,</ref><ref type="bibr" coords="3,284.96,118.99,7.50,8.74" target="#b2">3]</ref>. Multimodal Compact Bilinear (MCB) <ref type="bibr" coords="3,467.31,118.99,9.96,8.74" target="#b5">[6]</ref>, Multimodal Low-rank Bilinear (MLB) <ref type="bibr" coords="3,306.49,130.95,14.61,8.74" target="#b9">[10]</ref>, and Multimodal Tucker Fusion for Visual Question Answering (MUTAN) <ref type="bibr" coords="3,302.05,142.90,10.52,8.74" target="#b2">[3]</ref> are current VQA methods that employ bilinear transformation to encode image and question. As with these, we used a ResNet-152 model, that was pre-trained on the ImageNet dataset, to extract visual features.</p><p>Common models employed to extract question features include Long Shortterm Memory (LSTM) <ref type="bibr" coords="3,236.42,202.82,9.96,8.74" target="#b6">[7]</ref>, Gated Recurrent Units (GRU) <ref type="bibr" coords="3,388.60,202.82,9.96,8.74" target="#b3">[4]</ref>, and Skip-thought vectors <ref type="bibr" coords="3,169.58,214.78,14.61,8.74" target="#b11">[12]</ref>. Skip-thought vectors is a powerful unsupervised encoder-decoder approach that has been used in many recent VQA models <ref type="bibr" coords="3,384.49,226.73,11.25,8.74" target="#b5">[6,</ref><ref type="bibr" coords="3,395.74,226.73,11.25,8.74" target="#b9">10,</ref><ref type="bibr" coords="3,407.00,226.73,7.50,8.74" target="#b2">3]</ref>. In the present work, we not only used Skip-thought vectors but also evaluated the use of a pretrained BERT model <ref type="bibr" coords="3,233.86,250.64,10.52,8.74" target="#b4">[5]</ref> to extract question features. The BERT model has obtained state-of-the-art results on a wide variety of NLP tasks recently.</p><p>Attention mechanisms have led to breakthroughs in many NLP applications, for example, in neural machine translation <ref type="bibr" coords="3,324.67,286.65,9.96,8.74" target="#b1">[2]</ref>, and in computer vision, such as in image classification <ref type="bibr" coords="3,235.76,298.60,14.61,8.74" target="#b16">[17]</ref>. Propelled by the remarkable success accomplished by attention mechanisms in computer vision and NLP, numerous VQA models have employed attention schemes to improve predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>For the task of VQA <ref type="bibr" coords="3,228.13,379.72,9.96,8.74" target="#b8">[9]</ref>, we are interested in predicting the most likely answer, â, given a question, q, about an image, v. The problem can be stated as</p><formula xml:id="formula_0" coords="3,252.43,413.88,223.92,16.65">â = arg max a∈A P (a | q, v, Θ), (<label>1</label></formula><formula xml:id="formula_1" coords="3,476.35,413.88,4.24,8.74">)</formula><p>where A is the set of possible answers and Θ denotes all model parameters. Figure <ref type="figure" coords="3,182.37,454.24,4.98,8.74">2</ref> illustrates the proposed method. It uses pre-trained networks to extracts image and question features (in red and green, respectively), and feed them to a fusion scheme. These features are combined using an attention mechanism <ref type="bibr" coords="3,163.12,490.10,10.52,8.74" target="#b5">[6]</ref> (orange) to compute global image features, ṽ. We proposed an efficient bilinear transformation that takes two inputs: global image features and global question features, q, and yields a single latent feature vector, f , that is then linearly mapped to the answer vector (white) to generate the output. The proposed bilinear fusion scheme is further described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Proposed Method</head><p>To encode questions and images, we first make use of a multi-glimpse attention mechanism <ref type="bibr" coords="3,187.17,601.11,10.52,8.74" target="#b5">[6]</ref> to compute global image features, ṽ = [ω T 1 , . . . , ω T G ] T ∈ R KG , where K denotes the dimensions of the identity core tensor, that is decomposed using Tucker Decomposition in the attention scheme (see <ref type="bibr" coords="3,390.37,625.02,10.52,8.74" target="#b2">[3]</ref> for more details), and G is the number of glimpses. Fig. <ref type="figure" coords="4,153.45,222.64,3.87,8.74">2</ref>: Proposed method. We used a ResNet-152 model, that was pre-trained on the ImageNet dataset, to extract image features. Skip-thought vectors or a pre-trained BERT is employed to extract question features. These features are passed through an attention mechanism to produce global image features, ṽ, while the question features are linearly transformed to obtain global question features, q. We then apply the proposed bilinear transformation on these global features to compute output features, f , before calculating the output probability vector over the possible answers.</p><p>The global question features can be written as</p><formula xml:id="formula_2" coords="4,258.87,362.56,221.73,11.37">q = ReLU(W q q + b q ),<label>(2)</label></formula><p>where q ∈ R KG , q ∈ R J are the question features, and W q ∈ R KG×J and b q ∈ R KG denote the weight and bias terms, respectively. ReLU is the rectified linear unit activation function. Given these, the output features of the proposed model are encoded as</p><formula xml:id="formula_3" coords="4,172.56,442.32,308.03,33.76">fi = ReLU   KG j=1 KG k=1 qj w f ijk ṽk + b f i   = ReLU qT W f i ṽ + b f i ,<label>(3)</label></formula><p>where f ∈ R K , W f i ∈ R KG×KG and b f i ∈ R denote the weight and bias terms in the bilinear scheme, respectively.</p><p>The probabilities of each target answer over all possible target answers are then written as</p><formula xml:id="formula_4" coords="4,247.92,537.40,232.67,11.92">f = SoftMax(W a f + b a ),<label>(4)</label></formula><p>where f ∈ R N , and W a ∈ R N ×K and b a ∈ R N denote the weight and bias terms, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details and Training</head><p>The proposed method, illustrated in Figure <ref type="figure" coords="4,331.25,620.25,3.87,8.74">2</ref>, contains three different components: an image model (see Section 4.1), a question model (see Section 4.2), and the proposed fusion with an attention mechanism model. The implementation and training details of the latter one is discussed below.</p><p>To implement the attention mechanism, we followed the description in <ref type="bibr" coords="5,467.31,118.99,9.96,8.74" target="#b2">[3]</ref>. We used the Adam optimizer <ref type="bibr" coords="5,265.28,130.95,15.50,8.74" target="#b10">[11]</ref> with a learning rate of 0.0001, a batch size of 128 and used a dropout rate of 0.5 for all linear and bilinear layers. We trained the proposed model for 100 epochs on an Nvidia GTX 1080 Ti GPU, the training time for the whole network with the attention scheme was around 1.5 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ensemble of Multiple Models</head><p>We employed ensemble learning to build a committee from a collection of trained VQA models, each casts a weighted vote for the predicted answer, in order to use the wisdom of the crowd to produce better predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Model</head><p>We preprocessed and augmented the images before passing them through the pre-trained ResNet-152 model to extract image features.</p><p>To remove unwanted outer areas (text and/or background) from an image, we applied the following sequence of image processing techniques: Data augmentation was applied on the pre-processed dataset before the images were sent to the network to improve the generalization. We used two types of data augmentation: (i) rotate the image by a randomly selected number of degrees from the range [-20, 20], and (ii) randomly scale the image size using a scaling factor in the range [0.9, 1.1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Question Model</head><p>We evaluated the use of Skip-thought vectors and a pre-trained BERT model for extracting the question features. These features were then used in the VQA models (see Table <ref type="table" coords="5,215.69,656.12,3.87,8.74">2</ref>). We used the same preprocessing techniques for the questions as was used in <ref type="bibr" coords="6,146.34,312.68,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="6,156.85,312.68,7.01,8.74" target="#b5">6]</ref>. These were: (i) removing the punctuation marks, and (ii) converting to lower-case.</p><p>To overcome the challenge of seeing new words in the medical domain, we employed a Word2Vec model trained on the Google News dataset that includes three million words vectors. We then used a linear regression model without regularization to map the Word2Vec to the Skip-thought embedding space <ref type="bibr" coords="6,439.55,372.70,15.01,8.74" target="#b11">[12]</ref>. This enabled our Skip-thought vectors to generate 2,400-dimensional word features.</p><p>BERT is a deep bidirectional transformer encoder that has obtained new state-of-the-art results on multiple NLP tasks <ref type="bibr" coords="6,342.47,408.80,9.96,8.74" target="#b4">[5]</ref>. The BERT model was pretrained for general-purpose language understanding on a large text corpus, called WikiText-103, on two unique tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP) <ref type="bibr" coords="6,254.92,444.67,9.96,8.74" target="#b4">[5]</ref>. We employed two pre-trained BERT models: (i) bert-base-multilingual-uncased, and (ii) bert-base-multilingual-cased, to extract question features. Of each pre-trained model, we used a feature-based approach by generating ELMo-like <ref type="bibr" coords="6,247.86,480.54,15.50,8.74" target="#b13">[14]</ref> pre-trained contextual representations using two methods: (i) Second-to-Last Hidden (768-dimensional), and (ii) Concat Last Four Hidden (3,072-dimensional) (see Table <ref type="table" coords="6,328.76,504.45,4.98,8.74">7</ref> in <ref type="bibr" coords="6,348.69,504.45,10.51,8.74" target="#b4">[5]</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fusion Model with Attention Mechanism</head><p>In addition to the proposed model, the ensemble contained MLB and MUTAN models, for which we used freely available PyTorch code.</p><p>We integrated ten different MLB models <ref type="bibr" coords="6,330.79,580.80,15.50,8.74" target="#b9">[10]</ref> in the ensemble (see Table <ref type="table" coords="6,468.97,580.80,3.87,8.74">2</ref>). To prevent overfitting, we reduced the dimensions of the identity core tensor, K, to 64, 100 and 200 (the original value was K = 1,200, see <ref type="bibr" coords="7,383.99,118.99,14.76,8.74" target="#b9">[10]</ref>). Furthermore, we replaced all hyperbolic tangent (tanh) activations by ReLU activation functions.</p><p>We employed five different versions of the MUTAN architecture <ref type="bibr" coords="7,439.44,143.23,10.52,8.74" target="#b2">[3]</ref> in the ensemble model (see Table <ref type="table" coords="7,254.92,155.19,3.87,8.74">2</ref>). All hyper-parameters were set as in <ref type="bibr" coords="7,429.50,155.19,9.96,8.74" target="#b2">[3]</ref>. As with the MLB model, all hyperbolic tangent activations were replaced by ReLU activation functions.</p><p>Both MLB and MUTAN were trained to minimize the categorical cross entropy loss using the Adam optimizer with a learning rate of 0.0001 and exponential decay rates of β 1 = 0.9 and β 2 = 0.999. As in the proposed model, the batch size was 128 and the model was trained for 100 epochs. As for the proposed method, the training time for both the MLB and the MUTAN models on an Nvidia GTX 1080 Ti GPU was about 1.5 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ensemble Model</head><p>By varying the pre-trained question models and a few hyper-parameters of the fusion schemes, we trained more than 40 base models separately on the training set. We then evaluated their performance on the validation set to select the top 26 performing models (see Table <ref type="table" coords="7,281.82,340.11,3.87,8.74">2</ref>), and built ensemble models using those 26 models.</p><p>To generate the outputs for the test set, we trained the 26 aforementioned models on the concatenation of the training and validation sets with the aim of making the networks learn a wider range of answers.</p><p>We then used two ensemble techniques: the average,</p><formula xml:id="formula_5" coords="7,274.13,421.89,206.46,30.20">ã = 1 M M m=1 f m ,<label>(5)</label></formula><p>and the weighted average,</p><formula xml:id="formula_6" coords="7,251.99,483.82,228.60,29.67">ãweighted = M m=1 w m f m M m=1 w m ,<label>(6)</label></formula><p>where ã, ãweighted ∈ R N are the output probability vectors over the answers, f m ∈ R N is the answer vector corresponding to model m that was computed by Equation <ref type="formula" coords="7,193.95,549.23,3.87,8.74" target="#formula_4">4</ref>. The M is the number of models, and w m ∈ R is the weight corresponding to the performance of the mth model (computed as the mean accuracy over the last 21 epochs on the validation set, as seen in Table <ref type="table" coords="7,447.54,573.14,3.87,8.74">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we detail the ImageCLEF-VQA-Med dataset, and compare the proposed method to MLB <ref type="bibr" coords="7,251.76,644.16,15.50,8.74" target="#b9">[10]</ref> and MUTAN <ref type="bibr" coords="7,331.48,644.16,10.52,8.74" target="#b2">[3]</ref> when applied on the validation set. In addition, we discuss the results of the ensemble model on the test set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Material</head><p>The ImageCLEF-VQA-Med data were partitioned in three sets: (i) a training set of 3,200 images with 12,792 Question &amp; Answer (QA) pairs, (ii) a validation set of 500 images with 2,000 QA pairs, and (iii) a test set of 500 images with 500 questions. Different from previous challenges, the organizers of the ImageCLEF-VQA-Med 2019 categorized the questions in four groups: Modality, Plane, Organ System, and Abnormality. The task was to answer the questions about the medical images in the test set as correctly as possible. The evaluation metrics were: (i) strict accuracy, defined as the percentage of correctly classified predictions, and (ii) Bilingual Evaluation Understudy (BLEU) score that computes the similarity between n-grams of the ground truth answers and the corresponding predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Discussion</head><p>Table <ref type="table" coords="8,163.17,488.67,4.98,8.74" target="#tab_0">1</ref> compares the performance of the proposed method to MLB <ref type="bibr" coords="8,444.71,488.67,15.50,8.74" target="#b9">[10]</ref> and MUTAN <ref type="bibr" coords="8,176.65,500.62,9.96,8.74" target="#b2">[3]</ref>, while Table <ref type="table" coords="8,248.62,500.62,4.98,8.74">2</ref> shows the mean and standard error of the accuracies of the last 21 epochs of the 26 best-performing methods on the validation set. From Table <ref type="table" coords="8,207.78,524.53,4.98,8.74" target="#tab_0">1</ref> and Table <ref type="table" coords="8,263.97,524.53,4.98,8.74">2</ref> we see that: (1) the proposed method performs better than state-of-the-art methods on the ImageCLEF-VQA-Med dataset, (2) bert-base-multilingual-uncased gives better question representations than bertbase-multilingual-cased does, and (3) the question features extracted by the pretrained BERT models are as good as those produced by the Skip-thought vectors.</p><p>There are two possible explanations to why the proposed model outperforms MLB and MUTAN. First, the ReLU overcomes the vanishing gradient problem that hyperbolic tangent activation functions suffers from. It thus allows the proposed model to learn faster and therefore it may perform better. Second, by using the bilinear transformation instead of an inner product operation to produce the global question and image features, that are used in MLB and MU-TAN, the proposed method considers every possible combination of elements Table <ref type="table" coords="9,161.63,127.36,3.87,8.74">2</ref>: Mean accuracy (and standard errors) computed from the last 21 epochs for the 26 best-performing models on the validation set. The first column indicates the fusion scheme with attention mechanism that was used. The second column contains the question models that was used to extract the question features. Here, "bert-uncased" and "bert-cased" are bert-base-multilingual-uncased and bert-base-multilingual-cased, respectively (see Section 4.2). J denotes the dimension of the question feature space, while K and G are the dimensions of the identity core tensor and the number of glimpses, respectively (see Section 3.1). Note that we used ReLU activation functions for all models. from two aforementioned features, and thus become more capable of learning a larger range of answers.</p><p>Table <ref type="table" coords="9,176.10,620.25,4.98,8.74">3</ref> presents the accuracy and BLEU scores of the ensemble models on the validation and test sets. We selected the top 6 performing ensemble models on the validation set and used those to make predictions to submit to the evaluation server. As can be seen in Table <ref type="table" coords="9,274.30,656.12,3.87,8.74">3</ref>, the ensemble of 11 proposed models resulted Table <ref type="table" coords="10,162.65,127.36,3.87,8.74">3</ref>: Results of the ensemble models on the validation and test sets in the ImageCLEF-VQA-Med 2019. # denotes the number of base models used in the ensemble, while "Run" is the submission ID on the leaderboard. "skip-thought", "bert-768", and "bert-3072" denote the ensembles of base models, that use different types of question models: Skip-thought vectors, 768-dimensional BERT and 3072-dimensional BERT, respectively. Our results won the 3rd place at ImageCLEF-VQA-Med 2019 without using any additional training data. in a 1 % improvement on strict accuracy, which is consistent with the literature results of using ensembles. Our best performing model, that achieved the strict accuracy of 61.60 and the BLEU score of 63.89 on the validation set, was the ensemble of 11 proposed models (see Table <ref type="table" coords="10,212.77,436.30,3.87,8.74">2</ref>). This ensemble model also performed the best on the test set (61.60 accuracy and 63.89 BLEU score), and won 3rd place in the ImageCLEF-VQA-Med 2019 challenge without using additional training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a novel fusion scheme for the VQA task. The proposed approach was shown to perform better than current methods in the ImageCLEF-VQA-Med 2019 challenge. In addition, we introduced an image preprocessing pipeline and utilized a pre-trained BERT model <ref type="bibr" coords="10,346.93,553.68,10.52,8.74" target="#b4">[5]</ref> to extract question features for further processing. Last, we presented an ensemble method that boosted the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,260.87,345.82,8.74;2,134.77,272.82,170.05,8.74;2,134.77,115.83,79.54,79.54"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Examples of questions and images and their corresponding answers in the ImageCLEF-VQA-Med 2019 challenge.</figDesc><graphic coords="2,134.77,115.83,79.54,79.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,144.73,657.44,202.42,7.47;4,199.88,132.95,38.89,7.34;4,262.99,154.63,32.68,7.34;4,266.66,163.44,25.33,7.34;4,278.17,193.51,3.75,5.50;4,179.52,184.45,53.52,7.34;4,443.16,150.84,13.45,5.50;4,163.94,160.97,84.68,7.34;4,322.43,184.41,75.55,7.34;4,335.58,159.08,49.24,7.34;4,333.68,132.23,53.04,7.34;4,407.85,185.17,66.46,7.34;4,308.15,154.82,3.12,5.50;4,366.81,172.27,2.62,5.50;4,367.64,145.46,1.95,5.50"><head></head><label></label><figDesc>https://github.com/facebook/fb.resnet.torch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,133.99,351.55,251.14,8.74;5,133.99,363.50,346.60,8.74;5,151.70,375.46,7.75,8.74;5,133.99,387.41,346.61,8.74;5,151.70,399.37,131.81,8.74;5,133.99,411.32,165.99,8.74;5,133.99,423.28,294.23,8.74;5,133.99,435.23,210.38,8.74;5,133.99,447.19,180.46,8.74;5,133.99,459.14,346.60,8.74;5,151.70,471.10,12.73,8.74;5,133.99,483.05,271.29,8.74;5,129.01,495.01,330.18,8.74;5,129.01,506.96,194.22,8.74;5,129.01,518.92,157.06,8.74"><head>( 1 )</head><label>1</label><figDesc>Normalize the intensities of the input image to 0-255. (2) Apply Otsu's method to binarize the normalized image using a threshold of 5. (3) Apply an open operation on the thresholded image with a rectangular structuring element of size 40 × 40. (4) Fill the holes of the binary image. (5) Remove all connected components, except the two largest ones. (6) Compute a bounding-box of the foreground. (7) Crop the image to the bounding box. (8) Apply an open operation with a rectangular structuring element of size 50 × 50. (9) Crop the normalized image to the enlarged bounding box. (10) Multiply the results from steps (8) and (9) to obtain a cropped image. (11) Resize the cropped image to 448 × 448. (12) Z-normalize the resized image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,161.52,264.03,292.33,8.74;6,134.77,186.79,65.71,65.71"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Example images passed through the pre-processing pipeline.</figDesc><graphic coords="6,134.77,186.79,65.71,65.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,134.77,127.36,345.83,141.16"><head>Table 1 :</head><label>1</label><figDesc>Mean accuracy (and standard errors) computed from the last 21 epochs on the validation set for MUTAN, MLB (with default hyper-parameters), and the proposed model. K and G are the dimensions of the identity core tensor and the number of glimpses, respectively (see Section 3.1 for details). Note that we used Skip-thought vectors for all models.</figDesc><table coords="8,148.96,190.68,314.38,77.84"><row><cell>Fusion</cell><cell>Question</cell><cell>Activation</cell><cell>K</cell><cell>G</cell><cell>Mean</cell><cell>SE</cell></row><row><cell cols="2">MUTAN [3] skip-thought</cell><cell>tanh</cell><cell>n.a.</cell><cell>2</cell><cell>58.35</cell><cell>0.18</cell></row><row><cell></cell><cell>skip-thought</cell><cell>tanh</cell><cell>100</cell><cell>8</cell><cell>58.96</cell><cell>0.11</cell></row><row><cell>MLB [10]</cell><cell>skip-thought</cell><cell>tanh</cell><cell>200</cell><cell>4</cell><cell>58.23</cell><cell>0.16</cell></row><row><cell></cell><cell>skip-thought</cell><cell>tanh</cell><cell>1200</cell><cell>4</cell><cell>58.74</cell><cell>0.12</cell></row><row><cell>Proposed</cell><cell>skip-thought</cell><cell>ReLU</cell><cell>64</cell><cell>8</cell><cell>60.12</cell><cell>0.17</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="6,144.73,613.61,301.76,7.47;6,144.73,624.57,322.58,7.47;6,144.73,635.53,80.52,7.47;6,144.73,646.48,255.19,7.47;6,144.73,657.44,174.17,7.47"><p>https://github.com/Cadene/skip-thought.torch/tree/master/pytorch https://blog.einstein.ai/the-wikitext-long-term-dependency-languagemodeling-dataset/ https://github.com/huggingface/pytorch-pretrained-BERT https://github.com/Cadene/vqa.pytorch</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,634.88,337.63,7.86;10,151.52,645.84,329.07,7.86;10,151.52,656.80,329.07,7.86;11,151.52,119.67,329.07,7.86;11,151.52,130.63,91.16,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,166.77,645.84,313.82,7.86;10,151.52,656.80,24.70,7.86">VQA-Med: Overview of the medical visual question answering task at Image-CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="10,215.40,656.80,265.19,7.86;11,151.52,119.67,27.09,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings (CEUR-WS.org</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,141.02,337.63,7.86;11,151.52,151.98,247.18,7.86" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m" coord="11,296.12,141.02,184.47,7.86;11,151.52,151.98,85.88,7.86">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.96,162.36,337.64,7.86;11,151.52,173.32,236.77,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,362.00,162.36,118.60,7.86;11,151.52,173.32,142.19,7.86">MUTAN: Multimodal Tucker fusion for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,315.09,173.32,33.01,7.86">ICCV. p</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,183.70,337.63,7.86;11,151.52,194.66,329.07,7.86;11,151.52,205.62,281.85,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="11,217.23,194.66,263.36,7.86;11,151.52,205.62,120.48,7.86">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.96,216.01,337.63,7.86;11,151.52,226.96,329.07,7.86;11,151.52,237.92,97.80,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,383.13,216.01,97.46,7.86;11,151.52,226.96,255.86,7.86">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.96,248.31,337.64,7.86;11,151.52,259.27,329.07,7.86;11,151.52,270.23,133.43,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="11,455.25,248.31,25.34,7.86;11,151.52,259.27,324.77,7.86">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno>arXiv preprint:1606.01847</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,280.61,337.64,7.86;11,151.52,291.54,92.85,7.89" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,288.76,280.61,100.73,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,398.69,280.61,81.90,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,301.95,337.63,7.86;11,151.52,312.91,329.07,7.86;11,151.52,323.87,329.07,7.86;11,151.52,334.83,329.07,7.86;11,151.52,345.79,329.07,7.86;11,151.52,356.75,329.07,7.86;11,151.52,367.71,329.07,7.86;11,151.52,378.67,329.07,7.86;11,151.52,389.62,329.07,7.86;11,151.52,400.58,216.27,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,268.29,356.75,212.30,7.86;11,151.52,367.71,124.23,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,297.67,367.71,182.92,7.86;11,151.52,378.67,329.07,7.86;11,151.52,389.62,122.46,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="11,305.55,389.62,171.07,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="11,142.96,410.97,337.64,7.86;11,151.52,421.90,294.25,7.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,240.04,410.97,240.55,7.86;11,151.52,421.93,38.91,7.86">Visual question answering: Datasets, algorithms, and future challenges</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,197.40,421.93,176.52,7.86">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,432.31,337.98,7.86;11,151.52,443.27,257.14,7.86" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv preprint: 1610.04325</idno>
		<title level="m" coord="11,404.56,432.31,76.03,7.86;11,151.52,443.27,113.53,7.86">Hadamard product for low-rank bilinear pooling</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,453.65,337.97,7.86;11,151.52,464.61,93.19,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="11,239.72,453.65,176.61,7.86">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.61,475.00,337.98,7.86;11,151.52,485.96,259.41,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,194.42,485.96,82.23,7.86">Skip-thought vectors</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,297.66,485.96,24.18,7.86">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,496.34,337.97,7.86;11,151.52,507.30,329.07,7.86;11,151.52,518.26,86.01,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,327.02,496.34,153.57,7.86;11,151.52,507.30,103.94,7.86">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,275.64,507.30,200.74,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,528.64,337.98,7.86;11,151.52,539.60,329.07,7.86;11,151.52,550.56,97.80,7.86" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m" coord="11,227.51,539.60,179.35,7.86">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.61,560.94,337.97,7.86;11,151.52,571.90,231.27,7.86" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="11,278.92,560.94,201.67,7.86;11,151.52,571.90,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.61,582.29,337.98,7.86;11,151.52,593.25,329.07,7.86;11,151.52,604.21,300.55,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,288.02,593.25,124.44,7.86">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,432.51,593.25,48.08,7.86;11,151.52,604.21,271.88,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,614.59,337.97,7.86;11,151.52,625.55,329.07,7.86;11,151.52,636.51,329.07,7.86;11,151.52,647.47,181.78,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,404.76,614.59,75.83,7.86;11,151.52,625.55,329.07,7.86;11,151.52,636.51,76.42,7.86">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,248.67,636.51,231.92,7.86;11,151.52,647.47,97.71,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="842" to="850" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
