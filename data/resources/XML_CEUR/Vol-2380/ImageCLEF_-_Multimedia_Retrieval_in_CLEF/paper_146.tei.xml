<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.61,115.96,314.14,12.62;1,175.37,133.89,264.61,12.62;1,211.61,151.82,192.14,12.62">Full training versus fine tuning for radiology images concept detection task for the ImageCLEF 2019 challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,170.54,189.49,71.34,8.74"><forename type="first">Priyanshu</forename><surname>Sinha</surname></persName>
							<email>priyanshusinha@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Mentor Graphics India Pvt. Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.44,189.49,98.27,8.74"><forename type="first">Saptarshi</forename><surname>Purkayastha</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Indiana University Purdue University</orgName>
								<address>
									<postCode>46202</postCode>
									<settlement>Indianapolis</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.65,189.49,59.70,8.74"><forename type="first">Judy</forename><surname>Gichoya</surname></persName>
							<email>gichoya@ohsu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Oregon Health Science University</orgName>
								<address>
									<postCode>97239</postCode>
									<settlement>Portlnd</settlement>
									<region>OR</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.61,115.96,314.14,12.62;1,175.37,133.89,264.61,12.62;1,211.61,151.82,192.14,12.62">Full training versus fine tuning for radiology images concept detection task for the ImageCLEF 2019 challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0B6A8409A1F3B899613EAA3AEF30AC28</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Transfer Learning</term>
					<term>Layer wise Fine Tuning</term>
					<term>Deep Learning in Radiology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Concept detection from medical images remains a challenging task that limits implementation of clinical ML/AI pipelines because of the scarcity of the highly trained experts to annotate images. There is a need for automated processes that can extract concrete textual information from image data. ImageCLEF 2019 provided us a set of images with labels as UMLS concepts. We participated for the first time for the concept detection task using transfer learning. Our approach involved an experiment of layerwise fine tuning (full training) versus fine tuning based on previous reported recommendations for training classification, detection and segmentation tasks for medical imaging. We ranked number 9 in this year's challenge, with an F1 result of 0.05 after three entries. We had a poor result from performing layerwise tuning (F1 score of 0.014) which is consistent with previous authors who have described the benefit of full training for transfer learning. However when looking at the results by a radiologist, the terms do not make clinical sense and we hypothesize that we can achieve better performance when using medical pretrained image models for example PathNet and utilizing a hierarchical training approach which is the basis of our future work on this dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Concept detection from medical images remains a challenging task that limits implementation of clinical ML/AI pipelines because of the scarcity of the highly trained experts to annotate images. ImageCLEF is an annual challenge now in its third year that seeks contributions that provide techniques to map visual information to condensed textual descriptions. The process of automatic extraction of high level concepts from low level features is difficult when the images have occlusion, background clutter, intra-class variation, pose and lighting changes <ref type="bibr" coords="2,463.56,154.86,12.77,8.74" target="#b0">[1]</ref>.</p><p>Participants from past challenges in 2017 and 2018 noted a broad range of content and hence the 2019 <ref type="bibr" coords="2,261.36,188.91,10.52,8.74" target="#b1">[2]</ref> challenge was narrowed down in focus to only radiology images <ref type="bibr" coords="2,212.93,200.87,9.96,8.74" target="#b2">[3]</ref>. The focus on concept detection in the 2019 challenge is important because it is the first step of automatic image captioning, while also providing metadata to support context based image retrieval. This was our first time participating in the ImageCLEF challenge. The challenge is a multi-label classification problem, where one radiology image can have multiple labels. Previous participants had good performance when using transfer learning, hence we focussed on optimizing the ResNet50 <ref type="bibr" coords="2,383.78,282.75,10.52,8.74" target="#b4">[5]</ref> network which had the best performance compared to VGG19 <ref type="bibr" coords="2,324.75,294.70,9.96,8.74" target="#b3">[4]</ref>, Xception Net <ref type="bibr" coords="2,403.14,294.70,10.52,8.74" target="#b5">[6]</ref> and Inception-ResNetV2 <ref type="bibr" coords="2,181.68,306.66,9.96,8.74" target="#b6">[7]</ref>. We ranked number 9 in this year's challenge, with an F1 result of 0.05 after three entries. We had a poor result from performing layerwise tuning (F1 score of 0.014) which is consistent with previous authors who have described the benefit of full training during transfer learning. However when looking at the results by a radiologist, the terms do not make clinical sense and we hypothesize that we can achieve better performance when using medical pretrained image models for example PathNet which is the basis of our future work on this dataset. We describe our approach in detail in the remaining sections of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DATASET</head><p>A total of 6,031,814 image -caption pairs were extracted from PubMed Open Access and after processing were reduced to 72,187 radiology images from various modalities. This dataset included archived images from February 2018 to February 2019 <ref type="bibr" coords="2,200.65,478.38,9.96,8.74" target="#b2">[3]</ref>. Table <ref type="table" coords="2,244.76,478.38,4.98,8.74" target="#tab_0">1</ref> shows a summary of the images in the training, test and validation set. We did not use additional radiology training data for the purpose of our submission to this challenge. Each label is a UMLS concept provided as a csv file. Table <ref type="table" coords="2,218.84,514.24,4.98,8.74" target="#tab_1">2</ref> shows a representative sample of the data showing images in the training (First row), validation (second row) and test set (third row). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STUDY EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Analysis</head><p>The ImageCLEF images were formatted to the Imagenet directory style where the directory name is the UMLS label. This was because our approach was mainly based on transfer learning and would make repeat experiments easy to perform. Summary statistics of the dataset found 5217 unique UMLS/label concepts. There was image imbalance with approximately 90% of the labels containing less than 100 images; and 30% labels containing a single image. Table <ref type="table" coords="4,475.61,222.08,4.98,8.74" target="#tab_2">3</ref> shows the top 10 concepts occurring in the highest frequency in the training set. Analysis of the top 25 labels (summarized in Figure <ref type="figure" coords="4,362.25,463.37,4.43,8.74" target="#fig_0">1</ref>) show that there is persistent data imbalance with one label containing more than 6500 images (C0441633 -"Scanning") and one label containing less than 2000 images (C0006104 -"Brain"). We therefore discarded labels containing less than 1000 images and used class weight technique from sklearn for balancing our training data <ref type="bibr" coords="4,451.36,511.19,9.96,8.74" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>Each input image was resized into 224x224 pixels without cropping. We used a batch size of 32 with learning rate 0.0001. The batches were formed by randomly shuffling the dataset. Optimization was performed using Adam optimizer with default beta 1 (0.9) and beta 2 (0.999). Image augmentation during training was performed using the Keras ImageDataGenerator. Augmentations performed include rescaling, rotation, zooming, shearing and horizontal flipping. A total of 100 epochs were executed. We split the data to 85% training set and 15% validation set. The network was trained using the Keras framework with tensorflow as the backend, running on a NVIDIA Quadro P6000 GPU. We treated this as multi label classification problem and limited our training to the top 25 labels. Our base model was ResNet50, from which we removed the fully connected top layers and added our own auxiliary convolutional layer along with dense layers. To prevent overfitting, we used dropout between dense layers. After evaluating our performance with fine tuning the last layers and reviewing the literature on fine tuning versus full training <ref type="bibr" coords="5,349.17,403.02,9.96,8.74" target="#b8">[9]</ref>, we embarked on layerwise fine tuning using Resnet50(run 2). In the second run we sequentially trained each layer while freezing others. For this approach we decreased the learning rate for higher layers and fine tuned it layer wise by unfreezing layers below a particular layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation and Analysis</head><p>Tajbakhsh et al <ref type="bibr" coords="5,209.23,502.68,10.52,8.74" target="#b8">[9]</ref> performed the most comprehensive experiment evaluating the approach of fine tuning a network versus training a network from scratch. In their review of classification, detection and segmentation tasks using multiple imaging modalities including radiology, colonoscopy and ultrasound, they demonstrate better performance with layerwise fine tuning. Our attempt to replicate their superior performance when approaching concept detection task on the ImageCLEF 2019 dataset led to lower performance when layerwise fine tuning (F1 score of 0.014) versus whole fine tuning the network as a whole (F1 score 0.05) summarized in table 4. Our poor comparative performance may be due to poor selection of hyperparameters for fine tuning the network.</p><p>Our approach included a clinical review of some of the sample output by a radiologist who is one of the authors of this paper, and we notice a large discrepancy in the utility of the generated concepts (Table <ref type="table" coords="5,376.37,656.12,3.87,8.74" target="#tab_4">5</ref>). For example the first row demonstrated a chest xray with a pneumoperitoneum, and our model does not generate terms closely related to the actual radiograph interpretation. We hypothesize that a stepwise approach to training where ontology hierarchies for example laterality and anatomy are maintained may generate a superior performance that is clinically meaningful. Despite previous documentations of superior performance with layer wise fine tuning of medical image tasks, we had a poor performance with this approach for concept detection. There is an opportunity to improve on layer wise fine tuning for such tasks. We advance the challenge by reviewing clinical relevance of output, for which despite our performance at number 9 in the challenge we found that the clinical utility of the concepts detected was low and hypothesize that we can achieve better performance and improved clinical utility using a hierarchical approach to training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,222.59,311.49,170.17,7.89;5,134.77,115.84,345.81,180.88"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Frequency plot of top 25 concepts.</figDesc><graphic coords="5,134.77,115.84,345.81,180.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,200.64,557.34,214.09,71.90"><head>Table 1 .</head><label>1</label><figDesc>Data and corresponding number of images.</figDesc><table coords="2,246.80,588.10,121.75,41.14"><row><cell>Set</cell><cell>No of Images</cell></row><row><cell>Training Set</cell><cell>56629</cell></row><row><cell>Validation Set</cell><cell>14157</cell></row><row><cell>Test Set</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,134.77,116.41,345.83,18.85"><head>Table 2 .</head><label>2</label><figDesc>Table displaying image examples from the training set (first two row), validation set (third row) and test set (fourth row).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,236.20,265.40,142.95,148.61"><head>Table 3 .</head><label>3</label><figDesc>Top 10 training concepts.</figDesc><table coords="4,263.33,296.16,88.70,117.85"><row><cell cols="2">Concept Frequency</cell></row><row><cell>C0441633</cell><cell>6733</cell></row><row><cell>C0043299</cell><cell>6321</cell></row><row><cell>C1962945</cell><cell>6318</cell></row><row><cell>C0040395</cell><cell>6235</cell></row><row><cell>C0034579</cell><cell>6127</cell></row><row><cell>C0817096</cell><cell>5981</cell></row><row><cell>C0040405</cell><cell>5801</cell></row><row><cell>C1548003</cell><cell>5159</cell></row><row><cell>C0221198</cell><cell>4513</cell></row><row><cell>C0772294</cell><cell>4512</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,232.42,115.91,150.53,59.60"><head>Table 4 .</head><label>4</label><figDesc>F1 Score of Different Runs.</figDesc><table coords="6,258.74,144.90,97.86,30.60"><row><cell cols="2">Run ID F1 Score</cell></row><row><cell>Run 1 26815</cell><cell>0.05</cell></row><row><cell cols="2">Run 2 27011 0.014</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,134.77,292.57,345.83,281.58"><head>Table 5 .</head><label>5</label><figDesc>Sampled images from the test dataset with the generated concepts and radiologist generated terms.</figDesc><table coords="6,142.45,336.73,323.10,237.42"><row><cell>Sample Image Concept Detected</cell><cell></cell><cell>Radiologist generated terms</cell></row><row><cell cols="2">-C0751437(adenohypophyseal</cell><cell>-Chest Xray</cell></row><row><cell>dis)</cell><cell></cell><cell>-pneumoperitoneum</cell></row><row><cell cols="2">-C0079595 (diagnostic imaging</cell><cell>-frontal radiograph</cell></row><row><cell>technique)</cell><cell></cell></row><row><cell>-C0023884</cell><cell>(gastrointestinal</cell></row><row><cell>tract)</cell><cell></cell></row><row><cell cols="2">-C0003842 (arterial)</cell></row><row><cell cols="2">-C0751438 (posterior pituitary</cell><cell>-Skull radiograph</cell></row><row><cell>dis)</cell><cell></cell><cell>-cervical hardware</cell></row><row><cell cols="2">-C0013516 (dx ultrasound-</cell><cell>-mandibular fusion</cell></row><row><cell>heart)</cell><cell></cell><cell>-hardware</cell></row><row><cell cols="2">-C0221874 (spacers)</cell><cell>-lateral view</cell></row><row><cell cols="2">-C0042449 (venous subtree)</cell></row><row><cell cols="2">-C0023884(gastrointestinal</cell><cell>-Lateral radiograph</cell></row><row><cell>tract)</cell><cell></cell><cell>-Chest Xray</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,269.20,342.25,7.86;7,146.91,280.16,171.48,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,283.34,269.20,197.26,7.86;7,146.91,280.16,107.98,7.86">Concept Detection on Medical Images using Deep Residual Learning Network</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Katsios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,290.13,342.24,7.86;7,146.91,301.09,333.68,7.86;7,146.91,312.05,333.68,7.86;7,146.91,323.01,333.68,7.86;7,146.91,333.97,333.68,7.86;7,146.91,344.93,333.68,7.86;7,146.91,355.88,333.68,7.86;7,146.91,366.84,333.68,7.86;7,146.91,377.80,333.68,7.86;7,146.91,388.76,333.68,7.86;7,146.91,399.72,239.99,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,443.03,366.84,37.57,7.86;7,146.91,377.80,230.43,7.86">Overview of ImageCLEF 2019 : Challenges, Datasets and Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Henning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien And</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tran</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Triet And</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vitali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">García</forename><surname>Seco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Narciso</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ergina</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodríguez</forename><surname>Roberto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Cuevas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikos</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Konstantinos</forename><surname>Karampidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,388.00,377.80,92.60,7.86;7,146.91,388.76,188.68,7.86;7,353.50,388.76,127.09,7.86;7,146.91,399.72,183.54,7.86">Proceedings of the Tenth International Conference of the CLEF Association</title>
		<meeting>the Tenth International Conference of the CLEF Association<address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="7,138.35,409.69,342.24,7.86;7,146.91,420.65,333.68,7.86;7,146.91,431.61,333.68,7.86;7,146.91,442.57,333.67,7.86;7,146.91,453.53,333.67,7.86;7,146.91,464.48,333.68,7.86;7,146.91,475.44,333.68,7.86;7,146.91,486.40,333.68,7.86;7,146.91,497.36,333.68,7.86;7,146.91,508.32,74.75,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,408.70,409.69,71.90,7.86;7,146.91,420.65,202.73,7.86">Radiology objects in context (ROCO): A multimodal image dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich ; Hein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zahnd</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Demirci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Moriconi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,373.17,420.65,107.42,7.86;7,146.91,431.61,333.68,7.86;7,146.91,442.57,333.67,7.86;7,146.91,453.53,119.85,7.86;7,371.46,453.53,109.13,7.86;7,146.91,464.48,16.79,7.86;7,313.31,497.36,21.30,7.86">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Cheplygina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Mateus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Trucco</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Granger</surname></persName>
		</editor>
		<editor>
			<persName><surname>Jannin</surname></persName>
		</editor>
		<meeting><address><addrLine>LABELS; Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018-09-16">2018. September 16, 2018. 2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Cham</note>
</biblStruct>

<biblStruct coords="7,138.35,518.29,342.24,7.86;7,146.91,529.25,333.67,7.86;7,146.91,540.21,164.98,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,312.09,518.29,168.50,7.86;7,146.91,529.25,15.36,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,185.18,529.25,295.41,7.86;7,146.91,540.21,81.97,7.86">Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,550.18,342.24,7.86;7,146.91,561.14,154.51,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,279.36,550.18,201.22,7.86;7,146.91,561.14,73.85,7.86">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct coords="7,138.35,571.11,342.25,7.86;7,146.91,582.06,333.68,7.86;7,146.91,593.02,59.90,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,197.87,571.11,263.36,7.86">Xception: Deep Learning with Depthwise Separable Convolutions</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,146.91,582.06,305.23,7.86">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,602.99,342.24,7.86;7,146.91,613.95,293.39,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,354.54,602.99,126.05,7.86;7,146.91,613.95,212.52,7.86">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2016-02">Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,623.92,195.74,7.86;7,380.56,623.92,9.22,7.86;7,436.23,623.92,44.36,7.86;7,146.91,634.88,23.55,7.86;7,198.44,634.88,67.06,7.86;7,293.48,634.88,34.31,7.86;7,355.75,634.88,39.94,7.86;7,423.66,634.88,56.93,7.86;7,146.91,645.84,360.41,7.86;7,146.91,656.80,100.45,7.86" xml:id="b7">
	<monogr>
		<ptr target="https://scikit-learn.org/stable/modules/generated/sklearn.utils.classweight.computeclassweight.html" />
		<title level="m" coord="7,146.42,623.92,187.67,7.86;7,380.56,623.92,9.22,7.86;7,436.23,623.92,44.36,7.86;7,146.91,634.88,23.55,7.86;7,198.44,634.88,58.12,7.86">sklearn.utils.class weight.compute class weight -scikit-learn 0.21.2 documentation</title>
		<imprint>
			<date type="published" when="2019-05-27">27-May-2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,119.67,342.24,7.86;8,146.91,130.63,333.68,7.86;8,146.91,141.59,333.68,7.86;8,146.91,152.55,43.29,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,235.10,130.63,245.49,7.86;8,146.91,141.59,92.00,7.86">Convolutional neural networks for medical image analysis: full training or fine tuning?</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">T</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianming</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,250.08,141.59,106.88,7.86">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1312" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,163.51,337.98,7.86;8,146.91,174.47,333.68,7.86;8,146.91,185.43,333.68,7.86;8,146.91,196.39,188.24,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,221.26,174.47,250.61,7.86">Overview of the ImageCLEFmed 2019 Concept Prediction Task</title>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Friedrich</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">García</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Henning</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/,2019" />
	</analytic>
	<monogr>
		<title level="m" coord="8,146.91,185.43,104.93,7.86">CLEF2019 Working Notes</title>
		<title level="s" coord="8,259.36,185.43,170.39,7.86">CEUR Workshop Proceedings (CEUR-WS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,207.34,337.98,7.86;8,146.91,218.30,333.68,7.86;8,146.91,229.26,333.68,7.86;8,146.91,240.22,333.68,7.86;8,146.91,251.18,333.67,7.86;8,146.91,262.14,333.68,7.86;8,146.91,273.10,333.68,7.86;8,146.91,284.06,333.68,7.86;8,146.91,295.02,333.68,7.86;8,146.91,305.98,333.67,7.86;8,146.91,316.93,333.68,7.86;8,146.91,327.89,239.99,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,250.12,262.14,230.47,7.86;8,146.91,273.10,333.68,7.86;8,146.91,284.06,284.96,7.86;8,357.90,295.02,122.69,7.86;8,146.91,305.98,225.51,7.86">Friedrich and Alba García Seco de Herrera and Narciso Garcia and Ergina Kavallieratou and Carlos Roberto del Blanco and Carlos Cuevas Rodríguez and Nikos Vasillopoulos and Konstantinos Karampidis and</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cid</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitri</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aleh</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,384.95,305.98,95.64,7.86;8,146.91,316.93,188.68,7.86;8,353.50,316.93,127.09,7.86;8,146.91,327.89,214.90,7.86">Proceedings of the Tenth International Conference of the CLEF Association (CLEF</title>
		<meeting>the Tenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
