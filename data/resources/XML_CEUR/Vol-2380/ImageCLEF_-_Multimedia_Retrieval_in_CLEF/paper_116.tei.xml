<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.70,115.96,321.95,12.62;1,170.84,133.89,273.69,12.62">LSTM in VQA-Med, is it really needed? JCE study on the ImageCLEF 2019 dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,231.37,171.56,48.49,8.74"><forename type="first">Avi</forename><surname>Turner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Software Engineering College of Engineering Jerusalem</orgName>
								<orgName type="institution">Azrieli</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,307.01,171.56,72.51,8.74"><forename type="first">Assaf</forename><forename type="middle">B</forename><surname>Spanier</surname></persName>
							<email>assaf.spanier@mail.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Software Engineering College of Engineering Jerusalem</orgName>
								<orgName type="institution">Azrieli</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.70,115.96,321.95,12.62;1,170.84,133.89,273.69,12.62">LSTM in VQA-Med, is it really needed? JCE study on the ImageCLEF 2019 dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3DCE6E74ABDEEC0696B09C41447D80E4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>VQA-Med</term>
					<term>LSTM</term>
					<term>ImageCLEF-2019</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the contribution of the Department of Software Engineering at the Azrieli College of Engineering, Jerusalem, Israel to the ImageCLEF VQA-Med 2019 task. This task was inspired by the recent ever greater success of visual question answering (VQA) in the general domain. Given medical images accompanied by clinically relevant questions, participating systems were tasked with answering questions based on the image content.We explored and implemented a two-stage model. The first stage predicts the category of the textual question, while the second stage is comprised of 5 sub-models. Each sub-model is a classic VQA deep learning module with two branches for feature extraction, the first using CNN to extract image features, and the second using embedding (and optionally LSTM) to extract textual features. The network then combines the two feature branches to predict the appropriate answer. We found that most sub-models didnt need LSTM to achieve high scores on the validation and test data-sets. We submitted 10 models for the challenge, our best submission overall ranked 9th out of 17.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ever increasing demand for automated computer systems (AI) to assist clinical medical practice addresses two main audiences: Doctors who use these systems to get a second opinion on their diagnosis; and patients who increasingly have easy access to comprehensive and detailed medical data which they find bewildering. Thus, addressing patients, the systems motivation is to help them have a better understanding of their medical condition, by providing detailed explanations of the results of their medical tests and scans, which is something that doctors, naturally, are unable to do for each data item of each patients file. The current access to ones detailed medical file without explanation leads to the unfortunate situation that patients turn to searching the Internet and online forums to better understand their condition, reaching misleading information and false conclusions. Consequently, this often worries patients, either because insufficiently specific details of their health are considered, or even worse, because irrelevant, false, inexpert information is found.</p><p>Visual question answering (VQA) <ref type="bibr" coords="2,298.08,160.97,10.52,8.74" target="#b0">[1]</ref> is a subfield of automated systems (AI) relevant to these kinds of problems. The task of VQA is to produce textual answers to textual questions asked in the context of a specific image. This is illustrated in Fig 1 : given an image and a question, a VQA system should supply an answer relevant to the question in the context of the image. A VQA <ref type="bibr" coords="2,185.03,452.88,10.52,8.74" target="#b1">[2]</ref> system questions takes textual questions as input with the images they refer to, and combines data from the image and the question text to arrive at the most relevant answer. To produce answers to specific questions, VQA systems combine natural language processing methods with advanced computer vision techniques. The application of VQA to the field of medicine is a twofold challenge, not only are medical texts and images significantly different from those in the general computer vision field, but the resources and labelled data available in the medical field are quite limited relative to what is available in the general field. Evidenced by the 260,000 image COCO-QA Challenge dataset of general images, this quantity contrasted with the 5,000 VQA-Med medical image dataset. Following the recent successes of VQA in the general computer vision field and the challenge posed by the medical field, as of 2018, ImageClef 2019 <ref type="bibr" coords="2,470.08,584.39,10.52,8.74" target="#b2">[3]</ref> published a second round of the VQA-Med Challenge <ref type="bibr" coords="2,369.79,596.34,9.96,8.74" target="#b3">[4]</ref>. This paper deals with the problems of VQA in the medical field. The rest of this paper is organized as follows. First, we describe some related work. Next, we describe the database and challenge characteristics. Then, we describe our method in detail. Lastly, results are presented in Section 4, followed by conclusions and future work in the last Section.</p><p>The VQA COCO-QA Challenge is studying a problem very similar to the VQA-Med task. VQA <ref type="bibr" coords="3,208.44,155.18,10.52,8.74" target="#b0">[1]</ref> has been held every year since 2016. The dataset is public domain based. The prevalent approach to VQA uses recurrent neural networks, such as LSTMs <ref type="bibr" coords="3,205.65,179.09,9.96,8.74" target="#b4">[5]</ref>, to encode the textual questions, and deep convolution networks, such as VGG-16, to encode and extract features from the images <ref type="bibr" coords="3,467.30,191.05,9.96,8.74" target="#b5">[6]</ref>. Based on these ideas, a plethora of other methods have been proposed in the literature: including attention, dynamic models, and even incorporating external databases.</p><p>In this study, we took a different approach: our objective was to use classic VQA methods <ref type="bibr" coords="3,201.45,250.89,9.96,8.74" target="#b6">[7]</ref>. We analyzed those methods in order to determine their advantages and limitations with respect to the necessity of the LSTM layer and other parameters. We utilized conventional VQA approaches, optimizing their parameters, to find the best prediction method and its corresponding imaging and text features, which provided the best evidence as to whether or not the LSTM layer is necessary to achieve a high score or not. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description and Dataset</head><p>The Challenge dataset comprised of a training dataset of 3,200 medical images and 12,792 Question and Answer (QA) pairs, a validation dataset of 500 medical images and 2,000 QA pairs, and a test dataset of 500 medical images and 500 questions with answers withheld. The questions are divided into 4 categories: Modality, Plane, Organ System and Abnormality.</p><p>The evaluation of the participant systems of the VQA-Med 2019 task was conducted based on two metrics: BLEU, and Accuracy (Strict). Accuracy (Strict) is an adapted version of the accuracy metric from the general domain VQA task that considers exact matching of a participant provided answer and the ground truth answer. BLEU <ref type="bibr" coords="3,231.29,644.16,15.50,8.74" target="#b9">[10]</ref> is used to capture the similarity between a system generated answer and the ground truth answer. Each answer is converted to lower-case, all punctuation was removed, and the answer was tokenized to individual words. Stopwords were removed using NLTKs9 English stopword list. Snowball stemming10 was applied to increase the coverage of overlaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>The input to our method is an image and a questions referring to it. The output is an answer for the question in the context of the given image. Questions. This stage uses embedding and an MLPClassifier, and it optimizes the log-loss function using LBFGS or stochastic gradient descent. We used the sklearn package <ref type="bibr" coords="4,205.97,337.41,10.52,8.74" target="#b7">[8]</ref> for this, with its default parameters. The second stage is a classic VQA deep learning module, which takes the question and image as a combined input and predicts the appropriate answer. The text undergoes preprocessing and embedding the output of this branch is treated as features. We investigated whether LSTM was needed at the next stage or not. Image features are extracted using a CNN network (VGG 19) <ref type="bibr" coords="4,467.31,644.16,9.96,8.74" target="#b8">[9]</ref>. Next, the features from both branches are merged using fully-connected layers We will start with a short comparison of our results with those of other groups, then we will focus in on our own submissions. When participating groups were compared using only the best performing submission from each group we placed 9th out of 17 groups, achieving a strict accuracy score of 0.53 compared to 0.62 achieved by the best performing groups submission. Looking at the number of submissions we submitted 10, while the average number of submissions per group was 4.7 (80 submissions by 17 groups). However, in term of average performance across all submissions we placed 8th, trading places with LIST due to the small variation in performance between our submissions, which were all between places 25 and 44 (of 80 total submissions), while LIST groups submissions were between places 24 and 57.</p><p>We will review and analyze the submissions in order of the scores they achieved on the test set, looking particularly at the following features of the submitted models: 1) Optimization Function, 2) Activation Function, 3) Loss Function, 4) batch size, 5) size of fully-connected layers, 6) number of units in the LSTM layer, and 7) whether Class Weights were used.</p><p>Since this paper is focused on finding whether LSTM is needed for VQA tasks, and we wanted the effect and contribution of LSTM to be highlighted, we chose to work with a very simple convolution network, and used the VGG network. when evaluating optimization functions we found that RMSprop produced the best results across all the submitted models. Results clearly indicated using Softmax for Optimization alongside Categorical Crossentropy as the Loss Function was the best option, which was expected as they are the most natural choice for a task like this <ref type="bibr" coords="5,243.90,564.34,14.61,8.74" target="#b9">[10]</ref>. These were the parameters used in the three highest performing submissions (by test set scores). Batch size was 32 for all question categories, except the Abnormal -Yes/No category which required a batch size of 75, submissions with lower batch sizes produced less accurate results.</p><p>Lets turn to the last three parameters:</p><p>• Size of fully-connected layers • Number of units in the LSTM layer • Whether Class Weights were used</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Submissions Details</head><p>We will review our ten submissions in order of their Challenge test set results. Note that each submission is comprised of five sub-models -one per question category.</p><p>In the tables presented per model, each row represents a sub-model (for a question category), the columns are:</p><p>-Column 1 -question category that sub-model was trained for -Column 2-3 -sub-models validation set scores (strict-accuracy and BLEU) -Column 4 -size of fully-connected layers -Column 5 -number of units in the LSTM layer (LM in short) -6 -Loss Function used -Column 7 -Activation Function used (act, in short) -Column 8 -batch size -Column 9 -number of epochs (epo, in short) -Column 10 -whether Class Weights were used Best Performing Submission The submission with the highest test set score had the following characteristics: Fully-connected layer size of 14 for all submodels This is the highest number used among our submissions, and our findings indicate that a higher number of fully-connected layers was more successful in generalizing from the validation set. An LSTM layer was used only in the submodel handling the Abnormality -Other question category. In the training and validation datasets Yes and No answer frequencies were not balanced for the Abnormality Yes/No category. We therefore investigated whether class weights would improve accuracy and found that they did. See test set results in Table <ref type="table" coords="6,475.61,427.21,4.98,8.74" target="#tab_2">4</ref> and validation results in Table <ref type="table" coords="6,271.05,439.16,4.98,8.74" target="#tab_0">1</ref>  2nd and 3rd best performing submissions The differences between these models and the best submission were not great, nor were the differences in the scores they both achieved 0.53 strict and 0.55 BLEU. Compared to the best performing submission, these two had fewer epochs and smaller fully-connected layer sizes. See test set results in Table <ref type="table" coords="7,309.87,130.95,4.98,8.74" target="#tab_2">4</ref> and validation results in Table <ref type="table" coords="7,455.98,130.95,4.98,8.74">2</ref> and Table <ref type="table" coords="7,162.16,142.90,4.98,8.74" target="#tab_1">3</ref> Table </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper presents research done in the context of participation in the VQA-Med Challenge. We analyzed VQA classifiers and feature extraction methods for image and text classification in the context of medical images in the VQA-Med 2019 task. We found that none of the sub-models needed LSTM, except the one handling the Abnormality Other questions category, the most complex task, which also required fully-connected layers of size 21, unlike all the other categories, for which fully-connected layers of size 14 were sufficient. Class weights are needed only in cases were a significant imbalance between answer class frequency exists as there was in this challenge in the Abnormality Yes/No question category. We submitted 10 models, our best submission ranking 9th out of 17. All source codes are available at https://github.com/turner11/VQAMED,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Work</head><p>This paper focused on the question of whether and when LSTM may be useful for VQA tasks. We therefore chose to work with a very simple convolution network, the VGG network. Further research on the effect and contribution of the LSTM module is needed in order to look at a broader range of convolution networks, including more advanced versions, such as ResNet and Inception, and their effect on results. We intend to investigate the effects of using larger size fully-connected layers and more epochs. Looking at batch size, found that our best performing submissions had a batch size of 32, with 75 for the Abnormality Yes/No submodel, while all lower batch sizes produced less accurate results. The batch size was limited by computing resources, and we intend to examine larger batch sizes with stronger processors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,395.91,345.83,7.89;2,134.77,406.89,179.77,7.86;2,147.83,252.44,319.69,128.70"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Given an image and a question, a VQA system should supply an answer in the context of the question and the given image.</figDesc><graphic coords="2,147.83,252.44,319.69,128.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,447.21,345.83,7.89;3,134.77,458.19,140.59,7.86;3,134.77,342.21,339.98,90.23"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. VQA-Med, texts as well as images pertaining to the medical field are significantly different and more complex.</figDesc><graphic coords="3,134.77,342.21,339.98,90.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,432.01,229.82,48.57,8.74;4,134.77,241.77,345.83,8.74;4,134.77,253.73,345.82,8.74;4,134.77,265.68,345.82,8.74;4,134.77,277.64,345.82,8.74;4,134.77,289.59,345.82,8.74;4,134.77,301.55,345.83,8.74"><head></head><label></label><figDesc>Fig 2. The system is comprised of two stages: The first predicts the category of the textual question Fig 3, while the second is a classic VQA module which combines the question and image to predict a relevant answer (see Fig 4 below). The first stage classifies the question into 5 question categories: Modality, Plane, Organ System and 2 Abnormality categories: Note, we subdivide the tasks given Abnormality class into two categories: Questions with a Yes or No answer, and All Other</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,134.77,537.36,345.82,7.89;4,134.77,548.34,345.82,7.86;4,134.77,559.30,97.32,7.86;4,141.35,373.22,332.67,149.37"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The first stage predicts the category of the textual question. Classifying into the following 5 question categories: Modality, Plane, Organ System, Abnormality, Yes/No and Abnormality, Other</figDesc><graphic coords="4,141.35,373.22,332.67,149.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,134.77,220.61,345.83,7.89;5,134.77,231.59,211.65,7.86;5,139.60,115.84,336.15,90.00"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. A classic VQA deep learning module, which takes the question and image as a combined input and predicts the appropriate answer</figDesc><graphic coords="5,139.60,115.84,336.15,90.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.77,468.48,345.83,116.73"><head>Table 1 .</head><label>1</label><figDesc>Best performing submission. cross. stands for categorical crossentropy, Abnorm for Abnormality, LM for LSTM, epo for epochs, act for activation</figDesc><table coords="6,186.94,500.24,239.80,84.97"><row><cell cols="2">category acc bleu FC LM loss</cell><cell>act.</cell><cell>batch size</cell><cell>epo</cell><cell>class weight</cell></row><row><cell cols="4">Organ 0.7 0.70 14 0 cross softmax 32</cell><cell cols="2">7 NO</cell></row><row><cell cols="4">Plane 0.74 0.74 14 0 cross softmax 32</cell><cell cols="2">7 NO</cell></row><row><cell cols="6">Modality 0.82 0.82 14 0 cross softmax 32 10 NO</cell></row><row><cell cols="4">Abnorm. 0.724 0.76 21 128 cross softmax 32</cell><cell cols="2">3 NO</cell></row><row><cell>Abnorm. yes no</cell><cell cols="3">0.02 0.05 14 0 cross softmax 75</cell><cell>2</cell><cell>yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,172.29,345.83,280.50"><head>Table 3 .</head><label>3</label><figDesc>3rd performing submission. cross. stand for categorical crossentropy, Abnorm for Abnormality, LM for LSTM, epo for epochs, act for activation</figDesc><table coords="7,134.77,172.29,345.83,280.50"><row><cell cols="5">2. 2nd performing submission. cross. stand for categorical crossentropy, Abnorm</cell></row><row><cell cols="5">for Abnormality, LM for LSTM, epo for epochs, act for activation</cell></row><row><cell cols="3">Category Accuracy BLEU FC LM Loss act.</cell><cell>batch size</cell><cell>epo</cell><cell>class weight</cell></row><row><cell>Organ</cell><cell>0.66</cell><cell cols="2">0.68 14 0 cross softmax 32</cell><cell>7 NO</cell></row><row><cell>Plane</cell><cell>0.74</cell><cell cols="2">0.74 8 0 cross softmax 32</cell><cell>5 NO</cell></row><row><cell cols="2">Modality 0.72</cell><cell cols="3">0.75 14 0 cross softmax 32 10 NO</cell></row><row><cell>Abnorm.</cell><cell>0.02</cell><cell cols="2">0.04 21 128 cross softmax 32</cell><cell>3 NO</cell></row><row><cell>Abnorm. yes no</cell><cell>0.78</cell><cell cols="2">0.78 14 0 cross softmax 75</cell><cell>2 YES</cell></row><row><cell cols="3">Category Accuracy BLEU FC LM Loss act.</cell><cell>batch size</cell><cell>epo</cell><cell>class weight</cell></row><row><cell>Organ</cell><cell>0.63</cell><cell cols="2">0.65 14 0 cross softmax 32</cell><cell>9 NO</cell></row><row><cell>Plane</cell><cell>0.72</cell><cell cols="2">0.72 14 0 cross softmax 32</cell><cell>7 NO</cell></row><row><cell cols="2">Modality 0.72</cell><cell cols="2">0.75 14 0 cross softmax 32</cell><cell>7 NO</cell></row><row><cell>Abnorm.</cell><cell>0.02</cell><cell cols="2">0.02 21 128 cross softmax 32</cell><cell>3 NO</cell></row><row><cell>Abnorm. yes no</cell><cell>0.78</cell><cell cols="2">0.78 14 0 cross softmax 75</cell><cell>2 YES</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,216.44,499.82,182.47,138.65"><head>Table 4 .</head><label>4</label><figDesc>Test accuracy of all 10 summations These submissions either did not use Class Weights at all or did not use them exclusively for the Abnormality -Yes/No category, and the size of fully-connected layers was smaller, emphasizing the importance of these elements to the network. See test set results in</figDesc><table coords="7,242.14,520.62,131.08,117.85"><row><cell cols="3">submission rank accuracy BLEU</cell></row><row><cell>1</cell><cell>0.54</cell><cell>0.55</cell></row><row><cell>2</cell><cell>0.53</cell><cell>0.55</cell></row><row><cell>3</cell><cell>0.52</cell><cell>0.57</cell></row><row><cell>4</cell><cell>0.53</cell><cell>0.558</cell></row><row><cell>5</cell><cell>0.52</cell><cell>0.55</cell></row><row><cell>6</cell><cell>0.52</cell><cell>0.55</cell></row><row><cell>7</cell><cell>0.52</cell><cell>0.57</cell></row><row><cell>8</cell><cell>0.50</cell><cell>0.54</cell></row><row><cell>9</cell><cell>0.50</cell><cell>0.51</cell></row><row><cell>10</cell><cell>0.50</cell><cell>0.56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.77,154.86,345.83,63.69"><head>Table 4 ,</head><label>4</label><figDesc>Submissions performing 7th and 10th These submissions did not include LSTM, their low scores proving the importance of this layer for handling complex tasks such as the Abnormality -Other question category. See test set results in</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,134.77,221.77,35.15,8.74"><head>Table 4 ,</head><label>4</label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>All source codes are available at https://github.com/turner11/</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,634.88,337.64,7.86;8,151.52,645.84,329.07,7.86;8,151.52,656.80,35.84,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,309.71,634.88,170.88,7.86;8,151.52,645.84,115.27,7.86">Visual question answering: Datasets, algorithms, and future challenges</title>
		<author>
			<persName coords=""><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,274.34,645.84,173.48,7.86">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,119.67,337.63,7.86;9,151.52,130.63,329.07,7.86;9,151.52,141.59,329.07,7.86;9,151.52,152.55,20.99,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,305.35,130.63,123.62,7.86">Vqa: Visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,447.41,130.63,33.18,7.86;9,151.52,141.59,251.71,7.86">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,163.51,337.63,7.86;9,151.52,174.47,329.07,7.86;9,151.52,185.43,329.07,7.86;9,151.52,196.39,329.07,7.86;9,151.52,207.34,123.48,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,221.67,185.43,37.88,7.86;9,287.30,185.43,193.29,7.86;9,151.52,196.39,120.33,7.86">Multimedia retrieval in lifelogging, medical, nature, and security applications</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,290.61,196.39,186.15,7.86">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
	<note>Imageclef</note>
</biblStruct>

<biblStruct coords="9,142.96,218.30,337.64,7.86;9,151.52,229.26,329.07,7.86;9,151.52,240.22,329.07,7.86;9,151.52,251.18,298.37,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,277.47,229.26,203.12,7.86;9,151.52,240.22,129.30,7.86">VQA-Med: Overview of the medical visual question answering task at imageclef 2019</title>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,299.62,240.22,180.98,7.86;9,151.52,251.18,46.41,7.86">CLEF2019 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,262.14,337.64,7.86;9,151.52,273.10,125.83,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,325.43,262.14,97.75,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,431.76,262.14,48.84,7.86;9,151.52,273.10,31.76,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,284.06,337.63,7.86;9,151.52,295.02,329.07,7.86;9,151.52,305.98,144.86,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,309.17,284.06,171.42,7.86;9,151.52,295.02,38.08,7.86">Answer-type prediction for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,206.76,295.02,273.83,7.86;9,151.52,305.98,44.51,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4976" to="4984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,316.93,337.64,7.86;9,151.52,327.89,329.07,7.86;9,151.52,338.85,218.72,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,314.68,327.89,124.77,7.86">Vqa: Visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,447.52,327.89,33.07,7.86;9,151.52,338.85,138.34,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="31" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,349.81,337.63,7.86;9,151.52,360.77,329.07,7.86;9,151.52,371.73,329.07,7.86;9,151.52,382.69,211.84,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,259.44,371.73,166.09,7.86">Scikit-learn: Machine learning in python</title>
		<author>
			<persName coords=""><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,437.96,371.73,42.63,7.86;9,151.52,382.69,103.02,7.86">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,393.65,337.63,7.86;9,151.52,404.61,275.91,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,327.39,393.65,153.20,7.86;9,151.52,404.61,114.45,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.61,415.56,337.97,7.86;9,151.52,426.52,329.07,7.86;9,151.52,437.48,20.99,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,473.68,415.56,6.91,7.86;9,151.52,426.52,144.85,7.86">A tutorial on the cross-entropy method</title>
		<author>
			<persName coords=""><forename type="first">Pieter-Tjerk De</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dirk</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reuven</forename><forename type="middle">Y</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,304.51,426.52,115.79,7.86">Annals of operations research</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="67" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
