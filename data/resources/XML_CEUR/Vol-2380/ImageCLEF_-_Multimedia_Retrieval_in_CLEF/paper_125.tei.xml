<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.67,115.96,306.01,12.62;1,186.38,133.89,242.59,12.62">JUST at ImageCLEF 2019 Visual Question Answering in the Medical Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,134.77,171.56,60.28,8.74"><forename type="first">Aisha</forename><surname>Al-Sadi</surname></persName>
							<email>asalsadi16@cit.just.edu.jo</email>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.50,171.56,66.26,8.74"><forename type="first">Bashar</forename><surname>Talafha</surname></persName>
							<email>talafha@live.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,282.22,171.56,94.54,8.74"><forename type="first">Mahmoud</forename><surname>Al-Ayyoub</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.22,171.56,66.87,8.74"><forename type="first">Yaser</forename><surname>Jararweh</surname></persName>
							<email>yijararweh@just.edu.jo</email>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.46,183.51,59.97,8.74"><forename type="first">Fumie</forename><surname>Costen</surname></persName>
							<email>fumie.costen@manchester.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.67,115.96,306.01,12.62;1,186.38,133.89,242.59,12.62">JUST at ImageCLEF 2019 Visual Question Answering in the Medical Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F1FF7463222000E640CF857E51865D82</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF 2019</term>
					<term>Visual Question Answering</term>
					<term>Medical Image Interpretation</term>
					<term>Medical Questions and Answers</term>
					<term>VGG Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our method for the Medical Domain Visual Question Answering (VQA-Med) Task of ImageCLEF 2019. The aim is to build a model that is able to answer questions about medical images. Our proposed model consists of sub-models, each specializing in answering a specific type of questions. Specifically, the sub-models we have are: "plane" model, "organ systems" model, "modality" models, and "abnormality" models. All of these models are basically image classification models based on pre-trained VGG16 network. We do not rely on the questions for the answers prediction since the questions on each type are repetitive. However, we do rely on them to determine the suitable model to be used for producing the answers and determine the suitable answer format. Our best model achieves 57% accuracy and 0.591 BLEU score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the advances in the computer vision (CV) and natural language processing (NLP) fields, new challenging tasks emerge and one of them is Visual Question Answering (VQA), which grabbed the attention of both research communities. VQA is basically about answering a specific question about a given image. Thus, there is a need to combine CV techniques that provide an understanding of the image's content with NLP techniques that provide an understanding of the question and the ability to produce the answer. Obviously, the difficulty level of the problem depends on the expected answer types, whether they are yes/no questions, multiple choice questions or open-ended questions.</p><p>Recently, VQA has been applied to different specific domains such as the medical domain. Medical VQA poses its own set of issues/challenges that are different from the ones faced in general domain VQA. Some of these challenges are related to the processing of medical images and the difficulties in handling all kinds of images for different body parts and extracting regions of interest that vary greatly for the different medical cases and ailments. The other set of challenges are related to the understanding of the questions and the ability to process very technical medical terms as well as non-medical terms used by common users. The resources required to address all of these challenges are massive and there are restrictions related to using them and integrating them into a single model. Thus, the Medical VQA is still at very early stages, but it is expected to improve over time <ref type="bibr" coords="2,280.44,250.50,9.96,8.74" target="#b3">[4]</ref>.</p><p>This paper presents our participation in VQA-Med 2019 task <ref type="bibr" coords="2,427.26,262.46,9.96,8.74" target="#b2">[3]</ref>, which is organized by ImageCLEF 2019 <ref type="bibr" coords="2,274.28,274.41,9.96,8.74" target="#b5">[6]</ref>. This is the second installment of this task<ref type="foot" coords="2,476.12,272.84,3.97,6.12" target="#foot_0">3</ref> with the aim of answering questions about medical images. For that, we create sub-models, where each sub-model is specialized for answering a specific type of questions. All our models use the pre-trained convolutional neural networks (CNN), VGG16 <ref type="bibr" coords="2,206.86,322.23,9.96,8.74" target="#b8">[9]</ref>, for visual features extractions.</p><p>The rest of paper is organized as follows. Section 2 presents the most relevant work, which includes the participants' models in the VQA-Med 2018 challenge <ref type="bibr" coords="2,134.77,358.10,9.96,8.74" target="#b3">[4]</ref>. Section 3 presents detailed analysis of the dataset, which we find useful in building our models. In Section 4, we present our proposed models for answering questions of each type. Validation results for all models and final test results are presented in Section 5. Finally, the paper is concluded in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>The general VQA challenge <ref type="foot" coords="2,259.01,446.18,3.97,6.12" target="#foot_1">4</ref> which is held every year starting from 2016, is based on a large dataset of real-world images with different question types such as yes/no questions, number questions, and other questions. Different approaches were applied for the task and most solutions rely on deep learning techniques that combine the use of word embedding with different recurrent neural networks (RNN) for text embedding and features extraction, and CNN for visual features extraction supplemented with advanced techniques such as attention mechanisms.</p><p>For the medical domain, the task is different as the nature of medical images requires knowledge in the medical domain in order to understand them. So, a special challenge is organized for it. The first version of this competition is VQA-Med 2018 <ref type="bibr" coords="2,215.55,579.26,9.96,8.74" target="#b3">[4]</ref>. The dataset used in the 2018 version is different from the one used in the 2019 version. The 2018 version consists of 2,866 medical images and 6,413 questions answers pairs divided into training, validation, and testing sets. Two medical experts manually checked the automatically generated questions and answers for each image. The questions types are mixed between asking about a "region" within the image, asking about what the image shows, yes/no questions, and other question types including asking about abnormalities shown in the image and image kind, etc. Five teams submitted their work, most of their approaches use deep learning techniques. They use pre-trained CNN models to extract image features such as VGG16 and ResNet <ref type="bibr" coords="3,392.88,166.81,9.96,8.74" target="#b4">[5]</ref>. There are many approaches based on the encoder-decoder architecture with different components such as Long Short-Term Memory (LSTM) or Bidirectional LSTM (Bi-LSTM), with or without attention. In addition, there are some teams that used advanced techniques in the task such as the stacked attention networks and multimodal compact bilinear (MCB) pooling.</p><p>JUST team <ref type="bibr" coords="3,204.90,244.36,15.50,8.74" target="#b9">[10]</ref> used VGG16 for image features extraction. They used an LSTM-based encoder-decoder model where they feed the question to the encoder and then concatenate the hidden state of the encoder with the image features to feed them to the decoder as the initial hidden states. <ref type="foot" coords="3,377.94,278.65,3.97,6.12" target="#foot_2">5</ref>The FSST team <ref type="bibr" coords="3,224.45,297.99,10.52,8.74" target="#b1">[2]</ref> dealt with the task as a multi-label classification problem. They extracted image features using VGG16 and word embedding of the question and feed it to a Bi-LSTM network to extract question features. Then concatenated question features and image features and fed them to a decision tree classifier.</p><p>TU team <ref type="bibr" coords="3,195.03,363.57,15.50,8.74" target="#b10">[11]</ref> provided two models. In the first model, which is basically the same architecture of <ref type="bibr" coords="3,249.03,375.53,14.61,8.74" target="#b9">[10]</ref>, they used the pre-trained Inception-ResNet-v2 model to extract image features and Bi-LSTM instead of LSTM as <ref type="bibr" coords="3,426.68,387.48,14.61,8.74" target="#b9">[10]</ref>. In their second model, they computed the attention between the image features and the question features and concatenated it with the question features before feeding it to a Softmax layer for prediction.</p><p>NLM team <ref type="bibr" coords="3,199.13,441.12,10.52,8.74" target="#b0">[1]</ref> also created two models. For the first model, they used Stacked Attention Network (SAN) with VGG16 for image features and LSTM for question features. As for the second model, they used Multimodal Compact Bilinear pooling (MCB) with ResNet-50 and ResNet-152 for image features and 2-layer LSTM question features. In SAN model, they compute the attention over the image, then combine the image features and question features for the second attention layer, then pass it to a Softmax layer as a classification problem. For MCB model, they fine-tuned ResNet-50 and ResNet-152 on external medical images, then they combined the image features and question features to create a multimodal representation to predict the answer.</p><p>UMass team <ref type="bibr" coords="3,206.11,566.48,10.52,8.74" target="#b7">[8]</ref> used ResNet-152 in extract image features, and a pre-trained word embedding on Wikipedia pages, PubMed articles and Pittsburgh clinical notes for text features. They created multiple attention maps using co-attention mechanism between image features and text features. Then, they generated answers using a sampling method as a classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Description</head><p>The dataset used in VQA-Med 2019 consists of 3,200 medical images with 12,792 Question-Answer (QA) pairs as training data, 500 medical images with 2,000 QA pairs as validation data, and 500 medical images with 500 questions as test data. The data is equally distributed over four categories based on the question types which are: plane category, organ category, modality category, and abnormality category.</p><p>We can determine the question category from the question words, i.e., if the word 'plane' appears in the question, then this is a plane question. While if the words 'organ' or 'part' appear in the question, then this is an organ question. If the words 'normal', 'abnormal', 'alarm' or 'wrong' appear in the question, then this is an abnormality question. Otherwise, this is a modality question. This is useful for test data questions since the category of the question is not given like in the training and validation questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Plane Category Data</head><p>Question on planes come in one of the following formats: "in which plane", "Which plane", "what plane", "in what plane", "what is the plane", "what imaging plane is", and "what image plane". There are 16 planes. Figure <ref type="figure" coords="4,447.42,352.33,4.98,8.74" target="#fig_0">1</ref> shows main planes and their distributions in training and validation data. As evidence in this figure, the data is unbalanced, with some planes being more frequent than the others. In fact, this imbalance is noticeable across all categories data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Organ Systems Category Data</head><p>Question on organ systems come in one of the following formats: "what organ system is", "what part of the body is", "the ct/mri/ultrasound/x-ray scan shows what organ system", "which organ system is", "what organ system is", "what organ is this", etc. There are ten organ systems. Figure <ref type="figure" coords="5,371.76,130.95,4.98,8.74" target="#fig_1">2</ref> shows all organ systems and their distribution in training and validation data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modality Category Data</head><p>There are eight main modality categories: XR, CT, MR, US, MA, GI, AG, and PT. Under each of these categories, there is a number of subcategories. Each of XR and MA has one subcategory, while each of US, AG, and PT has two subcategories, GI has four subcategories, CT has seven subcategories, and finally MR has 17 subcategories.</p><p>The questions of modality part are more diverse, we can classify them into four types:</p><p>-Type 1: Questions whose answer is one of the main modality categories and its subcategory. Examples include "what modality was used to take this image", "how was this image taken", "what kind of image is this", etc. -Type 2: Yes/no questions. Examples include "is this an mri image", "was gi contrast given to the patient", etc. -Type 3: Questions whose answer is one of the choices explicitly mentioned in the question itself. Examples include "is this a contrast or noncontrast ct", "is this a t1 weighted, t2 weighted, or flair image", etc. -Type 4: Questions whose answer is one two or three choices that are not explicitly mentioned in the question. Examples include "what type of contrast did this patient have", "what is the mr weighting in this image", etc.</p><p>Table <ref type="table" coords="5,162.56,632.21,4.98,8.74" target="#tab_0">1</ref> shows modality questions types distribution in training and validation data. Figure <ref type="figure" coords="5,192.22,644.16,4.98,8.74" target="#fig_2">3</ref> shows the distribution of images of each main category from all questions types. Note that we are unable to determine the modality in some cases, such as "is this an mri image" with "no" as the answer. With the variations in modality questions types and large number of subcategories for some categories, we prepare different data formats in order to focus on specific aim in each model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Abnormality Category Data</head><p>Questions of this category come in one of the following formats.</p><p>-Type 1: Questions asking about abnormality in the image. For example, "what is the abnormality/wrong/alarming in this image". This type represents 97% of abnormality training questions and 95% of abnormality validation questions. -Type 2: Questions with yes/no answers such as "is this image normal" Or "is this image abnormal". This types represents 3% of abnormality training questions and 5% of abnormality validation questions.</p><p>For Type 1 questions, there are 1,461 different abnormalities in the 3,082 training images, and 407 different abnormalities in the 477 validation images.</p><p>It is worth mentioning that the dataset has wrong answers for some images that might affect the model's accuracy. This is expected since the data was generated automatically. Even for non-medical people like ourselves, we are able to detect some errors, but it needs an expert to determine all wrong answers and correct them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>Since we have different categories of questions, we create a special model for each category. Then, we combine them all in one model to be used for predicting answers. In order to use them correctly to answer a given question with a given image, we need to detect the suitable model to answer the question on the image and the question words. The following subsections describe the models we build for each subcategory before describing how to combine them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Plane Model</head><p>The questions format on this category are repetitive and all questions have the same meaning even if they use different words. So, it is expected that the questions would not contribute anything in answer predictions and only the image can determine the plane answer. Hence, we deal with this part as an image classification task. We use the pre-trained model VGG16 with the last layer (the Softmax layer) removed and all layers (except the last four) frozen. The output from this part is fed into two fully-connected layers with 1024 hidden nodes followed by a Softmax layer with 16 plane classes. Figure <ref type="figure" coords="7,374.46,445.74,4.98,8.74" target="#fig_3">4</ref> shows the plane model architecture in details. Since the data is unbalanced, we use class weights in order to give the classes with smaller numbers of images higher weights. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Organ Model</head><p>The questions formats here are also repetitive and have the same meaning. So, we rely on the images only to get the organ system answer, i.e., as an image classification task. We use the same model architecture for plane model except that the last layer, which is the Softmax layer, has the ten organ systems classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Modality Models</head><p>As mentioned in the modality data description in the dataset section, this category has different variations in question types and different main categories and subcategories. For this part, we create many models capable of answering every question type more accurately compared with what a general model can achieve. Firstly, we explain the models we create, and, later, we explain how to combine them.</p><p>- We did not create special models for the PT and AG categories as the data for building them are insufficient. The available data for the AG category is 81 training images and 18 validation images. Moreover, 96% of the training images belong to only one class, and all the validation images are only for that class as well. The same applies for the PT category. The available data consists of 21 training images and a single validation image. About 85% of the training images belong to only one class, and the validation image for that class is zero. So, if the predicted main modality category is AG or PT the subcategory answer will be the dominant class directly which are AN-Angiogram for AG and NM-Nuclear Medicine for PT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Abnormality Models</head><p>For abnormality Type 2 questions, which ask if the image normal or abnormal, we create a special image classification model for that purpose with the same architecture of the plane model except that that the Softmax layer predicts normal/abnormal labels. While for Type 1 questions, which ask about the abnormality in the image, we experiment with different models since the task is quite challenging due to the given data being too small for the very large number of different abnormalities answers. The following are the main four methods with which we experiment.</p><p>-Method 1: We use an encoder-decoder architecture that takes an image as input and produces an answer as the output. The questions have the same meaning despite having different formats, hence, they are expected to not play an important role in producing the answers. We feed the image into an LSTM and use the hidden states of that LSTM as initial states of another LSTM for the answer sequence. We then add the encoder output and the decoder output, and pass the results into a dense layer followed by a Softmax layer. -Method 2: In this method, we treat the problem as an image classification task using the same architecture of our previous models except that the Softmax layer has all unique abnormalities in the training data, which are 1,461 different abnormalities. -Method 3: Firstly, we predict plane and organ classes of the test image. We then calculate the cosine similarity between the VGG16 features of the test image and all training images that have the same plane and organ of the test image. Finally, we get the most similar image and output its abnormality as the answer. -Method 4: This is the same as Method 3, except that we take the two most similar images, and output the abnormality answer of the image which has the same abnormality question as that of the test image. If none of the two most similar image has the same question format, then we output the most similar image answer as in Method 3.</p><p>Algorithm 1 shows the steps taken to determine the required model for answer prediction. There are a lot of details that are not presented in the flowchart for simplicity, such as in modality questions Types 2-4, where different question formats are asking about specific things to determine the required model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation and Results</head><p>In this section, we report the evaluation results of each of the previous models on the validation data separately. Then, we report our official results in the VQA-Med 2019 challenge. The evaluation metrics are accuracy and BLEU score <ref type="bibr" coords="9,134.77,523.82,9.96,8.74" target="#b6">[7]</ref>. For all models, we conduct several experiments for different optimizers and learning rates and report the best results. Table <ref type="table" coords="9,349.31,535.78,4.98,8.74">2</ref> shows the evaluation results for the validation data for each model belonging to the modality category.</p><p>Note that the accuracy of M10 is misleading since it predicts the dominant class all the time. It is worth mentiong that the overall modality validation accuracy, which is 75.4%, is not the average accuracy of all models. It is the accuracy of predicting modality validation questions using these models.</p><p>For the abnormality models, the accuracy of normal/abnormal model is 77.7%. While for other abnormality questions, Table <ref type="table" coords="9,367.32,620.25,4.98,8.74" target="#tab_2">3</ref> shows the validation accuracy and BLEU score for the different four methods. So, the best abnormality validation accuracy is 17.59% resulting from using Normal/Abnormal Model and Method 2 Abnormality Model. For the whole model, here are the results. For plane questions, the validation accuracy is 76.2%, and, for organ questions, it is 74.2%. While the final modality model accuracy is 75.4% and the best abnormality model accuracy is 17.59%. So, the final validation accuracy is 60.85%.</p><p>For the VQA-Med 2019 challenge, we submit four runs of test data predictions. The four runs have the same predictions of plane, organ, and modality questions and the difference between them is only in the abnormality part. In each run, we use different method (Methods 1-4 as described in the abnormality model section). Table <ref type="table" coords="11,232.35,326.26,4.98,8.74" target="#tab_3">4</ref> shows our submissions results, Run-2 which deals with abnormality questions as an image classification has the best accuracy score and best BLEU score among our submissions. After the competition was finished and the test answers were published publicly, we compared our predicted answers with the correct answers. We found that the plane part has accuracy 72.8%, organ systems 70.4%, modality part 64%, and abnormality part 8%. Moreover, we discovered that, for the abnormality part, we submitted our predicted answers without stop words. So, some of our correct answers were considered as false ones. Correcting this part increase the accuracy of this part to 18.4%. Another technicality that caused some of the correct answers produced by our systems to be considered false is the fact that some modality questions have single correct answers in the testing set where they should have multiple correct answers accounting for the different formats in which the correct answer may appear. For example, if the actual answer is "ct w contrast iv", then, our system's predicted answer "ct with iv contrast" should be considered correct. So, by taking into consideration the two previous notes, the actual overall accuracy of our model reaches 57%.</p><p>In this paper, we described our participation in the ImageCLEF VQA-Med 2019 task. The proposed model consists of sub-models based on the pre-trained VGG16 model. Our model's overall accuracy is 57% with 0.591 BLEU score. Accuracy of the plane, organ, and modality models are good (ranging between 65% and 72%), however, the abnormality model's accuracy is rather low (18%), due to the difficulty of the task especially with the small dataset available. In the future, we plan on seeking the help of a medical expert in order to correct wrong answers and collect new data for the abnormality part.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,152.15,578.49,311.05,7.89;4,134.77,418.32,345.82,145.39"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Planes distribution in training data (left) and validation data (right).</figDesc><graphic coords="4,134.77,418.32,345.82,145.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,135.65,329.67,344.07,7.89;5,134.77,174.26,345.83,140.65"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Organ systems distribution in training data (left) and validation data (right).</figDesc><graphic coords="5,134.77,174.26,345.83,140.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,134.77,474.31,345.83,7.89;6,134.77,485.29,28.94,7.86;6,134.77,318.77,345.83,140.77"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Modality main categories distribution in training data (left) and validation data (right).</figDesc><graphic coords="6,134.77,318.77,345.83,140.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,241.68,632.07,132.00,7.89;7,134.77,501.64,345.83,115.66"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Plane model architecture</figDesc><graphic coords="7,134.77,501.64,345.83,115.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,224.12,185.06,167.11,85.45"><head>Table 1 .</head><label>1</label><figDesc>Modality questions distribution</figDesc><table coords="6,245.88,205.85,123.59,64.65"><row><cell></cell><cell cols="2">Training Validation</cell></row><row><cell cols="3">Type 1 1,380 (43%) 229 (46%)</cell></row><row><cell cols="3">Type 2 1,184 (37%) 179 (36%)</cell></row><row><cell cols="3">Type 3 445 (14%) 73 (14%)</cell></row><row><cell cols="2">Type 4 191 (6%)</cell><cell>19 (4%)</cell></row><row><cell>Total</cell><cell>3,200</cell><cell>500</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,140.99,291.44,339.60,121.48"><head></head><label></label><figDesc>M1, the general model, for classifying image modality into eight main categories (XR, CT, MR, US, MA, GI, AG, and PT). -M2 model for distinguishing MR images from CT images. -M3 model for distinguishing contrast from non-contrast CT images. -M4 model for distinguishing contrast from non-contrast MR images. -M5 model for classifying CT contrast types (GI/IV/GI and IV). -M6 model for classifying MR weighting types (T1/T2/Flair). -M7 model for classifying all CT subcategories. -M8 model for classifying all MR subcategories. -M9 model for classifying all GI subcategories. -M10 model for classifying all ultrasound subcategories.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,188.21,115.91,238.94,74.09"><head>Table 3 .</head><label>3</label><figDesc>Validation Abnormality Results</figDesc><table coords="11,188.21,136.71,238.94,53.29"><row><cell cols="2">Validation Accuracy (%) Validation BLEU Score</cell></row><row><cell>Method 1 0</cell><cell>0.046</cell></row><row><cell>Method 2 14.7</cell><cell>0.175</cell></row><row><cell>Method 3 14</cell><cell>0.193</cell></row><row><cell>Method 4 13</cell><cell>0.189</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,208.74,390.22,197.89,74.09"><head>Table 4 .</head><label>4</label><figDesc>Our Results in VQA-Med 2019 Results</figDesc><table coords="11,239.85,411.02,135.66,53.29"><row><cell cols="2">Accuracy (%) BLEU Score</cell></row><row><cell>Run 1 0.528</cell><cell>0.553</cell></row><row><cell>Run 2 0.534</cell><cell>0.591</cell></row><row><cell>Run 3 0.528</cell><cell>0.55</cell></row><row><cell>Run 4 0.528</cell><cell>0.55</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,645.84,260.15,7.86"><p>The first VQA-Med task<ref type="bibr" coords="2,246.99,645.84,9.73,7.86" target="#b3">[4]</ref> was organized by ImageCLEF 2018.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,144.73,656.80,128.38,7.86"><p>https://visualqa.org/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="3,144.73,656.80,182.64,7.86"><p>https://github.com/bashartalafha/VQA-Med</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Prediction Steps</head><p>Input: Image i and Question q if 'plane' word in q then Predict plane using Plane Model else if 'organ' or 'part' words in q then Predict organ plane using Organ Model else if 'normal', 'abnormal', 'alarm', or 'wrong' words in q then if q starts with "is this", "is there", "does this", "is the" or "are there" </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,283.52,337.64,7.86;12,151.52,294.48,329.07,7.86;12,151.52,305.44,329.07,7.86;12,151.52,316.39,154.79,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,463.44,283.52,17.15,7.86;12,151.52,294.48,272.10,7.86">Nlm at imageclef 2018 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,446.53,294.48,34.06,7.86;12,151.52,305.44,284.13,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,327.45,337.64,7.86;12,151.52,338.41,329.07,7.86;12,151.52,349.37,329.07,7.86;12,151.52,360.33,194.22,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,392.12,327.45,88.47,7.86;12,151.52,338.41,324.46,7.86">Deep neural networks and decision tree classifier for visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Allaouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Benamrou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Benamrou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,165.36,349.37,310.42,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,371.38,337.64,7.86;12,151.52,382.34,329.07,7.86;12,151.52,393.30,329.07,7.86;12,151.52,404.26,304.40,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,167.31,382.34,313.29,7.86;12,151.52,393.30,33.73,7.86">Vqa-med: Overview of the medical visual question answering task at imageclef 2019</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="12,207.20,393.30,273.39,7.86;12,151.52,404.26,14.99,7.86">CLEF 2019 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,415.31,337.63,7.86;12,151.52,426.27,329.07,7.86;12,151.52,437.23,46.58,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,431.40,415.31,49.18,7.86;12,151.52,426.27,279.75,7.86">Overview of the ImageCLEF 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,448.29,337.63,7.86;12,151.52,459.25,329.07,7.86;12,151.52,470.21,76.80,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,290.95,448.29,172.55,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,459.25,325.14,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,481.26,337.63,7.86;12,151.52,492.22,329.07,7.86;12,151.52,503.18,329.07,7.86;12,151.52,514.14,329.07,7.86;12,151.52,525.10,329.07,7.86;12,151.52,536.06,329.07,7.86;12,151.52,547.02,329.07,7.86;12,151.52,557.97,329.07,7.86;12,151.52,568.93,329.07,7.86;12,151.52,579.89,216.27,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,268.29,536.06,212.30,7.86;12,151.52,547.02,124.23,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,297.67,547.02,182.92,7.86;12,151.52,557.97,329.07,7.86;12,151.52,568.93,122.46,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="12,305.55,568.93,171.07,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="12,142.96,590.95,337.64,7.86;12,151.52,601.91,329.07,7.86;12,151.52,612.87,329.07,7.86;12,151.52,623.82,98.02,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,355.50,590.95,125.09,7.86;12,151.52,601.91,133.70,7.86">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,306.56,601.91,174.03,7.86;12,151.52,612.87,162.47,7.86">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,634.88,337.63,7.86;12,151.52,645.84,329.07,7.86;12,151.52,656.80,321.13,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,284.94,634.88,195.65,7.86;12,151.52,645.84,115.13,7.86">Umass at imageclef medical visual question answering (med-vqa) 2018 task</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,288.04,645.84,192.56,7.86;12,151.52,656.80,119.03,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,119.67,337.63,7.86;13,151.52,130.63,231.27,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="13,278.92,119.67,201.67,7.86;13,151.52,130.63,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.62,141.59,337.98,7.86;13,151.52,152.55,329.07,7.86;13,151.52,163.51,154.79,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,269.22,141.59,155.13,7.86">Just at vqa-med: A vgg-seq2seq model</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Talafha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,446.52,141.59,34.07,7.86;13,151.52,152.55,284.13,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,174.47,337.97,7.86;13,151.52,185.43,329.07,7.86;13,151.52,196.39,329.07,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,265.28,174.47,215.31,7.86;13,151.52,185.43,134.06,7.86">Employing inception-resnet-v2 and bi-lstm for medical domain visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,307.05,185.43,173.54,7.86;13,151.52,196.39,132.42,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
