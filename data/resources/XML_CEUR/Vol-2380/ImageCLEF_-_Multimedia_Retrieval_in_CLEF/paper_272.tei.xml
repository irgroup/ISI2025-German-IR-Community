<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.59,115.96,306.19,12.62;1,143.90,133.89,327.56,12.62">VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019</title>
				<funder>
					<orgName type="full">U.S. National Library of Medicine, National Institutes of Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,149.59,171.60,81.00,8.74"><forename type="first">Asma</forename><forename type="middle">Ben</forename><surname>Abacha</surname></persName>
							<email>asma.benabacha@nih.govsadid.hasan@philips.com</email>
							<affiliation key="aff0">
								<orgName type="department">Lister Hill Center</orgName>
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.15,171.60,68.14,8.74"><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Philips Research Cambridge</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.84,171.60,66.00,8.74"><forename type="first">Vivek</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Philips Research Cambridge</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,396.40,171.60,37.91,8.74"><forename type="first">Joey</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Philips Research Cambridge</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,444.87,171.60,20.89,8.74;1,217.92,183.55,77.02,8.74"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Lister Hill Center</orgName>
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.87,183.55,68.10,8.74"><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Applied Sciences Western Switzerland</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.59,115.96,306.19,12.62;1,143.90,133.89,327.56,12.62">VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1F96E4E81F59E1364E32FBDC91B72E2F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>Data Creation</term>
					<term>Deep Learning</term>
					<term>Radiology Images</term>
					<term>Medical Questions and Answers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an overview of the Medical Visual Question Answering task (VQA-Med) at ImageCLEF 2019. Participating systems were tasked with answering medical questions based on the visual content of radiology images. In this second edition of VQA-Med, we focused on four categories of clinical questions: Modality, Plane, Organ System, and Abnormality. These categories are designed with different degrees of difficulty leveraging both classification and text generation approaches. We also ensured that all questions can be answered from the image content without requiring additional medical knowledge or domain-specific inference. We created a new dataset of 4,200 radiology images and 15,292 question-answer pairs following these guidelines. The challenge was well received with 17 participating teams who applied a wide range of approaches such as transfer learning, multi-task learning, and ensemble methods. The best team achieved a BLEU score of 64.4% and an accuracy of 62.4%. In future editions, we will consider designing more goal-oriented datasets and tackling new aspects such as contextual information and domain-specific inference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction computational perspective, this Visual Question Answering (VQA) task presents an exciting problem that combines natural language processing and computer vision techniques. In recent years, substantial progress has been made on VQA with new open-domain datasets <ref type="bibr" coords="2,276.29,154.86,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="2,288.48,154.86,7.75,8.74" target="#b7">8]</ref> and approaches <ref type="bibr" coords="2,370.20,154.86,15.50,8.74" target="#b22">[23,</ref><ref type="bibr" coords="2,387.36,154.86,7.01,8.74" target="#b6">7]</ref>.</p><p>However, there are challenges that need to be addressed when tackling VQA in a specialized domain such as Medicine. Ben Abacha et al. <ref type="bibr" coords="2,403.48,178.98,10.52,8.74" target="#b3">[4]</ref> analyzed some of the issues facing medical visual question answering and described four key challenges (i) designing goal-oriented VQA systems and datasets, (ii) categorizing the clinical questions, (iii) selecting (clinically) relevant images, and (iv) capturing the context and the medical knowledge.</p><p>Inspired by the success of visual question answering in the general domain, we conducted a pilot task (VQA-Med 2018) in ImageCLEF 2018 to focus on visual question answering in the medical domain <ref type="bibr" coords="2,347.11,262.87,9.96,8.74" target="#b8">[9]</ref>. Based on the success of the initial edition, we continued the task this year with enhanced focus on a well curated and larger dataset.</p><p>In VQA-Med 2019, we selected radiology images and medical questions that (i) asked about only one element and (ii) could be answered from the image content. We targeted four main categories of questions with different difficulty levels: Modality, Plane, Organ system, and Abnormality. For instance, the first three categories can be tackled as a classification task, while the fourth category (abnormality) presents an answer generation problem. We intentionally designed the data in this manner to study the behavior and performance of different approaches on both aspects. This design is more relevant to clinical decision support than the common approach in open-domain VQA datasets <ref type="bibr" coords="2,431.74,394.59,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="2,443.92,394.59,7.75,8.74" target="#b7">8]</ref> where the answers consist of one word or number (e.g. yes, no, 3, stop).</p><p>In the following section, we present the task description with more details and examples. We describe the data creation process and the VQA-Med-2019 dataset in section 3. We present the evaluation methodology and discuss the challenge results respectively in sections 4 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>In the same way as last year, given a medical image accompanied by a clinically relevant question, participating systems in VQA-Med 2019 are tasked with answering the question based on the visual image content. In VQA-Med 2019, we specifically focused on radiology images and four main categories of questions: Modality, Plane, Organ System, and Abnormality. We mainly considered medical questions asking only about one element: e.g., "what is the organ principally shown in this MRI?", "in what plane is this mammograph taken?", "is this a t1 weighted, t2 weighted, or flair image?", "what is most alarming about this ultrasound?").</p><p>All selected questions can be answered from the image content without requiring additional domain-specific inference or context. Other questions including these aspects will be considered in future editions of the challenge, e.g.: "Is this modality safe for pregnant women?", "What is located immediately inferior to the right hemidiaphragm?", "What can be typically visualized in this plane?", "How would you measure the length of the kidneys?"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VQA-Med-2019 Dataset</head><p>We automatically constructed the training, validation, and test sets, by (i) applying several filters to select relevant images and associated annotations, and (ii) creating patterns to generate the questions and their answers. The test set was manually validated by two medical doctors. The dataset is publicly available <ref type="foot" coords="3,473.36,220.71,3.97,6.12" target="#foot_0">4</ref> . Figure <ref type="figure" coords="3,166.20,234.24,4.98,8.74">1</ref> presents examples from the VQA-Med-2019 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Medical Images</head><p>We selected relevant medical images from the MedPix<ref type="foot" coords="3,380.05,282.17,3.97,6.12" target="#foot_1">5</ref> database with filters based on their captions, modalities, planes, localities, categories, and diagnosis methods. We selected only the cases where the diagnosis was made based on the image. Examples of the selected diagnosis methods: CT/MRI Imaging, Angiography, Characteristic imaging appearance, Radiographs, Imaging features, Ultrasound, Diagnostic Radiology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Categories and Patterns</head><p>We targeted the most frequent question categories: Modality, Plane, Organ system and Abnormality (Ref:VQA-RAD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Modality: Yes/No, WH and closed questions. Examples:</head><p>was gi contrast given to the patient? what is the mr weighting in this image? what modality was used to take this image? is this a t1 weighted, t2 weighted, or flair image?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Plane: WH questions. Examples:</head><p>what is the plane of this mri? in what plane is this mammograph taken?  Patterns: For each category, we selected question patterns from hundreds of questions naturally asked and validated by medical students from the VQA-RAD dataset <ref type="bibr" coords="5,169.69,462.89,14.61,8.74" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Validation Sets</head><p>The training set includes 3,200 images and 12,792 question-answer (QA) pairs, with 3 to 4 questions per image. Table <ref type="table" coords="5,303.80,523.64,4.98,8.74" target="#tab_1">1</ref> presents the most frequent answers per category. The validation set includes 500 medical images with 2,000 QA pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Test Set</head><p>A medical doctor and a radiologist performed a manual double validation of the test answers. A total of 33 answers were updated by (i) indicating an optional part (8 answers), (ii) adding other possible answers <ref type="bibr" coords="5,363.85,608.30,16.38,8.74" target="#b9">(10)</ref>, or (iii) correcting the automatic answer. 15 answers were corrected, which corresponds to 3% of the test answers. The corrected answers correspond to the following categories: Abnormality (8/125), Organ (6/125), and Plane (1/125). For abnormality questions, the correction was mainly changing the diagnosis that is inferred, by the problem seen in the image. We expect a similar error rate in the training and validation sets that were generated using the same automatic data creation method. The test set consists of 500 medical images and 500 questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Methodology</head><p>The evaluation of the systems that participated in the VQA-Med 2019 task was conducted based on two primary metrics: Accuracy and BLEU. We use an adapted version of the accuracy metric from the general domain VQA<ref type="foot" coords="6,453.46,457.23,3.97,6.12" target="#foot_2">6</ref> task that strictly considers exact matching of a participant provided answer and the ground truth answer. We calculate the overall accuracy scores as well as the scores for each question category. To compensate for the strictness of the accuracy metric, BLEU <ref type="bibr" coords="6,232.45,506.62,15.50,8.74" target="#b14">[15]</ref> is used to capture the word overlap-based similarity between a system-generated answer and the ground truth answer. The overall methodology and resources for the BLEU metric are essentially similar to last year's task <ref type="bibr" coords="6,184.44,542.49,9.96,8.74" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>Out of 104 online registrations, 61 participants submitted signed end-user agreement forms. Finally, 17 groups submitted a total of 90 runs, indicating a notable interest in the VQA-Med 2019 task. Figure <ref type="figure" coords="6,337.12,624.62,4.98,8.74" target="#fig_1">2</ref> presents the results of the 17 participating teams. The best overall result was obtained by the Hanlin team, achieving 0.624 Accuracy and 0.644 BLEU score. Table <ref type="table" coords="7,383.25,118.99,4.98,8.74" target="#tab_4">2</ref> gives an overview of all participants and the number of submitted runs<ref type="foot" coords="7,357.95,129.37,3.97,6.12" target="#foot_3">7</ref> . The overall results of the participating systems are presented in Table <ref type="table" coords="7,330.71,142.90,4.98,8.74" target="#tab_2">3</ref> to Table <ref type="table" coords="7,378.30,142.90,4.98,8.74" target="#tab_3">4</ref> for the two metrics in a descending order of the scores (the higher the better). Detailed results of each run are described in the ImageCLEF 2019 lab overview paper <ref type="bibr" coords="7,407.70,166.81,14.61,8.74" target="#b10">[11]</ref>.  Similar to last year, participants mainly used deep learning techniques to build their VQA-Med systems. In particular, the best-performing systems leveraged deep convolutional neural networks (CNNs) like VGGNet <ref type="bibr" coords="8,354.93,502.78,14.33,7.86" target="#b17">[18]</ref> or ResNet <ref type="bibr" coords="8,419.13,502.78,14.33,7.86" target="#b9">[10]</ref> with a variety of pooling strategies e.g., global average pooling to encode image features and transformer-based architectures like BERT <ref type="bibr" coords="8,310.63,524.70,9.73,7.86" target="#b5">[6]</ref> or recurrent neural networks (RNN) to extract question features. Then, various types of attention mechanisms are used coupled with different pooling strategies such as multimodal factorized bilinear (MFB) pooling or multi-modal factorized high-order pooling (MFH) in order to combine multimodal features followed by bilinear transformations to finally predict the possible answers.</p><p>Analyses of the question category-wise<ref type="foot" coords="8,302.59,578.55,3.65,5.24" target="#foot_5">8</ref> accuracy in Table <ref type="table" coords="8,381.32,580.32,4.61,7.86" target="#tab_2">3</ref> suggest that in general, participating systems performed well to answer modality questions, followed by plane and organ questions because the possible types of answers for each of these question categories were finite. However, for the abnormality type questions, systems did not perform well in terms of accuracy because of the underlying complexity of open-ended Faculty of Sciences and Technologies, Tangier (Morocco) 7 minhvu <ref type="bibr" coords="9,164.82,226.64,12.45,6.12" target="#b20">[21]</ref> Umeå University (Sweden) &amp; University of Bern (Switzerland) 10 Team Pwc Med <ref type="bibr" coords="9,193.15,234.61,12.45,6.12" target="#b15">[16]</ref> Pricewaterhouse Coopers US Advisory (India) 5 Techno <ref type="bibr" coords="9,163.53,242.58,8.48,6.12" target="#b4">[5]</ref> Faculty of Technology Tlemcen (Algeria) 2 TUA1 <ref type="bibr" coords="9,160.27,250.55,12.45,6.12" target="#b23">[24]</ref> Tokushima University (Japan) 1 Turner.JCE <ref type="bibr" coords="9,179.83,258.52,12.45,6.12" target="#b18">[19]</ref> Azrieli College of Engineering Jerusalem (Israel) 10 UMMS <ref type="bibr" coords="9,163.46,266.49,12.45,6.12" target="#b16">[17]</ref> Worcester Polytechnic Institute &amp; University of Massachusetts Medical School (USA) 3 yan <ref type="bibr" coords="9,151.19,274.46,12.45,6.12" target="#b21">[22]</ref> Zhejiang University (China) &amp; National Institute of Informatics (Japan) 1</p><p>questions and possibly due to the strictness of the accuracy metric. To compensate for the strictness of the accuracy, we computed the BLEU scores to understand the similarity of the system generated answers and the ground-truth answers. The higher BLEU scores of the systems this year (0.631 best BLEU vs. 0.162 in 2018) further verify the effectiveness of the proposed deep learning-based models for the VQA task. Overall, the results obtained this year clearly denote the robustness of the provided dataset compared to last year's task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We presented the VQA-Med 2019 task, the new dataset, the participating systems, and official results. To ensure that the questions are naturally phrased, we used patterns from question asked by medical students to build clinically relevant questions belonging to our four target categories. We created a new dataset for the challenge<ref type="foot" coords="9,437.92,460.70,3.65,5.24" target="#foot_6">9</ref> following goal-oriented guidelines, and covering questions with varying degrees of difficulty. A wide range of approaches have been applied such as transfer learning, multi-task learning, ensemble methods, and hybrid approaches combining classification models and answer generation methods. The best team achieved 0.644 BLEU score and 0.624 overall accuracy. In future editions we are considering more complex questions that might include contextual information or require domain-specific inference to reach the right answer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,206.85,742.05,201.67,8.74"><head>3 )Fig. 1 :</head><label>31</label><figDesc>Fig. 1: Examples from VQA-Med-2019 test set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,208.54,471.97,198.29,8.74;7,152.06,201.84,319.75,255.56"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Results of VQA-Med 2019 on crowdAI</figDesc><graphic coords="7,152.06,201.84,319.75,255.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,118.96,364.51,309.34"><head></head><label></label><figDesc>are there abnormalities in this gastrointestinal image? what is the primary abnormality in the image? what is most alarming about this ultrasound?</figDesc><table coords="5,134.77,162.03,345.84,135.40"><row><cell>Planes (16): Axial; Sagittal; Coronal; AP; Lateral; Frontal; PA; Transverse;</cell></row><row><cell>Oblique; Longitudinal; Decubitus; 3D Reconstruction; Mammo-MLO; Mammo-</cell></row><row><cell>CC; Mammo-Mag CC; Mammo-XCC.</cell></row><row><cell>Organ Systems (10): Breast; Skull and Contents; Face, sinuses, and neck; Spine</cell></row><row><cell>and contents; Musculoskeletal; Heart and great vessels; Lung, mediastinum,</cell></row><row><cell>pleura; Gastrointestinal; Genitourinary; Vascular and lymphatic.</cell></row><row><cell>Modalities (36):</cell></row><row><cell>-[XR]: XR-Plain Film</cell></row><row><cell>-[CT]: CT-noncontrast; CT w/contrast (IV); CT-GI &amp; IV Contrast; CTA-CT</cell></row><row><cell>Angiography; CT-GI Contrast; CT-Myelogram; Tomography</cell></row><row><cell>-[MR]: MR-T1W w/Gadolinium; MR-T1W-noncontrast; MR-T2 weighted;</cell></row></table><note coords="5,151.70,300.65,336.68,8.74;5,151.70,312.60,347.57,8.74;5,151.70,324.56,328.90,8.74;5,151.70,336.51,328.90,8.74;5,151.70,348.47,207.46,8.74;5,140.99,360.27,223.13,8.77;5,140.99,372.10,103.33,8.77;5,140.99,383.92,339.61,8.77;5,151.70,395.91,53.55,8.74;5,140.99,407.71,156.53,8.77;5,140.99,419.53,243.70,8.77"><p>MR-FLAIR; MR-T1W w/Gd (fat suppressed); MR T2* gradient,GRE,MPGR, SWAN,SWI; MR-DWI Diffusion Weighted; MRA-MR Angiography/Venography; MR-Other Pulse Seq.; MR-ADC Map (App Diff Coeff); MR-PDW Proton Density; MR-STIR; MR-FIESTA; MR-FLAIR w/Gd; MR-T1W SPGR; MR-T2 FLAIR w/Contrast; MR T2* gradient GRE -[US]: US-Ultrasound; US-D-Doppler Ultrasound -[MA]: Mammograph -[GI]: BAS-Barium Swallow; UGI-Upper GI; BE-Barium Enema; SBFT-Small Bowel -[AG]: AN-Angiogram; Venogram -[PT]: NM-Nuclear Medicine; PET-Positron Emission</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,117.75,350.00,197.08"><head>Table 1 :</head><label>1</label><figDesc>VQA-Med-2019 Training Set: the Most Frequent Answers Per Category</figDesc><table coords="6,136.16,117.75,348.61,184.83"><row><cell>Category</cell><cell>Most frequent answers (#)</cell></row><row><cell>Modality</cell><cell>no (554), yes (552), xr-plain film (456), t2 (217), us-ultrasound (183), t1</cell></row><row><cell></cell><cell>(137), contrast (107), noncontrast (102), ct noncontrast (84), mr-flair</cell></row><row><cell></cell><cell>(84), an-angiogram (78), mr-t2 weighted (56), flair (53), ct w/contrast</cell></row><row><cell></cell><cell>(iv) (50), cta-ct angiograph (45)</cell></row><row><cell>Plane</cell><cell>axial (1558), sagittal (478), coronal (389), ap (197), lateral (151), frontal</cell></row><row><cell></cell><cell>(120), pa (92), transverse (76), oblique (50)</cell></row><row><cell>Organ</cell><cell>skull and contents (1216), musculoskeletal (436), gastrointestinal (352),</cell></row><row><cell>System</cell><cell>lung, mediastinum, pleura (250), spine and contents (234), genitouri-</cell></row><row><cell></cell><cell>nary (214), face, sinuses, and neck (191), vascular and lymphatic (122),</cell></row><row><cell></cell><cell>heart and great vessels (120), breast (65)</cell></row><row><cell cols="2">Abnormality yes (62), no (48), meningioma (30), glioblastoma multiforme (28), pul-</cell></row><row><cell></cell><cell>monary embolism (16), acute appendicitis (14), arteriovenous malfor-</cell></row><row><cell></cell><cell>mation (avm) (14), arachnoid cyst (13), schwannoma (13), tuberous</cell></row><row><cell></cell><cell>sclerosis (13), brain, cerebral abscess (12), ependymoma (12), fibrous</cell></row><row><cell></cell><cell>dysplasia (12), multiple sclerosis (12), diverticulitis (11), langerhan cell</cell></row><row><cell></cell><cell>histiocytosis (11), sarcoidosis (11)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,151.61,517.86,312.12,117.84"><head>Table 3 :</head><label>3</label><figDesc>VQA-Med 2019: Accuracy scores</figDesc><table coords="7,151.61,539.73,312.12,95.96"><row><cell>Team</cell><cell cols="5">Run ID Modality Plane Organ Abnormality Overall</cell></row><row><cell>Hanlin</cell><cell>26889</cell><cell>0.202</cell><cell>0.192 0.184</cell><cell>0.046</cell><cell>0.624</cell></row><row><cell></cell><cell>26853</cell><cell>0.202</cell><cell>0.192 0.184</cell><cell>0.042</cell><cell>0.620</cell></row><row><cell>minhvu</cell><cell>26881</cell><cell>0.210</cell><cell>0.194 0.190</cell><cell>0.022</cell><cell>0.616</cell></row><row><cell>TUA1</cell><cell>26822</cell><cell>0.186</cell><cell>0.204 0.198</cell><cell>0.018</cell><cell>0.606</cell></row><row><cell>UMMS</cell><cell>27306</cell><cell>0.168</cell><cell>0.190 0.184</cell><cell>0.024</cell><cell>0.566</cell></row><row><cell>AIOZ</cell><cell>26873</cell><cell>0.182</cell><cell>0.180 0.182</cell><cell>0.020</cell><cell>0.564</cell></row><row><cell cols="2">IBM Research AI 27199</cell><cell>0.160</cell><cell>0.196 0.192</cell><cell>0.010</cell><cell>0.558</cell></row><row><cell>LIST</cell><cell>26908</cell><cell>0.180</cell><cell>0.184 0.178</cell><cell>0.014</cell><cell>0.556</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,223.13,241.53,169.10,216.46"><head>Table 4 :</head><label>4</label><figDesc>VQA-Med 2019: BLEU scores</figDesc><table coords="8,237.44,263.41,140.47,194.59"><row><cell>Team</cell><cell>Run ID BLEU</cell></row><row><cell>Hanlin</cell><cell>26889 0.644</cell></row><row><cell>yan</cell><cell>26853 0.640</cell></row><row><cell>minhvu</cell><cell>26881 0.634</cell></row><row><cell>TUA1</cell><cell>26822 0.633</cell></row><row><cell>UMMS</cell><cell>27306 0.593</cell></row><row><cell>JUST19</cell><cell>27142 0.591</cell></row><row><cell>LIST</cell><cell>26908 0.583</cell></row><row><cell cols="2">IBM Research AI 27199 0.582</cell></row><row><cell>AIOZ</cell><cell>26833 0.579</cell></row><row><cell>Turner.JCE</cell><cell>26940 0.572</cell></row><row><cell>Team Pwc Med</cell><cell>26955 0.534</cell></row><row><cell>Techno</cell><cell>27079 0.486</cell></row><row><cell>abhishekthanki</cell><cell>26824 0.462</cell></row><row><cell>Dear stranger</cell><cell>26895 0.393</cell></row><row><cell cols="2">deepak.gupta651 27232 0.389</cell></row><row><cell>ChandanReddy</cell><cell>26946 0.323</cell></row><row><cell>IITISM@CLEF</cell><cell>26905 0.096</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,136.16,127.36,410.54,97.42"><head>Table 2 :</head><label>2</label><figDesc>Participating groups in the VQA-Med 2019 task.</figDesc><table coords="9,136.16,138.55,120.60,6.14"><row><cell>Team</cell><cell>Institution</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="3,144.73,645.84,149.49,7.86"><p>github.com/abachaa/VQA-Med-2019</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="3,144.73,656.80,111.66,7.86"><p>https://medpix.nlm.nih.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="6,144.73,656.80,147.83,7.86"><p>https://visualqa.org/evaluation.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="7,144.73,645.84,335.85,7.86;7,144.73,656.80,160.37,7.86"><p>There was a limit of maximum 10 run submissions per team. The table includes only the valid runs that were graded (total#</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="80" xml:id="foot_4" coords="7,320.45,656.80,90.05,7.86"><p>out of 90 submissions)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="8,144.73,645.84,335.85,7.86;8,144.73,656.80,290.50,7.86"><p>Note that the question category-wise accuracy scores are normalized (each divided by a factor of 4) so that the summation is equal to the overall accuracy.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6" coords="9,144.73,645.84,265.47,7.86;9,144.73,656.80,149.49,7.86"><p>www.crowdai.org/clef tasks/13/task dataset files?challenge id=53 github.com/abachaa/VQA-Med-2019</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the intramural research program at the <rs type="funder">U.S. National Library of Medicine, National Institutes of Health</rs>. We thank <rs type="person">Dr. James G. Smirniotopoulos</rs> and <rs type="person">Soumya Gayen</rs> from the <rs type="affiliation">MedPix team</rs> for their support.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,138.90,337.62,7.86;10,151.52,149.85,329.06,7.86;10,151.52,160.81,47.09,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,414.01,138.90,66.57,7.86;10,151.52,149.85,211.73,7.86">Just at imageclef 2019 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Al-Sadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Talafha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jararweh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Costen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,383.94,149.85,96.64,7.86;10,151.52,160.81,18.43,7.86">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,171.03,337.62,7.86;10,151.52,181.99,329.05,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,328.97,171.03,151.60,7.86;10,151.52,181.99,163.94,7.86">An encoder-decoder model for visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Allaouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Benamrou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,335.77,181.99,116.85,7.86">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,192.21,337.62,7.86;10,151.52,203.17,329.05,7.86;10,151.52,214.13,329.06,7.86;10,151.52,225.09,191.79,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,151.52,203.17,126.29,7.86">VQA: visual question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.279</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.279" />
	</analytic>
	<monogr>
		<title level="m" coord="10,298.39,203.17,182.19,7.86;10,151.52,214.13,101.76,7.86">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">December 7-13, 2015. 2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,235.31,337.62,7.86;10,151.52,246.27,329.05,7.86;10,151.52,257.23,329.05,7.86;10,151.52,268.19,329.05,7.86;10,151.52,279.15,78.85,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,151.52,246.27,308.62,7.86">NLM at imageclef 2018 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2125/paper165.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,257.23,329.05,7.86;10,151.52,268.19,14.22,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,289.37,337.62,7.86;10,151.52,300.33,264.29,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,299.38,289.37,181.19,7.86;10,151.52,300.33,94.88,7.86">Tlemcen university at imageclef 2019 visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bounaama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Abderrahim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,267.40,300.33,119.73,7.86">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,310.55,337.62,7.86;10,151.52,321.50,329.05,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,346.98,310.55,133.59,7.86;10,151.52,321.50,186.39,7.86">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,358.56,321.50,93.75,7.86">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,331.72,337.62,7.86;10,151.52,342.68,329.06,7.86;10,151.52,353.64,329.05,7.86;10,151.52,364.60,329.05,7.86;10,151.52,375.56,237.18,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,455.24,331.72,25.34,7.86;10,151.52,342.68,324.76,7.86">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D16/D16-1044.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,164.90,353.64,315.67,7.86;10,151.52,364.60,101.77,7.86">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">November 1-4, 2016. 2016</date>
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,385.78,337.62,7.86;10,151.52,396.74,329.06,7.86;10,151.52,407.70,329.06,7.86;10,151.52,418.66,329.06,7.86;10,151.52,429.62,163.64,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,411.97,385.78,68.61,7.86;10,151.52,396.74,329.06,7.86;10,151.52,407.70,28.51,7.86">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.670</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.670" />
	</analytic>
	<monogr>
		<title level="m" coord="10,204.03,407.70,276.55,7.86;10,151.52,418.66,69.98,7.86">2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">July 21-26, 2017. 2017</date>
			<biblScope unit="page" from="6325" to="6334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,439.84,337.62,7.86;10,151.52,450.80,329.06,7.86;10,151.52,461.75,329.06,7.86;10,151.52,472.71,302.45,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,431.39,439.84,49.18,7.86;10,151.52,450.80,248.53,7.86">Overview of imageclef 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2125/paper212.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,420.75,450.80,59.83,7.86;10,151.52,461.75,252.07,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,482.93,337.96,7.86;10,151.52,493.89,329.06,7.86;10,151.52,504.85,329.05,7.86;10,151.52,515.81,159.03,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,313.22,482.93,167.35,7.86;10,151.52,493.89,22.38,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m" coord="10,198.85,493.89,281.73,7.86;10,151.52,504.85,69.87,7.86">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,526.03,337.96,7.86;10,151.52,536.99,329.05,7.86;10,151.52,547.95,329.06,7.86;10,151.52,558.91,329.05,7.86;10,151.52,569.87,329.05,7.86;10,151.52,580.83,329.06,7.86;10,151.52,591.78,329.06,7.86;10,151.52,602.74,329.06,7.86;10,151.52,613.70,329.05,7.86;10,151.52,624.66,216.27,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,268.29,580.83,212.29,7.86;10,151.52,591.78,124.23,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,297.67,591.78,182.91,7.86;10,151.52,602.74,329.06,7.86;10,151.52,613.70,122.46,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="10,305.55,613.70,171.06,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="10,142.61,634.88,337.96,7.86;10,151.52,645.84,329.06,7.86;10,151.52,656.80,25.60,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,403.24,634.88,77.34,7.86;10,151.52,645.84,189.94,7.86">Leveraging medical visual question answering with supporting facts</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kornuta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shivade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Asseman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ozcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,361.72,645.84,118.86,7.86">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,119.67,337.96,7.86;11,151.52,130.63,329.06,7.86;11,151.52,141.57,266.34,7.89" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,393.92,119.67,86.65,7.86;11,151.52,130.63,260.54,7.86">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<ptr target="https://www.nature.com/articles/sdata2018251" />
	</analytic>
	<monogr>
		<title level="j" coord="11,420.48,130.63,60.10,7.86">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180251</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,152.55,337.97,7.86;11,151.52,163.51,74.87,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,253.25,152.55,133.96,7.86">Vqa-med: An xception-gru model</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,409.47,152.55,71.11,7.86;11,151.52,163.51,46.20,7.86">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,174.47,337.96,7.86;11,151.52,185.43,329.05,7.86;11,151.52,196.39,329.06,7.86;11,151.52,207.35,98.01,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,350.72,174.47,129.85,7.86;11,151.52,185.43,133.69,7.86">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,306.55,185.43,174.02,7.86;11,151.52,196.39,162.46,7.86">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,218.30,337.96,7.86;11,151.52,229.26,267.19,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,332.63,218.30,147.94,7.86;11,151.52,229.26,96.77,7.86">Medical visual question answering at imageclef 2019-vqa med</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gadgil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,270.30,229.26,119.73,7.86">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,240.22,337.96,7.86;11,151.52,251.18,207.86,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,268.25,240.22,212.33,7.86;11,151.52,251.18,38.08,7.86">Deep multimodal learning for medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,210.98,251.18,119.74,7.86">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,262.14,337.96,7.86;11,151.52,273.10,329.05,7.86;11,151.52,284.06,329.05,7.86;11,151.52,295.02,158.03,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,278.92,262.14,201.66,7.86;11,151.52,273.10,69.96,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
	</analytic>
	<monogr>
		<title level="m" coord="11,242.85,273.10,237.72,7.86;11,151.52,284.06,42.85,7.86">3rd International Conference on Learning Representations, ICLR 2015</title>
		<title level="s" coord="11,358.31,284.06,122.26,7.86">Conference Track Proceedings</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,305.98,337.96,7.86;11,151.52,316.94,274.92,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,250.25,305.98,230.32,7.86;11,151.52,316.94,105.39,7.86">Lstm in vqa-med, is it really needed? validation study on the imageclef 2019 dataset</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Spanier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,278.04,316.94,119.73,7.86">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,327.89,337.96,7.86;11,151.52,338.85,275.44,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,272.55,327.89,208.02,7.86;11,151.52,338.85,105.28,7.86">Mit manipal at imageclef 2019 visualquuestion answering in medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Thanki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Makkithaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,278.56,338.85,119.74,7.86">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,349.81,337.96,7.86;11,151.52,360.77,329.05,7.86;11,151.52,371.73,197.81,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,347.66,349.81,132.92,7.86;11,151.52,360.77,329.05,7.86;11,151.52,371.73,27.65,7.86">Ensemble of streamlined bilinear visual question answering models for the imageclef 2019 challenge in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nyholm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lfstedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,200.92,371.73,119.74,7.86">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,382.69,337.96,7.86;11,151.52,393.65,329.05,7.86;11,151.52,404.61,47.09,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="11,327.28,382.69,153.29,7.86;11,151.52,393.65,202.20,7.86">Zhejiang university at imageclef 2019 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,378.22,393.65,102.35,7.86;11,151.52,404.61,18.43,7.86">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,415.57,337.96,7.86;11,151.52,426.52,329.05,7.86;11,151.52,437.48,329.06,7.86;11,151.52,448.44,216.36,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="11,355.74,415.57,124.84,7.86;11,151.52,426.52,103.88,7.86">Stacked attention networks for image question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.10</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.10" />
	</analytic>
	<monogr>
		<title level="m" coord="11,279.29,426.52,201.28,7.86;11,151.52,437.48,136.89,7.86">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,459.40,337.96,7.86;11,151.52,470.36,329.05,7.86;11,151.52,481.32,25.60,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="11,277.21,459.40,203.36,7.86;11,151.52,470.36,191.12,7.86">Tua1 at imageclef 2019 vqa-med: A classification and generation model based on transfer learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,362.72,470.36,117.85,7.86">Working Notes of CLEF 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
