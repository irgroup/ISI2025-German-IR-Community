<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.27,115.96,334.81,12.62;1,228.99,133.89,157.37,12.62">Leveraging Medical Visual Question Answering with Supporting Facts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,192.93,171.86,70.47,8.74"><forename type="first">Tomasz</forename><surname>Kornuta</surname></persName>
							<email>tkornut@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research AI</orgName>
								<orgName type="institution" key="instit2">Almaden Research Center</orgName>
								<address>
									<settlement>San Jose</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.61,171.86,58.50,8.74"><forename type="first">Deepta</forename><surname>Rajan</surname></persName>
							<email>drajan@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research AI</orgName>
								<orgName type="institution" key="instit2">Almaden Research Center</orgName>
								<address>
									<settlement>San Jose</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,338.30,171.86,79.60,8.74"><forename type="first">Chaitanya</forename><surname>Shivade</surname></persName>
							<email>cshivade@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research AI</orgName>
								<orgName type="institution" key="instit2">Almaden Research Center</orgName>
								<address>
									<settlement>San Jose</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.85,183.81,66.11,8.74"><forename type="first">Alexis</forename><surname>Asseman</surname></persName>
							<email>alexis.asseman@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research AI</orgName>
								<orgName type="institution" key="instit2">Almaden Research Center</orgName>
								<address>
									<settlement>San Jose</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.83,183.81,71.68,8.74"><forename type="first">Ahmet</forename><forename type="middle">S</forename><surname>Ozcan</surname></persName>
							<email>asozcan@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research AI</orgName>
								<orgName type="institution" key="instit2">Almaden Research Center</orgName>
								<address>
									<settlement>San Jose</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.27,115.96,334.81,12.62;1,228.99,133.89,157.37,12.62">Leveraging Medical Visual Question Answering with Supporting Facts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AFCF86F8A1B110A0F1FD6F0800B1B67C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF 2019</term>
					<term>VQA-Med</term>
					<term>Visual Question Answering</term>
					<term>Supporting Facts Network</term>
					<term>Multi-Task Learning</term>
					<term>Transfer Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this working notes paper, we describe IBM Research AI (Almaden) team's participation in the ImageCLEF 2019 VQA-Med competition. The challenge consists of four question-answering tasks based on radiology images. The diversity of imaging modalities, organs and disease types combined with a small imbalanced training set made this a highly complex problem. To overcome these difficulties, we implemented a modular pipeline architecture that utilized transfer learning and multitask learning. Our findings led to the development of a novel model called Supporting Facts Network (SFN). The main idea behind SFN is to cross-utilize information from upstream tasks to improve the accuracy on harder downstream ones. This approach significantly improved the scores achieved in the validation set (18 point improvement in F-1 score). Finally, we submitted four runs to the competition and were ranked seventh.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the era of data deluge and powerful computing systems, deriving meaningful insights from heterogeneous information has shown to have tremendous value across industries. In particular, the promise of deep learning-based computational models <ref type="bibr" coords="1,199.79,530.75,15.50,8.74" target="#b14">[15]</ref> in accurately predicting diseases has further stirred great interest in adopting automated learning systems in healthcare <ref type="bibr" coords="1,421.45,542.71,9.96,8.74" target="#b1">[2]</ref>. A daunting challenge within the realm of healthcare is to efficiently sieve through vast amounts of multi-modal information and reason over them to arrive at a differential diagnosis. Longitudinal patient records including time-series measurements, text reports and imaging volumes form the basis for doctors to draw conclusive insights. In practice, radiologists are tasked with reviewing thousands of imaging studies each day, with an average of about three seconds to mark them as anomalous or not, leading to severe eye fatigue <ref type="bibr" coords="2,351.16,118.99,14.61,8.74" target="#b22">[24]</ref>. Moreover, clinical workflows have a sequential nature tending to cause delays in triage situations, where the existence of answers to key questions about a patient's holistic conditions can potentially expedite treatment. Thus, building effective question-answering systems for the medical domain by bringing advancements in machine learning research will be a game changer towards improving patient care.</p><p>Visual Question Answering (VQA) <ref type="bibr" coords="2,303.95,190.72,15.50,8.74" target="#b16">[17,</ref><ref type="bibr" coords="2,321.11,190.72,7.75,8.74" target="#b0">1]</ref> is a new exciting problem domain, where the system is expected to answer questions expressed in natural language by taking into account the content of the image. In this paper, we present results of our research on the VQA-Med 2019 dataset <ref type="bibr" coords="2,339.30,226.59,9.96,8.74" target="#b2">[3]</ref>, an open challenge associated with the ImageCLEF 2019 initiative <ref type="bibr" coords="2,299.93,238.55,14.61,8.74" target="#b9">[10]</ref>. The main issue here, in comparison to the other recent VQA datasets such as TextVQA <ref type="bibr" coords="2,365.29,250.50,15.50,8.74" target="#b21">[23]</ref> or GQA <ref type="bibr" coords="2,422.85,250.50,9.96,8.74" target="#b8">[9]</ref>, is dealing with scattered, noisy and heavily biased data. Hence, the dataset serves as a great use-case to study challenges encountered in practical clinical scenarios.</p><p>In order to address the data issues, we designed a new model called Supporting Facts Network (SFN) that efficiently shares knowledge between upstream and downstream tasks through the use of a pre-trained multi-task solver in combination with task-specific solvers. Note that posing the VQA-Med challenge as a multi-task learning problem <ref type="bibr" coords="2,269.13,334.19,10.52,8.74" target="#b3">[4]</ref> allowed the model to effectively leverage and encode relevant domain knowledge. Our multi-task SFN model outperforms the single task baseline by better adapting to label distribution shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The VQA-Med dataset</head><p>The VQA-Med 2019 <ref type="bibr" coords="2,223.36,413.70,10.52,8.74" target="#b2">[3]</ref> is a Visual Question Answering (VQA) dataset embedded in the medical domain, with a focus on radiology images. It consists of:</p><p>a training set of 3,200 images with 12,792 Question-Answer (QA) pairs, a validation set of 500 images with 2,000 QA pairs, and a test set of 500 images with 500 questions (answers were released after the end of the VQA-Med 2019 challenge).</p><p>In all splits the samples were divided into four categories, depending on the main task to be solved:</p><p>-C1: determine the modality of the image, -C2: determine the plane of the image, -C3: identify the organ/anatomy of interest in the image, and -C4: identify the abnormality in the image.</p><p>Our analysis of the dataset (distribution of questions, answers, word vocabularies, categories and image sizes) led to the following findings and system-design related decisions:</p><p>merge of the original training and validation sets, shuffle and re-sample new training and validation sets with a proportion of 19:1, use of weighted random sampling during batch preparation, addition of a fifth Binary category for samples with Y/N type questions, focus on accuracy-related metrics instead of the BLEU score, avoid label (answer classes) unification and cleansing, consider C4 as a downstream task and exclude it from the pre-training of input fusion modules, utilization of image size as an additional input cue to the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Supporting Facts Network</head><p>Typical VQA systems process two types of input, visual (image) and language (question), that need to undergo various transformations to produce the answer. Fig. <ref type="figure" coords="3,156.40,261.18,4.98,8.74" target="#fig_0">1</ref> presents a general architecture of such systems, indicating four major modules: two encoders responsible for encoding raw inputs to more useful representations, followed by a reasoning module that combines them and finally, an answer decoder that produces the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Reasoning Module</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Encoder</head><p>Question Encoder Answer Decoder Question Answer In the early prototypes of VQA systems, reasoning modules were rather simple and relied mainly on multi-modal fusion mechanisms. These fusion techniques varied from concatenation of image and question representations, to more complex pooling mechanisms such as Multi-modal Compact Bilinear pooling (MCB) <ref type="bibr" coords="3,170.61,496.77,10.52,8.74" target="#b5">[6]</ref> and Multi-modal Low-rank Bilinear pooling (MLB) <ref type="bibr" coords="3,422.00,496.77,14.61,8.74" target="#b11">[12]</ref>. Further, diverse attention mechanisms such as question-driven attention over image features <ref type="bibr" coords="3,160.61,520.68,15.50,8.74" target="#b10">[11]</ref> were also used. More recently, researchers have focused on complex multi-step reasoning mechanisms such as Relational Networks <ref type="bibr" coords="3,407.84,532.64,15.50,8.74" target="#b19">[21,</ref><ref type="bibr" coords="3,424.99,532.64,7.75,8.74" target="#b4">5]</ref> and Memory, Attention and Composition (MAC) networks <ref type="bibr" coords="3,357.48,544.59,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="3,369.65,544.59,11.62,8.74" target="#b17">18]</ref>. Despite that, certain empirical studies indicate early fusion of language and vision signals significantly boosts the overall performance of VQA systems <ref type="bibr" coords="3,343.71,568.50,14.61,8.74" target="#b15">[16]</ref>. Therefore, we explored the finding of an "optimal" module for early fusion of multi-modal inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture of the Input Fusion Module</head><p>One of our findings from analyzing the dataset was to use the image size as additional input cue to the system. This insight triggered an extensive architecture search that included, among others, comparison and training of models with:</p><p>different methods for question encoding, from 1-hot encoding with Bag-of-Words to different word embeddings combined with various types of recurrent neural networks, different image encoders, from simple networks containing few convolutional layers trained from scratch to fine-tuning of selected state-of-the-art models pre-trained on ImageNet, various data fusion techniques as mentioned in the previous section. The final architecture of our model is presented in Fig. <ref type="figure" coords="4,396.79,368.69,8.49,8.74">2a</ref>. We used GloVe word embeddings [20] followed by Long Short-Term Memory (LSTM) <ref type="bibr" coords="4,446.33,380.64,9.96,8.74" target="#b6">[7]</ref>. The LSTM outputs along with feature maps extracted from images using VGG-16 <ref type="bibr" coords="4,149.91,404.55,15.50,8.74" target="#b20">[22]</ref> were passed to the Fusion I module, implementing question-driven attention over image features <ref type="bibr" coords="4,269.01,416.51,14.61,8.74" target="#b10">[11]</ref>. Next, the output of that module was concatenated in the Fusion II module with image size representation created by passing image width and height through a fully connected (FC) layer.</p><p>Note that the green colored modules were initially pre-trained on external datasets (ImageNet and 6B tokens from Wikipedia 2014 and Gigaword 5 datasets for VGG-16 and GloVe models respectively) and later fine-tuned during training on the VQA-Med dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architectures of the Reasoning Modules</head><p>During the architecture search of the Input Fusion module we used the model presented in Fig. <ref type="figure" coords="4,208.16,548.52,3.87,8.74">3</ref>, with a simple classifier with two FC layers. These models were trained and validated on C1, C2 and C3 categories separately, while excluding C4. In fact, to test our hypothesis we trained some early prototypes only on samples from C4 and the models failed to converge.</p><p>After establishing the Input Fusion module we trained it on samples from C1, C2 and C3 categories. This served as a starting point for training more complex reasoning modules. At first, we worked on a model that exploited information about 5 categories of questions by employing 5 separate classifiers which used data produced by the Input Fusion module. Each of these classifiers essentially specialized in one question category and had its own answer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answer</head><p>Input Fusion Classifier The architecture of our final model, Supporting Facts Network is presented in Fig. <ref type="figure" coords="5,194.93,477.76,3.87,8.74">4</ref>. The main idea here resulted from the analysis of questions about the presence of abnormalities -to answer which the system required knowledge on image modality and/or organ type. Therefore, we divided the classification modules into two networks: Support networks (consisting of two FC layers) and final classifiers (being single FC layers). We added Plane (C2) as an additional supporting fact. The supporting facts were then concatenated with output from Input Fusion module in Fusion III and passed as input to the classifier specialized on C4 questions. In addition, since Binary Y/N questions were present in both C1 and C4 categories, we followed a similar approach for that classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>All experiments were conducted using PyTorchPipe <ref type="bibr" coords="5,366.66,632.21,14.61,8.74" target="#b13">[14]</ref>, a framework that facilitates development of multi-modal pipelines built on top of PyTorch <ref type="bibr" coords="5,442.18,644.16,14.61,8.74" target="#b18">[19]</ref>. Our models were trained using relatively large batches (256), dropout (0.5) and Adam optimizer <ref type="bibr" coords="6,178.70,118.99,15.50,8.74" target="#b12">[13]</ref> with a small learning rate (1e -4). For each experimental run, we generated a new training and validation set by combining the original sets, shuffling and sampling them in proportions of 19 : 1, thereby resulting in a validation set of size 5%. In Tab. 1 we present a comparison of average scores achieved by our baseline models using single classifier (IF-1C) and the Supporting Facts Networks (SFN). Our results clearly indicate the advantage of using 'supporting facts' over the baseline model with a single classifier. The SFN model by our team achieved a best score of (0.558 Accuracy, 0.582 BLEU score) on the test set as indicated by the CrowdAI leaderboard. One of the reasons for such a significant drop in performance is due to the presence of new answers classes in the test set that were not present both in the original training and validation sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>In this work, we introduced a new model called Supporting Facts Network (SFN), that leverages knowledge learned from combinations of upstream tasks in order to benefit additional downstream tasks. The model incorporates domain knowledge that we gathered from a thorough analysis of the dataset, resulting in specialized input fusion methods and five separate, category-specific classifiers. It comprises of two pre-trained shared modules followed by a reasoning module jointly trained with five classifiers using the multi-task learning approach. Our models were found to train faster and to deal much better with label distribution shifts under a small imbalanced data regime.</p><p>Among the five categories of samples present in the VQA-Med dataset, C4 and Binary turned out to be extremely difficult to learn, for several reasons. First, there were 1483 unique answer classes assigned to 3082 training samples related to C4. Second, both C4 and Binary required more complex reasoning and, besides, might be impossible to conclude by looking only at the question and content of the image. However, our observation that some of the information from simpler categories might be useful during reasoning on more complex ones, we refined the model by adding supporting networks. Given, modality, imaging plane and organ typically help narrow down the scope of disease conditions and/or answer whether or not an abnormality is present. Our empirical studies prove that this approach performs significantly better, leading to an 18 point improvement in F-1 score over the baseline model on the original validation set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,172.42,414.78,270.52,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. General architecture of Visual Question Answering systems</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,204.48,182.48,206.41,7.89"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Architecture with a single classifier (IF-1C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,190.31,345.82,95.45"><head>Table 1 .</head><label>1</label><figDesc>Summary of experimental results. All columns contain average scores achieved by 5 separately trained models on resampled training and validation sets. We also present scores achieved by the models on original sets (in the evaluation mode).</figDesc><table coords="6,139.53,190.31,336.30,50.54"><row><cell></cell><cell cols="3">Resampled Valid. Set</cell><cell cols="3">Original Train. Set</cell><cell cols="2">Original Valid. Set</cell></row><row><cell cols="3">Model Prec. Recall</cell><cell>F-1</cell><cell cols="2">Prec. Recall</cell><cell>F-1</cell><cell cols="2">Prec. Recall</cell><cell>F-1</cell></row><row><cell cols="2">IF-1C 0.630</cell><cell>0.435</cell><cell cols="2">0.481 0.683</cell><cell>0.497</cell><cell cols="2">0.545 0.690</cell><cell>0.499</cell><cell>0.548</cell></row><row><cell>SFN</cell><cell>0.759</cell><cell>0.758</cell><cell cols="2">0.758 0.753</cell><cell>0.692</cell><cell cols="2">0.707 0.762</cell><cell>0.704</cell><cell>0.717</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.96,209.22,337.64,7.86;7,151.52,220.18,329.07,7.86;7,151.52,231.14,213.60,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,167.16,220.18,135.17,7.86">VQA: Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,324.76,220.18,155.84,7.86;7,151.52,231.14,120.81,7.86">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,241.97,337.64,7.86;7,151.52,252.93,329.07,7.86;7,151.52,263.89,329.07,7.86;7,151.52,274.85,148.77,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,414.88,252.93,65.71,7.86;7,151.52,263.89,329.07,7.86;7,151.52,274.85,45.40,7.86">End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Kiraly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Reicher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Etemadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Naidich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,204.53,274.85,67.10,7.86">Nature Medicine</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,285.68,337.64,7.86;7,151.52,296.64,329.07,7.86;7,151.52,307.60,329.07,7.86;7,151.52,318.56,329.07,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,166.84,296.64,313.75,7.86;7,151.52,307.60,33.73,7.86">VQA-Med: Overview of the medical visual question answering task at imageclef 2019</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/" />
	</analytic>
	<monogr>
		<title level="m" coord="7,207.20,307.60,267.45,7.86">CLEF 2019 Working Notes. CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,329.37,295.50,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,206.45,329.39,72.99,7.86">Multitask learning</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,286.39,329.39,69.13,7.86">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,340.23,337.64,7.86;7,151.52,351.19,329.07,7.86;7,151.52,362.15,50.81,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,294.60,340.23,122.38,7.86">Object-based reasoning in VQA</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Desta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kornuta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,437.94,340.23,42.65,7.86;7,151.52,351.19,260.57,7.86">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1814" to="1823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,372.98,337.64,7.86;7,151.52,383.94,329.07,7.86;7,151.52,394.90,76.41,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,455.25,372.98,25.34,7.86;7,151.52,383.94,324.77,7.86">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,165.60,394.90,33.65,7.86">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,405.73,337.64,7.86;7,151.52,416.67,92.85,7.89" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,288.76,405.73,100.73,7.86">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,398.69,405.73,81.90,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,427.53,337.64,7.86;7,151.52,438.49,100.64,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,276.84,427.53,203.75,7.86;7,151.52,438.49,24.18,7.86">Compositional attention networks for machine reasoning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,196.88,438.49,26.62,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,449.32,337.64,7.86;7,151.52,460.28,297.74,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,284.00,449.32,196.60,7.86;7,151.52,460.28,131.36,7.86">Gqa: a new dataset for compositional question answering over real-world images</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09506</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,142.62,471.12,337.98,7.86;7,151.52,482.08,329.07,7.86;7,151.52,493.03,329.07,7.86;7,151.52,503.99,329.07,7.86;7,151.52,514.95,329.07,7.86;7,151.52,525.91,329.07,7.86;7,151.52,536.87,329.07,7.86;7,151.52,547.83,329.07,7.86;7,151.52,558.79,329.07,7.86;7,151.52,569.75,216.27,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,268.29,525.91,212.30,7.86;7,151.52,536.87,124.23,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,297.67,536.87,182.92,7.86;7,151.52,547.83,329.07,7.86;7,151.52,558.79,122.46,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="7,305.55,558.79,171.07,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="7,142.62,580.58,337.97,7.86;7,151.52,591.54,267.51,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Elqursh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03162</idno>
		<title level="m" coord="7,258.95,580.58,221.64,7.86;7,151.52,591.54,101.15,7.86">Show, ask, attend, and answer: A strong baseline for visual question answering</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,142.62,602.38,337.98,7.86;7,151.52,613.33,185.91,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,404.56,602.38,76.03,7.86;7,151.52,613.33,113.53,7.86">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,286.23,613.33,22.52,7.86">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,624.17,337.97,7.86;7,151.52,635.13,93.19,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="7,239.72,624.17,176.61,7.86">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,142.62,645.96,299.31,7.86" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kornuta</surname></persName>
		</author>
		<ptr target="https://github.com/ibm/pytorchpipe" />
		<title level="m" coord="7,205.55,645.96,50.47,7.86">PyTorchPipe</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,656.77,336.60,7.89" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,295.96,656.80,54.39,7.86">Deep learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,357.30,656.80,26.14,7.86">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,119.67,337.98,7.86;8,151.52,130.63,329.07,7.86;8,151.52,141.59,124.33,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,273.80,119.67,206.79,7.86;8,151.52,130.63,156.42,7.86">The Visual QA devil in the details: The impact of early fusion and batch norm on CLEVR</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,330.07,130.63,150.53,7.86;8,151.52,141.59,95.67,7.86">ECCV&apos;18 Workshop on Shortcomings in Vision and Language</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,152.55,337.98,7.86;8,151.52,163.51,329.07,7.86;8,151.52,174.47,167.19,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,263.18,152.55,217.42,7.86;8,151.52,163.51,175.82,7.86">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,350.62,163.51,129.97,7.86;8,151.52,174.47,73.89,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="1682" to="1690" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,185.43,337.97,7.86;8,151.52,196.39,329.07,7.86;8,151.52,207.34,211.60,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,468.30,185.43,12.29,7.86;8,151.52,196.39,182.05,7.86">On transfer learning using a MAC model variant</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Marois</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jayram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Albouy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kornuta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bouhadjar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Ozcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,355.11,196.39,125.48,7.86;8,151.52,207.34,182.93,7.86">NeurIPS&apos;18 Visually-Grounded Interaction and Language (ViGIL) Workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,218.30,337.98,7.86;8,151.52,229.26,327.47,7.86;8,134.77,240.22,345.83,7.86;8,151.52,251.18,329.07,7.86;8,151.52,262.14,215.28,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,303.85,229.26,146.46,7.86">Automatic differentiation in pytorch</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,325.45,240.22,155.14,7.86;8,151.52,251.18,35.30,7.86;8,207.71,251.18,272.88,7.86;8,151.52,262.14,120.77,7.86">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2017. 2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Glove: Global vectors for word representation</note>
</biblStruct>

<biblStruct coords="8,142.62,273.10,337.97,7.86;8,151.52,284.06,329.07,7.86;8,151.52,295.02,261.25,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="8,203.61,284.06,219.67,7.86">A simple neural network module for relational reasoning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,442.66,284.06,37.93,7.86;8,151.52,295.02,167.77,7.86">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,305.98,337.97,7.86;8,151.52,316.93,231.27,7.86" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="8,278.92,305.98,201.67,7.86;8,151.52,316.93,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.62,327.89,337.98,7.86;8,151.52,338.85,329.07,7.86;8,151.52,349.81,25.60,7.86" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08920</idno>
		<title level="m" coord="8,211.11,338.85,134.26,7.86">Towards vqa models that can read</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,142.62,360.77,337.97,7.86;8,151.52,371.73,329.07,7.86;8,151.52,382.69,329.07,7.86;8,151.52,393.65,329.07,7.86;8,151.52,404.61,115.42,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="8,449.11,371.73,31.49,7.86;8,151.52,382.69,240.30,7.86">Medical sieve: a cognitive assistant for radiologists and cardiologists</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Syeda-Mahmood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Walach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Beymer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gilboa-Solomon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kisilev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kakrania</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Compas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Negahdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,413.25,382.69,67.34,7.86;8,176.08,393.65,107.75,7.86">Computer-Aided Diagnosis</title>
		<imprint>
			<biblScope unit="volume">9785</biblScope>
			<biblScope unit="page">97850</biblScope>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
