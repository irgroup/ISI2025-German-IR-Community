<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.68,115.96,328.00,12.62;1,141.40,133.89,332.55,12.62;1,144.45,151.82,326.45,12.62">Feature and Deep Learning Based Approaches for Automatic Report Generation and Severity Scoring of Lung Tuberculosis from CT Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.64,189.49,72.79,8.74"><forename type="first">Kirill</forename><surname>Bogomasov</surname></persName>
							<email>bogomasov@hhu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institut für Informatik</orgName>
								<orgName type="institution">Heinrich-Heine-Universität Düsseldorf</orgName>
								<address>
									<addrLine>Universitätsstraße 1</addrLine>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.95,189.49,56.22,8.74"><forename type="first">Daniel</forename><surname>Braun</surname></persName>
							<email>daniel-braun@hhu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institut für Informatik</orgName>
								<orgName type="institution">Heinrich-Heine-Universität Düsseldorf</orgName>
								<address>
									<addrLine>Universitätsstraße 1</addrLine>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.45,189.49,73.64,8.74"><forename type="first">Andreas</forename><surname>Burbach</surname></persName>
							<email>andreas.burbach@hhu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institut für Informatik</orgName>
								<orgName type="institution">Heinrich-Heine-Universität Düsseldorf</orgName>
								<address>
									<addrLine>Universitätsstraße 1</addrLine>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.34,189.49,95.30,8.74"><forename type="first">Ludmila</forename><surname>Himmelspach</surname></persName>
							<email>ludmila.himmelspach@hhu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institut für Informatik</orgName>
								<orgName type="institution">Heinrich-Heine-Universität Düsseldorf</orgName>
								<address>
									<addrLine>Universitätsstraße 1</addrLine>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.94,201.45,62.85,8.74"><forename type="first">Stefan</forename><surname>Conrad</surname></persName>
							<email>stefan.conrad@hhu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institut für Informatik</orgName>
								<orgName type="institution">Heinrich-Heine-Universität Düsseldorf</orgName>
								<address>
									<addrLine>Universitätsstraße 1</addrLine>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.68,115.96,328.00,12.62;1,141.40,133.89,332.55,12.62;1,144.45,151.82,326.45,12.62">Feature and Deep Learning Based Approaches for Automatic Report Generation and Severity Scoring of Lung Tuberculosis from CT Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DF72EB8F9E7C75E5E3F4504A92BE558A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>automatic CT report</term>
					<term>tuberculosis severity scoring</term>
					<term>medical image classification</term>
					<term>feature extraction</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper presents two approaches for automatic Computed Tomography (CT) report and tuberculosis (TB) severity scoring which were two subtasks of ImageCLEFtuberculosis 2019 challenge. While our first approach uses image processing techniques for feature extraction from CT scans, our second approach uses artificial neural networks (ANN) for predicting probabilities for different lung irregularities associated with pulmonary tuberculosis and tuberculosis severity assessment. The results showed that our feature-based approach is still a competitive method that achieved rank 3 of 54 in the severity scoring subtask and rank 7 of 35 in the CT report subtask.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The tuberculosis task <ref type="bibr" coords="1,235.74,496.09,10.52,8.74" target="#b4">[5]</ref> of the ImageCLEF 2019 <ref type="bibr" coords="1,364.20,496.09,15.50,8.74" target="#b9">[10]</ref> challenge consisted of two subtasks dealing with analysis of Computed Tomography (CT) images of patients suffering from pulmonary tuberculosis. The aim of subtask #1 was the tuberculosis severity assessment based on CT scans. The subtask #2 was dedicated to the automatic generation of a CT report including the information about the left and right lung affection, presence of calcifications, presence of caverns, pleurisy, and lung capacity decrease. Both subtasks shared the same data set consisting of CT images and additional patient's meta data including information about education, imprisonment, disability, comorbidity, and others.</p><p>Last year our team participated in the severity scoring subtask at Image-CLEFtuberculosis 2018 challenge <ref type="bibr" coords="1,281.58,615.64,9.96,8.74" target="#b5">[6]</ref>. Our feature-based approach achieved rank 10 of 36 regarding the RMSE measure <ref type="bibr" coords="2,303.93,118.99,9.96,8.74" target="#b1">[2]</ref>. This result showed that our methods could compete with more complicated and computationally intensive methods in the field of deep learning. Since our feature-based approach provided a descriptive image classification framework, we decided to improve and to adapt it to the requirements of both subtasks of the ImageCLEF 2019 challenge <ref type="bibr" coords="2,450.60,166.81,9.96,8.74" target="#b4">[5]</ref>. On the other hand, taking the last years research trends into account, we developed a new deep learning-based approach.</p><p>2 Feature Based Approach for Automatic CT Report Generation and Tuberculosis Severity Scoring</p><p>In this section we describe our feature-based approach for automatic CT report and severity score prediction from CT scans. The main motive for developing a feature-based approach was the ability not only to predict the probabilities for different lung irregularities but also the ability to mark them in CT scans. This could also be helpful for physicians during manual assessment of CT scans. Furthermore, our approach provides information about the influence of different lung damages and additional patient's data on the tuberculosis severity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>Some features that we used for the automatic CT report were extracted from the original CT scans, while other features were easier to extract from binary images. Therefore, we binarized all CT scans using IsoData method <ref type="bibr" coords="2,444.09,404.02,14.61,8.74" target="#b12">[13]</ref>. We used lung masks for extraction of all features for the CT report task. Some of the lung masks that were provided by the organizers of the task <ref type="bibr" coords="2,431.19,427.93,10.52,8.74" target="#b6">[7]</ref> still did not cover large lesions. For this reason we decide to use our own lung masks extracted by the segmentation algorithm described in <ref type="bibr" coords="2,357.91,451.84,9.96,8.74" target="#b3">[4]</ref>. This algorithm examines the silhouettes of extracted masks for irregularities and reconstructs the masks. Although the reconstructed lung masks did not perfectly cover the entire lung, they still contained more lung pixels than lung masks provided by the organizers of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Automatic CT Report Generation</head><p>Presence of Calcification Pulmonary calcification in CT scans was determined for left and right lung separately depending on the number of pixels that were identified as part of calcification. Since different Hounsfield Unit (HU) ranges for pulmonary calcification in CT scans were proposed in the literature <ref type="bibr" coords="2,134.77,596.34,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="2,146.95,596.34,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="2,156.35,596.34,12.73,8.74" target="#b11">12]</ref> and the Hounsfield Units were not standardized in CT scans in the data set, we decided on a relatively large range between 300 HU and 3000 HU.</p><p>In this way, we were able to identify calcifications of different density. On the other hand, our range for calcification contains the HU range for bones that were often erroneously covered by the lung masks. To reduce the presence of bones in the examined lung area, we adjusted the lung masks in a preprocessing step by removing pixels of their boundaries along the z-axis using morphological erosion function <ref type="bibr" coords="3,223.20,130.95,15.50,8.74" target="#b10">[11]</ref> with a disk of radius four pixels. Since many CT scans contained noise patches that could be erroneously classified as calcified nodules, we removed all objects smaller than 10 pixels that were identified as calcifications. Finally, we added up the pixels of found calcifications over all CT scan slices along the z-axis in the file. If either left or right lung or both contained more than 400 calcification pixels, we stated the probability of presence of lung calcifications as 1 otherwise as 0. This threshold value was determined based on the cross-validation Area Under the ROC Curve (AUC) value for presence of calcification on the training set. Since Hounsfield Unit range for plastic and metal overlaps our range for calcification, our method for detection of calcification presence tended to false positives for patients that had medical appliances in the lung. To prevent misclassifications in such cases, the shape of found calcifications could be additionally examined.</p><p>Presence of Caverns At ImageCLEFtuberculosis 2018 <ref type="bibr" coords="3,389.15,315.82,9.96,8.74" target="#b5">[6]</ref>, we used a simple approach for detection of pulmonary caverns. The principal idea of the method was detecting caverns as dark spots surrounded by light tissue in binarized CT image slices along the z-axis <ref type="bibr" coords="3,268.55,351.69,9.96,8.74" target="#b1">[2]</ref>. The main weak point of our approach was that trachea and bronchi were incorrectly recognized as caverns. Therefore, we cut out the middle part of the lung to avoid false positives. Unfortunately, that workaround has led to many false negatives because our method did not detect caverns that were either completely or partly located in the cut out part of the lung. For this reason we improved our last year approach for detection of pulmonary caverns by examining the entire lung.</p><p>The Fleischner glossary defines pulmonary cavities as thick-walled gas-filled spaces <ref type="bibr" coords="3,165.43,447.33,9.96,8.74" target="#b8">[9]</ref>. The main difference to trachea and bronchi is that cavities are completely covered by cavity walls. Therefore, we validated a cavern in a binarized CT scan slice along the z-axis as such only if its pixels were detected as pixels of a cavern in the CT scan slices along the x-and y-axes. We estimated the volumes of pulmonary caverns and their walls for right and left lung separately by adding up the pixels of validated cavities and cavity walls over all CT image slices along the z-axis. We used these four features for training a linear regression model for predicting the presence of caverns.</p><p>Our improved method reliably detected caverns in CT scans in the training set as long as the distances between the slices in the scans were not too large so that all cavity walls were depicted in the CT images. Unfortunately, our approach still produced false positives due to artifacts on the images mainly caused by the heartbeat of patients. Therefore, an additional preprocessing step is needed for elimination of artifacts in CT scans.</p><p>Presence of Pleurisy Pleurisy is inflammation of pleura which is a thin membrane that covers the lungs <ref type="bibr" coords="3,257.76,644.16,9.96,8.74" target="#b0">[1]</ref>. Since inflammation often leads to thickening of the tissue and pleura thickening increases the distance between the lung and bones, in our approach for pleurisy detection, we compared the average distance between the boundaries of the lung masks and bones in images along the z-axis in patients with and without pleurisy. For that purpose we overlayed the lung masks and the bone masks which represent pixels of the original CT scan with Hounsfield Units between 300 and 3000. In the resulting image, we calculated the average distance between pixels of the lung mask boundaries and the nearest bone pixels. Then we averaged the distances between lung and bones over all CT scan slices along the z-axis for right and left lung separately and used them for training a linear regression model for pleurisy prediction.</p><p>Lung Capacity Decrease The lung capacity is the maximum amount of air that the lung can hold. Some kinds of lung tissue damage caused by Mycobacterium tuberculosis (MTB) bacteria may decrease the capacity of the lung. Since an automatic detection and classification of different types of lung lesions from CT scans is a challenging problem, we predicted the probability of the lung capacity decrease based on the estimated ratio of the lung tissue to the entire lung volume. Assuming that the lung tissue ratio compared to the lung volume is larger in patients with decreased lung capacity than in patients with normal lung capacity, in our approach, we did not differentiated between healthy and damaged lung tissue. Similar to our last year approach <ref type="bibr" coords="4,385.63,351.41,9.96,8.74" target="#b1">[2]</ref>, we calculated the ratio of the lung tissue as a relation of white pixels in the binarized CT image to the number of pixels in the lung mask averaged over all slices along the z-axis. Finally, we trained a linear regression model for lung capacity decrease prediction using the ratios of the lung tissue for left and right lungs as features.</p><p>Right and Left Lungs Affected Mycobacterium tuberculosis (MTB) bacteria causes more kinds of lung damage than calcifications, caverns, pleurisy, and lung capacity decrease. Therefore, the estimation model for probability of lung affection based on the probabilities for lung damage described before did not achieve satisfactory results on the training set. On the other hand, raw feature values that we extracted for predicting the probability of aforementioned lung damage, provided more information about further lesions in the lung. For this reason, we used the number of calcification pixels in the lung, average distance between the lung and bones, and the ratio of lung tissue to the lung volume for left and right lung, separately, as features for training random forests models for predicting the probabilities of affection of lungs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tuberculosis Severity Scoring</head><p>At ImageCLEFtuberculosis 2018 <ref type="bibr" coords="4,279.94,596.34,9.96,8.74" target="#b5">[6]</ref>, our system achieved its best results for tuberculosis severity score prediction using three features: the cavern volume, the volume of cavern walls, and the infection ratio <ref type="bibr" coords="4,339.69,620.25,9.96,8.74" target="#b1">[2]</ref>. This year we used data from the CT report task combined with provided patient's meta data. Using linear regression as classifier, we obtained the 5-fold cross-validation AUC of approximately 0.8 for severity score on the training set. The most important features for severity score prediction were the probability of left and right lung affection, information about the imprisonment, the probability for pleurisy, and information about education. Although some features seemed to play an insignificant role, their elimination diminished the AUC value for severity score. Since some features from meta data were very important for severity score prediction, we tested our linear regression model on the training set only using patient's meta data. We obtained AUC value of approximately 0.75. On the other hand, the linear regression model trained only using data from CT report achieved the same AUC value. Although we were aware that feature values predicted for the CT report task were inaccurate to some degree, we used them combined with provided patient's meta data for training a linear regression model for TB severity score prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Learning Based Approach</head><p>Deep Learning has been applied on solving medical relevant research questions. Among other things it is used for classification of brain and lung tumors. Thus, Liu and Kang <ref type="bibr" coords="5,198.42,352.73,15.50,8.74" target="#b16">[17]</ref> for example achieves an AUC value of 0.981 with their ANN on the LIDC-IDRI data set <ref type="bibr" coords="5,257.42,364.69,15.50,8.74" target="#b17">[18]</ref> for the binary classification of lung cancer.</p><p>In addition to the classification of the CT scans into the predefined disease stages, the task can be subdivided into a further subtask, namely the segmentation. We suspect that the occurrence of disease-typical symptoms, such as calcification, caverns and pleurisy, may help in the subsequent classification. The topic of the localization and classification of objects is the subject of many scientific publications. Some of the most promising approaches are based on the U-Net architecture <ref type="bibr" coords="5,134.77,464.84,14.61,8.74" target="#b14">[15]</ref>. This is shown, for example, by the fact that the winner of the 2018 BraTS Challenge used a U-Net variant <ref type="bibr" coords="5,276.17,476.79,14.61,8.74" target="#b15">[16]</ref>. The BraTS data set contains of CT scans of brain tumor patients and is therefor similar to the given tuberculosis data.On the one hand an advantage of the U-Net architecture is that the network considers the semantic context of the entire image during segmentation, on the other the hand U-Net architecture needs only a small amount of training examples to produce good results. Regarding the low amount of training data of the two tuberculosis tasks, this is a sufficiently important feature. We will use one architecture for both tasks, severity scoring and CT report, with the only difference being the number of final classifications to represent the different amount of possible labels. Isensee et al. showed that the architecture of the U-Nets is already so high-performant that a meaningful pre-and post-processing offers a greater potential for improvement than the change of the architecture <ref type="bibr" coords="5,413.84,608.30,14.61,8.74" target="#b13">[14]</ref>. Therefore, we start our processing pipeline with preprocessing and extend the architecture of the original U-Net <ref type="bibr" coords="5,232.36,632.21,15.50,8.74" target="#b14">[15]</ref> by an additional classification CNN. Afterwards we finish our approach with postprocessing. The exact explanation follows in the next sub-chapters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>The data set contains several anomalies which make preprocessing necessary. The CT scans in the given data set have 3 different values {-3024, -2048, -1024} for "outside of body" -mark. Probably because the images were taken by different scanners and are not standardized. For this reason, some serious jumps can be found in the value ranges of the Hounsfield Units. Beside of that, there are even higher values for some noisy pixels. Similar to <ref type="bibr" coords="6,367.41,355.44,14.61,8.74" target="#b18">[19]</ref>, we used a four-stage preprocessing to standardize the CT scans.</p><p>- -Optional Step 4: In the following the lung area is segmented with the binary masks from the original data set 1. Finally we reduced the image size by removing "0"-values in border area.</p><p>Figure <ref type="figure" coords="6,166.20,512.34,4.98,8.74" target="#fig_0">1</ref> shows the three options of preprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture</head><p>As mentioned previously, our chosen architecture is based on the original U-Net approach, but we changed the original 2D convolutional layer to 3D. Additionally we added a final classification CNN, based on the well known VGG19 architecture <ref type="bibr" coords="6,156.91,596.34,14.61,8.74" target="#b19">[20]</ref>, for a binary output, since we have a two-classes problem. Figure <ref type="figure" coords="6,475.61,596.34,4.98,8.74" target="#fig_2">2</ref> shows a draft of the resulting network architecture.</p><p>During the training and in the later classification we limit the input to 16slice sliding windows, which contains coherent slices along z-axis, for two reasons. First, this reduces the requirements on GPU memory. Second, we now have a fixed input depth without the need to scale it. This not only serves to reduce the requirements on GPU memory, but has also proven to be a useful value to enhance the precision. Complementary, a more accurate prediction is produced, because of a several classification results for each image. In the next step, we halve the image, separating it into left and right lungs. This distinction is not taken into account during training. Finally, we scale the input data to 192 × 256 with a bilinear interpolation. This results in an input tensor of 192 × 256 × 16.</p><p>For the U-Net, as segmentation network, we chose a depth of four with a number of eight filters for the first convolutional layers. We use maxpooling for the downscaling path and a transposed convolution for the upscaling path. Furthermore, batch normalization is applied after each convolution and a dropout value of 30% for the last convolutional layer in the downscaling path. As activation function we use the rectified linear unit. To get our segmentation mask we use a convolutional layer with filter size one in each direction. For task one this results in one segmentation mask, due to the fact that we have a binary classification. Contrary to that, we use five segmentation masks for task two. Even though there exist six labels in the task, we only need one probability to distinguish the affection of the left or right lung due to the splitting of the lung as preprocessing. An example of different segmentation masks for task two can be seen in Figure <ref type="figure" coords="7,213.03,520.84,3.87,8.74" target="#fig_3">3</ref>.  The segmentation mask is used as input in our final classification CNN. For the CNN we also use a depth of four with eight as number of filters for the first convolutional layers. Like for the segmentation network, batch normalisation for all and a 30% dropout for the last convolutional layer are applied. A leaky rectified linear unit is used as activation. The final layer is a dense layer with one neuron to represent the probability of the label. In task one we have one classification network, but for task two we use five independent classification networks, one for each label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Postprocessing</head><p>The network predicts the class of 16-slice windows of the CT scan. To get an overall prediction P for a whole CT-scan, an aggregation of a set of predictions has to be made. Therefore we divide each CT scan into three sections of same size. For each of these sections a prediction p i with {p i ∈ R|0 ≤ p i ≤ 1 ∧ i ∈ {1, . . . , 6}} is calculated. Taking into account the left and right half, we get a total of six results. Now we propose four methods to merge these six partial results p i into one final result P .</p><p>1. Average: The result is defined as P = {p i |i ∈ {1, . . . , 6}}, namely the average prediction value over all partial predictions. 2. Max-Rule: For this rule we define D lef t and D right as the number of lung slices in z-direction of the left respectively right lung. Also let S pos be the set of positive predictions for which holds that p i ≥ 0.5 with i ∈ {1, . . . , 6}.</p><p>Similary, S neg is the set of negative predictions defined as S neg = {p i &lt; 0.5|i ∈ {1, . . . , 6}}. Like Algorithm 1 shows, we first check if part of the lungs is missing. This occurs due to the fact that the size of the left and the right lung can diverge due to the preprocessing while reducing the zero values at the image borders. Consequently, we make the assumption that this difference is a sign of serious illness. Therefore, if the difference between D lef t and D right exceeds a threshold τ ∈ N, the maximal partial prediction value p i is chosen as probability. If the depth of the lung does not differ too much, we let the majority decide and therefore choose the maximum respectively the minimum value from the set, S pos or S neg , that has more elements. If the two sets have an equal amount of elements, the value with the smallest distance to the respective target value 0 or 1 is chosen. 3. Average-Rule: Similar to Max-Rule, the only difference is that the calculation of the resulting prediction value P does not select the maximum or minimum but the average over all values of the corresponding result set S pos respectively S neg . 4. Confidence correction: For each window of a CT scan from the validation data set, consisting of 16 slices, the coefficient which is necessary to change the prediction of the respective window, is calculated so that the classification result is the correct class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation and Results</head><p>This section shows final performance results of submitted runs in the severity scoring (subtask #1) and CT report (subtask #2) challenge. The final ranking in the severity task was done based on the Area Under the ROC Curve (AUC) value, while the final ranking in the CT report task was done based on the average AUC value. Table <ref type="table" coords="9,215.65,457.13,4.98,8.74" target="#tab_1">1</ref> summarizes the results for Top-10 submitted runs with the highest AUC value and the best run for our deep learning-based approach for severity scoring task. Table <ref type="table" coords="10,254.14,330.65,4.98,8.74" target="#tab_2">2</ref> lists the results for Top-10 submitted runs with the highest mean AUC value and the best run for our deep learning-based approach for CT report task. In the following subsection we describe the results for our approaches in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Results for the Feature Based Approach</head><p>Since we used results from the CT report task for TB severity score prediction, it is more sensible to start describing results for the CT report task. As highlighted in Table <ref type="table" coords="10,176.96,462.60,3.87,8.74" target="#tab_2">2</ref>, our best run for the feature based approach was ranked on the seventh place. In this run we predicted the probabilities for lung irregularities as described in Section 2.2. In our second best run, we predicted the probability of presence of caverns only based on the number of cavern pixels in left and right lungs, separately, omitting the pixels of cavern walls. This run was ranked on the eighth place which is a worse result. Unfortunately, we did not receive the detailed evaluation results, so we can not comment on the performance of our approach regarding prediction of other lung irregularities.</p><p>In severity scoring task, the best run for our feature based approach was ranked on the third place among 54 submitted runs. In this run we predicted the severity score using patient's meta data and the results from our best run in CT report task. The prediction of severity score in our second best run was based on patient's meta data and the results from our second best run in CT report task. Although we did not submit a run for TB severity score predicted only on the basis of provided patient's meta data, the results for these two runs showed a positive impact of results from the CT report task on the tuberculosis severity score prediction. For our evaluation we used different input data. We differentiated between train-/validation split and the complete dataset as training basis. The validation set consists of 10 images.</p><p>For Severity Score Task we set up the preprocessing, as shown in Table <ref type="table" coords="11,455.62,335.88,3.87,8.74" target="#tab_3">3</ref>. For our runs, we used either full preprocessing, just segmentation or no preprocessing at all. Run 08 is an exception, therefore we took an average of run 5, run 6 and run 7. Table <ref type="table" coords="11,193.27,371.74,4.98,8.74" target="#tab_3">3</ref> shows the list of postprocessing configurations of each run.</p><p>The highest AUC score is achieved by run 06. In this case the network got the raw input data. We presume that the good AUC score is due to the fact that the network finds relevant points outside our region of interest, which is removed through preprocessing. This can be supported by the fact that the segmentation alone generates the worst results. However, the accuracy of run 06 is lower than that of run 08. It is interesting that no neural network from those three, that we calculate the average on, can achieve such a high accuracy by itself. It seems that the networks found different features and learned differently, so in the connection they complemented each other and the accuracy increased. Surprisingly, with an accuracy of 0.453, run 02 score performed significantly worse than the other constellations. Presumably, this is because of our validation set size of only 10 images, which is potentially too small. And thus, the calculated coefficients cannot be generalized.</p><p>Since we had only a limited amount of runs for CT reportings, we decided to use only those constellations, that were trained on the whole data set. Because it seemed to be more reasonable to train on more data. Table <ref type="table" coords="11,395.68,567.94,4.98,8.74" target="#tab_4">4</ref> shows the results. The greatest value for Mean AUC of 0.6315 and Min AUC share CT R run 1 and CT R run 2. Compared to the third run, this shows, that for this task the preprocessing may be more valuable as for task 1. CT R run 3.csv shows rather moderate results of 0.561 Mean AUC, which is still better than random, but still leaves space for improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we have shown that our feature-based approach is still competitive to our deep learning-based method and to methods of other participants of the tuberculosis task. Our best run achieved the third place regarding the AUC value in the severity assessment subtask and the seventh place regarding the mean AUC value in the CT report subtask. Although the results obtained by our approach are promising, we still see potential for improvement of our approach to achieve even better results in both subtasks. Regarding that our neural network was not as deep as other networks in the literature, our results are promising. Especially the U-Net architecture seems to be beneficial and can be a good starting point for more research. Our preprocessing was only beneficial for subtask #2, which is surprising and therefore it would be interesting to investigate which parts of the lung had an effect on the resulting predictions. Data augmentation unexpectedly led to bad results in our first tests and we therefore refrained from using it. But we like to further investigate the usefulness of data augmentation for this task in combination with our network. Furthermore, we will test the network on other data sets, especially with segmentation data to train the U-Net separately. We hope that by this the segmentation layers will find meaningful areas, that can show us symptoms of such diseases. And regarding the results for subtask #2, more training epochs would be surely beneficial too and therefore the training will continue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,134.91,243.69,345.54,7.89;6,134.77,115.84,345.83,113.08"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Left: No preprocessing. Middle: Only segmentation. Right: Full preprocessing.</figDesc><graphic coords="6,134.77,115.84,345.83,113.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,151.70,386.28,328.89,8.74;6,151.70,398.23,328.89,8.74;6,151.70,410.19,328.89,8.74;6,151.70,422.14,117.92,8.74;6,140.99,433.86,339.60,8.77;6,151.70,445.84,270.01,8.74;6,140.99,457.56,183.42,8.77"><head>Step 1 :</head><label>1</label><figDesc>Remove empty gap. "NULL"-representing pixel values outside of body are often much lower than the values inside. To prevent that no area of the examination remains empty, each "NULL" -representing pixel is replaced with the next higher value. -Step 2: Removing noise by range limits. The new value range is limited to [-1000; +2000]. Outside pixel values are set to the limit value. -Step 3: Min-max normalization to [0,1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,207.13,264.38,201.08,7.89;7,134.77,115.84,345.83,133.78"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The architecture of the proposed network.</figDesc><graphic coords="7,134.77,115.84,345.83,133.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,134.77,644.07,345.83,7.89;7,134.77,655.05,119.48,7.86;7,134.77,557.29,345.83,72.01"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The left image shows the input slice. All others show the activations in the different segmentation masks.</figDesc><graphic coords="7,134.77,557.29,345.83,72.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,134.77,118.71,183.76,218.38"><head></head><label></label><figDesc>Algorithm 1 Definition of Max-Rule</figDesc><table coords="8,134.77,131.94,183.76,205.15"><row><cell>Require: τ ∈ N</cell></row><row><cell>if |D lef t -D right | &gt; τ then</cell></row><row><cell>P ⇐ max(Spos ∪ Sneg)</cell></row><row><cell>else</cell></row><row><cell>if |Spos| = |Sneg| then</cell></row><row><cell>if 1 -max(Spos) &lt; min(Sneg) then</cell></row><row><cell>P ⇐ max(Spos)</cell></row><row><cell>else</cell></row><row><cell>P ⇐ min(Sneg)</cell></row><row><cell>end if</cell></row><row><cell>else</cell></row><row><cell>if |Spos| &gt; |Sneg| then</cell></row><row><cell>P ⇐ max(Spos)</cell></row><row><cell>else</cell></row><row><cell>P ⇐ min(Sneg)</cell></row><row><cell>end if</cell></row><row><cell>end if</cell></row><row><cell>end if</cell></row><row><cell>return P</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,136.16,500.41,345.88,160.57"><head>Table 1 .</head><label>1</label><figDesc>Short overview of submitted runs for subtask 1 -Severity scoring.</figDesc><table coords="9,136.16,521.21,345.88,139.77"><row><cell>Group name</cell><cell>Run</cell><cell>AUC</cell><cell cols="2">Accuracy Rank</cell></row><row><cell>UIIP BioMed</cell><cell>SRV run1 linear.txt</cell><cell cols="2">0.7877 0.7179</cell><cell>1</cell></row><row><cell>UIIP</cell><cell>subm SVR Severity</cell><cell cols="2">0.7754 0.7179</cell><cell>2</cell></row><row><cell>HHU</cell><cell cols="4">SVR HHU DBS2 run01.txt 0.7695 0.6923 3</cell></row><row><cell>HHU</cell><cell>SVR HHU DBS2 run02.txt</cell><cell cols="2">0.7660 0.6838</cell><cell>4</cell></row><row><cell>UIIP BioMed</cell><cell>SRV run2 less features.txt</cell><cell cols="2">0.7636 0.7350</cell><cell>5</cell></row><row><cell>CompElecEngCU</cell><cell>SVR mlp-text.txt</cell><cell cols="2">0.7629 0.6581</cell><cell>6</cell></row><row><cell cols="4">San Diego VA HCS/UCSD SVR From Meta Report1c.csv 0.7214 0.6838</cell><cell>7</cell></row><row><cell cols="4">San Diego VA HCS/UCSD SVR From Meta Report1c.csv 0.7214 0.6838</cell><cell>8</cell></row><row><cell>MedGIFT</cell><cell>SVR SVM.txt</cell><cell cols="2">0.7196 0.6410</cell><cell>9</cell></row><row><cell cols="2">San Diego VA HCS/UCSD SVR Meta Ensemble.txt</cell><cell cols="2">0.7123 0.6667</cell><cell>10</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>HHU</cell><cell>run 6.csv</cell><cell cols="2">0.6393 0.5812</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,136.98,115.91,341.39,182.49"><head>Table 2 .</head><label>2</label><figDesc>Short overview of submitted runs for subtask 2 -CT report.</figDesc><table coords="10,136.98,136.71,341.39,161.69"><row><cell>Group name</cell><cell>Run</cell><cell cols="3">Mean AUC Min AUC Rank</cell></row><row><cell>UIIP BioMed</cell><cell cols="2">CTR run3 pleurisy as SegmDiff.txt 0.7968</cell><cell>0.6860</cell><cell>1</cell></row><row><cell>UIIP BioMed</cell><cell>CTR run2 2binary.txt</cell><cell>0.7953</cell><cell>0.6766</cell><cell>2</cell></row><row><cell>UIIP BioMed</cell><cell>CTR run1 multilabel.txt</cell><cell>0.7812</cell><cell>0.6766</cell><cell>3</cell></row><row><cell cols="2">CompElecEngCU CTRcnn.txt</cell><cell>0.7066</cell><cell>0.5739</cell><cell>4</cell></row><row><cell>MedGIFT</cell><cell>CTR SVM.txt</cell><cell>0.6795</cell><cell>0.5626</cell><cell>5</cell></row><row><cell>San Diego VA HCS/UCSD</cell><cell>CTR Cor 32 montage.txt</cell><cell>0.6631</cell><cell>0.5541</cell><cell>6</cell></row><row><cell>HHU</cell><cell>CTR HHU DBS2 run01.txt</cell><cell>0.6591</cell><cell cols="2">0.5159 7</cell></row><row><cell>HHU</cell><cell>CTR HHU DBS2 run02.txt</cell><cell>0.6560</cell><cell>0.5159</cell><cell>8</cell></row><row><cell>San Diego VA HCS/UCSD</cell><cell cols="2">CTR ReportsubmissionEnsemble2.csv 0.6532</cell><cell>0.5904</cell><cell>9</cell></row><row><cell>UIIP</cell><cell>subm CT Report</cell><cell>0.6464</cell><cell>0.4099</cell><cell>10</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell></cell><cell>CTR run 1.csv</cell><cell>0.6315</cell><cell>0.5161</cell><cell>12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,134.77,115.91,336.55,158.17"><head>Table 3 .</head><label>3</label><figDesc>Deep Learning-based Approach for Severity Scoring.</figDesc><table coords="11,134.77,136.71,336.55,137.37"><row><cell cols="3">Run name AUC Accuracy Preprocessing</cell><cell>Postprocessing</cell><cell>Data</cell></row><row><cell>run 06</cell><cell>0.6393 0.5812</cell><cell>-</cell><cell>method 1</cell><cell>validation split</cell></row><row><cell cols="3">run 08 1 0.6258 0.6068 mixed</cell><cell>method 1</cell><cell>validation split</cell></row><row><cell>run 04</cell><cell>0.6070 0.5641</cell><cell>complete</cell><cell>method 1</cell><cell>validation split</cell></row><row><cell>run 07</cell><cell>0.6050 0.5556</cell><cell>complete</cell><cell cols="2">method 3, τ = 5 all data</cell></row><row><cell>run 03</cell><cell>0.5692 0.5385</cell><cell>complete</cell><cell cols="2">method 3, τ = 10 validation split</cell></row><row><cell>run 05</cell><cell>0.5419 0.5470</cell><cell cols="2">segmentation only method 1</cell><cell>all data</cell></row><row><cell cols="2">baseline 0.5103 0.4872</cell><cell>complete</cell><cell cols="2">method 2, τ = 5 validation split</cell></row><row><cell>run 02</cell><cell>0.4452 0.4530</cell><cell>complete</cell><cell>method 4</cell><cell>validation split</cell></row><row><cell cols="5">4.2 Evaluation Results for the Deep Learning Based Approach</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,146.46,115.91,322.44,61.94"><head>Table 4 .</head><label>4</label><figDesc>Deep Learning-based Approach for CT Report.</figDesc><table coords="12,146.46,136.71,322.44,41.14"><row><cell>Run name</cell><cell cols="3">Mean AUC Min AUC Preprocessing</cell><cell cols="2">Postprocessing Data</cell></row><row><cell cols="2">CTR run 1.csv 0.6315</cell><cell>0.5161</cell><cell>complete</cell><cell>method 1</cell><cell>all data</cell></row><row><cell cols="2">CTR run 2.csv 0.6315</cell><cell>0.5161</cell><cell>complete</cell><cell>method 1</cell><cell>all data</cell></row><row><cell cols="2">CTR run 3.csv 0.5610</cell><cell>0.4477</cell><cell cols="2">segmentation only method 1</cell><cell>all data</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="11,144.73,656.80,158.77,7.86"><p>conglomerate of run 5, run 6 and run 7</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,138.35,513.91,313.16,7.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,248.84,513.94,83.65,7.86">Tuberculous Pleurisy</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">W</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mejia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,339.48,513.94,23.09,7.86">Chest</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="92" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,525.00,342.24,7.86;12,146.91,535.95,333.68,7.86;12,146.91,546.91,333.68,7.86;12,146.91,557.87,25.60,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,447.16,525.00,33.43,7.86;12,146.91,535.95,314.14,7.86">Feature-Based Approach for Severity Scoring of Lung Tuberculosis from CT Images</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bogomasov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Himmelspach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tatusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Conrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,146.91,546.91,333.68,7.86">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,568.93,342.24,7.86;12,146.91,579.86,333.68,7.89;12,146.91,590.85,25.60,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,204.57,568.93,276.02,7.86;12,146.91,579.89,89.80,7.86">A Quantitative Theory of the Hounsfield Unit and Its Application to Dual Energy Scanning</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,244.71,579.89,176.53,7.86">Journal of Computer Assisted Tomography</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="487" to="493" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,601.90,342.24,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,201.30,601.90,173.85,7.86">Automatic Lung Extraction from CT Scans</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Burbach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Bachelor&apos;s Thesis</note>
</biblStruct>

<biblStruct coords="12,138.35,612.96,342.24,7.86;12,146.91,623.92,333.68,7.86;12,146.91,634.88,333.67,7.86;12,146.91,645.84,333.68,7.86;12,146.91,656.80,212.65,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,146.91,623.92,333.68,7.86;12,146.91,634.88,171.38,7.86">Overview of ImageCLEFtuberculosis 2019 -Automatic CT-based Report Generation and Tuberculosis Severity Assessment</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="&lt;http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="12,340.77,634.88,139.82,7.86;12,146.91,645.84,138.31,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,119.67,342.25,7.86;13,146.91,130.63,333.68,7.86;13,146.91,141.59,333.68,7.86;13,146.91,152.55,333.68,7.86;13,146.91,163.51,22.02,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,377.45,119.67,103.14,7.86;13,146.91,130.63,333.68,7.86;13,146.91,141.59,109.57,7.86">Overview of ImageCLEFtuberculosis 2018 -detecting multi-drug resistance, classifying tuberculosis type, and assessing severity score</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="13,277.64,141.59,104.73,7.86">CLEF2018 Working Notes</title>
		<title level="s" coord="13,389.78,141.59,90.81,7.86;13,146.91,152.55,81.94,7.86">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,174.41,342.24,7.86;13,146.91,185.37,333.68,7.86;13,146.91,196.33,333.68,7.86;13,146.91,207.29,244.45,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,429.73,174.41,50.86,7.86;13,146.91,185.37,232.86,7.86">Efficient and fully automatic segmentation of the lungs in ct volumes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A</forename><surname>Jiménez Del Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,403.72,185.37,76.88,7.86;13,146.91,196.33,333.68,7.86;13,146.91,207.29,119.89,7.86">Proceedings of the VISCERAL Anatomy Grand Challenge at the 2015 IEEE International Symposium on Biomedical Imaging (ISBI)</title>
		<meeting>the VISCERAL Anatomy Grand Challenge at the 2015 IEEE International Symposium on Biomedical Imaging (ISBI)</meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,218.19,342.25,7.86;13,146.91,229.12,308.41,7.89" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,272.17,218.19,208.43,7.86;13,146.91,229.15,34.41,7.86">CT Demonstration of Calcification in Carcinoma of the Lung</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H M</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,189.02,229.15,174.16,7.86">Journal of Computer Assisted Tomography</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="867" to="871" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,240.05,342.25,7.86;13,146.91,250.98,333.68,7.89;13,146.91,261.96,60.92,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,160.01,251.01,239.01,7.86">Fleischner Society: Glossary of Terms for Thoracic Imaging</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Hansell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Bankier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Macmahon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Mcloud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">L</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Remy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,406.65,251.01,40.32,7.86">Radiology</title>
		<imprint>
			<biblScope unit="volume">246</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="697" to="722" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,272.87,337.98,7.86;13,146.91,283.82,333.68,7.86;13,146.91,294.78,333.68,7.86;13,146.91,305.74,333.67,7.86;13,146.91,316.70,333.68,7.86;13,146.91,327.66,333.68,7.86;13,146.91,338.62,333.68,7.86;13,146.91,349.58,333.68,7.86;13,146.91,360.54,333.68,7.86;13,146.91,371.50,141.36,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,221.90,327.66,258.69,7.86;13,146.91,338.62,75.92,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,243.10,338.62,237.49,7.86;13,146.91,349.58,333.68,7.86;13,146.91,360.54,76.05,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="13,230.46,360.54,170.59,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF 2019)<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="13,142.62,382.40,337.98,7.86;13,146.91,393.35,186.69,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,217.88,382.40,160.66,7.86">Erosion, dilation and related operators</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jankowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,402.68,382.40,77.92,7.86;13,146.91,393.35,158.02,7.86">Proceedings of 8th International Mathematica Symposium</title>
		<meeting>8th International Mathematica Symposium</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,404.26,337.98,7.86;13,146.91,415.21,333.68,7.86;13,146.91,426.15,74.42,7.89" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,168.28,415.21,191.67,7.86">The calcified lung nodule: What does it mean?</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Al-Jahdali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Irion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Al Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Koteyar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,363.40,415.21,117.19,7.86">Annals of Thoracic Medicine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="67" to="79" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,437.07,337.97,7.86;13,146.91,448.01,303.96,7.89" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,245.81,437.07,229.95,7.86">Picture Thresholding Using an Iterative Selection Method</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ridler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Calvard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,146.91,448.03,217.25,7.86">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="630" to="632" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,458.93,337.98,7.86;13,146.91,469.89,333.68,7.86;13,146.91,480.85,121.03,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,221.71,458.93,47.43,7.86">No New-Net</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,398.46,458.93,82.14,7.86;13,146.91,469.89,224.90,7.86">Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="234" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,491.75,337.97,7.86;13,146.91,502.71,333.68,7.86;13,146.91,513.67,243.44,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,306.44,491.75,174.15,7.86;13,146.91,502.71,95.24,7.86">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,248.74,502.71,231.85,7.86;13,146.91,513.67,121.74,7.86">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,524.57,337.97,7.86;13,146.91,535.53,333.67,7.86;13,146.91,546.49,160.61,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,227.54,524.57,253.05,7.86;13,146.91,535.53,172.48,7.86">Brain tumor segmentation and radiomics survival prediction: contribution to the BRATS 2017 challenge</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,341.17,535.53,139.41,7.86;13,146.91,546.49,38.06,7.86">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,557.39,337.98,7.86;13,146.91,568.35,333.68,7.86;13,146.91,579.31,140.09,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,229.44,557.39,251.16,7.86;13,146.91,568.35,34.95,7.86">Multiview convolutional neural networks for lung nodule classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1002/ima.22206</idno>
		<ptr target="https://doi.org/10.1002/ima.22206" />
	</analytic>
	<monogr>
		<title level="j" coord="13,206.68,568.35,124.08,7.86">Int. J. Imaging Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="12" to="22" />
			<date type="published" when="2017">2017</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,590.21,322.31,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,225.27,590.21,91.11,7.86">Data From LIDC-IDRI</title>
		<author>
			<persName coords=""><forename type="first">Iii</forename><surname>Armato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,324.34,590.21,114.92,7.86">The Cancer Imaging Archive</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,601.11,337.97,7.86;13,146.91,612.07,333.68,7.86;13,146.91,623.03,332.50,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,353.69,601.11,126.90,7.86;13,146.91,612.07,269.61,7.86">Convolutional Neural Networks for Multidrug-resistant and Drug-sensitive Tuberculosis Distinction</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Singhof</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tatusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Conrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,437.46,612.07,43.13,7.86;13,146.91,623.03,185.04,7.86">CLEF2017 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,633.93,337.98,7.86;13,146.91,644.88,252.36,7.86" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="13,274.05,633.93,206.54,7.86;13,146.91,644.88,73.50,7.86">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv 1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
