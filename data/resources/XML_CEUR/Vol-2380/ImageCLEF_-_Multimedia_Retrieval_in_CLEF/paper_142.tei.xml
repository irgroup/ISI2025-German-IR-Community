<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.86,115.96,297.63,12.62;1,145.87,133.89,323.62,12.62;1,235.20,151.82,144.96,12.62">Forged File Detection and Steganographic content Identification (FFDASCI) using Deep Learning Techniques</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,192.44,189.73,65.61,8.74"><roleName>Dr</roleName><forename type="first">M</forename><surname>Srinivas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Technology Warangal Telangana</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.47,189.73,60.62,8.74"><forename type="first">Akshay</forename><surname>Nayak</surname></persName>
							<email>anayak@student.nitw.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Technology Warangal Telangana</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,353.81,189.73,69.10,8.74"><forename type="first">Abhishek</forename><surname>Bhatt</surname></persName>
							<email>abhishekbhatt900@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Technology Warangal Telangana</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.86,115.96,297.63,12.62;1,145.87,133.89,323.62,12.62;1,235.20,151.82,144.96,12.62">Forged File Detection and Steganographic content Identification (FFDASCI) using Deep Learning Techniques</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CE0DA67ACD76C4148FF303BEAAD82715</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Transfer Learning</term>
					<term>Optimizer</term>
					<term>Activation Function</term>
					<term>Loss Function</term>
					<term>Adadelta</term>
					<term>Categorical cross entropy</term>
					<term>Down sampling</term>
					<term>Memory footprint</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our contribution in the identification and detection of Forged files and Steganographic content using Deep Neural Networks like Convolutional Neural Network and 3D-RESNET. We have used CNN in our research as CNN's are inspired by visual cortex. In other words, they are designed to extract consequential features which are relevant in classification i.e. the ones which minimizes the loss function. In this the kernel weights are learned by Gradient Descent so as to generate the perceptive features from images fed to the network which in result supplemented to fully connected layer that performs the final classification task. In our proposed approach we mainly consider the two different tasks. Firstly, Identification of Forged Images has been carried out in which detection of altered images which includes both extension and signature has been performed. In addition to this, we have predicted the original epitome of forged file by using convolutional neural network model which automatically classify them and are useful for large-scale image classification as it has increased ConvNet depth. Secondly, we have recognized the Steganographic content by applying 3D-RESNET. Here, we have given preference to Residual Networks in place of VGG16 as increasing the depth should increase the accuracy of network, as long as over-fitting is taken care of. In VGG16 increased depth is increasing the effect of vanishing gradient and degradation problem. In this work, ImageCLEF 2019 data set is used for identification of Forged Images and recognized the Steganographic content.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since the advancement of Internet, one of the important concerns has been the security of information. The creation of Cryptography has been made for securing the secrecy of communication and many methods have been identified for encrypting and decrypting data in order to keep the message secret. In addition of keeping the contents of the message secret, it may also be necessary to keep the existence of the message secret. Here, comes the Steganography which is being considered as the art and science of invisible communication. It has been very easy to conceal confidential information inside files. Steganography <ref type="bibr" coords="2,459.21,154.86,10.52,8.74" target="#b0">[1]</ref> is the practice of concealing files, messages, images and videos within another files, messages, images or videos. The word Steganography combines the Greek words stego meaning "covered" and graphics meaning "writing". Images are one of the most usual and efficient cover media for hiding the data. There are various problems associated with file forgery which we are discussing in this paper are as follows. Firstly, the digital forensics are skipping many important and useful content during investigations. They are unknowingly treating various image files as pdf files due to the modification of those files by changing their extension to pdf format which is the major fallback in their investigation. Secondly, there is a chance of sharing of illegal information by hiding the criminal action from the plain sight and invisible those files in front of investigators. Traditional based features extraction techniques <ref type="bibr" coords="2,268.56,298.32,10.52,8.74" target="#b1">[2]</ref> are more complicated, not optimized one and lack of discriminative capacity for stego images. By using deep learning based features give high level semantic information and more discriminate <ref type="bibr" coords="2,433.65,322.23,9.96,8.74" target="#b2">[3]</ref>.</p><p>This paper, reconciles the above-mentioned problems by using deep neural networks. Few years back, due to the less availability of required efficient data set, Machine Learning algorithms were very efficient with those sized data sets. The problem with it was of defining our own features that were to be learned by our model. It was Supervised Learning which made it even more complicated and complex. As the data set grows on and we have to implement on the sufficiently large amount of data set, all these models were not be able to perform well resulting in the emergence of Deep Learning field where the whole network is not fully connected type. Only the last few layers are fully connected layers as they are connected to every element of the input volume by reducing the extra number of Hyper-parameters. Deep Learning algorithms are working very well with the large data set applying supervised learning <ref type="bibr" coords="2,368.57,467.61,10.52,8.74" target="#b3">[4]</ref> where the model itself generates the weights and its feature vectors by training the fully connected layer. This is accomplished by making the kernel smaller than the input which means that we need to store fewer parameters, which both reduces the memory requirements of the model and improves its statistical efficiency. In this work, we use ImageCLEF 2019 challenging <ref type="bibr" coords="2,305.45,527.38,10.52,8.74" target="#b4">[5]</ref> data sets. In this data set contains various kind of task related to real time applications such as ImageCLEFCoral, ImageCLEFlifelog, ImageCLEFmedical and ImageCLEFsecurity. In this work, we select the ImageCLEFsecurity related task <ref type="bibr" coords="2,345.77,563.25,10.52,8.74" target="#b5">[6]</ref> for identification of Forged Images and have recognized the Steganographic content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Previous Research Work</head><p>There are few research works that have been performed and is going on for forged file detection and identification of steganographic content.</p><p>Forged File Detection Few research works have been performed in which the simplest and the naive method of looking only at file extensions. As there are various ways in which type of a files can be detected without even opening them. This is not very useful when one should have to detect large number of files since volume of files determine the detection speed. The use of extensions in file type detection is not efficient as extensions are easily spoofed and altered. It is easy to change the extension by several mouse clicks in various operating system. It is not necessary to open a file during classifying the files based on their extensions, in similar manner it is not necessary for to mislead these classification techniques. In open source OS like Linux, extensions are not required for the extensionbased file type detection. This OS is allowing optional extensions of any string regardless of file type which results in hiding these files from an inexperienced administrator.</p><p>File Type Detection using Magic Bytes is one of the most sophisticated method for file type detection. Magic Bytes are specific to binary files and rely on matching signatures which are varying in length in file headers or tails. Due to inadequate standards for the content in files, the new file type creators will include headers for uniquely identifying files of their type. For example, letters PK has been present at the beginning of every .zip file in order to identify the Zip format files as ZIP file format had been invented by PKWARE. This method is usually slower in checking the file extension as files are being opened for reading the small number of bytes for deciding the file format. If it matches the expected result then the given file is in specific format, else it is treated as suspicious. It works only on binary file types having magic bytes associated with them. This is the cons of magic bytes type detection when the person has to consider the risk associated with ignoring detection of ASCII based files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Steganographic Content Identification</head><p>Active approach is performed for determining the steganographic images. Digital Images require pre-processing such as watermarking or signatures like fingerprinting are being generated during image creation. But this is not very efficient or useful in authenticating the image when the internet is not having the large number of water marked and digital signatured images. Passive approach is the most expedient forgery detection technique called as blind forgery detection. The blind name factor ascends as it uses the received image for the originality check without any modification in image at the time of creation and capture. Copy-Move Detection Technique and Slicing are another passive approach where in the CMDT a part of an image is copied and pasted in some other location within the same image and in Slicing one or more images get combined together to form a new image. But copying a part of an image or slicing an image are not useful in finding whether an image is hiding some text behind it. Some sample of stego and non stego images are show in Fig. <ref type="figure" coords="3,191.41,656.12,4.98,8.74">1</ref> Fig. <ref type="figure" coords="4,212.76,274.06,4.13,7.89">1</ref>. Some of the stego and non-stego images samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Proposed Approach</head><p>For detecting a forged file, a pdf file has been given as an input to the network for reading each and every byte <ref type="bibr" coords="4,284.25,344.04,10.52,8.74" target="#b6">[7]</ref> and the frequency of each byte is being counted. After that a histogram has been plotted using the frequency distribution data table <ref type="bibr" coords="4,182.34,367.95,9.96,8.74" target="#b7">[8]</ref>. In the second stage of our work, the resulting histogram is being supplemented to VGG16 <ref type="bibr" coords="4,247.56,379.90,15.50,8.74" target="#b14">[16]</ref> which have been trained on the custom data for classifying the file types. The resulting output of the model has been predicted the actual type of file supplemented to it. The files have been predicted as images and are being stored as dataset for the next stage of proposed model resulting in classifying the image as stego or non-stego after boosting the ResNet50 <ref type="bibr" coords="4,465.09,427.72,15.50,8.74" target="#b11">[13]</ref> with image classified files.</p><p>Generally, ASCII values are varying in the range of [0, 255] resulting in the availability of 256 numbers of bins in range [0, 255]. The x-axis represents byte value and the y-axis represents the frequency of each byte value. The histogram could be generated in two formats. 1. Grey scale format <ref type="bibr" coords="4,381.95,488.12,10.52,8.74" target="#b8">[9]</ref> and 2. RGB format <ref type="bibr" coords="4,134.77,500.08,9.96,8.74" target="#b8">[9]</ref>. We have decided to go with RGB formatted histogram as it is convenient to amplify RGB image in VGG16 network as it takes the input images in three channels. Else, we have to convert the grey scaled images to an image having 3 channels containing the same pixels for each channel.  There are some layers present in between the convolutional and pooling layer which is called as ReLU playing an important role by only keeping the positive values. The Dense layer which is being worked as fully connected layer has SoftMax as an activation function <ref type="bibr" coords="5,288.20,557.83,15.50,8.74" target="#b10">[11]</ref> having 4 classes at last. For file forgery detection histogram based features and the fine-tuned VGG16 model is in our approach method and have shown in Fig. <ref type="figure" coords="5,318.47,581.74,36.19,8.74" target="#fig_2">3 stage-I</ref>.</p><p>After the classification of files as images using CNN based classification method, next we have crammed another network called ResNet50 <ref type="bibr" coords="5,435.28,608.30,15.50,8.74" target="#b11">[13]</ref> for to check the images are stego or non stego. We are supplementing our network with large dataset and for deep analysis we have considered ResNet50 as an important model having 50 layers for identifying the altered images hiding the steganographic content. ResNet50 has 25,583,592 trainable parameters and hav- ing 53,120 non-trainable parameters. We have trained the model with our custom dataset consisting of two types of image, one is stego images and the another is non-stego images. This time the network has been trained by the images itself instead of supplementing with the histograms of each type of images classifying the images into stego and non-stego. Fig. <ref type="figure" coords="6,315.12,387.04,4.98,8.74" target="#fig_2">3</ref> is showing the complete architecture of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Dataset</head><p>In this work, we used the dataset provided by ImageCLEFsecurity 2019 <ref type="bibr" coords="6,448.75,449.85,10.52,8.74" target="#b5">[6]</ref> containing of 9,000 images for three different kind of tasks. We have supplemented our Network model with a total of 2400 histogram images of first task. We have processed the images and have used RGB format in our model. We have trained the network with the customized dataset. The root folder named data contains four sub folders named GIF, JPG, PDF and PNG containing 400, 400, 1200 and 400 histogram images respectively which are representing each of the file type. We have divided the dataset in training and testing sets and 80% of the total data from dataset is used for training our model and the remaining 20% have been used for testing purpose. In second task, dataset for stego image classification consists of 1000 images dividing into 500 stego and 500 non-stego images indexed from 0001 02 to 1000 02.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Experimental Results</head><p>In this work, the performance of the proposed system is evaluated by measuring classification accuracy. In our FFDASCI model we have considered "categorical crossentropy" <ref type="bibr" coords="6,196.58,656.12,15.50,8.74" target="#b12">[14]</ref> as the loss function, "adadelta" as an optimiser, "metrices" as an accuracy standard and "SoftMax" as an activation function.For forgery detection task we have trained the VGG16 network on our custom forgery detection dataset for 12 epochs with 32 as batch size. We have achieved 99.93% validation accuracy and have also tested with Support Vector Machine (SVM) classifier on same forgery detection task dataset. With SVM classifier we achieved 99.73% of validation accuracy and the accuracy results are shown in Table <ref type="table" coords="7,417.59,178.77,3.87,8.74">1</ref>.4.  For stego image classification task, we have trained the ResNet50 network for the same number of epochs and having batch size as similar of VGG16. We have achieved 99.9% validation accuracy with the proposed method. In this task, we have also used SVM classifier to classify the stego image and 93.5% classification results are being achieved. Table <ref type="table" coords="7,279.40,512.54,4.24,8.74">1</ref>.4 shows the classification results of proposed method and SVM classification results On stego image data. Table <ref type="table" coords="7,177.43,644.16,4.24,8.74">1</ref>.4 shows the class wise accuracy for stego image discovery. We have been secured the testing accuracy of 99.90% accuracy from our proposed model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Conclusion</head><p>The histograms produced by frequency distribution of bytes of each file type are being separate from each other for File Type Detection. We have proposed a model which is useful in classifying the forged files and have been able to classify whether the files are stego or not. We have decreased the computational time complexity in detecting the forged files and steganographic content. There is no need of opening the file for file type detection which relieves from magic bytes strategy. As if human cortex can detect the forged file images, certainly network which would be specifically designed for this is more powerful and speeding up the detection time. Our model is efficient in identifying the steganographic content which were earlier skipped by the digital investigators. Using this each and every hidden and forged files are bring able to identify and helps in investigation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,374.70,535.94,105.89,8.74;4,134.77,547.90,345.83,8.74;4,134.77,559.85,345.83,8.74;4,134.77,571.81,64.59,8.74;4,149.71,584.39,330.88,8.74;4,134.77,596.34,345.83,8.74;4,134.77,608.30,345.82,8.74;4,134.77,620.25,345.82,8.74;4,134.77,632.21,345.83,8.74;4,134.77,644.16,345.82,8.74;4,134.77,656.12,345.83,8.74"><head></head><label></label><figDesc>The resulting histogram has been used in the proposed model for the file type identification can be analyzed by seeing the Fig 2, in which each file type has their own histogram representation. The histogram of each with varying file type, either may be of JPG, GIF, PNG or PDF format is different from each other. For classifying the actual file type, VGG16 [10] model plays an important role as an image classifier. We have re-trained the above mentioned VGG16 model with obtained histogram images for GIFs, JPGs, PDFs and PNGs using Transfer Learning which enables us to use pre-trained model by changing the output classifying layer. The neural network is generating "weights" by training it on very large dataset. These extracted</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,387.80,345.83,7.89;5,134.77,398.78,191.63,7.86;5,134.77,115.84,345.83,257.19"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Different type of files corresponding Histogram images. (a) gif file (b) jpg file (c)pdf file (d) png file related histogram images</figDesc><graphic coords="5,134.77,115.84,345.83,257.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,222.05,305.28,171.25,7.89;6,285.11,217.38,111.40,51.02"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Block diagram of FFDASCI model</figDesc><graphic coords="6,285.11,217.38,111.40,51.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.77,209.66,345.83,121.42"><head>Table I :</head><label>I</label><figDesc>Performance comparison of the proposed method with SVM classifier on forgery detection validation dataset.By using SVM classifier we calculate the class wise classification performance. Table1.4 shows the class wise accuracy for the file forgery detection using SVM classifier with histogram features.</figDesc><table coords="7,209.89,240.62,195.57,30.61"><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>Proposed Method (VGG16)</cell><cell>99.93</cell></row><row><cell>SVM</cell><cell>99.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,141.67,353.24,332.01,84.28"><head>Table II :</head><label>II</label><figDesc>Class wise performance results of the SVM classifier on forgery detection validation dataset.</figDesc><table coords="7,209.88,384.19,195.60,53.32"><row><cell cols="5">Classes Precision Recall F1-score Support</cell></row><row><cell>gif</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>69</cell></row><row><cell>jpg</cell><cell>0.99</cell><cell>1.00</cell><cell>0.99</cell><cell>83</cell></row><row><cell>pdf</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>138</cell></row><row><cell>png</cell><cell>1.00</cell><cell>0.99</cell><cell>0.99</cell><cell>79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,140.68,555.39,334.00,61.56"><head>Table III :</head><label>III</label><figDesc>Performance comparison of the proposed method with SVM classifier on stego image validation dataset.</figDesc><table coords="7,204.94,586.35,205.47,30.61"><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>Proposed Method (ResNet50)</cell><cell>99.9</cell></row><row><cell>SVM</cell><cell>93.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,150.50,115.94,314.35,61.56"><head>Table IV :</head><label>IV</label><figDesc>Class wise performance results of the SVM classifier on stego image validation dataset.</figDesc><table coords="8,186.88,146.90,241.59,30.61"><row><cell>Classes</cell><cell cols="4">Precision Recall F1-score Support</cell></row><row><cell>Non-Stego image</cell><cell>0.51</cell><cell>0.70</cell><cell>0.59</cell><cell>93</cell></row><row><cell>Stego image</cell><cell>0.62</cell><cell>0.42</cell><cell>0.50</cell><cell>107</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,395.01,342.24,7.86;8,146.91,405.97,333.68,7.86;8,146.91,416.93,159.79,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,265.49,395.01,210.83,7.86">An Introduction to Image Steganography Techniques</title>
		<author>
			<persName coords=""><forename type="first">Alaa</forename><forename type="middle">A</forename><surname>Altaay</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jabbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,169.67,405.97,310.92,7.86;8,146.91,416.93,92.21,7.86">International Conference on Advanced Computer Science Applications and Technologies (ACSAT)</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="122" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,427.58,342.24,7.86;8,146.91,438.54,333.68,7.86;8,146.91,449.50,260.69,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,303.53,427.58,177.06,7.86;8,146.91,438.54,162.79,7.86">Classification of medical images using edgebased features and sparse representation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Krishna</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,342.57,438.54,138.02,7.86;8,146.91,449.50,204.06,7.86">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,460.15,342.24,7.86;8,146.91,471.11,333.68,7.86;8,146.91,482.07,126.70,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,382.27,460.15,98.32,7.86;8,146.91,471.11,140.04,7.86">Deep dictionary learning for fine-grained image classification</title>
		<author>
			<persName coords=""><forename type="first">Mettu</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,317.70,471.11,162.89,7.86;8,146.91,482.07,70.68,7.86">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,492.72,342.24,7.86;8,146.91,503.68,333.68,7.86;8,146.91,514.64,237.96,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,375.94,492.72,104.65,7.86;8,146.91,503.68,225.03,7.86">Learning deep and sparse feature representation for fine-grained object recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,403.58,503.68,77.01,7.86;8,146.91,514.64,181.21,7.86">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,525.29,342.24,7.86;8,146.91,536.25,333.68,7.86;8,146.91,547.21,333.68,7.86;8,146.91,558.17,333.68,7.86;8,146.91,569.13,333.68,7.86;8,146.91,580.09,333.68,7.86;8,146.91,591.04,333.68,7.86;8,146.91,602.00,333.68,7.86;8,146.91,612.96,333.68,7.86;8,146.91,623.92,333.68,7.86;8,146.91,634.88,333.68,7.86;8,146.91,645.84,333.68,7.86;8,146.91,656.80,192.73,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,199.24,580.09,281.35,7.86;8,146.91,591.04,333.68,7.86;8,146.91,602.00,333.68,7.86;8,146.91,612.96,111.33,7.86;8,306.07,612.96,174.52,7.86;8,146.91,623.92,333.68,7.86;8,146.91,634.88,125.31,7.86">Friedrich and Alba Garcia Seco de Herrera and Narciso Garcia and Ergina Kavallier-atou and Carlos Roberto del Blanco and Carlos Cuevas Rodriguez and Nikos Vasil-lopoulos and Konstantinos Karampidis and Jon Chamberlain and Adrian Clark and An-tonio</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cid</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitri</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aleh</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph M ;</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,282.71,634.88,197.88,7.86;8,146.91,645.84,131.09,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="8,308.43,645.84,168.19,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12</date>
		</imprint>
	</monogr>
	<note>ImageCLEF 2019: Multimedia Retrieval in Medicine, Lifelogging, Se-curity and Nature, Experimental IR Meets Multilinguality, Multimodality, and Interac-tion</note>
</biblStruct>

<biblStruct coords="9,138.35,119.67,342.24,7.86;9,146.91,130.63,333.68,7.86;9,146.91,141.59,263.61,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,399.76,130.63,80.83,7.86;9,146.91,141.59,109.97,7.86">Overview of the Im-ageCLEFsecurity 2019 Task</title>
		<author>
			<persName coords=""><forename type="first">Konstantinos</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikos</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Cuevas Rodrguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Roberto Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ergina</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Narciso</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,267.99,141.59,112.63,7.86">CLEF working notes, CEUR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,152.55,342.24,7.86;9,146.91,163.51,333.67,7.86;9,146.91,174.47,186.69,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,459.30,152.55,21.29,7.86;9,146.91,163.51,333.67,7.86;9,146.91,174.47,49.83,7.86">Comparison of Classification Algorithms for File Type Detection A Digital Forensics Perspective</title>
		<author>
			<persName coords=""><forename type="first">Konstantinos</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ergina</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giorgos</forename><surname>Papadourakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,205.80,174.47,36.24,7.86">POLIBITS</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page">1520</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,185.43,342.24,7.86;9,146.91,196.39,333.68,7.86;9,146.91,207.34,333.68,7.86;9,146.91,218.30,221.19,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,394.20,185.43,86.39,7.86;9,146.91,196.39,333.68,7.86;9,146.91,207.34,86.81,7.86">A Comparative Analysis of Histogram Equalization based Techniques for Contrast Enhancement and Brightness Preserving</title>
		<author>
			<persName coords=""><forename type="first">Raju</forename><surname>Aedla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Dwarakish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reddy</forename><surname>Venkat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,241.47,207.34,239.12,7.86;9,146.91,218.30,113.08,7.86">International Journal of Signal Processing, Image Processing and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="353" to="366" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,229.26,342.24,7.86;9,146.91,240.22,281.78,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,265.80,229.26,214.80,7.86;9,146.91,240.22,21.76,7.86">A Theory Based on Conversion of RGB image to Gray image</title>
		<author>
			<persName coords=""><forename type="first">Tarun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karun</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,176.10,240.22,188.06,7.86">International Journal of Computer Application</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,251.18,337.97,7.86;9,146.91,262.14,283.05,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,330.53,251.18,150.06,7.86;9,146.91,262.14,115.02,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.62,273.10,337.98,7.86;9,146.91,284.06,301.63,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="9,260.22,273.10,220.37,7.86;9,146.91,284.06,128.43,7.86">Activation Functions: Comparison of trends in Practice and Research for Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">Chigozie</forename><surname>Nwankpa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03378</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.62,316.93,337.97,7.86;9,146.91,327.89,333.68,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,233.99,316.93,193.25,7.86">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,458.45,316.93,22.14,7.86;9,146.91,327.89,266.61,7.86">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,338.85,337.98,7.86;9,146.91,349.81,333.68,7.86;9,146.91,360.77,333.68,7.86;9,146.91,371.73,48.38,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,340.29,338.85,140.30,7.86;9,146.91,349.81,244.47,7.86">Automated problem identification: Re-gression vs classification via evolutionary deep networks</title>
		<author>
			<persName coords=""><forename type="first">Emmanuel</forename><surname>Dufourq</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><forename type="middle">A</forename><surname>Bassett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,404.06,349.81,76.54,7.86;9,146.91,360.77,329.65,7.86">Proceedings of the South African Institute of Computer Scientists and Information Technologists</title>
		<meeting>the South African Institute of Computer Scientists and Information Technologists</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,382.69,337.97,7.86;9,146.91,393.65,333.68,7.86;9,146.91,404.61,163.95,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,229.59,382.69,251.00,7.86;9,146.91,393.65,91.30,7.86">Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition</title>
		<author>
			<persName coords=""><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,268.11,393.65,212.48,7.86;9,146.91,404.61,87.15,7.86">IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="3154" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,415.56,337.98,7.86;9,146.91,426.52,333.68,7.86;9,146.91,437.48,73.72,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,253.19,415.56,227.40,7.86;9,146.91,426.52,14.75,7.86">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName coords=""><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,189.33,426.52,291.26,7.86">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
