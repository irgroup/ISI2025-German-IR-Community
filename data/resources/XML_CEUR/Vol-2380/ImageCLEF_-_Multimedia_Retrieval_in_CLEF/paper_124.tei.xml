<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.96,151.03,280.53,13.60;1,198.83,166.99,197.63,13.60">An Encoder-Decoder model for visual question answering in the medical domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,151.20,205.09,61.81,9.65"><forename type="first">Imane</forename><surname>Allaouzi</surname></persName>
							<email>imane.allaouzi@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="institution">Abdelmalek Essaâdi University Faculty of Sciences and Techniques Tangier</orgName>
								<address>
									<country key="MA">Morocco</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,284.28,205.09,88.16,9.65"><forename type="first">Mohamed</forename><forename type="middle">Ben</forename><surname>Ahmed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="institution">Abdelmalek Essaâdi University Faculty of Sciences and Techniques Tangier</orgName>
								<address>
									<country key="MA">Morocco</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.28,205.09,63.66,9.65"><forename type="first">Badr</forename><surname>Benamrou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="institution">Abdelmalek Essaâdi University Faculty of Sciences and Techniques Tangier</orgName>
								<address>
									<country key="MA">Morocco</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,162.96,151.03,280.53,13.60;1,198.83,166.99,197.63,13.60">An Encoder-Decoder model for visual question answering in the medical domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">62C81C93F1B10857803425EEF2589E8D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Transfer Learning</term>
					<term>Encoder-Decoder</term>
					<term>CNN</term>
					<term>LSTM</term>
					<term>Word Embedding</term>
					<term>Language Modeling</term>
					<term>Medical Imaging</term>
					<term>Visual Question Answering</term>
					<term>Greedy Search</term>
					<term>Beam Search</term>
					<term>NLP</term>
					<term>Computer Vision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the task of VQA-Med of ImageCLEF 2019. We proposed an encoder-decoder model that takes as input a medical question-image pair and generates an answer as output. The encoder network consists of a pre-trained CNN model that extracts prominent features from a medical image and a pre-trained word embedding along with LSTM to embed textual data. The answer generation is accomplished by the greedy search algorithm, which predicts the next word based on the previously generated words. Thus, the answer is built up by recursively calling the model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>With the widespread adoption of electronic medical record (EMR) systems, a large amount of medical information is becoming available such as doctors' reports, test results and medical images. This health information is a gold mine for artificial intelligence (AI) researchers who seek to enhance doctors' ability to analyze medical images, to support clinical decision making and improve patient engagement. One of the most exciting and challenging AI tasks is the visual question answering in the medical domain (VQA-Med) <ref type="bibr" coords="1,244.20,538.10,10.68,9.65" target="#b0">[1]</ref>. The main idea of VQA-Med system is to predict the right answer given a medical image accompanied with clinically relevant question. It is a difficult task because the computer system must understand and analyze the question (natural language processing, or NLP) as well as understand and process the image (computer vision).</p><p>Different approaches have been proposed to address the task of VQA-Med. Some of them treat the task as a generative problem generating answers in a comprehensive and well-formed textual description <ref type="bibr" coords="1,270.22,622.11,10.68,9.65" target="#b1">[2]</ref>, while others treat it as a multi-label classification problem in which the answer is chosen from among different choices <ref type="bibr" coords="2,124.68,162.37,10.91,9.65" target="#b2">[3,</ref><ref type="bibr" coords="2,135.59,162.37,7.27,9.65" target="#b3">4]</ref>.</p><p>This paper describes our participation in the task of VQA-Med of ImageCLEF 2019 <ref type="bibr" coords="2,149.77,186.37,10.60,9.65" target="#b4">[5]</ref>. We proposed an encoder-decoder model that takes as input a medical question-image pair and generates an answer as output. The encoder network consists of a pre-trained CNN model that extracts prominent features from a medical image and a pre-trained word embedding along with Long Short-Term Memory (LSTM) to embed textual data. The answer generation is accomplished by the greedy search algorithm, which predicts the next word based on the previously generated words.</p><p>Thus, the answer is built up by recursively calling the model.</p><p>The rest of this paper is organized as follows. Section 2 describes details of the provided dataset. Section 3 gives a detailed description of the proposed system. Section 4, presents metrics used to assess the performance of our system and also provides a presentation and analysis of the experimental results, and finally Section 5 concludes the presented work with some remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>VQA-Med dataset consists of 3,200 training medical images and 12,792 Question-Answer (QA) pairs, a validation set of 500 medical images with 2,000 QA pairs, and a test set of 500 medical images with 500 questions. Four categories of questions are considered: Modality, Plane, Organ system and Abnormality. The answer can be either "a single word", "a phrase containing 2-21 words", or "a yes/no". Table <ref type="table" coords="2,251.05,435.38,4.98,9.65">1</ref> illustrates some examples of medical images with associated question-answer pairs. Table <ref type="table" coords="2,148.80,471.16,3.41,8.72">1</ref>. Examples of medical images with associated question-answer pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medical Image</head><p>Question Answer</p><p>What part of the body is being imaged here? Skull and contents.</p><p>Which plane is the image shown in? Axial.</p><p>What abnormality is seen in the image? Right aortic arch with aberrant left subclavian artery.</p><p>Is this a t1 weighted image?</p><p>Yes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>To address the problem of VQA in the medical domain, we proposed an encoderdecoder model that takes as input a medical question-image pair and generates an answer as output. As shown in figure <ref type="figure" coords="3,278.15,283.09,3.77,9.65" target="#fig_0">1</ref>, the encoder network consists of a pre-trained DenseNet-21 model that extracts prominent features from the medical image and a pre-trained word embedding followed by two LSTM layers to embed the question and extract textual features. The textual and image features are concatenated together into one vector "QI vector". Our proposed model generates one word at a time. That is, all words generated so far are embedded, with the same word embedding used for questions, and each of word embedding is fed then into an LSTM with 1024 units.</p><p>The distributed representation of all words generated so far is concatenated with the "QI vector" to form an "encoder vector". The decoder uses the encoder vector as input in order to generate the next word, this is then fed to a fully connected layer of 256 neurons and then to the final layer, which has one neuron for each word in the output vocabulary and a softmax activation function to output a likelihood of each word in the vocabulary being the next word in the answer. Thus, the answer is built up by recursively calling the model with the previously generated words. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image encoding:</head><p>Our proposed model is a deep learning network with a high number of parameters. This type of model often overfits when training on small datasets. To prevent overfitting, the best solution is to use the transfer learning technique. The idea is to use a pre-trained CNN model on a large and similar dataset as a fixed feature extractor, as we expect higher-level features in the CNN to be relevant to our dataset as well.</p><p>Motivated by the results obtained by DenseNet-121 model on the task of medical image classification <ref type="bibr" coords="4,207.46,256.94,11.71,9.65" target="#b5">[6]</ref> and since we don't have a large dataset, we used a pre-trained DenseNet-121 on chexpert <ref type="bibr" coords="4,241.05,268.94,10.68,9.65" target="#b6">[7]</ref>, a large dataset of thorax chest-x-ray images. The network has four dense blocks, which have 6, 12, 24, 16 dense layers respectively. A dense block consists of a series of units. Each unit packs two convolutions, each preceded by Batch Normalization and ReLU activations. In addition, each unit generates a fixed number of feature vectors. This parameter, called growth rate, controls the amount of new information that layers can transmit. The layers between these dense blocks are transition layers which perform down-sampling of the features passing the network. A detailed explanation of DenseNet-121 architecture used in this work is shown in Table <ref type="table" coords="4,221.12,364.95,3.77,9.65" target="#tab_0">2</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question encoding:</head><p>To capture the sequential nature of language data, we modeled our questions using LSTM, a special type of Recurrent Neural Networks (RNNs). LSTM has demonstrated great success in various NLP tasks and is the state of the art algorithm for sequential data. It succeeds in being able to capture information about previous states to better inform the current prediction through its memory cell state. An LSTM consists of three main components: a forget gate, input gate, and output gate. These gates determine whether or not to let new input in (input gate), delete the information because it isn't important (forget gate) or to let it impact the output at the current time step (output gate).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Memory block in LSTM network.</head><p>A pre-trained word embedding <ref type="bibr" coords="5,258.59,577.69,11.71,9.65" target="#b7">[8]</ref> on biomedical texts from MEDLINE/PubMed using gensim's Word2Vec implementation is used to provide a distributed representation of words. Word Embeddings are much better at capturing the context around the words than using a one hot vector for every word. For this problem we used 200 dimension word embeddings and we did not tune them during the training process since we did not have sufficient data. These embeddings are passed into two LSTM layers with respectively 512 and 1024 units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer generation:</head><p>To predict an answer for a given image-question pair, we treated the task as text generation. This often operates by generating probability distributions across the vocabulary of output words and it is up to decoding algorithms to sample the probability distributions to generate the most likely sequences of words. To find the best decoder algorithm both greedy search and beam search are evaluated.</p><p>• Greedy search: A greedy algorithm uses a heuristic for making locally optimal choices at each step with the hope of finding a global optimum solution. This means that the algorithm chooses the most likely word in each step in the output sequence and does not take into account the entire sentence. Therefore, the quality of the final output sequence may be far from optimal, hence it is considered greedy.</p><p>• Beam search: Unlike greedy search, beam search allows for non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. The beam search expands all possible next steps and keeps the k most likely words, where k is a userspecified parameter and controls the number of beams or parallel searches through the sequence of probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.4</head><p>Dropout:</p><p>The proposed model is a deep neural network model and is trained on a small dataset. As a result, the model can learn statistical noise in the training data, resulting in poor performance and generalization on new testing data "overfitting". To reduce overfitting and improve generalization error, we used the dropout technique. Dropout is a very computationally cheap and remarkably effective regularization method. It works by randomly removing or dropping out inputs to a layer. This has the effect of making nodes in the network generally more robust to the inputs and reduces the number of training parameters, hence reduces the training time and memory requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Methodology:</head><p>Before applying the evaluation metrics, each answer undergoes the following preprocessing techniques:</p><p>• Lower-case: Converts each answer to lower-case.</p><p>• Tokenization: Divides the answer into individual words.</p><p>• Remove punctuation: Remove punctuation marks from answers.</p><p>Evaluation metrics used to evaluate our proposed VQA-Med model are:</p><p>• Accuracy (Strict):</p><p>The entire predicted answer must match the ground truth answer. • BLEU <ref type="bibr" coords="7,206.27,200.17,10.89,9.65" target="#b8">[9]</ref>: Capture the similarity between a system-generated answer and the ground truth answer.</p><p>Three experiments are conducted to evaluate our model:</p><p>• Expr1: Answers are generated using greedy search.</p><p>• Expr2: Answers are generated using beam search with k=2.</p><p>• Expr3: Answers are generated using beam search with k=3.</p><p>Our model is trained using RMSprop optimizer with an initial learning rate of 0.001 which is multiplied by 10 each time the validation loss plateau after an epoch. We have used a mini-batch size of 535 samples, a number of epochs up to 100, and the categorical cross-entropy as a loss function where the best model was selected based on the validation loss.</p><p>As shown in Table3, experiment 1 (Expr 1) achieves best results with a strict accuracy of 0.556 and BLEU score of 0.583. This means that for our case, greedy search is better than beam search algorithm. The following table provides the results obtained by our model and the three best run for the task of VQA-Med. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,136.08,678.28,194.37,8.72;3,131.16,460.08,333.00,216.00"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1.The proposed architecture for VQA-Med 2019.</figDesc><graphic coords="3,131.16,460.08,333.00,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,124.68,388.72,333.42,301.71"><head>Table 2 .</head><label>2</label><figDesc>The DenseNet-121 architecture.</figDesc><table coords="4,128.16,408.28,329.94,282.15"><row><cell>Layers</cell><cell>Output Size</cell><cell>DenseNet-121</cell></row><row><cell>Convolution</cell><cell>112x112</cell><cell>7x7 conv, stride2</cell></row><row><cell>Pooling</cell><cell>56x56</cell><cell>3x3 max pool, stride 2</cell></row><row><cell>Dense Block 1</cell><cell>56x56</cell><cell>1x1 conv</cell></row><row><cell></cell><cell></cell><cell>x 6</cell></row><row><cell></cell><cell></cell><cell>3x3 conv</cell></row><row><cell>Transition Layer 1</cell><cell>56x56</cell><cell>1x1 conv</cell></row><row><cell></cell><cell>28x28</cell><cell>2x2 average pool, stride 2</cell></row><row><cell>Dense Block 2</cell><cell>28x28</cell><cell>1x1 conv</cell></row><row><cell></cell><cell></cell><cell>x 12</cell></row><row><cell></cell><cell></cell><cell>3x3 conv</cell></row><row><cell>Transition Layer 2</cell><cell>28x28</cell><cell>1x1 conv</cell></row><row><cell></cell><cell>14x14</cell><cell>2x2 average pool, stride 2</cell></row><row><cell>Dense Block 3</cell><cell>7x7</cell><cell>1x1 conv</cell></row><row><cell></cell><cell></cell><cell>x24</cell></row><row><cell></cell><cell></cell><cell>3x3 conv</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,124.68,150.17,337.62,126.02"><head>Table 2 .</head><label>2</label><figDesc>The DenseNet-121 architecture (continued)</figDesc><table coords="5,128.16,169.72,334.14,106.47"><row><cell>Layers</cell><cell>Output Size</cell><cell>DenseNet-121</cell></row><row><cell>Transition Layer 3</cell><cell>14x14</cell><cell>1x1 conv</cell></row><row><cell></cell><cell>7x7</cell><cell>2x2 average pool, stride 2</cell></row><row><cell>Dense Block 4</cell><cell>7x7</cell><cell>1x1 conv</cell></row><row><cell></cell><cell></cell><cell>x 16</cell></row><row><cell></cell><cell></cell><cell>3x3 conv</cell></row><row><cell>Output</cell><cell>1x1</cell><cell>7x7 global average pool</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,124.68,419.68,284.42,69.44"><head>Table 3 .</head><label>3</label><figDesc>Experimental results on test dataset.</figDesc><table coords="7,128.88,439.24,280.22,49.88"><row><cell>Experiment</cell><cell>Accuracy</cell><cell>BLEU</cell></row><row><cell>Expr1</cell><cell>0.556</cell><cell>0.583</cell></row><row><cell>Expr2</cell><cell>0.538</cell><cell>0.556</cell></row><row><cell>Expr3</cell><cell>0.526</cell><cell>0.547</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,124.68,539.56,345.99,139.10"><head>Table 4 .</head><label>4</label><figDesc>Comparison with the three best VQA-Med methods.As shown in Table4, the best model achieved an accuracy of 0.624 and a BLUE score of 0.644. This means that it exceeds the results of our model with only 0.068 in terms of accuracy and 0.061 in terms of BLUE score. As a result, we can say that our model gives very good results.</figDesc><table coords="7,128.88,559.24,280.22,63.68"><row><cell>Model</cell><cell>Accuracy</cell><cell>BLEU</cell></row><row><cell>Hanlin</cell><cell>0.624</cell><cell>0.644</cell></row><row><cell>yan</cell><cell>0.62</cell><cell>0.64</cell></row><row><cell>minhvu</cell><cell>0.616</cell><cell>0.634</cell></row><row><cell>Our model (LIST)</cell><cell>0. 556</cell><cell>0.583</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion:</head><p>In this paper, we propose an Encoder-Decoder model for the task of visual question answering in the medical domain. VQA is a difficult and challenging task since it combines the fields of Computer Vision and NLP. This difficulty increases even more with the inherent nature of medical imaging. Our proposed model achieves great results with an accuracy of 0,556 and BLEU score of 0,583.To further substantiate these results, several improvements could be made such as the use of an attention mechanism that allows to pay more attention to specific regions that better represent the question instead of the whole image.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,132.65,314.20,337.96,8.72;8,141.71,325.25,328.79,8.72;8,141.71,336.16,283.55,8.72" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,173.75,325.25,296.75,8.72;8,141.71,336.16,62.98,8.72">VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,211.08,336.16,95.82,8.72">CLEF2019 Working Notes</title>
		<title level="s" coord="8,313.08,336.16,108.33,8.72">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.65,347.21,337.98,8.72;8,141.72,358.25,236.39,8.72" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,297.26,347.21,169.09,8.72">JUST at VQA-Med: A VGG-Seq2Seq Model</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Talafha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,141.72,358.25,117.90,8.72">CLEF 2018 Labs Working Notes</title>
		<title level="s" coord="8,265.81,358.25,108.45,8.72">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.65,369.17,337.80,8.72;8,141.72,380.21,328.96,8.72;8,141.72,391.25,172.05,8.72" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,335.62,369.17,134.82,8.72;8,141.72,380.21,257.02,8.72">Deep Neural Networks and Decision Tree classifier for Visual Question Answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Allaouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Benamrou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ben Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,406.33,380.21,64.35,8.72;8,141.72,391.25,53.58,8.72">CLEF 2018 Labs Working Notes</title>
		<title level="s" coord="8,201.48,391.25,108.43,8.72">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.65,402.17,337.93,8.72;8,141.71,413.21,328.90,8.72;8,141.71,424.25,46.29,8.72" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,295.32,402.17,175.26,8.72;8,141.71,413.21,127.95,8.72">UMass at ImageCLEF Medical Visual Question Answering (Med-VQA) 2018 Task</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,276.85,413.21,121.88,8.72">CLEF 2018 Labs Working Notes</title>
		<title level="s" coord="8,405.87,413.21,64.75,8.72;8,141.71,424.25,42.43,8.72">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.65,435.17,337.93,8.72;8,141.71,446.21,328.87,8.72;8,141.71,457.26,328.58,8.72;8,141.71,468.17,328.68,8.72;8,141.71,479.22,328.89,8.72;8,141.71,490.26,328.91,8.72;8,141.71,501.18,328.80,8.72;8,141.71,512.22,328.87,8.72;8,141.71,523.26,234.58,8.72" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,401.03,490.26,69.59,8.72;8,141.71,501.18,258.95,8.72">ImageCLEF 2019: Multimedia Retrieval in Medicine, Lifelogging, Security and Nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liauchukn</forename><forename type="middle">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Roberto Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cuevas Rodriguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,408.83,501.18,61.68,8.72;8,141.71,512.22,328.87,8.72;8,141.71,523.26,181.75,8.72">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 10th International Conference of the CLEF Association</title>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.63,534.18,338.06,8.72;8,141.71,545.22,280.88,8.72" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,285.23,534.18,185.46,8.72;8,141.71,545.22,155.48,8.72">A Novel Approach for Multi-Label Chest X-Ray Classification of Common Thorax Diseases</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Allaouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ben Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,303.23,545.22,47.18,8.72">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64279" to="64288" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.63,556.27,337.96,8.72;8,141.71,567.18,328.87,8.72;8,141.70,578.23,328.84,8.72;8,141.67,589.27,328.87,8.72;8,141.67,600.19,328.92,8.72;8,141.67,611.23,44.73,8.72" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,291.43,589.27,179.11,8.72;8,141.67,600.19,148.74,8.72">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ciurea-Ilcus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chute</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Haghgoo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shpanskaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seekins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Halabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">P M</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,307.78,600.19,162.80,8.72;8,141.67,611.23,41.29,8.72">Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.59,622.27,337.93,8.72;8,141.67,633.19,328.90,8.72;8,141.67,644.23,280.65,8.72" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,351.91,622.27,118.61,8.72;8,141.67,633.19,149.15,8.72">Deep Relevance Ranking Using Enhanced Document-Query Interactions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Brokos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,298.87,633.19,171.70,8.72;8,141.67,644.23,206.46,8.72">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2018)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP 2018)<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.59,655.28,337.93,8.72;8,141.67,666.19,328.86,8.72;8,141.67,677.24,205.06,8.72" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,353.53,655.28,116.99,8.72;8,141.67,666.19,131.24,8.72">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,309.56,666.19,160.96,8.72;8,141.67,677.24,152.00,8.72">ACL-2002: 40th Annual meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>PDF</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
