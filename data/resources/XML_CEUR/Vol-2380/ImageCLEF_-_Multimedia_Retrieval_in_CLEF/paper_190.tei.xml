<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.76,114.32,291.83,14.35;1,150.38,132.25,314.60,14.35;1,249.11,150.18,117.12,14.35">TUA1 at ImageCLEF 2019 VQA-Med: A classification and generation model based on transfer learning</title>
				<funder ref="#_wfRzgHx">
					<orgName type="full">JSPS KAKENHI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.86,188.35,65.87,9.96"><forename type="first">Yangyang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tokushima University</orgName>
								<address>
									<postCode>770-8506</postCode>
									<settlement>Tokushima</settlement>
									<country>JP</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.03,188.35,39.92,9.96"><forename type="first">Xin</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tokushima University</orgName>
								<address>
									<postCode>770-8506</postCode>
									<settlement>Tokushima</settlement>
									<country>JP</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,359.84,188.35,37.65,9.96"><forename type="first">Fuji</forename><surname>Ren</surname></persName>
							<email>ren@is.tokushima-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokushima University</orgName>
								<address>
									<postCode>770-8506</postCode>
									<settlement>Tokushima</settlement>
									<country>JP</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.76,114.32,291.83,14.35;1,150.38,132.25,314.60,14.35;1,249.11,150.18,117.12,14.35">TUA1 at ImageCLEF 2019 VQA-Med: A classification and generation model based on transfer learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F98DC71795423DDB6B3F83D683A1904C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>VQA-Med</term>
					<term>Inception-Resnet-v2</term>
					<term>BERT</term>
					<term>seq2seq</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this article, we describe a method for answering questions based on medical images in the ImageCLEF VQA-Med 2019 task. The VQA-Med 2019 mission involves four categories: modality, plane, organ system, and abnormality. We try to turn the strong artificial intelligence problem into multiple weak artificial intelligence problems. First, we use a simple classifier to distinguish the four categories by training questions only. Transfer learning is useful in easing the overfitting problem of neural network on small datasets. Then, we use Inception-ResNet-V2 and Bidirectional Encoder Representation from Transformers pre-training model as feature extractors to deal with medical images and questions, respectively. Next, we use additional classifiers to get answers in the modality, plane and organ system categories. Last, we use sequence-to-sequence model as a generator to get the answer in the abnormality category. Our submission ranks fourth, based on accuracy metric and BLEU metric, which shows that our method can effectively get answers from medical images and related questions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VQA-Med2019 [?]</head><p>[?] is a visual question answering (VQA) task in the medical domain. Health has consistently been our concern. Artificial intelligence (AI) has made significant breakthroughs in different tasks such as lesion recognition. Compared with image recognition, VQA task needs to further understand the content in images, which is more difficult. The VQA-Med2019 covers 36 image modalities including CT, MR, etc., 10 separate organ systems, 16 planes, and various abnormalities. Some examples are provided in Fig. <ref type="figure" coords="1,410.13,579.23,4.13,9.96" target="#fig_0">1</ref>. Most medical datasets are created for a single condition, like lung cancer. And VQA-Med looks at a variety of conditions and hopes to help physicians and patients in diagnosis by establishing a question and answer system. The result we submitted gets 0.606 accuracy score, and 0.633 BLEU score, ranking fourth, which shows that our method can effectively get answers from medical images and related questions.</p><p>The rest of this paper is structured as follows. Section 2 briefly reviews the work related to the VQA-Med task. Section 3 provides details of the method we proposed. Section 4 reports our experimental results and evaluation results. Section 5 presents conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In this task, we are using the VQA approach. VQA tasks involve image processing and natural language processing. The open domain VQA tasks have various question objects and ways. Most VQA tasks are classified tasks. The existing method of processing VQA task is mainly the Hierarchical Co-Attention Model [?], which has a good performance in classification.</p><p>Some hospitals have begun using computer-aided diagnostic tools to advise physicians. However, Most of the existing auxiliary diagnoses are only for a certain field, such as lung cancer. As for the question and answering, due to the diversity of answers, the evaluation is quite difficult. So far the accuracy rate is far from the level of human physicians. In addition, datasets in the medical field, especially datasets involving multiple diseases, are rare, and medical image datasets of many research institutions are not disclosed.</p><p>The VQA-Med2019 data set involves a variety of conditions, but the existing technology cannot deal with different types of pictures such as MR, CT, and different organ parts such as brain, lungs, and abdomen at the same time. We try to build classifiers and generator to simplify this strong AI problem. As mentioned in section 1, we propose a model for VQA-Med2019. This system is primarily composed of multiple classifiers and a generator. As is shown in Fig. <ref type="figure" coords="3,163.37,654.89,4.13,9.96" target="#fig_1">2</ref>, simple classifier is used to classify questions, additional classifiers are used for modality, plane and organ system questions, generator correspond to abnormality problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>For medical images, we use data enhancement methods to randomly shift, shear, and scale the image. We have not used methods such as rotation and inversion because there may be position-related questions. As for the texts, we convert all the questions and answers into lowercase letters and remove the punctuation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classifier</head><p>Since the test set does not provide the categories, we first train a simple classifier to classify questions. The simple classifier can divide the dataset into four distinct categories: modality, plane, organ system, and abnormality by observing only the questions. On this basis, we train three additional classifiers for modality, plane, and organ system problems, respectively. The data part of the next section describes the answers to these three types of questions are under a variety of options. Three additional classifiers extract features from images and questions: the features of the medical images are extracted by the IRV2 pre-trained on the Imagenet [?]; the features of the questions are extracted by the weighted public BERT pre-training model. After that, we use Multi-Layer Perception (MLP) to unify the feature dimensions of the images and questions and concatenate them together. Then, we use the classification layer to output the categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generator</head><p>The generator is utilized to solve abnormality questions. There are not any options for this category, and all the answers need to be generated by the model from understanding the information of the images. The generator is built based on sequence-to-sequence (seq2seq) model. The encoder part, similar to the classifier, uses IRV2 to extract features of the medical images and concatenates the questions features extracted by the BERT as the initial state of the decoder. The decoder bases on a long-short term memory [?] network. We put "sos" token as the initial input, continuously loops to generate the probability distribution of the next word, and puts the most likely word as the input of the next moment until the output is "eos" token. In the prediction part, we use beam search [?] to output the answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>The VQA-Med2019 training set contains 3200 medical images and clearly identifies four categories of questions and answers (Q&amp;A) pairs of 12792 pairs; the validation set contains 500 images and 2000 Q&amp;A pairs; the test set contains 500 images and 500 questions, and no label for the type of questions. For modality, plane, organ system, the dataset gives the options for the answers (refer to the readme document in the dataset). As for the abnormality part, the answer involves a variety of specific disease types, and there are no options for us to choose. This dataset involves a wide range of medical images and Q&amp;A pairs, close to the real medical environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation metrics</head><p>We choose the accuracy metric commonly used in VQA task. The accuracy metric considers the exact match between the answer and the fact.Besides, we choose BLEU [?] metric commonly used in machine translation. BLEU metric calculates the frequency of words that appear together between the answer and the fact, in other words, the similarity between the answer and the fact. Both metrics are automatically scored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Result</head><p>We submit just one run. The result we submitted gets 0.606 accuracy score, and 0.633 BLEU score. This result ranks fourth in the leaderboard. Compare with the first ranking, we are 0.018 score behind in accuracy and 0.011 score behind in BLEU.</p><p>After comparing with the published ground truth, we found that in the categories of plane and organ system, both classification accuracies exceed 70%, but in modality, the accuracy is less than 70%. It may be due to the large number of modality options and the small difference between them (there are 17 suboptions of MR, such as "MR-STIR", "MR-FIESTA", etc.), which is difficult to distinguish.</p><p>As for the category of abnormality, the result we generated is quite different from the ground truth. Perhaps because of the small amount of data, the process of answer generation is much more related to the frequency of words in the training set (such as "multiforme", "glioblastoma", etc.), rather than the medical images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this article, we describe our participation in VQA-Med2019 Task: to answer questions based on the medical images. We use a simple classifier to categorize the questions, then use the additional classifiers or generator for different types of questions to get the corresponding answers. In this process, we use transfer learning including IRV2 and BERT to prevent over-fitting problem that might have on small dataset. We obtain 0.606 accuracy score, and 0.633 BLEU score, indicating that our proposed method has a certain effect. It may be explained by the wide range of datasets and the small number of training for a particular condition. This method is far from being a human physician.</p><p>Our future work will focus on making the answers more accurate. Consider that there is a certain amount of noise in the dataset, such as Some CT images labelled as x-ray images. We may improve the accuracy of the answers by correcting or eliminating the noise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,327.81,345.82,8.97;2,134.77,338.77,101.50,8.97;2,134.77,115.82,345.83,198.30"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. VQA-Med2019 contains four categories of questions: modality, plane, organ system, and abnormality.</figDesc><graphic coords="2,134.77,115.82,345.83,198.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,586.29,345.83,8.97;3,134.77,597.25,48.18,8.97;3,134.77,353.14,345.83,219.45"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An overview of our proposed model architecture which has four classifiers and a generator.</figDesc><graphic coords="3,134.77,353.14,345.83,219.45" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research has been partially supported by <rs type="funder">JSPS KAKENHI</rs> Grant Number <rs type="grantNumber">19K20345</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wfRzgHx">
					<idno type="grant-number">19K20345</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,288.79,200.46,8.97;6,359.47,288.79,121.12,8.97;6,151.53,299.75,329.06,8.97;6,151.53,310.71,329.06,8.97;6,151.53,321.66,304.48,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,167.32,299.75,313.27,8.97;6,151.53,310.71,33.74,8.97">Vqa-med: Overview of the medical visual question answering task at imageclef 2019</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="6,207.22,310.71,273.37,8.97;6,151.53,321.66,14.99,8.97">CLEF 2019 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,331.91,337.64,8.97;6,151.53,342.87,329.06,8.97;6,151.53,353.83,281.94,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,219.50,342.87,261.08,8.97;6,151.53,353.83,120.52,8.97">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,142.96,364.08,337.63,8.97;6,151.53,375.04,329.06,8.97;6,151.53,385.99,199.03,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,403.67,364.08,76.92,8.97;6,151.53,375.04,134.03,8.97">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,330.58,375.04,150.01,8.97;6,151.53,385.99,93.94,8.97">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,396.24,337.63,8.97;6,151.53,407.20,329.06,8.97;6,151.53,418.16,25.61,8.97" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,346.98,396.24,133.61,8.97;6,151.53,407.20,189.88,8.97">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,142.96,428.41,337.63,8.97;6,151.53,439.36,329.06,8.97;6,151.53,450.32,329.06,8.97;6,151.53,461.28,202.53,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,416.94,428.41,63.65,8.97;6,151.53,439.36,264.15,8.97">Overview of the ImageCLEF 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="6,437.46,439.36,43.13,8.97;6,151.53,450.32,61.28,8.97">CLEF2018 Working Notes</title>
		<title level="s" coord="6,223.10,450.32,179.06,8.97">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,471.53,337.63,8.97;6,151.53,482.49,92.89,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,288.77,471.53,100.71,8.97">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,398.68,471.53,81.91,8.97">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,492.74,337.64,8.97;6,151.53,503.69,329.07,8.97;6,151.53,514.65,329.06,8.97;6,151.53,525.61,329.06,8.97;6,151.53,536.57,329.06,8.97;6,151.53,547.53,329.06,8.97;6,151.53,558.49,329.06,8.97;6,151.53,569.45,329.06,8.97;6,151.53,580.41,329.06,8.97;6,151.53,591.37,220.94,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,268.31,547.53,212.28,8.97;6,151.53,558.49,124.24,8.97">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,297.67,558.49,182.91,8.97;6,151.53,569.45,329.06,8.97;6,151.53,580.41,122.45,8.97">Proceedings of the Tenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="6,305.56,580.41,171.05,8.97">LNCS Lecture Notes in Computer Science</title>
		<meeting>the Tenth International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 09-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="6,142.96,601.61,337.64,8.97;6,151.53,612.57,145.62,8.97" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,316.41,601.61,164.19,8.97;6,151.53,612.57,116.95,8.97">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,622.82,337.63,8.97;6,151.53,633.78,329.06,8.97;6,151.53,644.74,329.06,8.97;6,151.53,655.70,98.06,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,355.50,622.82,125.09,8.97;6,151.53,633.78,133.71,8.97">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,306.57,633.78,174.02,8.97;6,151.53,644.74,162.49,8.97">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,118.57,337.97,8.97;7,151.53,129.53,329.06,8.97;7,151.53,140.49,146.26,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,360.34,118.57,120.25,8.97;7,151.53,129.53,200.57,8.97">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,372.46,129.53,108.13,8.97;7,151.53,140.49,117.58,8.97">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,151.45,337.97,8.97;7,151.53,162.41,180.62,8.97" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02960</idno>
		<title level="m" coord="7,256.92,151.45,223.67,8.97;7,151.53,162.41,14.76,8.97">Sequence-to-sequence learning as beam-search optimization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
