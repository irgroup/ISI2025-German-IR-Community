<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.90,152.67,313.46,12.64;1,157.22,170.67,280.64,12.64">ImageSem at ImageCLEFmed Caption 2019 Task: a Two-stage Medical Concept Detection Strategy</title>
				<funder ref="#_ygKZ76J">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_vFbtsyq">
					<orgName type="full">Non-profit Central Research Institute Fund of Chinese Academy of Medical Sciences</orgName>
				</funder>
				<funder ref="#_dRFTqUR">
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder ref="#_dAYvh3b #_qMtbAeZ">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,200.33,209.70,40.13,8.96"><forename type="first">Zhen</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information / Medical Library</orgName>
								<orgName type="institution">Chinese Academy of Medical Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Peking Union Medical College</orgName>
								<address>
									<postCode>100020</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.81,209.70,55.24,8.96"><forename type="first">Xuwen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information / Medical Library</orgName>
								<orgName type="institution">Chinese Academy of Medical Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Peking Union Medical College</orgName>
								<address>
									<postCode>100020</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.31,209.70,40.22,8.96"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information / Medical Library</orgName>
								<orgName type="institution">Chinese Academy of Medical Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Peking Union Medical College</orgName>
								<address>
									<postCode>100020</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,360.77,209.70,27.46,8.96"><forename type="first">Jiao</forename><surname>Li</surname></persName>
							<email>li.jiao@imicams.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information / Medical Library</orgName>
								<orgName type="institution">Chinese Academy of Medical Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Peking Union Medical College</orgName>
								<address>
									<postCode>100020</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.90,152.67,313.46,12.64;1,157.22,170.67,280.64,12.64">ImageSem at ImageCLEFmed Caption 2019 Task: a Two-stage Medical Concept Detection Strategy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BA521D49F0791C87ED5E8E306D158DEA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Concept Detection</term>
					<term>Transfer Learning</term>
					<term>Multi-label Classification</term>
					<term>Pre-classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the participation of the Image Semantics group (ImageSem) of the Institute of Medical Information at the ImageCLEFmed Caption task, which was launched by ImageCLEF 2019. The Concept Detection subtask aims at identifying 5,528 semantic concepts from 70,786 training images and 10,000 test images. In this study, we proposed the two-stage concept detection strategy, including the medical image pre-classification based on body parts and the transfer learning-based multi-label classification model. We totally submitted 10 runs in the final evaluation. The evaluation results showed that we achieved an F1 Score of 0.2235, which ranked 8th overall. There is still a great room for improving the performance of concept detection from large-scale medical images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>Introduction 1   The ImageCLEF task <ref type="bibr" coords="1,213.65,482.75,11.69,8.96" target="#b0">[1]</ref> contributes to enhancing the computational methods for machine understandable medical images <ref type="bibr" coords="1,278.92,494.75,10.90,8.96" target="#b1">[2,</ref><ref type="bibr" coords="1,292.95,494.75,7.28,8.96" target="#b2">3]</ref>. ImageCLEFmed Caption 2019 <ref type="bibr" coords="1,434.35,494.75,11.75,8.96" target="#b3">[4]</ref> focus on the concept detection task, which aims to identify the UMLS <ref type="bibr" coords="1,389.48,506.75,12.07,8.96" target="#b4">[5]</ref> Concept Unique Identifiers (CUIs) for a given medical image from the biomedical literature. On behalf of the Institute of Medical Information, Chinese Academy of Medical Sciences, our Image Semantics group (ImageSem) participated in the concept detection task of Im-ageCLEFmed Caption 2019, and submitted 10 runs to the final evaluation. Fig. <ref type="figure" coords="1,154.46,566.75,4.98,8.96" target="#fig_1">1</ref> shows the workflow and submissions of ImageSem in ImageCLEFmed Caption 2019. On the basis of data analysis and preprocessing, we applied two kinds of concept detection methods. For one thing, we referenced our previous work in Im-ageCLEFcaption 2018 task <ref type="bibr" coords="1,234.53,602.75,10.60,8.96" target="#b5">[6]</ref>, and applied the transfer learning-based multi-label classification model to the overall training set to predict high-frequency concepts. For another thing, we proposed a two-stage medical concept detection strategy. Specifically, for a given medical image, a pre-classification model was used to determine which body part the image belongs to, and multiple labels were predicted based on the corresponding multi-label classification model. Finally we collected useful concepts using different concept selection strategies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data analysis</head><p>The ImageCLEFmed Caption 2019 task provides a subset of the Radiology Objects in COntext (ROCO) dataset <ref type="bibr" coords="2,231.01,499.31,10.72,8.96" target="#b6">[7]</ref>. To focus on radiology images and non-compound figures, automatic filtering with deep learning systems as well as manual revisions were applied, reducing the dataset to 70,786 radiology images of several medical imaging modalities. It is further divided into a training set (56,629 images) and a validation set (14,157 images). In the concept detection task, a set of CUIs was provided for each image, totally 5,528 annotated concepts (CUIs). Table <ref type="table" coords="2,342.30,559.31,4.98,8.96" target="#tab_0">1</ref> shows the concept distribution in the overall dataset, and Table <ref type="table" coords="2,255.85,571.31,4.98,8.96" target="#tab_1">2</ref> shows the top ranked concepts in the training set. It is observed that the high-frequency concepts account for most proportion (about 97.7%) of the overall occurrence in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data preprocessing</head><p>Selecting concepts and images for multi-label classification models. Considering the uneven concept distribution in table 1, we define the problem of detecting high-frequency concepts from medical images as a multi-label classification task. For training the multi-label classification model, we selected 87 CUIs appeared in more than 1,000 medical images, 548 CUIs appeared in 100 to 1,000 images, and 1,263 CUIs appeared in 10 to 100 images, respectively. Then we extracted all the medical images containing high-frequency CUIs from the training set and constructed corresponding subsets, namely F1000, F100, and F10. For each medical image, we filtered out low-frequency CUIs.</p><p>Backtracking semantic types of CUIs and manual image annotation for pre-classification. To realize the pre-classification of medical images based on different body parts, we backtracked the semantic types of all CUIs from the UMLS and selected useful TUIs for automatically assigning images to different body parts, e.g. T023, which stands for "Body Part, Organ, or Organ Component" includes multiple body-related CUIs. Then concepts with T023 were automatically extracted and manually classified to corresponding body parts. We extracted images annotated with pre-defined concepts of corresponding categories, and manually check each image subset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In the ImageCLEFcaption 2018 task, we applied two methods to identify multiple concepts for a specific image, including the transfer learning-based multi-label classifica-tion model and the image retrieval-based topic model <ref type="bibr" coords="4,341.35,150.18,10.60,8.96" target="#b5">[6]</ref>. The experimental results indicated that the transfer learning-based multi-label classification method was robust on high-frequency concept detection across different data sets, while the image retrievalbased topic models identified the high-frequency concepts and low-frequency concepts at the same time, but depended heavily on the quality of the retrieved images.</p><p>In the ImageCLEFmed Caption 2019 task, for one thing, we continued to use the transfer learning-based multi-label classification model to identify high frequency concepts, for another thing, we paid more attention to the distinction of labels between images of different body parts, and classified medical images based on body parts before the concept detection process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transfer learning-based multi-label classification</head><p>The problem of detecting high-frequency concepts from medical images was viewed as a multi-label classification task, and Convolutional Neural Networks (CNNs) was employed to assign one or multiple CUIs to a specific medical image. We used the Inception-V3 <ref type="bibr" coords="4,159.02,344.25,11.72,8.96" target="#b7">[8]</ref> and ResNet152 <ref type="bibr" coords="4,239.00,344.25,10.67,8.96" target="#b8">[9]</ref>, which were pre-trained on the ImageNet datasets including 1.2 million images with more than 1,000 common object classes <ref type="bibr" coords="4,431.70,356.25,15.74,8.96" target="#b9">[10]</ref>. The fully-connected layer before the last softmax layer was replaced and the parameters of the pre-trained CNN model were transferred as the initial parameters of our multi-label classification model. During the training process, we selected 87 CUIs appeared in more than 1,000 medical images in the training set as high-frequency labels, and collected corresponding medical images from the training set, namely F1000 subset. Then we fine-tuned network weights layer by layer and adjust parameters based on the validation set. For a given test image, top N concepts which prediction probability higher than the threshold were selected as the predicted labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Medical image pre-classification based on body parts</head><p>By observing the radiology images of the ROCO dataset from the ImageCLEFmed Caption 2019 task, and analyzing the semantic type of some concept CUIs, we were inspired to cluster the images into different categories based on different kinds of body parts. First, we summarized four body part-related categories based on the medical imaging reading diagnostic atlas <ref type="bibr" coords="4,238.25,574.31,15.43,8.96" target="#b10">[11]</ref>, including "abdomen", "chest", "head and neck" and "skeletal muscle". Second, we cluster concepts in the training set according to their semantic type, e.g., concepts with the TUI number T023 (Body Part, Organ, or Organ Component) or T029 (Body Location or Region) were automatically extracted and classified to corresponding categories. Third, some part of medical images with annotated concepts in the training set were classified into different categories. We manually double check the images being assigned to different categories and created four body partbased image-concepts subset. Finally, we employed the AlexNet <ref type="bibr" coords="4,391.15,660.26,16.76,8.96" target="#b11">[12]</ref> model to automatically classify the rest of medical images in the training set to different categories, as well as the validation set and the test set, which achieved the best accuracy of 84.73% on the validation set. We had also applied other networks to perform pre-classification, such as the ResNet152 and the Inception V3, however, the complex network structure showed no significant advantage in the classification performance. Table <ref type="table" coords="5,422.59,174.18,4.98,8.96" target="#tab_2">3</ref> shows the distribution of medical images in different body part categories. Then we could train multi-label classification models on different medical image categories, respectively </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Two-stage medical concept detection</head><p>On the basis of the above works, we proposed a two-stage medical concept detection model. For a given medical image, the computer will firstly determine which body part the given image belongs to, after the pre-classification step, multiple labels will be predicted based on the corresponding multi-label classification model, the Inception V3 model we used in this study. Different concept selection strategies were also applied to different categories, such as using concept of frequency higher than 100, output top N concepts, or concepts with score above a specific threshold, etc. Then we combined the best output of different categories, which evolved plenty of combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted Runs</head><p>We submitted the following 10 runs of concept detection to the ImageCLEFmed Caption 2019 task (see Table <ref type="table" coords="5,227.74,509.63,3.75,8.96" target="#tab_3">4</ref>): </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run1 (F1TOP1):</head><p>This submission employed the two-stage concept detection strategy, in which medical images were firstly pre-classified into different body parts using Alexnet, then multiple concepts were predicted for the given image using multi-label classification models trained on the corresponding image subset. The max epoch was set to 30 and the learning rate was set to 0.001. For the images in the test set, we selected concepts with frequency above 100 in the training set as the training labels. If the given image was classified to the "abdomen" or the "chest" subset, output the top7 concepts of corresponding multi-label classification model. If the given image belongs to the "head &amp; neck" or the "skeletal muscle" subset, output the top 5 concepts. Finally, we combined all of the selected concepts as overall results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run2 (F1TOP2):</head><p>The same training process as the F1TOP1 except that we selected th e top 5 concepts for the images belongs to the "abdomen" subset, concepts with score above 0.2 for the "chest" subset, top 7 concepts for the "head &amp; neck" subset and conc epts with score above 0.1 for the "skeletal muscle" subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run3 (F1TOP5_Pmax):</head><p>The same training process as the F1TOP1 except that we sel ected the top 5 concepts for the images belongs to the "abdomen", the "chest" and the "head &amp; neck" subset, and the top 3 concepts for the "skeletal muscle" subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run4 (F1TOP3):</head><p>The same training process as the F1TOP1 except that we selected th e top 10 concepts for the images belongs to the "abdomen", the top 5 concepts for the "chest" subset, concepts with score above 0.1 for the "head &amp; neck" subset, and the to p 7 concepts for the "skeletal muscle" subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run5 (07Comb_F1Top1):</head><p>The same training process as the F1TOP1 except that we s elected the top 7 concepts for the images belongs to the "abdomen", concepts with sco re above 0.3 for the "chest" subset, the top 5 concepts for the "head &amp; neck" subset, a nd concepts with score above 0.25 for the "skeletal muscle" subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run6 (F1TOP5_Rmax):</head><p>The same training process as the F1TOP1 except that we sel ected the top 10 concepts for the images belongs to the "abdomen", concepts with sco re above 0.1 for the "chest" subset, the top 7 concepts for the "head &amp; neck" subset, a nd the top 10 concepts for the "skeletal muscle" subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run7 (08Comb_Pmax):</head><p>The same training process as the F1TOP1 except that we sel ected the top 3 concepts for the images belongs to the "abdomen", the "chest", the "he ad &amp; neck" and the "skeletal muscle" subset. The above combination of parameters ac hieved the best precision rate in our validating experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run8 (09Comb_Rmax_new):</head><p>The same training process as the F1TOP1 except that we selected the concepts with score above 0.05 for the images belongs to the "abdome n", the "chest", the "head &amp; neck" and the "skeletal muscle" subset. The above combi nation of parameters achieved the best recall rate in our validating experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run9 (yu_1000_inception_v3_top6):</head><p>This submission utilized the transfer learning-b ased multi-label classification method, which is using the Inception V3 model pre-trai ned on the ImageNet dataset to perform multi-label classification. The batch size was set to 20, the max epoch was set to 30 and the learning rate was set to 0.003. For the i mages in the test set, we selected 87 concepts with frequency above 1000 in the trainin g set as the training labels, and output the top 6 concepts for each test image.   Lung) was also meaningful to the given image. The good data quality, as well as the pre-classification based on body parts contribute to the preferable performance on detecting semantic concepts from large-scale medical images. In summarization, we achieved an F1 score of 0.2235, ranked 8th in the overall submission results, but there is still a great room for improvement in the further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper presents the participation of the Image Semantics group (ImageSem) at the ImageCLEFmed Caption 2019 task. We submitted 10 runs in the concept detection task. Multiple concepts were identified for interpreting medical images by the two-stage concept detection strategy, including the medical image pre-classification based on body parts and the transfer learning-based multi-label classification. The evaluation results showed that we achieved an F1 Score of 0.2235, which was superior to our former achievement in ImageCLEFcaption 2018. The reason for the improvement may due to the good data quality, as well as the pre-classification of medical images based on predefined body part categories. However, the work of semantic concept detection on large-scale open medical images still needs further research, and we will try to seek more useful semantic clues from external labelled data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,124.70,665.59,345.51,8.10;1,124.70,675.91,345.88,8.10;1,124.70,686.23,45.18,8.10"><head>1</head><label></label><figDesc>Copyright (c) 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CLEF 2019, 9-12 September 2019, Lugano, Switzerland.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,163.34,345.14,268.41,8.10;2,124.70,183.40,345.90,153.00"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Workflow of ImageSem at the ImageCLEFmed Caption 2019 Task</figDesc><graphic coords="2,124.70,183.40,345.90,153.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,124.70,666.26,345.62,8.96;6,124.70,678.26,345.76,8.96;7,124.70,150.18,345.81,8.96;7,124.70,162.18,345.87,8.96;7,124.70,174.18,345.79,8.96;7,124.70,186.18,74.76,8.96"><head></head><label></label><figDesc>This submission employed the transfer learning -based concept detection using the ResNet152 model pre-trained on the ImageNet data set. The batch size was set to 20, the max epoch was set to 30 and the learning rate wa s set to 0.003. For the images in the test set, we also selected 87 concepts with frequen cy above 1000 in the training set as the training labels, and output the top 6 concepts f or each test image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,125.30,285.14,344.41,8.10;7,128.78,296.18,337.76,8.10;7,145.34,307.22,304.55,8.10;7,124.70,195.40,345.90,80.65"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An example of concept detection from the validation set of the ImageCLEFmed Caption 2019 task. The GT concepts were ground truth provided by the ImageCLEF organizers, while the Predict Concepts were results of our two-stage medical concept detection model.</figDesc><graphic coords="7,124.70,195.40,345.90,80.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,124.70,330.21,345.92,8.96;7,124.70,342.21,346.08,8.96;7,124.70,354.21,345.90,8.96;7,124.70,366.21,345.89,8.96;7,124.70,378.21,345.97,8.96;7,124.70,390.21,345.82,8.96;7,124.70,402.21,345.77,8.96;7,124.70,414.21,240.14,8.96"><head>Fig. 2</head><label>2</label><figDesc>Fig.2shows an example of concept detection from the validation set of the Im-ageCLEFmed Caption 2019 collection. The predicted concepts matched four labels (in red) with the ground truth concepts, while the unmatched concept (C0102410983129; Lung) was also meaningful to the given image. The good data quality, as well as the pre-classification based on body parts contribute to the preferable performance on detecting semantic concepts from large-scale medical images. In summarization, we achieved an F1 score of 0.2235, ranked 8th in the overall submission results, but there is still a great room for improvement in the further research.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.54,318.14,321.16,98.22"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the concepts from the training set and the validation set.</figDesc><table coords="3,134.54,337.58,321.16,78.78"><row><cell>Frequency</cell><cell>Number</cell><cell>Proportion of Num</cell><cell>Occurrence</cell><cell>Proportion of occur</cell></row><row><cell>0-10</cell><cell>3630</cell><cell>65.67%</cell><cell>9987</cell><cell>2.31%</cell></row><row><cell>10-100</cell><cell>1263</cell><cell>22.85%</cell><cell>45630</cell><cell>10.54%</cell></row><row><cell>100-1000</cell><cell>548</cell><cell>9.91%</cell><cell>173472</cell><cell>40.09%</cell></row><row><cell>1000+</cell><cell>87</cell><cell>1.57%</cell><cell>203664</cell><cell>47.06%</cell></row><row><cell>Total</cell><cell>5528</cell><cell>100.00%</cell><cell>432753</cell><cell>100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,138.14,433.96,307.81,169.14"><head>Table 2 .</head><label>2</label><figDesc>Top10 high-frequency concepts in the training set.</figDesc><table coords="3,138.14,453.40,307.81,149.70"><row><cell>CUI</cell><cell>Associated Image</cell><cell>UMLS Term</cell></row><row><cell>C0441633</cell><cell>8425</cell><cell>diagnostic scanning</cell></row><row><cell>C0043299</cell><cell>7906</cell><cell>x-ray procedure</cell></row><row><cell>C1962945</cell><cell>7902</cell><cell>radiogr</cell></row><row><cell>C0040395</cell><cell>7697</cell><cell>tomogr</cell></row><row><cell>C0034579</cell><cell>7564</cell><cell>pantomogr</cell></row><row><cell>C0817096</cell><cell>7470</cell><cell>thoracics</cell></row><row><cell>C0040405</cell><cell>7164</cell><cell>x-ray computer assisted tomography</cell></row><row><cell>C1548003</cell><cell>6428</cell><cell>radiograph</cell></row><row><cell>C0221198</cell><cell>5678</cell><cell>visible lesion</cell></row><row><cell>C0772294</cell><cell>5677</cell><cell>alesion</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,138.38,222.11,322.33,90.33"><head>Table 3 .</head><label>3</label><figDesc>Statistics of medical images pre-classified into different body part categories.</figDesc><table coords="5,138.38,240.18,322.33,72.27"><row><cell>Dataset</cell><cell>Abdomen</cell><cell>Chest</cell><cell cols="3">Head and neck Skeletal Muscle Total</cell></row><row><cell>Manual annotated</cell><cell>7546</cell><cell>5406</cell><cell>6000</cell><cell>4000</cell><cell>22952</cell></row><row><cell>Training</cell><cell>19430</cell><cell>12458</cell><cell>15445</cell><cell>9296</cell><cell>56629</cell></row><row><cell>Validation</cell><cell>4802</cell><cell>3040</cell><cell>4003</cell><cell>2312</cell><cell>14157</cell></row><row><cell>Test</cell><cell>3578</cell><cell>2277</cell><cell>2607</cell><cell>1538</cell><cell>10000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,137.30,533.56,320.78,147.69"><head>Table 4 .</head><label>4</label><figDesc>Submission runs by the ImageSem group in ImageCLEFmed Caption 2019 task</figDesc><table coords="5,152.18,552.52,297.88,128.73"><row><cell>Submission Run</cell><cell>Rank overall</cell><cell>Mean F1 Score</cell></row><row><cell>F1TOP1.txt</cell><cell>8</cell><cell>0.2235690</cell></row><row><cell>F1TOP2.txt</cell><cell>9</cell><cell>0.2227917</cell></row><row><cell>F1TOP5_Pmax.txt</cell><cell>10</cell><cell>0.2216225</cell></row><row><cell>F1TOP3.txt</cell><cell>11</cell><cell>0.2190201</cell></row><row><cell>07Comb_F1Top1.txt</cell><cell>12</cell><cell>0.2187337</cell></row><row><cell>F1TOP5_Rmax.txt</cell><cell>13</cell><cell>0.2147437</cell></row><row><cell>08Comb_Pmax.txt</cell><cell>18</cell><cell>0.1912173</cell></row><row><cell>09Comb_Rmax_new.txt</cell><cell>40</cell><cell>0.1121941</cell></row><row><cell>yu_1000_inception_v3_top6.csv</cell><cell>52</cell><cell>0.0009450</cell></row><row><cell>yu_1000_resnet_152_top6.csv</cell><cell>53</cell><cell>0.0008925</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This study was supported by the <rs type="funder">Non-profit Central Research Institute Fund of Chinese Academy of Medical Sciences</rs> (Grant No. <rs type="grantNumber">2018-I2M-AI-016</rs>, Grant No. <rs type="grantNumber">2017PT63010</rs> and Grant No. <rs type="grantNumber">2018PT33024</rs>); the <rs type="funder">National Natural Science Foundation of China</rs> (Grant No. <rs type="grantNumber">81601573</rs>) and the <rs type="funder">Fundamental Research Funds for the Central Universities</rs> (Grant No. <rs type="grantNumber">3332018153</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vFbtsyq">
					<idno type="grant-number">2018-I2M-AI-016</idno>
				</org>
				<org type="funding" xml:id="_dAYvh3b">
					<idno type="grant-number">2017PT63010</idno>
				</org>
				<org type="funding" xml:id="_ygKZ76J">
					<idno type="grant-number">2018PT33024</idno>
				</org>
				<org type="funding" xml:id="_dRFTqUR">
					<idno type="grant-number">81601573</idno>
				</org>
				<org type="funding" xml:id="_qMtbAeZ">
					<idno type="grant-number">3332018153</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,132.67,219.11,337.45,8.10;8,141.74,230.15,328.95,8.10;8,141.74,241.07,328.50,8.10;8,141.74,252.11,329.02,8.10;8,141.74,263.18,328.60,8.10;8,141.74,274.10,328.53,8.10;8,141.74,285.14,328.90,8.10;8,141.74,296.18,328.74,8.10;8,141.74,307.10,219.62,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,403.08,263.18,67.26,8.10;8,141.74,274.10,245.85,8.10">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,408.97,274.10,61.30,8.10;8,141.74,285.14,328.90,8.10;8,141.74,296.18,160.73,8.10">Proceedings of the 10th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="8,330.32,296.18,140.17,8.10;8,141.74,307.10,14.95,8.10">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,132.67,318.14,337.83,8.10;8,141.74,329.18,328.99,8.10;8,141.74,340.10,328.81,8.10;8,141.74,351.14,219.50,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,405.18,318.14,65.31,8.10;8,141.74,329.18,328.99,8.10;8,141.74,340.10,88.56,8.10">Overview of Im-ageCLEFcaption 2017 -the image caption prediction and concept extraction tasks to understand biomedical images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcí A Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="8,250.23,340.10,220.32,8.10;8,141.74,351.14,39.82,8.10">CLEF 2017 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,362.18,337.95,8.10;8,141.74,373.10,328.73,8.10;8,141.74,384.14,273.47,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,397.36,362.18,73.26,8.10;8,141.74,373.10,141.84,8.10">Overview of the Im-ageCLEF 2018 caption prediction tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcí A Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="8,301.81,373.10,168.66,8.10;8,141.74,384.14,88.43,8.10">CLEF 2018 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,395.18,337.75,8.10;8,141.74,406.10,328.79,8.10;8,141.74,417.14,328.99,8.10;8,141.74,428.18,93.60,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,394.27,395.18,76.15,8.10;8,141.74,406.10,153.51,8.10">Overview of the Im-ageCLEFmed 2019 concept detection task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcí A Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="1613-0073" />
	</analytic>
	<monogr>
		<title level="m" coord="8,314.20,406.10,99.55,8.10">CLEF 2019 Working Notes</title>
		<title level="s" coord="8,420.39,406.10,50.14,8.10;8,141.74,417.14,90.22,8.10">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Lugano</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,439.12,337.68,8.10;8,141.74,450.16,252.02,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,200.21,439.12,270.14,8.10;8,141.74,450.16,50.21,8.10">Effective mapping of biomedical text to the umls metathesaurus: the metamap program</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,210.77,450.16,113.01,8.10">Proceedings.AMIA Symposium</title>
		<meeting>.AMIA Symposium</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,461.20,337.98,8.10;8,141.74,472.12,328.76,8.10;8,141.74,483.16,244.32,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,281.20,461.20,189.44,8.10;8,141.74,472.12,106.25,8.10">ImageSem at ImageCLEF 2018 caption task: image retrieval and transfer learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="8,266.67,472.12,98.98,8.10">CLEF 2018 Working Notes</title>
		<title level="s" coord="8,372.05,472.12,98.45,8.10;8,141.74,483.16,39.47,8.10">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,494.20,337.87,8.10;8,141.74,505.12,328.80,8.10;8,141.74,516.16,328.80,8.10;8,141.74,527.20,328.82,8.10;8,141.74,538.12,147.36,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,361.87,494.20,108.67,8.10;8,141.74,505.12,132.42,8.10">Radiology Objects in COntext (ROCO): a multimodal image dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,376.91,505.12,93.63,8.10;8,141.74,516.16,328.80,8.10;8,141.74,527.20,55.57,8.10">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<title level="s" coord="8,356.50,527.20,114.06,8.10;8,141.74,538.12,14.95,8.10">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>LABELS; STENT; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018. 2018. 2018. 2018</date>
			<biblScope unit="volume">11043</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,549.16,338.10,8.10;8,141.74,560.20,328.49,8.10;8,141.74,571.12,198.80,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,359.35,549.16,111.41,8.10;8,141.74,560.20,99.49,8.10">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,261.13,560.20,209.10,8.10;8,141.74,571.12,111.58,8.10">Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2016 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,582.16,338.06,8.10;8,141.74,593.20,169.68,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,255.29,582.16,172.77,8.10">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,434.83,582.16,35.89,8.10;8,141.74,593.20,111.58,8.10">Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="70" to="778" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.40,604.12,338.19,8.10;8,141.74,615.19,328.80,8.10;8,141.74,626.23,288.50,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,328.39,615.19,142.15,8.10;8,141.74,626.23,33.00,8.10">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,180.69,626.23,148.99,8.10">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.40,637.06,338.26,8.19;8,141.74,648.19,123.24,8.10" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<title level="m" coord="8,216.65,637.15,184.04,8.10">Medical imaging reading diagnostic atlas. 2nd edn</title>
		<meeting><address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<publisher>People&apos;s Medical Publishing House</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.40,659.23,338.46,8.10;8,141.74,670.15,328.81,8.10;8,141.74,681.19,317.45,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,320.71,659.23,150.15,8.10;8,141.74,670.15,176.04,8.10">ALEXNET feature extraction and multikernel learning for object oriented classification</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,324.23,670.15,146.31,8.10;8,141.74,681.19,243.84,8.10">J ISPRS -International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="page" from="277" to="281" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
