<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.18,115.96,321.00,12.62;1,150.96,133.89,313.44,12.62;1,183.39,151.82,248.58,12.62">ImageCLEF 2019: CT Image Analysis for TB Severity Scoring and CT Report Generation using Autoencoded Image Features</title>
				<funder ref="#_bNaRZe2">
					<orgName type="full">CRDF</orgName>
				</funder>
				<funder>
					<orgName type="full">National Institute of Allergy and Infectious Diseases, National Institutes of Health, U.S. Department of Health and Human Services, USA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,269.86,189.49,75.63,8.74"><forename type="first">Siarhei</forename><surname>Kazlouski</surname></persName>
							<email>kozlovski.serge@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">United Institute of Informatics Problems</orgName>
								<address>
									<settlement>Minsk</settlement>
									<country key="BY">Belarus</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.18,115.96,321.00,12.62;1,150.96,133.89,313.44,12.62;1,183.39,151.82,248.58,12.62">ImageCLEF 2019: CT Image Analysis for TB Severity Scoring and CT Report Generation using Autoencoded Image Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2E2E73B39B83B79AFE05F3ADE31944B2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computed Tomography</term>
					<term>Tuberculosis</term>
					<term>Deep Learning</term>
					<term>Autoencoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a possible approach for the automated analysis of 3D Computed Tomography (CT) images based on the usage of feature vectors extracted by a deep convolutional 3D autoencoder network. Conventional classification models were used on top of the "autoencoded" feature vectors as well as vectors of meta-information paired with the images. The proposed CT image analysis approach was used by participant UIIP (Siarhei Kazlouski) for accomplishing the two subtasks of the ImageCLEF Tuberculosis task of the ImageCLEF 2019 international competition. Employing the proposed approach allowed to achieve the 2nd best performance on the TB Severity Scoring subtask and the 6th best performance in the TB CT Report subtask.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated analysis of 3D CT images is an example of a task that can be solved during the development of computer assisted diagnosis systems which may be used for lung disease screening for the early detection of pathology. While promising results have been shown in automated analysis of medical images of some modalities <ref type="bibr" coords="1,185.94,507.71,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,197.72,507.71,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="1,206.73,507.71,4.10,8.74" target="#b6">[7]</ref><ref type="bibr" coords="1,210.82,507.71,4.10,8.74" target="#b7">[8]</ref><ref type="bibr" coords="1,214.92,507.71,8.19,8.74" target="#b8">[9]</ref>, the task of CT image analysis remains challenging due to the complexity and scarcity of data. A CT image is 3D data which can often be represented as a set of 2D slices with the inter-slice distance varying between 0.5 and 5 mm. Variability in the sizes and shapes of CT image voxels implies difficulties in the application of many image analysis algorithms, while low availability of CT imaging data makes it difficult to use data-greedy approaches, for example, deep learning.</p><p>Despite the lack of data available, the approach for the analysis of 3D CT images proposed with this study employs the idea of trying to get 3D image descriptors by utilizing a 3D autoencoder network <ref type="bibr" coords="1,344.43,615.31,9.96,8.74" target="#b5">[6]</ref>. The motivation for this idea is the potential for maximum information usage as soon as the network is able to work with the entire 3D image. Extracted features were further analyzed using conventional classification models which were ensembled with models trained on image metadata. One can note, that pure 3D classification networks could be used instead of conventional models on top of autoencoded features. The advantage of the used approach is its generality: once we got autoencoded descriptors, a conventional classification model could be easily and quickly trained for arbitraty labelling (either TB severity, or one provided in CT report findings, or some arbitrary findings classes which are not mentioned in the competition), as well as research interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Subtasks and datasets</head><p>The Tuberculosis task <ref type="bibr" coords="2,235.61,287.24,10.52,8.74" target="#b1">[2]</ref> of ImageCLEF 2019 Challenge <ref type="bibr" coords="2,388.46,287.24,10.52,8.74" target="#b4">[5]</ref> included two subtasks: TB Severety (SVR) and CT Report (CTR), both dealing with 3D CT images. The same CT imaging data was used in both subtasks and included 218 images in the training dataset and 117 in the test dataset. Along with the CT images, the lungs masks <ref type="bibr" coords="2,263.24,335.06,10.52,8.74" target="#b2">[3]</ref> and additional information about the patients was provided. The metadata included information about the presence of disability, relapse, presence of TB symptoms, co-morbidity, bacillarity, drug resistance status, patient's education, being an ex-prisoner, smoking status and alcohol addiction. The frequencies of occurrence of each metadata label are listed in Table <ref type="table" coords="2,162.16,394.84,3.87,8.74" target="#tab_0">1</ref>. The subtask #1 (SVR subtask) was dedicated to the problem of categorizing TB cases into one of two classes: high severity and low severity. The task was to predict TB Severity class ("HIGH"/"LOW")</p><p>The subtask #2 (CTR subtask) was dedicated to the automated generation of CT reports which indicate the presence of several types of abnormalities in the lungs. Such automated annotation of CT scans is important for the development of the dedicated image databases. The task was to predict the presence of six types of findings in CT scans. Information about the corresponding labels is listed in Table <ref type="table" coords="3,200.41,130.95,3.87,8.74" target="#tab_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>This section contains a description of the methods used within the current study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data preprocessing</head><p>Since the key idea of the approach proposed is based on using an autoencoder network, the extremely small amount of data samples we have becomes the main challenge, especially keeping in mind the sample dimensionality. The second issue is the sample dimensionality itself, which restricts possible model architectures due to GPU memory limitations.</p><p>In order to overcome the mentioned issues the following concepts were used: a) image size was reasonably decreased, b) the autoencoder was trained not on per-CT, but on per-lung basis which simultaneously doubled the training dataset size and decreased the sample dimensionality two times. Data augmentation was also used. Detailed steps of data preprocessing during the training stage were as follows:</p><p>1) Each CT image was split into two parts, each containing one lung. The split was performed roughly, into equal parts by splitting on the middle Y coordinate. The part containing the left lung was used as it is, and the part containing the right lung was reflected along Y axis in order to make the right lung oriented similar to left one. Lung images were normalized to 0-1 scale. The provided lungs masks were treated the same way (except for normalization).</p><p>2) For each lung a random transformation was generated and applied to the lung and its mask. The mask was binarized after the transformation and applied to the lung image (all voxels outside of the masked lung area were set to zero). The resulting image was resized to 128 x 128 x 128 pixels and normalized once again. Transformations included 3D shift, rotation, scale, crop and shear, and were applied sequentially with a probability of 50% for each transform. The ranges of parameters used for the transformation are presented in Table <ref type="table" coords="3,452.13,656.12,3.87,8.74" target="#tab_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training and validation subsets</head><p>The training dataset provided by the organizers was split into training and validation subsets for models development. Two splits were used, one for autoencoder model training, and another one for classification models training. For autoencoder training, a randomly sampled 90% of training data was used for training and the remaining 10% was used for validation. For classification models the fixed random 5-fold cross-validation split was used. Smaller validation size in the autoencoder case is motivated by maximizing training set size, while validation loss for the autoencoder model is less important. For classification tasks scoring is crucial, so cross-validation was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Autoencoder Training</head><p>A custom convolutional architecture was used for the autoencoder model. Several trials with different hyperparmeters (number of layers, kernel size, filter number) were exectuted and the model with the best achieved validation loss was selected. The selected architecture is presented in Table <ref type="table" coords="4,337.65,440.87,3.87,8.74" target="#tab_3">4</ref>. Each convolutional layer had a kernel size of (3,3,3) and was followed by a 3D max pooling layer with parameters (2,2,2) in the encoder part and a 3D upsample layer with parameter (2,2,2) in the decoder part. This setup resulted in an encoded feature vector of size 256.</p><p>The autoencoder model was trained using Adam optimizer and was performed in three stages. On the first stage the model was trained on the mixture of left and right lung images using data augmentation as described before. The training was performed until any significant improvements in validation loss were observed. Specifically, model trained for 72 epochs was selected on this stage. On the second stage the retrieved pretrained model was finetuned with a 10 times smaller learning rate separately for the left and right lungs using data augmentation, resulting in two different models, one for the left and one for the right lungs. Training stop criteria was the same, resulting in 20 epochs of training. So, on the first two stages 92 randomly augmented versions of each original CT were used for model training. Finally, learning rate was decreased 10 times again and each of the two models were finetuned for 2 epochs on competition data without data augmentation.</p><p>On the inference stage both autoencoder models were used to generate feature vectors for the left and right lung of each CT image, so the resulting feature Analysis of encoded vectors showed that around half of the components of the vector are very close to zero for all images, which probably reflects the nonoptimal architecture and/or weights of model. As soon as no better model could be retrieved in experiments, it was decided just to drop the "zero" components of the encoded vector, which resulted in the final version of the encoded CT images descriptors containing 220 components each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prediction of CT Report labels and TB Severity.</head><p>Since all of the predictions in both subtasks are binary classification problems, they were treated and solved in the same way. The idea may be formulated as follows: for each of the requested binary class labels (which are: Severity-High, LeftLungAffected, RightLungAffected, LungCapacityDecrease, Calcification, Pleurisy, Caverns) build a binary classification model which will take encoded image descriptors and available meta information about patients as input features.</p><p>Because of the different nature of encoded image descriptors and image meta information it was decided to train separate classification models for each feature type and then ensemble the models' output rather than concatenating feature vectors.</p><p>Conventional classification models were used for working with both types of features and included scikit-learn package implementation of SVM, K-neighbors classifier (kNN), random forest classifier (RF) and AdaBoost classifier. Hyper parameters for each of the models were tuned by cross-validation using random parameter search.</p><p>In case of meta information features were used "as is", while for autoencoded image features their PCA-transformed presentation with 3,5,10,50 components were used as well.</p><p>The resulting algorithm can be described as follows. 1) For each of 5 folds, take the autoencoded features and fit PCA using the training set and transform validation set using 3, 5, 10, 50 components. Thus six alternative feature vectors were presented for each image in each crossvalidation split: meta information (META), encoded features (AEC), and PCAtransformed encoded features with 3, 5, 10, 50 components (PCA3, PCA5, PCA10, PCA50).</p><p>2) Sample random hyper parameters for each of 4 classifier types. Parameter ranges are presented in Table <ref type="table" coords="6,263.88,214.65,4.98,8.74" target="#tab_4">5</ref> (only valid parameters combinations were used).</p><p>For each of the target labels:</p><p>3) Get the mean AUC-ROC score at cross-validation for all models and sampled hyper parameters.</p><p>4) Select the best models according to the achieved scores. Models selection was performed not just by the top score value, but using the following heurisctic: 1) classifier-feature type combination were used only once, 2) in the case that scores are reasonably close, more simple models have priority (examples: a) if the kNN model with 11 neighbors scores 0.80 and with 2 neighbors scores 0.78, the second one is used; b) if the same model scores 0.8 on the 50 component PCA features and 0.77 on the 3-component, the second one is used). The application of the described algorithm resulted in the selection of from 1 to 4 best models for each target class prediction, which were ensembled during final prediction by the simple averaging of class probabilities. A summary on the selected models and their validation performance is presented in Table <ref type="table" coords="6,445.60,545.78,3.87,8.74" target="#tab_5">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submissions and results</head><p>As the result of this study, the described method was applied to generate predictions for the competition test dataset. Predictions were submitted by participant UIIP for the CTR and SVR subtasks. The full list of the submitted results for both subtasks is available at the task web page<ref type="foot" coords="6,340.38,635.94,3.97,6.12" target="#foot_0">1</ref> . Before generating the final submission, the autoencoder model and selected classifiers used for predicting the test data were trained on the whole available training dataset. Final probabilities of each target class were calculated as the average probability of the selected classification models.</p><p>Table <ref type="table" coords="7,178.74,467.00,4.98,8.74" target="#tab_6">7</ref> shows the best results achieved by the participants in the CTR subtask. The run submitted by UIIP achieved the 6th best mean AUC, while also demostrating the worst minimum AUC for the prediction of the presence of lung abnormalities. The average mean result on the contrary to the worst minimum AUC demonstrate, that selected models might work pretty well for some of the target labels in the report, while failing for other labels. Since all targets demonstrate similar performances on the validation set, test results may be caused by overfitting to validation and the different distribution of train and test sets in the competition, or by some errors in validation set generation (in particular, the uneven distribution of meta information and targets classes itself was not carefully treated in experiments).</p><p>Table <ref type="table" coords="7,177.06,608.30,4.98,8.74" target="#tab_7">8</ref> shows the best results achieved by the participants in the SVR subtask. The run submitted by UIIP achieved the 2nd highest value for AUC, and shared the 1st best accuracy with UIIP BioMed participant. Achieved AUC correlates with validation scores and demonstrates the efficiency of the used approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>The results of this study allow us to draw the following conclusions:</p><p>-Despite data scarcity, deep autoencoder networks may be used for extracting reasonable descriptors of 3D CT data if some tricks for training set extension are used. -Meta information about patients are helpful for the more accurate predictions of TB characteristics. -Although the used approach demonstrated good performance in the SVR subtask, it was not very reliable for the generation of the CT report, which means the suggested method is not very stable or at least needs more careful validation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,135.81,425.86,343.74,127.29"><head>Table 1 .</head><label>1</label><figDesc>Frequences of positive metadata labels for SVR subtask in the datasets, %.</figDesc><table coords="2,197.60,446.24,220.15,106.92"><row><cell>Label</cell><cell>In Training set</cell><cell>In Test set</cell></row><row><cell>Disability</cell><cell>16</cell><cell>13</cell></row><row><cell>Relapse</cell><cell>35</cell><cell>36</cell></row><row><cell>SymptomsOfTB</cell><cell>54</cell><cell>40</cell></row><row><cell>Comorbidity</cell><cell>56</cell><cell>48</cell></row><row><cell>Bacillary</cell><cell>85</cell><cell>92</cell></row><row><cell>HigherEducation</cell><cell>13</cell><cell>16</cell></row><row><cell>ExPrisoner</cell><cell>12</cell><cell>10</cell></row><row><cell>Alcoholic</cell><cell>22</cell><cell>25</cell></row><row><cell>Smoking</cell><cell>52</cell><cell>60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,134.77,158.77,345.82,114.59"><head>Table 2 .</head><label>2</label><figDesc>Frequences of positive labels for Severity and CTR subtasks in the Training dataset.</figDesc><table coords="3,218.94,188.36,177.48,85.00"><row><cell>Finding</cell><cell>In Training set</cell></row><row><cell>SeverityHigh</cell><cell>49</cell></row><row><cell>LeftLungAffected</cell><cell>72</cell></row><row><cell>RightLungAffected</cell><cell>81</cell></row><row><cell>LungCapacityDecrease</cell><cell>30</cell></row><row><cell>Calcification</cell><cell>13</cell></row><row><cell>Pleurisy</cell><cell>7</cell></row><row><cell>Caverns</cell><cell>40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,143.06,115.91,331.77,83.46"><head>Table 3 .</head><label>3</label><figDesc>Parameters of augmentation applied to the Training dataset.</figDesc><table coords="4,143.06,136.29,331.77,63.08"><row><cell>Transform</cell><cell>Parameter value</cell></row><row><cell>Shift, pixels</cell><cell>up to 5% of each side (X, Y, Z) size</cell></row><row><cell>Rotation center</cell><cell>Shifted from image center up to 3% of each side (X, Y, Z) size</cell></row><row><cell>Rotation angle</cell><cell>up to 5 degrees</cell></row><row><cell>Scaling</cell><cell>up to 5%</cell></row><row><cell>Shear</cell><cell>up to 0.01 absolute value, each of six components</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,134.77,115.91,345.82,215.98"><head>Table 4 .</head><label>4</label><figDesc>Autoencoder architecture.</figDesc><table coords="5,134.77,134.54,345.82,197.35"><row><cell>Layer</cell><cell>Number of filters</cell></row><row><cell>Convolution3D</cell><cell>128</cell></row><row><cell>Convolution3D</cell><cell>64</cell></row><row><cell>Convolution3D</cell><cell>64</cell></row><row><cell>Convolution3D</cell><cell>32</cell></row><row><cell>Convolution3D</cell><cell>32</cell></row><row><cell>Convolution3D</cell><cell>32</cell></row><row><cell>Flatten</cell><cell></cell></row><row><cell>Convolution3D</cell><cell>32</cell></row><row><cell>Convolution3D</cell><cell>32</cell></row><row><cell>Convolution3D</cell><cell>32</cell></row><row><cell>Convolution3D</cell><cell>64</cell></row><row><cell>Convolution3D</cell><cell>64</cell></row><row><cell>Convolution3D</cell><cell>128</cell></row><row><cell cols="2">vectors of CT were a concatenation of the left and right lung encoded descriptors,</cell></row><row><cell>with 512 components in total.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,144.97,366.29,325.42,116.33"><head>Table 5 .</head><label>5</label><figDesc>Classifier parameters search ranges.</figDesc><table coords="6,144.97,386.66,325.42,95.96"><row><cell>Classifier</cell><cell>Parameters range (format: min .. max .. step)</cell></row><row><cell>KNeighborsClassifier</cell><cell>neighbors number: 1 .. 20 .. 1)</cell></row><row><cell>RandomForestClassifier</cell><cell>estimators number: 1 .. 100 .. 5</cell></row><row><cell></cell><cell>max tree depth number: 1 .. 6 .. 1</cell></row><row><cell>AdaBoostClassifier</cell><cell>estimators number: 1 .. 200 .. 5</cell></row><row><cell></cell><cell>learning rate : 0.1 .. 1 .. 0.2</cell></row><row><cell>SVM</cell><cell>C: 10e-5 .. 10e5 .. by power of 10</cell></row><row><cell></cell><cell>degree : 1 .. 7 .. 1</cell></row><row><cell></cell><cell>kernel: 'linear', 'poly', 'rbf'</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,146.79,115.91,321.78,246.10"><head>Table 6 .</head><label>6</label><figDesc>Selected classification models.</figDesc><table coords="7,146.79,134.54,321.78,227.47"><row><cell>Label</cell><cell>Classifier</cell><cell>Features</cell><cell>AUC-ROC</cell></row><row><cell>LeftLungAffected</cell><cell>RF(E*=20, D**=1)</cell><cell>PCA5</cell><cell>0.77</cell></row><row><cell></cell><cell>SVM(linear, C=10e5)</cell><cell>PCA5</cell><cell>0.81</cell></row><row><cell>RightLungAffected</cell><cell>RF(E=20, D=1)</cell><cell>PCA5</cell><cell>0.79</cell></row><row><cell></cell><cell>SVM(rbf, C=10e5)</cell><cell>PCA5</cell><cell>0.77</cell></row><row><cell>Calcification</cell><cell>SVM(poly, d=2, C=0.01)</cell><cell>META</cell><cell>0.84</cell></row><row><cell></cell><cell>RF(E=16, D=1)</cell><cell>PCA10</cell><cell>0.80</cell></row><row><cell></cell><cell>kNN(neighbors=11)</cell><cell>PCA5</cell><cell>0.82</cell></row><row><cell></cell><cell>SVM(linear, C=10e3)</cell><cell>AEC</cell><cell>0.90</cell></row><row><cell>Caverns</cell><cell>AdaBoost(E=15, lr=0.2)</cell><cell>AEC</cell><cell>0.89</cell></row><row><cell>Pleurisy</cell><cell>RF(E=6, D=2)</cell><cell>META</cell><cell>0.82</cell></row><row><cell></cell><cell>RF(E=20, D=1)</cell><cell>AEC</cell><cell>0.89</cell></row><row><cell></cell><cell>RF(E=10, D=1)</cell><cell>PCA10</cell><cell>0.90</cell></row><row><cell>LungCapacityDecr.</cell><cell>RF(E=6, D=2)</cell><cell>META</cell><cell>0.78</cell></row><row><cell></cell><cell>RF(E=20, D=1)</cell><cell>AEC</cell><cell>0.83</cell></row><row><cell></cell><cell>SVM(linear, C=10e3)</cell><cell>PCA5</cell><cell>0.72</cell></row><row><cell>Severity</cell><cell>kNN(neighbors=11)</cell><cell>PCA5</cell><cell>0.71</cell></row><row><cell></cell><cell>RF(E=8, D=2)</cell><cell>PCA10</cell><cell>0.82</cell></row><row><cell></cell><cell>RF(E=30, D=2)</cell><cell>AEC</cell><cell>0.87</cell></row><row><cell>*estimators number</cell><cell></cell><cell></cell><cell></cell></row><row><cell>**max depth</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,166.23,115.91,282.89,138.25"><head>Table 7 .</head><label>7</label><figDesc>The best participants' runs submitted for the CTR subtask.</figDesc><table coords="8,166.61,136.29,282.15,117.88"><row><cell>Group Name</cell><cell>Mean AUC</cell><cell>Min AUC</cell><cell>Rank</cell></row><row><cell>UIIP BioMed</cell><cell>0.7968</cell><cell>0.6860</cell><cell>1</cell></row><row><cell>CompElecEngCU</cell><cell>0.7066</cell><cell>0.5739</cell><cell>2</cell></row><row><cell>MedGIFT</cell><cell>0.6795</cell><cell>0.5626</cell><cell>3</cell></row><row><cell>San Diego VA HCS/UCSD</cell><cell>0.6631</cell><cell>0.5541</cell><cell>4</cell></row><row><cell>HHU</cell><cell>0.6591</cell><cell>0.5159</cell><cell>5</cell></row><row><cell>UIIP</cell><cell>0.6464</cell><cell>0.4099</cell><cell>6</cell></row><row><cell>MostaganemFSEI</cell><cell>0.6273</cell><cell>0.4877</cell><cell>7</cell></row><row><cell>UniversityAlicante</cell><cell>0.6190</cell><cell>0.5366</cell><cell>8</cell></row><row><cell>PwC</cell><cell>0.6002</cell><cell>0.4724</cell><cell>9</cell></row><row><cell>LIST</cell><cell>0.5523</cell><cell>0.4317</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,166.87,268.44,281.61,149.21"><head>Table 8 .</head><label>8</label><figDesc>The best participants' runs submitted for the SVR subtask.</figDesc><table coords="8,180.66,288.82,254.03,128.84"><row><cell>Group Name</cell><cell>AUC</cell><cell>Accuracy</cell><cell>Rank</cell></row><row><cell>UIIP BioMed</cell><cell>0.7877</cell><cell>0.7179</cell><cell>1</cell></row><row><cell>UIIP</cell><cell>0.7754</cell><cell>0.7179</cell><cell>2</cell></row><row><cell>HHU</cell><cell>0.7695</cell><cell>0.6923</cell><cell>3</cell></row><row><cell>CompElecEngCU</cell><cell>0.7629</cell><cell>0.6581</cell><cell>4</cell></row><row><cell>San Diego VA HCS/UCSD</cell><cell>0.7214</cell><cell>0.6838</cell><cell>5</cell></row><row><cell>MedGIFT</cell><cell>0.7196</cell><cell>0.6410</cell><cell>6</cell></row><row><cell>UniversityAlicante</cell><cell>0.7013</cell><cell>0.7009</cell><cell>7</cell></row><row><cell>MostaganemFSEI</cell><cell>0.6510</cell><cell>0.6154</cell><cell>8</cell></row><row><cell>SSN College of Engineering</cell><cell>0.6264</cell><cell>0.6068</cell><cell>9</cell></row><row><cell>University of Asia Pacific</cell><cell>0.6111</cell><cell>0.6154</cell><cell>10</cell></row><row><cell>FIIAugt</cell><cell>0.5692</cell><cell>0.5556</cell><cell>11</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,144.73,656.80,223.17,7.86"><p>https://www.imageclef.org/2019/medical/tuberculosis/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This study was partly supported by the <rs type="funder">National Institute of Allergy and Infectious Diseases, National Institutes of Health, U.S. Department of Health and Human Services, USA</rs> through the <rs type="funder">CRDF</rs> project <rs type="grantNumber">DAA3-18-64818-1 "Year 7</rs>: <rs type="projectName">Belarus TB Database and TB Portals</rs>".</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_bNaRZe2">
					<idno type="grant-number">DAA3-18-64818-1 &quot;Year 7</idno>
					<orgName type="project" subtype="full">Belarus TB Database and TB Portals</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.35,185.94,342.24,7.86;9,146.91,196.90,333.68,7.86;9,146.91,207.83,333.68,7.89;9,146.91,218.82,171.89,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,448.77,185.94,31.82,7.86;9,146.91,196.90,303.24,7.86">A deep learning-based algorithm for 2-D cell segmentation in microscopy images</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Al-Kofahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zaltsman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rusu</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-018-2375-z</idno>
		<ptr target="https://doi.org/10.1186/s12859-018-2375-z" />
	</analytic>
	<monogr>
		<title level="j" coord="9,458.96,196.90,21.63,7.86;9,146.91,207.86,58.56,7.86">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">365</biblScope>
			<date type="published" when="2018-10">Oct 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,229.72,342.24,7.86;9,146.91,240.68,333.68,7.86;9,146.91,251.64,333.68,7.86;9,146.91,262.60,333.68,7.86;9,146.91,273.56,91.16,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,162.81,240.68,317.78,7.86;9,146.91,251.64,170.77,7.86">Overview of ImageCLEFtuberculosis 2019 -automatic ct-based report generation and tuberculosis severity assessment</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="9,340.28,251.64,140.32,7.86;9,146.91,262.60,139.14,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,284.46,342.24,7.86;9,146.91,295.42,333.68,7.86;9,146.91,306.38,333.68,7.86;9,146.91,317.34,333.68,7.86;9,146.91,328.30,168.95,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,429.73,284.46,50.86,7.86;9,146.91,295.42,219.32,7.86">Efficient and fully automatic segmentation of the lungs in ct volumes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A</forename><surname>Jiménez Del Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,382.87,306.38,97.72,7.86;9,146.91,317.34,230.71,7.86">Proceedings of the VIS-CERAL Anatomy Grand Challenge at the 2015 IEEE ISBI</title>
		<title level="s" coord="9,426.89,317.34,53.70,7.86;9,146.91,328.30,67.97,7.86">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Jiménez Del Toro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Foncubierta-Rodríguez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<meeting>the VIS-CERAL Anatomy Grand Challenge at the 2015 IEEE ISBI</meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,339.20,342.24,7.86;9,146.91,350.16,333.68,7.86;9,146.91,361.12,333.68,7.86;9,146.91,372.08,333.68,7.86;9,146.91,383.01,333.68,7.89;9,146.91,394.00,165.95,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,429.62,350.16,50.97,7.86;9,146.91,361.12,333.68,7.86;9,146.91,372.08,329.28,7.86">the CAME-LYON16 Consortium: Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ehteshami Bejnordi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Veta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Johannes Van Diest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A W M</forename><surname>Van Der Laak</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2017.14585</idno>
		<ptr target="https://doi.org/10.1001/jama.2017.14585" />
	</analytic>
	<monogr>
		<title level="j" coord="9,146.91,383.04,27.00,7.86">JAMA</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2199" to="2210" />
			<date type="published" when="2017">12 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,404.90,342.24,7.86;9,146.91,415.86,333.68,7.86;9,146.91,426.82,333.68,7.86;9,146.91,437.78,333.67,7.86;9,146.91,448.74,333.68,7.86;9,146.91,459.70,333.68,7.86;9,146.91,470.66,333.68,7.86;9,146.91,481.62,333.68,7.86;9,146.91,492.58,333.68,7.86;9,146.91,503.53,141.36,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,221.90,459.70,258.69,7.86;9,146.91,470.66,75.91,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,243.10,470.66,237.49,7.86;9,146.91,481.62,333.68,7.86;9,146.91,492.58,76.05,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="9,230.46,492.58,170.59,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF 2019)<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="9,138.35,514.44,342.24,7.86;9,146.91,525.40,333.67,7.86;9,146.91,536.36,97.29,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,279.97,514.44,200.62,7.86;9,146.91,525.40,58.81,7.86">Using very deep autoencoders for content-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,212.48,525.40,230.31,7.86">19th European Symposium on Artificial Neural Networks</title>
		<meeting><address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-01">January 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,547.26,342.25,7.86;9,146.91,558.22,333.68,7.86;9,146.91,569.18,333.67,7.86;9,146.91,580.14,190.83,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,258.45,547.26,222.15,7.86;9,146.91,558.22,234.58,7.86">Detection of lung pathologies using deep convolutional networks trained on large X-ray chest screening database</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,404.50,558.22,76.09,7.86;9,146.91,569.18,333.67,7.86;9,146.91,580.14,48.92,7.86">Proceedings of the 14th international conference on Pattern Recognition and Information Processing (PRIP&apos;2019)</title>
		<meeting>the 14th international conference on Pattern Recognition and Information Processing (PRIP&apos;2019)<address><addrLine>Minsk, Belarus</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 21-23 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,591.04,342.25,7.86;9,146.91,602.00,333.68,7.86;9,146.91,612.96,333.67,7.86;9,146.91,623.92,333.67,7.86;9,146.91,634.88,333.67,7.86;9,146.91,645.84,333.68,7.86;9,146.91,656.77,333.68,7.89;10,146.91,119.67,333.68,7.86;10,146.91,130.63,281.49,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,300.36,645.84,180.23,7.86;9,146.91,656.80,196.73,7.86">Predicting breast tumor proliferation from whole-slide images: The TUPAC16 challenge</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Veta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">J</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Stathonikos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Beca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wollmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Rohr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rousson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hedlund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tellez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zerhouni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lanyi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Viana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">A</forename><surname>Phoulady</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Qaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sjblom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Molin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Paeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">I C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Van Diest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2019.02.012</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S1361841518305231" />
	</analytic>
	<monogr>
		<title level="j" coord="9,355.10,656.80,104.78,7.86">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="111" to="121" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,141.59,342.24,7.86;10,146.91,152.55,333.68,7.86;10,146.91,163.51,333.68,7.86;10,146.91,174.44,327.82,7.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,206.01,152.55,274.58,7.86;10,146.91,163.51,329.51,7.86">Evaluation of the diagnostic accuracy of computer-aided detection of tuberculosis on chest radiography among private sector patients in Pakistan</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M A</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Ferrand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Khowaja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-018-30810-1</idno>
		<ptr target="https://doi.org/10.1038/s41598-018-30810-1" />
	</analytic>
	<monogr>
		<title level="j" coord="10,146.91,174.47,67.67,7.86">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12339</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
