<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,307.13,115.90,124.32,12.90;1,224.59,135.97,166.19,10.75">at ImageCLEF 2019 Visual Question Answering Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,189.49,172.58,68.34,8.64"><forename type="first">Rabia</forename><surname>Bounaama</surname></persName>
							<email>rabea.bounaama@univ-tlemcen.dz</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Biomedical Engineering Laboratory</orgName>
								<orgName type="institution">Tlemcen University</orgName>
								<address>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,281.67,172.58,59.50,8.64"><forename type="first">Mohammed</forename><surname>El</surname></persName>
						</author>
						<author>
							<persName coords="1,343.65,172.58,77.75,8.64"><forename type="first">Amine</forename><surname>Abderrahim</surname></persName>
							<email>mohammedelamine.abderrahim@univ-tlemcen.dz</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Laboratory of Arabic Natural Language Processing</orgName>
								<orgName type="institution">Tlemcen University</orgName>
								<address>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tlemcen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,307.13,115.90,124.32,12.90;1,224.59,135.97,166.19,10.75">at ImageCLEF 2019 Visual Question Answering Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8DE9DDF391A95A78ABBCCF05E1C1DBBE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CNNs</term>
					<term>neural networks</term>
					<term>VQA-Med task</term>
					<term>RNN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our methodology of techno team participation at ImageCLEF Medical Visual Question Answering 2019 task. VQA-Med task is a challenge which combines computer vision with Natural Language Processing (NLP) in order to build a system that manages responses based on set of medical images and questions that suit them. We used a jointly learning for text and image method in order to solve the task, we tested a publicly available VQA network. We apply neural network and visual semantic embeddings method on this task. Our approach based on CNNs and RNN model achieve 0.486 of BLEU score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There are many more complex questions that can be asked in medical Radiology, which is very rich of images and textual reports, is a prime area where VQA could assist radiologists in reporting findings for a complicated patient or benefit trainees who have questions about the size of a mass or presence of a fracture <ref type="bibr" coords="1,383.93,483.12,10.58,8.64" target="#b0">[1]</ref>. VQA system is expected to reason over both visual and textual information to infer the correct answer <ref type="bibr" coords="1,134.77,507.03,10.58,8.64" target="#b1">[2]</ref>. So medical VQA systems define as a computer vision and Artificial Intelligence (AI) problem that aims to answer questions asked by health care professionals about medical images <ref type="bibr" coords="1,199.82,530.94,10.73,8.64" target="#b0">[1]</ref>.Artificial neural network models have been studied for many years in the hope of achieving human-like performance in several fields such as speech and image understanding <ref type="bibr" coords="1,221.20,554.86,10.58,8.64" target="#b2">[3]</ref>. VQA could be used to improve human-computer interaction as a natural way to query visual content. It has garnered a large amount of interest from the deep learning, computer vision, and NLP communities <ref type="bibr" coords="1,369.97,578.77,10.58,8.64" target="#b3">[4]</ref>.</p><p>ImageCLEF provide medical image collections, annotated toward several evaluation challenges including VQA, image captioning, and tuberculosis <ref type="bibr" coords="1,406.87,602.89,11.38,8.64" target="#b4">[5,</ref><ref type="bibr" coords="1,418.26,602.89,11.38,8.64" target="#b9">10]</ref>. We participate in the task of VQA in the medical domain.</p><p>Participating systems are tasked with answering the question based on the visual image content. The evaluation of the VQA-Med task participant systems is con ducted by using two metrics: BLEU and accuracy.</p><p>The following of this paper is organized as follows. In section 2 we present some related works. In section 3 we describe our approach and more specifically we present the dataset and discuss in detail the models and techniques used in our submitted run. The conclusion and future work perspectives are presented in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Convolutional Neural Networks (CNNs) make a promising model for the ImageNET classification task to medical modality such as in the work of <ref type="bibr" coords="2,384.50,260.81,10.79,8.64" target="#b0">[1,</ref><ref type="bibr" coords="2,395.29,260.81,7.19,8.64" target="#b5">6]</ref>, where the authors of Novasearch team <ref type="bibr" coords="2,218.45,272.77,11.62,8.64" target="#b0">[1]</ref> evaluate the CNNs classifier with medical images in order to build a Medical Image Retrieval System (MIRS) to classify each subfigure, from a collection of figures from compound images found in biomedical literature.</p><p>Another work of subfigure classification task at ImageCLEF 2016 <ref type="bibr" coords="2,415.24,308.82,11.62,8.64" target="#b5">[6]</ref> used modern Deep CNNs in order to predict the modality of a medical image with two main groups : Diagnostic Images and Generic Biomedical Illustration. To extract information from medical images and build their textual features, they used Bag-of-Words (BoW) and Bag of Visual Words (BoVW) approaches.</p><p>In the work of <ref type="bibr" coords="2,207.82,368.78,11.62,8.64" target="#b1">[2]</ref> they used Multi-modal Factorized Bilinear(MFB) pooling as well as Multimodal Factorized High-order(MFH) pooling to solve the task in order to build a system that is able to reason over medical images and questions and generate the corresponding answers at ImageCLEF Med-VQA 2018 Task.</p><p>The main idea proposed by <ref type="bibr" coords="2,264.10,416.78,11.62,8.64" target="#b6">[7]</ref> is about automatically generate questions and images selected from the literature based on ImageCLEF data where they apply Stacked Attention Network (SAN) which was proposed to allow multi-step reasoning for answer prediction, and Multimodal Compact Bilinear pooling (MCB) with two attention layers based on CNNs.</p><p>The authors of <ref type="bibr" coords="2,212.56,476.74,11.62,8.64" target="#b0">[1]</ref> introduce VQA-RAD, a manually constructed VQA dataset in radiology where clinicians asked naturally occurring questions about radiology images and provided reference answers in order to encourage the community to design VQA tools with the goals of improving patient care where they use a balanced images sample from MedPix. The annotation of the dataset was generated by volunteer clinical trainees and validated by expert radiologists, they train their data using deep learning and bootstrapping approaches. They provide the data in JSON, XML, and Excel format. The final VQA-RAD dataset contains 3515 total visual questions.</p><p>Another line of work in <ref type="bibr" coords="2,248.81,572.57,11.62,8.64" target="#b7">[8]</ref> focuses in a new ways of synthesizing QA pairs from currently available image description datasets. They propose to use neural networks and visual semantic embeddings using LSTM on MS-COCO dataset. Their final model was not able to consume image features as large as 4096 dimensions at one time step, where the dimensionality reductions lose some useful information.</p><p>Deep neural networks have recently achieved very good results in representation learning and classification of images. With all this effort, there is still no widely used method to construct these systems. This is due to the fact that the medical domain requires high accuracy and especially the rate of false negatives to be very low, so we studied several VQA networks and we selected deep neural networks models for our participation in VQA-Med 2019.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method and Results</head><p>To solve the task of VQA-med at image CLEF 2019, we chose to use CNN and RNN models without intermediate stages such as object detection and image segmentation.</p><p>All existing methods and VQA algorithms consist of:</p><p>-Extracting image features (image featurization).</p><p>-Extracting question features (question featurization).</p><p>-Combining features to produce an answer <ref type="bibr" coords="4,320.48,202.87,11.62,8.64" target="#b3">[4]</ref> (see figure <ref type="figure" coords="4,378.85,202.87,3.60,8.64" target="#fig_1">1</ref>).</p><p>In our case, we chose the approach used by <ref type="bibr" coords="4,325.00,222.25,10.58,8.64" target="#b7">[8]</ref>. We treat the task as a classification problem and we apply neural network and visual semantic embeddings method. We assume that the answers consist of only a single word. The process of building the classification model includes preprocessing and extraction of visual features from already labelled images and their own questions. Our system learns image regions relevant to answer the clinical questions. Images and question are represented as global features which are merged to predict the answers. The effectiveness of the model is evaluated by using new images.</p><p>We have used visual semantic embeddings to connect a CNN and a Recurrent Neural Networks (RNN). Our model is built on the basis of the LSTM (Long short-term memory) which is an easier form of RNN to train the dataset. Because of its very uniform architecture for extracting features from images we used 16 convolutional layers of VGG-16 and in order to generate questions as inputs examples, we used RNN which is the apropriate technique to use with sequential data <ref type="bibr" coords="4,358.88,524.93,10.58,8.64" target="#b8">[9]</ref>. The LSTM(s) outputs are introduced into a softmax layer to generate answers.</p><p>The answer prediction task is modeled as N-class classification problem and performed using a one-layer neural network.</p><p>Our model results are shown in the table below (see table <ref type="table" coords="4,380.24,572.75,3.60,8.64" target="#tab_0">2</ref>).</p><p>The analysis of the results obtained by all the participants, in terms of accuracy and BLEU, shows that the best approach is the one used by the Hanlin team. The results obtained by all the participants vary between 0.624 and 0 in terms of accuracy and between 0.644 and 0.025 in terms of BLEU. The Hanlin team thus obtained the highest scores (0.624, 0.644) while the IITISM @ CLEF team obtained the lowest scores (0.0, 0.025). The results obtained by our system (0.462, 0.485) compared with other systems are encouraging and we hope to make improvements in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we present techno team approach used in VQA at ImageCLEF 2019 task. We evaluate currently existing VQA system by testing a publicly available VQA network.</p><p>We found that the RNN model based on the feature fusion is helpful on improving the system's performance. But it is still very naive in many situations.</p><p>It should be noted that we encountered the problem of overfitting which is a major problem in neural networks.</p><p>As a result, we achieved 0.486 BLEU score in the challenge. In the future we consider working on in order to obtain the optimum deep learning layer structure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,170.03,85.01,10.75;3,134.77,192.00,54.50,8.96;3,134.77,209.01,281.21,8.64;3,141.74,224.41,338.85,9.03;3,151.70,236.75,30.71,8.64;3,141.74,247.49,338.86,9.03;3,151.70,259.83,48.71,8.64;3,141.74,270.57,279.21,9.03;3,134.77,299.53,345.82,8.64;3,134.77,311.49,74.15,8.64"><head></head><label></label><figDesc>In the scope of the VQA-Med challenge, three datasets were provided:-The training set contains 12792 question-answer pairs associated with 3200 training images. -The validation set contains 2000 question-answer pairs associated with 500 validation images. -The test set contains 500 questions associated with 500 test images. The classes for each question category are: Modality, Plane, Organ system and Abnormality (see table1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,341.14,345.83,8.12;3,134.77,352.45,127.50,7.77;3,157.14,484.02,148.69,7.77;3,157.14,494.98,138.46,7.77;3,319.89,494.98,119.93,7.77;3,157.14,611.85,159.97,7.77;3,157.14,622.81,90.79,7.77;3,319.89,622.81,111.33,7.77"><head>Table 1 .</head><label>1</label><figDesc>Example of a medical images and the associated questions and answers from the training set of ImageCLEF 2019 VQA-Med Q: what type of imaging modality is used to acquire the image? A: us ultrasound Q: what plane was used? A: axial Q:what organ system is evaluated primarily? A:face, sinuses, and neck Q: is this image normal? A:yes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,263.56,362.32,88.24,8.12;4,152.05,276.46,311.25,71.12"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Model process</figDesc><graphic coords="4,152.05,276.46,311.25,71.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,256.86,115.83,101.64,50.37"><head>Table 2 .</head><label>2</label><figDesc>Techno team score.</figDesc><table coords="5,278.72,147.07,57.92,19.13"><row><cell cols="2">accuracy BLEU</cell></row><row><cell>0.462</cell><cell>0.486</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,138.13,417.91,342.46,7.77;5,146.47,428.87,334.12,7.77;5,146.47,439.83,79.19,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,474.12,417.91,6.47,7.77;5,146.47,428.87,294.43,7.77">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soumya</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abacha</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,446.72,428.87,33.88,7.77;5,146.47,439.83,15.63,7.77">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180251</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.13,450.52,342.46,7.77;5,146.47,461.48,153.92,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,313.38,450.52,167.21,7.77;5,146.47,461.48,39.34,7.77">UMass at ImageCLEF Medical Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Yalei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Feifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Max</forename><forename type="middle">P</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,193.04,461.48,78.96,7.77">Med-VQA) 2018 Task</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.13,472.16,342.46,7.77;5,146.47,483.12,334.12,7.77;5,146.47,494.08,250.84,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,392.41,472.16,88.18,7.77;5,146.47,483.12,171.63,7.77">Application of data mining techniques for medical image classification</title>
		<author>
			<persName coords=""><forename type="first">Maria-Luiza</forename><surname>Antonie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zaiane</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Osmar</forename><forename type="middle">R</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandru</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,321.67,483.12,158.92,7.77;5,146.47,494.08,127.64,7.77">Proceedings of the Second International Conference on Multimedia Data Mining</title>
		<meeting>the Second International Conference on Multimedia Data Mining</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.13,504.76,342.46,7.77;5,146.47,515.72,315.39,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,288.95,504.76,191.64,7.77;5,146.47,515.72,63.63,7.77">Visual question answering: Datasets, algorithms, and future challenges</title>
		<author>
			<persName coords=""><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,214.10,515.72,149.04,7.77">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="2017">2017</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.13,526.40,342.46,7.77;5,146.47,537.36,334.12,7.77;5,146.47,548.32,334.12,7.77;5,146.47,558.99,334.12,8.06;5,146.47,570.24,26.15,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,296.74,537.36,183.85,7.77;5,146.47,548.32,166.04,7.77">VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Asma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><forename type="middle">V</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="5,322.36,548.32,106.19,7.77">CLEF2019 Working Notes</title>
		<title level="s" coord="5,433.26,548.32,47.33,7.77;5,146.47,559.28,88.57,7.77">CEUR Workshop Proceedings.CEUR</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 9-12. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.13,580.92,342.46,7.77;5,146.47,591.88,334.12,7.77;5,146.47,602.84,44.08,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,296.17,580.92,184.42,7.77;5,146.47,591.88,224.61,7.77">Traditional Feature Engineering and Deep Learning Approaches at Medical Classification Task of ImageCLEF 2016</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,376.89,591.88,83.67,7.77">CLEF (Working Notes)</title>
		<imprint>
			<biblScope unit="page" from="304" to="317" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.13,613.52,342.46,7.77;5,146.47,624.48,334.12,7.77;5,146.47,635.44,88.90,7.77" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="5,255.04,624.48,225.56,7.77;5,146.47,635.44,60.51,7.77">NLM at ImageCLEF 2018 Visual Question Answering in the Medical Domain</title>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soumya</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sivaramakrishnan</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.13,646.13,342.46,7.77;5,146.47,657.08,315.55,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,328.81,646.13,151.78,7.77;5,146.47,657.08,51.38,7.77">Exploring models and data for image question answering</title>
		<author>
			<persName coords=""><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,204.00,657.08,184.89,7.77">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2953" to="2961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.13,119.96,342.46,7.77;6,146.47,130.92,266.53,7.77" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="6,335.87,119.96,144.72,7.77;6,146.47,130.92,115.54,7.77">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName coords=""><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05101</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,142.24,141.88,338.35,7.77;6,146.47,152.84,334.12,7.77;6,146.47,163.80,334.12,7.77;6,146.47,174.76,334.12,7.77;6,146.47,185.71,334.12,7.77;6,146.47,196.67,334.12,7.77;6,146.47,207.63,334.12,7.77;6,146.47,218.59,334.12,7.77;6,146.47,229.55,334.12,7.77;6,146.47,240.51,334.12,7.77;6,146.47,251.47,334.12,7.77;6,146.47,262.43,105.34,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,335.35,185.71,145.25,7.77;6,146.47,196.67,334.12,7.77;6,146.47,207.63,334.12,7.77;6,146.47,218.59,29.52,7.77;6,322.11,218.59,158.48,7.77;6,146.47,229.55,158.27,7.77">Friedrich and Alba García Seco de Herrera and Narciso Garcia and Ergina Kavallieratou and Carlos Roberto del Blanco and Carlos Cuevas Rodríguez and Nikos Vasillopoulos and Konstantinos Karampidis and Jon Chamberlain and</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cid</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitri</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aleh</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,312.16,229.55,168.43,7.77;6,146.47,240.51,334.12,7.77;6,146.47,251.47,80.00,7.77">Proceedings of the 10th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="6,232.04,251.47,148.96,7.77">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF 2019)<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">September 9-12, (2019</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
