<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,182.39,115.96,250.57,12.62;1,196.10,133.89,223.15,12.62">Image Steganalysis with Very Deep Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,273.50,171.56,68.36,8.74"><forename type="first">Naoya</forename><surname>Mamada</surname></persName>
							<email>mamada.n.aa@d.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Nagatsuta, Yokohama</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,182.39,115.96,250.57,12.62;1,196.10,133.89,223.15,12.62">Image Steganalysis with Very Deep Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9E0A7DE27602E89B17478D980A8C3F71</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Steganalysis</term>
					<term>Deep Learning</term>
					<term>Convolutional Neural Network</term>
					<term>Image Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Steganography is a technique that embeds secret messages into commonplace data. In contrast, steganalysis is a technique to identify steganography-applied data and to recover hidden message in that data. Effective steganalysis methods are in demand for it is suspected that steganography is made use of by antisocial groups or persons to hide messages from police or intelligence agencies. In this situation, Im-ageCLEF 2019 Security is held to showcase image steganalysis methods. In the competition track 2: stego image discovery we used natural image classification deep learning models to tackle the problem and get F1 score 0.660, precision score 0.508 and recall score 0.944 to win the third place.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Steganography is a technique that embeds secret messages, into commonplace data such as family pictures, pieces of classical music or scenery videos. In contrast, steganalysis is a technique to identify steganography-applied data and to recover hidden messages in that data. Effective steganalysis methods are in demand for it is suspected that steganography is made use of by antisocial groups or persons to hide messages from police or intelligence agencies. In this situation, ImageCLEF 2019 Security <ref type="bibr" coords="1,255.58,497.82,21.03,8.74">[3][4]</ref> is held to showcase image steganalysis methods. ImageCLEF 2019 Security has 3 tasks; Task 1: Identify Forged Images, Task 2: Identify Stego Images and Task 3: Retrieve the Message. In Task 1, we are tasked to identify files' original extensions. In Task 2, we are tasked to identify steganography-applied images (stego images). In Task 3, we are tasked to recover hidden messages from stego images. We participated in task 1 and task 2. In task 1, only the first 4 bytes of signature were tampered and we perfectly classified the files using the preserved part of file signatures bytes. As task 1 has the trivial solution, in this working note, we will describe our solution for task 2. Our contribution is that we show that image classification models for natural images are usable for steganalysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Steganalysis methods are divided into statistics based methods and feature based methods. The former detects anomalous statistics such as least significant bit(LSB) <ref type="bibr" coords="2,200.52,165.85,10.52,8.74" target="#b6">[7]</ref> to distinguish stego images from normal images. The latter detects anomalous image features such as discrete cosine transformation coefficients patterns <ref type="bibr" coords="2,201.90,189.76,10.52,8.74" target="#b5">[6]</ref> or hue patterns. As deep learning models show an impressive ability to recognize patterns in images, many researchers propose deep learning based steganalysis methods recently. However, many of them use relatively shallow networks such as 6 layers <ref type="bibr" coords="2,284.06,225.62,14.61,8.74" target="#b12">[13]</ref>, 14 layers <ref type="bibr" coords="2,347.54,225.62,15.50,8.74" target="#b13">[14]</ref> or 20 layers <ref type="bibr" coords="2,420.67,225.62,14.61,8.74" target="#b11">[12]</ref>, and very deep networks such as 51 layers <ref type="bibr" coords="2,277.64,237.58,10.52,8.74" target="#b7">[8]</ref> or 60 layers <ref type="bibr" coords="2,346.16,237.58,15.50,8.74" target="#b10">[11]</ref> are rarely used and Wu et al. <ref type="bibr" coords="2,149.01,249.53,15.50,8.74" target="#b10">[11]</ref> reported that when the number of layers is smaller than 50, the detection rate decreases as the number increases but model with 60 layers showed overfitting phenomenon and the accuracy degraded. The important difference between steganalysis deep learning models and natural image classification deep learning models is that the most of the former have predefined and fixed high pass convolution layer in the top of the networks <ref type="bibr" coords="2,355.95,309.31,15.06,8.74" target="#b12">[13]</ref>[14] <ref type="bibr" coords="2,386.06,309.31,15.06,8.74" target="#b11">[12]</ref>[8] <ref type="bibr" coords="2,412.41,309.31,15.06,8.74" target="#b10">[11]</ref>. In spite of these works, we show that very deep natural image classification models can be diverted to steganalysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Materials and Methods</head><p>We used ImageNet <ref type="bibr" coords="2,223.09,387.07,10.52,8.74" target="#b0">[1]</ref> pretrained SE-ResNeXt-50 <ref type="bibr" coords="2,359.34,387.07,10.52,8.74" target="#b1">[2]</ref> and SE-ResNeXt-101 to classify images. We trained each model for 25 epochs with batch size 50. Optimizer was Momentum SGD and the learning rate was initially 0.2 and decayed by cosine annealing <ref type="bibr" coords="2,222.48,422.94,9.96,8.74" target="#b4">[5]</ref>. Loss function was softmax cross entropy. We conducted random flipping and random cropping of 96 2 out of the original images as data augmentation. We used Chainer <ref type="bibr" coords="2,280.04,446.85,15.50,8.74" target="#b9">[10]</ref> deep learning library in experiments. For reference, we tested stegdetect <ref type="bibr" coords="2,270.53,458.80,10.52,8.74" target="#b5">[6]</ref> to identify stego images.</p><p>For submission rank 15 and 16, we trained SE-ResNeXt-101 and SE-ResNeXt-50 in a 5-fold cross validation manner. We pick up the smallest validation loss weights and inferred test images. We conducted 10-crop of 96 2 patch as a test time augmentation <ref type="bibr" coords="2,221.49,506.62,9.96,8.74" target="#b8">[9]</ref>. For submission rank 10, we trained 5 SE-ResNeXt-101 models in a 5-fold cross validation manner. With different 2 random seed for fold splitting, we conducted the training and finally get 10 different models. Averaging the 10 predictions on each testing image, we got the ensemble prediction score. For submission rank 23, we used stegdetect 'simple' mode. For submission rank 26, we mistakenly submitted the submission rank 16, 0s and 1s oppositely.</p><p>We trained SE-ResNeXt-50 from random initialization but it was too unstable and we abandoned it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Table <ref type="table" coords="2,162.13,644.16,4.98,8.74" target="#tab_0">1</ref> and Figure <ref type="figure" coords="2,221.15,644.16,4.98,8.74">1</ref> show the results of our submissions. All the deep learning models outperformed stegdetect, classifier based on jpeg discrete cosine trans-form coefficients statistics. Model ensemble (submission rank 10) shows consistent improvement over single models. However, the effects of model depth (submission rank 15 and 16) are unclear in the results. Fig. <ref type="figure" coords="3,217.63,470.13,4.13,7.89">1</ref>. Results of our submissions on the leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>All of the deep learning models, the results show, have high recall and low precision. It means that the models tend to classify normal images as stego images. The reason for this bias is unclear but it possibly because the models are trained to search for structures like block noise. Figure <ref type="figure" coords="3,376.12,584.39,4.98,8.74">2</ref> is a positive sample in the training set. There are many black squares those are faintly visible on the white background. We regard that those squares are signs of stego images in this dataset. Such a pattern can be seen in normal jpeg images because of the block noise phenomenon, especially when the images are intensively compressed images. We consider that the models can detect such patterns but cannot classify stego signs from block noise well. The superior performance of the ensemble model (submission rank 10) possibly because of the improvement of classification performance between the block-noise and stego signs. We point out that performances of 101-layers model (submission rank 16) is slightly better than that of 50-layers model. This is contrary to Wu et al. <ref type="bibr" coords="4,328.64,154.86,14.61,8.74" target="#b10">[11]</ref>, which found the model start to degrade when the depth is deeper than 50. It appears that ImageNet-pretrainig contributes to stable training and prevents deeper-model degradation.</p><p>In this note, we have presented to use natural image classification deep learning models for stego image analysis. We have shown that the deep learning models can outperform a traditional model that is based on cosine discrete transform coefficients statistics. And we have shown that the model ensemble technique can boost the performance. We also have found that with ImageNet pretraining we can use very deep neural networks for steganalysis without degradation. We believe that to test ImageNet pre-trained very deep networks with predefined high-pass filters is a promising next step.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,163.47,175.19,285.34,81.71"><head>Table 1 .</head><label>1</label><figDesc>Results of our submissions on the leaderboard.</figDesc><table coords="3,163.47,193.85,285.34,63.06"><row><cell cols="4">Submission Rank Run ID F-Measure Precision Recall</cell><cell>Model</cell></row><row><cell>10</cell><cell>26830</cell><cell>0.660</cell><cell cols="2">0.508 0.944 SE-ResNeXt-101</cell></row><row><cell>15</cell><cell>26817</cell><cell>0.613</cell><cell cols="2">0.473 0.872 SE-ResNeXt-101</cell></row><row><cell>16</cell><cell>26771</cell><cell>0.613</cell><cell cols="2">0.479 0.852 SE-ResNeXt-50</cell></row><row><cell>23</cell><cell>26787</cell><cell>0.529</cell><cell>0.542 0.516</cell><cell>stegdetect</cell></row><row><cell>26</cell><cell>26770</cell><cell>0.243</cell><cell cols="2">0.673 0.148 SE-ResNeXt-50</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,142.96,536.95,337.63,7.86;4,151.52,547.91,329.07,7.86;4,151.52,558.87,329.07,7.86;4,151.52,569.83,182.07,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,461.91,536.95,18.68,7.86;4,151.52,547.91,205.51,7.86">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2009.5206848" />
	</analytic>
	<monogr>
		<title level="m" coord="4,408.27,547.91,72.32,7.86;4,151.52,558.87,203.51,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06">2009. June 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,142.96,580.44,337.64,7.86;4,151.52,591.37,236.95,7.89" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="4,303.81,580.44,136.92,7.86">Squeeze-and-excitation networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR abs/1709.01507</idno>
		<ptr target="http://arxiv.org/abs/1709.01507" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,142.96,602.00,337.63,7.86;4,151.52,612.96,329.07,7.86;4,151.52,623.92,329.07,7.86;4,151.52,634.88,329.07,7.86;4,151.52,645.84,329.07,7.86;4,151.52,656.80,329.07,7.86;5,151.52,119.67,329.07,7.86;5,151.52,130.63,329.07,7.86;5,151.52,141.59,329.07,7.86;5,151.52,152.55,329.07,7.86;5,151.52,163.51,329.07,7.86;5,151.52,174.47,58.14,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,268.29,656.80,212.30,7.86;5,151.52,119.67,122.19,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380" />
	</analytic>
	<monogr>
		<title level="m" coord="5,294.27,119.67,186.33,7.86;5,151.52,130.63,329.07,7.86;5,151.52,141.59,130.72,7.86;5,308.07,141.59,109.44,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="5,425.42,141.59,55.17,7.86;5,151.52,152.55,115.65,7.86;5,151.52,163.51,164.28,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF 2019)<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-12">September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings (CEUR-WS</note>
</biblStruct>

<biblStruct coords="5,142.96,185.43,337.64,7.86;5,151.52,196.39,329.07,7.86;5,151.52,207.34,329.07,7.86;5,151.52,218.30,145.96,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,259.98,196.39,200.13,7.86">Overview of the ImageCLEFsecurity 2019 task</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cuevas Rodrguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,151.52,207.34,289.45,7.86">CLEF2019 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,229.26,337.64,7.86;5,151.52,240.20,236.95,7.89" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,256.98,229.26,192.27,7.86">SGDR: stochastic gradient descent with restarts</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>CoRR abs/1608.03983</idno>
		<ptr target="http://arxiv.org/abs/1608.03983" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,251.18,38.20,7.86;5,207.18,251.18,12.03,7.86;5,245.24,251.18,59.41,7.86;5,330.68,251.18,36.87,7.86;5,393.58,251.18,17.92,7.86;5,437.52,251.18,43.07,7.86;5,151.52,262.14,349.15,7.86" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Provos</surname></persName>
		</author>
		<ptr target="http://www.outguess.org/detection.php" />
		<title level="m" coord="5,245.24,251.18,59.41,7.86;5,330.68,251.18,36.87,7.86;5,393.58,251.18,17.92,7.86;5,437.52,251.18,39.15,7.86">Steganography detection with stegdetect</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,273.10,337.63,7.86;5,151.52,284.03,254.94,7.89" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="5,249.59,273.10,226.94,7.86">Steganalysis: Detecting LSB steganographic techniques</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<idno>CoRR abs/1405.5119</idno>
		<ptr target="http://arxiv.org/abs/1405.5119" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,295.02,337.64,7.86;5,151.52,305.98,222.26,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="5,248.36,295.02,169.77,7.86">Spatial image steganalysis based on resnext</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Muttoo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCT.2018.8600132</idno>
		<ptr target="https://doi.org/10.1109/ICCT.2018.8600132" />
		<imprint>
			<date type="published" when="2018">10 2018</date>
			<biblScope unit="page" from="1213" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.96,316.93,337.63,7.86;5,151.52,327.87,199.32,7.89" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="5,278.92,316.93,201.67,7.86;5,151.52,327.89,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.62,338.85,337.98,7.86;5,151.52,349.81,60.94,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="5,236.74,338.85,243.85,7.86;5,151.52,349.81,32.28,7.86">Chainer : a next-generation open source framework for deep learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.62,360.77,337.97,7.86;5,151.52,371.70,329.07,7.89;5,151.52,382.69,221.03,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="5,259.46,360.77,177.59,7.86">Deep residual learning for image steganalysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-017-4440-4</idno>
		<ptr target="https://doi.org/10.1007/s11042-017-4440-4" />
	</analytic>
	<monogr>
		<title level="j" coord="5,443.47,360.77,37.12,7.86;5,151.52,371.73,62.42,7.86">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="10437" to="10453" />
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.62,393.65,337.97,7.86;5,151.52,404.58,236.95,7.89" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="5,191.09,393.65,252.69,7.86">Deep convolutional neural network to detect J-UNIWARD</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<idno>CoRR abs/1704.08378</idno>
		<ptr target="http://arxiv.org/abs/1704.08378" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.62,415.56,337.97,7.86;5,151.52,426.50,318.00,7.89" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="5,317.88,415.56,162.71,7.86;5,151.52,426.52,46.22,7.86">Yedrouj-net: An efficient CNN for spatial steganalysis</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yedroudj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Comby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chaumont</surname></persName>
		</author>
		<idno>CoRR abs/1803.00407</idno>
		<ptr target="http://arxiv.org/abs/1803.00407" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,142.62,437.48,337.98,7.86;5,151.52,448.44,329.07,7.86;5,151.52,459.40,172.40,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="5,342.94,437.48,137.66,7.86;5,151.52,448.44,56.25,7.86">Restegnet: a residual steganalytic network</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-019-7601-9</idno>
		<ptr target="https://doi.org/10.1007/s11042-019-7601-9" />
	</analytic>
	<monogr>
		<title level="j" coord="5,220.44,448.44,158.22,7.86">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2019-04">04 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
