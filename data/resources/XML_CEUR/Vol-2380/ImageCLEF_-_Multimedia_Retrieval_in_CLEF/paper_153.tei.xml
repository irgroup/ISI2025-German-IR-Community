<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.10,115.96,311.16,12.62;1,243.46,133.89,128.43,12.62">A Multimedia Modular Approach to Lifelog Moment Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,148.95,171.84,81.33,8.74"><forename type="first">Maxime</forename><surname>Tournadre</surname></persName>
							<email>maxime@tournadre.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Atos BDS</orgName>
								<address>
									<settlement>Bezons</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,238.39,171.84,78.94,8.74"><forename type="first">Guillaume</forename><surname>Dupont</surname></persName>
							<email>guillaume.dupont.rm@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Atos BDS</orgName>
								<address>
									<settlement>Bezons</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.77,171.84,69.94,8.74"><forename type="first">Vincent</forename><surname>Pauwels</surname></persName>
							<email>vincentpauwels8@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Atos BDS</orgName>
								<address>
									<settlement>Bezons</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,403.73,171.84,62.69,8.74"><forename type="first">Bezeid</forename><surname>Cheikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Atos BDS</orgName>
								<address>
									<settlement>Bezons</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.06,183.80,74.00,8.74"><forename type="first">Mohamed</forename><surname>Lmami</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Atos BDS</orgName>
								<address>
									<settlement>Bezons</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.31,183.80,110.98,8.74"><forename type="first">Alexandru</forename><forename type="middle">Lucian</forename><surname>Ginsca</surname></persName>
							<email>alexandru-lucian.ginsca@atos.net</email>
							<affiliation key="aff0">
								<orgName type="institution">Atos BDS</orgName>
								<address>
									<settlement>Bezons</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.10,115.96,311.16,12.62;1,243.46,133.89,128.43,12.62">A Multimedia Modular Approach to Lifelog Moment Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">399ED8234705368A14EB0ED71A114A36</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Natural Language Processing</term>
					<term>Life logging</term>
					<term>Computer Vision</term>
					<term>Multimedia</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lifelogging offers a mean for people to record their day to day life. However, without proper tools for retrieving specific moments, the large volume of collected data is rendered less useful. We present in this paper our contribution to the ImageCLEF Lifelog evaluation campaign. We describe a modular approach that covers both textual analysis and semantic enrichment of queries and targeted concept augmentation of visual data. The proposed retrieval system relies on an inverted index of visual concepts and metadata and accommodates different versions of clustering and filtering modules. We tested over 10 fully automatic combinations of modules and a human guided approach. We observed that with even minimal human intervention, we can obtain a significant increase of the F1 score, ranking fourth in the competition leader board.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, interest in lifelogging appeared with the breakthrough of affordable wearable cameras and smartphones apps. It consists in logging the daily life of an individual using various data (i.e. image, biometrics, location etc.). These objects produce a huge amount of personal data, in various forms, that could be used to build great applications to improve the users life (help for the elderly or semantic video search engine). To research this field, a few datasets were constituted to allow researchers to compare methods through workshops, competitions and tasks.</p><p>Since 2017, IMAGEClef <ref type="bibr" coords="1,257.16,590.58,10.52,8.74" target="#b0">[1]</ref> hosts every year the Lifelog Task <ref type="bibr" coords="1,418.40,590.58,10.52,8.74" target="#b1">[2]</ref> [3] in order to compare different approaches on the same environment. This year, the Lifelog Moment Retrieval Task consists in returning a selection of 10 images for specific topics. IMAGEClef Lifelog provides extracted visual concepts and many metadata for each image as GPS coordinates, the UTC time, the number of steps, the heart rate ect.</p><p>New approaches with beyond state-of-the-art performances are expected to be found during this challenge. This would allow, for instance, to build a powerful semantic search engine and to retrieve specific moments easier. Besides this, there are a wide range of possible application domains, which explains the popularity of related workshops. In this paper we detail multiple automatic approaches as well as a human guided one which give users possibility to choose between predefined filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The origins of image retrieval can be traced back to 1979 when a conference on Database Techniques for Pictorial Applications was held in Florence <ref type="bibr" coords="2,454.40,296.79,9.96,8.74" target="#b3">[4]</ref>. In the beginning, images were first annotated with text and then searched using a text-based approach. Unfortunately, generating automatic descriptive texts for a wide spectrum of images is not feasible without manual help. Annotating images manually is obviously very expensive and it thus limited the text-based methods. In 1992, the National Science Foundation of the United-States organized a workshop to identify new directions in image database management <ref type="bibr" coords="2,436.73,368.52,9.96,8.74" target="#b4">[5]</ref>. It was then recognized that indexing visual information on their inherent properties (shape, color etc.) was more efficient and intuitive. Since then, the application potential of image database management techniques has attracted the attention of researchers. In the past decade, many Content-based image retrieval have been developed.</p><p>For the Lifelog Moment Retrieval (LMRT) task of the LifeLog challenge, the two major criteria to achieve are relevance and diversity. To improve the relevance, most of the approaches relied on the extraction of concepts or features by existing pre-trained models. This is an important time-saver in these challenges. Some systems <ref type="bibr" coords="2,230.14,488.30,10.52,8.74" target="#b5">[6]</ref> chose to augment the data using other models [7] such as ResNet50 <ref type="bibr" coords="2,191.95,500.25,10.52,8.74" target="#b6">[8]</ref> trained on ImageNet for visual concepts, or VGG16 trained on Place365 to retrieve places.</p><p>Other approaches used one binary classifier per topic or classifiers with a minimum confidence level for each topic. Many teams also used a blur detector to filter the images before selection. To ensure diversity, a team <ref type="bibr" coords="2,420.88,548.30,10.52,8.74" target="#b7">[9]</ref> decided to cluster the hyper-features (such as oriented gradient or color histograms features obtained through various models) of the images. Last but not least, all participants used various NLP methods to interpret the topics. In this field, the most popular tools were WordNet <ref type="bibr" coords="2,261.65,596.12,15.50,8.74" target="#b8">[10]</ref> and Word2Vec <ref type="bibr" coords="2,347.47,596.12,14.61,8.74" target="#b9">[11]</ref>.</p><p>In the multimedia research field, IMAGEClef is not alone. Many other related workshops and tasks occur on a regular basis such as NTCIR <ref type="bibr" coords="2,426.71,620.25,15.50,8.74" target="#b10">[12]</ref> or ACM Multimedia <ref type="bibr" coords="2,189.24,632.21,14.61,8.74" target="#b11">[13]</ref>. Launched in the late 1997, NTCIR hold its conference each year in Tokyo. It put an accent on east-Asian language such as Japanese, Korean and Chinese. Like the IMAGEClef competition, a fair number of tasks are proposed, and among them is a LifeLog action retrieval. ACM Multimedia is an international conference which is held each year in a different country. Different workshops revolving around images, text, video, music and sensor data are proposed and a special part of the conference is the art program, which explores the boundaries of computer science and art.</p><p>3 Main components of a modular LMRT system</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>Among the data delivered by the organizers, we have chosen to keep some of them and to add others. We kept the qualitative and persistent data and, therefore, did not take into account the following: lat, long, song, steps, calories, glucose, heart rate and distance. The metadata UTC-time was especially very useful and we will detail its contribution in the clustering part.</p><p>In the following, we will provide more details on the extraction of visual concepts. The user's images were transmitted with visual concepts extracted from several neural networks :</p><p>-The top 10 attributes predicted by using the PlaceCNN <ref type="bibr" coords="3,392.47,346.14,14.61,8.74" target="#b12">[14]</ref>, trained on SUN attribute dataset <ref type="bibr" coords="3,228.72,358.10,14.61,8.74" target="#b13">[15]</ref>. -The top 5 categories and their scores predicted by using the PlaceCNN, trained on Place365 dataset <ref type="bibr" coords="3,276.45,382.01,14.61,8.74" target="#b14">[16]</ref>. -Class name, bounding box and score on up to the best 25 objects for each image. They are predicted by using Faster RCNN <ref type="bibr" coords="3,366.49,405.92,14.61,8.74" target="#b15">[17]</ref>, trained on the COCO dataset <ref type="bibr" coords="3,186.62,417.87,14.61,8.74" target="#b16">[18]</ref>.</p><p>In order to diversify the learning databases we added the following:</p><p>-The top 3 labels with their scores predicted by using VGG19. This architecture was created by VGG (Visual Geometry Group) from the University of Oxford <ref type="bibr" coords="3,185.49,481.63,15.50,8.74" target="#b17">[19]</ref> and trained on ImageNet <ref type="bibr" coords="3,317.25,481.63,14.61,8.74" target="#b18">[20]</ref>. -The top 3 labels with their scores predicted by using ResNet50 trained on ImageNet. We took an implementation of Microsoft which was the winner of ILSVRC 2015 <ref type="bibr" coords="3,226.70,517.50,9.96,8.74" target="#b6">[8]</ref>. -The top 3 labels with their scores predicted by using InceptionV3 implemented by Google <ref type="bibr" coords="3,234.24,541.41,15.50,8.74" target="#b19">[21]</ref> also trained on ImageNet. -The top 10 labels with their scores predicted by using Retinanet <ref type="bibr" coords="3,431.50,553.37,15.50,8.74" target="#b20">[22]</ref> (object detection) implemented by Facebook.</p><p>As another approach, we also built one or two SVM (Support Vector Machines) <ref type="bibr" coords="3,134.77,597.20,15.50,8.74" target="#b21">[23]</ref> per topic thanks to its keywords. They have been created based on the FC2's layer of VGG16 implemented in Keras. We have two versions, the first one is fully automatic with an automatic scraping on the web and the second one is built with guided scraping in order to reduce the noise and to better specify the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Queries</head><p>The very first step of the online system is the interpretation of a topic. How could we define how an image corresponds to an undefined topic? We tried several techniques based on the idea to obtain a semantic representation of each topic. The way the query is interpreted has a direct impact on the ranking, as it will change the input of the ranking system. We chose to keep a set of keywords for each topic. In our toolbox, Python-RAKE<ref type="foot" coords="4,331.87,194.26,3.97,6.12" target="#foot_0">1</ref> is a package used to extract basic keywords from a topic and WordNet allows to generate pre-defined synonyms. Finally, Word2Vec models provide another way to generate synonyms. It will be more detailed in the next section. The way we combined these tools will be detailed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ranking</head><p>For a given query, our system returns a ranking (full or partial) of the images. "How could we rank them?" is the general problem of this part. We explored several methods, from a basic counting of labels selected with a match, to the average of semantic similarities between labels and a topic. Each ranking is specific to a well-defined method and may be more efficient than another depending on the next processing (i.e. clustering, SVM weights...)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Filters</head><p>Another way to change the images selection, is to adjust some requirements. For instance, if we try to retrieve the images corresponding to "restaurant" in our index we would not find 'fast food restaurant' with an exact match criterion. It may look trivial, but changing the way an image is selected opens a wide range of possibilities, especially within the NLP field. We can easily recover images depending on their visual concepts and metadata. However, we may use some Word2Vec models to apply a similarity threshold or use a Levenshtein distance threshold and much more. In our guided method, we also used conditional filters. For instance, the topic "Driving home" where the user must be driving home from work, was treated with two conditions: the user must have been at work in the last hour and he must arrive at his home in the next fifteen minutes.</p><p>Combined with a primary ranking, the filters allow to narrow down the selection to suppress the noise. Both precision and recall were usually improved thanks to these approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Clustering</head><p>Since the evaluation method is F@10 -harmonic mean of precision and recall -it is important to augment the diversity of the selected images. That's the reason why it matters to focus on clustering in order to improve diversity without changing relevance. Consequently, we chose to explore two different approaches, one using temporal similarity and one using visual similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Methods</head><p>We realized 12 runs (11 graded) with a combination of different methods for each run. This section aims at explaining these methods. Every run followed the global pipeline described below. The process will be detailed further in this section. Synonyms -This method helps to diversify the topic representation by adding synonyms to the basic extracted keywords. We use WordNet to generate predefined synonyms based on the WordNet hierarchy. Then, we compute the similarity between these synonyms and the basic keywords. This similarity score enables us to select the relevant synonyms. In our runs, the similarity was computed by the GoogleNews-vector-negative300 which is a Word2Vec model. Another method to compute similarity uses the vector representation of each words in a Word2Vec model. This similarity is useful to filter irrelevant synonyms. Our final method consists of selecting 5 synonyms from WordNet with a threshold (0.5) on the Word2Vec model similarity. Furthermore we keep only synonyms which don't match partially to keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Choice of subset</head><p>We tested different solutions to choose a subset to work on. We usually used one of the following approach to reduce the working set, but in a few runs we kept the whole dataset.</p><p>Match -We first create an inverted index. An inverted index in this case will take a label or a metadata as an input and will return all the images owning the input. Using an inverted index is time-saver and useful to interact with the dataset. Then, we recover through the index the images for which at least one label or metadata is part of the keywords.</p><p>Partial match -Similarly to match, we select labels that match partially at least one of our keywords. Two words are partial match if and only if one is a part of the other (i.e. "food" and "fast-food"). The verification was done through the clause 'in' in Python. This aims to increase the recall but it introduces some noise as well.</p><p>Similarity -Another approach consists in computing the semantic similarity between each key of the index and keywords. Only images that have a label similar enough to a keyword are selected. We didn't use this approach in our submitted run, however a similarity threshold at 0.6 should be efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scoring</head><p>Label counting -In this basic method, the score is the number of time the image was picked in the inverted index detailed earlier in section 4.2.</p><p>Topic Similarity -In this alternative method, we compute the similarity score between each label and keywords. We then pick the maximum similarity for each keyword as a score. The final score for an image is the mean keywords scores.</p><p>SVM -In terms of a visual approach, we used transfer learning to train a dedicated binary SVM for each topic. In the end, this provided us with a probability score for each image to be part of the topic based on visual features.</p><p>The first step to create our SVM was to establish a training set, so we used the Google Search API to get images from Google Images<ref type="foot" coords="6,370.50,564.91,3.97,6.12" target="#foot_1">2</ref> . The negative examples are shared between the SVMs and these are images of scenes and objects from daily life. Here are the words we chose to use for the negative images : bag, car, cat, daily life photography, dog, fish, footballer, horse, newyork, python, smartphone and sunset . The positives were scrapped by querying the title of the topic and a combination of keywords and synonyms. After the scrapping, we filtered manually the positive to avoid the noise.</p><p>Every SVM had around 2000 negative and 200 positive examples. Each image was then processed through VGG16 to extract the visual features, which are the input vectors of our SVM. As we intend to use the probability given by the SVM to establish a threshold in a few runs, we tried to use an isotonic calibration. However, as our training set wasn't big enough, we chose to keep the output probability.</p><p>SVM performances could be upgraded with a bigger training set, calibration and data augmentation. Unfortunately, we didn't have the time and the resources to deal with at this moment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Refinement</head><p>Weighting -The refinement by weighting is based on the idea to combine different scoring methods. In fact, we define a new ranking based on the prediction of multiple approaches.</p><p>Thresholding -In a similar way, the thresholding considers a minimum score in other methods as a prerequisite to be kept in the ranking.</p><p>Visual Clustering -To maximize the diversity of returned images, one approach was to cluster the first 200 images with the highest score on the features extracted by our neural networks. We extracted these features with the second output layer of VGG16 as explained earlier. We clustered the images using the K-means algorithm to retrieve 10 clusters. It greatly improved the diversity of images (i.e. recall score) but inevitably reduced the precision score as only one of many correct images is returned sometimes. It did not improve the overall F@10 score. The second approach for clustering that we will present next proved to be more reliable.</p><p>Temporal Clustering -In order to find the maximum number of moments for a specified topic, the temporal approach seemed to be the most logical. We wanted to form different clusters spaced at least one hour, and to detect noise. Thus, we chose to use DBSCAN<ref type="foot" coords="7,235.56,516.73,3.97,6.12" target="#foot_2">3</ref> (Density-based spatial clustering of applications with noise) on the first 250 images with the highest score.</p><p>This algorithm, created in 1996 just needs two arguments: a distance and a minimum number of points to form a cluster. Consequently, we converted the UTC times into minutes and then applied the algorithm implemented in Scikit-Learn. The best parameters on average for the topics were one hour for the maximal distance between two points to be considered as the same moment and 5 images minimum to form a moment.</p><p>In addition, we also used a blur filter from the OpenCV library to reject all blurry images. Once the blur filter and the clustering were completed, we had to select ten images. Finally, we chose to show the images with the best scores from different clusters. However, many other methods may be used to implement the image selection between clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Formatting</head><p>The last process before sending the run to the evaluation system, is to normalize our score to obtain a confidence score ([0,1]) and combine the topics results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Detailed runs</head><p>Automatic runs With these selected runs we wanted to establish a few comparisons between our methods. They will be analyzed in section 5. Table <ref type="table" coords="8,447.65,251.28,4.98,8.74">1</ref> shows the automatic runs.</p><p>Table <ref type="table" coords="8,196.65,292.39,4.13,7.89">1</ref>. Correspondence between automatic runs and methods used Table <ref type="table" coords="9,163.23,143.73,4.98,8.74" target="#tab_0">3</ref> shows the results obtained by our different runs in the test set. The scoring is F@10, which is the harmonic mean of two measurements computed on each topic. The first one is the precision, whether the ten first are linked to the given topic. The second one is the proportion of different moments found on the given topic. Thus, good precision is not enough to achieve a high score; both recall and precision should be high to reach a great score.</p><p>From run 1 to 6, "Mixed run" means that additional concepts were extracted (visual) and that process were done on the label (textual). The "Mixed" aspect and 8 is the same, but we add the refinement by the SVM, which rely on the visual approach. Run 9 to 11 do not use the textual approach during the process, however they were trained beforehand. Table <ref type="table" coords="9,177.44,500.54,4.98,8.74" target="#tab_1">4</ref> presents the comparison that our runs allow us to do. Thus we are able to define which method worked best in this specific context. An attempt to generalize does not guarantee identical results. Despite the risk for Time Clustering to reduce the precision if there is not enough moments found in the K first images, its usage constantly increased the F@10 score. In a similar way, weight a ranking with SVM's prediction shows an increase in F@10. Combining these two refinement methods may be great.</p><p>However, the winner of these runs is clearly the human-guided method. Combining the human understanding and the mixed automatic runs, it reaches 0.255 at F@10 where our automatics runs didn't surpass 0.116. Then it would be interesting, in future works, to establish a hybrid framework which asks the operator to represent topics through filters and then make a ranking with the automatic approaches. It would use SVM to weight the confidence and Time Clustering to ensure cluster diversity. Figure <ref type="figure" coords="9,271.89,656.12,4.98,8.74">2</ref> gives an example of how it works. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this working-note we presented our approach to the LMRT task of the LifeLog IMAGEClef competition. We chose two frameworks: one fully automatic, and one with human assistance. We extracted feature vectors, and used meta-data on location and time for each image. Our approach relies on linking keywords and their synonyms from topics to our data. We made the human assistance framework available because some filters work better on some topics than others.</p><p>To cluster the remaining images, we found out that the time clustering had better results than the visual clustering. The sole purpose of clustering was to improve the diversity of moments retrieved. It seems logical that the more images are separated in time, the more they can fit in different moment. Moreover, the DBSCAN algorithm selects automatically the number of clusters and identifies noise images. Therefore, it is superior to the k-means algorithm used in visual clustering. DBSCAN did not achieve great results on visual clustering because the distance between feature vectors is not well defined.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,202.27,416.80,210.81,7.89;5,134.77,204.99,345.82,197.03"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. General pipeline for the treatment of a topic</figDesc><graphic coords="5,134.77,204.99,345.82,197.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="10,134.77,266.65,345.82,148.26"><head></head><label></label><figDesc></figDesc><graphic coords="10,134.77,266.65,345.82,148.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,226.87,295.27,160.69,167.13"><head>Table 3 .</head><label>3</label><figDesc>Results obtained on test set</figDesc><table coords="9,226.87,314.33,160.69,148.08"><row><cell>Run</cell><cell>Category</cell><cell>F@10 on test</cell></row><row><cell cols="2">1 Automatic -Mixed</cell><cell>0.077</cell></row><row><cell cols="2">2 Automatic -Mixed</cell><cell>0.036</cell></row><row><cell cols="2">3 Automatic -Mixed</cell><cell>0.036</cell></row><row><cell cols="2">4 Automatic -Mixed</cell><cell>0.078</cell></row><row><cell cols="2">5 Automatic -Mixed</cell><cell>0.053</cell></row><row><cell cols="2">6 Automatic -Mixed</cell><cell>0.083</cell></row><row><cell cols="2">7 Automatic -Mixed</cell><cell>0.101</cell></row><row><cell cols="2">8 Automatic -Mixed</cell><cell>0.068</cell></row><row><cell cols="2">9 Automatic -Visual</cell><cell>0.099</cell></row><row><cell cols="3">10 Automatic -Visual Not evaluated</cell></row><row><cell cols="2">11 Automatic -Visual</cell><cell>0.116</cell></row><row><cell cols="2">12 Human-Guided</cell><cell>0.255</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,134.77,115.91,345.82,321.65"><head>Table 4 .</head><label>4</label><figDesc>Comparison of the methods used -Bold shows the cases where one method was significantly better than the other</figDesc><table coords="10,141.72,147.67,331.91,91.29"><row><cell>Runs</cell><cell>Compared methods</cell><cell>Best result</cell></row><row><cell>1 -2</cell><cell>Time vs Visual clustering</cell><cell>Time Clustering</cell></row><row><cell>3 -4 , 5 -6</cell><cell>Topic Similarity vs Selection</cell><cell>Selection</cell></row><row><cell>3 -5 , 4 -6</cell><cell>Keywords vs Keywords + Synonyms</cell><cell>Keywords + Synonyms</cell></row><row><cell>6 -9</cell><cell>Classic vs SVM</cell><cell>SVM</cell></row><row><cell>7 -8</cell><cell>Weighted vs Threshold use of SVM result</cell><cell>Weighted</cell></row><row><cell>9 -11</cell><cell>SVM vs SVM + Time Clustering</cell><cell>SVM + Time Clustering</cell></row><row><cell>11 -12</cell><cell>Automatic vs Human-Guided</cell><cell>Human-Guided</cell></row></table><note coords="10,168.65,429.68,278.06,7.89"><p>Fig. 2. Example of feasible framework for Lifelog Moments Retrieval</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.73,656.80,235.45,7.86"><p>Python-RAKE : https://github.com/fabianvf/python-rake</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.73,656.80,236.05,7.86"><p>https://developers.google.com/custom-search/v1/overview</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="7,144.73,656.80,327.07,7.86"><p>https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>The main difficulty regarding the LMRT task was to process a great deal of multimodal data and find the optimal processing. Fine tuning parameters and thresholds by hand was often a good method but it limits the scalability of the system. As each topic requires slightly different approaches, there is still some work to do to achieve a fully automatic and adaptive system for moment retrieval such as improving the topics interpretation and the automatic setting of appropriate filters.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Interactive runs During our interactive runs, the user had to interpret a topic through filters. The subset selection is done by these filters. However, the keywords and synonyms are still extracted as they will be needed to compute the scoring of a few methods. Table <ref type="table" coords="8,276.59,542.24,4.98,8.74">2</ref> shows the interactive runs. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,254.24,337.64,7.86;11,151.52,265.20,329.07,7.86;11,151.52,276.16,329.07,7.86;11,151.52,287.12,329.07,7.86;11,151.52,298.07,329.07,7.86;11,151.52,309.03,329.07,7.86;11,151.52,319.99,329.07,7.86;11,151.52,330.95,329.07,7.86;11,151.52,341.91,329.07,7.86;11,151.52,352.87,329.07,7.86;11,151.52,363.83,329.07,7.86;11,151.52,374.79,20.99,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,216.07,309.03,264.52,7.86;11,151.52,319.99,80.17,7.86">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="11,256.74,319.99,223.85,7.86;11,151.52,330.95,61.50,7.86;11,239.20,330.95,110.28,7.86;11,165.87,352.87,249.22,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<title level="s" coord="11,357.69,330.95,122.90,7.86;11,151.52,341.91,44.73,7.86;11,207.28,363.83,166.04,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12 2019</date>
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings (CEUR-WS</note>
</biblStruct>

<biblStruct coords="11,142.96,386.55,337.64,7.86;11,151.52,397.51,329.07,7.86;11,151.52,408.47,329.07,7.86;11,151.52,419.43,329.07,7.86;11,151.52,430.38,91.16,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,264.50,397.51,216.09,7.86;11,151.52,408.47,136.39,7.86">Overview of ImageCLEFlifelog 2019: Solve my life puzzle and Lifelog Moment Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V.-T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="11,310.97,408.47,169.61,7.86;11,151.52,419.43,68.81,7.86">CLEF2019 Working Notes, CEUR Workshop Proceedings</title>
		<title level="s" coord="11,325.54,419.43,21.70,7.86">CEUR</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">September 09-12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,442.15,337.63,7.86;11,151.52,453.11,329.07,7.86;11,151.52,464.07,329.07,7.86;11,151.52,475.02,218.27,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,156.22,453.11,324.38,7.86;11,151.52,464.07,32.92,7.86">Overview of imagecleflifelog 2018: daily living understanding and lifelog moment retrieval</title>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org¿" />
	</analytic>
	<monogr>
		<title level="m" coord="11,206.32,464.07,274.26,7.86;11,151.52,475.02,11.46,7.86">CLEF2018 Working Notes (CEUR Workshop Proceedings). CEUR-WS</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,486.79,332.82,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="11,194.73,486.79,183.98,7.86">Data base techniques for pictorial applications</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Blaser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,498.55,337.64,7.86;11,151.52,509.51,155.60,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,189.87,498.55,226.88,7.86">Nsf workshop on visual information management systems</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,427.72,498.55,52.88,7.86;11,151.52,509.51,24.99,7.86">ACM Sigmod Record</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="57" to="75" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,521.27,337.64,7.86;11,151.52,532.23,61.69,7.86;11,139.37,543.99,341.22,7.86;11,151.52,554.95,329.07,7.86;11,151.52,565.91,79.91,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,410.25,521.27,70.34,7.86;11,151.52,532.23,53.73,7.86;11,387.55,543.99,93.05,7.86;11,151.52,554.95,45.15,7.86">A survey on deep transfer learning</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cuevas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,218.39,554.95,220.45,7.86">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
	<note>Retrieving events in life logging</note>
</biblStruct>

<biblStruct coords="11,142.96,577.67,337.64,7.86;11,151.52,588.63,329.07,7.86;11,151.52,599.59,291.57,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,151.52,588.63,215.30,7.86">Resnet winner of the ImageNet Competition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaiming Hewith Xiangyu Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename></persName>
		</author>
		<ptr target="http://image-net.org/challenges/talks/ilsvrc2015deepresiduallearningkaiminghe.pdf" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,611.35,337.63,7.86;11,151.52,622.31,52.29,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,272.33,611.35,208.26,7.86;11,151.52,622.31,44.34,7.86">Multimedia lab@ imageclef 2018 lifelog moment retrieval task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,634.08,294.40,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,213.84,634.08,60.78,7.86">About wordnet</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>University</surname></persName>
		</author>
		<ptr target="https://wordnet.princeton.edu" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,645.84,337.97,7.86;11,151.52,656.80,287.57,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="11,363.83,645.84,116.75,7.86;11,151.52,656.80,122.83,7.86">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,119.67,337.98,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,329.07,7.86;12,151.52,152.55,102.14,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,410.52,119.67,70.07,7.86;12,151.52,130.63,154.59,7.86">Ntcir lifelog: The first test collection for lifelog research</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,329.43,130.63,151.16,7.86;12,151.52,141.59,325.24,7.86">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="705" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,163.51,337.98,7.86;12,151.52,174.47,329.07,7.86;12,151.52,185.43,329.07,7.86;12,151.52,196.39,72.44,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,194.17,174.47,278.51,7.86">Lta 2017: The second workshop on lifelogging tools and applications</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dimiccoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,161.81,185.43,273.41,7.86">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1967" to="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,207.34,337.98,7.86;12,151.52,218.30,329.07,7.86;12,151.52,229.26,154.35,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,390.08,207.34,90.51,7.86;12,151.52,218.30,175.55,7.86">Learning deep features for scene recognition using places database</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,351.15,218.30,129.44,7.86;12,151.52,229.26,72.40,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,240.22,337.98,7.86;12,151.52,251.18,329.07,7.86;12,151.52,262.14,167.43,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,332.37,240.22,148.23,7.86;12,151.52,251.18,171.87,7.86">The sun attribute database: Beyond categories for deeper scene understanding</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,336.48,251.18,144.11,7.86;12,151.52,262.14,24.59,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="59" to="81" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,273.10,337.98,7.86;12,151.52,284.06,329.07,7.86;12,151.52,295.02,229.07,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,401.06,273.10,79.53,7.86;12,151.52,284.06,146.69,7.86">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,309.15,284.06,171.45,7.86;12,151.52,295.02,80.53,7.86">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,305.98,337.97,7.86;12,151.52,316.93,329.07,7.86;12,151.52,327.89,132.80,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,323.10,305.98,157.49,7.86;12,151.52,316.93,160.13,7.86">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,334.07,316.93,146.52,7.86;12,151.52,327.89,60.05,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,338.85,337.97,7.86;12,151.52,349.81,329.07,7.86;12,151.52,360.77,241.46,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,236.68,349.81,181.96,7.86">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,443.42,349.81,37.18,7.86;12,151.52,360.77,120.45,7.86">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,371.73,337.98,7.86;12,151.52,382.69,234.89,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="12,287.15,371.73,193.44,7.86;12,151.52,382.69,70.40,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,393.65,337.98,7.86;12,151.52,404.61,329.07,7.86;12,151.52,415.56,194.31,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,404.96,393.65,75.64,7.86;12,151.52,404.61,133.83,7.86">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,308.78,404.61,171.81,7.86;12,151.52,415.56,91.43,7.86">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,426.52,337.98,7.86;12,151.52,437.48,329.07,7.86;12,151.52,448.44,320.40,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,272.41,437.48,127.99,7.86">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,423.42,437.48,57.18,7.86;12,151.52,448.44,257.20,7.86">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,459.40,337.97,7.86;12,151.52,470.36,329.07,7.86;12,151.52,481.32,83.96,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,375.92,459.40,104.67,7.86;12,151.52,470.36,36.03,7.86">Focal loss for dense object detection</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,207.46,470.36,269.41,7.86">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,492.28,337.97,7.86;12,151.52,503.24,315.42,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,267.36,492.28,213.23,7.86;12,151.52,503.24,57.63,7.86">Support vector machine classification for object-based image analysis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tzotsos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Argialas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,230.73,503.24,114.91,7.86">Object-Based Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="663" to="677" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
