<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,173.83,115.96,267.69,12.62;1,151.31,133.89,312.74,12.62;1,289.23,151.82,36.89,12.62">Predicting Tuberculosis Related Lung Deformities from CT Scan Images Using 3D CNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.90,189.63,63.16,8.74"><forename type="first">Anup</forename><surname>Pattnaik</surname></persName>
							<email>anup.a.pattnaik@pwc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">PricewaterhouseCoopers US Advisory</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,216.88,189.63,72.02,8.74"><forename type="first">Sarthak</forename><surname>Kanodia</surname></persName>
							<email>sarthak.p.kanodia@pwc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">PricewaterhouseCoopers US Advisory</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.16,189.63,76.80,8.74"><forename type="first">Rahul</forename><surname>Chowdhury</surname></persName>
							<email>rahul.r.chowdhury@pwc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">PricewaterhouseCoopers US Advisory</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.93,189.63,67.53,8.74"><forename type="first">Smita</forename><surname>Mohanty</surname></persName>
							<email>smita.u.mohanty@pwc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">PricewaterhouseCoopers US Advisory</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,173.83,115.96,267.69,12.62;1,151.31,133.89,312.74,12.62;1,289.23,151.82,36.89,12.62">Predicting Tuberculosis Related Lung Deformities from CT Scan Images Using 3D CNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2B54A346F36788E66481A864B90DDEA3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D-CNN</term>
					<term>Neural Networks</term>
					<term>Deep Learning</term>
					<term>Medical Imaging</term>
					<term>CT</term>
					<term>Tuberculosis</term>
					<term>ImageCLEF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CT scan of lung has become an invaluable tool in the diagnosis of tuberculosis. However, analysis of 3-D image data is time consuming and relies heavily on trained expertise. As an attempt to automate this approach without compromising accuracy advanced AI algorithms have been explored to draw clinically actionable hypothesis. The approach comprises of detailed image processing, followed by feature extraction using tensor flow and 3-D CNN to further augment the metadata with the features extracted from the image data and finally perform 6 class binary classification using random forest. On the test dataset the method resulted in an overall mean AUC of 0.6.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tuberculosis is a very common and contagious disease caused by the bacteria Mycobacterium tuberculosis and is one of the top 10 causes of death worldwide, according to the World Health Organization (WHO). In 2017, 10 million people were diagnosed with TB and 1.6 million people died from the disease. Medical imaging plays a very important part in diagnosing TB and determining the medical course.</p><p>The ImageCLEF 2019 Tuberculosis task <ref type="bibr" coords="1,330.18,555.30,10.52,8.74" target="#b0">[1]</ref> from ImageCLEF 2019 <ref type="bibr" coords="1,448.22,555.30,10.52,8.74" target="#b1">[2]</ref> consists of 2 subtasks -CT report and severity scoring, both of which focus on improving and automating existing TB diagnosis by analysing medical image through the application of state of the art deep learning techniques and developing a framework capable of extracting necessary information; minimising human intervention throughout the process. The aim of the severity scoring subtask was to assess TB severity score on a scale of 1 (critical) to 5 (very good), whereas that of the CT report subtask was to generate an automatic report based on CT image which would include the following information in binary form: Left lung affected, right lung affected, presence of calcifications, presence of caverns, pleurisy, lung capacity decrease. This article describes the solution provided by PwC to the CT report subtask using 3-D convolutional neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Datasets</head><p>For ImageCLEF 2019 tuberculosis task, both the CT report and severity scoring subtasks use the same dataset containing 335 chest 3-D CT scans of TB patients -of which 218 are used for training and 117 are used for testing. The image data is supported by clinically relevant metadata on the following attributes in binary form: disability, relapse, symptoms of TB, comorbidity, bacillary, drug resistance, higher education, ex-prisoner, alcoholic, smoking.</p><p>The 3-D CT images for individual patients were provided in the form of 2-D slices with dimension of 512×512 pixels and number of slices varying from 100 to 150. The CT images were stored in NIFTI file format which stores raw voxel intensities in Hounsfield units (HU) along with the image metadata viz. image dimensions, voxel size in physical units, slice thickness etc.</p><p>Automatic extracted masked image <ref type="bibr" coords="2,306.60,367.93,10.52,8.74" target="#b2">[3]</ref> of the lungs were also provided along with the image data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The solution to the CT report task is based on a two-stage approach: data preprocessing and modeling. In the data pre-processing stage, the CT scan images and mask images were processed to be able to fed in directly to the neural network architecture developed.</p><p>In the pre-processing stage, the CT scan images were resized, slices were concatenated to maintain consistency across patients (20 slices per patient were generated) and segmented. The mask image data along with the CT images were used to extract lung volume. The detailed methodology for pre-processing is explained in detail in the next subsection. The processed scans were then passed through a neural network and features were extracted. The extracted features, lung volume and patient attribute metadata were then fed into a machine learning classifier and trained on the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Pre-processing</head><p>The training image data provided comprised of 3-D CT scanned images of 218 TB patients with slice size of 512×512 pixels and variable number of slices i.e. the images were of size (512, 512, z) where z is the number of slices in the CT scan and it varied depending on the resolution of the scanner. Due to limitation of computation power and to make the framework computationally efficient, the large 3-D images were pre-processed before feeding into a convolution network architecture. The pre-processing of images involved resizing, concatenation and segmentation, to reduce size, have uniform number of slices per patient and find the regions within each image that are more probable of having deformities.</p><p>Resizing The first step of image pre-processing was to downsize the CT images to 150×150 pixels using OpenCV as shown in Fig. <ref type="figure" coords="3,357.91,449.77,3.87,8.74" target="#fig_1">2</ref> Concatenation To handle the problem of non-uniformity in depth in terms of number of slices (ranging from about 100 to 150 per patient) i.e. to have constant number of slices for each patient, concatenation was used to create 20 slices per patient as shown in Fig. <ref type="figure" coords="4,243.22,154.86,4.98,8.74" target="#fig_2">3</ref> below. <ref type="bibr" coords="4,282.52,154.86,10.52,8.74" target="#b3">[4]</ref> The vector representation of consecutive slices were very similar to each other, so in order to maintain heterogeneity between slices, and also to reduce the number of slices, multiple consecutive slices were concatenated. The number of slices to concatenate was determined by dividing the total number of slices for each patient by 20 (e.g. if a patient had 100 slices, every 5 consecutive slices were concatenated to generate 20 slices overall) and then the average value of the concatenated slices was considered as the value of the new slice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. [4]</head><p>For cases where number of processed slices were less than 20, the last slice was appended more than once (in case of 19 or 18 slices). 6 patients were dropped entirely which had less than 18 slices. Custom Mask Generation Apart from the 3-D CT scanned images, masks of the lung images were also provided which are segmented images of the original lung image, created by cutting off the left and right lung fields from the lung parts and removing the surrounding noise. The masks could be used to get the areas of interest within the original image which would then be fed to the deep learning architecture. Since the masked images played a crucial part in the entire classification process, custom masks were generated by performing the steps mentioned in the Segmentation section below. It allowed modification and fine tuning the region of interest within the lungs by changing the hyperparameters involved in the segmentation process. It also served as a benchmark to compare the results obtained by using the masked images that is part of the dataset provided.</p><p>Segmentation The next step in pre-processing was segmentation of lung structures because the regions of interests lies inside the lungs. In the CT scans, the lungs are the darker regions whereas the brighter regions inside the lungs are blood vessels or air. The lung structures were segmented from each slice of the CT scan image and it was tried not to lose the possible region of interests attached to the lung wall.</p><p>The segmentation of lung structures is a very challenging problem because: homogeneity is not present in the lung region, pulmonary structures have similar densities, scanners and scanning protocols for capturing images are usually different. For getting the segmented lungs 8 steps were carried out using scikitimage package. <ref type="bibr" coords="5,203.39,401.96,10.52,8.74" target="#b4">[5]</ref> Conversion to binary image In the first step of segmentation, concatenated slices of images were converted into binary images as shown in Fig. <ref type="figure" coords="5,403.46,439.51,8.49,8.74" target="#fig_3">4a</ref>. Typical radiodensities of various parts of a CT scan are shown in Table <ref type="table" coords="5,393.23,451.46,3.87,8.74" target="#tab_0">1</ref>. A thresholding of -600 HU was applied to segment lung parenchyma. <ref type="bibr" coords="5,359.90,463.42,10.51,8.74" target="#b4">[5]</ref>  Removing the blobs connected to the border In order to filter the noise obtained as a result of segmentation, and to be able to classify the images properly, the regions which were connected to the border of the image were removed as shown in Fig. <ref type="figure" coords="6,166.73,130.95,8.85,8.74" target="#fig_3">4b</ref>. Labelling Connected regions of integer array of the images were labelled, as given in Fig. <ref type="figure" coords="6,165.53,351.82,8.49,8.74" target="#fig_4">5a</ref>, using skimage.measure label function. Two pixels are connected when they are neighbors and have the same value. In 2-D, they can be neighbors either in a 1 or 2-connectivity sense. Labels with 2 largest areas were kept i.e. both lungs and components which had area less than lungs were removed as shown in Fig. <ref type="figure" coords="6,380.45,400.10,8.86,8.74" target="#fig_4">5b</ref>. Erosion Operation Erosion operation (with a disk of radius 2) was performed to separate the lung nodule attached to the blood vessels. It was done using binary erosion() which sets a pixel at (i,j) to the minimum over all pixels in the neighborhood centered at (i,j). Erosion shrunk bright regions and enlarged dark regions as given in Fig. <ref type="figure" coords="6,238.82,656.12,8.49,8.74" target="#fig_5">6a</ref>.</p><p>Closure Operation Closure operation (with a disk of radius 10) <ref type="bibr" coords="7,404.96,118.99,10.52,8.74" target="#b5">[6]</ref> was performed to keep nodules attached to the lung wall. It was done using binary closing() which can remove small dark spots (i.e. pepper) and connect small bright cracks. This tended to close up (dark) gaps between (bright) features as shown in Fig. <ref type="figure" coords="7,134.77,166.81,8.86,8.74" target="#fig_5">6b</ref>. Filling in the small holes inside binary mask Sometimes due to imperfections in the binary conversion identified by the optimal thresholding, a set of background regions (black pixels), lying completely within the foreground regions or region of interest (white pixels), are formed in binary image . This regions known as holes might be useful. To take this region into account, binary fill holes() function was used to fill them and image as shown in Fig. <ref type="figure" coords="7,332.10,428.15,9.96,8.74" target="#fig_6">7a</ref> was obtained.</p><p>Superimposing The last step in the segmentation involved superimposing the binary mask generated on the input image to obtain the region of interest (see Fig. <ref type="figure" coords="7,155.10,474.59,9.59,8.74" target="#fig_6">7b</ref>) which can be fed as an input to a CNN model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lung Volume Extraction</head><p>As additional features the total left and right lung volumes were quantified. This was done using image semantic segmentation. Semantic segmentation is a process of classifying each pixel of an image to an appropriate class. This was carried out using the Deeplab v3 <ref type="bibr" coords="8,286.47,176.40,9.96,8.74" target="#b6">[7]</ref>, which in-turn is based on Googles MIT licensed Tensorflow library within a framework of deep convolutional networks. Deeplab v3 has won several image segmentation competition including PASCAL VOC 2012. <ref type="bibr" coords="8,188.61,212.27,10.52,8.74" target="#b6">[7]</ref> Two innovations in particular have contributed immensely on improving the overall accuracy by Deeplab. One, is a layer of conditional random fields on top of CNN that aids in smoothing the predictions. Second, is atrous convolutions which are novel dilated convolution steps that allow to capture more context around an object without increasing the input data to the CNN model. <ref type="bibr" coords="8,167.14,272.04,10.52,8.74" target="#b7">[8]</ref> CT images from the training data (150 training, 50 validation) were used as input (all slices included) along with the masks provided. Pixels labels for left lung, right lung and background was extracted from the mask images. CT image along with the masks were converted into Tensorflow format (tfrecords) using Deeplab v3 code. One innovation that reduced the training time significantly was transfer learning which was supported by Deeplab v3 code. A pretrained model (named Xception, trained by Google for their winning algorithm for PASCAL VOC 2012 competition <ref type="bibr" coords="8,239.39,368.01,10.79,8.74" target="#b8">[9]</ref>) was used, where the last 2 layers was removed and retrained using CT scan image data along with the masks. Training was carried out for 200,000 iterations and model was evaluated every 10,000 iterations. The final model gave a mean intersection-over-union of 0.879 for the validation data. The model generated, was used to predict one of the 3 classes for every pixel, furthermore total volume of left and right lung was estimated by considering the total number of pixels belonging to each class across all slices for an individual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Flow</head><p>Post the pre-processing and noise removal of the 2-D images of the patients, we obtained 20 slices per patient. Each patient was now represented by a sequence of 20, 150*150 pixel images and a vector with 12 features obtained from the provided patient attribute metadata and additional lung volume features that was generated using the image mask data along with the CT images.</p><p>All the pre-processing steps were performed on the 218 patients train data and it was split in the ratio of 70:30 to create the training and validation sets.</p><p>Deep Learning Architecture A 3-D CNN model was used to process the sequence of images of the patients. The model was implemented using Keras version 2.2.4 with the support of GPUs, which helped reduce the training time of the neural network significantly. The 3-D CNN model training ran on an Ubuntu server machine equipped with 2 NVIDIA Tesla P4 GPU accelerator. The GPUs were accessed using Google Cloud Platform.</p><p>The sequence of images was passed on to the neural network as a tensor of shape (number of images per batch, 150, 150, 20, 1). The tensor was passed through a sequence of Conv3D, MaxPooling3D, BatchNormalization, Dropout and dense layers to finally produce a 6×1 vector for each of the patients in the training set. The 6×1 vector contained confidence scores for the 6 anomalies that are being detected. The neural network was trained to minimise the sum of loss across all the classes.</p><p>The model did not perform well in terms of accuracy and was generating same confidence score for all patients in the validation set for multiple classes. This indicated that the training data was not sufficient to train a neural network with 12 layers and it was necessary to reduce the size of the model architecture. Additionally since the model was trained on minimizing the sum of losses of 6 classes, it was observed that the number of epochs required and hyperparameters tuned for converging to global minima did not match with the individual convergence of 2 classes.</p><p>The second approach involved building a neural network with the same architecture but instead of training the neural network on minimizing the sum of losses across 6 classes, the network was trained on minimizing the loss of individual classes. The idea was to build 6 different neural networks for binary classification of the 6 classes with the same architecture but tuned with different set of hyperparameters. This ensured that all the classes were trained until they converged. The overall sum of losses was better than the previous model. However, this model predicted the confidence scores of all the patients in the test set very close to each other and the scores were always within a limited range. This again suggested that there was insufficient training data to train this model.</p><p>Finally, the size of the neural network was reduced to 8 layers and trained to minimize the sum of losses across all classes. The model performed better than the previous two models and the confidence score had significant variance across patients in the validation set. The model was fed with mini batches of data containing the 20 slices of images. The model was trained for 100 epochs with batch size fixed at 15. The train and validation sets converged to a cross entropy loss of 3.05 and 3.32 respectively within 100 epochs.We used SGD (Stochastic Gradient Descent) as the optimizer with a learning rate of 10 -4 and decay rate of 10 -6 . Gradient Descent is an optimization algorithm, based on a convex function, that tweaks its parameters iteratively to minimize a given function to its local minimum. Stochastic Gradient updates the parameters for each training example, one by one. This makes SGD faster than other widely used optimizers like Batch Gradient Descent. We also tried using the mask data given to us instead of creating it from the input data image through first 2 steps of segmentation mentioned in section 3.1.3 above. The idea was to leverage the mask data provided to us keeping the same final approach as mentioned above and to have a comparative analysis of the results obtained through the 2 approaches before finalising the best approach.</p><p>The deep learning model had several hyperparameters and multiple iterations were run to tune the parameters. The activation function used was tanh as it usually performs well for a binary classification. The kernel and pool size was kept as <ref type="bibr" coords="10,171.05,528.68,11.62,8.74" target="#b7">(8,</ref><ref type="bibr" coords="10,182.67,528.68,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="10,190.42,528.68,7.75,8.74" target="#b7">8)</ref>; relatively larger sizes were used as the data needed to shrink before passing on to the dense layer. The stride size was fixed at (4,4,4) for the same reason. SGD with learning rate e -04 and decay rate e -06 was used. To compensate the low learning rate values, the model was trained on 100 epochs. Multiple combination of the number of neurons were tried in the 2 dense layers (dense 1 and dense 2) and the numbers were fixed at 256 for dense 1 layer and 16 for dense 2 layer. The dropout ratio was tuned at 0.6 to ensure the model does not overfit.</p><p>Machine Learning The deep learning model was trained on the 180 patients in the training dataset and the parameters were tuned. The penultimate dense layer was a vector of shape (16,). This vector was appended with the metadata feature vector of shape (12,) and the resultant vector was trained on a couple of Machine Learning Classifiers. The deep learning model was trained to minimize the sum of losses across classes and the deep learning model itself classified the patients across classes. Using the ML classifiers helped us to take into consideration both the metadata and the 16 element vector which was generated as the penultimate layer of the neural network. The 16 element vector carried the features from the model which was trained to minimize the sum of losses. The features from the deep learning model were leveraged and appended with the metadata to input the resultant vector to a machine learning classifier.</p><p>2 machine learning classifiers: Support Vector Classifier and Random Forest Classifier were tried. Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. GridSearchCV was used to tune the parameters of the model -the regularization parameter C and Gamma parameter. The regularization parameter was used to specify the extent to which misclassifying each training example had to be avoided. The gamma parameter defines how far the influence of a single training example reaches.</p><p>Random forest is an ensemble algorithm. Ensemble algorithms combine more than one algorithm of same or different kind for classifying. Random forest classifier creates a set of decision trees from randomly selected subset of training set. GridSearchCV was used to tune the parameters of the model-n estimators, max depth and max features. n estimators is the number of decision trees considered for making the decision, max depth is the maximum number of levels in each decision tree and max features is the maximum number of features considered for splitting a node. Random forest performed relatively better on the data as compared to SVC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>AUC values from the different approaches that was tried are provided below. The values reported here are on the validation set of 32 patients. 8 layer multi label classification was the final model that was used for the task. That model performed the best on the validation set with a mean AUC of 0.634. The next best model in terms of mean AUC was the multi label classification in which the mask data was leveraged instead of segmenting the original image. One of the drawbacks of that model was that it performed very poorly on classifying patients for the Caverns class. The 6 individual neural net models and the 12 layer multi label classification model both produced similar metrics on the validation set, giving out a mean AUC of 0.58 and 0.59 respectively.</p><p>The results from the 2 submitted runs on the test set are provided below:  <ref type="figure" coords="14,470.63,519.81,9.96,8.74" target="#fig_10">11</ref> shows image slices for each of these patients with the defects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>Due to time and computation resource limitations during the course of the challenge, a lot of possible enhancements to the existing model could not be explored. Some of the enhancements to the pre-processing stage that can be considered as immediate next steps are leveraging the mask image data to get better segmentation and classification output, finding lung nodule candidates and regions of interest from the segmented lungs, augmenting data to take care of class imbalance present in some categories, adding annotations to the data slices, and  considering weights while aggregating slices instead of a simple average. Exhaustive hyperparameter training that could not be explored due to computing limitations, could be explored further.</p><p>Based on recent literature reviews on image analysis using deep learning and looking at multiple submissions to the challenge using deep learning algorithms, it shows that deep learning holds great promise. Exploring other deep learning architectures and evaluating the impact on the model results remains to be seen.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,246.87,319.41,121.62,7.89;3,134.77,115.83,345.83,188.81"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overall approach flow.</figDesc><graphic coords="3,134.77,115.83,345.83,188.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,633.60,166.86,7.89;3,336.07,633.62,144.52,7.86;3,134.77,480.49,345.84,138.34"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. a. Original 512×512 pixels image. b. Resized image of 150×150 pixels.</figDesc><graphic coords="3,134.77,480.49,345.84,138.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,134.77,626.20,345.83,7.89;4,134.77,330.45,345.83,280.97"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Concatenated slices of 3-D CT scan of a patient arranged in a vertical sequence.</figDesc><graphic coords="4,134.77,330.45,345.83,280.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,161.88,291.40,291.60,7.89;6,134.77,163.28,345.83,113.35"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. a. Binary image; b. Image obtained after removing border blobs.</figDesc><graphic coords="6,134.77,163.28,345.83,113.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,169.62,559.83,276.11,7.89;6,134.77,432.43,345.83,112.63"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. a. Labelled image; b. Labelled image keeping 2 largest areas.</figDesc><graphic coords="6,134.77,432.43,345.83,112.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,182.53,322.78,250.31,7.89;7,134.77,195.60,345.84,112.40"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Image after: a. erosion operation; b. closure operation.</figDesc><graphic coords="7,134.77,195.60,345.84,112.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,149.23,633.04,316.90,7.89;7,134.77,505.87,345.84,112.40"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Image after: a. filling small holes; b. superimposing on the input image</figDesc><graphic coords="7,134.77,505.87,345.84,112.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,134.77,470.77,345.83,7.89;10,134.77,481.75,318.40,7.86;10,194.29,115.84,226.77,340.16"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Graphs showing the convergence of the training and validation sets for the 2 classes (Pleurisy and Caverns) which did not converge with the global minima.</figDesc><graphic coords="10,194.29,115.84,226.77,340.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="11,174.95,584.15,265.45,7.89;11,208.46,115.83,198.43,453.55"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Neural network architecture of single binary classification.</figDesc><graphic coords="11,208.46,115.83,198.43,453.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="12,134.77,290.98,345.83,7.89;12,134.77,301.96,162.68,7.86;12,194.29,115.84,226.77,160.37"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Graph showing the training and validation convergence of the model trained on minimizing the loss across all classes.</figDesc><graphic coords="12,194.29,115.84,226.77,160.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="13,134.77,584.15,345.82,7.89;13,134.77,595.14,38.45,7.86;13,134.77,115.83,345.83,453.55"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Neural network architecture of the model which does multi label binary classification.</figDesc><graphic coords="13,134.77,115.83,345.83,453.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="15,196.93,372.58,221.50,7.89;15,134.77,250.88,345.83,106.92"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Image slices of patients: a. 117; b. 140; c. 199.</figDesc><graphic coords="15,134.77,250.88,345.83,106.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,154.38,496.63,306.60,106.27"><head>Table 1 .</head><label>1</label><figDesc>Typical radiodensities of various substances (in HU) in a CT scan</figDesc><table coords="5,210.47,517.90,194.42,85.00"><row><cell>Substance</cell><cell>Radiodensity (HU Range)</cell></row><row><cell>Air</cell><cell>-1000</cell></row><row><cell>Lung</cell><cell>-400 to -600</cell></row><row><cell>Nodule</cell><cell>-150</cell></row><row><cell>Fat</cell><cell>-60 to -100</cell></row><row><cell>Water and Blood</cell><cell>0</cell></row><row><cell>Soft Tissue</cell><cell>+40 to +80</cell></row><row><cell>Bone</cell><cell>+400 to +1000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="14,148.05,115.91,319.25,72.90"><head>Table 2 .</head><label>2</label><figDesc>Model performance from various approaches on validation set</figDesc><table coords="14,148.05,136.68,319.25,52.12"><row><cell>Methods</cell><cell cols="3">Accuracy Mean AUC Min AUC</cell></row><row><cell>8 layer multi label binary classification</cell><cell>0.74</cell><cell>0.634</cell><cell>0.51</cell></row><row><cell>6 separate NN for binary classification</cell><cell>0.72</cell><cell>0.58</cell><cell>0.44</cell></row><row><cell>12 layer multi label binary classification</cell><cell>0.68</cell><cell>0.59</cell><cell>0.42</cell></row><row><cell>Multi label classification using mask data</cell><cell>0.73</cell><cell>0.61</cell><cell>0.36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="14,134.77,341.35,345.83,187.20"><head>Table 3 .</head><label>3</label><figDesc>Model performance from submitted runs on test set Patient 117,140 and 190) from the validation set, with greater lung defects were analysed separately to test the predictions from the model and compare it with the actual flags present in the metadata for 6 classes. Table below lists down the model prediction probabilities and actual flag in metadata for these 3 patients. It can be inferred that the developed model performs well in classifying first 5 classes but caverns prediction can still be improved. Fig.</figDesc><table coords="14,134.77,362.12,328.03,106.65"><row><cell>Methods</cell><cell cols="2">Mean AUC Min AUC</cell></row><row><cell>8 layer multi label binary classification</cell><cell>0.6</cell><cell>0.472</cell></row><row><cell>6 separate neural networks for binary classification</cell><cell>0.554</cell><cell>0.427</cell></row><row><cell>5 Results Analysis</cell><cell></cell><cell></cell></row><row><cell>3 patients (</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="15,152.23,115.91,310.89,118.56"><head>Table 4 .</head><label>4</label><figDesc>Comparison of predicted probability with actual class Patient No. 117 Patient No. 140 Patient No. 199</figDesc><table coords="15,152.23,155.86,310.89,78.62"><row><cell>Deformity Class</cell><cell cols="6">Predicted Actual Predicted Actual Predicted Actual</cell></row><row><cell>Left Lung Affected</cell><cell>0.94</cell><cell>1</cell><cell>0.95</cell><cell>1</cell><cell>0.85</cell><cell>1</cell></row><row><cell>Right Lung Affected</cell><cell>0.75</cell><cell>1</cell><cell>0.88</cell><cell>1</cell><cell>0.96</cell><cell>1</cell></row><row><cell>Lung Capacity Decrease</cell><cell>0.47</cell><cell>0</cell><cell>0.63</cell><cell>1</cell><cell>0.04</cell><cell>0</cell></row><row><cell>Calcification</cell><cell>0.13</cell><cell>0</cell><cell>0.12</cell><cell>0</cell><cell>0.1</cell><cell>0</cell></row><row><cell>Pleurisy</cell><cell>0.02</cell><cell>0</cell><cell>0.05</cell><cell>0</cell><cell>0.03</cell><cell>0</cell></row><row><cell>Caverns</cell><cell>0.79</cell><cell>0</cell><cell>0.86</cell><cell>1</cell><cell>0.68</cell><cell>0</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="15,138.35,536.05,342.24,7.86;15,146.91,547.01,333.68,7.86;15,146.91,557.97,333.68,7.86;15,146.91,568.93,333.68,7.86;15,146.91,579.89,118.61,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,248.12,547.01,232.48,7.86;15,146.91,557.97,286.17,7.86">Overview of ImageCLEFtuberculosis 2019 -Automatic CT-based Report Generation and Tuberculosis Severity Assessment</title>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitri</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aleh</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/" />
	</analytic>
	<monogr>
		<title level="m" coord="15,437.95,557.97,42.64,7.86;15,146.91,568.93,59.07,7.86">CLEF 2019 Working Notes</title>
		<title level="s" coord="15,214.07,568.93,166.13,7.86">CEUR Workshop Proceedings (CEUR-WS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.35,591.04,342.24,7.86;15,146.91,602.00,333.68,7.86;15,146.91,612.96,333.68,7.86;15,146.91,623.92,333.68,7.86;15,146.91,634.88,333.68,7.86;15,146.91,645.84,333.68,7.86;15,146.91,656.80,333.68,7.86;16,146.91,119.67,333.68,7.86;16,146.91,130.63,333.68,7.86;16,146.91,141.59,333.68,7.86;16,146.91,152.55,333.68,7.86;16,146.91,163.51,171.07,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,192.03,119.67,288.57,7.86;16,146.91,130.63,333.68,7.86;16,146.91,141.59,42.73,7.86">ImageCLEF 2019: Multimedia Retrieval in Medicine, Lifelogging, Security and Nature In: Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vitali</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitri</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aleh</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Narciso</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ergina</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Roberto Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Cuevas Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikos</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Konstantinos</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,196.12,141.59,284.47,7.86;16,146.91,152.55,69.70,7.86">Proceedings of the 10th International Conference of the CLEF Association (CLEF 2019)</title>
		<title level="s" coord="16,310.38,152.55,166.24,7.86">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 10th International Conference of the CLEF Association (CLEF 2019)<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-12">September 9-12 2019</date>
			<biblScope unit="page">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,138.35,174.47,342.25,7.86;16,146.91,185.43,333.68,7.86;16,146.91,196.39,333.68,7.86;16,146.91,207.34,181.78,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,178.96,185.43,283.16,7.86">Efficient and fully automatic segmentation of the lungs in CT volumes</title>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oscar</forename><forename type="middle">A</forename><surname>Jiménez-Del-Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrien</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,242.14,196.39,238.45,7.86;16,146.91,207.34,131.85,7.86">Proceedings of the VISCERAL Challenge at ISBI. No. 1390 in CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</editor>
		<meeting>the VISCERAL Challenge at ISBI. No. 1390 in CEUR Workshop Proceedings</meeting>
		<imprint>
			<date type="published" when="2015-04">Apr 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,138.35,218.30,342.24,7.86;16,146.91,229.26,223.78,7.86" xml:id="b3">
	<monogr>
		<ptr target="http://www.kaggle.com/sentdex/first-pass-through-data-w-3d-convnet" />
		<title level="m" coord="16,146.91,218.30,149.52,7.86">Kaggle Data Science Bowl Article</title>
		<imprint>
			<date type="published" when="2019-05-26">26 May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,138.35,240.22,342.24,7.86;16,146.91,251.18,333.68,7.86;16,146.91,262.14,333.68,7.86;16,146.91,273.10,164.88,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,415.44,240.22,65.15,7.86;16,146.91,251.18,279.00,7.86">Automated Segmentation of Lung Regions using Morphological Operators in CT scan</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sasidhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramesh</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ravi</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bhaskar</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,432.52,251.18,48.07,7.86;16,146.91,262.14,262.17,7.86">Proceedings of the International Journal of Scientific &amp; Engineering Research</title>
		<idno type="ISSN">2229-5518</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date>September 2013 1114</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,138.35,284.06,342.24,7.86;16,146.91,295.02,333.68,7.86;16,146.91,305.98,329.43,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="16,285.29,284.06,195.30,7.86;16,146.91,295.02,200.74,7.86">A Computer Aided Diagnosis (CAD) System for the detection of pulmonary nodules on CT scans</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Labib</forename><surname>Samir</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Habib</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Giza, Egypt</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Systems and Biomedical Engineering Department, Faculty of Engineering, Cairo University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="16,138.35,316.93,342.24,7.86;16,146.91,327.89,333.68,7.86;16,146.91,338.85,333.68,7.86;16,146.91,349.81,267.49,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,187.67,327.89,292.92,7.86;16,146.91,338.85,198.06,7.86">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<author>
			<persName coords=""><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,354.34,338.85,126.25,7.86;16,146.91,349.81,137.32,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018-04">Apr 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,138.35,360.77,342.24,7.86;16,146.91,371.73,333.68,7.86;16,146.91,382.69,137.61,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="16,146.91,371.73,329.29,7.86">Rethinking Atrous Convolution for Semantic Image Segmentation</title>
		<author>
			<persName coords=""><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.05587" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="16,138.35,393.65,342.24,7.86;16,146.91,404.61,48.70,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="16,217.23,393.65,259.12,7.86">Xception: Deep Learning with Depthwise Separable Convolutions</title>
		<author>
			<persName coords=""><forename type="first">Francois</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<publisher>Google, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
