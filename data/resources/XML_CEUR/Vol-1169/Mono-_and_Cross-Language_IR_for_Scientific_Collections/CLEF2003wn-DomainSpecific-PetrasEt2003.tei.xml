<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,132.96,70.39,346.01,11.47;1,170.28,93.07,271.38,11.47">UC Berkeley at CLEF 2003 -Russian Language Experiments and Domain-Specific Cross-Language Retrieval</title>
				<funder ref="#_ky5QkAg">
					<orgName type="full">DARPA (Department of Defense Advanced Research Projects Agency)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,202.50,127.74,54.04,8.22"><forename type="first">Vivien</forename><surname>Petras</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Management and Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.29,127.74,70.38,8.22"><forename type="first">Natalia</forename><surname>Perelman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Management and Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.46,127.74,48.88,8.22"><forename type="first">Fredric</forename><surname>Gey</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">UC Data Archive &amp; Technical Assistance</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,132.96,70.39,346.01,11.47;1,170.28,93.07,271.38,11.47">UC Berkeley at CLEF 2003 -Russian Language Experiments and Domain-Specific Cross-Language Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9FCC841009D97400E0A09F228B78AD74</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As in the previous years, Berkeley's group 1 experimented with the domain-specific CLEF collection GIRT as well as with Russian as query and document language. The GIRT collection was substantially extended this year and we were able to improve our retrieval results for the query languages German, English and Russian. For the GIRT retrieval experiments, we utilized our previous experiences by combining different translations, thesaurus matching, decompounding for German compounds and a blind feedback algorithm. We find that our thesaurus matching technique compares to conventional machine translation for Russian and German against English retrieval and outperforms machine translation for English to German retrieval. With the introduction in CLEF 2003 of a Russian document collection, we participated in the CLEF main task with monolingual and bilingual runs for the Russian collection. For bilingual retrieval our approaches were query translation (for German or English as topic languages) and 'fast' document translation (for English as the topic language). Document translation significantly underperformed query translation (using the PROMPT translation system).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For several years, Berkeley's group 1 has experimented with domain-specific collections and investigated thesaurus-aided retrieval within the CLEF environment. We theorize that collections enhanced with subject terms from a controlled vocabulary contain more query-relevant words and phrases and, furthermore, that retrieval using a thesaurus-enhanced collection and / or queries enriched with controlled vocabulary terms will be more precise. This year's GIRT collection has been extended to contain more than 150,000 documents (as opposed to the 70,000 documents it contained in the previous years) and we investigated the usefulness of a thesaurus in a bigger document collection. The larger a document collection is, the more individual documents can be found for any chosen controlled vocabulary term. In a worst-case scenario, this effect could nullify the specificity of the thesaurus terms and have a negative outcome on the retrieval performance. However, our experiments show that incorporating the thesaurus data will achieve performance improvements. Using the multilingual GIRT thesaurus (German, English, Russian) to translate query files for bilingual retrieval has proven to be useful for performance improvement. Our so-called thesaurus matching technique is comparable to machine translation for Russian and German, but outperforms the tested machine translation systems for English to German. However, the competitiveness of thesaurus matching versus machine translation depends on the existence of controlled vocabulary terms in the query fields and the size and quality of the thesaurus. CLEF 2003 was the first time a Russian language document collection was available in CLEF. We have worked for several years with Russian topics in both the GIRT task and the CLEF main tasks, so we welcomed the opportunity to do Russian monolingual retrieval and bilingual retrieval No unusual methodology was applied to the Russian collection, however encoding was an issue and we ended up using the KOI-8 encoding scheme for both documents and topics.</p><p>For our retrieval experiments, the Berkeley group is using the technique of logistical regression as described in <ref type="bibr" coords="1,92.58,672.83,10.05,8.22" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The GIRT Retrieval Experiments 2.1 The GIRT collection</head><p>The GIRT collection (German Indexing and Retrieval Testdatabase) consists of 151,319 documents in the social science domain. The documents contain titles, abstracts and controlled vocabulary terms describing reports and papers indexed by the GESIS organization (http://www.social-science-gesis.de). The GIRT controlled vocabulary terms are based on the Thesaurus for the Social Sciences <ref type="bibr" coords="2,373.38,140.58,11.03,8.22" target="#b1">[2]</ref> and are provided in German and English. A German-Russian translation table is also provided. For the 2003 CLEF experiments, two parallel GIRT corpora were made available: (1) German GIRT 4 contains document fields with German text, and (2) English GIRT 4 contains the translations of these fields into English. This year, we carried out the monolingual task in both the German and English corpus, testing which parts of the document (title, abstract, or thesaurus terms) will provide relevant input for retrieval. We also experimented with the bilingual task by using German, English and Russian as query languages against both corpora. For all runs against the German collection, we used our decompounding procedure to split German compound words into individual terms. The procedure is described in <ref type="bibr" coords="2,321.43,249.84,11.01,8.22" target="#b2">[3]</ref> and <ref type="bibr" coords="2,352.24,249.84,10.04,8.22" target="#b3">[4]</ref>. All runs used only title and description fields from the topics. Additionally, we used our blind feedback algorithm for all runs to improve performance. The blind feedback algorithm assumes the top 20 documents as relevant and selects 30 terms from these documents to add to the query. From our experience, using the decompounding procedure and our blind feedback algorithm increases the performance anywhere between 10 and 30%. The run BKGRMLGG1 (Table <ref type="table" coords="2,511.71,293.09,3.93,8.22" target="#tab_0">1</ref>) for example, which reached an average precision of 0.4965 in the official run, would have yielded only 0.3288 average precision without decompounding and blind feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GIRT Monolingual Retrieval</head><p>For the GIRT monolingual task, we performed two experiments for each of the German and English corpora: a monolingual run against an index containing all document fields and a monolingual run against an index without the controlled vocabulary fields. As was expected, the runs against the indexes containing all fields yielded better retrieval results than the runs against the smaller indexes. For comparison purposes, we also constructed two additional indexes containing only the controlled vocabulary terms and the controlled vocabulary terms and the titles respectively. The results for the German and English monolingual runs can be found in tables 1 and 2. Judging from these results, the controlled vocabulary terms have a positive impact on the retrieval results, but not as big as the abstract. Runs without the thesaurus terms lose only about 16% of their average precision, whereas runs without the abstract lose about 29%. An index that only contains titles would only yield a performance of 0.1820 in average precision, which confirms the theory that most titles are not as expressive of an article's content as the controlled vocabulary terms or the abstract.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Comparing these results to last year's, the bigger collection size might have an impact. Last year, the indexes with title and abstract and title and thesaurus terms yielded about the same results. Both were about 23% worse than the general index containing all fields. This could mean that the thesaurus terms in the bigger collection do not have as much expressive power and are not as discriminating as they are in a smaller collection. However, the results can also be explained by other influences: (i) the queries contain less terms found in the thesaurus, (ii) the abstracts are more expressive, (iii) there were less controlled vocabulary terms assigned to each document. For the English GIRT corpus, the results seem to be quite different. Here the index with only title and thesaurus term fields yields almost as good a result as the general index. The index without the thesaurus terms shows a performance only half as good as the general index. However, this result can probably be explained by the fact that there are far fewer abstracts in the English GIRT corpus than there are controlled vocabulary terms. The title and thesaurus terms seem to bear the brunt of the retrieval effort in this collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">GIRT Bilingual Retrieval</head><p>We submitted 5 official runs for the GIRT bilingual task and used all query languages (German, English and Russian) available. Generally, the runs against the English GIRT collection (with translated query files from German and Russian) yielded better results than the runs against the German GIRT collection. This can be most probably attributed to the better quality of machine translation systems for the English language as opposed to the German language. However, there does not seem to be a high variation in the results between the Russian and German / English query languages, which points to a rapid improvement in the machine translation for Russian, which can be seen in the definite increase of precision figures as compared to the detrimental results of last year.</p><p>We used two machine translation systems for each query language: L &amp; H Power Translator and Systran for German and English; and Promt and Systran for the Russian language. We also used our thesaurus matching as one translation technique <ref type="bibr" coords="4,189.44,90.78,10.03,8.22" target="#b4">[5]</ref>, which will be further discussed in part 2.4.</p><p>For thesaurus matching, we identify phrases and terms from the topics files and search them against the thesaurus. Once we find an appropriate thesaurus term, we substitute the query term or phrase with the thesaurus term in the language used for retrieval.</p><p>The results for the bilingual runs against German and English and a comparison of the different translation techniques can be found in tables 3 &amp; 4 for Russian to German and English to German respectively and From the Russian runs against the German GIRT corpus, one can see the superior quality of the Promt translator (about 30% better results than the Systran Babelfish translating system). The Systran system is also handicapped in that it has no direct translation from Russian to German. English was used as a Pivot language and could have introduced additional errors or ambiguities. Nevertheless, a combination of both translating systems reaches an improvement in overall precision, but not in recall.</p><p>Our thesaurus matching technique -although with a much more restricted vocabulary -compares with the Systran translator in precision and reaches a better recall. This can be explained with the superior quality (in terms of relevance for retrieval) of the thesaurus terms in a search statement. Whereas in last year's experiment the combination of translation and thesaurus matching achieved a performance improvement of 30%, this year the combination achieves only marginal improvements in precision and recall. This can mostly be explained with the improved quality of the machine translation system Promt, so that our thesaurus matching technique does not add as many high-quality terms to the query as it did last year. For English to German retrieval, the L+H Power Translator system reaches much better results in retrieval than Systran, so that the combination of both translations actually degraded the retrieval performance of the overall run (although recall increased slightly). Two queries negatively impacted the retrieval results using machine translation: 94 (Homosexuality and Coming-Out) and 98 (Canadian Foreign Policy). Both were caused by wrong translations of critical search words. "Coming-Out" for query 94 was translated into "Herauskommen" (a direct translation of the English phrases), although the phrase remains as is in German as a borrowed construct. Query 98 contains the phrase "foreign policy", which was translated into "fremde Politik", a common mistake in word-for-word translation systems. Although "foreign" is most commonly translated with "fremd", in the phrase "foreign policy" it should become the compound "Aussenpolitik" -an error that dropped this query's precision to 0.0039. However, the phrase "foreign policy" is a controlled vocabulary term and was therefore correctly translated using our thesaurus matching technique. Using thesaurus matching improved this query's average precision to 0.3798. For English to German retrieval, thesaurus matching proved to be most effective; this run outperformed the best machine translation run by roughly 10%. Combining machine translations and translations using our thesaurus matching improves performance even more: the BKGRBLEG5 run outperformed the best machine translation run by 18%. Also for Russian to English retrieval, the Promt translator shows superior quality -even better than for Russian to German. It outperforms the Systran translator in a way that a combination of the translations actually proves to be disadvantageous to the retrieval outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Our thesaurus matching run yields the worst results of all runs -this is partly due to the fact that there is no direct mapping table between the Russian and English thesaurus version so that German had to be used as a pivot language. In the process of mapping the Russian queries to the German and then English thesaurus versions, information was lost and consequently two queries (93 &amp; 95) could not be effectively translated and no documents were retrieved from the English collection. Nevertheless, a translation using thesaurus matching adds new and relevant search terms to some queries so that a combination of machine translation plus thesaurus matching translation slightly outperformed the best machine translation run by 6%.</p><p>Run Name BKGRBLGE2 BKGRBLGE3 BKGRBLGE1 BKGRBLGE4 BKGRBLGE5 Once again, the L+H Power translator outperforms the Systran translator also when it comes to the opposite direction of English to German retrieval. However, a combination of the two MT systems marginally outperforms L+H in precision and makes an impact on recall. Thesaurus matching from German to English reaches a result similar to any of the machine translations systems but the combination of the L+H Power translation and our translation from thesaurus matching achieves a performance improvement of 17%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The Effectiveness of Thesaurus Matching</head><p>Thesaurus matching is a translation technique where the system relies exclusively on the vocabulary of the thesaurus to provide a translation. The topic files are searched for terms and phrases that occur in the thesaurus and are then substituted by their foreign language counterparts. A more detailed description can be found in <ref type="bibr" coords="7,500.02,125.40,9.98,8.22" target="#b4">[5]</ref>. Due to this process, the translated query consists of controlled vocabulary terms in the appropriate language and untranslated words that were not found in the thesaurus. This has the advantage of emphasizing highly relevant search terms (which will occur in the thesaurus term fields of the relevant documents) but also has a major drawback. The technique will only work when the queries contain enough words and phrases that occur in the multilingual thesaurus and when those terms and phrases represent the meaning of the search statement. Fortunately, almost all queries contain more than one term that can be found in the thesaurus and therefore translated. Nevertheless, most of the variation in our retrieval results (comparing query by query to the machine translation results) can be accounted for by looking at which queries contain the most thesaurus terms and how many good phrases our algorithm can detect. A large general thesaurus should be able to provide a good translation approximation but specialized thesauri with highly technical vocabulary might not fare as well. However, depending on the nature of the query, specialized thesauri could help in identifying important search terms from a search statement. Additionally, our thesaurus matching technique might be able to improve: (i) by allowing a better fuzzy match between query terms and thesaurus terms, (ii) by incorporating partial matching of query terms to thesaurus terms, (iii) by exploiting narrower and broader term relationships in the thesaurus when expanding the query, or (iv) by exploiting the use-instead and used-for relationships in the thesaurus (which we have ignored so far).</p><p>Further experiments should show whether our thesaurus matching technique can improve and -considering that its competitive advantage over the three investigated MT systems lies in its ability to translate phrases -whether it can compete against phrase dictionaries as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Russian Retrieval for the CLEF main task</head><p>CLEF 2003 marked the first time a document collection has been available and evaluated in the Russian language. The CLEF Russian collection consisted of 16,716 articles from Izvestia newspaper from 1995. This is a small number of documents by most CLEF measures (the smallest other collection of CLEF 2003, Finnish, has 55,344 documents; the Spanish collection has 454,045 documents). There were 37 Russian topics, which were chosen by the organizers from the 60 topics of the CLEF main multilingual task. In our bilingual retrieval we worked with English and German versions of these topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding Issues</head><p>The Russian document collection was supplied in the UTF-8 unicode encoding, as were the Russian version of the topics. However, since the stemmer we employ is in KOI8 format, the entire collection was converted into KOI8 encoding. In indexing the collection, we converted upper-case letters to lower-case and applied Snowball's Russian stemmer (http://snowball.tartarus.org/russian/stemmer.html) together with Russian stopword list created by merging the Snowball list with a translation of the English stopword list. In addition the PROMPT translation system would also only work on KOI8 encoding which meant that our translations from English and German also would come in that encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Russian Monolingual Retrieval</head><p>We submitted four Russian monolingual runs, the results of which are summarized below. All runs utilized blind feedback, choosing the top 30 terms from the top ranked 20 documents of an initial retrieval run. This was the same methodology used above in the GIRT retrieval. For BKRUMLRR1 and BKRUMLRR2 runs we used TITLE and TEXT document fields for indexing. BKRUMLRR3 and BKRUMLRR4 were run against an index containing TITLE, TEXT, SUBJECT, GEOGRAPHY, and RETRO fields.</p><p>The results of our retrieval are summarized in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Russian Bilingual Retrieval</head><p>We submitted six bilingual runs against the Russian document collection. These runs only indexed the TITLE and TEXT fields of each Russian document, so are directly comparable only to the monolingual runs BKMLRURR1 and BKMLRURR2 above. Four of these runs (BKRUBLGR1, BKRUBLGR2, BKRUBLER1, BKRUBLER2) utilized query translation from either German or English topics into Russian.</p><p>Run Translation to Russian was done using the PROMPT online translation facility at http://www.translate.ru The only difference between runs numbered one and two was the addition of the narrative field in topic indexing.</p><p>Two final runs (BKRUMLEE1 and BKRUMLEE2) utilized a technique developed by Aitao Chen, called 'Fast Document Translation' <ref type="bibr" coords="9,182.42,112.44,9.97,8.22" target="#b5">[6]</ref>. Instead of doing complete document translation using MT software, the MT system is used to translate the entire vocabulary of the document collection on a word-by-word basis without the contextualization of position in sentence with respect to other words. Using this technique will choose only one translation for a polysemous word, but this defect is compensated by extremely fast translations of the all the documents into the target language. We submitted 246.252 unique Russian words from the Izvestia collection to the PROMPT translation system (this was done 5,000 words at a time) for translation to English and then used this to translate all the documents into English. Monolingual retrieval was performed by matching the English versions of the topics against the translated English document collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Brief Analysis of Russian Retrieval Performance</head><p>Bilingual retrieval was in all cases worse than monolingual (Russian-Russian) retrieval in terms of overall precision. German Russian retrieval was comparable to English Russian retrieval for TD runs, but the English Russian TDN run was substantially better than its German Russian counterpart. Speculation (without evidence) is that de-compounding the German narrative before translation would have improved the performance. Fast document translation runs significantly underperformed query translation runs, which differs from experiments with other languages; we are investigating why this is the case.</p><p>Because of the nature of the retrieval results by query for the Russian collection (eleven of the 28 topics have 2 or fewer relevant documents) one has to be cautious about drawing conclusions from the results. In general, monolingual retrieval substantially outperformed bilingual retrieval over almost all topics. However, for Topic 169 the bilingual retrieval is much better (best precision 1.0 for German-to-Russian) than the monolingual, with the best run being German-to-Russian where the German topic contains the words CD-Brennern which translates to laser disc (лазерного диска) and music industry (Musikindustrie музыкальной индустрии) instead of the use, in the Russian version of topic 169, of the words компакт-дисков (compact disk) and аудиопромышленности (audio industry) which aren't very discriminating. The German Russian retrieval for Topic 187 (with one relevant document) fell victim to translation problems: "Radioactive waste" in English is expressed in German as " radioaktivem Müll". The English "waste" is translated correctly as "отходы" while the German "Müll" is translated as "мусору," or "garbage". This and other differences in translation lead to a decrease from 1.0 precision for English bilingual to 0.25 for German bilingual for topic 187. Several other topics have the same disparity of translation. We merged the document rankings from German and English bilingual runs using un-normalized retrieval status value -the resulting ranked list showed no significant improvement in performance. It would be useful to try merging the topic translations (adjusting for word count weights) before retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Summary and Acknowledgments</head><p>Berkeley's group 1 participated in the CLEF GIRT tasks and CLEF Main tasks for Russian mono-and bilingual retrieval. We experimented with German, English and Russian as collection and query languages.</p><p>Within the GIRT domain-specific collection, we investigated the use of thesauri in document retrieval, document index enhancement and query translation. Documents that have controlled vocabulary terms added to the usual title and abstract information prove advantageous in retrieval because the thesaurus terms add valuable search terms to the index. An index containing titles, abstracts and thesaurus terms will always outperform an index only containing title and abstract. However, the theory that thesaurus terms might be able to substitute abstracts because of their specific nature was premature. Retrieval involving thesauri can be influenced by several factors: the size of the collection, the size of the controlled vocabulary and the nature of the queries. For topic translations, we found that although a combination of different machine translation systems might not always outperform an individual machine translation system, a combination of a machine translation system and our thesaurus matching technique does. Thesaurus matching outperformed machine translation in English to German retrieval and added new and relevant search terms for all other query languages. For German and Russian queries, thesaurus matching yielded comparable results to machine translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,94.08,427.50,423.93,247.14"><head>Table 1 .</head><label>1</label><figDesc>Monolingual runs on the German GIRT 4 corpus. Official runs are BKGRMLGG1 and BKGRMLGG2.</figDesc><table coords="2,141.60,427.50,326.68,225.06"><row><cell>Name</cell><cell cols="4">BKGRMLGG1 BKGRMLGG2 BKGRMLGG3 BKGRMLGG4</cell></row><row><cell>Document Fields</cell><cell>All</cell><cell cols="2">Title, Abstract Title, Thesaurus</cell><cell>Thesaurus</cell></row><row><cell>Retrieved</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell></row><row><cell>Relevant</cell><cell>2117</cell><cell>2117</cell><cell>2117</cell><cell>2117</cell></row><row><cell>Rel Ret</cell><cell>1860</cell><cell>1767</cell><cell>1624</cell><cell>1474</cell></row><row><cell>Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>at 0.00</cell><cell>0.9416</cell><cell>0.9270</cell><cell>0.8900</cell><cell>0.8053</cell></row><row><cell>at 0.10</cell><cell>0.8042</cell><cell>0.7668</cell><cell>0.7001</cell><cell>0.6194</cell></row><row><cell>at 0.20</cell><cell>0.7096</cell><cell>0.6938</cell><cell>0.6110</cell><cell>0.5544</cell></row><row><cell>at 0.30</cell><cell>0.6682</cell><cell>0.5827</cell><cell>0.5320</cell><cell>0.3800</cell></row><row><cell>at 0.40</cell><cell>0.5810</cell><cell>0.5140</cell><cell>0.4262</cell><cell>0.3337</cell></row><row><cell>at 0.50</cell><cell>0.5242</cell><cell>0.4287</cell><cell>0.3345</cell><cell>0.2797</cell></row><row><cell>at 0.60</cell><cell>0.4359</cell><cell>0.3385</cell><cell>0.2708</cell><cell>0.2239</cell></row><row><cell>at 0.70</cell><cell>0.3784</cell><cell>0.2607</cell><cell>0.1949</cell><cell>0.1555</cell></row><row><cell>at 0.80</cell><cell>0.2970</cell><cell>0.1527</cell><cell>0.1041</cell><cell>0.0827</cell></row><row><cell>at 0.90</cell><cell>0.1764</cell><cell>0.0916</cell><cell>0.0436</cell><cell>0.0301</cell></row><row><cell>at 1.00</cell><cell>0.0528</cell><cell>0.0362</cell><cell>0.0010</cell><cell>0.0010</cell></row><row><cell>Avg. Precision</cell><cell>0.4965</cell><cell>0.4199</cell><cell>0.3530</cell><cell>0.2935</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,118.86,212.15,374.21,257.52"><head>Table 2 .</head><label>2</label><figDesc>Monolingual runs against the English GIRT 4 corpus. Official runs are BKGRMLEE1 and BKGRMLEE2.</figDesc><table coords="3,141.06,212.15,324.34,224.58"><row><cell>Name</cell><cell cols="4">BKGRMLEE1 BKGRMLEE2 BKGRMLEE3 BKGRMLEE4</cell></row><row><cell>Document Fields</cell><cell>All</cell><cell cols="2">Title, Abstract Title, Thesaurus</cell><cell>Thesaurus</cell></row><row><cell>Retrieved</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell></row><row><cell>Relevant</cell><cell>1332</cell><cell>1332</cell><cell>1332</cell><cell>1332</cell></row><row><cell>Rel Ret</cell><cell>1214</cell><cell>763</cell><cell>1160</cell><cell>1092</cell></row><row><cell>Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>at 0.00</cell><cell>0.9373</cell><cell>0.7762</cell><cell>0.9464</cell><cell>0.6722</cell></row><row><cell>at 0.10</cell><cell>0.7739</cell><cell>0.5993</cell><cell>0.8086</cell><cell>0.5373</cell></row><row><cell>at 0.20</cell><cell>0.7220</cell><cell>0.4648</cell><cell>0.6701</cell><cell>0.4557</cell></row><row><cell>at 0.30</cell><cell>0.6462</cell><cell>0.3700</cell><cell>0.5998</cell><cell>0.4111</cell></row><row><cell>at 0.40</cell><cell>0.5863</cell><cell>0.2684</cell><cell>0.5449</cell><cell>0.3733</cell></row><row><cell>at 0.50</cell><cell>0.5324</cell><cell>0.1909</cell><cell>0.4751</cell><cell>0.3376</cell></row><row><cell>at 0.60</cell><cell>0.4734</cell><cell>0.1343</cell><cell>0.4225</cell><cell>0.2822</cell></row><row><cell>at 0.70</cell><cell>0.4127</cell><cell>0.0774</cell><cell>0.3714</cell><cell>0.2320</cell></row><row><cell>at 0.80</cell><cell>0.3437</cell><cell>0.0477</cell><cell>0.3062</cell><cell>0.1820</cell></row><row><cell>at 0.90</cell><cell>0.2508</cell><cell>0.0415</cell><cell>0.2033</cell><cell>0.0877</cell></row><row><cell>at 1.00</cell><cell>0.0846</cell><cell>0.0414</cell><cell>0.0570</cell><cell>0.0458</cell></row><row><cell>Avg. Precision</cell><cell>0.5192</cell><cell>0.2484</cell><cell>0.4853</cell><cell>0.3207</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,92.58,166.55,426.90,327.60"><head>Table 3 .</head><label>3</label><figDesc>table 5  &amp; 6  for Russian and German to English respectively. All runs are against the full indexes containing all document fields. Bilingual Russian runs against the German GIRT 4 corpus. Official runs are BKGRBLRG1 and BKGRBLRG2.</figDesc><table coords="4,132.18,212.15,347.58,249.12"><row><cell>Run Name</cell><cell cols="5">BKGRBLRG3 BKGRBLRG4 BKGRBLRG1 BKGRBLRG5 BKGRBLRG2</cell></row><row><cell>Transl.</cell><cell></cell><cell></cell><cell></cell><cell>Thes.</cell><cell>Sys + Promt +</cell></row><row><cell>Technique</cell><cell>Systran</cell><cell>Promt</cell><cell>Sys + Promt</cell><cell>Matching</cell><cell>Thes.</cell></row><row><cell>Retrieved</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell></row><row><cell>Relevant</cell><cell>2117</cell><cell>2117</cell><cell>2117</cell><cell>2117</cell><cell>2117</cell></row><row><cell>Rel Ret</cell><cell>1264</cell><cell>1555</cell><cell>1547</cell><cell>1343</cell><cell>1577</cell></row><row><cell>Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>at 0.00</cell><cell>0.5301</cell><cell>0.6674</cell><cell>0.7035</cell><cell>0.5067</cell><cell>0.7281</cell></row><row><cell>at 0.10</cell><cell>0.3381</cell><cell>0.5086</cell><cell>0.5756</cell><cell>0.3816</cell><cell>0.5994</cell></row><row><cell>at 0.20</cell><cell>0.2698</cell><cell>0.4256</cell><cell>0.4796</cell><cell>0.3239</cell><cell>0.5121</cell></row><row><cell>at 0.30</cell><cell>0.2472</cell><cell>0.3750</cell><cell>0.4261</cell><cell>0.2873</cell><cell>0.4591</cell></row><row><cell>at 0.40</cell><cell>0.2271</cell><cell>0.3275</cell><cell>0.3711</cell><cell>0.2480</cell><cell>0.3966</cell></row><row><cell>at 0.50</cell><cell>0.1960</cell><cell>0.2880</cell><cell>0.3301</cell><cell>0.1950</cell><cell>0.3395</cell></row><row><cell>at 0.60</cell><cell>0.1598</cell><cell>0.2257</cell><cell>0.2570</cell><cell>0.1487</cell><cell>0.2690</cell></row><row><cell>at 0.70</cell><cell>0.1262</cell><cell>0.1864</cell><cell>0.1946</cell><cell>0.0997</cell><cell>0.2072</cell></row><row><cell>at 0.80</cell><cell>0.0987</cell><cell>0.1380</cell><cell>0.1383</cell><cell>0.0769</cell><cell>0.1346</cell></row><row><cell>at 0.90</cell><cell>0.0657</cell><cell>0.0654</cell><cell>0.0713</cell><cell>0.0393</cell><cell>0.0741</cell></row><row><cell>at 1.00</cell><cell>0.0154</cell><cell>0.0116</cell><cell>0.0103</cell><cell>0.0027</cell><cell>0.0009</cell></row><row><cell>Avg. Precision</cell><cell>0.1925</cell><cell>0.2798</cell><cell>0.3117</cell><cell>0.1983</cell><cell>0.3269</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,121.08,71.58,369.73,271.14"><head>Table 4 .</head><label>4</label><figDesc>Bilingual English runs against the German GIRT 4 corpus. Official run is BKGRBLEG1.</figDesc><table coords="5,134.04,71.58,343.85,249.00"><row><cell>Run Name</cell><cell cols="5">BKGRBLEG2 BKGRBLEG3 BKGRBLEG1 BKGRBLEG4 BKGRBLEG5</cell></row><row><cell>Transl.</cell><cell></cell><cell></cell><cell></cell><cell>Thes.</cell><cell></cell></row><row><cell>Technique</cell><cell>L+H Power</cell><cell>Systran</cell><cell>Sys + L+H</cell><cell>Matching</cell><cell>L+H + Thes.</cell></row><row><cell>Retrieved</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell></row><row><cell>Relevant</cell><cell>2117</cell><cell>2117</cell><cell>2117</cell><cell>2117</cell><cell>2117</cell></row><row><cell>Rel Ret</cell><cell>1656</cell><cell>1488</cell><cell>1672</cell><cell>1712</cell><cell>1803</cell></row><row><cell>Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>at 0.00</cell><cell>0.7719</cell><cell>0.6633</cell><cell>0.7184</cell><cell>0.8823</cell><cell>0.9257</cell></row><row><cell>at 0.10</cell><cell>0.6607</cell><cell>0.5201</cell><cell>0.6425</cell><cell>0.7505</cell><cell>0.8118</cell></row><row><cell>at 0.20</cell><cell>0.5757</cell><cell>0.4348</cell><cell>0.5727</cell><cell>0.6004</cell><cell>0.6898</cell></row><row><cell>at 0.30</cell><cell>0.5114</cell><cell>0.3841</cell><cell>0.5300</cell><cell>0.5510</cell><cell>0.6132</cell></row><row><cell>at 0.40</cell><cell>0.4569</cell><cell>0.3548</cell><cell>0.4568</cell><cell>0.5089</cell><cell>0.5606</cell></row><row><cell>at 0.50</cell><cell>0.3996</cell><cell>0.3084</cell><cell>0.3747</cell><cell>0.4475</cell><cell>0.4787</cell></row><row><cell>at 0.60</cell><cell>0.3471</cell><cell>0.2608</cell><cell>0.3109</cell><cell>0.3869</cell><cell>0.3832</cell></row><row><cell>at 0.70</cell><cell>0.3002</cell><cell>0.2082</cell><cell>0.2363</cell><cell>0.3132</cell><cell>0.3197</cell></row><row><cell>at 0.80</cell><cell>0.2159</cell><cell>0.1686</cell><cell>0.1781</cell><cell>0.2477</cell><cell>0.2421</cell></row><row><cell>at 0.90</cell><cell>0.1141</cell><cell>0.0985</cell><cell>0.0918</cell><cell>0.1259</cell><cell>0.1374</cell></row><row><cell>at 1.00</cell><cell>0.0189</cell><cell>0.0117</cell><cell>0.0099</cell><cell>0.0196</cell><cell>0.0121</cell></row><row><cell>Avg. Precision</cell><cell>0.3886</cell><cell>0.3001</cell><cell>0.3669</cell><cell>0.4299</cell><cell>0.4606</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,134.88,542.39,342.21,134.95"><head>Table 5 .</head><label>5</label><figDesc>Bilingual Russian runs against the English GIRT 4 corpus. Official run is BKGRBLRE1.</figDesc><table coords="5,134.88,542.39,342.21,134.95"><row><cell>Name</cell><cell cols="5">BKGRBLRE2 BKGRBLRE3 BKGRBLRE1 BKGRBLRE4 BKGRBLRE5</cell></row><row><cell>Transl.</cell><cell></cell><cell></cell><cell></cell><cell>Thes.</cell><cell></cell></row><row><cell>Technique</cell><cell>Systran</cell><cell>Promt</cell><cell>Sys + Promt</cell><cell cols="2">Matching Promt + Thes.</cell></row><row><cell>Retrieved</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell></row><row><cell>Relevant</cell><cell>1332</cell><cell>1332</cell><cell>1332</cell><cell>1332</cell><cell>1332</cell></row><row><cell>Rel Ret</cell><cell>997</cell><cell>1084</cell><cell>1042</cell><cell>935</cell><cell>1077</cell></row><row><cell>Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>at 0.00</cell><cell>0.7193</cell><cell>0.7922</cell><cell>0.7696</cell><cell>0.7309</cell><cell>0.8350</cell></row><row><cell>at 0.10</cell><cell>0.5466</cell><cell>0.6579</cell><cell>0.6251</cell><cell>0.5206</cell><cell>0.7103</cell></row><row><cell>at 0.20</cell><cell>0.4931</cell><cell>0.6165</cell><cell>0.5856</cell><cell>0.4443</cell><cell>0.6281</cell></row><row><cell>at 0.30</cell><cell>0.4559</cell><cell>0.5671</cell><cell>0.5522</cell><cell>0.4120</cell><cell>0.5638</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,134.04,362.22,342.99,245.28"><head>Table 6 .</head><label>6</label><figDesc>Bilingual German runs against the English GIRT 4. Official run is BKGRBLGE1.</figDesc><table coords="6,134.04,362.22,340.37,223.26"><row><cell>Transl.</cell><cell></cell><cell></cell><cell></cell><cell>Thes.</cell><cell></cell></row><row><cell>Technique</cell><cell>L+H Power</cell><cell>Systran</cell><cell>Sys + L+H</cell><cell>Matching</cell><cell>L+H + Thes.</cell></row><row><cell>Retrieved</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell><cell>25000</cell></row><row><cell>Relevant</cell><cell>1332</cell><cell>1332</cell><cell>1332</cell><cell>1332</cell><cell>1332</cell></row><row><cell>Rel Ret</cell><cell>1067</cell><cell>1116</cell><cell>1121</cell><cell>1074</cell><cell>1197</cell></row><row><cell>Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>at 0.00</cell><cell>0.7505</cell><cell>0.8080</cell><cell>0.8338</cell><cell>0.7401</cell><cell>0.8298</cell></row><row><cell>at 0.10</cell><cell>0.6486</cell><cell>0.6089</cell><cell>0.6630</cell><cell>0.6054</cell><cell>0.7388</cell></row><row><cell>at 0.20</cell><cell>0.5921</cell><cell>0.5145</cell><cell>0.5707</cell><cell>0.5267</cell><cell>0.6933</cell></row><row><cell>at 0.30</cell><cell>0.4958</cell><cell>0.4549</cell><cell>0.5185</cell><cell>0.4921</cell><cell>0.6020</cell></row><row><cell>at 0.40</cell><cell>0.4354</cell><cell>0.3921</cell><cell>0.4777</cell><cell>0.4466</cell><cell>0.5465</cell></row><row><cell>at 0.50</cell><cell>0.3924</cell><cell>0.3573</cell><cell>0.4001</cell><cell>0.4204</cell><cell>0.4810</cell></row><row><cell>at 0.60</cell><cell>0.3482</cell><cell>0.3179</cell><cell>0.3591</cell><cell>0.3702</cell><cell>0.4306</cell></row><row><cell>at 0.70</cell><cell>0.2965</cell><cell>0.2837</cell><cell>0.2971</cell><cell>0.3155</cell><cell>0.3701</cell></row><row><cell>at 0.80</cell><cell>0.2561</cell><cell>0.2451</cell><cell>0.2401</cell><cell>0.2738</cell><cell>0.2995</cell></row><row><cell>at 0.90</cell><cell>0.2036</cell><cell>0.2076</cell><cell>0.1816</cell><cell>0.2091</cell><cell>0.2267</cell></row><row><cell>at 1.00</cell><cell>0.0830</cell><cell>0.0722</cell><cell>0.0808</cell><cell>0.0917</cell><cell>0.0954</cell></row><row><cell>Avg. Precision</cell><cell>0.4022</cell><cell>0.3748</cell><cell>0.4068</cell><cell>0.3977</cell><cell>0.4731</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,92.58,653.39,411.88,18.96"><head>Table 7 .</head><label>7</label><figDesc>Results were reported by the CLEF organizers for 28 topics which had one or more relevant documents.</figDesc><table coords="8,158.52,82.32,295.02,247.62"><row><cell>Run Name</cell><cell cols="4">BKRUMLRR1 BKRUMLRR2 BKRUMLRR3 BKRUMLRR4</cell></row><row><cell>Index</cell><cell>Koi</cell><cell>Koi</cell><cell>Koi-all</cell><cell>Koi-all</cell></row><row><cell>Topic fields</cell><cell>TD</cell><cell>TDN</cell><cell>TD</cell><cell>TDN</cell></row><row><cell>Retrieved</cell><cell>28000</cell><cell>28000</cell><cell>28000</cell><cell>28000</cell></row><row><cell>Relevant</cell><cell>151</cell><cell>151</cell><cell>151</cell><cell>151</cell></row><row><cell>Rel Ret</cell><cell>125</cell><cell>127</cell><cell>146</cell><cell>148</cell></row><row><cell>Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>at 0.00</cell><cell>0.5201</cell><cell>0.6626</cell><cell>0.5311</cell><cell>0.6503</cell></row><row><cell>at 0.10</cell><cell>0.5201</cell><cell>0.6626</cell><cell>0.5311</cell><cell>0.6503</cell></row><row><cell>at 0.20</cell><cell>0.4844</cell><cell>0.5750</cell><cell>0.5278</cell><cell>0.6208</cell></row><row><cell>at 0.30</cell><cell>0.4777</cell><cell>0.5409</cell><cell>0.5047</cell><cell>0.5597</cell></row><row><cell>at 0.40</cell><cell>0.4309</cell><cell>0.4370</cell><cell>0.4554</cell><cell>0.5009</cell></row><row><cell>at 0.50</cell><cell>0.4007</cell><cell>0.3992</cell><cell>0.4375</cell><cell>0.4522</cell></row><row><cell>at 0.60</cell><cell>0.3087</cell><cell>0.2873</cell><cell>0.3448</cell><cell>0.3699</cell></row><row><cell>at 0.70</cell><cell>0.2382</cell><cell>0.2368</cell><cell>0.3107</cell><cell>0.3401</cell></row><row><cell>at 0.80</cell><cell>0.1637</cell><cell>0.1612</cell><cell>0.2965</cell><cell>0.3093</cell></row><row><cell>at 0.90</cell><cell>0.1210</cell><cell>0.1206</cell><cell>0.2535</cell><cell>0.2641</cell></row><row><cell>at 1.00</cell><cell>0.1210</cell><cell>0.1206</cell><cell>0.2392</cell><cell>0.2471</cell></row><row><cell>Avg. Precision</cell><cell>0.3338</cell><cell>0.3655</cell><cell>0.3878</cell><cell>0.4395</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,189.18,333.00,233.57,8.22"><head>Table 7 :</head><label>7</label><figDesc>Berkeley Monolingual Russian runs for CLEF 2003.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We experimented with the CLEF 2003 Russian document collection with both monolingual Russian and bilingual to Russian from German and English topics. In addition to query translation methodology for bilingual retrieval, we tried a fast document translation method to English and performed English-English monolingual retrieval, which did not perform as well as query translation.</p><p>We would like to thank <rs type="person">Aitao Chen</rs> for supplying his German decompounding software and for performing the fast document translation from Russian to English. This research was supported in part by <rs type="funder">DARPA (Department of Defense Advanced Research Projects Agency)</rs> under research grant <rs type="grantNumber">N66001-00-1-8911</rs>: <rs type="projectName">Translingual Information Detection Extraction and Summarization (TIDES</rs>) within the <rs type="institution">DARPA Information Technology Office</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ky5QkAg">
					<idno type="grant-number">N66001-00-1-8911</idno>
					<orgName type="project" subtype="full">Translingual Information Detection Extraction and Summarization (TIDES</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,106.63,222.78,412.87,8.22;10,92.58,233.64,426.90,8.22;10,92.58,244.38,45.75,8.22" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,237.26,222.78,282.23,8.22;10,92.58,233.64,81.27,8.22">Full text retrieval based on probabilistic equations with coefficients fitted by logistic regression</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,275.04,233.64,187.45,8.22">The Second Text Retrieval Conference (TREC-2)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1994-03">March 1994</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.12,255.24,412.20,8.22;10,92.58,266.10,213.24,8.22" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,172.08,255.24,133.12,8.22">Thesaurus for the Social Science</title>
		<editor>H. Schott</editor>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Informations-Zentrum Sozialwissenschaften Bonn</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>German-English. English-German</note>
</biblStruct>

<biblStruct coords="10,106.49,276.84,412.93,8.22;10,92.58,287.70,426.95,8.22;10,92.58,298.56,426.79,8.22;10,92.58,309.30,230.96,8.22" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,143.50,276.84,258.20,8.22">Multilingual information retrieval using english and chinese queries</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,233.52,287.70,286.01,8.22;10,92.58,298.56,247.28,8.22">Evaluation of Cross-Language Information Retrieval Systems: Second Workshop of the Cross-Language Evaluation Forum, CLEF-2001</title>
		<title level="s" coord="10,120.78,309.30,155.80,8.22">Springer Computer Science Series LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001. 2002</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,106.12,320.16,413.35,8.22;10,92.58,331.02,201.24,8.22" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,142.52,320.16,180.48,8.22">Cross-language Retrieval Experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,409.92,320.16,109.55,8.22;10,92.58,331.02,83.60,8.22">Working Notes for the CLEF 2002 Workshop 19-20</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09">2002. September. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.41,341.76,412.08,8.22;10,92.58,352.62,426.67,8.22" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,252.45,341.76,267.04,8.22;10,92.58,352.62,74.12,8.22">Using Thesauri in Cross-Language Retrieval of German and French Indexed Collections</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Petras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,225.30,352.62,155.47,8.22">Proceedings of the CLEF 2002 Workshop</title>
		<meeting>the CLEF 2002 Workshop</meeting>
		<imprint>
			<publisher>Springer Computer Science Series</publisher>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="10,106.92,363.48,412.52,8.22;10,92.58,374.22,369.46,8.22" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,191.58,363.48,327.86,8.22;10,92.58,374.22,76.45,8.22">Multilingual Information Retrieval Using Machine Translation, Relevance Feedback, and Decompounding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,230.76,374.22,202.73,8.22">Information Retrieval Journal: Special Issue on CLEF</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
