<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,210.54,70.81,191.12,13.14">Sheffield at ImageCLEF 2003</title>
				<funder ref="#_f9ahEpB">
					<orgName type="full">EPSRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,242.52,97.32,46.83,8.22"><forename type="first">Paul</forename><surname>Clough</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Sheffield</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.60,97.32,61.95,8.22"><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Sheffield</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,210.54,70.81,191.12,13.14">Sheffield at ImageCLEF 2003</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8DF7BF9F84F65603849EEABEF5823439</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we use the Systran machine translation system for translating queries for cross language image retrieval in a pilot experiment at CLEF 2003, called ImageCLEF. The approach we have taken is to assume we have little experience in CLIR, few available resources and a limited time in which to create a working CLIR system for this task. In this preliminary study, we investigate the effectiveness of Systran on short queries by comparing a manual assessment of translation adequacy with an automatic score derived using NIST's mteval evaluation tool for machine translation output. We discuss the kinds of translation errors encountered during this analysis and show the impact on retrieval effectiveness for individual queries in the ImageCLEF task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The core component of a Cross Language Information Retrieval (CLIR) system is the method used to translate the query from the source language into the language of the document collection (the target language). However, this component involves specialised IR knowledge and familiarity with the source and target languages, or does it? Imagine you are a company or organisation without these kinds of resources and you want a quick-fix solution to a cross language problem. Can anything be done without buying in the necessary expertise? Can you also evaluate how well the translation process has done without being able to understand the source language? We would argue yes, although not without requiring certain resources (e.g. a CLIR test collection).</p><p>We experiment with using a "black-box" translation module: Systran, one of the oldest commercial machine translation systems, which is widely used in industry and available for free via a Web-based interface. Our experiences with using Systran have found that no multilingual processing is necessary as would normally be required when dealing with cross language retrieval, e.g. tokenisation, case and diacritic normalisation, decompounding and morphological analysis. This sounds ideal, but are there any problems with using Systran to perform the translation? What kinds of translation errors are encountered, does translation quality vary across different source languages, and how does translation quality affect retrieval performance? These are the kinds of questions we wanted to address in this study which formed our entry for ImageCLEF 2003.</p><p>The ImageCLEF test collection can be used to evaluate retrieval performance, but does not necessarily reflect the quality of translation because many factors other than translation might affect performance, e.g. the retrieval system, retrieval enhancements such as query expansion, the relevance assessments or the use of content-based retrieval methods. Therefore, to enable us to investigate where translation errors occur and assess the success of Systran independently from retrieval, we manually assess translation adequacy, and show whether this correlates with an automated approach to measuring translation quality as used in MT evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">The ImageCLEF task</head><p>ImageCLEF is a pilot experiment run at CLEF 2003, dealing with the retrieval of images by their captions in cases where the source and target languages differ (see <ref type="bibr" coords="1,306.00,612.78,10.98,8.22" target="#b0">[1]</ref> for further information about ImageCLEF). Because the document to be retrieved is both visual and textual, approaches to this task may involve the use of both multimodal and multilingual retrieval methods. The primary task at this year's ImageCLEF is an ad hoc retrieval task in which fifty topics were selected for retrieval and described using a topic title and narrative. Only the title is translated into Dutch, Italian, Spanish, French, German and Spanish, and therefore suitable for CLIR. As coordinators of this task, we found that assessors used both the image and the caption during their judgment for relevance, and therefore we know that this task involves more than just CLIR. Further challenges include: <ref type="bibr" coords="1,508.56,677.64,11.03,8.22" target="#b0">(1)</ref> captions that are typically short in length, <ref type="bibr" coords="1,254.14,688.49,11.02,8.22" target="#b1">(2)</ref> images that vary widely in their content and quality, and (3) short user search requests which provide little context for translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Systran</head><p>As a translation system, Systran is considered by many as a direct MT system (because the whole process relies on dictionary lookup between a source and target language), although the stages resemble a transfer-based MT system. Currently the on-line version of Systran offers bi-directional translation between 20 language pairs, including languages from Western Europe, Asia, Eastern Europe, and in 2004 they plan to release English-Arabic.</p><p>There are essentially three stages to Systran: analysis, transfer and synthesis. The first stage, analysis, preprocesses the source text and performs functions such as character set conversion, spelling correction, sentence segmentation, tokenisation, and POS tagging. Also during the analysis phase, Systran performs partial analysis on sentences from the source language, capturing linguistic information such as predicate-argument relations, major syntactic relationships, identification of noun phrases and prepositional phrase attachment using their own linguistic formalism and dictionary lookup.</p><p>After analysis of the source language, the second process of transfer aims to match with the target language through dictionary lookup, and then apply rules to re-order the words according to the target language syntax, e.g. restructure propositions and expressions. The final synthesis stage tidies up the target text and determines grammatical choice to make the result coherent. This stage relies heavily on large tables of rules to make its decisions. For more information, consult <ref type="bibr" coords="2,248.09,266.10,11.01,8.22" target="#b1">[2]</ref> and <ref type="bibr" coords="2,277.32,266.10,10.03,8.22" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Manual assessment of translation quality</head><p>Assessing the quality of the output produced by a machine translation (MT) system offers a challenging problem to researchers. Organisations such as DARPA and NIST have established the necessary resources and framework in which to experiment with, and evaluate, MT systems as part of managed competitions, similar to the TREC (see, e.g. <ref type="bibr" coords="2,129.00,361.25,10.61,8.22" target="#b6">[7]</ref>) and CLEF (see, e.g. <ref type="bibr" coords="2,223.77,361.25,10.61,8.22" target="#b3">[4]</ref>) campaigns. For manual evaluation<ref type="foot" coords="2,370.92,359.18,3.05,5.32" target="#foot_0">1</ref> , three dimensions upon which to base judgment include translation adequacy, fluency and informativeness. Translation quality is normally assessed across an entire document when measuring fluency and informativeness, but adequacy is assessed between smaller units (e.g. paragraphs or sentences) which provide a tighter and more direct semantic relationship.</p><p>To assess adequacy, a high quality reference translation and the output from an MT system are divided into segments to evaluate how well the meaning is conveyed between the versions. Fluency measures how well the translation conveys its content with regards to how the translation is presented and involves no comparison with the reference translation. Informativeness measures how well an assessor has understood the content of a translated document by asking them questions based on the translation and assessing the number answered correctly.</p><p>Given titles from the ImageCLEF test collection in Chinese, Dutch, French, Spanish, German and Italian; we first passed these through the on-line version of Systran to translate them into English, the language of the ImageCLEF document collection. We then asked assessors to judge the adequacy of the translation by assuming the English translation would be that for submission to a retrieval system for an ad hoc task. Translators who had previously been involved with creating the ImageCLEF test collection were chosen to assess translation quality because of their familiarity with the topics and the collection, each assessor given topics in their native language.</p><p>Translators were asked to assess topic titles 2 in the source language with the Systran English version and make a judgment on how well the translation captured the meaning of the original (i.e. how adequate the translated version would be for retrieval purposes). A five-point scale was used to assess translation quality, a score of 5 representing a very good translation (i.e. the same or semantically-equivalent words and syntax), to very bad (i.e. no translation, or the wrong words used altogether). Assessors were asked to take into account the "importance" of translation errors in the scoring, e.g. for retrieval purposes, mis-translated proper nouns might be considered worse than other parts-of-speech. Table <ref type="table" coords="3,116.55,69.18,4.71,8.22">1</ref> shows an example topic title for each language and translation score for very good to good (5-4), okay (3) and bad to very bad (2-1) to provide an idea of the degree of error for these adequacy scores. We find that assessment varies according to each assessor; some being stricter than others, which suggest that, further manual assessments may help to reduce subjectivity. In some cases, particularly Spanish, the source language title contains a spelling mistake which obviously affects translation quality. Some assessors allowed for this in their rating, others did not, therefore suggesting the need to manually check all topics for errors prior to evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Example adequacy ratings assigned manually Table <ref type="table" coords="3,116.61,463.01,4.71,8.22">1</ref> highlights some of the errors produced by the MT system: (1) un-translated words, e.g. "Muzikanten and their instruments", (2) incorrect translation of proper nouns, e.g. "Bateaux sur Loch Lomond" translated as "Boats on Lomond Log" and "Il monte Ben Nevis" translated as "the mount Very Nevis", and (3) mistranslations, e.g. "damage de guerre" translated as "ramming of war". The limited context of the topic titles also produces errors where Systran produces the wrong meaning of a word, e.g. "Scottish blowing chapels" where kapelle is mis-translated as chapel, rather than the correct word band. However, Systran does seem to be able to handle different entry formats for diacritics (accents above characters) which play an important part in selecting the correct translation of a word, e.g. in the query "Casas de te' en la costa" (tearooms by the seaside), the word te' is translated correctly as té (sea) rather than te (you).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Automatic assessment of translation quality</head><p>Although most accurate (and most subjective), manual evaluation is time-consuming and expensive, therefore automatic approaches to assess translation quality have also been proposed, such as the NIST mteval 3 tool. This approach divides documents into segments and computes co-occurrence statistics based on the overlap of word n-grams between a reference translation produced manually and an MT version. This method has been shown to correlate well with adequacy, fluency and informativeness because n-grams capture both lexical overlap and syntactic structure <ref type="bibr" coords="3,163.74,638.76,10.00,8.22" target="#b2">[3]</ref>.</p><p>In the latest version of mteval, two metrics are used to compute translation quality: IBM's BLEU and NIST's own score. Both measures are based on n-gram co-occurrence, although a modified version of NIST's score has been shown to be the preferred measure. These scores assume that the reference translation is of high quality, and that documents assessed are from the same genre. Both measures are also influenced by changes in literal The mountain Ben Nevis form, such that translations with the same meaning but using different words score lower than those that appear exactly the same. This is justified in assuming the manual reference translation is the "best" translation possible and the MT version should be as similar to this as possible. For n-gram scoring, the NIST formula is:</p><p>where β is chosen to make the brevity penalty factor = 0.5 when the number of words in the system output is 2/3 of the average number of words in the reference translation.</p><p>N is the n-gram length.</p><p>L ref is the average number of words in a reference translation, averaged over all reference translations.</p><p>L sys is the number of words in the translation being scored.</p><p>The NIST formula uses info(w 1 …w n ) to weight the "importance" of n-grams based on their length, i.e. that longer n-grams are less likely than shorter ones, and reduces the effects of segment length on the translation score. The information weight is computed from n-gram counts across the set of reference translations. The brevity penalty factor is used to minimise the impact on the score of small variations in the length of a translation. The mteval tool enables control of the n-gram length and maximises matches by normalising case, keeping numerical information as single words, tokenising punctuation into separate words, and concatenating adjacent non-ASCII words into single words.</p><p>In our experiments, we make the assumption that the English topic title is the reference translation, rather than ask the translators to produce an English version from an original. For example, the English topic title "North Street St Andrews" was translated into French as "La rue du Nord St Andrews". We assume that if this were translated into English again, the "best" translation would be "North Street St Andrews". Given that the translators used in the manual assessment were those who created the non-English translations from the English titles in the first place, we feel this assumption can be justified.</p><p>Because manual assessment is based on translation adequacy for retrieval, the Systran version "The street of North St Andrews" (a literal interpretation of the French version) is given a high adequacy rating even though it differs in syntax from the reference translation "North Street St Andrews". The result is that the NIST score for a larger n-gram length would be low and not correlate with the score given manually (see Table <ref type="table" coords="4,476.29,513.23,4.71,8.22">1</ref> for more examples). Therefore to minimise this we compute the NIST score for an n-gram length of 1 word, reducing the measure to simply counting word overlap. In this case, the weighting function has the effect of reducing the importance of those words occurring frequently, e.g. function words.   </p><formula xml:id="formula_0" coords="4,225.66,105.02,187.60,68.20">∑ ∑ ∑ =                                             = N 1 n</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The GLASS retrieval system</head><p>At Sheffield, we have implemented our own version of a probabilistic retrieval system called GLASS, based on the "best match" BM25 weighting operator (see, e.g. <ref type="bibr" coords="5,307.76,92.94,9.94,8.22" target="#b4">[5]</ref>). Captions were indexed using all 8 fields, which include a title, description, photographer, location and set of manually assigned index categories and the default settings of case normalisation, removal of stopwords and word stemming.</p><p>To improve document ranking using BM25, we used an approach where documents containing all query terms were ranked higher than any other. We first identified documents containing all query terms, computed the BM25 score and ranked these highest, followed by all other documents containing at least one query term, again ranked by their BM25 score. The top 1000 images and captions returned for each topic title formed our entry to ImageCLEF. Evaluation was carried out using the set of relevant images for each topic (qrels) which forms part of the ImageCLEF test collection and the NIST information retrieval evaluation program, trec_eval <ref type="foot" coords="5,496.50,188.80,3.39,6.39" target="#foot_3">4</ref> . We evaluate retrieval effectiveness using average precision for each topic, and across topics mean average precision (or MAP) is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Translation quality</head><p>Figure <ref type="figure" coords="5,120.44,271.38,4.71,8.22">1</ref> shows a stacked bar chart of manual assessment scores obtained across each language for each topic. Each bar represents a topic and a maximum bar height of 30 would represent each assessor rating the translation as very good. As expected, the quality of translation is dependent on the topic title, although the majority of topics do get an overall rating that is less than 50-66% of the maximum possible value. The 6 topics with the highest overall manual rating (over 25) are topics 3 (Picture postcard views of St Andrews), 22 (Ruined castles in England), 43 (British windmills), 45 Harvesting), 47 (People dancing) and 49 (Musicians and their instruments). The 2 lowest scoring topics (an overall score &lt; 15) are topics 34 (Dogs rounding-up sheep) and 48 Museum exhibits). Some translations of these topics include: Chinese Dutch German French Italian Spanish</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1</head><p>Manual assessment scores for each ImageCLEF topic Chinese appears to exhibit the greatest variation of scores, and from Table <ref type="table" coords="6,376.12,69.18,4.71,8.22" target="#tab_5">3</ref> has one of the lowest average rating scores (Dutch being the lowest). The Chinese Systran translations are on average the shortest and 14% of the topics get a rating of very bad (3rd highest), and 28% a rating of very good (the lowest). From Table <ref type="table" coords="6,485.43,90.78,3.52,8.22" target="#tab_5">3</ref>, Italian has the highest average manual rating, followed closely by German and Spanish suggesting these are strong bilingual pairings for Systran. French has the highest number of topics rated very poor, followed by Chinese and Italian which is perhaps surprising as French-English is claimed to be one of Systran's strongest translations. Upon inspection, many of these low scores are from words which have not been translated. Italian has the highest number of topics rated very good, followed by German then French. Spanish has fewest topics given a very poor rating. A summary of manual and automatic topic assessment for each source language</p><p>Figure <ref type="figure" coords="6,120.43,316.86,4.71,8.22" target="#fig_1">2</ref> shows a stacked bar chart of the automatic ratings of each topic (the Y axes between the manual and automatic graphs are not comparable) and immediately we see a much larger degree of variation across topics.</p><p>From Table <ref type="table" coords="6,142.01,338.58,3.55,8.22" target="#tab_5">3</ref>, Chinese also has the lowest average NIST score (1.68), which can be explained by the large proportion of topics with a zero score (38%). From Table <ref type="table" coords="6,314.22,349.32,3.52,8.22" target="#tab_5">3</ref>, German and French have the highest average NIST score, followed by Dutch and Spanish.  Table <ref type="table" coords="6,117.79,636.54,4.71,8.22">4</ref> shows the translations with a zero NIST score where the reference and Systran translations have no words which overlap. In many cases, however, this is simply because different words are used to express the same concept, or lexical variations of the word (such as plurals) are used instead. For information retrieval, this is important because if a simple word co-occurrence model is used with no lexical expansion, the queries may not match documents (although in some cases the lexical variations will recover these). This highlights one of the limitations of using mteval for assessing translation quality in CLIR, particularly when the queries are short.</p><p>Table <ref type="table" coords="7,228.56,580.14,4.71,8.22">4</ref> Translations with a NIST score of 0</p><p>These differences also contribute to the lack of correlation between the manual and automatic assessments (shown in Table <ref type="table" coords="7,159.38,612.60,3.41,8.22" target="#tab_5">3</ref>). For Chinese Systran sometimes produces no translation (given a manual score of 1), and there appear to be more cases when the translation has gone seriously wrong. For Dutch, erroneous translations are caused also caused by the incorrect translation of compounds (which also occurs in German).</p><p>The most highly correlated scores are between the assessments for French (using Spearman's rho) suggesting that topics which receive a high manual assessment also receive a high automatic score, thereby confirming the use of an automatic evaluation tool to assess translation quality for CLIR (particularly for French).</p><p>The correlation between manual and automatic results is not consistent across languages, however, where, for example, the correlation for Italian is lowest and not significant (at p&lt;0.01). From Table <ref type="table" coords="8,458.29,79.98,3.54,8.22">4</ref>, many Italian translations are rated highly by manual assessment and the kinds of translations suggest that the problem derives from the inability of mteval to determine semantic equivalents between translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Retrieval performance</head><p>Figure <ref type="figure" coords="8,121.70,136.74,4.71,8.22">3</ref> shows a graph of recall versus precision across all topics and for each language using the strict intersection <ref type="foot" coords="8,137.16,145.52,3.05,5.32" target="#foot_4">5</ref> set of ImageCLEF relevance judgments. The graph follows a typical pattern showing that as the number of relevant documents found increases (recall), the precision decreases as relevant documents appear in lower rank positions. As with other results from CLIR experiments, the monolingual results are higher than those for translated queries, showing that these do not retrieve as well. Chinese has the lowest precision-recall curve, and is noticeably lower than the rest of the languages which seem to bunch together and follow a similar shape. The French curve is the highest of the languages, which matches with Table <ref type="table" coords="8,381.75,201.65,4.71,8.22" target="#tab_5">3</ref> where French has the lowest NIST score, the least number of topics with a zero NIST score, and a high proportion of topics with a high manual assessment rating. Figure <ref type="figure" coords="8,121.03,559.56,4.71,8.22">4</ref> provides a breakdown of average precision for each topic and the stacked bar chart shows average precision for monolingual retrieval and mean average precision across all languages excluding English. Some languages will perform better or worse for each topic (depending on the quality of translation), but the graph provides an overall indication of those topics making analysis clearer. Across all languages (excluding English) and topics, the mean average precision is 0.420 (with a standard deviation of 0.23) which is on average 75% of monolingual performance (Table <ref type="table" coords="8,219.41,613.67,4.71,8.22">4</ref> shows the breakdown across languages).</p><p>Topics which perform poorly include 4 (seating inside a church), 5 (woodland scenes), 29 (wartime aviation), 41 (a coat of arms) and 48 (museum exhibits). These exhibit average NIST scores of 2.63, 0.64, 2.80, 3.71 and 3.83 respectively, and manual ratings of 3, 3.7, 4.17, 3.5 and 1.83 respectively. In some cases, the translation quality is high, but the retrieval low, e.g. topic 29, because relevance assessment for cross language image retrieval is based upon the image and caption. There are cases when images are not relevant, even though they contain query terms in the caption, e.g. the image is too small, too dark, the object of interest is obscured or in the background, or the caption contains words which do not describe the image contents (e.g. matches on fields such as the photographer, or notes which provide background meta-information).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4</head><p>Monolingual average precision and MAP across systems (excluding English) for each topic Table <ref type="table" coords="9,120.00,393.72,4.71,8.22" target="#tab_6">5</ref> summarises retrieval performance for each language, and also shows the correlation between manual/automatic assessment of translation quality and average precision for each language. We find that French has the highest MAP score (78% monolingual), followed by German (75% monolingual) and Spanish (73% monolingual). On average, MAP and translation quality is correlated (using Spearman's rho with p&lt;0.01) for both the manual and automatic assessments which suggests that a higher quality of translation does give better retrieval performance in general, particularly for Chinese, German and French (manual assessments) and Spanish, French and Dutch (automatic assessments). A summary of retrieval performance and its correlation with translation quality</p><p>We might expect MAP to correlate well with the NIST score for the GLASS system because both are based on word co-occurrences, but it is interesting to note that retrieval effectiveness is correlated just as highly with the manual assessments, even though correlation between the manual and automatic assessments is not always itself high. This is useful as it shows that as a CLIR task, the quality of translation has a significant impact on retrieval thereby enabling, in general, retrieval effectiveness to indicate the quality of translation. Remaining factors may be due to relevance assessments, the IR system, pseudo relevance feedback or use of other retrieval-enhancing methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and future work</head><p>We have shown that cross language image retrieval for the ImageCLEF ad hoc task is possible with little or no knowledge of CLIR, or requirement of linguistic resources. Using Systran as a translation "black-box" requires little effort, but at the price of having no control over translation or being able to recover when translation goes wrong. In particular, Systran provides only one translation version which may not be correct and would provide better CLIR if several alternatives were output. There are many cases when proper names are mistranslated, words with diacritics not interpreted properly, and words translated incorrectly because of the limited degree of context. Because the task of CLIR does not necessarily require syntactic correctness, we find Systran can be used successfully for translation between a wide range of language pairs where essentially we make use of only the large dictionaries maintained by Systran.</p><p>We evaluated the quality of translation using both manual assessments, and an automatic tool used extensively in MT evaluation. We find that quality varies between different languages for Systran based on both the manual and automatic score which is correlated, sometimes highly, for all languages. There are limitations, however, with the automatic tool which would improve correlation for query quality in CLIR evaluation, such as resolving literal equivalents for semantically similar terms, reducing words to their stems, removing function words, and maybe using a different weighting scheme for query terms (e.g. weight proper names highly). We aim to experiment further with semantic equivalents using Wordnet, and also assess whether correlation between the manual and automatic scores can be improved by using longer n-gram lengths.</p><p>Using a probabilistic retrieval system, we obtain a mean average precision score which is 75% of the monolingual score. Although Chinese retrieval is lowest at 51%, this would still provide multi-lingual access to the ImageCLEF test collection, albeit needing improvement. Also, given that the task is not purely text, but also involves images, this score may be improved using content-based methods of retrieval. We aim to experiment with pseudo relevance feedback, and in particular improve performance using query expansion based on EuroWordnet, a European version of Wordnet.</p><p>As a retrieval task, we have shown that translation quality does affect retrieval performance because of the correlation between manual assessments and retrieval performance, implying that in general, higher translation quality results in higher retrieval performance. We have also shown that for some languages, the manual assessments correlate well with the automatic assessment suggesting this method could be used to measure translation quality given a CLIR test collection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,176.28,614.94,31.59,8.22;6,244.01,614.94,191.87,8.22"><head>Figure 2 Automatic</head><label>2</label><figDesc>Figure 2Automatic NIST scores for each ImageCLEF topic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,187.92,535.80,31.65,8.22;8,255.66,535.80,168.53,8.22"><head></head><label></label><figDesc>Figure 3Precision-recall graph for the Sheffield entry</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,92.70,545.69,426.82,115.41"><head></head><label></label><figDesc>Table2shows example translations and their corresponding NIST score for Chinese translations. To use mteval, we created a reference containing the English versions of the topic titles where each title represents a segment within a document, and a test file containing the Systran versions in the same format.</figDesc><table coords="4,128.76,614.02,322.54,47.08"><row><cell>NIST score</cell><cell>Reference translation</cell><cell>Test translation</cell></row><row><cell>8.1294</cell><cell>Mountain scenery</cell><cell>Mountain scenery</cell></row><row><cell>3.3147</cell><cell>People dancing</cell><cell>dances people</cell></row><row><cell>1.727</cell><cell>Picture postcards by the valentine</cell><cell>the Tanzania photography company</cell></row><row><cell></cell><cell>photographic company</cell><cell>photographs scenery postcard</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,147.66,674.94,316.76,8.22"><head>Table 2 Example</head><label>2</label><figDesc></figDesc><table /><note coords="4,251.18,674.94,213.23,8.22"><p>translations and corresponding NIST score (for Chinese)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,92.70,176.89,422.74,126.59"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="6,92.70,176.89,422.74,107.09"><row><cell></cell><cell>Avg</cell><cell>Avg</cell><cell>man-NIST</cell><cell cols="4">Avg translation length</cell><cell>% topics</cell><cell>% topics</cell><cell>% topics</cell></row><row><cell></cell><cell>manual</cell><cell>NIST</cell><cell>correlation</cell><cell></cell><cell cols="2">(words)</cell><cell></cell><cell>with</cell><cell>with</cell><cell>with</cell></row><row><cell></cell><cell>score</cell><cell>score</cell><cell>(Spearman's</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>manual</cell><cell>manual</cell><cell>NIST</cell></row><row><cell></cell><cell></cell><cell></cell><cell>rho)</cell><cell>Min</cell><cell cols="2">Max Mean</cell><cell>SD</cell><cell>score of 1</cell><cell>score of 5</cell><cell>score=0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(very bad)</cell><cell>(very good)</cell><cell></cell></row><row><cell>Chinese</cell><cell>3.34</cell><cell>1.68</cell><cell>0.268*</cell><cell>0</cell><cell>14</cell><cell>3.76</cell><cell>2.65</cell><cell>14%</cell><cell>28%</cell><cell>38%</cell></row><row><cell>Dutch</cell><cell>3.32</cell><cell>3.27</cell><cell>0.426*</cell><cell>1</cell><cell>13</cell><cell>4.32</cell><cell>2.30</cell><cell>8%</cell><cell>30%</cell><cell>12%</cell></row><row><cell>German</cell><cell>3.64</cell><cell>3.67</cell><cell>0.492*</cell><cell>0</cell><cell>9</cell><cell>3.96</cell><cell>1.85</cell><cell>8%</cell><cell>44%</cell><cell>10%</cell></row><row><cell>French</cell><cell>3.38</cell><cell>3.67</cell><cell>0.647*</cell><cell>2</cell><cell>10</cell><cell>4.78</cell><cell>1.96</cell><cell>24%</cell><cell>40%</cell><cell>8%</cell></row><row><cell>Italian</cell><cell>3.65</cell><cell>2.87</cell><cell>0.184</cell><cell>1</cell><cell>11</cell><cell>5.12</cell><cell>2.05</cell><cell>12%</cell><cell>50%</cell><cell>18%</cell></row><row><cell>Spanish</cell><cell>3.64</cell><cell>3.24</cell><cell>0.295*</cell><cell>1</cell><cell>8</cell><cell>4.38</cell><cell>1.52</cell><cell>6%</cell><cell>34%</cell><cell>10%</cell></row><row><cell cols="3">*correlation significant at p&lt;0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,126.60,481.99,313.26,128.51"><head>Table 5</head><label>5</label><figDesc></figDesc><table coords="9,158.16,481.99,281.70,108.95"><row><cell>Language</cell><cell>Mean</cell><cell>MAP-manual</cell><cell>MAP-NIST</cell><cell>% of</cell></row><row><cell></cell><cell>Average</cell><cell>correlation</cell><cell>correlation</cell><cell>monolingual</cell></row><row><cell></cell><cell>Precision</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(MAP)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Chinese</cell><cell>0.285</cell><cell>0.472*</cell><cell>0.384*</cell><cell>51%</cell></row><row><cell>Dutch</cell><cell>0.390</cell><cell>0.412*</cell><cell>0.426*</cell><cell>69%</cell></row><row><cell>German</cell><cell>0.423</cell><cell>0.503*</cell><cell>0.324*</cell><cell>75%</cell></row><row><cell>French</cell><cell>0.438</cell><cell>0.460*</cell><cell>0.456*</cell><cell>78%</cell></row><row><cell>Italian</cell><cell>0.405</cell><cell>0.394*</cell><cell>0.378*</cell><cell>72%</cell></row><row><cell>Spanish</cell><cell>0.408</cell><cell>-0.061</cell><cell>0.462*</cell><cell>73%</cell></row><row><cell>Monolingual</cell><cell>0.562</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">*correlation significant at p&lt;0.01</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,97.62,685.05,277.50,7.38"><p>See, e.g. the TIDES translation pages: http://www.ldc.upenn.edu/Projects/TIDES/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,97.62,694.83,207.66,7.38"><p>In cases of multiple translations, we used the first translation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,97.68,704.55,313.89,7.38"><p>We used mteval-v09.pl which can be downloaded from: http://www.nist.gov/speech/tests/mt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,97.68,704.54,179.29,7.39"><p>We used a version of trec_eval supplied by UMASS.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="8,97.62,694.82,407.80,7.39;8,92.70,704.54,232.81,7.39"><p>Strict intersection is the smallest set of relevance documents including only those which co-occur between assessors and marked as relevant (not including those judged as partially relevant).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgments</head><p>We would like to thank members of the <rs type="institution">Natural Language Processing group and Department of Information Studies</rs> for their time and effort in producing manual assessments. Thanks also to <rs type="person">Hideo Joho</rs> for help and support with the GLASS system, and in particular his modified BM25 ranking algorithm, and thanks to <rs type="institution">NTU</rs> for providing Chinese versions of the ImageCLEF titles. This work was carried out within the Eurovision project at <rs type="institution">Sheffield University</rs>, funded by the <rs type="funder">EPSRC</rs> (Eurovision: <rs type="grantNumber">GR/R56778/01</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_f9ahEpB">
					<idno type="grant-number">GR/R56778/01</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language</head><p>Reference </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,109.63,531.86,405.18,7.39" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,211.21,531.86,175.13,7.39">The CLEF 2003 cross language image retrieval task</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,400.68,531.87,88.48,7.38">Proceedings of CLEF2003</title>
		<meeting>CLEF2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.59,551.30,396.85,7.39" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><surname>Heisoft</surname></persName>
		</author>
		<ptr target="http://www.heisoft.de/volltext/systran/dok2/howworke.htm" />
		<title level="m" coord="10,138.73,551.30,80.69,7.39">How does Systran work</title>
		<imprint>
			<date type="published" when="2003-07">July 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.65,570.80,408.37,7.39;10,109.62,580.58,328.12,7.39" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,301.03,570.80,216.99,7.39;10,109.62,580.58,102.96,7.39">Automatic Evaluation of Machine Translation Quality Using Ngram Co-Occurrence Statistics</title>
		<ptr target="http://www.nist.gov/speech/tests/mt/resources/scoring.htm" />
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.57,600.02,393.52,7.39;10,109.62,609.74,254.41,7.39" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,204.98,600.02,200.22,7.39">Cross-Language System Evaluation: The CLEF Campaigns</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,420.53,600.03,82.56,7.38;10,109.62,609.75,161.77,7.38">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1067" to="1072" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.60,629.24,399.49,7.39;10,109.62,638.96,283.96,7.39" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,253.88,629.24,242.77,7.39">Okapi at TREC-7: automatic ad hoc, filtering VLC and interactive track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
		<idno>TREC-7</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,109.62,638.97,86.07,7.38">NIST Special Publication</title>
		<meeting><address><addrLine>Gaithersburg, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="253" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.59,658.46,396.70,7.39;10,109.62,668.18,211.46,7.39" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,139.18,658.46,340.41,7.39">The SYSRAN linguistics platform: A software solution to manage multilingual corporate knowledge</title>
		<ptr target="http://www.systransoft.com/Technology/SLP.pdf" />
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Systran</publisher>
		</imprint>
	</monogr>
	<note>White paper</note>
</biblStruct>

<biblStruct coords="10,109.61,687.62,345.69,7.39" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,219.36,687.62,66.25,7.39">Overview of TREC</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,318.19,687.63,88.52,7.38">Proceedings of TREC2001</title>
		<meeting>TREC2001</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
