<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,106.48,146.03,390.02,18.08;1,168.56,167.95,265.87,18.08;1,109.94,189.87,234.96,18.08;1,344.90,188.10,22.11,12.55;1,373.96,189.87,119.08,18.08">Lexical and Algorithmic Stemming Compared for 9 European Languages with Hummingbird SearchServer TM at CLEF 2003</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2003-07-20">July 20, 2003</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,259.82,224.93,83.37,10.46;1,271.19,238.87,60.64,10.46"><forename type="first">Stephen</forename><surname>Tomlinson Hummingbird</surname></persName>
							<email>stephen.tomlinson@hummingbird.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Ottawa</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,106.48,146.03,390.02,18.08;1,168.56,167.95,265.87,18.08;1,109.94,189.87,234.96,18.08;1,344.90,188.10,22.11,12.55;1,373.96,189.87,119.08,18.08">Lexical and Algorithmic Stemming Compared for 9 European Languages with Hummingbird SearchServer TM at CLEF 2003</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2003-07-20">July 20, 2003</date>
						</imprint>
					</monogr>
					<idno type="MD5">66E292E10758C435FF66240EEA9A94E9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hummingbird participated in the monolingual information retrieval tasks of the Cross-Language Evaluation Forum (CLEF) 2003: for natural language queries in 9 European languages (German, French, Italian, Spanish, Dutch, Finnish, Swedish, Russian and English) find all the relevant documents (with high precision) in the CLEF 2003 document sets. For each language, SearchServer scored higher than the median average precision on more topics than it scored lower. In a comparison of experimental SearchServer lexical stemmers with Porter's algorithmic stemmers, the biggest differences were for the languages in which compound words are frequent (German, Dutch, Finnish and Swedish). SearchServer scored significantly higher in average precision for German and Finnish, apparently from its ability to split compound words and find terms when they are parts of compounds in these languages. Most of the differences for the other languages appeared to be from SearchServer's lexical stemmers performing inflectional stemming while the algorithmic stemmers often additionally performed derivational stemming; these differences did not pass a significance test.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hummingbird SearchServer<ref type="foot" coords="1,208.98,550.46,3.97,7.32" target="#foot_0">1</ref> is an indexing, search and retrieval engine for embedding in Windows and UNIX information applications. SearchServer, originally a product of Fulcrum Technologies, was acquired by Hummingbird in 1999. Founded in 1983 in Ottawa, Canada, Fulcrum produced the first commercial application program interface (API) for writing information retrieval applications, Fulcrum r Ful/Text TM . The SearchServer kernel is embedded in many Hummingbird products, including SearchServer, an application toolkit used for knowledge-intensive applications that require fast access to unstructured information.</p><p>SearchServer supports a variation of the Structured Query Language (SQL), SearchSQL TM , which has extensions for text retrieval. SearchServer conforms to subsets of the Open Database Connectivity (ODBC) interface for C programming language applications and the Java Database Connectivity (JDBC) interface for Java applications. Almost 200 document formats are supported, such as Word, WordPerfect, Excel, PowerPoint, PDF and HTML.</p><p>SearchServer works in Unicode internally <ref type="bibr" coords="1,285.64,694.99,10.52,10.46" target="#b2">[3]</ref> and supports most of the world's major character sets and languages. The major conferences in text retrieval evaluation (CLEF <ref type="bibr" coords="1,431.03,706.94,9.96,10.46" target="#b0">[1]</ref>, NTCIR <ref type="bibr" coords="1,483.33,706.94,10.52,10.46" target="#b3">[4]</ref> and TREC <ref type="bibr" coords="2,121.57,287.81,10.79,10.46" target="#b6">[7]</ref>) have provided opportunities to objectively evaluate SearchServer's support for natural language queries in more than a dozen languages. This (draft) paper looks at experimental work with SearchServer for the task of finding relevant documents for natural language queries in 9 European languages using the CLEF 2003 test collections. For the experiments described in this paper, an experimental post-5.x version of SearchServer was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>The CLEF 2003 document sets consisted of tagged (SGML-formatted) news articles (mostly from 1994 and 1995) in 9 different languages: German, French, Italian, Spanish, Dutch, Swedish, Finnish, Russian and English. Compared to last year, Russian was new, and there were more documents in Spanish, German, Italian, French and English. The English documents included some British English for the first time. Table <ref type="table" coords="2,290.66,470.56,4.98,10.46" target="#tab_0">1</ref> gives the sizes.</p><p>The CLEF organizers created 60 natural language "topics" (numbered 141-200) and translated them into many languages. Each topic contained a "Title" (subject of the topic), "Description" (a one-sentence specification of the information need) and "Narrative" (more detailed guidelines for what a relevant document should or should not contain). The participants were asked to use the Title and Description fields for at least one automatic submission per task this year to facilitate comparison of results.</p><p>For more information on the CLEF test collections, see the CLEF web site <ref type="bibr" coords="2,434.98,554.25,9.96,10.46" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Indexing</head><p>A separate SearchServer table was created for the documents of each language. For details of the SearchServer syntax, see last year's paper <ref type="bibr" coords="2,274.95,612.48,9.96,10.46" target="#b7">[8]</ref>.</p><p>Unlike last year, we used SearchServer's default of not indexing accents for all languages, except for Russian, for which we indexed the combining breve (Unicode 0x0306) so that the Cyrillic Short I (0x0419) was not normalized to the Cyrillic I (0x0418).</p><p>We treated the apostrophe as a word separator for all languages except English. Typically, a couple hundred stop words were excluded from indexing for each language (e.g. "the", "by" and "of" in English). The Porter web site <ref type="bibr" coords="2,348.05,684.21,10.52,10.46" target="#b4">[5]</ref> contains stop word lists for most European languages. We used its list for Russian, but our lists for other languages may contain differences.</p><p>SearchServer internally uses Unicode. A different option to SearchServer's translation text reader was specified for Russian (UTF8 UCS2) than for the other languages (Win 1252 UCS2) because the Russian documents were encoded in the UTF-8 character set and the documents for the other languages were encoded in the Latin-1 character set. (A custom text reader, cTREC, was also updated to maintain support for the CLEF guidelines of only indexing specifically tagged fields; the new British and Russian collections necessitated the update.)</p><p>By default, the SearchServer index supports both exact matching (after some Unicode-based normalizations, such as converting to upper-case and decomposed form) and matching of inflections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lexical Stemming</head><p>For many languages (including all 9 European languages of CLEF 2003), SearchServer includes the option of finding inflections based on lexical stemming (i.e. stemming based on a dictionary or lexicon for the language). For example, in English, "baby", "babied", "babies", "baby's" and "babying" all have "baby" as a stem. Specifying an inflected search for any of these terms will match all of the others. The lexical stemming of the experimental development version of SearchServer used for the experiments in this paper was based on Inxight LinguistX Platform 3.5. Unlike the previous two years, the lexical stemming was conducted in an "expanded" mode which tolerates missing accents (e.g. unlike last year, "bebes" stems to "bébé" in French) and handles more plural cases (e.g. unlike last year, "PCs" stems to "PC" in English).</p><p>For all languages, we used inflectional stemming which generally retains the part of speech (e.g. a plural of a noun is typically stemmed to the singular form). We did not use derivational stemming which would often change the part of speech or the meaning more substantially (e.g. "performer" is not stemmed to "perform").</p><p>SearchServer's lexical stemming includes compound-splitting (decompounding) for compound words in German, Dutch and Finnish (but not for Swedish in this version, and not for the other languages as it is not generally applicable). For example, in German, "babykost" (baby food) has "baby" and "kost" as stems.</p><p>SearchServer's lexical stemming also supports some spelling variations. In English, British and American spellings have the same stems, e.g. "labour" stems to "labor", "hospitalisation" stems to "hospitalization" and "plough" stems to "plow".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Intuitive Searching</head><p>For all runs, we used SearchServer's Intuitive Searching, i.e. the IS ABOUT predicate of Search-SQL, which accepts unstructured natural language text. For example, for the German version of topic 41 (from a previous year), the Title was "Pestizide in Babykost" (Pesticides in Baby Food), and the Description was "Berichte über Pestizide in Babynahrung sind gesucht" (Find reports on pesticides in baby food). A corresponding SearchSQL query would be:</p><p>SELECT RELEVANCE('V2:3') AS REL, DOCNO FROM CLEF03DE WHERE FT TEXT IS ABOUT 'Pestizide in Babykost Berichte über Pestizide in Babynahrung sind gesucht' ORDER BY REL DESC;</p><p>For the Russian queries, the statement "SET CHARACTER SET 'UTF8C' " was previously executed because the queries were in UTF-8 instead of Latin-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Statistical Relevance Ranking</head><p>SearchServer's relevance value calculation is the same as described last year <ref type="bibr" coords="3,427.69,691.53,9.96,10.46" target="#b7">[8]</ref>. Briefly, Search-Server dampens the term frequency and adjusts for document length in a manner similar to Okapi <ref type="bibr" coords="3,118.90,715.44,10.52,10.46" target="#b5">[6]</ref> and dampens the inverse document frequency using an approximation of the logarithm. SearchServer's relevance values are always an integer in the range 0 to 1000. SearchServer's RELEVANCE METHOD setting can be used to optionally square the importance of the inverse document frequency (by choosing a RELEVANCE METHOD of 'V2:4' instead of 'V2:3'). The importance of document length to the ranking is controlled by Search-Server's RELEVANCE DLEN IMP setting (scale of 0 to 1000). For all runs in this paper, REL-EVANCE METHOD was set to 'V2:3' and RELEVANCE DLEN IMP was set to 750.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Query Stop Words</head><p>We automatically removed words such as "find", "relevant" and "document" from the topics before presenting them to SearchServer, i.e. words which are not stop words in general but were commonly used in the CLEF topics as general instructions. For the submitted runs, the lists were developed by examining the CLEF 2000, 2001 and 2002 topics (not this year's topics). An evaluation in last year's paper <ref type="bibr" coords="4,224.66,252.27,10.52,10.46" target="#b7">[8]</ref> found this step to be of only minor impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Query Expansion</head><p>For one of the submitted runs for each language (the runs with identifiers ending with 'e', e.g. humDE03tde), the first 3 rows from the other submitted run for the language (e.g. humDE03td) were used to find additional query terms. Only terms appearing in at most 5% of the documents (based on the most common inflection of the term) were included. Mathematically, the approach is similar to Rocchio feedback with weights of one-half for the original query and one-sixth for each of the 3 expansion rows. See section 5.2 of <ref type="bibr" coords="4,298.40,358.33,10.52,10.46" target="#b8">[9]</ref> for more details. This is the first time we have used a blind feedback technique for CLEF submissions. We did not use it for any of the diagnostic experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Evaluation Measures</head><p>The evaluation measures are likely explained in an appendix of this volume. Briefly: "Precision" is the percentage of retrieved documents which are relevant. "Precision@n" is the precision after n documents have been retrieved. "Average precision" for a topic is the average of the precision after each relevant document is retrieved (using zero as the precision for relevant documents which are not retrieved). "Recall" is the percentage of relevant documents which have been retrieved. "Interpolated precision" at a particular recall level for a topic is the maximum precision achieved for the topic at that or any higher recall level. For a set of topics, the measure is the mean of the measure for each topic (i.e. all topics are weighted equally).</p><p>The Monolingual Information Retrieval tasks were to run 60 queries against document collections in the same language and submit a list of the top-1000 ranked documents to CLEF for judging (in May 2003). CLEF produced a "qrels" file for each of the 9 tasks: a list of documents judged to be relevant or not relevant for each topic. (For Swedish, this draft paper uses the preliminary set of qrels.)</p><p>For some topics and languages, no documents were judged relevant. The precision scores are just averaged over the number of topics for which at least one document was judged relevant.</p><p>For tables focusing on the impact of one particular difference in approach (such as a stemming method as in Table <ref type="table" coords="4,177.50,619.80,3.87,10.46" target="#tab_1">2</ref>), the columns are as follows:</p><p>• "Experiment" is the language and topic fields used (for example, "-td" indicates the Title and Description fields were used).</p><p>• "AvgDiff" is the average (mean) difference in the precision score.</p><p>• "95% Confidence" is an approximate 95% confidence interval for the average difference calculated using Efron's bootstrap percentile method 2 [2] (using 100,000 iterations). If zero is not in the interval, the result is "statistically significant" (at the 5% level), i.e. the feature is unlikely to be of neutral impact, though if the average difference is small (e.g. &lt;0.020) it may still be too minor to be considered "significant" in the magnitude sense.</p><p>• "vs." is the number of topics on which the precision was higher, lower and tied (respectively) with the feature enabled. These numbers should always add to the number of topics for the language (as per Table <ref type="table" coords="5,217.38,166.15,3.87,10.46" target="#tab_2">3</ref>).</p><p>• "2 Largest Diffs (Topic)" lists the two largest differences in the precision score (based on the absolute value), with each followed by the corresponding topic number in brackets (the topic numbers range from 141 to 200).</p><p>For tables providing multiple precision scores (such as Table <ref type="table" coords="5,365.91,229.91,3.87,10.46" target="#tab_2">3</ref>), listed for each run are its mean average precision (AvgP), the mean precision after 5, 10 and 20 documents retrieved (P@5, P@10 and P@20 respectively), the mean interpolated precision at 0% and 30% recall (Rec0 and Rec30 respectively), and the mean precision after R documents retrieved (P@R) where R is the number of relevant documents for the topic. The number of topics with at least one relevant document is also included in this table, though it is a property of the test collection, not of the run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">Submitted Runs</head><p>In the identifiers for the submitted runs (e.g. humDE03tde), the first 3 letters "hum" indicate a Hummingbird submission, the next 2 letters are the language code, and the number "03" indicates CLEF 2003. "t", "d" and "n" indicate that the Title, Description and Narrative field of the topic were used (respectively). "e" indicates that query expansion from blind feedback was used. The submitted runs all used inflections from SearchServer's lexical stemming.</p><p>The following language codes were used: "DE" for German, "EN" for English, "ES" for Spanish, "FI" for Finnish, "FR" for French, "IT" for Italian, "NL" for Dutch, "RU" for Russian, and "SV" for Swedish.</p><p>For each language, we submitted a "td" and "tde" run (namely "humDE03td", "humDE03tde", "humFR03td", "humFR03tde", "humIT03td", "humIT03tde", "humES03td", "humES03tde", "humNL03td", "humNL03tde", "humFI03td", "humFI03tde", "humSV03td", "humSV03tde", "humRU03td" and "humRU03tde"). Note that monolingual English submissions were not allowed. For Russian, additional runs were requested for the judging pools, so we also submitted Title-only runs ("humRU03t" and "humRU03te") and full topic runs ("humRU03tdn" and "humRU03tdne"). For 3 other Russian submissions ("humRU03tm", "humRU03tdm", "humRU03tdnm"), the "m" was meant to indicate that morphology (stemming) was disabled, but by accident for these runs the CHARACTER SET was set to Latin-1 instead of UTF-8, which led to precision scores of almost zero.</p><p>The scores of the submitted runs are likely listed in an appendix of this volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Comparison of Lexical and Algorithmic Stemming</head><p>The experimental version of SearchServer used for these experiments allows plugging-in of custom stemming modules. As a test for this feature, we have experimented with plugging-in Porter's algorithmic "Snowball" stemmers <ref type="bibr" coords="5,239.32,629.83,9.96,10.46" target="#b4">[5]</ref>. For English, the Porter2 version was used. Table <ref type="table" coords="5,133.16,641.78,4.98,10.46" target="#tab_1">2</ref> contains the results of a diagnostic experiment comparing average precision for the short (Title-only) queries when the only difference is the stemmer used: the experimental Search-Server lexical stemmer or Porter's algorithmic stemmer. Positive differences indicate that the SearchServer stemmer led to a higher score and negative differences indicate that the algorithmic stemmer led to a higher score. SearchServer's stemmer scored significantly higher for Finnish and German and significantly lower for Swedish. The differences for the other languages didn't pass the significance test.</p><p>To try to better understand the differences between these approaches to stemming, we look at least at the topics for each language with the two biggest differences in the average precision score (usually we look at more than two). We just look at the shorter Title-only topics for ease of analysis (fewer words in the query makes it easier to see what caused the difference) and because shorter queries are preferred by users anyway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">English Stemming</head><p>English topics 180 (Bankruptcy of Barings), 179 (Resignation of NATO Secretary General), 175 (Everglades Environmental Damage) and 168 (Assassination of Rabin) show that the algorithmic stemmer often performs derivational stemming (whereas the SearchServer stemmer is known to just do inflectional stemming as described earlier). In the case of topic 180, derivational stemming lowered the average precision score because it was harmful for this topic to match "Barings" with "bare", "bares" and "barely". But for topics 179, deriving "resign" and "resigned" from "resignation" was apparently helpful. Likewise, for topic 175, deriving "environment" from "environmental" was apparently helpful, and in topic 168 deriving "assassin" from "assassination" was apparently helpful. SearchServer's stemmer internally has the option of derivational stemming for English (and handles all of these cases similarly), but there is not currently an option to enable it. It might make for an interesting future experiment to try it. English topic 200 (Flooding in Holland and Germany) illustrated that another difference for English is the handling of apostrophe-S. Perhaps surprisingly, the algorithmic stemmer never removes apostrophe-S. The SearchServer stemmer does remove it in some cases, e.g. it appears SearchServer scored higher on topic 200 because it matched "Holland's" with "Holland" and "Germany's" with "Germany". In topic 169, SearchServer matched "NATO's" with "NATO" and "general's" with "general". But in topic 168, "Rabin's" was not matched with "Rabin", so SearchServer is not using a simple rule (a more familiar case is that SearchServer does not match "Parkinson's" to "Parkinson"). For the other languages, we treated the apostrophe as a word separator, so handling of apostrophes won't be an issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">French Stemming</head><p>French topics 145 (Le Japon et ses importations de riz (Japanese Rice Imports)) and 177 (La consommation de lait en Europe (Milk Consumption in Europe)) illustrate that the French algorithmic stemmer also does some derivational stemming. In topic 145, the algorithmic stemmer matched the noun "imports" with verb forms such as "importé" and "importer", which apparently was helpful to the average precision score (though additionally deriving the unrelated terms "importance" and "important" might be disconcerting to a user). It also derived "Japonais" from "Japon". In topic 177, deriving "consommateurs" (consumers) and "consommateur" (consumer) from "consommation" (consumption) apparently hurt average precision.</p><p>French topic 162 (l'Union Européenne et les douanes turques (EU and Turkish Customs)) shows that sometimes SearchServer handles irregular inflections that the algorithmic stemmer does not.</p><p>SearchServer matched "turques" with "turc" and "turcs", unlike the algorithmic stemmer. Both matched "turques" with "turque". The algorithmic stemmer additionally derived "turquie" which appears to be why it scored higher on this topic. Overall, for the French topics, Table <ref type="table" coords="7,478.84,134.27,4.98,10.46" target="#tab_1">2</ref> shows that neither stemmer scored significantly higher than the other (the confidence interval contains zero).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Italian Stemming</head><p>In Italian topic 161 (Diete per Celiaci (Diets for Celiacs)), the algorithmic stemmer found the one relevant document by matching "celiaci" with "celiaca". SearchServer stemmed "celiaci" to "celiare" and "celiaca" to itself and so did not make this match. We should investigate this case further.</p><p>In Italian topic 157 (Campionesse di Wimbledon (Wimbledon Lady Winners)), both stemmers matched "campionesse" with "campionessa", but SearchServer additionally matched "campioni" and "campione", which hurt average precision in this case.</p><p>In Italian topic 187 (Trasporto Nucleare in Germania (Nuclear Transport in Germany)), Search-Server scored higher, apparently from matching "nucleare" with "nucleari", unlike the algorithmic stemmer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Spanish Stemming</head><p>In Spanish topic 186 (Coalición del gobierno holandés (Dutch Coalition Government)), Search-Server matched "holandés" with "holandeses" and "holandesa", unlike the algorithmic stemmer, and SearchServer scored a good 0.57 average precision, but the algorithmic stemmer derived "holandés" to "holanda", which apparently helped it score higher (0.75).</p><p>In Spanish topic 151 (Las maravillas del Mundo Antiguo (Wonders of Ancient World)), the algorithmic stemmer derived more terms from "maravillas" (wonders) such as "maravilloso" (wonderful) which hurt precision. Both stemmers matched "Antiguo" with "antiguos" and "antigua" (among others), and SearchServer additionally matched "antiquísima" which may have been helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">German Stemming</head><p>For German topic 174 (Bayerischer Kruzifixstreit (Bavarian Crucifix Quarrel)), SearchServer split the compound word "Kruzifixstreit" and found many relevant documents by matching terms such as "Kruzifix", "Kruzifixen" and "Kruzifixe" (and also "Streit", though it seemed less important in this case). The algorithmic stemmer does not support compound-splitting, and "Kruzifixstreit" did not itself appear in the document set (nor did any compound variant of it), so it scored dramatically lower for this topic as can be seen in Table <ref type="table" coords="7,338.21,560.02,3.87,10.46" target="#tab_1">2</ref>.</p><p>For German topic 158 (Fußball-Rowdys in Dublin (Soccer Riots in Dublin)), even though there was no compound word in the query, the relevant documents used compound words such as "Fussballrowdies" and "Fussballfans" which SearchServer successfully matched but the algorithmic stemmer did not.</p><p>German topic 190 (Kinderarbeit in Asien (Child Labor in Asia)) shows that compound-splitting is not always helpful. In this topic it hurt precision a lot to split "Kinderarbeit", presumably because the term was typically used in that form in the relevant documents, and a lot of other documents used the German words for children and work in other contexts. (This happens a lot in information retrieval; a technique that works well on average can still have a substantial percentage of cases for which it is harmful. While there may be room for automatic improvement, it's a good idea for applications to let the user override the defaults when desired.)</p><p>Overall for German, Table <ref type="table" coords="7,223.96,703.49,4.98,10.46" target="#tab_1">2</ref> shows that the SearchServer stemmer scored significantly higher on average, presumably because of compound-splitting. It appears it would be hard to isolate the impact of other differences because even when none of the query terms are compound words, the terms in the documents may be parts of compounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Dutch Stemming</head><p>Dutch topic 174 (Beierse Kruisbeeldstrijd (Bavarian Crucifix Quarrel)) is the Dutch version of the crucifix query examined earlier for German. SearchServer scores highly for similar reasons, i.e. SearchServer splits the compound and matches "kruisbeeld" and "strijd" among other forms. "Kruisbeeldstrijd" did not itself appear in the document set and the algorithmic stemmer scored dramatically lower.</p><p>Dutch topic 165 (Golden Globes 1994 (Golden Globes 1994)) is a case for Dutch in which a large difference in average precision did not result from compound handling differences. SearchServer apparently scored higher from matching "Globes" with "Globe" and perhaps also from matching "golden" with "gelden". If compound words aren't as frequent in Dutch as German, that may be why the overall differences between the stemmers did not quite pass the significance test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Finnish Stemming</head><p>For Finnish topic 185 (Hollantilaisten valokuvat Srebrenicasta (Dutch Photos of Srebrenica)), SearchServer did not match any of "Srebrenicassa", "Srebrenica" and "Srebrenican", variants of "Srebrenicasta" in the relevant document matched by the algorithmic stemmer. Srebrenica is a proper noun. Porter mentions in <ref type="bibr" coords="8,231.80,318.48,10.52,10.46" target="#b4">[5]</ref> that "in a language in which proper names are inflected (Latin, Finnish, Russian ...), a dictionary-based stemmer will need to remove i-suffixes independently of dictionary look-up, because the proper names will not of course be in the dictionary." We should investigate if we are handling proper nouns adequately for languages such as Finnish and Russian.</p><p>Finnish topic 196 (Japanilaisten pankkien fuusio (Merger of Japanese Banks)) also illustrates how inflective a language Finnish is. SearchServer matched several terms in the two relevant documents that the algorithmic stemmer did not such as "Japanilaisen", "Japaniin", "Japanilaiset", "japanilaisia", "japanilaispankin" (a compound) and "pankin", apparently helping it to score much higher.</p><p>Finnish topic 147 ( Öljyonnettomuudet ja linnut (Oil Accidents and Birds)) is a case showing the importance of compounding to Finnish. SearchServer matched terms such as "Onnettomuuksien", "linturyhmä", "öljyonnettomuuksien", "lintuvahinkojen", " Öljykatastrofi", "öljy" and "lintuja" (just to name a few) which appeared to be missed by the algorithmic stemmer (though not all of these were from compound-splitting) and SearchServer scored substantially higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Swedish Stemming</head><p>Swedish topic 188 (Tysk stavningsreform (German Spelling Reform)) shows that when a lexiconbased stemmer does not support compound-splitting for a language with frequent compounds (which is currently the case for SearchServer regarding Swedish), a secondary penalty is that inflections of compounds can be missed. In this topic, SearchServer did not match "stavningsreform" to "stavningsreformen", even though it matches "reform" to "reformen", presumably because the lexicon does not contain most compound words. The algorithmic stemmer did match "stavningsreformen" which apparently is why it scored higher on this topic.</p><p>For Swedish topic 144 (Uppror i Sierra Leone och diamanter (Sierra Leone Rebellion and Diamonds)), it appears the difference in the score was from SearchServer matching "uppror" with "upproret" while the algorithmic stemmer did not. SearchServer's behaviour looks reasonable but it appears it was not helpful in this case just by chance (the top retrieved documents had similar relevance scores and the small shift caused by this difference happened to move down a relevant document). Swedish topic 187 (Kärnavfallstransporter i Tyskland (Nuclear Transport in Germany)) is another case like topic 188. SearchServer did not match "Kärnavfallstransporter", a Swedish compound word, with "kärnavfallstransport" nor "kärnavfallstransporten" (even though SearchServer does match "transporter", "transport" and "transporten" with each other). The algorithmic stemmer handled all of these cases and scored higher on this topic. Swedish topic 179 (NATO:s generalsekreterares avsked (Resignation of NATO Secretary General)) is a case in which the opposite happened. SearchServer matched "generalsekreterares" with "generalsekreterare" while the algorithmic stemmer did not. Perhaps this word is handled because even though it looks like a compound, it probably is better not to split it because it has a different meaning as one word than it does if split in two. SearchServer scored higher on this topic.</p><p>Overall, Swedish is the one language (of the nine investigated) in which SearchServer's stemmer scored significantly lower than the algorithmic stemmer overall. Even though neither stemmer supports compound-splitting for Swedish, it appears for the lexicon-based stemmer this has a secondary penalty of causing inflections of some compounds to be missed. Adding compoundsplitting support for Swedish would both overcome this issue and also allow terms to be found when they are parts of compounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Russian Stemming</head><p>For Russian, it appears the algorithmic stemmer tends to match more terms than SearchServer's stemmer, which might be a derivational vs. inflectional difference again, but we haven't investigated in detail yet. For Russian topics 187 and 177, SearchServer's stemmer scored higher, apparently from matching fewer forms of the Russian words for "nuclear" and "milk" respectively, which helped precision. The algorithmic stemmer scored higher on topic 148, apparently from matching more variations of the Russian word for "ozone". Overall, the differences did not pass a significance test.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,148.82,117.28,305.37,137.19"><head>Table 1 :</head><label>1</label><figDesc>Sizes of CLEF 2003 Document Sets</figDesc><table coords="2,148.82,130.03,305.37,124.43"><row><cell>Language</cell><cell>Text Size (uncompressed)</cell><cell>Number of Documents</cell></row><row><cell>Spanish</cell><cell>1,158,177,739 bytes (1105 MB)</cell><cell>454,045</cell></row><row><cell>German</cell><cell>704,523,506 bytes (672 MB)</cell><cell>294,809</cell></row><row><cell>Dutch</cell><cell>558,560,087 bytes (533 MB)</cell><cell>190,604</cell></row><row><cell>English</cell><cell>601,737,745 bytes (574 MB)</cell><cell>169,477</cell></row><row><cell>Italian</cell><cell>378,831,019 bytes (361 MB)</cell><cell>157,558</cell></row><row><cell>Swedish</cell><cell>374,371,465 bytes (357 MB)</cell><cell>142,819</cell></row><row><cell>French</cell><cell>344,961,357 bytes (329 MB)</cell><cell>129,806</cell></row><row><cell>Finnish</cell><cell>143,902,109 bytes (137 MB)</cell><cell>55,344</cell></row><row><cell>Russian</cell><cell>68,802,653 bytes (66 MB)</cell><cell>16,716</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,112.98,117.28,377.03,139.12"><head>Table 2 :</head><label>2</label><figDesc>Lexical vs. Algorithmic Stemming for Average Precision, Title-only queries</figDesc><table coords="6,112.98,131.96,377.03,124.43"><row><cell cols="2">Experiment AvgDiff</cell><cell>95% Confidence</cell><cell>vs.</cell><cell>2 Largest Diffs (Topic)</cell></row><row><cell>FI-stem-t</cell><cell>0.131</cell><cell>( 0.032, 0.231)</cell><cell>28-14-3</cell><cell>-0.998 (185), 0.929 (196)</cell></row><row><cell>DE-stem-t</cell><cell>0.104</cell><cell>( 0.054, 0.159)</cell><cell>39-13-4</cell><cell>0.833 (174), 0.596 (158)</cell></row><row><cell>NL-stem-t</cell><cell>0.035</cell><cell>(-0.009, 0.082)</cell><cell>28-20-8</cell><cell>0.635 (174), 0.494 (165)</cell></row><row><cell>RU-stem-t</cell><cell>0.018</cell><cell>(-0.046, 0.098)</cell><cell>10-8-10</cell><cell>0.800 (187), 0.338 (177)</cell></row><row><cell>ES-stem-t</cell><cell>0.005</cell><cell>(-0.008, 0.017)</cell><cell>29-14-14</cell><cell>-0.183 (186), 0.170 (151)</cell></row><row><cell>FR-stem-t</cell><cell>-0.004</cell><cell>(-0.027, 0.017)</cell><cell>18-14-20</cell><cell>-0.359 (145), 0.254 (177)</cell></row><row><cell>EN-stem-t</cell><cell>-0.005</cell><cell>(-0.025, 0.019)</cell><cell>13-23-18</cell><cell>0.469 (180), -0.225 (179)</cell></row><row><cell>IT-stem-t</cell><cell>-0.028</cell><cell>(-0.078, 0.006)</cell><cell>22-18-11</cell><cell>-1.000 (161), -0.287 (157)</cell></row><row><cell>SV-stem-t</cell><cell>-0.030</cell><cell>(-0.060,-0.005)</cell><cell>14-24-15</cell><cell>-0.500 (188), -0.333 (144)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,105.94,136.66,391.14,378.22"><head>Table 3 :</head><label>3</label><figDesc>Precision with Lexical, Algorithmic and No Stemming, Title-only queries</figDesc><table coords="9,105.94,151.35,391.14,363.54"><row><cell>Run</cell><cell>AvgP</cell><cell>P@5</cell><cell>P@10</cell><cell>P@20</cell><cell>Rec0</cell><cell>Rec30</cell><cell>P@R</cell><cell>Topics</cell></row><row><cell>FI-lex-t</cell><cell>0.553</cell><cell>47.6%</cell><cell>35.3%</cell><cell>26.0%</cell><cell>0.762</cell><cell>0.682</cell><cell>52.5%</cell><cell>45</cell></row><row><cell>FI-alg-t</cell><cell>0.422</cell><cell>37.8%</cell><cell>27.8%</cell><cell>21.0%</cell><cell>0.682</cell><cell>0.539</cell><cell>40.9%</cell><cell>45</cell></row><row><cell>FI-none-t</cell><cell>0.301</cell><cell>30.2%</cell><cell>23.8%</cell><cell>17.8%</cell><cell>0.555</cell><cell>0.398</cell><cell>29.1%</cell><cell>45</cell></row><row><cell>DE-lex-t</cell><cell>0.424</cell><cell>59.6%</cell><cell>51.1%</cell><cell>40.8%</cell><cell>0.780</cell><cell>0.557</cell><cell>42.5%</cell><cell>56</cell></row><row><cell>DE-alg-t</cell><cell>0.319</cell><cell>47.5%</cell><cell>40.2%</cell><cell>31.5%</cell><cell>0.666</cell><cell>0.402</cell><cell>32.9%</cell><cell>56</cell></row><row><cell>DE-none-t</cell><cell>0.267</cell><cell>44.6%</cell><cell>35.7%</cell><cell>27.8%</cell><cell>0.635</cell><cell>0.333</cell><cell>28.6%</cell><cell>56</cell></row><row><cell>RU-lex-t</cell><cell>0.315</cell><cell>25.7%</cell><cell>17.5%</cell><cell>11.1%</cell><cell>0.572</cell><cell>0.449</cell><cell>29.4%</cell><cell>28</cell></row><row><cell>RU-alg-t</cell><cell>0.297</cell><cell>28.6%</cell><cell>20.7%</cell><cell>13.2%</cell><cell>0.510</cell><cell>0.420</cell><cell>26.0%</cell><cell>28</cell></row><row><cell>RU-none-t</cell><cell>0.254</cell><cell>25.0%</cell><cell>17.5%</cell><cell>10.4%</cell><cell>0.493</cell><cell>0.389</cell><cell>23.1%</cell><cell>28</cell></row><row><cell>SV-lex-t</cell><cell>0.338</cell><cell>35.5%</cell><cell>26.0%</cell><cell>19.2%</cell><cell>0.665</cell><cell>0.439</cell><cell>32.6%</cell><cell>53</cell></row><row><cell>SV-alg-t</cell><cell>0.368</cell><cell>35.5%</cell><cell>27.4%</cell><cell>20.2%</cell><cell>0.706</cell><cell>0.487</cell><cell>36.5%</cell><cell>53</cell></row><row><cell>SV-none-t</cell><cell>0.286</cell><cell>31.3%</cell><cell>23.2%</cell><cell>17.3%</cell><cell>0.593</cell><cell>0.352</cell><cell>28.2%</cell><cell>53</cell></row><row><cell>NL-lex-t</cell><cell>0.422</cell><cell>45.4%</cell><cell>38.0%</cell><cell>32.1%</cell><cell>0.671</cell><cell>0.514</cell><cell>40.3%</cell><cell>56</cell></row><row><cell>NL-alg-t</cell><cell>0.388</cell><cell>44.6%</cell><cell>34.8%</cell><cell>29.0%</cell><cell>0.652</cell><cell>0.505</cell><cell>37.6%</cell><cell>56</cell></row><row><cell>NL-none-t</cell><cell>0.372</cell><cell>42.9%</cell><cell>33.9%</cell><cell>28.1%</cell><cell>0.649</cell><cell>0.487</cell><cell>37.1%</cell><cell>56</cell></row><row><cell>FR-lex-t</cell><cell>0.447</cell><cell>40.4%</cell><cell>31.5%</cell><cell>24.5%</cell><cell>0.689</cell><cell>0.549</cell><cell>41.5%</cell><cell>52</cell></row><row><cell>FR-alg-t</cell><cell>0.451</cell><cell>40.4%</cell><cell>31.7%</cell><cell>25.0%</cell><cell>0.672</cell><cell>0.559</cell><cell>41.1%</cell><cell>52</cell></row><row><cell>FR-none-t</cell><cell>0.413</cell><cell>38.1%</cell><cell>29.2%</cell><cell>23.3%</cell><cell>0.671</cell><cell>0.518</cell><cell>38.7%</cell><cell>52</cell></row><row><cell>ES-lex-t</cell><cell>0.405</cell><cell>51.9%</cell><cell>44.0%</cell><cell>36.1%</cell><cell>0.803</cell><cell>0.535</cell><cell>40.1%</cell><cell>57</cell></row><row><cell>ES-alg-t</cell><cell>0.400</cell><cell>50.9%</cell><cell>43.2%</cell><cell>35.9%</cell><cell>0.783</cell><cell>0.521</cell><cell>39.6%</cell><cell>57</cell></row><row><cell>ES-none-t</cell><cell>0.374</cell><cell>46.7%</cell><cell>42.6%</cell><cell>34.4%</cell><cell>0.762</cell><cell>0.494</cell><cell>37.4%</cell><cell>57</cell></row><row><cell>IT-lex-t</cell><cell>0.394</cell><cell>40.4%</cell><cell>30.2%</cell><cell>21.9%</cell><cell>0.683</cell><cell>0.487</cell><cell>36.2%</cell><cell>51</cell></row><row><cell>IT-alg-t</cell><cell>0.422</cell><cell>41.2%</cell><cell>30.8%</cell><cell>22.4%</cell><cell>0.727</cell><cell>0.526</cell><cell>40.0%</cell><cell>51</cell></row><row><cell>IT-none-t</cell><cell>0.367</cell><cell>35.7%</cell><cell>25.9%</cell><cell>19.7%</cell><cell>0.649</cell><cell>0.445</cell><cell>34.1%</cell><cell>51</cell></row><row><cell>EN-lex-t</cell><cell>0.448</cell><cell>38.5%</cell><cell>34.4%</cell><cell>27.8%</cell><cell>0.676</cell><cell>0.550</cell><cell>43.4%</cell><cell>54</cell></row><row><cell>EN-alg-t</cell><cell>0.453</cell><cell>38.5%</cell><cell>34.3%</cell><cell>27.2%</cell><cell>0.678</cell><cell>0.547</cell><cell>43.2%</cell><cell>54</cell></row><row><cell>EN-none-t</cell><cell>0.435</cell><cell>40.0%</cell><cell>32.4%</cell><cell>27.1%</cell><cell>0.676</cell><cell>0.542</cell><cell>42.8%</cell><cell>54</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,118.45,581.99,366.10,139.13"><head>Table 4 :</head><label>4</label><figDesc>Impact of Lexical Stemming on Average Precision, Title-only queries</figDesc><table coords="9,118.45,596.68,366.10,124.43"><row><cell cols="2">Experiment AvgDiff</cell><cell>95% Confidence</cell><cell>vs.</cell><cell>2 Largest Diffs (Topic)</cell></row><row><cell>FI-lex-t</cell><cell>0.252</cell><cell>( 0.149, 0.360)</cell><cell>32-11-2</cell><cell>1.000 (147), 0.999 (187)</cell></row><row><cell>DE-lex-t</cell><cell>0.157</cell><cell>( 0.103, 0.213)</cell><cell>43-10-3</cell><cell>0.843 (174), 0.627 (192)</cell></row><row><cell>RU-lex-t</cell><cell>0.062</cell><cell>( 0.002, 0.146)</cell><cell>15-7-6</cell><cell>0.978 (187), 0.223 (143)</cell></row><row><cell>SV-lex-t</cell><cell>0.051</cell><cell>( 0.023, 0.085)</cell><cell>23-15-15</cell><cell>0.507 (195), 0.479 (192)</cell></row><row><cell>NL-lex-t</cell><cell>0.050</cell><cell>( 0.001, 0.102)</cell><cell>30-19-7</cell><cell>0.709 (174), 0.487 (188)</cell></row><row><cell>FR-lex-t</cell><cell>0.034</cell><cell>(-0.023, 0.091)</cell><cell>25-18-9</cell><cell>0.923 (175), -0.875 (141)</cell></row><row><cell>ES-lex-t</cell><cell>0.031</cell><cell>( 0.011, 0.052)</cell><cell>33-19-5</cell><cell>0.240 (164), 0.228 (181)</cell></row><row><cell>IT-lex-t</cell><cell>0.027</cell><cell>( 0.006, 0.050)</cell><cell>24-18-9</cell><cell>0.317 (171), 0.202 (200)</cell></row><row><cell>EN-lex-t</cell><cell>0.013</cell><cell>(-0.007, 0.038)</cell><cell>23-21-10</cell><cell>0.417 (144), 0.262 (158)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,725.62,407.77,9.50;1,90.00,735.09,423.10,9.49;1,90.00,745.67,114.25,8.37"><p>Fulcrum r is a registered trademark, and SearchServer TM , SearchSQL TM , Intuitive Searching TM and Ful/Text TM are trademarks of Hummingbird Ltd. All other copyrights, trademarks and tradenames are the property of their respective owners.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,105.24,735.26,407.88,8.37;4,90.00,744.72,318.30,8.37"><p>See last year's paper<ref type="bibr" coords="4,184.59,735.26,8.94,8.37" target="#b7">[8]</ref> for some comparisons of confidence intervals from the bootstrap percentile, Wilcoxon signed rank and standard error methods for both average precision and Precision@10.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,105.49,402.69,332.21,10.46" xml:id="b0">
	<monogr>
		<ptr target="http://www.clef-campaign.org/" />
		<title level="m" coord="10,105.49,402.69,188.59,10.46">Cross-Language Evaluation Forum web site</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,422.61,407.52,10.46;10,100.52,434.57,47.19,10.46" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,283.38,422.61,143.63,10.46">An Introduction to the Bootstrap</title>
		<author>
			<persName coords=""><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,454.49,407.52,10.46;10,100.52,466.44,310.45,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,188.11,454.49,223.00,10.46">Converting the Fulcrum Search Engine to Unicode</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Hodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,433.11,454.49,79.91,10.46;10,100.52,466.44,113.03,10.46">Sixteenth International Unicode Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-03">March 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,486.37,32.79,10.46;10,156.14,486.37,58.68,10.46;10,232.66,486.37,18.60,10.46;10,269.13,486.37,43.73,10.46;10,330.71,486.37,11.93,10.46;10,360.51,486.37,10.93,10.46;10,389.29,486.37,39.13,10.46;10,446.29,486.37,25.18,10.46;10,489.34,486.37,23.67,10.46;10,100.52,498.32,213.76,10.46" xml:id="b3">
	<monogr>
		<ptr target="http://research.nii.ac.jp/∼ntcadm/index-en.html" />
		<title level="m" coord="10,105.49,486.37,32.79,10.46;10,156.14,486.37,58.68,10.46;10,232.66,486.37,18.60,10.46;10,269.13,486.37,43.73,10.46;10,330.71,486.37,11.93,10.46;10,360.51,486.37,10.93,10.46;10,389.29,486.37,39.13,10.46;10,446.29,486.37,25.18,10.46;10,489.34,486.37,18.94,10.46">NTCIR (NII-NACSIS Test Collection for IR Systems) Home Page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,518.25,116.67,10.46;10,241.61,518.25,271.40,10.46;10,100.52,530.20,231.09,10.46" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,180.37,518.25,41.80,10.46;10,241.61,518.25,187.42,10.46">Snowball: A language for stemming algorithms</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://snowball.tartarus.org/texts/introduction.html" />
		<imprint>
			<date type="published" when="2001-10">October 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,550.13,407.51,10.46;10,100.52,562.09,412.49,10.46;10,100.52,574.04,412.48,10.46;10,100.52,586.00,227.72,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,192.16,562.09,87.01,10.46">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec3/t3proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="10,431.93,562.09,81.07,10.46;10,100.52,574.04,221.57,10.46">Overview of the Third Text REtrieval Conference (TREC-3)</title>
		<title level="s" coord="10,338.65,574.04,65.81,10.46">NIST Special</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="500" to="226" />
		</imprint>
		<respStmt>
			<orgName>City University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,605.92,303.88,10.46" xml:id="b6">
	<monogr>
		<ptr target="http://trec.nist.gov/" />
		<title level="m" coord="10,105.49,605.92,206.35,10.46">Text REtrieval Conference (TREC) Home Page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,625.85,394.17,10.46;10,499.65,624.77,12.85,7.32;10,100.52,637.80,412.50,10.46;10,100.52,649.76,239.22,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,192.76,625.85,306.90,10.46;10,499.65,624.77,12.85,7.32;10,100.52,637.80,41.83,10.46">Experiments in 8 European Languages with Hummingbird SearchServer TM at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<ptr target="http://clef.iei.pi.cnr.it:2002/workshop2002/WN/26.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,295.17,637.80,212.70,10.46">Working Notes for the CLEF 2002 Workshop</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,669.68,213.42,10.46;10,318.91,668.61,12.85,7.32;10,337.01,669.68,175.98,10.46;10,100.52,681.63,412.49,10.46;10,100.52,693.60,391.42,10.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,197.80,669.68,121.11,10.46;10,318.91,668.61,12.85,7.32;10,337.01,669.68,42.11,10.46">Hummingbird SearchServer TM at TREC</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec10/t10proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="10,207.93,681.63,273.83,10.46">Proceedings of the Tenth Text REtrieval Conference (TREC</title>
		<title level="s" coord="10,100.52,693.60,111.11,10.46">NIST Special Publication</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Tenth Text REtrieval Conference (TREC</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="500" to="250" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
