<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,169.84,115.06,272.04,13.64;1,106.48,130.18,398.85,13.64">Multilingual experiments of UTA at CLEF 2003 The impact of different merging strategies and word normalizing tools</title>
				<funder ref="#_taDGMdS">
					<orgName type="full">CLARITY</orgName>
				</funder>
				<funder ref="#_QC5c8sY">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,196.12,166.02,36.07,8.42"><forename type="first">Eija</forename><surname>Airio</surname></persName>
							<email>eija.airio@uta.fi</email>
						</author>
						<author>
							<persName coords="1,238.36,166.02,67.94,8.42"><forename type="first">Heikki</forename><surname>Keskustalo</surname></persName>
							<email>heikki.keskustalo@uta.fi</email>
						</author>
						<author>
							<persName coords="1,312.76,166.02,53.90,8.42"><forename type="first">Turid</forename><surname>Hedlund</surname></persName>
							<email>turid.hedlund@shh.fi</email>
						</author>
						<author>
							<persName coords="1,373.48,166.02,42.20,8.42"><forename type="first">Ari</forename><surname>Pirkola</surname></persName>
							<email>pirkola@tukki.jyu.fi</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Tampere</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Studies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,169.84,115.06,272.04,13.64;1,106.48,130.18,398.85,13.64">Multilingual experiments of UTA at CLEF 2003 The impact of different merging strategies and word normalizing tools</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6D14B43302E2CF1BAC0A2BA904F1D73B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There are two main translation approaches in a multilingual information retrieval task: either to translate the topics or to translate the datasets. The first one is an easier and more common approach. There are two indexing approaches: either to index the languages in the same index, or to build separate indexes for different languages. If the latter approach is used, retrieved result sets must be merged. Several merging strategies have been developed, but there have been no breakthroughs. University of Tampere used the raw score approach as the baseline in the CLEF 2003 runs and tested two new merging approaches. No remarkable differences were found between the merging methods. The index words can be stored as such (inflected index), or normalized. The most common normalizing method in IR is stemming. The tests of University of Tampere show that stemming is a good normalizing method for English, which is a language with weak morphology. For Finnish stemming gives remarkably worse results, the index built with the morphological analyzer gave better results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A multilingual information retrieval task deals with search requests in one source language and documents in several target languages. Two main approaches are possible. (a) The target documents are translated into the source language, or (b) the search requests are translated into the document languages. The first approach would be comfortable for the user. It is quite an unrealistic approach, however, because translating thousands of documents is an expensive and resource-demanding task. In addition, translation should be done for each possible source and target language separately. The second approach, translating only the search requests, is more realistic.</p><p>There are two approaches to index a multilingual document collection: (a) to build separate indexes for each document language, or (b) to build a common multilingual index. If the first approach is followed, then retrieval must be performed from each index separately, and the result lists have to be merged somehow. If a common index is created, there are two approaches: either (a) to perform retrieval by each target language separately, and then merge result lists, or (b) merge the search requests into a single query, and perform retrieval.</p><p>In CLEF 2003, University of Tampere (UTA) utilized the UTACLIR <ref type="bibr" coords="1,367.60,570.30,85.08,8.42">(Hedlund &amp; al. 2002)</ref> system for topic translation. The approach of separate indexes was followed, and three different merging strategies were tested. The impact of two different word normalizing methods on multilingual information retrieval was investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Word normalization methods</head><p>The area of linguistics concerned with the internal structure of words is called morphology. Inflectional morphology describes impacts of language syntax on words, for example plural of nouns, or tempus of verbs. Derivational morphology goes beyond the syntax: it may affect word meaning as well. The impact of morphology on information retrieval is language dependent. English, for example, has quite weak morphology, and word inflection does not have a great impact on IR. On the other hand, there are languages with strong morphology (e.g. Hungarian, Hebrew and Finnish), which may have hundreds or thousands of word form variants. The impact of word inflection on IR is considerable in these cases. <ref type="bibr" coords="2,381.28,79.64,79.08,8.42">(Krovetz 1993, 191.)</ref> There are two main approaches to handle inflection: (a) to normalize index words, or (b) to leave index words inflected and let users handle the problem. The search engines of Internet have mostly pushed the responsibility onto their users, which is understandable because of huge amounts and large diversity of data. Users of Internet search engines are guided either to use truncation (for example Alta Vista, http://www.altavista.com/) or to supply all requested word forms (for example Google, http://www.google.com/).</p><p>There are two kinds of tools for normalizing words: stemmers and morphological analyzers. The purpose of stemmers is to reduce morphological variance of words. There are several stemming techniques. The simplest stemming algorithms only remove word plural endings, while more developed ones handle a variety of suffixes in several steps. <ref type="bibr" coords="2,155.20,198.44,79.08,8.42">(Krovetz 1993, 191.)</ref> Stemming is a normalizing approach compatible with languages with weak morphology, because their inflection rules are easy to apply in a stemming algorithm. Stemming may not be the best normalizing method for languages with strong morphology, because it is not possible to create any simple rules for them. Morphological analyzers are more sophisticated normalizing tools. They incorporate full description of inflectional morphology of the language and a large lexicon of basic core vocabulary items. <ref type="bibr" coords="2,92.80,252.56,72.84,8.42">(Karlsson 1995, 1.)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Merging methods</head><p>There are many ways to merge the result lists. One of the simplest is the Round Robin approach, which bases on the idea that document scores are not comparable across the collections. Because one is ignorant about the distribution of relevant documents in the retrieved lists, an equal number of documents is taken from the beginning of every result list. <ref type="bibr" coords="2,205.84,341.24,104.64,8.42">(Hiemstra &amp; al. 2001, 108.)</ref> The Raw Score approach is based on the assumption that document scores are comparable across collections <ref type="bibr" coords="2,92.80,373.52,107.95,8.42">(Hiemstra &amp; al. 2001, 108)</ref>. The lists are sorted directly according to the document scores. The raw score approach has turned out to be one of the best basic methods <ref type="bibr" coords="2,320.20,384.44,46.59,8.42">(Chen 2002a</ref><ref type="bibr" coords="2,366.79,384.44,85.17,8.42" target="#b7">, Moulinier &amp; al. 2002</ref><ref type="bibr" coords="2,451.96,384.44,52.27,8.42" target="#b9">, Savoy 2001)</ref>.</p><p>Different methods for normalizing the scores have been developed. A typical Normalized Score approach is to divide the score by the maximum score of the collection. Some other balancing factor can be utilized as well.</p><p>Several more sophisticated approaches have been developed, but there have not been any breakthroughs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The UTACLIR process</head><p>The UTACLIR translation process we used in CLEF 2003 was the same that we utilized in our CLEF 2002 runs. The user gives the source and target language codes and the search request as input to UTACLIR. The system calls external resources (bilingual dictionaries, morphological analyzers, stemmers, gramming functions and stop lists) according to the language codes. UTACLIR processes source words as follows:</p><p>1) the word is normalized utilizing a morphological analyzer 2) the source stop words are removed 3) the normalized word is translated 4) if the word is translatable, the translated words are normalized (by a morphological analyzer or a stemmer, depending on the target language code) 5) the target stop words are removed (in the case that a morphological analyzer was applied in phase 4) 6) if the word is untranslatable in phase 4, two highest ranked words obtained in n-gram-matching are selected as query words.</p><p>The system assumes that there is a morphological analyzer for the source language, because source words in the translation dictionaries are in their base forms. Target language words can be normalized either by a morphological analyzer or a stemmer, depending on the target index. The target language code expresses the target normalizing type, as well as the target language. Target stop word removal is done only when a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Runs and results</head><p>In this section, we first describe the language resources used, then the collections, and the merging strategies adapted. Finally, we report results of our CLEF 2003 multilingual runs and the additional runs we performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language resources</head><p>We used the following language resources in the tests:</p><p>• Motcom GlobalDix multilingual translation dictionary (18 languages, total number of words 665 000) by Kielikone plc. Finland </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test collections and indexes</head><p>The test collections of "Large-multilingual" (Multilingual-8) track of CLEF 2003 were used for the tests.</p><p>Eleven indexes were built for the test. For English, Finnish and Swedish we built two indexes: one utilizing a stemmer, and one utilizing a morphological analyzer. For Dutch, French, German, Italian and Spanish we built a stemmed index for each.</p><p>The InQuery system, provided by the Center for Intelligent Information Retrieval at the University of Massachusetts, was utilized in indexing the databases and as a test retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Merging methods applied</head><p>We made tests with our CLEF 2002 result lists in order to select good, but simple merging strategies for CLEF 2003. We used raw score method as the baseline. On the basis of our tests, two more approaches were selected: the dataset size based method and the score difference based method.</p><p>The dataset size based method is based on the fact that it is likely that more relevant documents are found in a large dataset than in a small dataset. The number of document items taken from single result sets was calculated as follows: T * n / N, where T is the number of document items per topic in the single result list (in CLEF 2003 it was 1000), n is the dataset size and N is the total number of documents (the sum of documents in all the collections). 185 German, 81 French, 99 Italian, 106 English, 285 Spanish, 120 Dutch, 35 Finnish and 89 Swedish documents were selected for every topic in CLEF 2003 runs.</p><p>In score difference based method every score is compared with the best score of the topic. Only documents with the difference of scores under the predefined value are taken to the final list. This bases on the assumption that documents whose scores are much lower than the score of the top document, may not be relevant.</p><p>UTA did five multilingual runs in CLEF 2003. In the first three runs retrieval was performed from eight separate indexes: the morphologically analyzed English, Swedish and Finnish indexes, and the stemmed French, German, Dutch, Italian and Spanish indexes. The last two runs were merged from runs with eight stemmed indexes (English, Swedish, Finnish, French, German, Dutch, Italian and Spanish). The results were merged utilizing raw score, dataset size based and score difference per topic based methods. The difference value in utamul3 was 0.07, and in utamul5 0.08. These values were selected on the basis of test with CLEF 2002 lists.</p><p>There are no remarkable differences between the results of our different multilingual runs (Table <ref type="table" coords="4,472.72,166.04,3.42,8.42" target="#tab_1">1</ref>). The best results, 18.6 % average precision, were achieved by two runs, utamul1 and utamul4, applying approaches totally different from each other. In the first one we used morphologically analyzed / stemmed indexes, and applied raw score merging strategy. The latter was performed with stemmed indexes, and dataset size based merging strategy was applied. We made three additional multilingual runs to clarify the impact of different indexing and merging strategies on the result (Table <ref type="table" coords="4,158.92,408.08,3.42,8.42" target="#tab_2">2</ref>). In the first one we applied the raw score merging strategy for stemmed indexes. In the run utamul1 we applied the raw score strategy for morphologically analyzed / stemmed indexes, and achieved the best result of our runs. The average precision of our first additional run was 18.3 %, which is worse than the result of utamul1, but does not differ much from the results of our official runs.</p><p>In the second and third of our additional runs we applied the round robin merging approach. In the second run we used morphologically analyzed / stemmed indexes, and in the third one stemmed indexes. The result of both these runs were 18.4 % -again a result, which is very near our official results. We can order the merging strategies applied in our official and additional runs according to the result they give with the morphologically analyzed / stemmed and stemmed indexes. With morphologically analyzed / stemmed indexes the raw score method gives the best result, 18.6 % average precision. The second is the round robin strategy, which gives 18.4 % average precision. The third is the dataset size based method (18.2 %), and the last the score difference per topic method (18.2 %). The order is different with stemmed indexes. The dataset size based method is the best with 18.6 % average precision, and the score difference per topic is the second (18.5 %). The third is round robin strategy (18.4), and the last is raw score (18.3 %). It is possible that the differences between the results of morphologically analyzed / stemmed and stemmed indexes are caused by change, because they are very small. More testing should be done to clarify whether the effectiveness of a merging strategy depends on the index type.</p><p>The results of our official and additional multilingual runs do not show the performance difference of morphologically analyzed / stemmed vs. stemmed indexes. To test the impact of the indexing method on the retrieval result we made six additional runs: two monolingual English and four bilingual runs (two English -Swedish and two English -Finnish) (Table <ref type="table" coords="5,265.72,133.64,3.42,8.42" target="#tab_3">3</ref>). The average precision of monolingual English run was 1.5 % better with the stemmed index than with the index built by means of a morphological analyzer. The results of bilingual English -Finnish runs are opposite: the stemmed index performed badly. The average precision with the stemmed index was 44.1 % lower than with the index built by means of the morphological analyzer. The results of English -Swedish runs show similar trends, although the difference (-29.9 %) is smaller than in English -Finnish runs but still significant. These results confirm the assumption that stemming is a competitive normalizing approach for languages with weak morphology, but not for languages with strong morphology. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and conclusion</head><p>The impact of different normalizing methods, stemming and morphological analyzing, on the IR performance has not been investigated widely. The reason for that is presumably the fact that English is the traditional indexing language in IR tests. English is a language with weak morphology, which implies that stemming is an adequate word form normalization method. Our monolingual English tests with CLEF 2003 data support this, the result with the stemmed English index is even a little better than the result with the normalized index. The bilingual test we made with Finnish and Swedish indexes show opposite results. The results with stemmed indexes in these languages are much worse than the results with the index built utilizing a morphological analyzer. When high precision is demanded, stemming is not an adequate normalizing method with languages with strong morphology.</p><p>On the other hand, the most used IR systems in the real life are the search engines of Internet. They use inflected indexes, which means that the users have to handle inflection. Truncation is possible with some search engines, while others guide their users to supply all the possible forms of their search words. Loss of recall is liable, but recall may not be important in www searching. In many cases more important is precision, which may be good even if the user has not perfect language skills.</p><p>In most multilingual experiments separate indexes are created for different languages, and various result merging strategies are tested. The results of test made with a merged index are not very promising <ref type="bibr" coords="5,440.32,615.68,47.67,8.42">(Chen 2002b</ref><ref type="bibr" coords="5,487.99,615.68,31.13,8.42;5,92.80,626.48,34.03,8.42">, Nie &amp; Jin 2002)</ref>. In the real life, the situation of separate indexes and result merging, occurs quite rarely, however. This would be a reason to direct the research towards the strategies of the merged index approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,109.72,231.27,389.16,92.60"><head></head><label></label><figDesc>• Morphological analyzers FINTWOL, SWETWOL and ENGTWOL by Lingsoft plc. Finland • Stemmers for Spanish and French, by ZPrise • A stemmer for Italian, by the University of Neuchatel • A stemmer for Dutch, by the University of Utrecht • Stemmers for English, German, Finnish and Swedish, SNOWBALL stemmers by Dr Martin Porter • English stop word list, created on the basis of InQuery's default stop list for English • Finnish stop word list, created on the basis of the English stop list • Swedish stop word list, created at University of Tampere</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,92.80,230.64,413.40,141.23"><head>Table 1 .</head><label>1</label><figDesc>Average precision of official multilingual runs of UTA in CLEF 2003</figDesc><table coords="4,92.80,254.04,413.40,117.83"><row><cell></cell><cell>Index type</cell><cell>Merging strategy</cell><cell>Average precision %</cell><cell>Difference %</cell></row><row><cell>utamul1</cell><cell>morphologically</cell><cell>raw score</cell><cell>18.6</cell><cell></cell></row><row><cell></cell><cell>analyzed / stemmed</cell><cell></cell><cell></cell><cell></cell></row><row><cell>utamul2</cell><cell>morphologically</cell><cell>dataset size based</cell><cell>18.3</cell><cell>-1.6</cell></row><row><cell></cell><cell>analyzed/ stemmed</cell><cell></cell><cell></cell><cell></cell></row><row><cell>utamul3</cell><cell>morphologically</cell><cell>score difference per</cell><cell>18.2</cell><cell>-2.1</cell></row><row><cell></cell><cell>analyzed / stemmed</cell><cell>topic</cell><cell></cell><cell></cell></row><row><cell>utamul4</cell><cell>stemmed</cell><cell>dataset size based</cell><cell>18.6</cell><cell>0.0</cell></row><row><cell>utamul5</cell><cell>stemmed</cell><cell>Score difference per</cell><cell>18.5</cell><cell>-0.5</cell></row><row><cell></cell><cell></cell><cell>topic</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,92.80,516.00,354.96,65.39"><head>Table 2 .</head><label>2</label><figDesc>Average precision of additional multilingual runs</figDesc><table coords="4,92.80,539.28,354.96,42.11"><row><cell>Index type</cell><cell>Merging strategy</cell><cell>Average precision %</cell></row><row><cell>stemmed</cell><cell>raw score</cell><cell>18.3</cell></row><row><cell>morphologically analyzed / stemmed</cell><cell>round robin</cell><cell>18.4</cell></row><row><cell>stemmed</cell><cell>round robin</cell><cell>18.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,92.80,230.64,425.40,141.23"><head>Table 3 .</head><label>3</label><figDesc>Average precision of monolingual English, bilingual English -Swedish and bilingual English -Finnish runs using alternative indexes</figDesc><table coords="5,92.80,264.84,408.72,107.03"><row><cell>Language</cell><cell>Index type</cell><cell cols="3">Average precision % Difference % units Difference %</cell></row><row><cell>English</cell><cell>morphologically</cell><cell>45.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell>analyzed</cell><cell></cell><cell></cell><cell></cell></row><row><cell>English</cell><cell>stemmed</cell><cell>46.3</cell><cell>+0.7</cell><cell>+1.5</cell></row><row><cell>Finnish</cell><cell>morphologically</cell><cell>34.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell>analyzed</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Finnish</cell><cell>stemmed</cell><cell>19.0</cell><cell>-15.0</cell><cell>-44.1</cell></row><row><cell>Swedish</cell><cell>morphologically</cell><cell>27.1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>analyzed</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Swedish</cell><cell>stemmed</cell><cell>19.0</cell><cell>-8.1</cell><cell>-29.9</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The InQuery search engine was provided by the <rs type="institution">Center for Intelligent Information Retrieval at the University of Massachusetts</rs>.</p></div>
<div><head>ENGTWOL (Morphological</head><p><rs type="projectName">Transducer Lexicon Description</rs> of English): Copyright (c) 1989-1992 <rs type="person">Atro Voutilainen</rs> and <rs type="person">Juha Heikkilä</rs>. FINTWOL (Morphological Description of Finnish): Copyright (c) <rs type="person">Kimmo Koskenniemi</rs> and Lingsoft plc. 1983-1993. GERTWOL (Morphological Transducer Lexicon Description of German): Copyright (c) 1997 Kimmo Koskenniemi and Lingsoft plc. TWOL-R (Run-time Two-Level Program): Copyright (c) <rs type="person">Kimmo Koskenniemi</rs> and Lingsoft plc. 1983-1992. GlobalDix Dictionary Software was used for automatic word-by-word translations. Copyright (c) 1998 Kielikone plc, Finland. MOT Dictionary Software was used for automatic word-by-word translations. Copyright (c) 1998 Kielikone plc, Finland.</p><p>This work was partly financed by <rs type="funder">CLARITY</rs> (<rs type="programName">Information Society Technologies Programme</rs>, <rs type="grantNumber">IST-2000-25310</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_QC5c8sY">
					<orgName type="project" subtype="full">Transducer Lexicon Description</orgName>
				</org>
				<org type="funding" xml:id="_taDGMdS">
					<idno type="grant-number">IST-2000-25310</idno>
					<orgName type="program" subtype="full">Information Society Technologies Programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,92.80,320.96,381.56,8.42;6,92.80,331.88,370.26,8.92" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,379.12,320.96,95.24,8.42;6,92.80,331.88,101.28,8.42">Cross-language Retrieval Experiments at CLEF 2002</title>
		<author>
			<persName coords=""><forename type="first">Eija</forename><forename type="middle">&amp;</forename><surname>Airio</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Keskustalo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Heikki</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hedlund</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Turid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ari</forename><surname>Pirkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,210.88,332.02,168.39,8.78">Working Notes for the CLEF 2002 Workshop</title>
		<meeting><address><addrLine>Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="5" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.80,353.48,424.08,8.92;6,92.80,364.28,119.70,8.92" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,170.92,353.48,179.80,8.42">Cross-language Retrieval Experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,386.80,353.62,130.08,8.78;6,92.80,364.42,35.89,8.78">Working Notes for the CLEF 2002 Workshop</title>
		<meeting><address><addrLine>Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002. 2002</date>
			<biblScope unit="page" from="5" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.80,385.88,413.36,8.92;6,92.80,396.68,424.14,8.92;6,92.80,407.48,99.30,8.42" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,171.52,385.88,266.71,8.42">Multilingual Information Retrieval Using English and Chinese Queries</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,454.72,386.02,51.44,8.78;6,92.80,396.82,178.09,8.78">Evaluation of Cross-Language Information Retrieval Systems</title>
		<title level="s" coord="6,277.24,396.82,128.37,8.78">Lecture notes in computer science</title>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.80,429.20,421.00,8.42;6,92.80,440.00,405.72,8.92;6,92.80,450.68,426.18,8.92" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,451.24,429.20,62.56,8.42;6,92.80,440.00,227.53,8.42">Utaclir @ CLEF 2001 -Effects of compound splitting and n-gram techniques</title>
		<author>
			<persName coords=""><forename type="first">Turid</forename><forename type="middle">&amp;</forename><surname>Hedlund</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Keskustalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ari</forename><surname>Heikki &amp; Pirkola</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Airio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">&amp;</forename><surname>Eija</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,336.76,440.14,161.76,8.78;6,92.80,450.82,65.17,8.78">Evaluation of Cross-language Information Retrieval Systems</title>
		<title level="s" coord="6,164.44,450.82,132.67,8.78">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="118" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.80,472.40,398.70,8.42;6,92.80,483.20,390.32,8.92;6,92.80,494.00,418.50,8.92;6,92.80,504.92,33.90,8.42" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,408.40,472.40,83.10,8.42;6,92.80,483.20,314.19,8.42">Translation resources, merging strategies, and relevance feedback for cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Djoerd</forename><forename type="middle">&amp;</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renée</forename><surname>Wessel &amp; Pohlmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thijs</forename><surname>Westerveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,422.92,483.34,60.20,8.78;6,92.80,494.14,253.99,8.78">Cross-language information retrieval and evaluation</title>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001. 2069. 2001</date>
			<biblScope unit="page" from="102" to="115" />
		</imprint>
	</monogr>
	<note>Lectures in computer science</note>
</biblStruct>

<biblStruct coords="6,92.80,526.40,425.36,8.92;6,92.80,537.20,92.10,8.92" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,174.16,526.40,259.42,8.42">SWETWOL: A Comprehensive Morphological Analyser for Swedish</title>
		<author>
			<persName coords=""><forename type="first">Fred</forename><surname>Karlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,450.52,526.54,67.64,8.78;6,92.80,537.34,41.68,8.78">Nordic Journal of Linguistics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.80,558.92,395.48,8.92;6,92.80,569.60,416.22,8.92" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,181.84,558.92,170.08,8.42">Viewing Morphology as an Inference Process</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Krovetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,368.32,559.06,119.96,8.78;6,92.80,569.74,355.75,8.78">Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development In Information Retrieval</title>
		<meeting>the 16th Annual International ACM SIGIR Conference on Research and Development In Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.80,591.32,405.52,8.42;6,92.80,602.12,290.82,8.92" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,291.28,591.32,207.04,8.42">Thomson Legal and Regulatory Experiments for CLEF</title>
		<author>
			<persName coords=""><forename type="first">Isabelle</forename><forename type="middle">&amp;</forename><surname>Moulinier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Molina-Salgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,126.64,602.26,168.39,8.78">Working Notes for the CLEF 2002 Workshop</title>
		<meeting><address><addrLine>Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002. 2002</date>
			<biblScope unit="page" from="91" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.80,623.72,404.58,8.92;6,92.80,634.52,396.42,8.92" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,174.28,623.72,213.52,8.42">Towards a unified approach to CLIR and multilingual IR</title>
		<author>
			<persName coords=""><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,404.20,623.86,93.18,8.78;6,92.80,634.66,204.22,8.78">SIGIR 2002 Workshop I, Cross-language information retrieval: a research map</title>
		<meeting><address><addrLine>Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
		<respStmt>
			<orgName>University of Tampere</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.80,656.12,388.56,8.42;6,92.80,666.92,419.76,8.92;6,92.80,677.84,359.34,8.92" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,247.84,656.12,233.52,8.42;6,92.80,666.92,86.64,8.42">Report on the TREC-9 Experiment: Link-Based Retrieval and Distributed Collections</title>
		<author>
			<persName coords=""><forename type="first">Jacques</forename><forename type="middle">&amp;</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yves</forename><surname>Rasolofo</surname></persName>
		</author>
		<idno>500- 249</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,195.76,667.06,194.14,8.78">Proceedings of the Ninth Text Retrieval Conference</title>
		<title level="s" coord="6,396.40,667.06,96.36,8.78">NIST Special Publication</title>
		<meeting>the Ninth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="579" to="588" />
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
