<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,106.27,146.03,390.44,18.08;1,496.74,144.26,5.98,12.55">Cross-language experiments with IR-n system *</title>
				<funder ref="#_pzeAmCF #_BtZTqFW">
					<orgName type="full">Spanish Government (CICYT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,224.87,181.09,70.09,10.46"><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
							<email>llopis@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="laboratory">Grupo de investigación en Procesamiento del Lenguaje y Sistemas de Información</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.66,181.09,60.48,10.46"><forename type="first">Rafael</forename><surname>Muñoz</surname></persName>
							<email>rafael@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="laboratory">Grupo de investigación en Procesamiento del Lenguaje y Sistemas de Información</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,106.27,146.03,390.44,18.08;1,496.74,144.26,5.98,12.55">Cross-language experiments with IR-n system *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3071008A818EDE49E805EC1657CE2B83</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the third participation of IR-n system at CLEF-2003. Two previous participation are focused on Spanish monolingual task. This year, we participated in three different tasks: multilingual task (four languages), bilingual task (Italian-Spanish) and monolingual task (Spanish, German, French, Italian). This paper describes the experiments carried out as training process in order to set up the main system features and shows the results obtained. These results shows that IR-n system obtains good scores for the three tasks improving the average of the CLEF 2003 systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information Retrieval (IR) systems have to find the relevant documents to a user's query from a document collection. We can find different kind of IR system at the literature. On the one hand, if the document collection and the user's question are written in the same language then the IR system is a monolingual system. On the other hand, if the document collection and the user's question are written in different languages then the IR system is a bilingual (two different languages) or multilingual (more than two languages) system. Obviously, the document collection for multilingual system is written in two different languages at least. This paper presents the adaptation of IR-n system <ref type="bibr" coords="1,322.28,513.94,10.52,10.46" target="#b6">[7]</ref> to participate at CLEF'2003. Our system participates in the following tasks:</p><p>• monolingual tasks:</p><p>-Spanish -French -German -Italian</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• bilingual tasks:</head><p>-Italian-Spanish</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• multilingual tasks:</head><p>-Spanish-German-French-Italian IR-n system is an IR system based on passages instead of traditional systems based on full document. Every passage is made up of a fragment or piece of text <ref type="bibr" coords="2,375.37,122.31,10.52,10.46" target="#b0">[1,</ref><ref type="bibr" coords="2,388.30,122.31,7.01,10.46" target="#b5">6]</ref>. Theses systems calculates the document's relevance studying their passage's relevances. IR-n system calculates the similitude between user's query and documents using a set of passages.</p><p>This proposal adds the following advantages:</p><p>• To consider the proximity of appearance of query terms into the documents.</p><p>• To define a new information transmission unit, more adequate both users than further treatment.</p><p>• To avoid normalization problems of documents.</p><p>2 IR-n: a information retrieval system based on passages</p><p>This section presents the conceptual modelling of IR-n. The following main features are presented in the next subsections:</p><p>1. Passage concept.</p><p>2. Similarity measure between the user's question and the documents collection 3. Similarity measure between the user's question and the documents collection based on similarity passages 4. The use of query expansion in the IR-n system</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Passage concept</head><p>First Passage Retrieval systems (PR) used the paragraph as passage size. The use of paragraph as passage unit caused the built of an heterogeneous collection of passages due to the different size of paragraphs. Moreover, this segmentation do not guarantee the fact that every passage is related to different subjects. For this reason, further proposals of PR systems used more than one paragraph as passage.</p><p>A different trend proposes the use of a number of words as passage <ref type="bibr" coords="2,410.80,498.32,10.52,10.46" target="#b5">[6,</ref><ref type="bibr" coords="2,425.40,498.32,7.01,10.46" target="#b0">1]</ref>. These proposals solve the heterogenous size problem of previous PR systems. Moreover, this kind of system can to adapte easily the number of words to the document collection and the user's query. This flexibility is very important to increase the performance of systems <ref type="bibr" coords="2,390.53,534.19,9.96,10.46" target="#b5">[6]</ref>. However, these systems lose the syntactic structure. In fact, a passage made up of 100 words can including one or two incomplete sentences.</p><p>Between the word and the paragraph exists an unit with structure that it is the sentence. The main aspect of the sentence is the self-contained of meaning. This aspect is very important in Information Retrieval because the answer of system can be understood by the user. Obviously, an only sentence does not have an enough identity to determine if a document that contains it is relevant in relation to certain topic. Although it establishes some limits and helps to value the fact that terms of the user's query appear in the same sentence. Since the sentence does not have an entity the sufficiently complete to define a passage, the passages are defined as a set of consecutive sentences. The system IR-n uses the sentence as basic information unit to define the passages. The size of passage can be adapted to improve the efficiency of the IR-n system. The size of passage is measured in number of sentences. The use of sentence to define passages presents advantages against the use of paragraph or word.</p><p>The use of paragraph as unit to define the passage has two main problems:</p><p>-It is possible that the documents collection does not have information about the paragraphs marks in the document.</p><p>-The paragraphs can be used for visual reasons instead of structural reasons of the document.</p><p>The use of a number of words as unit to define the passage presents two problems:</p><p>-The number of words to be considered as a passage depends on the writing style used. The same event is described using less words in a document of news agency that in a newspaper.</p><p>If the same event is also written in a novel the number of words will be bigger.</p><p>-If the system uses words to define the passage, it can happen that the lack of structure of the considered text fragment can cause the does not understanding of the text recovered. This is due to the fact that the passage can start and end in any part of document.</p><p>Finally, the use of sentences to define the passage presents the following advantages:</p><p>-A sentence usually expresses an idea in the document.</p><p>-Usually documents use the punctuation signs to separate ideas. There are algorithms to obtain each sentence from a document using their superficial structure with a precision of 100% <ref type="bibr" coords="3,141.47,309.61,14.61,10.46" target="#b9">[10]</ref>.</p><p>-Sentences are full units allowing both to show an understandable information by users, or to provide this information to a subsequent system (for example a system of Question Answering). For this reason, the use of sentences improves the proposals that define the passages using a number of word.</p><p>-The use of sentences to define the passage allows to work with a heterogeneous document collection written by different authors an with different literary styles. Moreover, the size is a parameter of the system easily adaptable to the language, kind of texts, the size of the user's query, or the final use of the recovered passages. In this case, it is similar to the window model that can re-size the wide of window depending on the document collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Similarity measure between passage and user's question</head><p>At the beginning, the system IR-n used the traditional measure of the cosine <ref type="bibr" coords="3,432.66,479.43,14.61,10.46" target="#b10">[11]</ref>. Nevertheless, further experiments carried out using other similarity measures obtained better results. The similarity measures of the system IR-n differs from traditional IR system that IR-n system does not use the normalization factors related to the passage or document size. This is due to the fact that passage size is the same for all documents. So, the system IR-n calculates the similarity between a passage P and the user's query q in the following way:</p><formula xml:id="formula_0" coords="3,233.18,563.11,279.82,21.77">sim(Q, P ) = t∈Q∧P (w Q,t • w P,t )<label>(1)</label></formula><p>where:</p><formula xml:id="formula_1" coords="3,230.89,611.13,277.86,24.93">w Q,t = f req q,t • log e ( N -f req t f req t ) (<label>2</label></formula><formula xml:id="formula_2" coords="3,508.76,617.87,4.24,10.46">)</formula><formula xml:id="formula_3" coords="3,220.18,647.76,292.81,12.30">w P,t = 1 + log e (1 + log e (f req p,t + 1))<label>(3)</label></formula><p>where f req Y,t is the number of appearances or the frequency of the term t in the passage or the question Y . N is the total number of documents in the collection and f req t is the number of different documents that contain the term t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Similarity measure of document based on similarity passages</head><p>All systems of PR calculate the similarity measure of the document in function of the similarity measure of their passages using the sum of similarity measure for each passage or using the best similarity measure of passage for each document. The experiments carried out in <ref type="bibr" coords="4,454.85,152.66,10.52,10.46" target="#b4">[5]</ref> have been ran by the IR-n system, obtaining better results when using the best similarity measure of passage like similarity measure of the document.</p><p>Our proposal is based on the fact that if a passage is relevant then the document is also relevant. In fact, if a PR system uses the sum of every similarity measure of passage the the system has the same behavior like IR system based on document adding concepts of proximity.</p><p>Moreover, the use of the best similarity measure of passage allows to obtain the best passage improving further search process.</p><p>The system IR-n calculates the similarity measure of the document based on the best similarity measure of their passages in the following way:</p><formula xml:id="formula_4" coords="4,234.21,284.16,278.79,18.58">sim(Q, D) = max ∀ i:P i ∈D sim(Q, P i ) (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Query expansion</head><p>The techniques of query expansion allow to locate relevant documents that do not contain the exactly words of the user's query. Different studies have been carried out in order to add these techniques to the system IRn. These studies were two: the former, in the CLEF-2001 <ref type="bibr" coords="4,353.62,371.77,9.96,10.46" target="#b8">[9]</ref>, the system added the synonyms of the terms of user's question. This experiment achieved lower results than the IR-n system without question expansion. The latter, in the CLEF-2002 <ref type="bibr" coords="4,356.90,395.67,9.96,10.46" target="#b7">[8]</ref>, a model of Relevance Feedback was proposed achieving a few better results.</p><p>This year, the pattern proposed in <ref type="bibr" coords="4,262.72,419.58,10.52,10.46" target="#b2">[3]</ref> has been used, but adapting to the passage retrieval. This algorithm increases the relevance of each added term because are closely to the remaining term of question in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training process</head><p>This section describes the experiments carried out in order to obtain and to optimize some system's features to improve the performance of the system. The training corpus used in these experiments was the Clef-2002 document collection. Moreover, all experiments have only been carried out using short questions, that is the system only used the title and description from the query. The following subsections explain the specific experiments carried out to every CLEF task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Monolingual experiments</head><p>First experiments are focused on establishing the adequate number of sentence (N ) to make up the passage for each language (Spanish, Italian, German, French and English). The performance of the system was measured using the standard average interpolated precision (AvgP). For every language, the stemmers and the stop-word lists used were provided by the organization of the clef <ref type="foot" coords="4,104.67,639.10,3.97,7.32" target="#foot_0">1</ref> .</p><p>Table <ref type="table" coords="4,132.86,652.14,4.98,10.46" target="#tab_0">1</ref> shows the scores achieved for each language without query expansion. The German scores are also obtained without splitting the compound nouns. The obtained figures show the best results for German, French and English using 14 sentences, for Spanish using 9 sentences and for Italian using 8 sentences. The bigger size for German, English and French is due to the kind of document collections used for each language. The three collections are made up of documents with a bigger number of sentences that Spanish and Italian documents. Moreover, the lowest scores achieved for German language (0,3364) show the influence of splitting the compound nouns. The  Once, the size of passage was established for each language, the following experiment was carried out in order to study the influence of query expansion. The IR-n system uses a feedback technique to apply the query expansión. The IR-n system adds to the query the T more important term from the P more relevant passage according to <ref type="bibr" coords="5,327.30,346.45,9.96,10.46" target="#b1">[2]</ref>. Table <ref type="table" coords="5,375.50,346.45,4.98,10.46" target="#tab_2">3</ref> shows the scores achieved by IR-n system using the 5 more important term from the five and ten more relevant passages, and using the 10 more important term from the five and ten more relevant passages. Best results were obtained using the 10 more relevant passages, and the 10 more important term for Spanish, Italian and English, and the 5 more frequent term for German an French. This experiment shows that query expansion increase the obtained scores for all languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bilingual and Multilingual task</head><p>We use three different translator in order to obtain an automatic translation of queries. The three used translator were PowerTranslator, FreeTranslator 2 and Google 3 . In multilingual task (4 languages), the queries written in English were translating to French, Spanish and German. Once, we obtained the query translation, four different experiments were carried out in order to choose the best translation. The three first ones only used a translation and the last one used the merge of all translations as query. Table <ref type="table" coords="5,240.77,512.28,4.98,10.46" target="#tab_2">3</ref> shows the scores achieved in the four experiments developed using every document collection as the same way as monolingual task. Best scores were achieved using the merge of translations. The IR-n system was run obtained three different rank document collections in multilingual task. According to <ref type="bibr" coords="5,288.47,548.15,9.96,10.46" target="#b1">[2]</ref>, there are a few simple ways to merge ranked list of documents from different collections. We use two different method: M1) the first method is to normalize the relevance score for each topic, dividing all relevance scores by the relevance score of the top most ranked document for the same topic. M2) This method uses the following formula 2 www.freetranslation.com 3 www.google.com/language tools?hl=es T 5 10 P No expansion 5 10 5 10 Spanish 0,5042 0.5176 0,5122 0,5327 0,5441 Italian 0,4207 0,4428 0,4583 0,4491 0,4679 German 0,4027 0,4379 0,4499 0,4148 0,4438 French 0,4731 0,4991 0,5286 0,4980 0,5114 English 0,5057 0,5108 0,5034 0,5066 0,5139 </p><formula xml:id="formula_5" coords="6,209.06,317.75,303.94,12.33">rsv j = (rsv j -rsv min )/(rsv max -rsv min )<label>(5)</label></formula><p>in which rsv j is the original retrieval status value, and rsv min and rsv max are the minimum and maximum document scores values that a collection could achieve for the current request. Table <ref type="table" coords="6,90.01,359.60,4.98,10.46" target="#tab_4">5</ref> shows the scores achieved using both merging method. These scores show that best results are obtained using M2 merging method.</p><p>In bilingual task, an additional problem was found. We do not have a direct translator Italian-Spanish and Spanish-Italian. We had to translate Italian to English and late English to Spanish. This process carries out more errors than a directly translation. Table <ref type="table" coords="6,395.02,407.42,4.98,10.46" target="#tab_5">6</ref> shows the scores achieved in the bilingual task. At the same way as multilingual task, the best score was obtained using the merge of translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation at Clef-2003</head><p>The IR-n system used in order to participate in CLEF'2003 was the best IR-n configuration obtained in the training process using the CLEF'2002 collection.</p><p>The following subsections show the runs carried out and the scores achieved in the monolingual, bilingual and multilingual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Monolingual task</head><p>Two different runs have been submitted for each Spanish, French and Italian monolingual tasks. The first run does not use query expansion and the last one using it (IRn-xx -noexp and IRn-xx -exp, where xx are the language -es, fr or it-). Four different runs were submitted for German. The first and second runs follows the same strategies as previous languages but without splitting the compound nouns (IRn-al -noexp-nsp and IRn-al -exp-nsp). The third and fourth experiments used the splitting of compound nouns with and without expansion (IRn-al -noexp-sp and IRn-al -exp-sp) Tables <ref type="table" coords="6,137.18,639.97,7.75,10.46" target="#tab_6">7,</ref><ref type="table" coords="6,149.40,639.97,4.98,10.46" target="#tab_7">8</ref> , 9 and 10 show the scores achieved for each run in the monolingual task. IRn system using query expansion obtained better results than the average scores of CLEF 2003 systems for Spanish, French and German and lower scores in Italian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Traslation</head><p>Free Power Babel Power+Free+Babel Italian-Spanish 0,4207 0.3367 0.3490 0.3480 0.3663  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bilingual task</head><p>Two different runs have been submitted for bilingual tasks (Italian-Spanish). The first run does not use query expansion and the last one using it (IRn-ites-noexp and IRn-ites-exp). The English was used as intermediate language due to the lack of a direct translator Italian to Spanish. Table <ref type="table" coords="8,90.01,372.81,9.96,10.46" target="#tab_10">11</ref> shows that IR-n system using query expansion for bilingual task increase around a 26% the average scores of CLEF'2003 bilingual system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multilingual task</head><p>Five runs were submitted to Multilingual task made up four languages. The first runs (IRnm-noexp-nsp) shows the scores achieved by IR-n system without query expansion and without splitting the German compound nouns. The second one (IRn-m-exp-nsp) presents the performance of IR-n system using query expansion and without splitting the compound nouns. The third and fourth runs (IRn-m-noexp-sp and IRn-m-exp-sp, respectively) are the same experiments but using the splitting of German compound noun. Finally, an additional experiment (IRn-mi-exp-sp) was carried out using the same passage's size for all languages (10 sentences), and using the query expansion and the splitting of compound nouns. This size was obtained experimentally in the training task.</p><p>Table <ref type="table" coords="8,131.72,538.63,9.96,10.46" target="#tab_11">12</ref> shows that IR-n system improve the average scores of CLEF'2003 around a 23% using the AvgP measure. Moreover, IR-n system also obtains around a 23% of improvement using he same size of passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>General conclusions are positive. On the one hand, IR-n system has obtained better results than the average of CLEF'2003, excluding for Italian monolingual task. Moreover, we want to remark that all runs submitted are carried out only using short queries (title and description) and the average provided by CLEF organization is made up for all system (systems using both short or long queries). On the other hand, the achieved improvement using a list of the most frequent compound nouns in German conduct us to develop for next participation an algorithm to split the compound nouns.</p><p>Also, we want to emphasize the good performance of IR-n system in our first participation in bilingual and multilingual tasks. However, we planned to use a new method but time problem stopped us to submitted a new run. We hope to participate with the new method in the next conference.</p><p>Moreover, we want to underline the good scores achieved using the same size of passages for all languages. Finally, we want to emphasize that IR-n system is an information retrieval system based on passages and independent of languages according to the scores obtained in our participation at CLEF 2003.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,95.98,108.92,423.52,123.17"><head>Table 1 :</head><label>1</label><figDesc>AvgP without query expansion</figDesc><table coords="5,95.98,108.92,423.52,123.17"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Passage size using number of sentences</cell></row><row><cell>Size</cell><cell>5</cell><cell></cell><cell>6</cell><cell></cell><cell>7</cell><cell></cell><cell></cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell></row><row><cell>Spanish</cell><cell cols="2">0,4839</cell><cell cols="2">0,4974</cell><cell cols="2">0,5015</cell><cell cols="2">0,5004</cell><cell>0,5042</cell><cell>0,5001</cell><cell>0,4982</cell><cell>0.4978</cell><cell>0.4973</cell><cell>0.4973</cell><cell>0.4983</cell></row><row><cell>Italian</cell><cell cols="2">0,4073</cell><cell cols="2">0,4165</cell><cell cols="2">0,4171</cell><cell cols="2">0,4207</cell><cell>0,4146</cell><cell>0,4190</cell><cell>0,4188</cell><cell>0,4193</cell><cell>0,4195</cell><cell>0,4166</cell><cell>0,4158</cell></row><row><cell>German</cell><cell cols="2">0,3236</cell><cell cols="2">0,3278</cell><cell cols="2">0,3268</cell><cell cols="2">0,3267</cell><cell>0,3287</cell><cell>0,3293</cell><cell>0,3315</cell><cell>0,3327</cell><cell>0,3350</cell><cell>0,3364</cell><cell>0,3363</cell></row><row><cell>French</cell><cell cols="2">0,4260</cell><cell cols="2">0,4347</cell><cell cols="2">0,4442</cell><cell cols="2">0,4519</cell><cell>0,4529</cell><cell>0,4625</cell><cell>0,4655</cell><cell>0,4685</cell><cell>0,4716</cell><cell>0,4731</cell><cell>0,4725</cell></row><row><cell>English</cell><cell cols="2">0,4675</cell><cell cols="2">0,4697</cell><cell cols="2">0,4800</cell><cell cols="2">0,4883</cell><cell>0,4882</cell><cell>0,4957</cell><cell>0,4923</cell><cell>0,4945</cell><cell>0,4979</cell><cell>0,5057</cell><cell>0,5038</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Passage size using number of sentences</cell></row><row><cell>Size</cell><cell></cell><cell>5</cell><cell></cell><cell>6</cell><cell></cell><cell>7</cell><cell></cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell></row><row><cell cols="2">No Splitted</cell><cell cols="2">0,3236</cell><cell cols="2">0,3278</cell><cell cols="2">0,3268</cell><cell>0,3267</cell><cell>0,3287</cell><cell>0,3293</cell><cell>0,3315</cell><cell>0,3327</cell><cell>0,3350</cell><cell>0,3364</cell><cell>0,3363</cell></row><row><cell>Splitted</cell><cell></cell><cell cols="2">0,3843</cell><cell cols="2">0,3894</cell><cell cols="2">0,3936</cell><cell>0,3933</cell><cell>0,3972</cell><cell>0,3982</cell><cell>0,3984</cell><cell>0,3981</cell><cell>0,4003</cell><cell>0,4027</cell><cell>0,4021</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,90.00,242.89,423.02,66.20"><head>Table 2 :</head><label>2</label><figDesc>German monolingual task: AvgP without query expansion using compound nouns lack of an algorithm to split compound nouns should us to use a list of more frequently compound noun made up of 200000 terms. The scores achieved for German using the compound list were better as show the table 2.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,212.52,739.15,177.96,10.46"><head>Table 3 :</head><label>3</label><figDesc>AvgP Using question expansion</figDesc><table coords="6,139.45,109.16,324.09,59.87"><row><cell>Traslation</cell><cell>Free</cell><cell cols="2">Power Babel Power+Free+Babel</cell></row><row><cell>Spanish</cell><cell cols="2">0,5042 0.4235 0.4336 0.4217</cell><cell>0.4371</cell></row><row><cell>Italian</cell><cell cols="2">0,4207 0.3367 0.3490 0.3480</cell><cell>0.3663</cell></row><row><cell>German</cell><cell cols="2">0,4027 0.3037 0.3092 0.3024</cell><cell>0.3245</cell></row><row><cell>French</cell><cell cols="2">0,4731 0.3835 0.4281 0.4077</cell><cell>0.4291</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,158.23,180.43,286.54,70.19"><head>Table 4 :</head><label>4</label><figDesc>Translation used as monolingual task</figDesc><table coords="6,158.23,203.10,286.54,47.52"><row><cell></cell><cell></cell><cell cols="3">Precision at N documents</cell><cell></cell><cell></cell></row><row><cell>Cob.</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>200</cell><cell>AvgP</cell></row><row><cell cols="7">M1 61.97 0.6760 0.6360 0.5860 0.5367 0.2755 0.3108</cell></row><row><cell cols="7">M2 72.42 0.6760 0.6480 0.6030 0.5653 0.3152 0.3621</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,90.00,262.02,353.37,42.29"><head>Table 5 :</head><label>5</label><figDesc>Scores achieved by IR-n system with document merging to normalize the document.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,191.69,728.73,219.62,10.46"><head>Table 6 :</head><label>6</label><figDesc>Bilingual scores using question expansion</figDesc><table coords="7,227.55,148.72,147.90,47.52"><row><cell></cell><cell>AvgP</cell><cell>Dif.</cell></row><row><cell cols="2">Clef Average 0.4649</cell></row><row><cell>IRn-es-exp</cell><cell cols="2">0.5056 +8.75%</cell></row><row><cell cols="2">IRn-es-noexp 0.4582</cell><cell>-1.44%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,164.32,207.64,274.37,145.31"><head>Table 7 :</head><label>7</label><figDesc>CLEF-2003 official results. Spanish monolingual task.</figDesc><table coords="7,228.26,305.43,146.48,47.52"><row><cell></cell><cell>AvgP</cell><cell>Dif.</cell></row><row><cell cols="2">Clef Average 0.4843</cell></row><row><cell>IRn-fr-exp</cell><cell cols="2">0.5128 +5.88%</cell></row><row><cell cols="2">IRn-fr-noexp 0.4853</cell><cell>0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,166.62,364.36,269.77,145.31"><head>Table 8 :</head><label>8</label><figDesc>CLEF-2003 official results. French monolingual task.</figDesc><table coords="7,233.08,462.14,136.85,47.52"><row><cell></cell><cell>AvgP</cell><cell>Dif.</cell></row><row><cell cols="2">Clef Average 0.4903</cell></row><row><cell>IRn-it-exp</cell><cell cols="2">0.4802 -2.06%</cell></row><row><cell cols="3">IRn-it-noexp 0.4547 -7.26%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="7,166.98,521.07,269.05,170.02"><head>Table 9 :</head><label>9</label><figDesc>CLEF-2003 official results. Italian monolingual task.</figDesc><table coords="7,221.33,618.86,160.34,72.23"><row><cell></cell><cell>AvgP</cell><cell>Dif.</cell></row><row><cell>Clef Average</cell><cell>0.4759</cell></row><row><cell cols="3">IRn-al-nexp-nsp 0.4267 -10.34%</cell></row><row><cell cols="2">IRn-al-exp-nsp. 0.4687</cell><cell>-1.51%</cell></row><row><cell cols="2">IRn-al-nexp-sp 0.4670</cell><cell>-1.87%</cell></row><row><cell>IRn-al-exp-sp.</cell><cell cols="2">0.5115 +7.48%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="7,161.26,702.49,280.49,10.46"><head>Table 10 :</head><label>10</label><figDesc>CLEF-2003 official results. German monolingual task.</figDesc><table coords="8,221.37,109.16,160.26,47.52"><row><cell></cell><cell>AvgP</cell><cell>Dif.</cell></row><row><cell>Clef Average</cell><cell>0.3665</cell></row><row><cell cols="2">IRn-ites-noexp 0.3662</cell><cell>0%</cell></row><row><cell>IRn-ites-exp</cell><cell cols="2">0.4610 +25.78%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="8,153.66,168.08,295.68,107.25"><head>Table 11 :</head><label>11</label><figDesc>CLEF-2003  official results. Italian-Spanish bilingual task.</figDesc><table coords="8,215.56,190.75,171.88,84.58"><row><cell></cell><cell>AvgP</cell><cell>Dif.</cell></row><row><cell>Clef Average</cell><cell>0.2752</cell></row><row><cell cols="2">IRn-m-noexp-nsp 0.3024</cell><cell>+9.88%</cell></row><row><cell>IRn-m-exp-nsp.</cell><cell cols="2">0.3281 +19.22%</cell></row><row><cell cols="2">IRn-m-noexp-sp 0.3074</cell><cell>+11.7%</cell></row><row><cell>IRn-m-exp-sp.</cell><cell cols="2">0.3377 +22.71%</cell></row><row><cell>IRn-mi-exp-sp.</cell><cell cols="2">0.3373 +22.56%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="8,180.26,286.72,242.48,10.46"><head>Table 12 :</head><label>12</label><figDesc>CLEF-2003 official results: Multilingual task.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,105.24,743.68,112.95,8.37"><p>http://www.unine.ch/info/clef</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>* This work has been partially supported by the <rs type="funder">Spanish Government (CICYT)</rs> with grant <rs type="grantNumber">TIC2000-0664-C02-02</rs> and (<rs type="projectName">PROFIT</rs>) with grant <rs type="grantNumber">FIT-150500-2002-416</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_pzeAmCF">
					<idno type="grant-number">TIC2000-0664-C02-02</idno>
					<orgName type="project" subtype="full">PROFIT</orgName>
				</org>
				<org type="funding" xml:id="_BtZTqFW">
					<idno type="grant-number">FIT-150500-2002-416</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.48,200.99,402.52,10.46;9,110.48,212.94,402.52,10.46;9,110.48,224.90,246.72,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,188.26,200.99,203.28,10.46">Passage-Level Evidence in Document Retrieval</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,412.80,200.99,100.20,10.46;9,110.48,212.94,398.37,10.46">Proceedings of the 17th Annual International Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17th Annual International Conference on Research and Development in Information Retrieval<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1994-07">July 1994</date>
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,244.82,402.52,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,166.78,244.82,229.57,10.46">Cross-Languaje Retrieval Experiments at CLEF-2002</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,417.53,244.82,26.71,10.46">CLEF</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="5" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,264.75,402.53,10.46;9,110.48,276.70,402.53,10.46;9,110.48,288.66,402.52,10.46;9,110.48,300.62,281.60,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,110.48,276.70,326.75,10.46">Question Answering: CNLP at the TREC-10 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Diekema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taffet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mccracken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ozgencil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Yilmazel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Liddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,464.30,276.70,48.71,10.46;9,110.48,288.66,140.67,10.46">Tenth Text REtrieval Conference (Notebook)</title>
		<meeting><address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<publisher>National Institute of Standards and Technology</publisher>
			<date type="published" when="2001-11">nov 2001</date>
			<biblScope unit="page" from="500" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,320.54,402.52,10.46;9,110.48,332.50,191.46,10.46" xml:id="b3">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="9,110.48,320.54,246.75,10.46">Workshop of Cross-Language Evaluation Forum (CLEF</title>
		<title level="s" coord="9,393.03,320.54,119.97,10.46;9,110.48,332.50,30.03,10.46">Lecture notes in Computer Science</title>
		<meeting><address><addrLine>Roma, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,352.42,402.52,10.46;9,110.48,364.38,402.52,10.46;9,110.48,376.33,215.41,10.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,236.09,352.42,233.70,10.46">Subtopic structuring for full-length document access</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Plaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,496.16,352.42,16.84,10.46;9,110.48,364.38,402.52,10.46;9,110.48,376.33,37.33,10.46">Sixteenth International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-06">jun 1993</date>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,396.25,402.52,10.46;9,110.48,408.21,376.90,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,269.72,396.25,186.86,10.46">Effective Ranking with Arbitrary Passages</title>
		<author>
			<persName coords=""><forename type="first">Marcin</forename><surname>Kaszkiel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,467.54,396.25,45.46,10.46;9,110.48,408.21,237.11,10.46">Journal of the American Society for Information Science (JASIS)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="344" to="364" />
			<date type="published" when="2001-02">February 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,428.13,402.52,10.46;9,110.48,440.10,163.74,10.46" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
		</author>
		<title level="m" coord="9,188.67,428.13,295.14,10.46">IR-n un sistema de Recuperación de Información basado en pasajes</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Universidad de Alicante</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="9,110.48,460.02,402.52,10.46;9,110.48,471.98,402.52,10.46;9,110.48,483.93,300.45,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,273.48,460.02,213.60,10.46">IR-n system, a passage retrieval system at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,122.47,471.98,266.82,10.46">Workshop of Cross-Language Evaluation Forum (CLEF 2001)</title>
		<title level="s" coord="9,396.78,471.98,116.22,10.46;9,110.48,483.93,30.03,10.46">Lecture notes in Computer Science</title>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,503.85,402.52,10.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,269.82,503.85,93.07,10.46">IR-n system at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,403.39,503.85,26.71,10.46">CLEF</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,523.78,402.52,10.46;9,110.48,535.73,402.53,10.46;9,110.48,547.69,206.90,10.46" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,245.82,523.78,267.19,10.46;9,110.48,535.73,402.53,10.46;9,110.48,547.69,107.23,10.46">Emerging Technologies in Accounting and Finance, chapter Sentence Boundary and Named Entity Recognition in EXIT System: Information Extraction System of Notarial Texts</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="129" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,567.61,402.54,10.46;9,110.48,579.57,274.37,10.46" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,263.34,567.61,245.83,10.46">A term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,110.48,579.57,176.63,10.46">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="123" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
