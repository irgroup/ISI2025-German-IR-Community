<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.28,86.47,400.97,12.19">JHU/APL Experiments in Tokenization and Non-Word Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.26,115.77,61.94,8.74"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
							<email>mcnamee@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University Applied Physics Laboratory</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.74,115.77,63.55,8.74"><forename type="first">James</forename><surname>Mayfield</surname></persName>
							<email>mayfield@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University Applied Physics Laboratory</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.28,86.47,400.97,12.19">JHU/APL Experiments in Tokenization and Non-Word Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C459BA05CA61767CBD4D6D5CFB3B7DC4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the past we have conducted experiments that investigate the benefits and peculiarities attendant to alternative methods for tokenization, particularly overlapping character n-grams. This year we continued this line of work and report new findings reaffirming that the judicious use of n-grams can lead to performance surpassing that of word-based tokenization. In particular we examined: the relative performance of n-grams and a popular suffix stemmer; a novel form of n-gram indexing that approximates stemming and achieves fast run-time performance; various lengths of n-grams; and the use of n-grams for robust translation of queries using an aligned parallel text. For the CLEF 2003 evaluation we submitted monolingual and bilingual runs for all languages and language pairs, multilingual runs using English as a source language, and a first attempt and cross-language spoken document retrieval. Our key findings are that shorter n-grams (n=4 and n=5) outperform a popular stemmer in non-Romance languages, that direct translation of n-grams is feasible using an aligned corpus, that translated 5-grams yield superior performance to words, stems, or 4grams, and that a combination of indexing methods is best of all.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In the past we have examined a number of issues pertaining to how documents and queries are represented. This has been a particular interest in our work with the HAIRCUT retrieval system due to the consistent success we have observed with the use of overlapping character n-grams. Simple measures that can be uniformly applied to text processing, regardless of language, reduce developer effort and appear to be at least as effective as approaches that rely on language-specific processing, and perhaps more so. They are increasingly used when linguistic resources are unavailable <ref type="bibr" coords="1,308.70,451.11,16.47,8.74" target="#b10">[11]</ref>[14] <ref type="bibr" coords="1,341.64,451.11,16.47,8.74" target="#b14">[15]</ref>, but in general have not been widely adopted. We believe that this may be due in part to a belief that n-grams are not as effective as competing approaches (an idea that we attempt to refute here), and also due to a fear of increased index-time and runtime costs. We do not focus on the second concern here; few studies addressing the performance implications of n-gram processing have been undertaken (but see <ref type="bibr" coords="1,282.59,497.07,15.06,8.74" target="#b9">[10]</ref>), and we hope this gap is soon filled.</p><p>Over this past year we investigated several issues in tokenization. Using the CLEF 2002 and 2003 test suites as an experimental framework, we attempt to answer the following questions:</p><p>o Should diacritical marks be retained? o What length of character n-grams results in the best performance? o Does the optimal length vary by language? o Are n-grams as effective as stemmed words? o Can n-gram processing be sped up? o What peculiarities arise when n-grams are used for bilingual retrieval? o Are n-grams effective for cross-language spoken document retrieval?</p><p>We submitted official runs for the monolingual, bilingual, multilingual tracks and participated in the first cross-language spoken document benchmark. For all of our runs we used the HAIRCUT system and a statistical language model similarity calculation. Many of our official runs were based on n-gram processing though we found that by using a combination of n-grams and stemmed words better performance can be obtained. For our bilingual runs we relied on pre-translation query expansion. We also developed a new method of translating queries, using n-grams rather than words as the elements to be translated. This method does not suffer from several key obstacles in dictionary-based translation, such as word lemmatization, matching of multiple word expressions, and out-of-vocabulary words such as common surnames <ref type="bibr" coords="1,461.04,715.40,15.38,8.74" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>HAIRCUT supports a variety of indexing terms and represents documents using a bag-of-terms model. Our general method is to process the text for each document, reducing all terms to lower-case. Generally words were deemed to be white-space delimited tokens in the text; however, we preserve only the first 4 digits of a number and we truncate any particularly long tokens (those greater than 35 characters in length). Once words are identified we optionally perform transformations on the words to create indexing terms (e.g., stemming). So-called stopwords are retained in our index and the dictionary is created from all words present in the corpus.</p><p>We have wondered whether diacritical marks have much effect upon retrieval performance -for a long time we have been retaining diacritical marks as part of our ordinary lexical processing, in keeping with a keep-itsimple approach. One principled argument for retaining inflectional marks is that they possess a deconflationary effect when content words that differ only in diacritics have different meaning. For example, the English words resume (to continue) and résumé (a summary of one's professional life) can be distinguished by differences in diacritics. On the other hand, such marks are not always uniformly applied, and furthermore, if retained, might distinguish two semantically related words. Stephen Tomlinson investigated preservation of diacritics using the CLEF 2002 collection and reported that it was helpful in some cases (Finnish) and harmful in others (Italian and French) <ref type="bibr" coords="2,329.81,282.81,15.37,8.74" target="#b15">[16]</ref>. We found similar results (see Table <ref type="table" coords="2,498.16,282.81,3.63,8.74">1</ref>), though the effect is seen only for words, not n-grams. As there is practically no effect, we opted to remove such accents routinely. Intuitively we thought that removing the distinction might improve corpus statistics when n-grams are used. Whenever stemming was used, words were first stemmed, and then any remaining marks were removed; this enabled the stemmer to take advantage of marks when present. N-grams were produced from the same sequence of words; however, we attempt to detect sentence boundaries to prevent generating n-grams across sentence boundaries.</p><p>language DE EN ES FI FR IT NL SV words -0.0002 0.0028 0.0146 -0.0363 0.0139 0.0076 -0.0005 0.0045 4-grams -0.0028 -0.0093 0.0019 0.0075 0.0077 -0.0090 0.0009 -0.0056 Table <ref type="table" coords="2,95.93,411.21,3.76,8.74">1</ref>. Absolute difference in mean average precision when accented marks were removed. HAIRCUT uses gamma compression to reduce the size of the inverted file. Within-document positional information is not retained, but both document-id and term frequencies are compressed. We also produce a 'dual file' that is a document-indexed collection of term-ids and counts. Construction of this data structure doubles our on-disk space requirements, but confers advantages such as being able to quickly examine individual document representations. This is particularly useful for automated (local) query expansion. Our lexicon is stored as a B-tree but nodes are compressed in memory to maximize the number of in-memory terms subject to physical memory limitations. For the indexes created for CLEF 2003 memory was not an issue as only O(10 6 ) distinct terms were found in each collection.</p><p>We use a statistical language model for retrieval akin to those presented by Miller et al. <ref type="bibr" coords="2,426.44,537.57,11.69,8.74" target="#b8">[9]</ref> and Hiemstra <ref type="bibr" coords="2,497.32,537.57,11.69,8.74" target="#b1">[2]</ref> with Jelinek-Mercer smoothing <ref type="bibr" coords="2,193.23,549.09,13.02,8.74" target="#b2">[3]</ref>. In this model, relevance is defined as</p><formula xml:id="formula_0" coords="2,189.78,560.61,202.02,30.59">P(D | Q) = αP(q | D) + (1-α)P(q | C) [ ] q ∈Q ∏ ,</formula><p>where Q is a query, D is a document, C is the collection as a whole, and α is a smoothing parameter. The probabilities on the right side of the equation are replaced by their maximum likelihood estimates when scoring a document. The language model has the advantage that term weights are mediated by the corpus.</p><p>Our experience has been that this type of probabilistic model outperforms a vector-based cosine model or a binary independence model with Okapi BM25 weighting.</p><p>For the monolingual, bilingual, and multilingual tasks, all of our submitted runs were based on a combination of several base runs. Our method for combination was to normalize scores by probability mass and to then merge documents by score. All of our runs were automatic runs and used only the title and description topic fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Experiments</head><p>For our monolingual work we created several indexes for each language using the permissible document fields appropriate to each collection. Our four basic methods for tokenization were unnormalized words, stemmed words obtained through the use of the Snowball stemmer, 4-grams, and 5-grams. Information about each index is shown in From the table above it can be seen that the percentage of relevant documents for each subcollection is closely related to its contribution to the overall number of documents. This would suggest that collection size might be a useful factor for multilingual merging. We also note that n-gram indexing results in increased disk storage costs. This cost is driven by the increased number of postings in the inverted file when n-gram indexing is performed.</p><p>Our use of 4-grams and 5-grams as indexing terms represents a departure from previous work using 6-grams <ref type="bibr" coords="3,70.56,392.73,10.64,8.74" target="#b5">[6]</ref>. We conducted tests using various lengths of n-grams for all eight CLEF 2002 languages and found that choices of n=4 or n=5 performed best. Figure <ref type="figure" coords="3,277.35,404.19,5.01,8.74" target="#fig_0">1</ref> charts performance using six different term indexing strategies; a value of α=0.5 was used throughout and no relevance feedback was attempted. We determined that use of n=4 or n=5 is best in all eight languages though it is hard to distinguish between the two. 6-grams are clearly not as effective in these languages. There are differences in performance depending on the value of smoothing constant, α, that is used, though we have yet to test whether these differences are significant or merely represent overtraining on the 2002 test set. The effect of smoothing parameter selection in language model-based retrieval was investigated by Zhai and Lafferty <ref type="bibr" coords="4,446.60,119.91,15.37,8.74" target="#b16">[17]</ref>. We report on our results investigating the effect of n-gram length, with additional detail and further experiments in a forthcoming manuscript <ref type="bibr" coords="4,169.65,142.89,10.65,8.74" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Differing Tokenization</head><p>In additional to determining good values for n, we also wanted to see if n-grams remained an attractive technique in comparison to stemmed words. Having no substantive experience with stemming, we were pleased to discover that the Snowball stemmer <ref type="bibr" coords="4,267.28,188.79,15.36,8.74" target="#b12">[13]</ref>, a derivative of the Porter stemmer extended to many languages by Porter, provides a set of rules for all of the CLEF 2003 languages. Furthermore, the software contains Java bindings so it fit seamlessly with the HAIRCUT system. We decided to make a comparison between raw words, stems, 4-grams, 5-grams, and a surrogate technique based on n-grams that might approximate stems. Our n-gram approximation to stemming was based on picking the word-internal n-gram for each word with lowest document frequency (i.e., we picked the least common n-gram for each word). As an example, consider the words 'juggle', 'juggles', and 'juggler'. The least common 5-gram for the first two is 'juggl', however the least common 5-gram for 'juggler' is 'ggler' 1 . The least common 4-gram for all three words is 'jugg'. We hypothesize that high IDF n-gram affixes will span portions of words that exhibit little morphological variation. This method has the advantage of providing some morphological normalization, but it does not increase the number of postings in an inverted file. This can be viewed either as a way to approximate stems or a way of lowering the computational cost of using n-grams. We found that n-grams did outperform stems, and that our pseudo stems based on n-grams were better than raw words, but not as effective as a rule-based stemmer (see Figure <ref type="figure" coords="4,99.30,361.17,3.63,8.74" target="#fig_1">2</ref>). Details about this work can be found in Mayfield and McNamee <ref type="bibr" coords="4,373.77,361.17,10.64,8.74" target="#b4">[5]</ref>. 1 The Snowball stemmer also fails to transform juggler to a canonical form.</p><p>On the 2003 test collection we produced base runs for words, stems (using the Snowball stemmer), 4-grams, and 5-grams. Performance (based on average precision) for each is reported in Table <ref type="table" coords="5,412.01,96.15,3.76,8.74" target="#tab_2">3</ref>. All of these runs used blind relevance feedback and used an α value of 0.3 with words and stems, or 0.8 with n-grams. None of these runs were submitted as official runs; instead, we created hybrid runs using multiple methods. In the past we have found that combination from multiple runs can confer a nearly 10% improvement in performance. Savoy has also reported improvements from multiple term types <ref type="bibr" coords="5,386.41,142.89,15.38,8.74" target="#b14">[15]</ref>. To produce our official monolingual runs we decided to combine runs based on the Snowball stemmer with runs using n-grams as indexing terms. Runs named aplmoxxa used 4-grams and stems while runs named aplmoxxb used 5-grams and stems. However, due to a mistake while creating the scripts used to produce all of our runs, we inadvertently failed to perform blind relevance feedback for our monolingual submissions. Routinely we expand queries to 60 terms using additional terms ranked after examining the top 20 and bottom 75 (of 1000) documents. Failing to use blind relevance feedback had a detrimental effect on our official runs. Our official monolingual runs are described in  <ref type="table" coords="5,96.52,569.73,3.76,8.74">4</ref>. Official results for monolingual task. The shaded row contains results for a comparable, unofficial English run. The two columns at the far right report a corrected value for mean average precision when blind relevance feedback is applied, and the relative difference compared to the corresponding official run.</p><p>It appears that several of our runs would have increased substantially if we had correctly used blind relevance feedback. Relative improvements of more than 5% were seen in German, Russian, and Spanish although performance would have dropped slightly in Swedish. The German and Spanish document collections are the two largest in the entire test suite. We wonder if relevance feedback may be more beneficial when larger collections are available, a conjecture partially explored by Kwok and Chan <ref type="bibr" coords="5,377.61,661.65,10.64,8.74" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual Experiments</head><p>This year the Bilingual task focused on retrieval involving four language pairs, which notably did not contain English as a source or target language. This is only significant because of the difficulty in locating direct translation resources for some language pairs and the fact that many translation resources are available when English is one of the languages involved. The four language pairs are German to Italian, Finnish to German, French to Dutch, and Italian to Spanish. For the 2002 campaign we relied on a single translation resource: bilingual wordlists extracted from parallel corpora. We built a large alignable collection from a single source, the Official Journal of the EU [18], and we again used this resource as our only source of translations for 2003. The parallel corpus grew by about 50% this year, so a somewhat larger resource was available. First we describe the construction of the parallel corpus and the extraction of our bilingual wordlists, then we discuss our overall strategy for bilingual retrieval, and finally we report on our official results.</p><p>Our collection was obtained through a nightly crawl of the Europa web site where we targeted the Official Journal of the European Union <ref type="bibr" coords="6,203.85,188.07,15.39,8.74">[18]</ref>. The Journal is available in each of the E.U. languages and consists mainly of governmental topics, for example, trade and foreign relations. We had data available from December 2000 through May 2003. Though focused on European topics, the time span is 5 to 8 years after the CLEF-2002 document collection. The Journal is published electronically in PDF format and we wanted to create an aligned collection. We started with 33.4 GB of PDF documents and converted them to plain text using the publicly available pdftotext software (version 1.0). Once converted to text, documents were split into pieces using conservative rules for page breaks and paragraph breaks. Many of the documents are written in outline form, or contain large tables, so this pre-alignment processing is not easy. We ended up with about 300MB of text, per language, that could be aligned. Alignment was carried out using the char_align program <ref type="bibr" coords="6,154.26,291.45,10.65,8.74" target="#b0">[1]</ref>. In this way we created an aligned collection of approximately 1.2 million passages; these 'documents' were each about 2 or 3 sentences in length.</p><p>We performed pairwise alignments between languages pairs, for example, between German and Italian. Once aligned, we indexed each pairwise-aligned collection using the technique described for the CLEF-2003 document collections. Again, we created four indexes per sub-collection, per language -one each of words, stems, 4-grams and 5-grams. Our goal was to support query term translation, so for each source language term occurring in at least 4 documents, we attempted to determine a translation of the same token type in the target language. At this point we should mention that the 'proper' translation of an n-gram is decidedly slippery -clearly there can be no single correct answer. Nonetheless, we simply relied on the large volume of n-grams to smooth topic translation. For example, the central 5-grams of the English phrase 'prime minister' include 'ime_m', 'me_mi', and 'e_min'. The derived 'translations' of these English 5-grams into French are 'er_mi', '_mini', and 'er_mi', respectively. This seems to work as expected for the French phrase 'premier ministre', although the method is not foolproof. Consider n-gram translations from the phrase 'communist party' (parti communiste): '_commu' (mmuna), 'commu' (munau), 'ommun' (munau), 'mmuni' (munau), 'munis' (munis), 'unist' (unist), 'nist_' (unist), 'ist_p' (ist_p), 'st_pa' (1_re_), 't_par' (rtie_), '_part' (_part), 'party' (rtie_), and 'arty_' (rtie_). The lexical coverage of translation resources is a critical factor for good CLIR performance, so the fact that almost any n-gram has a 'translation' should improve performance. The direct translation of n-grams may offer a solution to several key obstacles in dictionarybased translation. Word normalization is not essential since sub-word strings will be compared. Translation of multiword expressions can be approximated by translation of word-spanning n-grams. Out-of-vocabulary words, particularly proper nouns, can be be partially translated by common n-gram fragments or left untranslated in close languages.</p><p>We extracted candidate translations as follows. First, we would take a candidate term as input and identify documents containing this term in the source language subset of the aligned collection. Up to 5000 documents were considered; we bounded the number for reasons of efficiency and because we felt that performance was not enhanced appreciably when a greater number of documents was used. If no document contained this term, then it was left untranslated. Second, we would identify the corresponding documents in the target language. Third, using a statistic that is similar to mutual information, we would extract a single potential translation. Our statistic is a function of the frequency of occurrence in the whole collection and the frequency in the subset of aligned documents. In this way we extracted the single-best target language term for each source language term in our lexicon (not just the query terms in the CLEF topics). When 5-grams were used this process took several days. Table <ref type="table" coords="6,96.85,693.62,5.01,8.74" target="#tab_4">5</ref> lists examples of translating within the designated language pairs using each type of tokenization. Mistakes are evident; however, especially when pre-translation expansion is used the overall effectiveness is quite high. We believe the redundancy afforded by translating multiple n-grams for each query word also reduces loss due to erroneous translations. Finally, incorrect translations may still prove helpful if they are a collocation rather than an actual translation. We remain convinced that pre-translation query expansion is a tremendously effective method to improve bilingual performance. Therefore we used each CLEF 2003 document collection as an expansion collection for the source language queries. Queries were expanded to a list of 60 terms, and then we attempted to translate each using our corpus-derived resource. In the past we have been interested in using n-grams as terms, however, we have worked with bilingual wordlists for translation. This year we decided to create translingual mappings using the same tokenization in both the source and target languages. Thus for each of the four language pairs, we created four different lists (for a total of 16): one list per type of indexing term (i.e., word, stem, 4-gram, or 5-gram). Again using experiments on the CLEF 2002 collection, we determined that mappings between n-grams were more efficacious than use of word-to-word or stem-to-stem mappings.</p><p>Thus different tokenization can be used for initial search, pre-translation expansion, query translation, and target language retrieval. In testing we found the best results using both n-grams and stems for an initial source-language search, then we extracted ordinary words as 'expansion' terms, and finally we translated each n-gram contained in the expanded source language word list into n-grams in the target language (or stems into stems, as appropriate). The process is depicted in Figure <ref type="figure" coords="7,344.56,494.55,3.90,8.74">3</ref>:</p><p>Figure <ref type="figure" coords="7,100.67,713.13,3.76,8.74">3</ref>. Illustration of bilingual processing. The initial input to translation is an expanded list of plain words extracted from a set of documents obtained by retrieval in the source language collection. These words are optionally tokenized (e.g., to stems or n-grams), and the constituent query terms are then translated using the mappings derived from the parallel texts. Multiple base runs are combined to create a final ranked list. Our runs named aplbixxyya are bilingual runs that were translated directly from the source language to the target language; each run was a combination of four base runs that either used words, stems, 4-grams, or 5grams, with (post-translation) relevance feedback. The runs named aplbixxyyb were combined in the same way, however the four constituent base runs did not make use of post-translation feedback. When words or stems were used a value of 0.3 was used for alpha; when n-grams were used the value was 0.5. The base runs are compared in Figure <ref type="figure" coords="8,166.41,273.51,3.76,8.74" target="#fig_3">4</ref>.  From observing the data in Table <ref type="table" coords="8,206.19,613.47,5.01,8.74" target="#tab_5">6</ref> and Figure <ref type="figure" coords="8,259.32,613.47,3.77,8.74" target="#fig_3">4</ref>, it would appear that the use of post-translation feedback did not enhance performance when multiple runs were combined. The two types of runs seemed to perform similarly in two language pairs (Finnish to German and Italian to Spanish); however, the merged runs without relevance feedback did better for the German to Italian and French to Dutch runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IT query</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ribellioni in</head><p>Combination of methods resulted in between a 3 and 10% gain depending on language pair. We have not yet had the opportunity to retrospectively analyze the contribution to our overall performance of pre-translation expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilingual Experiments</head><p>We initially thought to create runs for the multilingual task in the exact same way as for the bilingual task. However, we decided to use English as our source language and we had to create translation lists for seven languages using four tokenization types (a total of 28 mappings). Construction of the 5-gram lists took longer than expected and so we had to modify our plans for our official submission. We decided to submit a hybrid run based on words, stems, and 4-grams; merging was again accomplished using normalized scores. As with the bilingual task, runs ending in 'a' denote the use of post-translation relevance feedback, while runs ending in 'b' did not use feedback (see </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spoken Document Evaluation</head><p>This was our first time using the TREC-8 and TREC-9 spoken document dataset. Our submissions were created in very short order -in one day. We pre-processed the data so it had similar SGML markup as the ad hoc TREC collections and then indexed the English text using only 5-grams. The index took 33 minutes to build. We did not make use of any collection expansion for these runs. Our processing was similar to the work we did for the bilingual track, except that we used only 5-grams as translation terms and did not use pre-translation expansion (which was not permitted for 'primary' submissions).</p><p>The runs we submitted for the spoken document evaluation are summarized in Table <ref type="table" coords="9,412.80,380.01,3.76,8.74" target="#tab_8">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>For the first time we were able to directly compare words, various lengths of character n-grams, a suffix stemmer, and an n-gram alternative to stemming, all using the same retrieval engine. We found that n-grams of shorter lengths (n=4 or n=5) were preferable across the CLEF 2003 languages and that n-grams generally outperformed use of the Snowball stemmer: 4-grams had a 8% mean relative advantage across the 9 languages compared to stems; however stemming was better in Italian and Spanish (by 17% and 5% respectively). We found best performance can be obtained using a combination of methods. If emphasis is placed on accuracy over storage requirements or response time, this approach is reasonable. For bilingual retrieval we identified a method for direct translation of n-grams instead of word-based translation. Without the use of relevance feedback, 5-grams outperformed stems by an average of 17% over the four bilingual pairs though 4-grams appeared to lose much of their monolingual superiority. When feedback was used, the gap narrowed substantially.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,70.56,716.19,438.54,8.74;3,70.56,727.65,191.33,8.74"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Relative efficacy of different tokenization methods using the CLEF 2002 test set. Note that blind relevance feedback was not used for these runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,70.56,711.57,404.82,8.74"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparing words, stemmed words, 4-grams, and approximate stemming (2002 collection).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,70.56,578.97,438.49,8.74;8,70.56,590.49,137.70,8.74"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Analysis of the base-runs used for bilingual retrieval. The best APL run was achieved in each instance through run combination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,91.14,133.41,397.40,152.62"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="3,91.14,133.41,397.40,152.62"><row><cell></cell><cell></cell><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">language #docs</cell><cell cols="2">%docs #rel</cell><cell>%rel</cell><cell cols="4">index size (MB) / unique terms (1000s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>words</cell><cell>stems</cell><cell>4-grams</cell><cell>5-grams</cell></row><row><cell>DE</cell><cell>294805</cell><cell>18.3</cell><cell cols="6">1825 18.2 265 / 11.88 219 / 860 705 / 219 1109 / 1230</cell></row><row><cell>EN</cell><cell>166754</cell><cell>10.3</cell><cell cols="2">1006 10.0</cell><cell cols="3">143 / 302 123 / 235 504 / 166</cell><cell>827 / 917</cell></row><row><cell>ES</cell><cell>454041</cell><cell>28.2</cell><cell cols="2">2368 23.6</cell><cell cols="4">303 / 525 251 / 347 990 / 217 1538 / 1144</cell></row><row><cell>FI</cell><cell>55344</cell><cell>3.4</cell><cell>483</cell><cell>4.8</cell><cell>89 / 977</cell><cell cols="2">60 / 520 136 / 138</cell><cell>229 / 709</cell></row><row><cell>FR</cell><cell>129804</cell><cell>8.1</cell><cell>946</cell><cell>9.4</cell><cell>91 / 262</cell><cell cols="2">76 / 178 277 / 144</cell><cell>440 / 724</cell></row><row><cell>IT</cell><cell>157558</cell><cell>9.7</cell><cell>809</cell><cell>8.0</cell><cell>115 / 374</cell><cell cols="2">92 / 224 329 / 144</cell><cell>529 / 721</cell></row><row><cell>NL</cell><cell>190605</cell><cell>11.8</cell><cell cols="2">1577 15.7</cell><cell cols="4">161 / 683 147 / 575 469 / 191 759 / 1061</cell></row><row><cell>RU</cell><cell>16715</cell><cell>1.0</cell><cell>151</cell><cell>1.5</cell><cell>25 / 253</cell><cell>25 / 253</cell><cell>44 / 136</cell><cell>86 / 569</cell></row><row><cell>SV</cell><cell>142819</cell><cell>8.9</cell><cell>889</cell><cell>8.8</cell><cell>94 / 505</cell><cell cols="2">80 / 361 258 / 162</cell><cell>404 / 863</cell></row><row><cell>total</cell><cell>1608445</cell><cell></cell><cell>10054</cell><cell></cell><cell cols="3">1286 MB 1073 MB 3712 MB</cell><cell>5921 MB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,70.56,289.29,324.59,8.74"><head>Table 2 .</head><label>2</label><figDesc>Summary information about the test collection and index data structures</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,70.56,154.83,422.03,68.74"><head>Table 3 .</head><label>3</label><figDesc>Mean average precision for CLEF 2003 base runs with maximal values highlighted.</figDesc><table coords="5,87.12,154.83,405.47,56.75"><row><cell></cell><cell>DE</cell><cell>EN</cell><cell>ES</cell><cell>FI</cell><cell>FR</cell><cell>IT</cell><cell>NL</cell><cell>RU</cell><cell>SV</cell></row><row><cell>words</cell><cell cols="9">0.4175 0.4988 0.4773 0.3355 0.4590 0.4856 0.4615 0.2550 0.3189</cell></row><row><cell>stems</cell><cell cols="9">0.4604 0.4679 0.5277 0.4357 0.4780 0.5053 0.4594 0.2550 0.3698</cell></row><row><cell cols="10">4-grams 0.5056 0.4692 0.5011 0.5396 0.5244 0.4313 0.4974 0.3276 0.4163</cell></row><row><cell cols="10">5-grams 0.4869 0.4610 0.4695 0.5468 0.4895 0.4568 0.4618 0.3271 0.4137</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,70.56,306.75,438.51,271.72"><head></head><label></label><figDesc>Table 4 and corrected scores are presented on the far right.</figDesc><table coords="5,70.56,341.73,419.12,236.74"><row><cell>run id</cell><cell>MAP</cell><cell cols="7">=Best &gt;=Median Rel. Found Relevant # topics MAP' % change</cell></row><row><cell cols="2">aplmodea 0.4852</cell><cell>2</cell><cell>31</cell><cell>1721</cell><cell>1825</cell><cell>56</cell><cell>0.5210</cell><cell>7.39%</cell></row><row><cell cols="2">aplmodeb 0.4834</cell><cell>2</cell><cell>27</cell><cell>1732</cell><cell></cell><cell></cell><cell>0.5050</cell><cell>4.46%</cell></row><row><cell cols="2">aplmoena 0.4943</cell><cell></cell><cell></cell><cell>977</cell><cell>1006</cell><cell>54</cell><cell>0.5040</cell><cell>1.96%</cell></row><row><cell cols="2">aplmoenb 0.5127</cell><cell></cell><cell></cell><cell>980</cell><cell></cell><cell></cell><cell>0.5074</cell><cell>-1.03%</cell></row><row><cell cols="2">aplmoesa 0.4679</cell><cell>3</cell><cell>32</cell><cell>2226</cell><cell>2368</cell><cell>57</cell><cell>0.5311</cell><cell>13.50%</cell></row><row><cell cols="2">aplmoesb 0.4538</cell><cell>3</cell><cell>32</cell><cell>2215</cell><cell></cell><cell></cell><cell>0.5165</cell><cell>13.82%</cell></row><row><cell cols="2">aplmofia 0.5514</cell><cell>12</cell><cell>31</cell><cell>475</cell><cell>483</cell><cell>45</cell><cell>0.5571</cell><cell>1.03%</cell></row><row><cell cols="2">aplmofib 0.5459</cell><cell>9</cell><cell>31</cell><cell>475</cell><cell></cell><cell></cell><cell>0.5649</cell><cell>3.49%</cell></row><row><cell cols="2">aplmofra 0.5228</cell><cell>9</cell><cell>35</cell><cell>924</cell><cell>946</cell><cell>52</cell><cell>0.5415</cell><cell>3.58%</cell></row><row><cell cols="2">aplmofrb 0.5148</cell><cell>9</cell><cell>37</cell><cell>920</cell><cell></cell><cell></cell><cell>0.5168</cell><cell>0.39%</cell></row><row><cell>aplmoita</cell><cell>0.4620</cell><cell>7</cell><cell>21</cell><cell>776</cell><cell>809</cell><cell>51</cell><cell>0.4784</cell><cell>3.54%</cell></row><row><cell cols="2">aplmoitb 0.4744</cell><cell>8</cell><cell>22</cell><cell>771</cell><cell></cell><cell></cell><cell>0.4982</cell><cell>5.02%</cell></row><row><cell cols="2">aplmonla 0.4817</cell><cell>3</cell><cell>42</cell><cell>1485</cell><cell>1577</cell><cell>56</cell><cell>0.5088</cell><cell>5.63%</cell></row><row><cell cols="2">aplmonlb 0.4709</cell><cell>2</cell><cell>40</cell><cell>1487</cell><cell></cell><cell></cell><cell>0.4841</cell><cell>2.86%</cell></row><row><cell cols="2">aplmorua 0.3389</cell><cell>2</cell><cell>17</cell><cell>115</cell><cell>151</cell><cell>28</cell><cell>0.3728</cell><cell>10.00%</cell></row><row><cell cols="2">aplmorub 0.3282</cell><cell>4</cell><cell>16</cell><cell>113</cell><cell></cell><cell></cell><cell>0.3610</cell><cell>10.00%</cell></row><row><cell cols="2">aplmosva 0.4515</cell><cell>7</cell><cell>36</cell><cell>840</cell><cell>889</cell><cell>53</cell><cell>0.4358</cell><cell>-3.47%</cell></row><row><cell cols="2">aplmosvb 0.4498</cell><cell>6</cell><cell>38</cell><cell>838</cell><cell></cell><cell></cell><cell>0.4310</cell><cell>-4.18%</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,67.08,84.76,445.43,246.21"><head>Table 5 .</head><label>5</label><figDesc>Examples of term-to-term translation</figDesc><table coords="7,67.08,84.76,445.43,234.30"><row><cell></cell><cell>Desired</cell><cell>DEIT</cell><cell></cell><cell>FIDE</cell><cell></cell><cell>FRNL</cell><cell></cell><cell>ITES</cell><cell></cell></row><row><cell></cell><cell>Mapping</cell><cell>DE</cell><cell>IT</cell><cell>FI</cell><cell>DE</cell><cell>FR</cell><cell>NL</cell><cell>IT</cell><cell>ES</cell></row><row><cell>words</cell><cell>milk</cell><cell>milch</cell><cell>latte</cell><cell>maidon</cell><cell>milch</cell><cell>lait</cell><cell>melk</cell><cell>latte</cell><cell>leche</cell></row><row><cell></cell><cell cols="9">olympic olympische olimpico olympialaisiin olympischen olympique olympisch olimpico olimpico</cell></row><row><cell>stems</cell><cell>milk</cell><cell>milch</cell><cell>latt</cell><cell>maido</cell><cell>milch</cell><cell>lait</cell><cell>melk</cell><cell>latt</cell><cell>lech</cell></row><row><cell></cell><cell>olympic</cell><cell>olymp</cell><cell>olimp</cell><cell>olymp</cell><cell>olymp</cell><cell cols="2">olymp olympisch</cell><cell>olimp</cell><cell>olimp</cell></row><row><cell>4-</cell><cell>first 4-gram</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>grams</cell><cell>(milk)</cell><cell>milc</cell><cell>latt</cell><cell>maid</cell><cell>land</cell><cell>lait</cell><cell>melk</cell><cell>latt</cell><cell>lech</cell></row><row><cell></cell><cell>last 4-gram</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(milk)</cell><cell>ilch</cell><cell>latt</cell><cell>idon</cell><cell>milc</cell><cell>lait</cell><cell>melk</cell><cell>atte</cell><cell>acte</cell></row><row><cell></cell><cell>first 4-gram</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(olympic)</cell><cell>olym</cell><cell>olim</cell><cell>olym</cell><cell>olym</cell><cell>olym</cell><cell>olym</cell><cell>olim</cell><cell>olim</cell></row><row><cell></cell><cell>last 4-gram</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(olympic)</cell><cell>sche</cell><cell>rope</cell><cell>siin</cell><cell>n_au</cell><cell>ique</cell><cell>isch</cell><cell>pico</cell><cell>pico</cell></row><row><cell>5-</cell><cell>first 5-gram</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>grams</cell><cell>(milk)</cell><cell>milch</cell><cell>_latt</cell><cell>maido</cell><cell>milch</cell><cell>_lait</cell><cell>_melk</cell><cell>latte</cell><cell>leche</cell></row><row><cell></cell><cell>last 5-gram</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(milk)</cell><cell>milch</cell><cell>_latt</cell><cell>aidon</cell><cell>milch</cell><cell>lait_</cell><cell>_melk</cell><cell>latte</cell><cell>leche</cell></row><row><cell></cell><cell>first 5-gram</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(olympic)</cell><cell>olymp</cell><cell>olimp</cell><cell>olymp</cell><cell>olymp</cell><cell>olymp</cell><cell>_olym</cell><cell>olimp</cell><cell>olimp</cell></row><row><cell></cell><cell>last 5-gram</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(olympic)</cell><cell>ische</cell><cell>urope</cell><cell>isiin</cell><cell>ichen</cell><cell>pique</cell><cell cols="2">pisch mpico</cell><cell>_olim</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,93.00,518.01,417.25,169.36"><head>Table 6 .</head><label>6</label><figDesc>Official results for bilingual task.</figDesc><table coords="7,93.00,518.01,417.25,169.36"><row><cell>Sierra Leone e i</cell><cell></cell><cell cols="2">combattimenti militare ribelli rivoluzionario</cell><cell>Tokenization &amp; Translation</cell></row><row><cell></cell><cell></cell><cell>guerriglieri</cell><cell>leone</cell></row><row><cell></cell><cell></cell><cell>diamanti sierra</cell></row><row><cell></cell><cell></cell><cell>diamantifero</cell><cell>…</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Words</cell><cell>N-grams</cell></row><row><cell></cell><cell>combates</cell><cell>militares</cell></row><row><cell>ES docs</cell><cell cols="2">ribeldes rivolucionario guerriglieri leona diamantes sierra diamantes …</cell><cell>_comb, comba, ebate, … _sier, sierra, erra_, erril, … _diam, diama, … milit, itari, …</cell></row><row><cell></cell><cell></cell><cell></cell><cell>…</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,117.96,167.91,343.72,80.14"><head>Table 7</head><label>7</label><figDesc></figDesc><table coords="9,277.94,167.91,7.26,8.74"><row><cell>).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,70.56,251.31,169.89,8.74"><head>Table 7 .</head><label>7</label><figDesc>APL results for multilingual task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="9,70.56,380.01,349.76,104.68"><head>Table 8 .</head><label>8</label><figDesc>Submissions for the Cross-Language Spoken Document Evaluation</figDesc><table coords="9,416.56,380.01,3.76,8.74"><row><cell>.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>This work should not be taken as an argument against language resources, but rather as further evidence that knowledge-light methods can be quite effective, when optimized. We are particularly excited about the use of non-word translation (i.e., using direct n-gram translation) as this appears to have the potential to avoid several pitfalls that plague dictionary-based translation of words.</p><p>We are still analyzing our results from the multilingual and spoken-document tracks and hope to report on them more fully in our revised paper.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,88.57,98.62,420.48,7.85;10,70.56,108.94,290.80,7.85" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,142.73,98.62,264.93,7.85">Char_align: A program for aligning parallel texts at the character level</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,420.90,98.62,88.15,7.85;10,70.56,108.94,234.57,7.85">Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 31st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.57,119.32,420.47,7.85;10,70.56,129.64,177.86,7.85" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,142.68,119.32,194.44,7.85">Using Language Models for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>The Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Center for Telematics and Information Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph. D. Thesis</note>
</biblStruct>

<biblStruct coords="10,88.58,139.96,420.48,7.85;10,70.56,150.34,317.28,7.85" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,186.61,139.96,258.43,7.85">Interpolated Estimation of Markov Source Parameters from Sparse Data</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,142.80,150.34,113.74,7.85">Pattern Recognition in Practice</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Gelsema</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Kanal</surname></persName>
		</editor>
		<meeting><address><addrLine>North Holland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page" from="381" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.57,160.66,420.47,7.85;10,70.56,170.98,9.00,7.85;10,79.56,168.92,4.01,5.23;10,86.28,170.98,422.76,7.85;10,70.56,181.36,38.35,7.85" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,193.38,160.66,212.12,7.85">Improving Two-Stage Ad-Hoc Retrieval for Short Queries</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,426.30,160.66,82.74,7.85;10,70.56,170.98,9.00,7.85;10,79.56,168.92,4.01,5.23;10,86.28,170.98,385.79,7.85">the Proceedings of the 21 st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-98)</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="250" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.59,191.68,420.53,7.85;10,70.56,202.00,365.10,7.85" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,202.99,191.68,88.75,7.85">Single N-gram Stemming</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,363.66,191.68,145.47,7.85;10,70.56,202.00,338.98,7.85">Proceedings of the Twenty-Sixth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the Twenty-Sixth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.58,212.38,420.46,7.85;10,70.56,222.70,83.31,7.85" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,209.30,212.38,151.92,7.85">Scalable Multilingual Information Access</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,438.84,212.38,70.20,7.85;10,70.56,222.70,79.01,7.85">Proceedings of the CLEF 2002 Workshop</title>
		<meeting>the CLEF 2002 Workshop</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.57,233.02,420.44,7.85;10,70.56,243.40,438.51,7.85;10,70.56,253.72,220.21,7.85" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,203.91,233.02,305.10,7.85;10,70.56,243.40,34.60,7.85">Comparing Cross-Language Query Expansion Techniques by Degrading Translation Resources</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,130.83,243.40,378.24,7.85;10,70.56,253.72,77.14,7.85">the Proceedings of the 25th Annual International Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="159" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.58,264.04,420.49,7.85;10,70.56,274.42,89.97,7.85" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,205.18,264.04,257.40,7.85">Character N-gram Tokenization for European Language Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>To appear in Information Retrieval</note>
</biblStruct>

<biblStruct coords="10,88.59,284.74,420.46,7.85;10,70.56,295.06,9.00,7.85;10,79.56,293.00,6.00,5.23;10,88.14,295.06,420.91,7.85;10,70.56,305.44,109.61,7.85" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,229.90,284.74,190.84,7.85">A hidden Markov model information retrieval system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,440.88,284.74,68.16,7.85;10,70.56,295.06,9.00,7.85;10,79.56,293.00,6.00,5.23;10,88.14,295.06,379.06,7.85">Proceedings of the 22 nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 22 nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Berkeley, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.60,315.76,420.48,7.85;10,70.56,326.14,323.25,7.85" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,273.72,315.76,235.36,7.85;10,70.56,326.14,104.60,7.85">Performance and Scalability of a Large-Scale N-gram Based Information Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nicholas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,195.22,326.14,122.22,7.85">the Journal of Digital Information</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2000-01">January 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.57,336.46,420.45,7.85;10,70.56,346.78,120.76,7.85" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,234.11,336.46,161.45,7.85">The University of Amsterdam at CLEF 2002</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,406.14,336.46,102.89,7.85;10,70.56,346.78,54.71,7.85">Working Notes of the CLEF 2002 Workshop</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.58,357.16,420.47,7.85;10,70.56,367.48,309.16,7.85" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,299.30,357.16,209.75,7.85;10,70.56,367.48,154.70,7.85">Dictionary-Based Cross-Language Information Retrieval: Problems, Methods, and Research Findings</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pirkola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hedlund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Keskusalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,234.84,367.48,77.14,7.85">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="209" to="230" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.57,377.80,420.46,7.85;10,70.56,388.18,89.70,7.85" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,133.55,377.80,179.60,7.85">Snowball: A Language for Stemming Algorithms</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://snowball.tartarus.org/texts/introduction.html" />
		<imprint>
			<date type="published" when="2003-03-13">13 March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.57,398.50,420.47,7.85;10,70.56,408.82,199.49,7.85" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,298.11,398.50,166.87,7.85">Cross-language Retrieval at Twente and TNO</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Reidsma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,478.02,398.50,31.02,7.85;10,70.56,408.82,124.42,7.85">Working Notes of the CLEF 2002 Workshop</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="111" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.57,419.20,420.46,7.85;10,70.56,429.52,139.77,7.85" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,122.73,419.20,293.89,7.85">Cross-language information retrieval: experiments based on CLEF 2000 corpora</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,423.18,419.20,85.85,7.85;10,70.56,429.52,60.59,7.85">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="115" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.57,439.84,420.47,7.85;10,70.56,450.22,201.05,7.85" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,147.42,439.84,319.79,7.85">Experiments in 8 European Languages with Hummingbird SearchServer at CLEF 2002</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,478.02,439.84,31.02,7.85;10,70.56,450.22,124.42,7.85">Working Notes of the CLEF 2002 Workshop</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">214</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.58,460.54,420.47,7.85;10,70.56,470.86,438.51,7.85;10,70.56,481.24,151.43,7.85" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,185.92,460.54,323.12,7.85;10,70.56,470.86,32.50,7.85">A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.70,470.86,398.37,7.85;10,70.56,481.24,77.14,7.85">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
