<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,197.44,120.45,218.98,11.90">Clairvoyance CLEF-2003 Experiments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,223.84,150.79,27.28,8.44"><forename type="first">Yan</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clairvoyance Corporation</orgName>
								<address>
									<addrLine>5001 Baum Boulevard, Suite 700</addrLine>
									<postCode>15213-1854</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.16,150.79,65.72,8.44"><forename type="first">Greg</forename><surname>Grefenstette</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clairvoyance Corporation</orgName>
								<address>
									<addrLine>5001 Baum Boulevard, Suite 700</addrLine>
									<postCode>15213-1854</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.16,150.79,59.81,8.44"><forename type="first">David</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clairvoyance Corporation</orgName>
								<address>
									<addrLine>5001 Baum Boulevard, Suite 700</addrLine>
									<postCode>15213-1854</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,197.44,120.45,218.98,11.90">Clairvoyance CLEF-2003 Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CF735E3E9C773C025363527E963010F7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In CLEF 2003, Clairvoyance participated in the bilingual retrieval track with the German and Italian language pair. As we did not have any German-to-Italian translation resources, we used the Babel Fish translation service provided by Altavista.com for translating German topics into Italian, with English as a pivot language. Then the translated Italian topics were used for retrieving Italian documents from the Italian document collection. The translated Italian topics and the document collections were indexed using three different kinds of units: (1) linguistically meaningful units, (2) character 6-grams, and (3) a combination of 1 and 2. We submitted three automatic runs with the three indexing units.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Clairvoyance participated in the CLEF 2003 bilingual retrieval track using the German and Italian language pair. As we did not have German-to-Italian translation resources, we used the free Babel Fish translation service provided by Altavista.com for translating German topics into Italian, with English as a pivot language. The resulting translated Italian topics were used for retrieving Italian documents from the Italian document collection. The translated Italian topics and the document collections were indexed using three different kinds of features:</p><p>(1) linguistically meaningful units (e.g., words and NPs), (2) character 6-grams, and (3) a combination of 1 and 2. We submitted three automatic runs, each based on one of the three indexing units. In the following sections, we describe the details of our submission and present the performance results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CLARIT Cross-Language Information Retrieval</head><p>In CLEF 2003, we adopted query translation as the means for bridging the language gap between the query language and the document language for cross-language information retrieval. For German-to-Italian information retrieval, first, a German query string was translated into Italian via machine translation; then the translated Italian topics were used for retrieving Italian documents from the Italian document collection. For query and document processing, we used the CLARIT system <ref type="bibr" coords="1,330.40,507.91,10.13,8.44" target="#b0">[1]</ref>, in particular, those components encompassing a newly developed Italian NLP module (for extracting Italian phrases), indexing (term weighting and phrasal decomposition), retrieval, and "thesaurus extraction" (for extracting terms to support pseudo relevance feedback).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query Translation via a Pivot Language</head><p>The Babel Fish translation service (altavista.com) provides translation between selected language pairs including German-to-English and English-to-Italian. It does not provide translation service between German and Italian directly. So we used English as a pivot language, first translating the German topics to English and then translating the English topics into Italian. As an illustration of typical results for this process, Figure <ref type="figure" coords="1,481.00,612.55,4.69,8.44" target="#fig_0">1</ref> provides the translations from Babel Fish for Topic 141.</p><p>Even though there was increased degradation in query quality after translation, we felt that, except for translation of proper names, the quality of the translation from German to English and from English to Italian by Babel Fish was adequate for the purpose of cross-language information retrieval. We quantitatively evaluate this impression in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Italian Topic Processing</head><p>Once the topics were translated into Italian, we extracted two types of terms from the topics: (1) linguistically meaningful units or character n-grams.</p><p>To extract linguistically meaningful units, we used CLARIT Italian NLP. This NLP module makes use of a lexicon and finite-state grammar for extracting phrases such as NPs. The lexicon is based on the Multext Italian (2) English translation of (1) by Babel Fish:</p><p>Letter bomb for gravel farmer. Find information about the explosion of a letter bomb in the studio of the host Arabella gravel farmer with the television station PRO7.</p><p>(3) Ideal English topic from CLEF-2003:</p><p>Letter Bomb for Kiesbauer Find information on the explosion of a letter bomb in the studio of the TV channel PRO7 presenter Arabella Kiesbauer.</p><p>(4) Italian translation of (2) by Babel Fish:</p><p>Bomba della lettera per il coltivatore della ghiaia. Trovi le informazioni sull'esplosione di una bomba della lettera nell'studio del coltivatore della ghiaia di Arabella ospite con la stazione PRO7 della televisione.</p><p>(5) Ideal Italian topic from CLEF-2003:</p><p>Lettera Bomba per Kiesbauer Recupera le informazioni relative all'esplosione di una lettera bomba nello studio della presentatrice della rete televisiva PRO7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>Topic 141 and its translations from Babel Fish lexicon <ref type="foot" coords="2,120.76,363.15,3.05,5.49" target="#foot_0">1</ref> , which was expanded by adding punctuations and special characters. In addition, entries with accented vowels were duplicated by substituting the accented vowels with their corresponding unaccented vowels followed by an apostrophe ("'"). The final lexicon contained about 135,000 entries. An Italian stop word list<ref type="foot" coords="2,515.44,384.87,3.05,5.49" target="#foot_1">2</ref> , which contained 433 entries, was used to filter out stop words. The grammar specified the rules for constructing phrases, especially NPs, and morphological normalization rules for normalizing morphological variants to their root forms, e.g., "previsto" to "prever". In CLEF 2003 experiments, we extracted Adjectives, Verbs, and NPs as indexing terms.</p><p>Another way to construct terms is to use overlapping character n-grams. We have observed that our lexiconbased term extraction did not have complete coverage for morphological normalization. The n-gram approach we adopted was aimed at mitigating such an effect. For the submissions, we have used overlapping 6-grams, as it was previously reported to be effective <ref type="bibr" coords="2,249.52,484.87,10.13,8.44" target="#b1">[2]</ref>. Spaces and punctuations were included in the character 6-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CLARIT Indexing and Retrieval</head><p>CLARIT indexing involves statistical analysis of a text corpus and construction of an inverted index, with each index entry specifying the index word and a list of texts. CLARIT allows the index to be built upon full documents or variable-length subdocuments. We used subdocuments as the basis for indexing and document scoring in our experiments. The size of a subdocument was in the range of 8 sentences to 12 sentences.</p><p>CLARIT retrieval is based on the vector space retrieval model. Various similarity measures are supported in the model. For CLEF 2003, we used the dot product function for computing similarities between a query and a document: where W P (t) is the weight associated with the query term t and W D (t) is the weight associated with the term t in the document D. The two weights were computed as follows:</p><formula xml:id="formula_0" coords="2,178.96,603.21,131.36,16.08">). ( ) ( ) , ( t W t W D P sim D D P t P ⋅ = ∩ ∈</formula><p>where IDF and TF are standard inverse document frequency and term frequency statistics, respectively. IDF(t) was computed with the target corpus for retrieval. The coefficient C(t) is an "importance coefficient", which can be modified either manually by the user or automatically by the system (e.g., updated during feedback).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Post-Translation Query Expansion</head><p>Query expansion through (pseudo) relevance feedback has proved to be effective for improving IR performance <ref type="bibr" coords="3,93.04,228.31,10.04,8.44" target="#b2">[3]</ref>. We used pseudo relevance feedback for augmenting the queries. After retrieving some documents for a given topic from the target corpus, we took a set of top ranked documents, regarding them as relevant documents to the query, and extracted terms from the these documents. The terms were ranked based on the following formula:</p><p>where N is the number of documents in the target corpus, N t is the number of documents in the corpus that contain term t, R is the number of documents for feedback that are (presumed to be) relevant to the topic, and R t is the number of documents that are (presumed to be) relevant to the topic and contain term t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We submitted three automatic runs to CLEF 2003. All the queries used the title and description fields (Ttitle+Description) of the topics provided by CLEF 2003. The results presented below are based on relevance judgments of 42 topics, which have relevant documents in the Italian corpus. The three runs were:</p><p>• ccwrd: with linguistically meaningful units as indexing terms</p><p>• ccngm: with character 6-grams as indexing terms</p><p>• ccmix: a combination of linguistic units and character 6-grams as indexing units With the ccmix run, the combinations were constructed through a simple concatenation of the terms nominated by ccwrd and ccngm. We ran Italian monolingual experiments to obtain the baseline with ideal translations after obtaining the relevance judgments from CLEF 2003.</p><p>All the experiments were run with post-translation pseudo relevance feedback. The feedback-related parameters were based on training over CLEF 2002 topics. The settings for German-to-Italian retrieval were: extracting T=80 terms from the top N=25 retrieved documents with the Prob2 method. For the n-gram based indexing and the mixed model, an additional term cutoff percentage set to P=0.01. For the word-based indexing, the percentage cutoff is set to P=0.25. For Italian monolingual retrieval with words as indexing terms: T=50, N=50, P=0.1. For Italian monolingual retrieval with n-grams and the mixed model as indexing terms: T=80, N=25, P=0.05.  <ref type="bibr" coords="4,401.32,374.47,10.13,8.44" target="#b3">[4]</ref>, the comparison gives us a rough estimate of the quality of the translation module. Translation from English to Italian decreased performance in the range of 16.4% to 34.8%, while adding another layer of translation from German to English decreased performance further by 16.1% to 24.2%. This shows that translation service such as Babel Fish still needs to be improved for better CLIR performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,93.04,119.23,169.05,8.44;2,93.04,132.64,100.76,7.84;2,93.04,142.36,427.78,7.84;2,93.04,152.08,84.80,7.84"><head>( 1 )</head><label>1</label><figDesc>Original German topic from CLEF-2003: Briefbombe für Kiesbauer . Finde Informationen über die Explosion einer Briefbombe im Studio der Moderatorin Arabella Kiesbauer beim Fernsehsender PRO7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,93.04,581.95,427.93,148.48"><head>Table 1</head><label>1</label><figDesc>presents the results for our submitted runs, and Table2presents results for our training runs with CLEF 2002 topics. The monolingual baselines for the ccwrd*, ccngm*, and ccmix* runs are the optimal monolingual runs based on word based indexing, 6-gram based indexing, and mixed indexing, respectively. While the 6-gram based indexing produced higher average precision compared with the word based indexing for the CLEF 2002 topics, it significantly underperformed word based indexing for CLEF 2003 topics. Further examination is required to account for the difference in behavior of the two indexing methods for the two top sets.</figDesc><table coords="3,98.08,667.75,348.48,62.68"><row><cell>Run ID</cell><cell>Indexing Units</cell><cell>Recall</cell><cell>AP</cell><cell>% mono AP</cell></row><row><cell>ccwrd</cell><cell>Adj+VP+NPs</cell><cell>541/809</cell><cell>0.2303</cell><cell>67.2% (of 0.3428)</cell></row><row><cell>ccngm</cell><cell>Character 6-grams</cell><cell>456/809</cell><cell>0.1624</cell><cell>54.3% (of 0.2993)</cell></row><row><cell>ccmix</cell><cell>Adj+VP+NPs, character 6-grams</cell><cell>505/809</cell><cell>0.2098</cell><cell>61.7% (of 0.3402)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,93.04,120.08,425.70,630.75"><head>Table 1 :</head><label>1</label><figDesc>German-to-Italian retrieval performance with CLEF 2003 topics. All three runs are our submitted runs.</figDesc><table coords="3,144.40,120.08,284.76,182.52"><row><cell>W</cell><cell>D</cell><cell>(</cell><cell>t</cell><cell>)</cell><cell>=</cell><cell cols="2">TF</cell><cell cols="2">D</cell><cell cols="2">(</cell><cell>t</cell><cell>)</cell><cell>⋅</cell><cell cols="3">IDF</cell><cell></cell><cell>(</cell><cell>t</cell><cell>).</cell></row><row><cell>W</cell><cell>P</cell><cell>(</cell><cell>t</cell><cell>)</cell><cell>=</cell><cell>C</cell><cell>(</cell><cell>t</cell><cell>)</cell><cell></cell><cell cols="2">⋅</cell><cell cols="2">TF</cell><cell>P</cell><cell>(</cell><cell>t</cell><cell>)</cell><cell>⋅</cell><cell cols="2">IDF</cell><cell>(</cell><cell>t</cell><cell>)</cell></row><row><cell>Prob2(t)</cell><cell cols="2">=</cell><cell cols="4">log(R</cell><cell>t</cell><cell></cell><cell cols="2">+</cell><cell cols="3">1)</cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell cols="4">log(</cell><cell>N N</cell><cell>t</cell><cell>--</cell><cell>R R</cell><cell>t</cell><cell>+ + 2 1</cell><cell>-</cell><cell>1)</cell><cell>-</cell><cell>log(</cell><cell>+ R R t</cell><cell>1</cell><cell>-</cell><cell>1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,93.04,197.59,332.12,115.48"><head>Table 2 :</head><label>2</label><figDesc>German-to-Italian retrieval performance with CLEF 2002 topics.</figDesc><table coords="4,98.08,217.99,327.08,95.08"><row><cell>Topics</cell><cell>2002 Avg. Prec</cell><cell>2003 Avg. Prec</cell></row><row><cell>(1) Translated English (from German) to Italian</cell><cell>0.1549</cell><cell>0.1748</cell></row><row><cell>Performance change compared with (2)</cell><cell>(-24.2%)</cell><cell>(-16.1%)</cell></row><row><cell>Performance change compared with (3)</cell><cell>(-36.7%)</cell><cell>(-45.3%)</cell></row><row><cell>(2) Ideal English to Italian</cell><cell>0.2048</cell><cell>0.2083</cell></row><row><cell>Performance change compared with (3)</cell><cell>(-16.4%)</cell><cell>(-34.8%)</cell></row><row><cell>(3) Ideal Italian</cell><cell>0.2449</cell><cell>0.3197</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,93.04,324.91,427.93,58.00"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison between different versions of topicsTable 3 presents a comparison between different versions of the Italian topics for CLEF 2002 and CLEF 2003 topics. The average precision statistics were computed with word based indexing and with no feedback. Even through comparing different topic statements is not justified methodologically</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,97.96,745.89,223.28,7.63"><p>http://www.lpl.univ-aix.fr/projects/multext/LEX/LEX.SmpIt.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,97.96,755.61,155.48,7.63"><p>Obtained from http://www.unine.ch/Info/clef/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="4">Conclusions</head><p>Due to the lack of resources, our participation in CLEF 2003 was limited. We succeeded in submitting three runs for German-to-Italian retrieval, examining word based indexing and n-gram based indexing. Our results with CLEF 2002 and CLEF 2003 did not provide firm evidence of which indexing method is better. Future analysis is required in this direction.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="4,107.44,531.79,413.34,8.44;4,93.04,542.59,124.38,8.44" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,236.44,531.79,111.48,8.44">CLARIT-TREC Experiments</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G</forename><surname>Lefferts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,358.72,531.79,157.39,8.44">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="395" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,107.68,564.31,413.17,8.44;4,93.04,575.11,210.90,8.44" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,237.04,564.31,159.38,8.44">Scalable Multilingual Information Access</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,488.56,564.31,32.29,8.44;4,93.04,575.11,133.47,8.44">Working Notes for the CLEF 2002 Workshop</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="133" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,108.16,596.71,412.62,8.44;4,93.04,607.63,427.74,8.44;4,93.04,618.43,61.14,8.44" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,246.76,596.71,241.19,8.44">Statistical Methods for Cross-Language Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,172.36,607.63,147.68,8.44">Cross-Language Information Retrieval</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Grefenstette</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="23" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,109.48,640.15,411.18,8.44;4,93.04,650.83,427.77,8.44;4,93.04,661.75,331.02,8.44" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,164.56,640.15,197.76,8.44">The Philosophy of Information Retrieval Evaluation</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,182.08,650.83,338.73,8.44;4,93.04,661.75,57.03,8.44">Evaluation of Cross-Language Information Retrieval Systems: Proceedings of the CLEF 2001 Workshop</title>
		<title level="s" coord="4,157.00,661.75,133.40,8.44">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="355" to="370" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
