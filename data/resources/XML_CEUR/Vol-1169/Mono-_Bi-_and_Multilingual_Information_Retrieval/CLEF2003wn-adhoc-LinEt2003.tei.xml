<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,120.24,84.85,354.87,12.19">Merging Results by Using Predicted Retrieval Effectiveness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,225.36,119.61,65.01,8.74"><forename type="first">Wen-Cheng</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University Taipei</orgName>
								<address>
									<country key="TW">TAIWAN</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,309.95,119.61,59.94,8.74"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
							<email>hh_chen@csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University Taipei</orgName>
								<address>
									<country key="TW">TAIWAN</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,120.24,84.85,354.87,12.19">Merging Results by Using Predicted Retrieval Effectiveness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2D11A77364200B9CBE0FF15E58882BE1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we proposed several merging strategies to merge the result lists of each intermediate runs in distributed MLIR. The prediction of retrieval effectiveness was used to adjust the similarity scores of documents in the result lists. We introduced three factors affecting the retrieval effectiveness, i.e., the degree of translation ambiguity, the number of unknown words and the number of relevant documents in a collection for a given query. The results show that the normalizedby-top-k merging with translation penalty and collection weight outperforms the other merging strategies except the raw-score merging.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multilingual Information Retrieval abbreviated as MLIR facilitates the uses of queries in one language to access documents in various languages. Most of the previous approaches <ref type="bibr" coords="1,340.51,342.27,107.58,8.74" target="#b7">(Oard and Diekema, 1998)</ref> focused on how to unify the language usages in queries and documents. The adaptation of traditional information retrieval systems has been considered. Query translation and document translation methods have been introduced. The resources used in the translation have been explored.</p><p>In real world, multilingual document collections are distributed from various resources, and managed by information retrieval system of various architectures. How to integrate the results from heterogeneous resources is one of the major issues in MLIR. Merging result lists of individual languages is a commonly adopted approach. Document collections of each language are indexed and retrieved separately, and the result lists of each document collection are merged into a multilingual result list. The goal of result lists merging is to include as more relevant documents as possible in the final result list and to make relevant documents have higher ranks. Several attempts were done on this problem <ref type="bibr" coords="1,249.70,457.17,54.36,8.74" target="#b8">(Peters, 2002)</ref>. The simplest merging method is raw-score merging, which sorts all the documents by their original similarity scores, and then selects the top ranked documents. The second approach, round-robin merging, interleaves the results of each run based on the rank of each document. The third approach is normalized-score merging. For each topic, the similarity score of each document is divided by the maximum score in each result list. After adjusting scores, all results are put into a pool and sorted by the normalized score. <ref type="bibr" coords="1,90.90,526.11,31.09,8.74">Lin and</ref><ref type="bibr" coords="1,124.84,526.11,85.14,8.74">Chen (2002a, 2002b)</ref> proposed normalized-by-top-k merging to avoid the drawback of normalizedscore merging. Translation penalty is also considered during merging result lists. The performance of normalized-by-top-k with translation penalty is similar to that of raw-score merging. <ref type="bibr" coords="1,429.88,549.09,89.95,8.74;1,70.92,560.55,60.40,8.74">Moulinier and Molinasalgado (2002)</ref> proposed collection-weighted normalized score to merge result lists. The normalized collection score is used to adjust the similarity score between a document and a query. Collection score only reflects the similarity of a (translated) query and a document collection. This method could fail if a query is not translated well. <ref type="bibr" coords="1,97.18,595.05,54.85,8.74" target="#b12">Savoy (2002)</ref> used logistic regression to predict the relevance probability of documents according to the document score and the logarithm of the rank. This method does not consider the quality of query translation either. Furthermore, the relationship between the rank and the relevance of a document is not strong. <ref type="bibr" coords="1,483.62,618.03,40.87,8.74;1,70.92,629.49,129.71,8.74" target="#b0">Braschler, Göhring and Schäuble (2002)</ref> proposed feedback merging that interleaves the results according to the propositions of the predicted amount of relevant documents in each document collection. The amount of relevant information was estimated by the portion of overlap between the original query and the ideal query constructed from the top ranked documents. The experimental results showed that feedback merging had little impact.</p><p>In this paper, we will explore several merging strategies. The basic idea of our merging strategies is adjusting the similarity scores of documents in each result list to make them more comparable and to reflect the confidence in retrieval effectiveness. We assume that the importance of each intermediate run depends on their retrieval performance. We introduced three factors affecting the retrieval effectiveness, i.e., the degree of translation ambiguity, the number of unknown words and the number of relevant documents in a collection for a given query. The rest of this paper is organized as follows. Section 2 describes our merging strategies. Section 3 shows the IR model and query translation technique. Section 4 discusses the experimental results. Section 5 concludes the remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Merging Strategies</head><p>We aim to include as more relevant documents as possible in the final result list and to make relevant documents have higher ranks during merging. If a result list contains many relevant documents in the top ranks, i.e., it has good performance, the top ranked documents should be included in the final result list. On the other hand, if a result list has few or even no relevant documents, the final result list should not contain many documents from this list. Thus, the higher performance an individual run has, the more important it is. However, without the priori knowledge of a query in advance, it is challenging to predict the performance of an individual run for each document collection. The similarity score between a document and a query is one of a few clues that are common used. A document with higher similarity score seems to be more relevant to a specific query. Because document collections are various and the underlying IR systems may be different, the similarity scores of a query with different collections cannot be compared directly. The basic idea of our merging strategies is adjusting the similarity scores of documents in each result list to make them more comparable and to reflect the confidence in retrieval effectiveness. The characteristics of the underlying IR model, the effects of the query translation and the statistics of individual document collection will be addressed in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Normalized by Top K</head><p>Similarity scores reported by different information retrieval systems may differ a lot from each other. In vectorbased IR model, the similarity score defined by cosine formula ranges from 0 to 1, but the score may be much larger than 1 when Okapi system <ref type="bibr" coords="2,215.34,331.11,102.88,8.74" target="#b9">(Robertson, et al., 1998)</ref> is used. It is obvious that the scores cannot be compared directly. Thus, similarity scores have to be normalized to the same range to make them comparable at the first step. The approach of normalized-score merging maps the similarity scores of different result lists to the values within the same range. The major drawback is: if the maximum score is much higher than the second one in the same result list, the normalized-score of the document at rank 2 would be made lower even if its original score is high. Thus, the final rank of this document might be lower than that of the top ranked documents with similar original scores in another result list. A revised score normalization method is proposed as follows. The original score of each document is divided by the average score of top k documents instead of the maximum score. We call this normalized-by-top-k approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Translation Penalty</head><p>Similarity score reflects the degree of similarity between a document and a query. A document with higher similarity score seems to be more relevant to the desired query. However, if the query is not formulated well, e.g., inappropriate translation of a query, a document with high score may still not meet users' information needs.</p><p>When the result lists are merged, those documents that have high, but incorrect scores should not be included in the final result list. Thus, the effectiveness of each individual run has to be considered in the merging stage.</p><p>When query translation method is used to deal with the unification of language usages in queries and documents, queries are translated into target language and then the target language documents are retrieved. We can predict the multilingual retrieval performance based on the translation quality. Intuitively, using English to access English collection is expected to have better performance than using it to access other collections. Similarly, using a bilingual dictionary of more coverage is expected to be better than using dictionary of less coverage. Less ambiguous queries have also higher tendency to achieve better translation than more ambiguous queries. Normalization in Section 2.1 just reflects the same comparison basis, but does not consider the above issues. Two factors, i.e., the degree of translation ambiguity and the number of unknown words, are used to model the translation performance. For each query, we compute the average number of translation equivalents of query terms and the number of unknown words in each language pair, and use them to compute the weights of each cross-lingual run. The following formula is proposed to determine the weights.</p><formula xml:id="formula_0" coords="2,188.28,658.01,325.22,32.28">              - × +               - × + = i i i i n U c T c c W 1 50 51 3 2 2 1 . (<label>1</label></formula><formula xml:id="formula_1" coords="2,513.50,671.55,3.91,8.74">)</formula><p>where W i is the merging weight of query i in a cross-lingual run, T i is the average number of translation equivalents of query terms in query i, U i is the number of unknown words in query i, n i is the number of query terms in query i, and c 1 , c 2 and c 3 are tunable parameters, and c 1 +c 2 +c 3 =1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Collection Weight of Individual Document Collection</head><p>How many relevant documents there are in a collection for a given query is also an important factor for measuring retrieval effectiveness. If a document collection contains more relevant documents, it could have more contribution to the final result list. <ref type="bibr" coords="3,238.53,113.97,81.43,8.74" target="#b1">Callan, et al. (1995)</ref> proposed CORI net to rank distributed collections of the same language for a query. <ref type="bibr" coords="3,221.69,125.49,155.92,8.74" target="#b6">Moulinier and Molina-salgado (2002)</ref> used collection score to adjust the similarity score between a document and a query.</p><p>In our approach, a collection weight of a document collection with a query is defined as follows. A document collection is viewed as a huge document and represented as a collection vector. The ith element in a collection vector is the document frequency df of the ith index term in the collection. Similarly, the ith element in a query vector is the frequency of the ith index term in the query. Since document collections are in different languages, we do not use inverse collection frequency icf, which is analogous to idf. Cosine similarity formula shown as follows is used to compute the collection weight, which is included to the merging weight.</p><formula xml:id="formula_2" coords="3,218.82,221.60,298.48,86.61">i i i CW c W W × + = 4 ' (2) ∑ ∑ ∑ = = = = × × m j j m j ij m j j ij i df qtf df qtf CW 1 2 1 2 1 (3)</formula><p>where W i ' is the new merging weight of query i in an intermediate run,</p><p>W i is the merging weight described in Section 2.2, CW i is the collection weight of a target collection for query i, c 4 is tunable parameter, qtf ij is the term frequency of index term j in query i, df j is the document frequency of index term j in a target collection, and m is the number of index terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Predicting Retrieval Effectiveness by Linear Regression</head><p>Now three factors, i.e., the degree of translation ambiguity, the number of unknown words and the number of relevant documents for a given query in a document collection, are proposed to determine the retrieval effectiveness. We use linear regression to predict the retrieval effectiveness according to the three factors. The original similarity score of a document is normalized by normalized-by-top-k method, and the score of the predicted precision is added to the normalized score. Documents from all collections are sorted according to the adjusted similarity scores and the top ranked documents are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Query Translation and Document Indexing</head><p>In the experiments, Okapi IR system was adopted to index and retrieve documents. The weighting function was BM25 <ref type="bibr" coords="3,100.68,542.73,101.04,8.74" target="#b9">(Robertson, et al., 1998)</ref>. The document set used in CLEF 2003 Small-multilingual task consists of English, French, German and Spanish. The numbers of documents in English, French, German and Spanish document sets are <ref type="bibr" coords="3,150.28,565.71,164.72,8.74">169,477, 129,806, 294,809 and 454,045</ref>, respectively. The &lt;HEADLINE&gt; and &lt;TEXT&gt; sections in English documents were used for indexing. For Spanish documents, the &lt;TITLE&gt; and &lt;TEXT&gt; sections were used. While indexing French and German documents, the &lt;TITLE&gt;, &lt;TEXT&gt;, &lt;TI&gt;, &lt;LD&gt; and &lt;TX&gt; sections were used. The words in these sections were stemmed, and stopwords were removed. All letters were transformed to the lower cases. We adopted stopword lists and stemmers developed by University of Neuchatel<ref type="foot" coords="3,111.54,620.95,3.24,5.65" target="#foot_0">1</ref>  <ref type="bibr" coords="3,117.30,623.13,55.16,8.74" target="#b11">(Savoy, 2001)</ref>. English queries were used as the source language queries and translated into target languages, i.e., French, German and Spanish. A dictionary-based approach was adopted. For each English query term, we found its translation equivalents by looking up a dictionary. The first two translation equivalents with the highest occurrence frequency in the target language documents were considered as the target language query terms. If a query term does not have any translation equivalents, the original English query term was reserved in the translated query. The dictionaries we used are Ergane English-French, English-German and English-Spanish dictionaries. They are available at http://www.travlang.com/Ergane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We submitted four runs in CLEF 2003 Small-multilingual task. All runs used title and description fields. The details of each run are described in the follows.</p><p>1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NTUm4Topn</head><p>The result lists were merged by normalized-by-top-k merging strategy. The average similarity score of the top 100 documents were used for normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">NTUm4TopnTp</head><p>In this run, translation penalty was considered. The similarity scores of each document were first normalized by the average similarity score of the top 100 documents and then multiplied a weight determined by formula (1). The values of c 1 , c 2 and c 3 were 0, 0.4 and 0.6, respectively. In query translation, an English query term which did not have any translation equivalents by dictionary lookup was also used to retrieve target language documents. If such an English term occurs in the target language documents, it can be viewed as a word similar to the other translated words when the merging weight is computed. Table <ref type="table" coords="4,221.41,235.17,5.01,8.74" target="#tab_0">1</ref> lists the number of English query terms that do not have translation, but occur in target language collection. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NTUm4TopnTpCw</head><p>In this run, the collection weight was also considered. We used formula (2) to adjust the similarity score of each document. The values of parameters were the same as run NTUm4TopTp. The value of c 4 was 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">NTUm4TopnLinear</head><p>We used linear regression to determine the weights of the three variables, including the average number of translation equivalents of query terms, the portion of unknown words in a query and the collection weight of the target collection, to predict the performances of each intermediate run. CLEF 2001 and 2002 test sets were used as training data to estimate the parameters. The original similarity score of a document was normalized by normalized-by-top-k method, and the score of the predicted precision was added to the normalized score. The results of official runs are shown in Table <ref type="table" coords="4,281.62,384.57,3.74,8.74" target="#tab_1">2</ref>. To compare the effectiveness of our approaches with the past merging strategies, we also conducted several unofficial runs that used raw-score merging, normalized-score merging and round-robin merging strategies. The average precision of optimal merging proposed by <ref type="bibr" coords="4,477.51,407.55,50.34,8.74" target="#b2">Chen (2002)</ref> was regarded as an upper-bound, which was used to measure the performances of our merging strategies. The performances of the unofficial runs are also shown in Table <ref type="table" coords="4,317.92,430.53,3.77,8.74" target="#tab_1">2</ref>. The performance relative to optimal merging is enclosed in parentheses. The results show that the performances of the merging strategies we proposed were better than normalized-score merging and round-robin merging, but worse than raw-score merging. The experimental results in CLEF 2002 and NTCIR3 <ref type="bibr" coords="4,273.27,464.97,34.82,8.74">(Lin and</ref><ref type="bibr" coords="4,311.30,464.97,85.31,8.74">Chen, 2002a, 2002b)</ref> show that normalized-by-top-k merging overcomes the drawback of normalized-score merging. In CLEF 2003, the performance of normalizedby-top-k merging was still better than normalized-score merging. The performance dropped down slightly after considering translation penalty. From Table <ref type="table" coords="4,250.91,499.47,3.77,8.74" target="#tab_2">3</ref>, the performance of English-Spanish run was worse than the other intermediate runs, but the merging weights of three cross-lingual runs were similar. This is because that the average number of translation equivalents and the number of unknown words of three cross-lingual runs did not differ too much. After considering the collection weights of each document collection, the performance was improved and was about 7.12% increase to normalized-by-top-k merging. The performance of using the merging weight predicted by linear regression was slightly better than normalized-by-top-k merging, but worse than normalized-by-top-k with translation penalty and collection weight.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Merging problem is critical in distributed multilingual information retrieval. In this paper, we proposed several merging strategies to integrate the result lists of collections in different languages. We assume that the importance of each intermediate run depends on their retrieval performance. We introduced three factors affecting the retrieval effectiveness, i.e., the degree of translation ambiguity, the number of unknown words and the number of relevant documents in a collection for a given query. Normalized-by-top-k avoids the drawback of normalized-score merging. The experimental results show that considering translation penalty and collection weight improves performance. We also used linear regression to predict the retrieval effectiveness. The performance of using the merging weight predicted by linear regression is similar to normalized-by-top-k. The performances of our merging strategies were better than normalized-score merging and round-robin merging, but were worse than raw-score merging in single IR system environment. However, raw-scoring merging is not workable if different information retrieval systems are adopted.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,115.68,603.39,363.94,92.58"><head>Table 1 .</head><label>1</label><figDesc>Number of English query terms without translation but in target language corpora</figDesc><table coords="4,167.82,617.38,258.89,78.59"><row><cell>Language</cell><cell>French</cell><cell>German</cell><cell>Spanish</cell></row><row><cell># query terms</cell><cell>891</cell><cell>891</cell><cell>891</cell></row><row><cell># query terms without translation equivalents</cell><cell>326</cell><cell>322</cell><cell>251</cell></row><row><cell># query terms without translation equivalents but in target language corpora</cell><cell>209 (64.11%)</cell><cell>230 (71.43%)</cell><cell>147 (58.57%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,190.32,73.53,196.49,127.32"><head>Table 2 .</head><label>2</label><figDesc>Performances of merging strategies</figDesc><table coords="5,190.32,86.26,194.95,114.59"><row><cell>Run</cell><cell>Average precision</cell></row><row><cell>NTUm4Topn</cell><cell>0.1489 (60.97%)</cell></row><row><cell>NTUm4TopnTp</cell><cell>0.1478 (60.52%)</cell></row><row><cell>NTUm4TopnTpCw</cell><cell>0.1595 (65.32%)</cell></row><row><cell>NTUm4TopnLinear</cell><cell>0.1516 (62.08%)</cell></row><row><cell>Raw score merging</cell><cell>0.1691 (69.25%)</cell></row><row><cell>Normalized score merging</cell><cell>0.1366 (55.94%)</cell></row><row><cell>Round-robin merging</cell><cell>0.1412 (57.82%)</cell></row><row><cell>Optimum merging</cell><cell>0.2442</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,190.32,217.11,202.74,72.78"><head>Table 3 .</head><label>3</label><figDesc>Performances of intermediate runs</figDesc><table coords="5,190.32,229.84,202.74,60.05"><row><cell>Run</cell><cell># Topic</cell><cell>Average precision</cell></row><row><cell>English -&gt; English</cell><cell>54</cell><cell>0.5063</cell></row><row><cell>English -&gt; French</cell><cell>52</cell><cell>0.2568</cell></row><row><cell>English -&gt; German</cell><cell>56</cell><cell>0.2574</cell></row><row><cell>English -&gt; Spanish</cell><cell>57</cell><cell>0.0797</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,76.68,747.99,122.02,8.74"><p>http://www.unine.ch/info/clef/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,70.92,497.92,453.48,7.85;5,82.26,508.30,137.50,7.85" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,265.06,497.92,93.63,7.85">Eurospider at CLEF 2002</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Göhring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schäuble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,442.02,497.92,82.38,7.85;5,82.26,508.30,79.01,7.85">Working Notes for the CLEF 2002 Workshop</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="127" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.92,518.62,453.46,7.85;5,82.26,528.94,203.35,7.85;5,285.60,526.88,4.68,5.23;5,292.98,528.94,231.43,7.85;5,82.26,539.32,185.01,7.85" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,240.33,518.62,221.91,7.85">Searching Distributed Collections With Inference Networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,205.50,528.94,80.11,7.85;5,285.60,526.88,4.68,5.23;5,292.98,528.94,231.43,7.85;5,82.26,539.32,136.30,7.85">Proceedings of the 18 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Ingwersen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fidel</surname></persName>
		</editor>
		<meeting>the 18 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.92,549.64,267.18,7.85" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,139.51,549.64,176.00,7.85">Cross-language Retrieval Experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,344.34,549.64,180.16,7.85;5,82.26,560.02,99.70,7.85" xml:id="b3">
	<monogr>
		<title level="m" coord="5,417.12,549.64,107.38,7.85;5,82.26,560.02,54.71,7.85">Working Notes for the CLEF 2002 Workshop</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="5" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.92,570.34,453.55,7.85;5,82.26,580.66,442.04,7.85;5,82.26,591.04,118.84,7.85" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,200.82,570.34,106.18,7.85">NTU at NTCIR3 MLIR Task</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,446.34,570.34,78.13,7.85;5,82.26,580.66,320.96,7.85">Working Notes of the Third NTCIR Workshop Meeting. Part II: Cross Lingual Information Retrieval Task</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Kishida</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Ishida</surname></persName>
		</editor>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="101" to="105" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Informatics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.92,601.36,453.60,7.85;5,82.26,611.68,181.90,7.85" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,198.67,601.36,215.73,7.85">Merging Mechanisms in Multilingual Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,493.50,601.36,31.02,7.85;5,82.26,611.68,127.90,7.85">Working Notes for the CLEF 2002 Workshop</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.92,622.06,453.53,7.85;5,82.26,632.38,210.70,7.85" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,230.20,622.06,198.68,7.85">Thomson Legal and Regulatory experiments for CLEF</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Moulinier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Molina-Salgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,82.26,632.38,161.20,7.85">Working Notes for the CLEF 2002 Workshop</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="91" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.92,642.70,453.54,7.85;5,82.26,653.08,128.17,7.85" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,205.92,642.70,142.30,7.85">Cross-Language Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Diekema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,360.24,642.70,164.22,7.85;5,82.26,653.08,40.28,7.85">Annual Review of Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="223" to="256" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.92,663.40,251.79,7.85" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="5,157.20,663.40,161.20,7.85">Working Notes for the CLEF 2002 Workshop</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.92,673.72,453.52,7.85" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="5,264.80,673.72,243.53,7.85">Okapi at TREC-7: automatic ad hoc, filtering, VLC and interactive</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,82.26,684.10,442.14,7.85" xml:id="b10">
	<monogr>
		<title level="m" coord="5,231.96,684.10,233.75,7.85">Proceedings of the Seventh Text REtrieval Conference (TREC-7)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Seventh Text REtrieval Conference (TREC-7)</meeting>
		<imprint>
			<biblScope unit="page" from="253" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.92,704.74,453.68,7.85;5,82.26,715.12,442.14,7.85;5,82.26,725.44,209.63,7.85" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="5,142.45,704.74,320.93,7.85">Report on CLEF-2001 Experiments: Effective Combined Query-Translation Approach</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,254.24,715.12,221.12,7.85">Evaluation of Cross-Language Information Retrieval Systems</title>
		<title level="s" coord="5,82.26,725.44,127.25,7.85">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="27" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.92,735.82,453.47,7.85;5,82.26,746.14,210.70,7.85" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="5,143.86,735.82,295.89,7.85">Report on CLEF-2002 Experiments: Combining Multiple Sources of Evidence</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,82.26,746.14,161.20,7.85">Working Notes for the CLEF 2002 Workshop</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="31" to="46" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
