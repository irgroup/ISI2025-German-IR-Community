<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder>
					<orgName type="full">Physical Sciences Council</orgName>
				</funder>
				<funder ref="#_GR8P3YN #_zXZbykA #_4CCEJzZ #_XmaKV9V #_aAE885n">
					<orgName type="full">NWO</orgName>
				</funder>
				<funder ref="#_5Wf8DKC #_CrsgCKN #_xfHeZys #_eD32xcf">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,100.94,136.74,58.10,10.76"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<email>kamps@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,182.95,136.74,70.07,10.76"><forename type="first">Christof</forename><surname>Monz</surname></persName>
							<email>christof@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,276.93,136.74,82.88,10.76"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
						</author>
						<author>
							<persName coords="1,383.72,136.74,109.26,10.76"><forename type="first">Börkur</forename><surname>Sigurbjörnsson</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Amsterdam at CLEF</orgName>
								<address>
									<postCode>2003</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Language &amp; Inference Technology Group</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Nieuwe ; Achtergracht 166</addrLine>
									<postCode>1018 WV</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">838764AA7A3707EB09E5B2F7BCCC9A25</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our official runs for CLEF 2003. We took part in the monolingual task (for Dutch, Finnish, French, German, Italian, Russian, Spanish, and Swedish), and in the bilingual task (English to Russian, French to Dutch, German to Italian, Italian to Spanish). We also conducted our first experiments for the multilingual task (both multi-4 and multi-8), and took part in the GIRT task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this year's CLEF evaluation exercise we participated in four tasks. We took part in the monolingual tasks for each of the eight non-English languages for which CLEF provides document collections (Dutch, Finnish, French, German, Italian, Russian, Spanish, and Swedish). For the second year running, we took part in the bilingual task, and for the first time, we took part in the multilingual task. We also conducted experiments for the GIRT task.</p><p>Our participation in the monolingual task was motivated by a number of aims. Our first aim was to experiment with a number of linguistically motivated techniques, in particular stemming algorithms for all European languages <ref type="bibr" coords="1,100.68,401.22,15.27,8.97" target="#b14">[15]</ref>. Our second aim was to continue earlier experiments on compound splitting <ref type="bibr" coords="1,429.24,401.22,15.77,8.97" target="#b9">[10,</ref><ref type="bibr" coords="1,447.84,401.22,7.19,8.97" target="#b7">8]</ref>, this time for all the compound rich languages, Dutch, German, Finnish, and Swedish. A third aim was to continue our experiments with knowledge-poor techniques, by using character n-grams. Our final aim was to experiment with combinations of runs, such as the combination of linguistically motivated and knowledge-poor techniques, and the combination of different weighting schemes. In the bilingual task our aim was to evaluate the robustness of our monolingual retrieval results, and to experiment with a variety of translation resources <ref type="bibr" coords="1,369.20,460.99,15.77,8.97" target="#b15">[16,</ref><ref type="bibr" coords="1,388.00,460.99,12.45,8.97" target="#b11">12,</ref><ref type="bibr" coords="1,403.48,460.99,7.19,8.97" target="#b0">1]</ref>. The multilingual task was new to us. Our aims for this task were to experiment with unweighted and weighted combination methods, and with the effect of multiple languages on retrieval effectiveness. We continued our participation in the GIRT task. This year, our aim was to experiment with an improved version of a document reranking strategy, tailored to the presence of classification information in the collection <ref type="bibr" coords="1,289.05,508.81,10.58,8.97" target="#b7">[8]</ref>.</p><p>The paper is organized as follows. In Section 2 we describe the FlexIR system as well as the approaches used for each of the tasks in which we participated. Section 3 describes our official retrieval runs for CLEF 2003. In Section 4 we discuss the results we have obtained. Finally, in Section 5, we offer some conclusions regarding our document retrieval efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Retrieval Approach</head><p>All retrieval runs used FlexIR, an information retrieval system developed at the University of Amsterdam. The main goal underlying FlexIR's design is to facilitate flexible experimentation with a wide variety of retrieval components and techniques. FlexIR is implemented in Perl and supports many types of preprocessing, scoring, indexing, and retrieval tools, which proved to be a major asset for the wide variety of tasks in which we took part this year. Building on last year's experience we continued our work on combining different runs. Last year's focus was on combining runs that were generated using different morphological normalization processes. This year we added a further dimension: in addition to the Lnu.ltc-based vector space model that was used at CLEF 2001 and CLEF 2002, we wanted to experiment with other retrieval models, especially with the Okapi weighting scheme and with language models. Retrieval Models. FlexIR supports several retrieval models, including the standard vector space model, language models, and probabilistic models, all of which were used to obtain combined runs. Combined runs using the vector space model all use the Lnu.ltc weighting scheme <ref type="bibr" coords="2,272.17,97.25,11.62,8.97" target="#b1">[2]</ref> to compute the similarity between a query and a document. For the experiments on which we report in this note, we fixed slope at 0.2; the pivot was set to the average number of unique words per document. We also experimented with a number of alternative weighting schemes. For runs with the Okapi weighting scheme <ref type="bibr" coords="2,205.57,133.12,15.27,8.97" target="#b12">[13]</ref>, we used the following tuning parameters: k 1 = 1.5 and b = 0.55 for Dutch; k 1 = 1.5 and b = 0.55 for German; k 1 = 1.2 and b = 0.50 for Spanish; and k 1 = 0.8 and b = 0.35 for Swedish. For runs with a language model <ref type="bibr" coords="2,182.81,157.03,10.57,8.97" target="#b5">[6]</ref>, we used a uniform query term importance weight of 0.15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Morphological Normalization.</head><p>After CLEF 2002 we carried out extensive experiments with different forms of morphological normalizations for monolingual retrieval in all of the CLEF 2002 languages <ref type="bibr" coords="2,428.54,195.51,10.58,8.97" target="#b6">[7]</ref>. The options considered included word-based runs (where the tokens as they occur in the documents are indexed without processing), stemming (where we used stemmers from the Snowball family of stemmers), lemmatizing (where we used the lemmatizer built into the TreeTagger part-of-speech tagger), and compound splitting (for compound forming languages such as Dutch, Finnish, German, and Swedish). We also experimented with character n-grams (of length 4 and 5). The main lessons learned were two-fold: there is no language for which the best performing run significantly improves over the "split, and stem" run (treating splitting as a no-op for non-compound forming languages); and the hypothesis that 4-gramming is the best strategy is refuted for Spanish only. Notice that the comparisons did not involve combinations of runs, but only, what we call, base runs.</p><p>Stemming -To produce our base runs for CLEF 2003, we followed our own advice <ref type="bibr" coords="2,434.50,303.11,10.58,8.97" target="#b6">[7]</ref>. For all languages we created split-and-stemmed runs as well as n-gram runs. We used the family of Snowball stemming algorithms, available for all the nine languages of the CLEF collections. Snowball is a small string processing language designed for creating stemming algorithms for use in information retrieval <ref type="bibr" coords="2,368.53,338.97,15.27,8.97" target="#b14">[15]</ref>.</p><p>Decompounding -For the compound rich languages, Dutch, German, Finnish, and Swedish, we also apply a decompounding algorithm. We treat all the words occurring in the CLEF corpus as potential base words for decompounding, and also use their associated collection frequencies. We ignore words of length less than four as potential compound parts, thus a compound must have at least length eight. As a safeguard against oversplitting, we only regard compound parts that have a higher collection frequency than the compound itself. We consider linking elements -s-, -e-, and -en-for Dutch; -s-, -n-, -e-, and -en-for German; -s-, -e-, -u-, and -o-for Swedish; and none for Finnish. We prefer a split with no linking element over a split with a linking element, and a split with a single character linker over a two character linker.</p><p>Each document in the collection is analyzed and if a compound is identified, the compound is kept and all of its parts are added to the document. Compounds occurring in a query are analyzed in a similar way: the parts are simply added to the query. Since we expand both the documents and the queries with compound parts, there is no need for compound formation <ref type="bibr" coords="2,191.93,482.44,15.26,8.97" target="#b10">[11]</ref>.</p><p>n-Gramming -Zero-knowledge language independent runs were generated using character n-grams, with n = 5 for Finnish and n = 4 for all other languages; n-grams were not allowed to cross word boundaries.</p><p>Character Encodings. Until CLEF 2003, the languages of the CLEF collections all used the Latin alphabet. The addition of the new CLEF language, Russian, is challenging for the use of a non-Latin alphabet. The Cyrillic characters used in Russian can appear in variety of font encodings. The collection and topics are encoded using the UTF-8 or Unicode character encoding. We converted the UTF-8 encoding into a 1-byte per character encoding KOI8 or KOI8-R (for Kod Obmena Informatsii or Code of Information Exchange). <ref type="foot" coords="2,407.61,578.84,3.69,6.63" target="#foot_0">1</ref> We did all our processing, such as lower-casing, stopping, stemming, and n-gramming, on documents and queries in this KOI8 encoding. Finally, to ensure the proper indexing of the documents using our standard architecture, we converted the resulting documents into the Latin alphabet using the Volapuk transliteration. We processed the Russian queries in the same way as the documents.</p><p>Stopwords. Both topics and documents were stopped using the stopword lists from the Snowball stemming algorithms <ref type="bibr" coords="2,115.76,666.99,15.27,8.97" target="#b14">[15]</ref>, for Finnish we used the Neuchâtel-stoplist <ref type="bibr" coords="2,311.67,666.99,10.58,8.97" target="#b3">[4]</ref>. Additionally, we removed topic specific phrases such as 'Find documents that discuss . . . ' from the queries. We did not use a stop stem or stop n-gram list, but we first used a stop word list, and then stemmed/n-grammed the topics and documents.</p><p>Blind Feedback. Blind feedback was applied to expand the original query with related terms. We experimented with different schemes and settings, depending on the various indexing methods and retrieval models used. For our Lnu.ltc and Okapi runs term weights were recomputed by using the standard Rocchio method <ref type="bibr" coords="3,463.41,97.25,15.27,8.97" target="#b13">[14]</ref>, where we considered the top 10 documents to be relevant and the bottom 500 documents to be non-relevant. We allowed at most 20 terms to be added to the original query.</p><p>Combined Runs. For each of the CLEF 2003 languages we created base runs using a variety of indexing methods (see below). In addition, we used different retrieval models to create further runs (again, see below for details). We then combined our base runs using one of two methods, either a weighted interpolation or a three-way combination, as we will now explain.</p><p>The weighted interpolation was produced as follows. First, we normalized the retrieval status values (RSVs), since different runs may have radically different RSVs. For each run we reranked these values in [0, 1] using:</p><formula xml:id="formula_0" coords="3,254.46,227.61,85.00,23.48">RSV i = RSV i -min i max i -min i ;</formula><p>this is the Min Max Norm considered in <ref type="bibr" coords="3,240.88,260.52,10.57,8.97" target="#b8">[9]</ref>. Next, we assigned new weights to the documents using a linear interpolation factor λ representing the relative weight of a run:</p><formula xml:id="formula_1" coords="3,226.96,291.13,140.01,13.16">RSV new = λ • RSV 1 + (1 -λ) • RSV 2 .</formula><p>For λ = 0.5 this is similar to the simple (but effective) combSUM function used by Fox and Shaw <ref type="bibr" coords="3,486.92,316.31,10.58,8.97" target="#b4">[5]</ref>. The interpolation factors λ were obtained from experiments on the CLEF 2000, 2001, and 2002 data sets (whenever available). When we combined more than two runs, we gave all runs the same relative weight, resulting effectively in the familiar combSUM.</p><p>For the GIRT task, we created alternative base runs based on the usage of the keywords in the collection, and combined these with the text-based runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs</head><p>We submitted a total of 34 retrieval runs: 15 for the monolingual task, 8 for the bilingual task, 3 for the multi-4 task, 5 for the multi-8 task, and 3 for the GIRT task. Below we discuss these runs in some detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Monolingual Runs</head><p>All our monolingual runs used the title and description fields of the topics. Table <ref type="table" coords="3,402.95,487.14,4.98,8.97" target="#tab_0">1</ref> provides an overview of the runs that we submitted for the monolingual task. The third column in Table <ref type="table" coords="3,372.72,499.10,4.98,8.97" target="#tab_0">1</ref> indicates the type of run:</p><p>• (Split+)Stem -topic and document words are stemmed and compounds are split (for Dutch, German, Finnish, Swedish), using the morphological tools described in Section 2. For all eight languages, we use a stemming algorithm from the Snowball family <ref type="bibr" coords="3,282.93,542.93,15.27,8.97" target="#b14">[15]</ref>.</p><p>• n-Gram -both topic and document words are n-grammed, using the settings discussed in Section 2. For Finnish we use 5-grams, and for all other languages we use 4-grams.</p><p>• Combined -two base runs are combined, an n-gram run and a morphological run, using the interpolation factor λ given in the fourth column.</p><p>Additionally, for two languages where we expected the stemming algorithm to be particularly effective, Dutch and Spanish, we submitted the combination of three weighting schemes on the stemmed index (where we use decompounding for Dutch). We combine the run with Lnu.ltc with runs made with Okapi and a language model. Furthermore, we experimented with the Okapi weighting scheme on the stemmed, and decompounded indexes for German and Swedish, and submitted the combination with the 4-gram-run using the Lnu.ltc scheme.</p><p>Finally, we also submitted three base runs for Russian, a word-based run, a stemmed run, and a 4-gram run, all using the the settings discussed in Section 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bilingual Runs</head><p>We submitted a total of 7 bilingual runs, for English to Russian, French to Dutch, German to Italian, and Italian to Spanish. All our bilingual runs used the title and description fields of the topics. For the bilingual runs, we experimented with the WorldLingo machine translation <ref type="bibr" coords="4,294.56,357.41,16.60,8.97" target="#b15">[16]</ref> for translations into Dutch, Italian, and Spanish. For translation into Russian we used the PROMT-Reverso machine translation <ref type="bibr" coords="4,368.16,369.37,15.27,8.97" target="#b11">[12]</ref>. Table <ref type="table" coords="4,109.56,381.32,4.98,8.97" target="#tab_1">2</ref> provides an overview of the runs that we submitted for the bilingual task. The third column in decompounded for Dutch) run with a 4-gram run. Since we put particular interest in the translations to Dutch, we also submitted the two underlying base runs. Finally, we also submitted the stemmed and n-grammed base runs for the translation into Russian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multilingual Runs</head><p>We submitted a total of 8 multilingual runs, three for the small multilingual task and five for the large multilingual task, all using the title and description of the English topic set. For the multilingual runs, we experimented with the WorldLingo machine translation <ref type="bibr" coords="4,219.03,662.03,16.59,8.97" target="#b15">[16]</ref> for translations into Dutch, French, German, Italian, and Spanish. For translation into Swedish we used the first translation mentioned in the Babylon online dictionary <ref type="bibr" coords="4,457.05,673.99,10.58,8.97" target="#b0">[1]</ref>. Table <ref type="table" coords="4,110.79,685.94,4.98,8.97" target="#tab_2">3</ref> provides an overview of the runs that we submitted for the multilingual task. The fourth column in Table <ref type="table" coords="4,106.55,697.90,4.98,8.97" target="#tab_2">3</ref> indicates the document sets used. In effect, we conducted three sets of experiments: (i) on the four language small multilingual set (English, French, German, and Spanish), (ii) on the six languages for which we have an acceptable machine translation (also including Dutch and Italian), and (iii) on the seven languages (also including Swedish, but no Finnish documents) for which we have, at least, an acceptible bilingual dictionary. For each of these experiments, we submitted a number of combined runs, where we used the (unweighted) comb-SUM rule introduced by <ref type="bibr" coords="5,172.60,241.96,10.58,8.97" target="#b4">[5]</ref>. First, we combined a single, uniform run per language, in all cases a 4-gram run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Second, per language we formed a weighted combination of the 4-gram and stemmed run (with decompounding for Dutch, German, and Swedish). We used the following relative weights of the 4-gram run: 0.6 (Dutch), 0.4 (English), 0.7 (French), 0.5 (German), 0.6 (Italian), 0.5 (Spanish), and 0.8 (Swedish). These runs of the different languages were combined using the combSUM rule. Third, we simply formed a pool of all 4-gram and stemmed runs (where we decompounded for Dutch, German, and Swedish) for all languages, and derived a combined run using the combSUM rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The GIRT Task</head><p>We submitted a total of 3 runs for the GIRT task, all using both the German topics and collection. We used the title and description fields of the topics, and used the title and abstract fields of the collection. We experimented with a reranking strategy based on the keywords assigned to the documents, the resulting rerank runs also use the controlled-vocabulary fields in the collection. Table <ref type="table" coords="5,109.89,408.46,4.98,8.97" target="#tab_3">4</ref> provides an overview of the runs that we submitted for the GIRT task. The fourth column in German, although we did not use decompounding. The word-based run serves as a baseline for performance.</p><p>The other two runs experiment with an improved version of our keyword-based reranking strategy introduced at CLEF 2002 <ref type="bibr" coords="5,119.52,543.18,10.58,8.97" target="#b7">[8]</ref>. We calculate vectors for the keywords based on their (co)occurrences in the collection. The main innovation is in the use of higher dimensional vectors for the keywords, for which we use the best reduction onto a 100-dimensional euclidean space. The reranking strategy is as follows. We calculate vectors for all initially retrieved documents, by simply taking the mean of the vectors of keywords assigned to the documents. We calculate a vector for a topic by taking the relevance-weighted mean of the top 10 retrieved documents. We now have a vector for each of the topics, and for each of the retrieved documents. Thus, ignoring the RSV of the retrieved documents, we can simply rerank all documents by increasing euclidean distance between the document and topic vectors. Next, we combine the original text-based similarity scores of the baserun, with the keyword-based distances using the unweighted combSUM rule of <ref type="bibr" coords="5,208.81,638.82,10.58,8.97" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This section summarizes the results of our CLEF 2003 submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Monolingual Results</head><p>Table <ref type="table" coords="5,94.81,737.41,4.98,8.97" target="#tab_4">5</ref> contains our non-interpolated average precision scores for all languages. In addition to the scores for our submitted runs, the table also lists the scores the base runs that were used to generate the combined runs. Both the stemmed runs (with decompounding for the compound-rich languages) and the n-gram runs perform well, with the n-gram runs outperforming the stemmed runs for seven out of eight languages. Only for Italian, the stemmed run performs better than the 4-gram run. This deviating behavior for Italian may be due to the different ways of encoding marked characters in the Italian sub-collections <ref type="bibr" coords="6,333.66,276.50,10.58,8.97" target="#b2">[3]</ref>. The (binary) combination of the stemmed and n-grammed base runs leads to improvements over the best underlying score for seven out of eight languages. Only for Russian, the 4-gram run is somewhat better than the combined run. This may be due to the difference in performance of both underlying base runs. The Snowball stemmer for Russian has no evident effect for the monolingual topics, the score is even a fraction lower than the score of a plain word-based run.</p><p>At this time we do not know yet whether the high scores of the combinations involving an Okapi base run are to the effect of combining or due to the training and fine-tuning that we performed for our Okapi base runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bilingual Results</head><p>To begin with, Table <ref type="table" coords="6,153.91,406.83,4.98,8.97" target="#tab_5">6</ref> shows our MAP scores for the four bilingual sub tasks: French to Dutch, German to Italian, Italian to Spanish, and English to Russian. As for the monolingual runs, both the stemmed (and decompounded for Dutch) and the 4-grammed indexes perform well, with the 4-gram runs outperforming the stemmed runs for three out of four languages. The exception, this time, is Russian where the stemmed run is now better than the 4-gram run. The combination is effective for French to Dutch and German to Italian. For Italian to Spanish, the 4-gram base run (not submitted) scores better than the combined run, and for English to Russian the stemmed run is scoring better than the combined run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>French</head><p>A conclusion on the effectiveness of the Russian stemmer turns out to be premature. Although the stemmer failed to improve retrieval effectiveness for the monolingual Russian task, it turns out to be effective for the bilingual Russian task. Table <ref type="table" coords="6,94.01,737.41,4.98,8.97" target="#tab_6">7</ref> shows the decrease in effectiveness compared to the best monolingual run for the respective target language. The difference ranges from a 20% to a 27% decrease in MAP score. This seems quite acceptable, considering that we used a simple, straightforward machine translation for the bilingual tasks <ref type="bibr" coords="7,392.01,73.34,15.27,8.97" target="#b15">[16]</ref>. This result gives us some confidence in the robustness of the morphological normalization methods employed for building the indexes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dutch</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multilingual Results</head><p>Table <ref type="table" coords="7,94.87,131.97,4.98,8.97" target="#tab_7">8</ref> shows our multilingual MAP scores for the small multilingual task (covering four languages) and for the large multilingual task (covering all eight non-English CLEF languages). For the small multilingual task, first making a weighted combination per language outperforms the unweighted combination of all n-gram and stemmed runs. For the large multilingual task, when using only six of the eight languages, we see the same pattern: first making a weighted combination run per language (not submitted) outperforms the unweighted combination. However, when we include our Swedish results in the large multilingual task, we see that the unweighted combination of all the 4-gram and stemmed base even slightly outperforms the weighted combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi</head><p>Our results show that multilingual retrieval on a subpart of the collection (leaving out one or two languages) can still be an effective strategy. However, the results also indicate that the inclusion of further languages does consistently improve MAP scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results for the GIRT Task</head><p>Table <ref type="table" coords="7,94.61,407.78,4.98,8.97" target="#tab_8">9</ref> contains our MAP scores for the GIRT monolingual task. In addition to the scores for our submitted runs, the table also lists the scores for the base runs that were used to generate the rerank runs. The results for the GIRT tasks show, on the one hand, the effectiveness of stemming and n-gramming approaches over a plain word index. On the other hand, the results show a significant improvement of retrieval effectiveness due to our keyword-based reranking method. The improvement comes on top of the improvement due to blind feedback, and consistent even for high performing base runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GIRT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>The experiments on which we report in this note indicate a number of things. First, morphological normalization does improve retrieval effectiveness, especially for languages that have a more complex morphology than English. We also showed that n-gram-based approaches can be a viable option in the absence of linguistic resources to support deep morphological normalization. Although no panacea, the combination of runs provides a method that may help improve base runs, even high quality base runs. The interpolation factors required for the best gain in performance seem to be fairly robust across topic sets. Moreover, the effectiveness of the unweighted combination of runs is usually close to the weighted combination, and seems to gain in effectiveness the more base runs are available. Our bilingual experiments reconfirmed that a simple machine translation strategy can be effective for bilingual retrieval. The combination of bilingual runs, in turn, leads to an effective strategy for multilingual retrieval. Finally, our results for domain-specific retrieval show the effectiveness of stemming and n-gramming even for specialized collection. Moreover, manually assigned classification information in such scientific collections can be fruitfully exploited for improving retrieval effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,70.19,71.75,453.55,220.75"><head>Table 1 :</head><label>1</label><figDesc>Overview of the monolingual runs submitted. For combined runs column 3 gives the base runs that were combined, and column 4 gives the interpolation factor λ.</figDesc><table coords="4,129.28,71.75,335.13,188.69"><row><cell></cell><cell>Language</cell><cell>Type</cell><cell>Factor</cell></row><row><cell>UAmsC03GeGe4GiSb</cell><cell>DE</cell><cell>4-Gram/Split+stem</cell><cell>0.36</cell></row><row><cell cols="2">UAmsC03GeGe4GSbO DE</cell><cell cols="2">4-Gram (Lnu)/Split+stem (Okapi) 0.18</cell></row><row><cell>UAmsC03SpSp4GiSb</cell><cell>ES</cell><cell>4-Gram/Stem</cell><cell>0.35</cell></row><row><cell>UAmsC03SpSpSS3w</cell><cell>ES</cell><cell>Stem (Lnu/Okapi/LM)</cell><cell>-</cell></row><row><cell>UAmsC03FiFi5GiSb</cell><cell>FI</cell><cell>5-Gram/Split+stem</cell><cell>0.51</cell></row><row><cell>UAmsC03FrFr4GiSb</cell><cell>FR</cell><cell>4-Gram/Stem</cell><cell>0.66</cell></row><row><cell>UAmsC03ItIt4GiSb</cell><cell>IT</cell><cell>4-Gram/Stem</cell><cell>0.405</cell></row><row><cell>UAmsC03DuDu4GiSb</cell><cell>NL</cell><cell>4-Gram/Split+stem</cell><cell>0.25</cell></row><row><cell>UAmsC03DuDuSS3w</cell><cell>NL</cell><cell>Split+stem (Lnu/Okapi/LM)</cell><cell>-</cell></row><row><cell>UAmsC03RuRuWrd</cell><cell>RU</cell><cell>Word</cell><cell>-</cell></row><row><cell>UAmsC03RuRuSbl</cell><cell>RU</cell><cell>Stem</cell><cell>-</cell></row><row><cell>UAmsC03RuRu4Gr</cell><cell>RU</cell><cell>4-Gram</cell><cell>-</cell></row><row><cell>UAmsC03RuRu4GiSb</cell><cell>RU</cell><cell>4-Gram/Stem</cell><cell>0.60</cell></row><row><cell>UAmsC03SwSw4GiSb</cell><cell>SV</cell><cell>4-Gram/Split+stem</cell><cell>0.585</cell></row><row><cell cols="2">UAmsC03SwSw4GSbO SV</cell><cell cols="2">4-Gram (Lnu)/Split+stem (Okapi) 0.315</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,70.19,381.32,453.55,171.50"><head>Table 2 :</head><label>2</label><figDesc>Table2indicates the type of run. For all the four bilingual pairs, we submitted a combination of the stemmed (and Overview of the bilingual runs submitted. For combined runs column 4 gives the base runs that were combined, and column 5 gives the interpolation factor λ.</figDesc><table coords="4,140.81,415.76,312.07,105.01"><row><cell>Run</cell><cell cols="2">Topics Documents</cell><cell>Type</cell><cell>Factor</cell></row><row><cell>UAmsC03GeIt4GiSb</cell><cell>DE</cell><cell>IT</cell><cell>4-Gram/Stem</cell><cell>0.7</cell></row><row><cell>UAmsC03EnRu4Gr</cell><cell>EN</cell><cell>RU</cell><cell>4-Gram</cell><cell>-</cell></row><row><cell>UAmsC03EnRuSbl</cell><cell>EN</cell><cell>RU</cell><cell>Stem</cell><cell>-</cell></row><row><cell cols="2">UAmsC03EnRu4GiSb EN</cell><cell>RU</cell><cell>4-Gram/Stem</cell><cell>0.6</cell></row><row><cell>UAmsC03FrDu4Gr</cell><cell>FR</cell><cell>NL</cell><cell>4-Gram</cell><cell>-</cell></row><row><cell cols="2">UAmsC03FrDuSblSS FR</cell><cell>NL</cell><cell>Split+stem</cell><cell>-</cell></row><row><cell cols="2">UAmsC03FrDu4GiSb FR</cell><cell>NL</cell><cell cols="2">4-Gram/Split+stem 0.3</cell></row><row><cell>UAmsC03ItSp4GiSb</cell><cell>IT</cell><cell>ES</cell><cell>4-Gram/Stem</cell><cell>0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,70.19,71.75,453.54,136.31"><head>Table 3 :</head><label>3</label><figDesc>Overview of the multilingual runs submitted. Column 5 indicates the base runs used to generate the multilingual run.</figDesc><table coords="5,79.76,71.75,434.39,105.01"><row><cell></cell><cell>Task</cell><cell>Topics</cell><cell>Documents</cell><cell>Type</cell></row><row><cell>UAmsC03EnM44Gr</cell><cell cols="2">multi-4 EN</cell><cell>DE, EN, ES, FR</cell><cell>4 × 4-Gram</cell></row><row><cell cols="3">UAmsC03EnM44GiSb multi-4 EN</cell><cell>DE, EN, ES, FR</cell><cell>4 × 4-Gram/(Split+)stem</cell></row><row><cell>UAmsC03EnM4SS4G</cell><cell cols="2">multi-4 EN</cell><cell>DE, EN, ES, FR</cell><cell>4 × 4-Gram, 4 × (Split+)stem</cell></row><row><cell>UAmsC03EnM84Gr6</cell><cell cols="2">multi-8 EN</cell><cell>DE, EN, ES, FR, IT, NL</cell><cell>6 × 4-Gram</cell></row><row><cell cols="3">UAmsC03EnM8SS4G6 multi-8 EN</cell><cell>DE, EN, ES, FR, IT, NL</cell><cell>6 × 4-Gram, 6 × (Split+)stem</cell></row><row><cell>UAmsC03EnM84Gr</cell><cell cols="2">multi-8 EN</cell><cell cols="2">DE, EN, ES, FR, IT, NL, SV 7 × 4-Gram</cell></row><row><cell cols="3">UAmsC03EnM84GiSb multi-8 EN</cell><cell cols="2">DE, EN, ES, FR, IT, NL, SV 7 × 4-Gram/(Split+)stem</cell></row><row><cell>UAmsC03EnM8SS4G</cell><cell cols="2">multi-8 EN</cell><cell cols="2">DE, EN, ES, FR, IT, NL, SV 7 × 4-Gram, 7 × (Split+)stem</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,70.19,408.46,453.54,98.45"><head>Table 4 :</head><label>4</label><figDesc>Table 4   indicates the type of run. The stemmed and 4-grammed runs mimics the settings of our monolingual runs for Overview of the runs submitted for the GIRT task.</figDesc><table coords="5,156.55,442.33,280.81,45.23"><row><cell>Run</cell><cell cols="2">Topics Documents</cell><cell>Type</cell></row><row><cell>UAmsC03GeGiWrd</cell><cell>DE</cell><cell>DE</cell><cell>Word</cell></row><row><cell cols="2">UAmsC03GeGi4GriR DE</cell><cell>DE</cell><cell>Reranking of Stem</cell></row><row><cell cols="2">UAmsC03GeGiSbliR DE</cell><cell>DE</cell><cell>Reranking of 4-Gram</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,70.19,71.75,453.55,148.26"><head>Table 5 :</head><label>5</label><figDesc>Overview of MAP scores for all submitted monolingual runs and for the underlying base runs. Best scores are in boldface; base runs that were not submitted are in italics. The figures in brackets indicate the improvement of the combined run over the best underlying base run.</figDesc><table coords="6,83.30,71.75,427.33,105.01"><row><cell></cell><cell>Dutch</cell><cell>Finnish</cell><cell cols="2">French German</cell><cell>Italian</cell><cell cols="3">Russian Spanish Swedish</cell></row><row><cell>(Split+)Stem</cell><cell>0.4984</cell><cell>0.4453</cell><cell>0.4511</cell><cell>0.4840</cell><cell>0.4726</cell><cell>0.2536</cell><cell>0.4678</cell><cell>0.3957</cell></row><row><cell>n-Gram</cell><cell>0.4996</cell><cell>0.4774</cell><cell>0.4616</cell><cell>0.5005</cell><cell>0.4227</cell><cell>0.3030</cell><cell>0.4733</cell><cell>0.4187</cell></row><row><cell>Combined</cell><cell>0.5072</cell><cell>0.5137</cell><cell>0.4888</cell><cell>0.5091</cell><cell>0.4781</cell><cell>0.2988</cell><cell>0.4841</cell><cell>0.4371</cell></row><row><cell>(% Change)</cell><cell cols="8">(+1.52%) (+7.60%) (+5.89%) (+1.72%) (+1.16%) (-1.39%) (+2.28%) (+4.39%)</cell></row><row><cell>More runs:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lnu/Okapi/LM</cell><cell>0.5138</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.4916</cell><cell>-</cell></row><row><cell>Lnu/Okapi</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.5200</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.4556</cell></row><row><cell>Words</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.2551</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,70.19,440.03,453.55,100.44"><head>Table 6 :</head><label>6</label><figDesc>Overview of MAP scores for all bilingual runs. Best scores are in boldface. The figures in brackets indicate the improvement of the combined run over the best underlying base run.</figDesc><table coords="6,163.16,440.03,267.61,69.14"><row><cell></cell><cell></cell><cell>German</cell><cell>Italian</cell><cell>English</cell></row><row><cell></cell><cell>to Dutch</cell><cell>to Italian</cell><cell cols="2">to Spanish to Russian</cell></row><row><cell>(Split+)Stem</cell><cell>0.3693</cell><cell>0.3402</cell><cell>0.3160</cell><cell>0.2270</cell></row><row><cell>n-Gram</cell><cell>0.3803</cell><cell>0.3411</cell><cell>0.3588</cell><cell>0.1983</cell></row><row><cell>Combined</cell><cell>0.3835</cell><cell>0.3830</cell><cell>0.3535</cell><cell>0.2195</cell></row><row><cell cols="5">(% Change) (+0.84%) (+12.28%) (-1.48%) (-3.30%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,147.85,659.12,298.21,64.58"><head>Table 7 :</head><label>7</label><figDesc>Decrease in effectiveness for bilingual runs.</figDesc><table coords="6,147.85,659.12,298.21,45.23"><row><cell></cell><cell></cell><cell>Italian</cell><cell>Spanish</cell><cell>Russian</cell></row><row><cell>Best monolingual</cell><cell>0.5138</cell><cell>0.4781</cell><cell>0.4916</cell><cell>0.3030</cell></row><row><cell>Best bilingual</cell><cell>0.3835</cell><cell>0.3830</cell><cell>0.3588</cell><cell>0.2270</cell></row><row><cell>(% Change)</cell><cell cols="4">(-25.36%) (-19.89%) (-27.01%) (-25.08%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,70.19,165.24,453.54,88.49"><head>Table 8 :</head><label>8</label><figDesc>Overview of MAP scores for all multilingual runs. Best scores are in boldface; runs that were not submitted are in italics.</figDesc><table coords="7,306.34,165.24,143.15,20.92"><row><cell>-4</cell><cell>Multi-8</cell></row><row><cell></cell><cell>(without FI/SV) (without FI)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="7,70.19,441.05,453.54,76.53"><head>Table 9 :</head><label>9</label><figDesc>Overview of MAP scores for all submitted GIRT runs, and for the underlying base suns. Best scores are in boldface; base runs that were not submitted are in italics.</figDesc><table coords="7,211.97,441.05,169.97,45.23"><row><cell></cell><cell></cell><cell cols="2">Rerank % Change</cell></row><row><cell>Words</cell><cell>0.2360</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Stemmed 0.2832 0.3361</cell><cell>+18.68%</cell></row><row><cell>4-Gram</cell><cell cols="2">0.3449 0.3993</cell><cell>+15.77%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,84.53,710.36,439.19,7.17;2,70.19,719.82,20.58,7.17"><p>We used the excellent Perl package Convert::Cyrillic for conversion between character encodings and for lower-casing Cyrillic characters.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We want to thank <rs type="person">Harald Hammarstrom</rs> for advice on Finnish and Swedish, <rs type="person">Vera Hollink</rs> for technical support, and <rs type="person">Valentin Jijkoun</rs> for help with the Russian collection. <rs type="person">Jaap Kamps</rs> was supported by <rs type="funder">NWO</rs> under project number <rs type="grantNumber">400-20-036</rs>. <rs type="person">Christof Monz</rs> was supported by the <rs type="funder">Physical Sciences Council</rs> with financial support from <rs type="funder">NWO</rs> under project <rs type="grantNumber">612-13-001</rs>, and by a grant from <rs type="funder">NWO</rs> under project number <rs type="grantNumber">220-80-001</rs>. Maarten de Rijke was supported by grants from <rs type="funder">NWO</rs>, under project numbers <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">365-20-005</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">220-80-001</rs>, and <rs type="grantNumber">612.000.207</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GR8P3YN">
					<idno type="grant-number">400-20-036</idno>
				</org>
				<org type="funding" xml:id="_zXZbykA">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_4CCEJzZ">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_XmaKV9V">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_aAE885n">
					<idno type="grant-number">365-20-005</idno>
				</org>
				<org type="funding" xml:id="_5Wf8DKC">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_CrsgCKN">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_xfHeZys">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_eD32xcf">
					<idno type="grant-number">612.000.207</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,91.77,206.79,259.53,8.97" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,131.61,206.79,68.01,8.97">Online dictionary</title>
		<author>
			<persName coords=""><surname>Babylon</surname></persName>
		</author>
		<ptr target="http://www.babylon.com/" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,225.60,431.97,8.97;8,91.77,237.56,431.97,8.97;8,91.77,249.51,216.16,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,248.97,225.60,199.32,8.97">New retrieval approaches using SMART: TREC 4</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,120.42,237.56,194.10,8.97">The Fourth Text REtrieval Conference (TREC-4)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="500" to="236" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,268.33,342.44,8.97" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,121.41,268.33,129.50,8.97">Cross language evaluation forum</title>
		<author>
			<orgName type="collaboration" coords="8,91.77,268.33,20.84,8.97">CLEF</orgName>
		</author>
		<ptr target="http://www.clef-campaign.org/" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,287.14,431.97,8.97;8,91.77,300.35,22.81,7.04" xml:id="b3">
	<monogr>
		<ptr target="http://www.unine.ch/info/clef" />
		<title level="m" coord="8,169.00,287.14,190.63,8.97">CLEF resources at the University of Neuchâtel</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>CLEF-Neuchâtel</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,317.90,431.96,8.97;8,91.77,329.86,431.97,8.97;8,91.77,341.81,140.30,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,203.11,317.90,137.36,8.97">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,456.24,317.90,67.49,8.97;8,91.77,329.86,128.56,8.97">The Second Text REtrieval Conference (TREC-2)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="500" to="215" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,360.63,431.96,8.97;8,91.77,372.58,212.73,8.97" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,151.14,360.63,204.92,8.97">Using Language Models for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Center for Telematics and Information Technology, University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="8,91.77,391.39,431.97,8.97;8,91.77,403.35,123.83,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,294.15,391.39,225.34,8.97">Monolingual document retrieval for European languages</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hollink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,91.77,403.35,85.07,8.97">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,422.16,431.97,8.97;8,91.77,434.12,431.96,8.97;8,91.77,446.07,296.78,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,241.18,422.16,241.01,8.97">Combining evidence for cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,311.12,434.12,212.61,8.97;8,91.77,446.07,60.88,8.97">Evaluation of Cross-Language Information Retrieval Systems, CLEF</title>
		<title level="s" coord="8,180.04,446.07,140.52,8.97">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,464.88,431.97,8.97;8,91.77,476.84,431.96,8.97;8,91.77,488.79,431.96,8.97;8,91.77,500.75,22.42,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,132.41,464.88,308.35,8.97">Combining multiple evidence from different properties of weighting schemes</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,222.17,476.84,301.56,8.97;8,91.77,488.79,213.97,8.97">Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Ingwersen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fidel</surname></persName>
		</editor>
		<meeting>the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="180" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,519.56,431.97,8.97;8,91.77,531.52,431.97,8.97;8,91.77,543.47,431.96,8.97;8,91.77,555.43,126.34,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,202.12,519.56,321.62,8.97;8,91.77,531.52,77.79,8.97">Shallow morphological analysis in monolingual information retrieval for Dutch, German and Italian</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,439.27,531.52,84.47,8.97;8,91.77,543.47,161.76,8.97">Evaluation of Cross-Language Information Retrieval Systems</title>
		<title level="s" coord="8,379.31,543.47,140.30,8.97">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001. 2002</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="262" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,574.24,431.97,8.97;8,91.77,586.19,431.96,8.97;8,91.77,598.15,363.37,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,209.89,574.24,297.10,8.97">Improving the precision of a text retrieval system with compound analysis</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pohlmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,432.05,586.19,91.68,8.97;8,91.77,598.15,269.14,8.97">Proceedings of the 7th Computational Linguistics in the Netherlands Meeting (CLIN 1996)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Landsbergen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Odijk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Van Deemter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Veldhuijzen Van Zanten</surname></persName>
		</editor>
		<meeting>the 7th Computational Linguistics in the Netherlands Meeting (CLIN 1996)</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="115" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,616.96,342.63,8.97" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="8,166.42,616.96,65.71,8.97">Online translator</title>
		<ptr target="http://translation2.paralink.com/" />
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>PROMT-Reverso</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,635.77,431.97,8.97;8,91.77,647.73,183.37,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,272.54,635.77,194.41,8.97">Experimentation as a way of life: Okapi at TREC</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,476.13,635.77,47.60,8.97;8,91.77,647.73,105.93,8.97">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,666.54,431.96,8.97;8,91.77,678.50,431.96,8.97;8,91.77,690.45,109.29,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,162.68,666.54,176.89,8.97">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,434.68,666.54,89.05,8.97;8,91.77,678.50,229.72,8.97">The SMART Retrieval System: Experiments in Automatic Document Processing</title>
		<title level="s" coord="8,329.08,678.50,190.10,8.97">Prentice-Hall Series in Automatic Computation</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,709.26,431.96,8.97;8,91.77,722.47,22.81,7.04" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="8,135.09,709.26,209.49,8.97">Stemming algorithms for use in information retrieval</title>
		<author>
			<persName coords=""><surname>Snowball</surname></persName>
		</author>
		<ptr target="http://www.snowball.tartarus.org/" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,740.03,283.63,8.97" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="8,142.99,740.03,65.71,8.97">Online translator</title>
		<author>
			<persName coords=""><surname>Worldlingo</surname></persName>
		</author>
		<ptr target="http://www.worldlingo.com/" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
