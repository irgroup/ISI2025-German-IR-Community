<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.76,74.25,417.74,12.64;1,147.20,90.33,300.82,12.64">EXETER AT CLEF 2003: Experiments with Machine Translation for Monolingual, Bilingual and Multilingual Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,206.72,119.04,102.39,8.96"><forename type="first">Adenike</forename><forename type="middle">M</forename><surname>Lam-Adesina</surname></persName>
							<email>a.m.lam-adesina@ex.ac.uk</email>
						</author>
						<author>
							<persName coords="1,316.40,119.04,68.92,8.96"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gareth.jones@computing.dcu.ie</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Exeter</orgName>
								<address>
									<postCode>EX4 4QF</postCode>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.76,74.25,417.74,12.64;1,147.20,90.33,300.82,12.64">EXETER AT CLEF 2003: Experiments with Machine Translation for Monolingual, Bilingual and Multilingual Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">260AC046ECC38587728952CE1E1BA637</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The University of Exeter group participated in the monolingual, bilingual and multilingual-4 retrieval tasks this year. The main focus of our investigation this year was the small multilingual task comprising four languages, French, German, Spanish and English. We adopted a document translation strategy and tested four different merging techniques to combine results from the different sources to achieve an optimal performance. For both the monolingual and bilingual tasks we explored the use of a parallel collection for query expansion and term weighting and also experimented with updating synonym information to conflate British and American English word spellings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes our experiments for CLEF 2003. This year we participated in the monolingual, bilingual and multilingual retrieval tasks. The main focus of our participation this year was the multilingual task (being our first participation in this task), our submissions for the other two tasks build directly from our work from past experiments <ref type="bibr" coords="1,124.07,387.00,71.02,8.96">(CLEF 2001 and</ref><ref type="bibr" coords="1,199.58,387.00,50.31,8.96">CLEF 2002)</ref>. Our official submissions included monolingual runs for Italian, German, French and Spanish, bilingual German to Italian and Italian to Spanish, and the small multilingual tasks comprising English, French, German and Spanish collections. Our general approach was to use translation of both collections and topics into a common language. Thus the document collections were translated into English using Systran Version:3.0 Machine Translator (Sys), and all topics translated into English using either Systran Version:3.0 or Globalink Power Translation Pro Version 6.4 (Pro) Machine Translator (MT) systems. Following from our successful use of Pseudo-Relevance Feedback methods in past CLEF exercises <ref type="bibr" coords="1,471.61,467.52,48.28,8.96">(CLEF 2001</ref><ref type="bibr" coords="1,70.88,478.92,23.31,8.96">(CLEF , 2002) )</ref> and supported by past research work in text retrieval exercises <ref type="bibr" coords="1,352.87,478.92,11.28,8.96" target="#b0">[1]</ref>[2] <ref type="bibr" coords="1,375.43,478.92,11.28,8.96" target="#b2">[3]</ref>, we continued to use this method with success for improved retrieval. In our previous experimental work <ref type="bibr" coords="1,360.43,490.44,11.69,8.96" target="#b3">[4]</ref> <ref type="bibr" coords="1,372.12,490.44,11.69,8.96" target="#b4">[5]</ref> we demonstrated the effectiveness of a new PRF method of term selection from document summaries, and found it to be more reliable than query expansion from full documents, this method is again used in the results reported here. Following from last year, we again investigated the effectiveness of query expansion and term estimation from a parallel (pilot) collection <ref type="bibr" coords="1,174.23,536.51,11.72,8.96" target="#b5">[6]</ref> and found that caution needs to be exercised when using the collections to achieve improve retrieval for translated documents. The remainder of this paper is structured as follows: in Section 2 we present our system setup and the information retrieval methods used, Section 3 describes the pilot search strategy, Section 4 presents and discusses experimental results and Section 5 concludes the paper with a discussion of our findings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Setup</head><p>The basis of the experimental system was the City University research distribution version of the Okapi system. The documents and search topics were processed to remove stopwords from a list of about 260 words; suffix stripped using the Okapi implementation of Porter stemming <ref type="bibr" coords="1,325.16,665.28,11.71,8.96" target="#b6">[7]</ref> and terms were indexed using a small set of synonyms. Since the English document collection for CLEF 2003 incorporates both British and American documents, the synonym table was updated this year to include some common British words that have different American nomenclature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Term Weighting</head><p>Document terms are weighted using the Okapi BM25 weighting scheme developed in <ref type="bibr" coords="2,420.77,98.40,11.69,8.96" target="#b7">[8]</ref> and further elaborated in <ref type="bibr" coords="2,81.11,109.80,11.66,8.96" target="#b8">[9]</ref> and calculated as follows,</p><formula xml:id="formula_0" coords="2,118.76,129.92,348.75,34.84">) , ( ))) ( ( ) 1 (( * 1 ) 1 1 ( ) , ( ) ( ) , ( j i tf j ndl b b K K j i tf i cfw j i cw + × + - + × × = (1)</formula><p>where cw(i,j) represents the weight of term i in document j, cfw(i) is the standard collection frequency weight, tf(i,j) is the document term frequency, and ndl(j) is the normalized document length. ndl(j) is calculated as ndl(j) = dl(j)/avdl where dl(j) is the length of j and avdl is the average document length for all documents. k1 and b are empirically selected tuning constants for a particular collection. k1 is designed to modify the degree of effect of tf(i,j), while constant b modifies the effect of document length. High values of b imply that documents are long because they are verbose, while low values imply that they are long because they are multi-topic. In our experiments values of k1 and b are estimated based on the CLEF 2002 data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pseudo-Relevance Feedback</head><p>Retrieval of relevant documents is usually affected by short or imprecise queries. Relevance Feedback (RF) via query expansion, aims to improve initial query statements by addition of terms from user assessed relevant documents. These terms are assessed using document statistics and usually describe the information request better. Pseudo-Relevance Feedback (PRF) whereby relevant documents are assumed and used for query expansion is on average found to give improvement in retrieval performance although this is usually smaller than that observed for true user based RF. The main implementation issue for PRF is the selection of appropriate expansion terms. In PRF problems can arise if assumed relevant documents are indeed non-relevant thus leading to selection of inappropriate terms. However, the selection of such documents might suggest partial relevance, thus, term selection from relevant section might prove more beneficial. Our query expansion method selects terms from summaries of the top 5 ranked documents. The summaries were generated using the method described in <ref type="bibr" coords="2,240.81,446.40,10.69,8.96" target="#b3">[4]</ref>. The summary generation method combines the Luhn's Keyword Cluster Method <ref type="bibr" coords="2,136.08,457.91,15.43,8.96" target="#b9">[10]</ref>, Title terms frequency method <ref type="bibr" coords="2,279.39,457.91,10.69,8.96" target="#b3">[4]</ref>, Location/header method <ref type="bibr" coords="2,396.36,457.91,16.76,8.96" target="#b10">[11]</ref> and the Query-bias method <ref type="bibr" coords="2,70.88,469.43,16.68,8.96" target="#b11">[12]</ref> to form an overall significance score for each sentence. For all our experiments we used the top 6 ranked sentences as the summary of each document. From this summary we collected all non-stopwords and ranked them using a slightly modified version of the Robertson selection value (rsv) <ref type="bibr" coords="2,383.50,492.47,16.72,8.96" target="#b12">[13]</ref> reproduced below. The top 20 terms were then selected in all our experiments.</p><formula xml:id="formula_1" coords="2,113.96,523.41,328.64,16.08">) ( ) ( ) ( i rw i r i rsv × = ( 2 )</formula><p>where r(i) = number of relevant documents containing term i rw(i) is the standard Robertson/Sparck Jones relevance weight <ref type="bibr" coords="2,350.54,567.24,16.68,8.96" target="#b11">[12]</ref> reproduced below</p><formula xml:id="formula_2" coords="2,111.32,587.48,226.48,32.44">) 5 . 0 ) ( )( 5 . 0 ) ( ) ( ( ) 5 . 0 ) ( ) ( )( 5 . 0 ) ( ( log ) ( + - + - + + - - + = i r R i r i n i r R i n N i r i rw</formula><p>where n(i) = the total number of documents containing term i r(i) = the total number of relevant documents term i occurs in R = the total number of relevant documents for this query N = the total number of documents</p><p>In our modified version, although potential expansion terms are selected from the summaries of the top 5 ranked documents, they are ranked using the top 20 ranked documents from the initial run.</p><p>Query expansion is aimed at improving initial search topics in order to make it a better expression of user's information need. This is normally achieved by adding terms selected from assumed relevant documents retrieved from the test collection, to the initial query. However, it has been shown <ref type="bibr" coords="3,432.29,121.32,16.76,8.96" target="#b13">[14]</ref> that if additional documents are available these can be used in a pilot set for improved selection of expansion terms. The underlying assumption in this method is that a bigger collection than the test collection can help to achieve better term expansion and/or more accurate parameter estimation, and hopefully better retrieval and document ranking.</p><p>Based on this assumption we explore the idea of pilot searching in our CLEF experiments.</p><p>The Okapi submissions for the TREC-7 <ref type="bibr" coords="3,236.85,178.80,11.70,8.96" target="#b5">[6]</ref> and TREC-8 <ref type="bibr" coords="3,306.46,178.80,16.72,8.96" target="#b13">[14]</ref> ad hoc tasks used the TREC disks 1-5, of which the document test set is a subset, for parameter estimation and query expansion. The method was found to be very effective. In order to explore the utility of pilot searching for our experiments, we used the TREC-7 and TREC-8 ad hoc document test collection itself for our pilot runs. The pilot searching procedure is as carried out as follows:</p><p>1. Run the unexpanded initial query on the pilot collection using BM25 without feedback 2. Extract terms from the summaries of the top R assumed relevant documents 3. Select top ranked terms using (3) based on their distribution in the pilot collection 4. Add desired number of selected terms to initial query 5. Store equivalent pilot weight of terms 6. Either apply expanded query to the test collection and estimate weight based on test collection or Apply expanded query and estimated weight from pilot collection on the test collection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head><p>This section describes the establishment of the parameters of our experimental system and gives results from our investigations for CLEF 2003 monolingual, bilingual and multilingual tasks. We report procedures for system parameters selection, baseline retrieval results for all languages and translation systems without the application of feedback. Corresponding results after the application of different methods of feedback including results for term weight estimation from pilot collections. The CLEF 2003 topics consist of three fields: Title, Description and Narrative. All our experiments use the Title and the Description fields only. For all runs we present the average precision results (Avep), the % change from results for baseline no feedback runs (% chg) and the number of relevant documents retrieved out of the total number of relevant in collection (Rel_ret).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Selection of System Parameters</head><p>To set appropriate parameters for our runs, development runs were carried out using the CLEF 2002 collections. These document collections consist of those used for CLEF 2001 runs and are the same as those used for CLEF 2002. For CLEF 2003 more documents were added to all individual collections, and thus we are assuming that these parameters are suitable for these modified collections as well. The Okapi parameters were set as follows k1=1.4 b=0.6. For all our PRF runs, 5 documents were assumed relevant for term selection and document summaries comprised the best scoring 6 sentences in each case. Where the length of sentence was less than 6, half of the total number of sentences was chosen. The rsv values to rank the potential expansion terms were estimated based on the top 20 ranked assumed relevant documents. The top 20 ranked expansion terms taken from these summaries were added to the original query in each case. Based on results from our previous experiments, the original topic terms are upweighted by a factor of 3.5 relative to terms introduced by PRF. In our test runs we experimented with updated synonym information to conflate British and American English word spellings. This method resulted in a further 4% improvement in average precision compared to the baseline no feedback results for our English monolingual unofficial run for CLEF 2002 <ref type="foot" coords="3,380.48,645.81,3.24,5.83" target="#foot_0">1</ref> . We anticipate this being a useful technique for CLEF 2003 as well, and the updated synonym list is again used for all our experiments reported here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Monolingual runs</head><p>We submitted runs for four languages (German, French, Italian and Spanish) in the monolingual task. Official runs are marked with a * and additional unofficial runs are presented. In all cases, results are presented for the following:</p><p>1. Baseline run without feedback (exe*base) 2. Feedback runs using expanded query and term weights from the target collection (exe*mono) 3. Feedback runs using expanded query from pilot collection and term weights from test collection (exe*tcmono) 4. Feedback runs using expanded query and term weights from pilot collection (exe*tcqywgt) 5. An additional Feedback run is presented where query is expanded using a pilot run on a merged collection of all four text collection comprising the small multilingual collections. (exe*comqy) with the terms weights being taken from the test collection.</p><p>Note: * refers to the target language e.g sp -&gt; Spanish, de-&gt; German, it-&gt;Italian and fr-&gt;French. Results are presented for both Sys and Pro MT systems</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">German Monolingual runs</head><p>Table <ref type="table" coords="4,96.22,415.20,4.98,8.96">1</ref> Retrieval results for topic translation for German monolingual runs for both Sys and Pro MT, before and after applications of different feedback strategies.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">French Monolingual runs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Italian Monolingual runs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Spanish Monolingual runs</head><p>Table <ref type="table" coords="5,96.21,205.92,4.98,8.96">4</ref> Retrieval results for topic translation for Spanish monolingual runs for both Sys and Pro MT, before and after applications of different feedback strategies.</p><p>Examination of Tables 1 to 4 reveals a number of consistent trends. Considering first the baseline runs. In all cases Sys MT translation of the topics produces better results than use of Pro MT. This is not too surprising since the documents were also translated with Sys MT, and the result indicates that consistency (and perhaps quality) of translation is important. All results show that our PRF results in improvement in performance over the baseline in cases. The variations in PRF results for query expansion for the different methods explored are very consistent. The best performance is observed in all cases, except Pro MT Spanish, using only the test collection for expansion term selection and collection weighting. Thus, although query expansion from pilot collections has been shown to be very effective in other retrieval tasks <ref type="bibr" coords="5,299.19,320.87,10.69,8.96" target="#b5">[6]</ref>, the method did not work very well for CLEF 2003 documents and topics. Perhaps more surprising is the observation that term weight estimation from the pilot collection actually resulted in loss in average precision in most cases relative to the baseline. This result is very unexpected particularly since the method have been shown to be every effective and as been used with success in our past research work for CLEF 2001 and 2002. Query expansion from the merged document collection (used for the multilingual task) of Spanish, English, French, and German also resulted in improvement in retrieval performance, in general slightly less than that achieved in the best results for French, German and Spanish using only the test collection. The result for this method is lower for Italian run, this is most certainly due to the absence of the Italian collection in the merged collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Bilingual runs</head><p>For the Bilingual task we submitted runs for Italian and Spanish tasks. Official runs are marked with a * and additional unofficial runs are presented. In all cases, results are presented for the following:</p><p>6. Baseline run without feedback (exebasebi) 7. Feedback runs using expanded query and term weights from the target collection (exebi) 8. Feedback runs using expanded query from pilot collection and term weights from test collection (exe*q+dtc) 9. Feedback runs using expanded query and term weights from pilot collection (exe*qd+tc) 10. We investigated further the effectiveness of pilot collection and the impact of vocabulary differences for different languages. This is done by expanding initial query statement from the topic collection and then applying the expanded query on the target collection (i.e. for German-Italian bilingual runs initial German query statement is expanded from the German collection and applied on the test collection) exe*q+dbi 11. Additionally both the expanded query and the corresponding term weight is estimated from the topic collection exe*qd+bi Note: * and + refers to the either the topic or the target language e.g. sp -&gt; Spanish, de-&gt; German, it-&gt;Italian and fr-&gt;French. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Bilingual German to Italian</head><p>Table <ref type="table" coords="6,96.18,205.68,4.98,8.96">5</ref> Retrieval results for topic translation for Italian bilingual runs for both Sys and Pro MT, before and after applications of different feedback strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Bilingual Italian to Spanish</head><p>Table <ref type="table" coords="6,96.21,373.44,4.98,8.96">6</ref> Retrieval results for topic translation for Spanish bilingual runs for both Sys and Pro MT, before and after applications of different feedback strategies.</p><p>For our bilingual run we tried a new method of query expansion and term weight estimation from the topic language collection. This resulted in the best performance for the Italian bilingual run with about 33% improvement in average precision. This method also worked well for the Spanish bilingual run giving about 19% improvement in average precision compared with results for baseline with no feedback. The standard method of query expansion and term weight estimation from the test collection also proved effective for the Italian-Spanish task. The use of term weights from the topic collection gives a large improvement over the result using test collection weights positive in the case of the German-Italian task, but for the Italian-Spanish task this change has a negligible effect in the case of Systran MT and makes performance worse for Globalink MT. It is not immediately clear why these collections should behave differently, but it may relate to the size of the document collections, the Italian collection being much smaller than either of the German or Spanish collections. Query expansion and term weight estimation from pilot collection resulted in improvement in average precision ranging from 1.2% to 9% for both results, although it failed to achieve comparable performance to other methods, which is again surprising but consistent with the monolingual results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multilingual Retrieval</head><p>Multilingual information retrieval presents a more challenging task in cross-lingual retrieval experiments, whereby a user submit a request in a single language (e.g. English) in order to retrieve relevant documents in different languages e.g. English, Spanish, Italian, German, etc. We approached this task in two ways. First, we retrieved relevant documents using the English queries individually from the four different collections and then merged the results together using different techniques (described below). Secondly we merged all the collections together to form a single collection and performed retrieval directly from this collection without using a separate merging stage.</p><p>Different techniques for merging separate result lists to form a single list have been proffered and tested. All of the techniques suggest that making assumptions that the distribution of relevant documents in the results set for retrieval from individual collection is similar is not true <ref type="bibr" coords="6,307.35,725.40,15.43,8.96" target="#b14">[15]</ref>. Hence, straight merging of relevant documents from the sources will result in poor combination. Based on these assumptions we examined four merging techniques for combining the retrieved results from the four collections to form a single result list as follows: where u, p, s and d are the new document weight for all document in all collections and corresponding results are labelled exemult4* where * can be u, p, s or d depending on merging scheme used doc_wgt = the initial document weight gmax_wt = the global maximum weight i.e the highest document from all collections for a given query max_wt = the individual collection maximum weight for a given query min_wt = the individual collection minimum weight for a given query rank = the a parameter to control the effect of size of collection, a collection with more document get a higher rank (value ranges between 1.5 and 1).</p><p>To test the effectiveness of the merging schemes, we merged all the four text collection into a single large combined collection. Expanded queries from this combined test collection (exemultorg) and from the TREC data pilot collection (exemulttc) were then applied on the resultant merged collection. For all official runs (*) English queries are expanded from the TREC-7 and 8 pilot collections and then applied on the test collection.</p><p>Note: an additional run exemult4snew was conducted whereby the expanded query was estimated from the merged query collection and applied on the individual collection before merging using equation 5 above. Table <ref type="table" coords="7,96.18,615.83,4.98,8.96">8</ref> Retrieval results for small Multilingual task before and after applications of different merging strategies.</p><p>The baseline result for our multilingual run (exemultbase) perhaps might not present a realistic platform for comparison with the feedback run using the different merging strategies (exemult4*). This is mainly because it was achieved from a no feedback run from the merged multilingual collection. The multilingual results show that the different merging techniques provide similar retrieval performance. The result for merging strategy using equation 6 (which has been shown to be effective in past retrieval task) however resulted in about 14% loss in average precision compared to the baseline run. Also the merging strategies failed to show any improvement over raw score merging (row 3), although the merging strategy using equation 5, gave the highest number of relevant document retrieved for all the merging strategies. Both our bilingual and monolingual runs show that retrieval results using expansion query and term weight estimation from pilot collection resulted in loss in average precision compared to baseline no feedback run in most cases. This might have contributed to the poor result from the different merging techniques for the multilingual runs (exemult4*). For the multilingual results using the merging techniques (exemult4*), We expanded the initial English query and estimated the term weights from the pilot collection and then applied these to the individual collections. However, results from our monolingual runs using this method were not very encouraging, and this might perhaps have contributed to the poor results after the application of the different merging techniques compared to the method whereby all the collections are merged to form one big collection.</p><p>To test this hypothesis, we conducted an additional run whereby we used the merged collection as the pilot collection and expanded the initial query from it. The expanded query was then applied on the individual collections and resultant result file merged using equation 5. The result showed an improvement of about 4% compared to that achieved from the baseline no feedback run from the merged collection (Exemultbase). It also resulted in about 11% increase in average precision over result from query expansion from the pilot collection (Exemult4s).</p><p>The best result for the multilingual task was achieved by expanding the initial query from the pilot collection and applying it on the merged collection. Query expansion from the merged collection (exemultorg) also resulted in about 10% improvement in average precision. These results suggest that merging a collection in a multilingual task might be more beneficial than merging the result lists taken from the retrieval from individual collections. This result is presumably due to the more robust and consistent parameter estimation in the combined document collection. In many practical situations combining collections in this way is not practical and multilingual IR can be viewed as distributed information retrieval task where there may be varying degrees of cooperation between the various collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>For our participation in CLEF 2003 retrieval tasks we updated our synonym information to include common British and American English words. We explored the idea of query expansion from pilot collection and got some disappointing results which is contrary to past retrieval work utilizing the use of expanded queries and term weight estimation from pilot collections. This result may be caused by vocabulary and distribution mismatch between our translated test collection and the native English pilot collection, but further investigation is needed to ascertain whether this or other reasons underlie this negative result.</p><p>For the bilingual task we explored the idea of query expansion from a pilot collection in the topic language. This method resulted in better retrieval performance. Although we are working in English as our search language throughout this result is related to the ideas of pre-translation and post-translation feedback explored in earlier work on CLIR <ref type="bibr" coords="8,131.56,489.36,10.67,8.96" target="#b1">[2]</ref>, and we need to perform further runs to explore possible further gains from the combination of both forms of feedback. The different merging strategies used for combining our results for the multilingual task failed to perform better than raw score merging. Further investigation is needed to test these methods, particularly as some of them methods have been shown to be effective in past research. Merging the document collection resulted in better average precision than merging the result list. However, situations might arise whereby it is impossible to merge the various collections together, in this case an effective method of merging the result list is needed. Further investigation will be conducted to examine the possibility of improving the results achieved from merging result lists.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,70.88,570.96,446.44,20.48"><head>Table 2</head><label>2</label><figDesc>Retrieval results for topic translation for French monolingual runs for both Sys and Pro MT, before and after applications of different feedback strategies.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,70.88,319.68,444.23,427.64"><head>Table 3</head><label>3</label><figDesc>Retrieval results for topic translation for Italian monolingual runs for both Sys and Pro MT, before and after applications of different feedback strategies.</figDesc><table coords="4,285.20,319.68,148.65,8.96"><row><cell>Sys MT</cell><cell>Pro MT</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,119.29,110.39,347.70,566.73"><head></head><label></label><figDesc>Results are presented for both Sys and Pro MT systems</figDesc><table coords="5,125.72,110.39,341.28,80.97"><row><cell></cell><cell></cell><cell>Sys MT</cell><cell></cell><cell></cell><cell>Pro MT</cell><cell></cell></row><row><cell>Run-id</cell><cell>Avep</cell><cell>% chg</cell><cell>R-ret</cell><cell>Avep</cell><cell>% chg</cell><cell>R-ret</cell></row><row><cell>Exespbase</cell><cell>422</cell><cell>-</cell><cell>2163</cell><cell>393</cell><cell>-</cell><cell>2111</cell></row><row><cell>Exespmono</cell><cell cols="3">470* +11.3% 2195</cell><cell cols="3">452* +15.0% 2145</cell></row><row><cell>Exesptcmono</cell><cell>426*</cell><cell>+0.9%</cell><cell>2114</cell><cell>415</cell><cell>+5.6%</cell><cell>2081</cell></row><row><cell>Exesptcqywgt</cell><cell>372</cell><cell>-11.8%</cell><cell>1973</cell><cell>397</cell><cell>+1.0%</cell><cell>2039</cell></row><row><cell>Exespcomqy</cell><cell>462</cell><cell>+9.5%</cell><cell>2200</cell><cell>466</cell><cell cols="2">+18.6% 2148</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,76.64,724.56,83.55,8.96"><p>Given that the CLEF</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2002" xml:id="foot_1" coords="3,185.31,724.56,308.71,8.96;3,70.88,736.08,444.08,8.96;3,70.88,747.60,376.55,8.96"><p>English collection contains only American English documents, we found this improvement in performance from spelling conflation a little surprising for the CLEF 2002 task, and we intend to carry our further investigation into the specific sources of the improvement in performance.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,96.08,618.12,428.35,8.96;8,96.08,629.64,389.00,8.96;8,485.12,627.45,6.48,5.83;8,494.96,629.64,29.41,8.96;8,96.08,641.16,428.26,8.96;8,96.08,652.56,131.52,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,373.71,618.12,150.72,8.96;8,96.08,629.64,280.93,8.96">A Comparison of Query Translation Methods for English-Japanese Cross-Language Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">H</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,395.81,629.64,89.27,8.96;8,485.12,627.45,6.48,5.83;8,494.96,629.64,29.41,8.96;8,96.08,641.16,377.98,8.96">Proceedings of the 22 nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 22 nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="269" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,96.08,664.08,428.28,8.96;8,96.08,675.60,207.32,8.96;8,303.44,673.41,5.04,5.83;8,314.00,675.60,210.42,8.96;8,96.08,687.12,373.60,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,224.95,664.08,299.42,8.96;8,96.08,675.60,88.42,8.96">Phrasal Translation and Query Expansion Techniques for Cross-Language Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,207.64,675.60,95.76,8.96;8,303.44,673.41,5.04,5.83;8,314.00,675.60,210.42,8.96;8,96.08,687.12,208.20,8.96">Proceedings of the 20 th Annual International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 20 th Annual International ACM SIGIR conference on Research and Development in Information Retrieval<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,96.08,698.64,428.22,8.96;8,96.08,710.16,261.15,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,211.26,698.64,243.08,8.96">Improving Retrieval performance by Relevance Feedback</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,464.18,698.64,60.12,8.96;8,96.08,710.16,168.31,8.96">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="page" from="288" to="297" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,96.08,721.56,428.36,8.96;8,96.08,733.08,203.98,8.96;8,300.08,730.89,5.04,5.83;8,310.76,733.08,213.67,8.96;8,96.08,744.60,365.81,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,265.16,721.56,259.28,8.96;8,96.08,733.08,83.51,8.96">Applying Summarization Techniques for Term Selection in Relevance Feedback</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Lam-Adesina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,203.70,733.08,96.36,8.96;8,300.08,730.89,5.04,5.83;8,310.76,733.08,213.67,8.96;8,96.08,744.60,208.16,8.96">Proceedings of the 24 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,96.08,73.08,428.45,8.96;9,96.08,84.60,428.35,8.96;9,96.08,96.00,269.42,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,253.55,73.08,270.98,8.96;9,96.08,84.60,78.12,8.96">Exeter at CLEF 2001: Experiments with Machine Translation for Bilingual Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Lam-Adesina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,197.65,84.60,326.79,8.96;9,96.08,96.00,97.59,8.96">Proceedings of the CLEF 2001: Workshop on Cross-Language Information Retrieval and Evaluation</title>
		<meeting>the CLEF 2001: Workshop on Cross-Language Information Retrieval and Evaluation<address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="59" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,96.08,107.52,428.35,8.96;9,96.08,119.04,428.28,8.96;9,96.08,130.56,207.52,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,296.78,107.52,227.65,8.96;9,96.08,119.04,65.32,8.96">Okapi at TREC-7: automatic ad hoc, filtering, VLS and interactive track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,353.70,119.04,170.66,8.96;9,96.08,130.56,86.81,8.96">Overview of the Seventh Text REtrieval Conference (TREC-7)</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="253" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,96.08,142.08,296.02,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,147.58,142.08,130.22,8.96">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,284.16,142.08,32.18,8.96">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="10" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,96.08,153.60,428.19,8.96;9,96.08,165.00,382.47,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,380.30,153.60,68.31,8.96">Okapi at TREC-4</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Payne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,124.53,165.00,243.40,8.96">Overview of the Fourth Text Retrieval Conference (TREC-4)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="73" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,96.08,176.52,428.42,8.96;9,96.08,188.04,179.01,8.96;9,275.12,185.85,5.04,5.83;9,283.40,188.04,241.04,8.96;9,96.08,199.56,321.96,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,205.38,176.52,319.13,8.96;9,96.08,188.04,71.93,8.96">Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,186.26,188.04,88.83,8.96;9,275.12,185.85,5.04,5.83;9,283.40,188.04,241.04,8.96;9,96.08,199.56,169.00,8.96">Proceedings of the 17 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,96.08,211.08,428.21,8.96;9,96.08,222.60,80.33,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,143.92,211.08,192.56,8.96">The Automatic Creation of Literature Abstracts</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">P</forename><surname>Luhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,343.87,211.08,175.73,8.96">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,96.08,234.00,406.69,8.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,169.00,234.00,159.06,8.96">New Methods in Automatic Abstracting</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">P</forename><surname>Edmundson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,334.70,234.00,76.45,8.96">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="285" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,96.08,245.52,428.33,8.96;9,96.08,257.04,93.06,8.96;9,189.20,254.85,4.32,5.83;9,198.32,257.04,326.14,8.96;9,96.08,268.56,239.47,8.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,226.37,245.52,282.77,8.96">The Advantages of Query-Biased Summaries in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tombros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,96.08,257.04,93.06,8.96;9,189.20,254.85,4.32,5.83;9,198.32,257.04,326.14,8.96;9,96.08,268.56,85.34,8.96">proceedings of the 21 st Annual International ACM SIGIR Conference Research and Development in Information Retrieval</title>
		<meeting>the 21 st Annual International ACM SIGIR Conference Research and Development in Information Retrieval<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="2" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,96.08,280.08,406.93,8.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,161.40,280.08,151.68,8.96">On term selection for query expansion</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,319.94,280.08,102.28,8.96">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="359" to="364" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,96.08,291.60,428.25,8.96;9,96.08,303.00,295.82,8.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,209.72,291.60,63.07,8.96">Okapi/Keenbow</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,457.83,291.60,66.51,8.96;9,96.08,303.00,177.66,8.96">Overview of the Eighth Text REtrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="151" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,96.08,314.52,428.37,8.96;9,96.08,326.04,428.21,8.96;9,96.08,337.56,171.14,8.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,166.33,314.52,339.45,8.96">Report on CLEF-2002 Experiments: Combining Multiple Sources of Evidence</title>
		<author>
			<persName coords=""><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,96.08,326.04,424.05,8.96">Proceedings of the CLEF 2002: Workshop on Cross-Language Information Retrieval and Evaluation</title>
		<meeting>the CLEF 2002: Workshop on Cross-Language Information Retrieval and Evaluation<address><addrLine>Rome Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09">September 2002</date>
			<biblScope unit="page" from="31" to="46" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
