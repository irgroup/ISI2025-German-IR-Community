<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,95.12,108.47,406.29,12.30">CONTROL: CLEF-2003 with Open, Transparent Resources Off-Line</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,233.00,133.98,58.15,8.72"><forename type="first">Monica</forename><surname>Rogati</surname></persName>
							<email>mrogati@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.18,133.98,53.09,8.72"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
							<email>yiming@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,95.12,108.47,406.29,12.30">CONTROL: CLEF-2003 with Open, Transparent Resources Off-Line</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5F6D80FF684BFE7ADD9F03DAB9287D0D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Corpus-based approaches to CLIR have been studied for many years. However, using commercial MT systems for CLEF has been considered easier and better performing. Our goal is to be one of the CLEF participants who show that the hypothetical performance drop is not large enough to justify the loss of control and transparency, especially for research systems. We participated in two bilingual runs and the small multilingual run using software and data that are free to obtain, transparent and modifiable.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past years, a necessary condition for a good cross-or multi-lingual performance in CLEF appeared to be the use of commercial MT systems, be it purchased or freely available online (Systran etc.) <ref type="bibr" coords="1,419.69,296.34,10.78,8.72" target="#b0">[1,</ref><ref type="bibr" coords="1,430.47,296.34,7.19,8.72" target="#b2">3,</ref><ref type="bibr" coords="1,437.65,296.34,10.78,8.72" target="#b10">11]</ref>. While using black boxes to cross the language barrier allowed researchers to concentrate on important issues such as stemming, query pre-and post-processing, combining black boxes outputs, and multilingual merging, <ref type="bibr" coords="1,407.25,318.66,11.60,8.72" target="#b0">[1,</ref><ref type="bibr" coords="1,418.85,318.66,7.73,8.72" target="#b2">3,</ref><ref type="bibr" coords="1,426.58,318.66,11.60,8.72" target="#b10">11]</ref> we believe that query translation does play an essential role in CLIR, and that understanding, control and transparency are crucial in a research system. Online MT systems can be upgraded, lose their free status, or change parameters at will, making past experiments irreproducible. If such a dependence is permitted, research in IR in general can be similarly reduced to pre-and post-processing of Google I/O. Our goal is to attempt to move away from basing the core of our CLIR research system on a module that cannot be fully understood and modified, to which future access might not be guaranteed, and in which external changes are allowed and sometimes not even detected. The main challenge, however, is to do so while sacrificing as little performance as possible.</p><p>Our initial attempt to reach this goal (CLEF 2001) was disappointing in this respect, mainly because we disallowed using translation resources entirely and relied on the temporal correspondence between CLEF documents to produce a "parallel" corpus. In CLEF 2003 we relaxed the independence requirement to using transparent data and code, freely available or available for a modest one-time fee, which we can store locally, easily modify, recompile and process, and which cannot change in uncontrollable or undetectable ways. We participated in two bilingual tasks (DE-&gt;IT, IT-&gt;ES), and the small multilingual task, which involved four languages.</p><p>Our general approach was to rely on parallel corpora and GIZA++ <ref type="bibr" coords="1,339.79,497.82,11.51,8.72" target="#b7">[8]</ref> for query translation, and on Lemur <ref type="bibr" coords="1,501.07,497.82,11.39,8.72" target="#b8">[9]</ref> for retrieval. All these resources (as well as the stemmers we used where applicable) fulfill the criteria outlined above. Moreover, with the exception of LDC data, which we did not use in the official runs but did use in preliminary experiments, all these resources are free of charge and publicly available.</p><p>In section 2 we discuss the parallel data and preprocessing (stemming, stopping etc.). In section 3 we discuss our approach to bilingual retrieval in general as well as approaches for situations where a parallel corpus between the two languages does not exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Description and Preprocessing</head><p>We have used the European Parliament proceedings 1996-2001 <ref type="bibr" coords="1,324.42,620.22,10.46,8.72" target="#b5">[6]</ref>. It includes versions in 11 European languages: Romance (French, Italian, Spanish, Portuguese), Germanic (English, Dutch, German, Danish, Swedish), Greek and Finnish. Sentence aligned parallel corpora (English-X) have been prepared by the author of <ref type="bibr" coords="1,452.62,642.54,10.37,8.72" target="#b5">[6]</ref>. We have also prepared German-Italian and Italian-Spanish versions for the two bilingual CLEF tasks we participated in, by detecting almost identical English sentences and aligning the corresponding non-English sentences. Table <ref type="table" coords="1,494.57,664.98,4.85,8.72" target="#tab_0">1</ref> shows the size of the relevant parallel corpora post-processing, after stopping and stemming. Some sentence pairs have been eliminated after becoming empty post-processing. Note that our quick intersection of X-EN to Y-EN parallel corpora by taking only sentences where the English versions were close resulted in losing about ¾ of the corpus. A much better approach would have been to follow <ref type="bibr" coords="1,239.59,709.74,11.05,8.72" target="#b5">[6]</ref>'s procedure, most likely resulting in a corpus of comparable size with the English versions. We have also experimented with several other corpora, including Hansard set A for French (available from the Linguistic Data Consortium). Although the sentence-aligned version was much larger (2.7M sentences), preliminary experiments on CLEF '01 and '02 datasets showed a consistent performance drop (usually around 10%). As a result, Hansard has not been used for CLEF 2003.</p><p>We preprocessed the parallel corpora and CLEF documents by eliminating punctuation, stopwords, and document sections disallowed in the task description. We have used the Porter stemmer for English and the rule-based stemmers and stopword lists kindly provided by J. Savoy <ref type="bibr" coords="2,295.06,311.10,15.00,8.72" target="#b9">[10]</ref>. After stemming, we have used 5-grams as a substitute for German word decompounding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bilingual Retrieval</head><p>Our main focus in bilingual retrieval has been query translation without the use of commercial MT systems, including Systran. In this section we will discuss our bilingual retrieval system using a parallel corpus, as well as the challenge of handling language pairs for which parallel corpora do not exist. Conceptually, our approach consists of several steps:</p><p>1. Parallel corpora and test documents preprocessing 2. Dictionary generation from parallel corpora 3. Pseudo-Relevance Feedback in the source language 4. Query translation 5. Pseudo-Relevance Feedback in the target language 6. Retrieval</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dictionary Generation and Query Translation</head><p>We have used GIZA++ <ref type="bibr" coords="2,163.64,521.58,11.39,8.72" target="#b7">[8]</ref> as an implementation of IBM Model 1 <ref type="bibr" coords="2,330.82,521.58,10.37,8.72" target="#b1">[2]</ref>. GIZA++ takes a parallel corpus and generates a translation probability matrix. The number of training iterations was 10. Although GIZA++ implements the more sophisticated translation models discussed in <ref type="bibr" coords="2,250.74,544.02,10.46,8.72" target="#b1">[2]</ref>, we have not used them for efficiency reasons, and because word order is not a factor during retrieval. Query translation was done on a word-by-word basis. A significant difference from MT or online dictionary based approaches is that instead of using a rank-based cutoff (i.e. the first or first two variants for each word) we are using all translations weighted by their translation probability: q t = q s •M st where q t is the query in the target language, q s is the query in the source language, and M st is the translation matrix. M was pruned to 50 translations per word for efficiency reasons. This is similar to IBM and BBN CLIR approaches <ref type="bibr" coords="2,268.33,667.74,11.24,8.72" target="#b3">[4,</ref><ref type="bibr" coords="2,279.58,667.74,7.50,8.72" target="#b4">5]</ref> except the translation is not integrated in the retrieval model; only the query is translated. This approach has the welcome side effect of a very focused query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pseudo-Relevance Feedback and Retrieval</head><p>We have used the Lemur toolkit <ref type="bibr" coords="3,198.45,132.42,11.39,8.72" target="#b8">[9]</ref> to implement weighted query expansion, and we modified the retrieval interface to accept weighted queries as input. After query expansion is done in Lemur , the resulting query vector (q s , words + weights) is extracted for future translation. After translation, q t is loaded into Lemur for a new round of query expansion in the target language, followed by retrieval.</p><p>PRF and retrieval parameters we tuned include the number of documents to be considered relevant, the number of new query words added, the relative weight of added queries (usually 0.5) and term weighting method. There is one such parameter set for each pre-and post-translation query expansion, and for each language pair. However, experiments on CLEF 2001 and 2002 indicated that post-translation query expansion hurts performance by diluting the query in some languages, so the second set of parameters were set to 0 for the bilingual runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Handling language pairs with no available parallel corpora</head><p>The bilingual task this year was more challenging, in that we were aware of no Italian-Spanish or German-Italian parallel corpora. However, since most parallel corpora have English as one of the languages we had the option of using English as a pivot language in two ways:</p><p>1. to create a new parallel corpus if there is significant overlap (as described in Section 2). This is the least likely situation, but it does happen in the case where there is an underlying text translated in multiple languages, as it happened with the European Parliament corpus. 2. to translate first to English, then from English. This is where keeping and using translation probabilities is very useful. In traditional MT approaches, where the query is translated as a sentence twice, the (binary) mistakes accumulate, making the original meaning difficult to preserve. We believe the original meaning is easier to preserve when the entire query vector is translated, taking into account the translation probabilities:</p><formula xml:id="formula_0" coords="3,266.72,402.01,82.99,12.72">q t = q s •M s2EN • M EN2t</formula><p>where q t is the query in the target language, q s is the query in the source language, and M X2Y is the translation matrix for language X to language Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Official Runs (German-Italian and Italian-Spanish)</head><p>All our official runs use the Title and Description fields. Relevant parameters are pre-translation feedback documents/terms, whether a new parallel corpus was created or if English was used as a pivot language during translation. It is hard to draw conclusions from the official runs without more extensive experimentation on CLEF 2003 data (to be completed in the final version of the working notes). In particular, we are seeking an explanation for the extremely low relative performance of cmuI2Sparafb. If the run is not buggy, feedback performance is very unstable from one translation method to the other, and from language to language. This would not be a complete surprise, since feedback performance varied dramatically for French and Spanish on our system for CLEF 2001 and 2002 , and is the main reason why our runs are duplicated with their "low feedback" alternative in both bilingual and multilingual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multilingual Retrieval</head><p>By using English as the query language we have leveraged the parallel corpora that had English as one of the languages. We have experimented with several parallel corpora, but chose the European Parliament proceedings as the corpus for our CLEF submission. We performed bilingual retrieval as described in Section 3, and we used Lemur for English monolingual retrieval. We then merged the results using the two methods described in Section 4.1. The number of feedback documents and words were tuned for each language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Merging strategies</head><p>We examined two simple merging strategies: normalizing the individual scores and two step RSV <ref type="bibr" coords="4,453.22,272.94,10.37,8.72" target="#b6">[7]</ref>.</p><p>The first strategy consists of normalizing the first N document scores to fit in the [0,1] interval, then using the normalized scores to produce the final ranked document list. This strategy is easy, requires no training but it has been proved inferior to regression-based models or two-step RSV.</p><p>Two-step RSV is a reindexing-based method: top ranked documents from each collection are translated to the topic language, then reindexed. Note that this is fundamentally different from translating the test collection, which we would like to avoid. Only top documents are translated, instead of a large test collection. However, the disadvantage of this method is that translation and reindexing need to be done online. Document caching can somewhat alleviate this problem when there are many queries.</p><p>Translation is done on a word-by-word basis, using the translation matrix built from the parallel corpus. We use only the first two translations for efficiency; however, we allocate S slots to each untranslated word and distribute the translated words proportionally to their normalized translation probabilities. Due to lack of running time, official runs had S=3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Official Runs (Small Multilingual)</head><p>All our official runs use the Title and Description fields.  Note that in this case the feedback made little difference among the best runs. The merging strategy had a significant impact, with the two-step RSV being better as expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>Our main goal in participating in this year's CLEF was to prove that freedom from opaque, uncontrollable commercial systems does not have to mean poor CLIR performance for European languages. Many conceptual or implementation-related improvements can be made. They include better solutions for using a pivot language, especially when the domains do not match; better morphological processing, pseudo-relevant regression for merging etc.</p><p>6 References</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,69.32,535.14,38.57,8.72;4,140.48,535.14,212.86,8.72;4,140.48,546.30,22.25,8.72;4,363.68,535.14,22.78,8.72;4,468.80,535.14,26.81,8.72;4,69.32,557.94,46.06,8.72;4,140.48,557.94,212.86,8.72;4,140.48,569.22,22.25,8.72;4,363.68,557.94,20.69,8.72;4,468.80,558.18,26.81,8.72;4,69.32,580.86,53.09,8.72;4,140.48,580.86,208.02,8.72;4,363.70,580.86,22.77,8.72;4,468.81,580.86,26.78,8.72;4,69.32,592.62,279.20,8.72;4,363.71,592.62,20.69,8.72;4,468.80,592.86,26.81,8.72"><head></head><label></label><figDesc>cmuM4fb EN: 5/30, FR:10/20-5/20, ES:5/20-10/20, DE:15/20-30, FR:10/20-5/20, ES:5/20-10/20, DE:15/20-30, FR:0/0-5/20, ES:0/0-10/20, DE:5/20-10/30 Norm 0.3398 cmuM4lowfbre EN: 5/30, FR:0/0-5/20, ES:0/0-10/20, DE:5/20-10/30 2step 0.3773</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,69.32,150.18,418.48,52.41"><head>Table 1 : Size of the European Parliament parallel corpora (in sentences)</head><label>1</label><figDesc></figDesc><table coords="2,69.32,172.38,418.48,30.21"><row><cell>DE-IT</cell><cell>IT-ES</cell><cell>DE-EN</cell><cell>FR-EN</cell><cell>IT-EN</cell><cell>ES-EN</cell></row><row><cell>128505</cell><cell>150910</cell><cell>659773</cell><cell>674770</cell><cell>687890</cell><cell>738772</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,69.32,537.78,434.62,121.88"><head>Table 2 : Official Bilingual Runs Run Name Task Feedback docs/terms Parallel/Pivot Avg. Precision</head><label>2</label><figDesc></figDesc><table coords="3,69.32,566.70,401.45,92.96"><row><cell>cmuG2Icombfb</cell><cell>G2I</cell><cell>10/150</cell><cell>Pivot</cell><cell>0.3439</cell></row><row><cell>cmuG2Icomb</cell><cell>G2I</cell><cell>0/0</cell><cell>Pivot</cell><cell>0.3124</cell></row><row><cell>cmuG2Iparafb</cell><cell>G2I</cell><cell>10/150</cell><cell>Parallel</cell><cell>0.4117</cell></row><row><cell>cmuG2Ipara</cell><cell>G2I</cell><cell>0/0</cell><cell>Parallel</cell><cell>0.3669</cell></row><row><cell>cmuI2Scombfb</cell><cell>I2S</cell><cell>15/80</cell><cell>Pivot</cell><cell>0.4269</cell></row><row><cell>cmuI2Scomb</cell><cell>I2S</cell><cell>0/0</cell><cell>Pivot</cell><cell>0.4114</cell></row><row><cell>cmuI2Sparafb</cell><cell>I2S</cell><cell>15/80</cell><cell>Parallel</cell><cell>0.2921</cell></row><row><cell>cmuI2Spara</cell><cell>I2S</cell><cell>0/0</cell><cell>Parallel</cell><cell>0.4154</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,69.32,506.22,433.82,26.12"><head>Table 3 : Official Multilingual Runs Run Name Feedback docs/terms pre-and post translation Norm/2step merging Avg. Pr.</head><label>3</label><figDesc></figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,84.44,173.58,442.66,8.72;5,69.32,184.86,224.75,8.72" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,263.12,173.58,80.65,8.72">Eurospider at CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gohring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shauble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,472.28,173.58,54.82,8.72;5,69.32,184.86,173.22,8.72">Results of the CLEF2002 cross-language evaluation forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="5,83.84,200.82,443.23,8.72;5,69.32,211.98,244.90,8.72" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,320.21,200.82,206.86,8.72;5,69.32,211.98,82.64,8.72">The Mathematics of Statistical Machine Translation: Parameter Estimation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,158.39,211.98,102.21,8.72">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="312" />
			<date type="published" when="1993">1993. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,83.24,228.18,443.81,8.72;5,69.32,239.22,179.18,8.72" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,120.67,228.18,206.15,8.72">Cross-language Retrieval Experiments at CLEF-2002</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,428.84,228.18,98.21,8.72;5,69.32,239.22,127.48,8.72">Results of the CLEF2002 cross-language evaluation forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="5,83.26,255.42,384.17,8.72" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,224.93,255.42,142.60,8.72">Arabic Information Retrieval at IBM</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Mccarley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,373.23,255.42,22.83,8.72">TREC</title>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,83.24,277.74,430.06,8.72" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,240.54,277.74,168.41,8.72">TREC 2002 Cross-lingual Retrieval at BBN</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,417.00,277.74,96.30,8.72">TREC 2002 proceedings</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,83.12,300.06,403.74,8.72" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="5,124.38,300.06,278.64,8.72">Europarl: A Multilingual Corpus for Evaluation of Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Draft, Unpublished</note>
</biblStruct>

<biblStruct coords="5,83.60,316.26,443.41,8.72;5,69.32,327.42,349.94,8.72" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,287.60,316.26,235.87,8.72">SINAI on CLEF 2002: Experiments with merging strategies</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><forename type="middle">M</forename><surname>Martinez-Santiago</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Urena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,139.76,327.42,228.04,8.72">Results of the CLEF2002 cross-language evaluation forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="5,83.36,343.38,443.72,8.72;5,69.32,354.66,305.62,8.72" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,215.57,343.38,152.66,8.72">Improved Statistical Alignment Models</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,386.00,343.38,141.08,8.72;5,69.32,354.66,178.30,8.72">Proc. of the 38th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>of the 38th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Hongkong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,83.72,370.74,443.34,8.72;5,69.32,381.90,93.02,8.72" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,208.39,370.74,144.12,8.72">Experiments using the Lemur toolkit</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,369.92,370.74,157.14,8.72;5,69.32,381.90,88.47,8.72">Proceedings of the Tenth Text Retrieval Conference (TREC-10)</title>
		<meeting>the Tenth Text Retrieval Conference (TREC-10)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,88.88,397.98,438.14,8.72;5,69.32,409.02,195.28,8.72" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,153.18,397.98,270.06,8.72">A stemming procedure and stopword list for general French corpora</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,430.47,397.98,96.55,8.72;5,69.32,409.02,123.05,8.72">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="944" to="952" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,91.40,425.22,435.59,8.72;5,69.32,436.38,328.06,8.72" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="5,163.48,425.22,330.96,8.72">Report on CLEF-2002 Experiements: Combining multiple sources of evidence</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,117.80,436.38,228.04,8.72">Results of the CLEF2002 cross-language evaluation forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
