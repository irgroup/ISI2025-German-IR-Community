<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.80,146.03,391.38,18.08;1,249.83,167.95,103.34,18.08">Spoken Document Retrieval experiments with IR-n system</title>
				<funder ref="#_dA4cYrU #_TUrnTKb">
					<orgName type="full">Spanish Government (CICYT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,202.49,203.01,70.09,10.46"><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
							<email>llopis@dlsi.ua.es</email>
						</author>
						<author>
							<persName coords="1,295.28,203.01,105.25,10.46"><forename type="first">Patricio</forename><surname>Martínez-Barco</surname></persName>
							<email>patricio@dlsi.ua.es</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Información Departamento de Lenguajes y Sistemas</orgName>
								<orgName type="laboratory">Grupo de investigación en Procesamiento del Lenguaje y Sistemas de</orgName>
								<orgName type="institution">Informáticos</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.80,146.03,391.38,18.08;1,249.83,167.95,103.34,18.08">Spoken Document Retrieval experiments with IR-n system</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D75496605F223DC5A44E09E67E938707</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the first participation of IR-n system at Spoken Document Retrieval, focusing on the experiments we made before participation and showing the results we obtained. IR-n system is an Information Retrieval system based on passages and the recognition of sentences to define them. So, the main goal of this experiment is to adapt IR-n system to the spoken document structure by means of the utterance splitter and the overlapping passage technique allowing to match utterances and sentences</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Usually, research work on natural language processing has started from written documents instead of spoken documents due to spoken document processing has a lot of disadvantages induced by its informal disposition among other reasons.</p><p>As appointed by <ref type="bibr" coords="1,179.99,490.78,54.11,10.46">Dahlbäck [2]</ref>: "... spoken input is often incomplete, incorrect and contains interruptions and repairs; full sentences occur only very occasionally. Therefore new basic units for the development of dialogue models have to be proposed ..." Thus, some of the most important problems to solve in spoken document processing are <ref type="bibr" coords="1,492.38,538.59,9.96,10.46" target="#b2">[3]</ref>:</p><p>• The lack of punctuation marks, that impedes the well understanding of sentences because boundaries are unknown. This understanding must be induced by pause detection. This is the reason why the "sentence" concept is replaced by the "utterance" concept. Utterance is defined, from a pragmatic point of view, as a sequence of words chained by a speaker between two pauses. In the same way, the "paragraph" is replaced by the "turn" that is defined from a pragmatic point of view as the set of utterances that a speaker can express between two speaker changes (when several speakers participate in the dialogue), or the set of utterances that a speaker expresses about the same subject (in monologues or newsreels).</p><p>• Moreover, turns may be considered like null or empty when they do not contribute to the discourse, that is, turns having the function of pointing out the speaker is on the conversation: "ejem...", "yes...", "I know..."; as well as other turns without semantic content such as "good morning", "have a good weekend", and so on.</p><p>• Furthermore, turns can be interrupted due to overlaps, or speaker mistakes, causing repetitions and modifications of previous information.</p><p>This sort of problems is increased with problems derived from the automatic transcription process which incorporates noise, spelling mistakes, and unrecognizable words due to deficiencies in the original recording or speak recognition fails.</p><p>Due to this, the use of spoken documents in information retrieval tasks allows to test the system robustness against document mistakes. Then, the main goal of this paper is to test the robustness of IR-n System and to study some text processing techniques that could improve this robustness in spoken documents.</p><p>IR-n is an Information Retrieval system based on passages <ref type="bibr" coords="2,360.47,194.04,23.43,10.46">[4] [5]</ref>. Passages are defined using a fixed number of sentences from the original document. It seems obvious that IR-n has been developed to work on written documents with a clear structure based on known sentence boundaries. However, in order to test its robustness, IR-n has been submitted to the CLEF SDR Track.</p><p>SDR task is based on processing non-structured documents that proceed from an automatic transcription of radio news. Our main objective is to test if IR-n system can be applied to document collections where sentence boundaries are unknown. This experiment is focused on the estimation of sentence boundaries by means of the pauses recognized along the transcription process. So, the main hypothesis is based on the following ideas:</p><p>• longest pauses mean the end of utterances</p><p>• IR-n System can accept utterances instead of sentences to define passages. So, the experiments will focus on determining what is the average length of a pause between utterances to build an utterance splitter that will feed the IR-n system.</p><p>However, using this model, passage definitions may be faulty. The terms of a query may be dispersed among several passages, and some relevant documents may be discarded. This problem can be avoid by using passage overlapping, since this technique allows more than one passage sharing the same fragment of document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CL-SDR Track description</head><p>Cross-Language Spoken Document Retrieval (CL-SDR) is a new track proposed for CLEF 2003. The track is mostly based on existing resources, available by NIST, which were used at TREC-8 <ref type="bibr" coords="2,90.01,487.91,10.52,10.46" target="#b5">[6]</ref> and TREC-9 <ref type="bibr" coords="2,163.33,487.91,9.96,10.46" target="#b6">[7]</ref>.</p><p>The benchmark track is an extension of evaluation data prepared by NIST for TREC 8-9 SDR tracks. It has a collection of automatic transcripts (557 hours) of American-English news recordings broadcasted by ABC, CNN, Public Radio International, and Voice of America between February and June 1998. Transcripts are provided with known story boundaries (21,754 stories); and a collection of 100 English topics, either in terse or short format. The TREC collection has been extended with translations of the short topics into five European languages: Dutch, Italian, French, German, and Spanish.</p><p>Technical specifications of the task are shown in table <ref type="table" coords="2,343.19,583.56,3.87,10.46">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Passage definition at IR-n system</head><p>Taking advantage of using sentences in IR-n as a basic unit to the passage definition task, the sentence will be used to define the passage overlapping too.</p><p>The overlapping degree (G sol ) in IR-n system shows the sentence number from which the definition of the next passage starts. The main features of this value are the following:</p><p>1. G sol must be lower than the passage size. Having the same value means that no overlapping is used.</p><p>2. The lower the value G sol is, the higher the amount of text shared by two consecutive passages will be.</p><p>• Objective: the track aims at evaluating CLIR systems on noisy automatic transcripts of spoken documents with known story boundaries.</p><p>• Development data (from TREC 8 SDR):</p><p>1. Document collection: B1SK Baseline Transcripts, known bounds download from NIST.</p><p>2. Topics: Short topics in English, Dutch, French, German, Italian, and Spanish.</p><p>3. Relevance assessments: Topics-074-123.</p><p>4. Parallel document collections (optional and only available through LDC ): Textual resources.</p><p>• Evaluation data (from TREC 9 SDR):</p><p>1. Document collection: B1SK Baseline Transcripts, known bounds download from NIST.</p><p>2. Topics: Short topics in English , Dutch, French, German, Italian, and Spanish.</p><p>3. Relevance assessments: Topics-124-173.</p><p>4. Parallel document collections (optional and only available through LDC ): Textual resources.</p><p>• Primary Conditions (mandatory for all participants):</p><p>1. Monolingual IR without using any parallel collection (contrastive condition).</p><p>2. Bilingual IR from French or German.</p><p>• Secondary Condition (optional):</p><p>1. Monolingual IR using any available parallel collections.</p><p>2. Bilingual IR from other languages.</p><p>• Submission of runs:</p><p>1. Maximum 12 runs per participant, with the limit of 3 runs for each considered source language.</p><p>Table <ref type="table" coords="3,183.40,424.04,3.87,10.46">1</ref>: Technical specifications of the CLEF'2003 CL-SDR Track 3. As a result, the lower the value G sol is, the more number of passages will be defined in the document.</p><p>The use of passage overlapping means to redefine the passage concept to IR-n in the following way:</p><p>-Given a document D consisting of N sentences.</p><formula xml:id="formula_0" coords="3,289.39,544.30,223.61,11.35">D = f 1 ..f N (1)</formula><p>-Taken into account that n is the number of sentences integrating a passage.</p><p>-Given an overlapping degree G sol -The following passages will be defined from the document D</p><formula xml:id="formula_1" coords="3,181.72,637.23,331.28,12.32">P i = f G sol * (i-1)+1 , ..., f min(G sol * (i-1)+n,N ) , i ∈ [1..N/G sol -1]<label>(2)</label></formula><p>Given that definition, and supposing a passage size of 15 sentences, an overlapping degree of 10, and a document size of 35 sentences, the passage generation will be performed in the following way:</p><formula xml:id="formula_2" coords="3,102.18,704.39,68.55,50.94">1. P 1 = f 1 ..f 15 2. P 2 = f 11 ..f 25 3. P 3 = f 21 ..f 35</formula><p>The increase of the efficiency in document retrieval is an immediately advantage of passage overlapping. However, the response time increases (to a large extent when the overlapping degree is lower) because the number of passages to be evaluated is greater.</p><p>Nevertheless, the use of lower overlapping degrees improves the system results noticeably, and it has not excessive influence on the searching time.</p><p>Overlapping does not increase the searching cost so much due to two main reasons:</p><p>1. IR-n does not evaluate each one of the document passages, since the similarity measure <ref type="bibr" coords="4,491.47,189.26,10.52,10.46" target="#b0">[1]</ref> in some cases may be avoided. The first passage to be evaluated is the one starting in the first sentence of the document in which a query term appears. That is due to passages starting in a previous sentence can not obtain a similarity measure higher than this first passage, by the way in which the similarity measure has been defined in IR-n.</p><p>For this same reason, the last passage to be evaluated is the one finishing in the last sentence of the document in which a query term appears.</p><p>These same conclusions may be extended to passages not located at the limits of the document, that is, internal passages. Given an overlapping degree G sol , if a passage does not contain query terms during its first sentences then its evaluation can be omitted. For example, if G sol is equal to 1, the evaluation of those passages which first sentence does not contain any query term is not needed.</p><p>Because of this, the number of passages to be evaluated is reduced, and, consequently, to use of small overlapping degrees has not the same influence as if each passage of the document is evaluated.</p><p>2. Another important aspect is related to the system implementation. IR-n implementation is based on storing all the information about word occurrences in main memory. Thus, the segmentation process is performed during the execution over data structures located at main memory.</p><p>Considering that the most influencing factors to time processing are related to disc access times, this minor increase of time when a greater number of passages is processed, it is not significant to the final time.</p><p>For this reason, IR-n uses an overlapping degree (G sol =1) being the value that obtains the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental work</head><p>According to the track specification, the test collection used in this experiment was TREC-8. During these experiments several passages sizes (from 1 to 9 sentences) and several pause recognition sizes (0.1, 0.2, and 0.3 seconds) have been valuated. Moreover, the IR-n system with and without query expansion has been tested.</p><p>Tables 2, 3 and 4 show the results without query expansion. Tables 5, 6 and 7 show the results with query expansion. These tables show that the best result is obtained using the model with query expansion, a passage size of 5 sentences and 0.2 seconds to recognize a pause between two utterances at the utterance splitter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">System evaluation</head><p>This system was evaluated with the TREC SDR-9 collection according to the track specification. Moreover, a bilingual test was performed using French queries that were translated into English by Power Translator, Free-translator and Babel Fish.</p><p>Both monolingual and bilingual tests were performed with and without query expansion. The best results for monolingual and bilingual queries are shown in tables 8 and 9 respectively. Although we expected to know more information about other systems at the conference, we are pleased to see these results being above average for SDR track, taking into account that IR-n system was not designed to work on spoken documents.</p><p>Nevertheless, more experiments are expected to be done to increase the system performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,111.90,135.71,379.20,351.19"><head>Table 2 :</head><label>2</label><figDesc>Training results without query expansion using 0.1 seconds to discover pauses</figDesc><table coords="5,134.23,135.71,334.55,351.19"><row><cell></cell><cell></cell><cell cols="3">Precision at N documents</cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>200</cell><cell>AvgP</cell></row><row><cell cols="7">IR-n 1 F K3 78.49 0.4980 0.4490 0.3398 0.2837 0.1041 0.3301</cell></row><row><cell cols="7">IR-n 2 F K3 79.26 0.5347 0.4633 0.3612 0.3102 0.1067 0.3540</cell></row><row><cell cols="7">IR-n 3 F K3 79.43 0.5429 0.4735 0.3602 0.3095 0.1099 0.3695</cell></row><row><cell cols="7">IR-n 4 F K3 79.81 0.5592 0.4735 0.3786 0.3204 0.1106 0.3774</cell></row><row><cell cols="7">IR-n 5 F K3 79.65 0.5469 0.4878 0.3786 0.3224 0.1107 0.3812</cell></row><row><cell cols="7">IR-n 6 F K3 80.09 0.5633 0.5102 0.3888 0.3293 0.1120 0.3845</cell></row><row><cell cols="7">IR-n 7 F K3 80.14 0.5796 0.4980 0.3878 0.3279 0.1123 0.3852</cell></row><row><cell cols="7">IR-n 8 F K3 80.20 0.5796 0.4980 0.3888 0.3265 0.1135 0.3850</cell></row><row><cell cols="7">IR-n 9 F K3 80.31 0.5755 0.5000 0.3888 0.3197 0.1141 0.3817</cell></row><row><cell></cell><cell></cell><cell cols="3">Precision at N documents</cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>200</cell><cell>AvgP</cell></row><row><cell cols="7">IR-n 1 F K3 78.71 0.4898 0.4429 0.3490 0.2884 0.1052 0.3343</cell></row><row><cell cols="7">IR-n 2 F K3 79.26 0.5306 0.4694 0.3602 0.3095 0.1073 0.3600</cell></row><row><cell cols="7">IR-n 3 F K3 79.92 0.5633 0.4796 0.3786 0.3122 0.1101 0.3756</cell></row><row><cell cols="7">IR-n 4 F K3 79.70 0.5755 0.4959 0.3806 0.3204 0.1112 0.3825</cell></row><row><cell cols="7">IR-n 5 F K3 80.09 0.5755 0.5000 0.3888 0.3231 0.1121 0.3834</cell></row><row><cell cols="7">IR-n 6 F K3 80.14 0.5714 0.4878 0.3816 0.3245 0.1132 0.3823</cell></row><row><cell cols="7">IR-n 7 F K3 80.42 0.5714 0.4837 0.3857 0.3136 0.1145 0.3801</cell></row><row><cell cols="7">IR-n 8 F K3 80.64 0.5837 0.5000 0.3898 0.3177 0.1150 0.3842</cell></row><row><cell cols="7">IR-n 9 F K3 80.42 0.5796 0.5000 0.3949 0.3184 0.1142 0.3856</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,111.90,498.30,379.20,205.79"><head>Table 3 :</head><label>3</label><figDesc>Training results without query expansion using 0.2 seconds to discover pauses</figDesc><table coords="5,134.23,570.09,334.55,134.00"><row><cell></cell><cell></cell><cell cols="3">Precision at N documents</cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>200</cell><cell>AvgP</cell></row><row><cell cols="7">IR-n 1 F K3 78.88 0.4653 0.4347 0.3429 0.3007 0.1063 0.3341</cell></row><row><cell cols="7">IR-n 2 F K3 79.48 0.5469 0.4796 0.3745 0.3068 0.1088 0.3687</cell></row><row><cell cols="7">IR-n 3 F K3 79.98 0.5469 0.4776 0.3714 0.3238 0.1110 0.3784</cell></row><row><cell cols="7">IR-n 4 F K3 80.36 0.5837 0.4816 0.3796 0.3211 0.1128 0.3805</cell></row><row><cell cols="7">IR-n 5 F K3 80.20 0.5837 0.4796 0.3837 0.3136 0.1138 0.3794</cell></row><row><cell cols="7">IR-n 6 F K3 80.25 0.5878 0.4755 0.3816 0.3082 0.1145 0.3729</cell></row><row><cell cols="7">IR-n 7 F K3 80.25 0.5837 0.4837 0.3847 0.3095 0.1140 0.3751</cell></row><row><cell cols="7">IR-n 8 F K3 80.58 0.5714 0.4735 0.3796 0.3156 0.1135 0.3712</cell></row><row><cell cols="7">IR-n 9 F K3 80.64 0.5796 0.4633 0.3755 0.3156 0.1131 0.3672</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,111.90,715.48,379.20,10.46"><head>Table 4 :</head><label>4</label><figDesc>Training results without query expansion using 0.3 seconds to discover pauses</figDesc><table coords="6,139.76,117.84,323.49,134.00"><row><cell></cell><cell></cell><cell cols="3">Precision at N documents</cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>200</cell><cell>AvgP</cell></row><row><cell cols="7">IR-n 1 F1 83.44 0.5347 0.5082 0.3959 0.3320 0.1119 0.4029</cell></row><row><cell cols="7">IR-n 2 F1 83.94 0.6000 0.5143 0.4133 0.3422 0.1161 0.4307</cell></row><row><cell cols="7">IR-n 3 F1 84.98 0.5959 0.5204 0.4112 0.3544 0.1170 0.4373</cell></row><row><cell cols="7">IR-n 4 F1 85.42 0.6041 0.5388 0.4143 0.3517 0.1176 0.4392</cell></row><row><cell cols="7">IR-n 5 F1 85.15 0.5959 0.5408 0.4255 0.3612 0.1192 0.4494</cell></row><row><cell cols="7">IR-n 6 F1 85.20 0.6204 0.5306 0.4378 0.3653 0.1208 0.4530</cell></row><row><cell cols="7">IR-n 7 F1 85.42 0.6000 0.5327 0.4327 0.3680 0.1212 0.4503</cell></row><row><cell cols="7">IR-n 8 F1 85.31 0.6082 0.5327 0.4367 0.3653 0.1215 0.4528</cell></row><row><cell cols="7">IR-n 9 F1 85.37 0.6041 0.5388 0.4347 0.3639 0.1218 0.4489</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,119.10,263.24,364.80,170.02"><head>Table 5 :</head><label>5</label><figDesc>Training results with query expansion using 0.1 seconds to discover pauses</figDesc><table coords="6,139.76,299.26,323.49,134.00"><row><cell></cell><cell></cell><cell cols="3">Precision at N documents</cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>200</cell><cell>AvgP</cell></row><row><cell cols="7">IR-n 1 F1 82.51 0.5429 0.5102 0.4020 0.3361 0.1144 0.4160</cell></row><row><cell cols="7">IR-n 2 F1 84.43 0.5837 0.5469 0.4194 0.3476 0.1167 0.4421</cell></row><row><cell cols="7">IR-n 3 F1 85.09 0.5837 0.5449 0.4265 0.3497 0.1172 0.4540</cell></row><row><cell cols="7">IR-n 4 F1 85.37 0.6163 0.5429 0.4306 0.3578 0.1194 0.4606</cell></row><row><cell cols="7">IR-n 5 F1 85.53 0.6041 0.5469 0.4408 0.3680 0.1206 0.4620</cell></row><row><cell cols="7">IR-n 6 F1 85.64 0.6041 0.5490 0.4398 0.3653 0.1212 0.4619</cell></row><row><cell cols="7">IR-n 7 F1 85.92 0.6041 0.5367 0.4337 0.3639 0.1219 0.4584</cell></row><row><cell cols="7">IR-n 8 F1 85.97 0.6000 0.5408 0.4378 0.3633 0.1220 0.4596</cell></row><row><cell cols="7">IR-n 9 F1 85.86 0.6041 0.5347 0.4398 0.3612 0.1226 0.4594</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,119.10,444.66,364.80,170.02"><head>Table 6 :</head><label>6</label><figDesc>Training results with query expansion using 0.2 seconds to discover pauses F1 83.44 0.5551 0.4959 0.4102 0.3435 0.1177 0.4154 IR-n 2 F1 84.76 0.6000 0.5245 0.4224 0.3558 0.1211 0.4385 IR-n 3 F1 85.26 0.6531 0.5286 0.4337 0.3605 0.1238 0.4527 IR-n 4 F1 85.15 0.6286 0.5367 0.4235 0.3680 0.1248 0.4520 IR-n 5 F1 85.15 0.6327 0.5388 0.4265 0.3653 0.1254 0.4540 IR-n 6 F1 85.04 0.6367 0.5306 0.4276 0.3694 0.1257 0.4544 IR-n 7 F1 85.26 0.6367 0.5347 0.4255 0.3639 0.1254 0.4564 IR-n 8 F1 85.53 0.6367 0.5327 0.4316 0.3646 0.1251 0.4554 IR-n 9 F1 85.48 0.6367 0.5367 0.4265 0.3565 0.1252 0.4518</figDesc><table coords="6,139.76,480.68,319.81,35.17"><row><cell></cell><cell></cell><cell cols="3">Precision at N documents</cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>200</cell><cell>AvgP</cell></row><row><cell>IR-n 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,119.10,626.07,364.80,95.90"><head>Table 7 :</head><label>7</label><figDesc>Training results with query expansion using 0.3 seconds to discover pauses</figDesc><table coords="6,258.92,662.10,85.15,59.87"><row><cell>System</cell><cell>AvgP</cell></row><row><cell>ITC-irst</cell><cell>0.3944</cell></row><row><cell>Exeter</cell><cell>0,3843</cell></row><row><cell>Alicante</cell><cell>0,3637</cell></row><row><cell cols="2">JHU/APL 0,3192</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,110.53,733.37,381.94,10.46"><head>Table 8 :</head><label>8</label><figDesc>Monolingual results with query expansion using 0.3 seconds to discover pauses</figDesc><table coords="7,258.92,109.16,85.15,59.87"><row><cell>System</cell><cell>AvgP</cell></row><row><cell>ITC-irst</cell><cell>0.3064</cell></row><row><cell>Alicante</cell><cell>0,3032</cell></row><row><cell>Exeter</cell><cell>0,2876</cell></row><row><cell cols="2">JHU/APL 0,1941</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,90.00,180.43,395.07,43.17"><head>Table 9 :</head><label>9</label><figDesc>Bilingual results with query expansion using 0.3 seconds to discover pauses 6 Conclusions and future work</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>This work has been partially supported by the <rs type="funder">Spanish Government (CICYT)</rs> with grant <rs type="grantNumber">TIC2000-0664-C02-02</rs> and (<rs type="projectName">PROFIT</rs>) with grant <rs type="grantNumber">FIT-150500-2002-416</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_dA4cYrU">
					<idno type="grant-number">TIC2000-0664-C02-02</idno>
					<orgName type="project" subtype="full">PROFIT</orgName>
				</org>
				<org type="funding" xml:id="_TUrnTKb">
					<idno type="grant-number">FIT-150500-2002-416</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.50,391.44,407.51,10.46;7,105.50,403.40,407.50,10.46;7,105.50,415.35,407.51,10.46;7,105.50,427.30,236.21,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,488.04,391.44,24.96,10.46;7,105.50,403.40,293.18,10.46">Question Answering: CNLP at the TREC-10 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Diekema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taffet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mccracken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ozgencil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Yilmazel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Liddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,420.48,403.40,92.52,10.46;7,105.50,415.35,96.69,10.46">Tenth Text REtrieval Conference (Notebook)</title>
		<meeting><address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<publisher>National Institute of Standards and Technology</publisher>
			<date type="published" when="2001-11">nov 2001</date>
			<biblScope unit="page" from="500" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,447.23,407.51,10.46;7,105.50,459.18,407.51,10.46;7,105.50,471.14,234.29,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,169.10,447.23,128.65,10.46">Towards a dialogue taxonomy</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dahlbäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,188.84,459.18,214.16,10.46">Dialogue Processing in Spoken Language Systems</title>
		<title level="s" coord="7,481.67,459.18,31.33,10.46;7,105.50,471.14,130.57,10.46">Lecture Notes in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">Elisabeth</forename><surname>Maier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marion</forename><surname>Mast</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Susann</forename><surname>Luperfoy</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">1236</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,491.06,407.50,10.46;7,105.50,503.02,407.50,10.46;7,105.50,514.98,407.51,10.46" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Fernández</surname></persName>
		</author>
		<title level="m" coord="7,185.46,491.06,327.54,10.46;7,105.50,503.02,347.95,10.46">Un modelo para la especificación lingüística y la gestión computacional en diálogos hombre-máquina mediante instrucciones expresadas en lenguaje natural</title>
		<meeting><address><addrLine>Sevilla</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>Universidad de Sevilla, Departamento de Filología Inglesa. Facultad de Filología</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="7,105.50,534.90,407.50,10.46;7,105.50,546.86,407.50,10.46;7,105.50,558.81,300.45,10.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,270.90,534.90,215.87,10.46">IR-n system, a passage retrieval system at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,117.97,546.86,269.48,10.46">Workshop of Cross-Language Evaluation Forum (CLEF 2001)</title>
		<title level="s" coord="7,395.33,546.86,117.67,10.46;7,105.50,558.81,30.03,10.46">Lecture notes in Computer Science</title>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,578.74,407.51,10.46;7,105.50,590.69,407.50,10.46;7,105.50,602.64,259.59,10.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,380.71,578.74,110.93,10.46">IR-n system at Clef-2002</title>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Ferrández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,105.50,590.69,276.53,10.46">Workshop of Cross-Language Evaluation Forum (CLEF 2002)</title>
		<title level="s" coord="7,391.51,590.69,121.49,10.46;7,105.50,602.64,30.03,10.46">Lecture notes in Computer Science</title>
		<meeting><address><addrLine>Roma, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,622.57,407.50,10.46;7,105.50,634.53,281.60,10.46" xml:id="b5">
	<monogr>
		<title level="m" coord="7,105.50,622.57,144.15,10.46">Eighth Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="1999-11">nov 1999</date>
			<biblScope unit="page" from="500" to="246" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,654.46,407.50,10.46;7,105.50,666.41,281.60,10.46" xml:id="b6">
	<monogr>
		<title level="m" coord="7,105.50,654.46,141.74,10.46">Ninth Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="2000-11">nov 2000</date>
			<biblScope unit="page" from="500" to="249" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
