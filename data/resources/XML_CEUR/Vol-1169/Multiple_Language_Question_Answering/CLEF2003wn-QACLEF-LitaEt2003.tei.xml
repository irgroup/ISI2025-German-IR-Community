<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,182.84,108.47,230.71,12.30">Cross Lingual QA: A Modular Baseline</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,199.16,133.98,65.38,8.72"><forename type="first">Lucian</forename><forename type="middle">Vlad</forename><surname>Lita</surname></persName>
							<email>llita@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.57,133.98,56.52,8.72"><forename type="first">Monica</forename><surname>Rogati</surname></persName>
							<email>mrogati@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,333.54,133.98,63.50,8.72"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,182.84,108.47,230.71,12.30">Cross Lingual QA: A Modular Baseline</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1D4DE107BD729A920E232E3C07FD840A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work evaluates the applicability of currently available research tools to the problem of cross lingual QA. We establish a task baseline by combining a cross lingual IR system with a monolingual QA system in a very short amount of time. A higher precision strategy involves applying the monolingual QA system to an automatically translated question, assumed to be correct. A higher coverage strategy consists of a term weighted proximity measure with varied query expansion, tuned for each individual question type.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In our CLEF 2003 participation we evaluate the application of existing research modules to Cross-Lingual Question Answering (CLQA). The obvious first step towards solving this task is to combine cross lingual information retrieval with monolingual question answering. In order to set up a baseline with very little effort -one week's worth of work -we glued two existing off-the-(authors' research)-shelf components: a cross lingual information retrieval system and a monolingual question answering system, and tuned them on available question/answer datasets.</p><p>We have participated with two runs in the cross-lingual French-to-English CLEF task and we focused on quickly obtaining a system based on available tools and components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>Our CLEF system consists of two pre-existing components: a cross lingual information retrieval system (CLIR) and a monolingual question answering system (MQA), plus the necessary glue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The CLIR Component</head><p>The cross lingual information retrieval component <ref type="bibr" coords="1,271.39,474.30,11.39,8.72" target="#b3">[4]</ref> is a system trained for the CLEF 2003 cross lingual retrieval task. It uses a parallel corpus to train a translation model, which is then used for query translation. The system uses GIZA++ <ref type="bibr" coords="1,107.72,496.62,11.51,8.72" target="#b1">[2]</ref> to train the translation model and a retrieval system based on Lemur <ref type="bibr" coords="1,412.39,496.62,10.38,8.72" target="#b2">[3]</ref>. No proprietary machine translation systems including SysTran, Google etc, have been used and the parallel corpus is freely available.</p><p>The CLIR system produces both a list of relevant documents as well as a translated expanded query with corresponding weights for each word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The MQA Component</head><p>The monolingual question answering component is a high precision, pattern based QA system that relies on very few resources. The system is trained on the TREC <ref type="bibr" coords="1,252.51,590.46,11.56,8.72" target="#b5">[6]</ref> QA task datasets and has a limited question type coverage.</p><p>The MQA system implements a simplified version of the widespread pipeline QA architecture. Initially, the questions are filtered and classified into question types and relevant question terms are extracted. A straightforward sentence-level retrieval follows, producing candidate sentences in tokenized form. In the answer extraction phase, high confidence finite state transducers (FSTs) are applied and candidate answers are produced with their corresponding confidence scores. Answers with similar surface forms are grouped and unified into a stronger representative answer with a higher score.</p><p>Currently, no answer verification is performed and no feedback loops are present. The MQA system was built to rely on as few resources as possible. Hence, the transducers are based on surface form, capitalization features, WordNet <ref type="bibr" coords="2,69.32,107.10,11.51,8.72" target="#b6">[7]</ref> based features, and a short list of grammatical constructs. Named entity taggers and gazetteers are the two ubiquitous elements in QA architectures that are missing from our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Architecture</head><p>Our simple, ad-hoc architecture sets up an obvious baseline for the CLQA task. We approach the problem through two methods: a high precision method that is likely to answer few questions -especially given imperfect translations -and a higher recall method that covers most of the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t r a n s l a t i o n</head><p>A N S W E R S E T QUESTION d o c u m e n t s Figure <ref type="figure" coords="2,132.32,279.30,3.67,8.72">1</ref>. High precision approach</p><p>In our higher precision method, the source language question is first run through SysTran <ref type="bibr" coords="2,455.70,301.74,10.48,8.72" target="#b4">[5]</ref>, a proprietary translation system with a free, limited, online interface. The un-altered English translation is then passed to the MQA component, which produces a list of answers, ordered by confidence scores. The documents used in answer extraction are produced by the CLIR component. If any answers are obtained via this strategy, the system offers them as the final set of answers. In case no answers were extracted, the higher recall method is activated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RUN 1 query terms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QUESTION d o c u m e n t s A N S W E R S E T RUN 2 query terms</head><p>Figure <ref type="figure" coords="2,138.20,469.62,3.67,8.72">2</ref>. High coverage approach. In RUN 1, the query terms are produced directly from the SysTran translation. In RUN 2, the query terms are produced by the CLIR component.</p><p>Our higher coverage strategy also uses the CLIR component for document retrieval. A rudimentary question classification provides the mapping between the question text and answer types. The answer types correspond to entity types obtained by processing the relevant documents with a named entity tagger. Subsequently, given a set of question terms, we apply a term weighted proximity measure to candidate answers of the required type. Evidence for a particular answer is then combined with that of identical or similar answers in order to compute its final score.</p><p>The difference between our two CLEF runs consists in the way the question terms are produced. In the first run, the words in the SysTran translation are filtered using a stopword list, then used with the proximity measure. For the second run, the CLIR component takes over the query expansion and produces a weighted set of terms to be further applied in computing the proximity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In the high precision approach experiments, the system first translates the question from French to English using the SysTran online interface. The un-altered, translated question is considered grammatically and semantically correcti.e. a perfect translation -and is passed to the MQA component. Relevant documents are retrieved using the CLIR component and are also passed to the MQA component, which applies FSTs/patterns to identify candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLIR MQA SysTran</head><p>Proximity Measure NE Tagger CLIR SysTran Using the higher coverage strategy, our system first classifies each question as one of the following types: location, temporal, person, numeric, quantity and other. We obtain the relevant documents using the CLIR component, we tokenize them, and we apply a simple sentence splitting algorithm. The BBN Identifinder <ref type="bibr" coords="3,425.36,129.42,11.51,8.72" target="#b0">[1]</ref> named entity tagger is applied to the processed documents. Entities corresponding to the required question type are identified as candidate answers. For each candidate answer, we compute a term weighted proximity score: score = Σ i (w a * f(d i ) * w qi ) where w a is the term weight for the candidate answer, f(d i ) is a function based on the distance between the candidate answer and i th question term in its proximity, and w qi is the term weight for the i th question term. The term weighting methods considered were: okapi, idf, and ntc. The distance functions explored were the linear, quadratic, and exponential functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parameter tuning</head><p>The training set consisted of approximately one hundred questions selected from TREC 9 &amp; 10 question sets. The original English questions were translated by two French native speakers ♣ . The automatic SysTran translations were corrected in order to provide our system with reasonable training data. We used the limited data for parameter tuning for each individual question type. Table <ref type="table" coords="3,227.00,307.86,3.67,8.72">1</ref>. shows the final parameter set used for the CLEF runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Type Term Weighting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance function</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># expansion terms # documents</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Location</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. Final parameters for the CLEF cross lingual QA task</head><p>The number of query expansion terms was varied from 5 to 200 for the second run. For the first run, the query terms considered were only the non-stopwords in the SysTran translation of the source question. The number of relevant documents retrieved by CLIR was varied from 1 to 30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance at CLEF</head><p>The best MRR score we obtained was 0.1533 under the stricter policy, and 0.17083 under the looser. Our system did not offer any answer for nearly half of the questions, reflecting the fact that it is very conservative in terms of proposing answers with little evidence.</p><p>The monolingual component was trained on short, correct and meaningful English factoid questions. Out of 200 questions, the MQA approach worked on 11 questions, which were translated approximately correct. Out of these 11, it found correct answers for only 4 questions. For the other 7 questions it produced no answers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strict</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Our CLEF 2003 system combines the CLIR and MQA research modules into a CLQA baseline. There are several modifications which could clearly improve the performance. We have employed no question analysis in French. Phrase detection, reformulation, classification by question type and answer type in the source language (French in our case) would clearly improve the performance. Since early errors propagate through the question answering pipeline, accurate question analysis before automatic translation would allow the system to select the appropriate answer extraction method.</p><p>The current question analysis in the target language (English) is a minimal classification into 6 question types. Since automatic translation is less than perfect, phrase identification and reformulation is almost always out of the question.</p><p>The training data consists of factoid questions and answers selected from TREC datasets. The CLEF question set appears to contain more complex questions and questions that contain more content words compared to the TREC style questions. However the fact that our system was tuned on slightly different data than the CLEF test data is not detrimental since it approaches a more likely real life test. The retrieval step is tuned for the CLEF 2002 cross lingual IR task, and the query expansion is performed internally by the CLIR component. A document set of size 20 worked best for all question types. For fewer documents, there was not enough support for correct answers and for a larger document set size there was too much noise. The person question type required a wider query expansion.</p><p>The two runs showed that for this year's CLEF data, the CLIR-produced query expansion in English from the source question text in French is almost as good as using the question terms from the SysTran translation. This is particularly true for longer questions that involve more high-content words. The fact that proprietary components can play a more limited role in the system without drastic performance drops is certainly encouraging because it provides more control over the CLQA process.</p><p>The fact that the two runs were very similar in performance suggests that no question expansion -i.e. using the translated question terms in answer identification -is sufficient given our current basic methods. On the other hand, the lack of exact (SysTran) translation for the higher coverage strategy does not result in significant performance degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work</head><p>Our goal for this year's CLEF participation was to identify research issues for cross lingual question answering using a minimal baseline. In future research, we plan to explore issues such as language dependencies and semi-automatic question analysis for CLQA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,69.32,584.82,430.90,139.04"><head>Table 2 .</head><label>2</label><figDesc>Final CLEF scores for both runs -test set contains 200 questions</figDesc><table coords="3,96.80,584.82,403.42,54.20"><row><cell></cell><cell></cell><cell>% questions w/</cell><cell>Loose</cell><cell>% questions w/</cell><cell># NILs</cell><cell># NILs</cell></row><row><cell></cell><cell>MRR</cell><cell>a correct</cell><cell>MRR</cell><cell>a correct</cell><cell>proposed</cell><cell>correct</cell></row><row><cell></cell><cell></cell><cell>answer</cell><cell></cell><cell>answer</cell><cell></cell><cell></cell></row><row><cell>Run 1</cell><cell>0.1533</cell><cell>19%</cell><cell>0.17083</cell><cell>21%</cell><cell>92</cell><cell>8</cell></row><row><cell>Run 2</cell><cell>0.1316</cell><cell>15.5%</cell><cell>0.14916</cell><cell>17.5%</cell><cell>91</cell><cell>7</cell></row></table><note coords="3,69.32,710.95,4.75,8.22;3,76.64,715.14,116.10,8.72"><p>♣ many thanks to Antoine Raux</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,104.36,618.06,403.01,8.72;4,104.36,629.10,145.66,8.72" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,351.92,618.06,155.45,8.72;4,104.36,629.10,50.40,8.72">Nymble: A High-Performance Learning Name-Finder</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,161.72,629.10,82.73,8.72">Proceedings of ANLP</title>
		<meeting>ANLP</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,104.36,640.38,338.44,8.72" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,203.48,640.38,150.16,8.72">Improved Statistical Alignment Models</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,360.44,640.38,76.92,8.72">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,104.35,651.54,285.51,8.72" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="4,216.20,651.54,140.71,8.72">Experiments using the Lemur toolkit</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>TREC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="4,104.36,662.82,410.68,8.72" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rogati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<title level="m" coord="4,212.61,662.82,269.76,8.72">CONTROL: CLEF-2003 with Open, Transparent Resources Off-Line</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,104.36,673.86,188.37,8.72" xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Systran</surname></persName>
		</author>
		<ptr target="http://babel.altavista.com/translate.dyn" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,104.36,685.02,408.59,8.72" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,185.96,685.02,214.72,8.72">Overview of the TREC 2002 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,407.24,685.02,101.42,8.72">Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,104.36,696.30,213.61,8.72" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="4,104.36,696.30,163.61,8.72">WordNet: An Electronic Lexical Database</title>
		<imprint>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
