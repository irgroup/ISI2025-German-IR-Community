<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,92.13,74.75,410.32,14.50;1,232.41,91.31,130.64,14.50">Cross-Language French-English Question Answering using the DLT System at CLEF 2003</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,252.81,115.11,89.91,8.96"><forename type="first">Richard</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
							<email>richard.sutcliffe@ul.ieigal.gabbay@ul.ie9935029@student.ul.ie</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Documents and Linguistic Technology Group Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.33,126.87,48.95,8.96"><forename type="first">Igal</forename><surname>Gabbay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Documents and Linguistic Technology Group Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.57,138.63,68.32,8.96"><forename type="first">Aoife</forename><surname>O'gorman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Documents and Linguistic Technology Group Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,92.13,74.75,410.32,14.50;1,232.41,91.31,130.64,14.50">Cross-Language French-English Question Answering using the DLT System at CLEF 2003</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2EDD954EB78D3074E9CBDBE8395E26C8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This article outlines the participation of the Documents and Linguistic Technology (DLT) Group in the Cross Language French-English Question Answering Task of the Cross Language Evaluation Forum (CLEF). Our aim was to make an initial study of cross language question answering (QA) by adapting the system built for monolingual English QA for the Text REtrieval Conference (TREC) in 2002 <ref type="bibr" coords="1,383.13,372.87,64.51,8.96" target="#b4">(Sutcliffe, 2003)</ref>. Firstly, therefore, we outline the architecture of the TREC system which formed the basis of the work reported on here. Secondly, the many changes made to allow cross-language QA are described. Thirdly, the runs performed are presented together with the results we obtained. Finally, conclusions are drawn based on our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architecture of the TREC 2002 DLT System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Outline</head><p>In this section we summarise the structure of the DLT system used at TREC which formed the starting point for CLEF. Changes subsequently made are documented in the next section. Overall flow of control was as follows. Firstly, we identified the query type and hence decided upon the related named entities for which we would be searching. Secondly, we parsed the 50 TOPDOCS text files, dividing them into textual units using the markup. These files were supplied by TREC and were generated by their own retrieval system in response to the input query. Thirdly, we searched for instances of the named entities in the textual units and marked any which were found. Fourthly, we identified the winning instance using one of two possible strategies: highest-scoring or mostfrequent. These stages are now dealt with in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query Type Identification</head><p>We studied questions of each type and developed simple keyword-based heuristics to recognise them. This crude approach was suprisingly effective. In TREC 2002 425 of the 500 queries were correctly classified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Text File Parsing</head><p>Each document within the TOPDOCS file was divided into a series of segments corresponding to a short passage of text. First, text within a HEADLINE tag was extracted. Second, text within a TEXT tag was extracted and divided up into separate Ps. Finally a P was divided wherever three contiguous blanks were found. This last stage was to approximate sentence recognition. the resulting textual units were used in subsequent processing. To give the name of a German philosopher.</p><p>Table <ref type="table" coords="2,134.25,314.82,4.18,10.29">1</ref>: Some of the Question Types used in the DLT system. The second column shows a sample question for each type. The translations resulting from submission to Google are listed in the third column</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Named Entity Recognition</head><p>The type of question identified in the first step determined the type of named entity or entities to be searched for, as is standard practice in QA systems. Each segment identified as above was therefore inspected and all instances of appropriate named entities were tagged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Answer Entity Selection</head><p>Two methods were used: highest_scoring and most_frequent. In the first, we returned the named entity occurring in a textual unit which matched the keywords in the query best, chosen from any of the 50 TOPDOCS documents. In the second, we returned the named entity which most frequently occurred in the vicinity of query keywords, observed across all occurrences of the entity in the 50 TOPDOCS documents. Both strategies were unsophisticated but sometimes one or other of them can perform well on a particular query type.</p><p>In the next section we explain how the above architecture was adapted for CLEF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Architecture of the CLEF 2003 DLT System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Outline</head><p>The CLEF system had many similarities with the TREC one but also differed in a number of important respects. Flow of control was as follows. Firstly, the type of the French input query was identified along with the appropriate named entities. Secondly, the query was translated and processed in order to produce a search expression for retrieval. Thirdly, the search expression was submitted to a text retrieval engine yielding a set of documents. Fourthly, appropriate named entities were recognised in these. Fifthly, the winning named entity was identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Query Type Identification</head><p>As the input query was in French, our existing type identifier had to be re-written. This was undertaken by the third author as part of a separate project in which a French monolingual version of our TREC system was developed <ref type="bibr" coords="2,115.77,719.91,72.56,8.96">(O' Gorman, 2003)</ref>. The first step was removal of diacriticals from the query, replacing each by the same letter minus the diacritical (e.g. 'á' becomes 'a' etc.). After this the query was converted to lower-case.</p><p>Finally, simple keyword combinations were used to identify the query type. 19 query types were used and some of these can be seen in Table <ref type="table" coords="3,188.56,85.35,4.98,8.96">1</ref> together with an example of each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Query Translation and Re-Formulation</head><p>The next stage was translation of the French query into English. This was accomplished by submitting it to Google with original capitalisation and diacriticals <ref type="bibr" coords="3,282.81,140.67,110.11,8.96">(Google Translation, 2003)</ref>. The result was then tokenised and stopwords were removed from any material not in double quotes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Text Retrieval</head><p>Unlike in TREC, retrieval at CLEF was accomplished by indexing the documents ourselves. The document collection comprised 113,005 LA Times articles from 1994, 425MB in all. The search engine used was DTSearch <ref type="bibr" coords="3,114.33,219.51,25.17,8.96">(2000)</ref>. Documents were indexed using the file segmentation option working with '&lt;DOC&gt;' tags. The engine supports a number of search operators including distance between words (e.g. Word1 w/1 Word2 meaning Word1 must occur within 1 word of Word2) and boolean operators AND and OR. The next step therefore was to produce a search query on the basis of the terms identified in the previous step as follows:</p><p>• A ' w/1' connector was inserted between two capitalised words. It specifies that the second word must occur within one word from the first (in either direction). This feature was helpful because French to English translation often reverses the order of proper nouns.</p><p>• Double quotes were removed and the string of text within the double quotes was retained, but only if the first word after the double quote was untranslated. This was verified by checking the membership of the word in the original query list in French. Untranslated quotations were searched for exactly.</p><p>• An AND connector was inserted between all other terms.</p><p>• If the query included the atom ' did' , it was removed. If the rest of the query contained a verb which was among the list of verbs common to TREC questions, the verb was replaced with its past tense form. For example, 'die' in questions like 'How did X die?' was replaced with 'died'.</p><p>The query was then submitted to the engine and the first n DOCs returned were used in subsequent processing. In most cases n was 10 while in one experiment it was 20. Before the named entity recognition stage, each DOC was subdivided into separate P elements where present. These were then further subdivided wherever three contiguous blanks were found -a process which approximates to sentence recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Named Entity Recognition</head><p>The set of English named entities used and the methods for recognising them in CLEF were very similar to TREC <ref type="bibr" coords="3,71.01,518.67,64.75,8.96" target="#b4">(Sutcliffe, 2003)</ref>. Only two were added, general_name and planet. Type general_name recognises any sequence of up to five capitalised words interspersed by optional prepositions. It was inspired by <ref type="bibr" coords="3,420.21,530.43,78.08,8.96" target="#b0">Clarke et al. (2003)</ref> and is used in cases where the question type can not be determined. Planet uses a simple list of the eight planets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Answer Entity Selection</head><p>As previously, two forms of answer selection were used, highest-scoring and most-frequent. In the highestscoring method the named-entity instance is selected which occurs in the vicinity of the maximum number of keywords taken from the translated query, across all document passages. In the most-frequent strategy the namedentity occurring most frequently overall across all passages is chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Runs and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Two Experiments</head><p>We submitted two runs. These only differed in respect of the answer selection strategy. The first run used the highest-scoring strategy working with the best 10 documents returned by the retrieval system. The second run adopted the most-frequent approach, also using the best 10 documents. The query classification module was the same for both runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Type</head><p>Classif. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correct Classification Incorrect Classification Run 1 Run 2 Ru n 1 Run</head><formula xml:id="formula_0" coords="4,145.17,84.72,375.00,19.86">1 C NC R X U W R X U W R X U W R X U W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Results are summarised by query type in Table <ref type="table" coords="4,264.21,442.35,3.77,8.96">2</ref>. In respect of query classification it shows for each query type the number of queries assigned to that type which were correctly categorised along with the number incorrectly categorised. The overall rate of success was 79.5% which is broadly comparable to the 85% achieved in TREC. However, 90 queries were classified as unknown, i.e. nearly half of the set of 200. 58 of these were correctly classified. This shows the need to add further query types to the system in order to reduce unknown queries to a minimum. In addition various phrases which were frequently used in queries were not anticipated. Examples include 'à quelle époque' and 'à quel moment' , both of which were classified wrongly as unknown rather than when.</p><p>The performance of question answering in Run 1 can be summarised as follows. Out of the 159 queries classified correctly, 21 were answered correctly. Out of the remaining 41 queries classified incorrectly a further 2 were answered correctly. Overall performance was thus 23 / 200 i.e. 11.5%. Results for Run 2 were very similar. 21 of the 159 queries were answered correctly along with one of the 41 queries giving a total of 22 / 200 i.e. 11%. In both runs 117 questions were answered NIL. We discuss these results below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Platform</head><p>We used a Dell PC running Windows 2000 and having 256 Mb RAM. The whole system was written Quintus Prolog Release 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Overall performance of our system was modest at 11.5% in Run 1 but this was not worse than that in TREC 2002 where the score was 10% in Run 1. The best performance was on queries of type what_city in which the system scored 5 / 9 i.e. 56%. Our system was simple, re-used as many components as possible and was constructed in a short timeframe. Our limited aim was to identify the key issues in cross-language QA and in this we were successful. We now consider the components of the system in turn before discussing our findings regarding the project in general.</p><p>Query categorisation was not a success with 90 queries classified as unknown. Performance on the remainder was 79.5%. To improve this, further examples need to be examined and appropriate keywords and phrases added. It is likely that our simple approach can give a better result even with such simple changes and without the need for complex analysis. Turning to query formulation and document retrieval, this was the first time we had used our own retireval engine -for TREC we used the TOPDOCS and O' Gorman ( <ref type="formula" coords="5,374.40,155.91,17.89,8.96">2003</ref>) searched the web with Google. The query formulation using boolean operators and exact searches for untranslated quotations achieved much higher precision than was possible with the TOPDOCS where the PRISE engine no doubt uses a vector space type algorithm. On the other hand such an approach can lead to no documents being returned in the first instance, whereupon a query relaxation technique must be adopted in order to re-submit a slightly more general query.</p><p>Unfortunately we had no such strategy and this was the main reason why so many NIL results were returned (117 / 200 queries, i.e. 58.5% overall).</p><p>For named entity recognition essentially the same system was used as for TREC. This performs quite well and its main limitation is missing entity types which could readily be added. The main addition here was the entity general_name. In just two cases its use for processing queries of unknown type resulted in correct answers. For Query 106 'Of which political party Rudolf Scharping is member?' Run 1 returned the correct answer 'Social Democratic' . Run 2 returned the answer 'Kohl' . Run 2 returned the correct answer to Query 160 'To give the name of a German philosopher' : 'Habermas' . Run 1 returned the answer 'Western' . Finally, in at least one case where the answer was wrong it was not nonsensical -Query 8 'Which largest is the exporting country of pétrole gross?' (system' s answer: 'Kuwait' ). The last component of the system is answer selection. The two strategies used (highest-scoring and most frequent) were the same as in TREC and did not perform well. No answer patterns such as <ref type="bibr" coords="5,137.37,355.83,51.68,8.96" target="#b2">Hovy (2002)</ref> are used and so the selection is just being made on non-structural correspondences between terms in a query and candidate answers such as co-occurrence measures. We now present our findings in terms of the specific issues of cross-lingual QA.</p><p>• Translation of queries. Our approach is heavily reliant on obtaining a good translation. The results from Google were adequate but there were many errors which affected subsequent processing. A better strategy has to be worked out, perhaps involving a more detailed grammatical analysis of the query based on a specialised knowledge of query forms (rather than texts in general) followed by submission of individual query constituents for translation. In addition, several engines could be used and the results combined to overcome repeated errors of specific types committed by particular systems.</p><p>• Translation of names within queries. Some names were recognised by Google (e.g. 'États-Unis' ) and hence translated correctly (e.g. 'United States' ). However, many were left untranslated (e.g. Tchétchénie). The document collection refers to names in their American English form which can be radically different from the French term. For example the equivalent of Tchétchénie is Chechnya though it can appear in other variants. Generating the correct translation(s) for the whole range of names which might apear in a factoid question is a major task which we have not yet looked at in detail. Without accurate translations we will not answer questions mentioning such names correctly.</p><p>• Diacriticals. Our approach to these was to leave them in place for translation but to remove them entirely before question classification. This unsophisticated approach must surely lose important information. The processing of texts using diacriticals raises issues which we have not previously encountered when working with English documents. The most obvious of these is the need to match a word containing the appropriate accents with its equivalent without, while at the same time giving priority to exact matches. The present cross-language system was essentially a monolingual English one with a French front end but in the context of monolingual French-French QA attention would have to be paid to query formulation unless the retrieval engine used explicitly supported a morphologically sophisticated model of multilingual documents.</p><p>• Search queries returning no results. Intentionally narrow search expressions submitted to DTSearch could result in no matched document portions. This happened more often than we anticipated, the main reason being incorrectly translated (or not translated) parts of the input query. The answer is better translation together with query relaxation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,72.81,106.56,37.49,9.30;4,151.05,106.40,4.50,8.10;4,172.65,106.40,4.50,8.10;4,194.25,106.40,4.50,8.10;4,215.85,106.40,4.50,8.10;4,237.45,106.40,4.50,8.10;4,259.05,106.40,4.50,8.10;4,280.65,106.40,4.50,8.10;4,302.25,106.40,4.50,8.10;4,323.85,106.40,4.50,8.10;4,345.45,106.40,4.50,8.10;4,367.05,106.40,4.50,8.10;4,388.65,106.40,4.50,8.10;4,410.25,106.40,4.50,8.10;4,431.85,106.40,4.50,8.10;4,453.45,106.40,4.50,8.10;4,475.05,106.40,4.50,8.10;4,496.65,106.40,4.50,8.10;4,518.25,106.40,4.50,8.10;4,72.81,117.12,41.49,9.30;4,151.05,116.96,4.50,8.10;4,172.65,116.96,4.50,8.10;4,194.25,116.96,4.50,8.10;4,215.85,116.96,4.50,8.10;4,237.45,116.96,4.50,8.10;4,259.05,116.96,4.50,8.10;4,280.65,116.96,4.50,8.10;4,302.25,116.96,4.50,8.10;4,323.85,116.96,4.50,8.10;4,345.45,116.96,4.50,8.10;4,367.05,116.96,4.50,8.10;4,388.65,116.96,4.50,8.10;4,410.25,116.96,4.50,8.10;4,431.85,116.96,4.50,8.10;4,453.45,116.96,4.50,8.10;4,475.05,116.96,4.50,8.10;4,496.65,116.96,4.50,8.10;4,518.25,116.96,4.50,8.10;4,72.81,127.68,53.50,9.30;4,151.05,127.52,4.50,8.10;4,172.65,127.52,4.50,8.10;4,194.25,127.52,4.50,8.10;4,215.85,127.52,4.50,8.10;4,237.45,127.52,4.50,8.10;4,259.05,127.52,4.50,8.10;4,280.65,127.52,4.50,8.10;4,302.25,127.52,4.50,8.10;4,323.85,127.52,4.50,8.10;4,345.45,127.52,4.50,8.10;4,367.05,127.52,4.50,8.10;4,388.65,127.52,4.50,8.10;4,410.25,127.52,4.50,8.10;4,431.85,127.52,4.50,8.10;4,453.45,127.52,4.50,8.10;4,475.05,127.52,4.50,8.10;4,496.65,127.52,4.50,8.10;4,518.25,127.52,4.50,8.10;4,72.81,138.24,59.50,9.30;4,150.96,138.08,4.50,8.10;4,172.65,138.08,4.50,8.10;4,194.25,138.08,4.50,8.10;4,215.85,138.08,4.50,8.10;4,237.45,138.08,4.50,8.10;4,259.05,138.08,4.50,8.10;4,280.65,138.08,4.50,8.10;4,302.25,138.08,4.50,8.10;4,323.85,138.08,4.50,8.10;4,345.45,138.08,4.50,8.10;4,367.05,138.08,4.50,8.10;4,388.65,138.08,4.50,8.10;4,410.25,138.08,4.50,8.10;4,431.85,138.08,4.50,8.10;4,453.45,138.08,4.50,8.10;4,475.05,138.08,4.50,8.10;4,496.65,138.08,4.50,8.10;4,518.25,138.08,4.50,8.10;4,72.81,148.80,23.49,9.30;4,146.49,148.64,9.00,8.10;4,172.65,148.64,4.50,8.10;4,194.25,148.64,4.50,8.10;4,215.85,148.64,4.50,8.10;4,237.45,148.64,26.04,8.10;4,280.65,148.64,4.50,8.10;4,302.25,148.64,4.50,8.10;4,323.85,148.64,26.04,8.10;4,367.05,148.64,4.50,8.10;4,388.65,148.64,4.50,8.10;4,410.25,148.64,4.50,8.10;4,431.85,148.64,4.50,8.10;4,453.45,148.64,4.50,8.10;4,475.05,148.64,4.50,8.10;4,496.65,148.64,4.50,8.10;4,518.25,148.64,4.50,8.10;4,72.81,159.36,46.50,9.30;4,146.49,159.20,9.00,8.10;4,172.65,159.20,4.50,8.10;4,194.25,159.20,4.50,8.10;4,215.85,159.20,4.50,8.10;4,237.45,159.20,26.04,8.10;4,280.65,159.20,4.50,8.10;4,302.25,159.20,4.50,8.10;4,323.85,159.20,26.04,8.10;4,367.05,159.20,4.50,8.10;4,388.65,159.20,4.50,8.10;4,410.25,159.20,4.50,8.10;4,431.85,159.20,4.50,8.10;4,453.45,159.20,4.50,8.10;4,475.05,159.20,4.50,8.10;4,496.65,159.20,4.50,8.10;4,518.25,159.20,4.50,8.10;4,72.81,169.92,31.50,9.30;4,151.05,169.76,4.50,8.10;4,172.65,169.76,4.50,8.10;4,194.25,169.76,4.50,8.10;4,215.85,169.76,4.50,8.10;4,237.45,169.76,4.50,8.10;4,259.05,169.76,4.50,8.10;4,280.65,169.76,4.50,8.10;4,302.25,169.76,4.50,8.10;4,323.85,169.76,4.50,8.10;4,345.45,169.76,4.50,8.10;4,367.05,169.76,4.50,8.10;4,388.65,169.76,4.50,8.10;4,410.25,169.76,4.50,8.10;4,431.85,169.76,4.50,8.10;4,453.45,169.76,4.50,8.10;4,475.05,169.76,4.50,8.10;4,496.65,169.76,4.50,8.10;4,518.25,169.76,4.50,8.10;4,72.81,180.48,21.50,9.30;4,151.05,180.32,4.50,8.10;4,172.65,180.32,4.50,8.10;4,194.25,180.32,4.50,8.10;4,215.85,180.32,4.50,8.10;4,237.45,180.32,4.50,8.10;4,259.05,180.32,4.50,8.10;4,280.65,180.32,4.50,8.10;4,302.25,180.32,4.50,8.10;4,323.85,180.32,4.50,8.10;4,345.45,180.32,4.50,8.10;4,367.05,180.32,4.50,8.10;4,388.65,180.32,4.50,8.10;4,410.25,180.32,4.50,8.10;4,431.85,180.32,4.50,8.10;4,453.45,180.32,4.50,8.10;4,475.05,180.32,4.50,8.10;4,496.65,180.32,4.50,8.10;4,518.25,180.32,4.50,8.10;4,72.81,191.04,19.49,9.30;4,151.05,190.88,4.50,8.10;4,172.65,190.88,4.50,8.10;4,194.25,190.88,4.50,8.10;4,215.85,190.88,4.50,8.10;4,237.45,190.88,4.50,8.10;4,259.05,190.88,4.50,8.10;4,280.65,190.88,4.50,8.10;4,302.25,190.88,4.50,8.10;4,323.85,190.88,4.50,8.10;4,345.45,190.88,4.50,8.10;4,367.05,190.88,4.50,8.10;4,388.65,190.88,4.50,8.10;4,410.25,190.88,4.50,8.10;4,431.85,190.88,4.50,8.10;4,453.45,190.88,4.50,8.10;4,475.05,190.88,4.50,8.10;4,496.65,190.88,4.50,8.10;4,518.25,190.88,4.50,8.10;4,72.81,201.60,41.52,9.30;4,151.05,201.44,4.50,8.10;4,172.65,201.44,4.50,8.10;4,194.25,201.44,4.50,8.10;4,215.85,201.44,4.50,8.10;4,237.45,201.44,4.50,8.10;4,259.05,201.44,4.50,8.10;4,280.65,201.44,4.50,8.10;4,302.25,201.44,4.50,8.10;4,323.85,201.44,4.50,8.10;4,345.45,201.44,4.50,8.10;4,367.05,201.44,4.50,8.10;4,388.65,201.44,4.50,8.10;4,410.25,201.44,4.50,8.10;4,431.85,201.44,4.50,8.10;4,453.45,201.44,4.50,8.10;4,475.05,201.44,4.50,8.10;4,496.65,201.44,4.50,8.10;4,518.25,201.44,4.50,8.10;4,72.81,212.16,16.00,9.30;4,146.49,212.00,9.00,8.10;4,172.65,212.00,4.50,8.10;4,194.25,212.00,4.50,8.10;4,215.85,212.00,4.50,8.10;4,237.45,212.00,26.04,8.10;4,280.65,212.00,4.50,8.10;4,302.25,212.00,4.50,8.10;4,323.85,212.00,26.04,8.10;4,367.05,212.00,4.50,8.10;4,388.65,212.00,4.50,8.10;4,410.25,212.00,4.50,8.10;4,431.85,212.00,4.50,8.10;4,453.45,212.00,4.50,8.10;4,475.05,212.00,4.50,8.10;4,496.65,212.00,4.50,8.10;4,518.25,212.00,4.50,8.10;4,72.81,222.72,20.50,9.30;4,146.49,222.56,9.00,8.10;4,172.65,222.56,4.50,8.10;4,194.25,222.56,4.50,8.10;4,215.85,222.56,4.50,8.10;4,237.45,222.56,26.04,8.10;4,280.65,222.56,4.50,8.10;4,302.25,222.56,4.50,8.10;4,323.85,222.56,26.04,8.10;4,367.05,222.56,4.50,8.10;4,388.65,222.56,4.50,8.10;4,410.25,222.56,4.50,8.10;4,431.85,222.56,4.50,8.10;4,453.45,222.56,4.50,8.10;4,475.05,222.56,4.50,8.10;4,496.65,222.56,4.50,8.10;4,518.25,222.56,4.50,8.10;4,72.81,233.28,24.50,9.30;4,151.05,233.12,4.50,8.10;4,172.65,233.12,4.50,8.10;4,194.25,233.12,4.50,8.10;4,215.85,233.12,4.50,8.10;4,237.45,233.12,4.50,8.10;4,259.05,233.12,4.50,8.10;4,280.65,233.12,4.50,8.10;4,302.25,233.12,4.50,8.10;4,323.85,233.12,4.50,8.10;4,345.45,233.12,4.50,8.10;4,367.05,233.12,4.50,8.10;4,388.65,233.12,4.50,8.10;4,410.25,233.12,4.50,8.10;4,431.85,233.12,4.50,8.10;4,453.45,233.12,4.50,8.10;4,475.05,233.12,4.50,8.10;4,496.65,233.12,4.50,8.10;4,518.25,233.12,4.50,8.10;4,72.81,243.84,42.49,9.30;4,151.05,243.68,4.50,8.10;4,172.65,243.68,4.50,8.10;4,194.25,243.68,4.50,8.10;4,215.85,243.68,4.50,8.10;4,237.45,243.68,4.50,8.10;4,259.05,243.68,4.50,8.10;4,280.65,243.68,4.50,8.10;4,302.25,243.68,4.50,8.10;4,323.85,243.68,4.50,8.10;4,345.45,243.68,4.50,8.10;4,367.05,243.68,4.50,8.10;4,388.65,243.68,4.50,8.10;4,410.25,243.68,4.50,8.10;4,431.85,243.68,4.50,8.10;4,453.45,243.68,4.50,8.10;4,475.05,243.68,4.50,8.10;4,496.65,243.68,4.50,8.10;4,518.25,243.68,4.50,8.10;4,72.81,254.40,57.48,9.30;4,72.81,264.96,11.50,9.30;4,151.05,254.24,4.50,8.10;4,172.65,254.24,4.50,8.10;4,194.25,254.24,4.50,8.10;4,215.85,254.24,4.50,8.10;4,237.45,254.24,4.50,8.10;4,259.05,254.24,4.50,8.10;4,280.65,254.24,4.50,8.10;4,302.25,254.24,4.50,8.10;4,323.85,254.24,4.50,8.10;4,345.45,254.24,4.50,8.10;4,367.05,254.24,4.50,8.10;4,388.65,254.24,4.50,8.10;4,410.25,254.24,4.50,8.10;4,431.85,254.24,4.50,8.10;4,453.45,254.24,4.50,8.10;4,475.05,254.24,4.50,8.10;4,496.65,254.24,4.50,8.10;4,518.25,254.24,4.50,8.10;4,72.81,275.52,60.51,9.30;4,151.05,275.36,4.50,8.10;4,172.65,275.36,4.50,8.10;4,194.25,275.36,4.50,8.10;4,215.85,275.36,4.50,8.10;4,237.45,275.36,4.50,8.10;4,259.05,275.36,4.50,8.10;4,280.65,275.36,4.50,8.10;4,302.25,275.36,4.50,8.10;4,323.85,275.36,4.50,8.10;4,345.45,275.36,4.50,8.10;4,367.05,275.36,4.50,8.10;4,388.65,275.36,4.50,8.10;4,410.25,275.36,4.50,8.10;4,431.85,275.36,4.50,8.10;4,453.45,275.36,4.50,8.10;4,475.05,275.36,4.50,8.10;4,496.65,275.36,4.50,8.10;4,518.25,275.36,4.50,8.10;4,72.81,286.08,60.51,9.30;4,72.81,296.64,26.50,9.30;4,151.05,285.92,4.50,8.10;4,172.65,285.92,4.50,8.10;4,194.25,285.92,4.50,8.10;4,215.85,285.92,4.50,8.10;4,237.45,285.92,4.50,8.10;4,259.05,285.92,4.50,8.10;4,280.65,285.92,4.50,8.10;4,302.25,285.92,4.50,8.10;4,323.85,285.92,4.50,8.10;4,345.45,285.92,4.50,8.10;4,367.05,285.92,4.50,8.10;4,388.65,285.92,4.50,8.10;4,410.25,285.92,4.50,8.10;4,431.85,285.92,4.50,8.10;4,453.45,285.92,4.50,8.10;4,475.05,285.92,4.50,8.10;4,496.65,285.92,4.50,8.10;4,518.25,285.92,4.50,8.10;4,72.81,307.20,47.50,9.30;4,151.05,307.04,4.50,8.10;4,172.65,307.04,4.50,8.10;4,194.25,307.04,4.50,8.10;4,215.85,307.04,4.50,8.10;4,237.45,307.04,4.50,8.10;4,259.05,307.04,4.50,8.10;4,280.65,307.04,4.50,8.10;4,302.25,307.04,4.50,8.10;4,323.85,307.04,4.50,8.10;4,345.45,307.04,4.50,8.10;4,367.05,307.04,4.50,8.10;4,388.65,307.04,4.50,8.10;4,410.25,307.04,4.50,8.10;4,431.85,307.04,4.50,8.10;4,453.45,307.04,4.50,8.10;4,475.05,307.04,4.50,8.10;4,496.65,307.04,4.50,8.10;4,518.25,307.04,4.50,8.10;4,72.81,328.32,36.02,9.30;4,146.49,328.16,30.60,8.10;4,194.25,328.16,4.50,8.10;4,215.85,328.16,4.50,8.10;4,237.45,328.16,26.04,8.10;4,280.65,328.16,4.50,8.10;4,302.25,328.16,4.50,8.10;4,323.85,328.16,26.04,8.10;4,367.05,328.16,4.50,8.10;4,388.65,328.16,4.50,8.10;4,410.25,328.16,26.04,8.10;4,453.45,328.16,4.50,8.10;4,475.05,328.16,4.50,8.10;4,496.65,328.16,26.04,8.10;4,72.81,338.88,24.00,9.30;4,141.93,338.88,56.76,9.30;4,215.85,338.88,4.50,9.30;4,237.45,338.88,47.64,9.30;4,302.25,338.88,4.50,9.30;4,323.85,338.88,25.98,9.30;4,367.05,338.88,4.50,9.30;4,388.65,338.88,4.50,9.30;4,410.25,338.88,26.04,9.30;4,453.45,338.88,4.50,9.30;4,475.05,338.88,4.50,9.30;4,496.65,338.88,26.04,9.30;4,107.01,362.10,381.58,10.29;4,107.01,373.95,381.58,8.96;4,107.01,385.71,381.54,8.96;4,107.01,397.47,324.76,8.96"><head></head><label></label><figDesc>by Query Type. The columns C and NC show the numbers of queries of a particular type which were classified correctly and not correctly. Those classified correctly are then broken down into Right, ineXact, Unsupported and Wrong for each of the two runs Run 1 and Run 2. Finally, those classified incorrectly are broken down in the same way.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>• General names. Our experiment in recognising general names was not very successful. Part of the problem is accurate verification of candidates without knowing the question type. A better strategy has to be worked out for this as there will always be unknown questions.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,71.01,143.79,453.57,8.96;6,71.01,155.55,329.01,8.96" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,71.01,155.55,299.68,8.96">Statistical Selection of Exact Answers (MultiText Experiments for TREC</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kemkes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Laszlo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">L</forename><surname>Terra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">L</forename><surname>Tilker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2003. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,403.41,155.55,121.17,8.96;6,71.01,167.31,453.57,9.49;6,71.01,179.07,253.29,9.49" xml:id="b1">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="6,136.65,167.46,279.22,9.34">Proceedings of the Eleventh Text REtrieval Conference (TREC 2002)</title>
		<title level="s" coord="6,175.89,179.07,107.34,8.96">NIST Special Publication</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>the Eleventh Text REtrieval Conference (TREC 2002)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">November 19-22, 2002</date>
			<biblScope unit="page" from="500" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,71.01,261.39,453.68,9.49;6,71.01,273.15,269.73,8.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,168.45,261.54,197.16,9.34">A Typology of over 140 Question-Answer Types</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="www.isi.edu/natural-language/projects/webclopedia/Taxonomy/taxonomy_toplevel.html" />
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,71.01,296.67,453.57,8.96;6,71.01,308.43,127.52,8.96" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,171.21,296.67,195.35,8.96">Open Domain Question Answering in French</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>O'gorman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Limerick, Ireland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Undergraduate Final Year Project</note>
</biblStruct>

<biblStruct coords="6,71.01,331.95,453.57,8.96;6,71.01,343.71,453.57,9.49;6,71.01,355.47,302.97,9.49" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,176.61,331.95,233.88,8.96">Question Answering using the DLT System at TREC 2002</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,156.21,343.86,301.90,9.34">Proceedings of the Eleventh Text REtrieval Conference (TREC 2002)</title>
		<title level="s" coord="6,224.37,355.47,108.18,8.96">NIST Special Publication</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>the Eleventh Text REtrieval Conference (TREC 2002)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2003. November 19-22, 2002</date>
			<biblScope unit="page" from="500" to="251" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
