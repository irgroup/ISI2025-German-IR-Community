<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,101.76,71.36,391.58,15.41">The Multiple Language Question Answering Track at CLEF 2003</title>
				<funder ref="#_RKpv4xZ">
					<orgName type="full">Autonomous Province of Trento</orgName>
				</funder>
				<funder ref="#_eSde73J #_8bXTsXf">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_qAwRsS5">
					<orgName type="full">Spanish Government</orgName>
				</funder>
				<funder ref="#_cG39Zu3 #_cZYEfK4 #_bjVucha #_GHfWCMW">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,175.20,98.75,70.78,10.89"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
							<email>magnini@itc.it</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ITC-irst</orgName>
								<orgName type="department" key="dep2">Centro per la Ricerca Scientifica e Tecnologica Via Sommarive</orgName>
								<address>
									<postCode>38050</postCode>
									<settlement>Povo (</settlement>
									<region>TN)</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informaticos</orgName>
								<orgName type="institution" key="instit1">UNED</orgName>
								<orgName type="institution" key="instit2">Spanish Distance Learning University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Ciudad Universitaria</orgName>
								<address>
									<addrLine>c./Juan del Rosal 16</addrLine>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Language and Inference Technology Group</orgName>
								<orgName type="institution" key="instit1">ILLC</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<address>
									<addrLine>Nieuwe ; Achtergracht 166</addrLine>
									<postCode>1018 WV</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.49,98.75,74.37,10.89"><forename type="first">Simone</forename><surname>Romagnoli</surname></persName>
							<email>romagnoli@itc.it</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ITC-irst</orgName>
								<orgName type="department" key="dep2">Centro per la Ricerca Scientifica e Tecnologica Via Sommarive</orgName>
								<address>
									<postCode>38050</postCode>
									<settlement>Povo (</settlement>
									<region>TN)</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informaticos</orgName>
								<orgName type="institution" key="instit1">UNED</orgName>
								<orgName type="institution" key="instit2">Spanish Distance Learning University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Ciudad Universitaria</orgName>
								<address>
									<addrLine>c./Juan del Rosal 16</addrLine>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Language and Inference Technology Group</orgName>
								<orgName type="institution" key="instit1">ILLC</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<address>
									<addrLine>Nieuwe ; Achtergracht 166</addrLine>
									<postCode>1018 WV</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.61,98.75,72.08,10.89"><forename type="first">Alessandro</forename><surname>Vallin</surname></persName>
							<email>vallin@itc.it</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ITC-irst</orgName>
								<orgName type="department" key="dep2">Centro per la Ricerca Scientifica e Tecnologica Via Sommarive</orgName>
								<address>
									<postCode>38050</postCode>
									<settlement>Povo (</settlement>
									<region>TN)</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informaticos</orgName>
								<orgName type="institution" key="instit1">UNED</orgName>
								<orgName type="institution" key="instit2">Spanish Distance Learning University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Ciudad Universitaria</orgName>
								<address>
									<addrLine>c./Juan del Rosal 16</addrLine>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Language and Inference Technology Group</orgName>
								<orgName type="institution" key="instit1">ILLC</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<address>
									<addrLine>Nieuwe ; Achtergracht 166</addrLine>
									<postCode>1018 WV</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,154.80,110.27,52.93,10.89"><forename type="first">Jesús</forename><surname>Herrera</surname></persName>
							<email>jesus.herrera@lsi.uned.es</email>
						</author>
						<author>
							<persName coords="1,222.90,110.27,60.06,10.89"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
						</author>
						<author>
							<persName coords="1,298.65,110.27,58.43,10.89"><forename type="first">Víctor</forename><surname>Peinado</surname></persName>
						</author>
						<author>
							<persName coords="1,372.75,110.27,58.31,10.89"><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
							<email>felisa@lsi.uned.es</email>
						</author>
						<author>
							<persName coords="1,256.08,121.79,69.57,10.89"><surname>Maarten De Rijke</surname></persName>
						</author>
						<title level="a" type="main" coord="1,101.76,71.36,391.58,15.41">The Multiple Language Question Answering Track at CLEF 2003</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">22430FDC67DF1EED851E4CDB203C4539</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports on the pilot question answering track that was carried out within the CLEF initiative this year. The track was divided into monolingual and bilingual tasks: monolingual systems were evaluated within the frame of three non-English European languages, Dutch, Italian and Spanish, while in the crosslanguage tasks an English document collection constituted the target corpus for Italian, Spanish, Dutch, French and German queries. Participants were given 200 questions for each task, and were allowed to submit up to two runs per task with up to three responses (either exact answers or 50 bytes long strings) per question. We give here an overview of the track: we report on each task and discuss the creation of the multilingual test sets and the participants' results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The question answering (QA) track at TREC-8 represented the first attempt to emphasise the importance and foster research on systems that could extract relevant and precise information rather than documents. Question answering systems are designed to find answers to open domain questions in a large collection of documents. QA development has acquired an important role among the scientific community because it entails research in both natural language processing and information retrieval (IR), putting the two disciplines in contact. Differently from the IR scenario, a QA system processes questions formulated into natural language (instead of keyword-based queries) and retrieves answers (instead of documents). The past TREC conferences laid the foundations for a formalised and widely accepted evaluation methodology of QA systems, but the three tracks organised so far focused just on monolingual systems for the English language, which constitutes a drawback we tried to address. We were mainly interested in testing multilingual systems, and in particular to push the QA community into designing them. As the number of the participants and the results achieved by their systems show, we can argue that in the field of multilingual QA there is much work to do. Within the frame of planning and co-ordinating the research on question answering, outlined in Maybury's roadmap, multilingual QA has a pivotal role and should deserve much attention in the next years. Multilinguality represents a new area in QA research, and a challenging issue toward the development of more complex systems <ref type="bibr" coords="1,70.80,584.03,58.79,10.89">[Maybury 2002</ref>]. Multilinguality enables the user to pose a query in a language that is different from the language of the reference corpus. The cross-language perspective could be quite useful when the required information is not available in the user's language (as it often happens surfing the web) and in particular it fits for the cultural situation in Europe, where different languages co-exist and are in contact, although English has become a widespread and standardised means of communication. In a multilingual environment, QA systems and other natural language processing resources could even contribute to conserve endangered languages that are progressively losing importance and prestige, in the effort to ensure their survival, as in the case of the 'Te Kaitito' bilingual question answering system for English and Ma_ri <ref type="bibr" coords="1,233.24,676.19,70.95,10.89" target="#b6">[Knott et al. 2001]</ref>. Our activity, and in particular the production of two multilingual test sets that constitute reusable resources, can be regarded as a valuable contribution to the development of such cross-language systems <ref type="bibr" coords="1,441.19,698.99,74.64,10.89" target="#b3">[Burger et al. 2001</ref>]. The evaluation of cross-language resources is the key issue of the CLEF initiative, so our question answering track could not be limited to the English language. On the contrary, we attempted to raise interest on other European languages, like Italian, Spanish, Dutch, German and French. The basic novelty in comparison with the past TREC QA campaigns was the introduction of bilingual tasks, in which non-English queries are processed to find responses in an English document collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">QA at CLEF</head><p>Our pilot question answering track was structured in both monolingual and bilingual tasks. We organised three monolingual tasks for Dutch, Italian and Spanish, in which the questions, the corpus and the responses were in the same language. In contrast, in the cross-language tasks we had Italian, Spanish, Dutch, French or German queries that searched for answers in an English document collection. In output, the systems had to retrieve English answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Monolingual Tasks</head><p>Unlike previous TREC QA tracks, we focused on the evaluation and on the production of reusable resources for non-English QA systems. The monolingual tasks were designed for three different languages: Dutch, Italian and Spanish. For each language we generated 200 queries, 180 of which were completely shared between all the three tasks. Participants were given the questions and the corresponding monolingual corpus: the task consisted in returning automatically, i.e. with no manual intervention, a ranked list of <ref type="bibr" coords="2,439.55,271.31,62.51,10.89">[docid, answer]</ref> pairs per question such that the retrieved document supported the answer. Participants were given 200 questions for each task, and were allowed to submit up to two runs per task with up to three responses per query. They could return either exact answers or 50 bytes long strings that contained the answer, although they were not allowed to use both modalities within the same run. Following the TREC model, we formulated 20 questions that had no known answer in the corpora: systems indicated their belief that there was no answer in the document collection by returning "NIL" instead of the <ref type="bibr" coords="2,204.76,340.43,61.25,10.89">[docid, answer]</ref> pair. The monolingual Italian question answering task was planned and carried out under the co-ordination of the Italian Centro per la Ricerca Scientifica e Tecnologica (ITC-irst), that was in charge for the supervision of the whole QA track. We could use the document collections released at CLEF 2002, made up of articles drawn from a newspaper (La Stampa) and a press agency (SDA) of the year 1994. The entire Italian target corpus was 200 Mb wide (about 27 millions words) and it was made available to registered participants at the end of last January, so that they could test their systems using the document collection well in advance. The UNED NLP group (Spanish Distance Learning University), as Spanish member of the CLEF consortium, was in charge for the monolingual Spanish task. The collection we were allowed to employ was the one released at CLEF 2002, i.e. more than 200,000 news from EFE Press Agency of the year 1994. The Language and Inference Technology Group at the University of Amsterdam took care of the monolingual Dutch task. The collection used was the CLEF 2002 Dutch collection, which consists of two full years of the Algemeen Dagblad and NRC Handelsblad newspapers, adding up to about 200,000 documents of 540 Mb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-Language Tasks</head><p>Our interest in developing QA systems for languages other than English was not the only achievement we pointed at: the great novelty introduced in the CLEF QA track was multilinguality, whose potentialities are currently out of the scope of the TREC competition. Cross-language QA systems are crucially important when the language of the query and the language of the document collection are different, and in multicultural situations such a possibility is far from being remote. Searching information in the World Wide Web for instance is often difficult because the document retrieved is in a language we cannot understand. In this sense the crosslanguage tasks we organised represent a good chance to push the QA community to design and evaluate multilingual systems. The cross-language tasks consisted in searching an English corpus to find English responses to queries posed in a different language. The target document collection we used was a corpus made up of Los Angeles Times articles of the year 1994, that was the same employed in last year's CLEF campaign. We translated into five languages the original two hundred English questions we generated, so we were able to organise five different bilingual tasks: Dutch, French, German, Italian and Spanish. As in the monolingual tasks, participants had to process 200 questions (15 had no answer in the corpus) posed in one of the five languages and could choose to submit either exact answers or 50 bytes strings, without mixing them in the same run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Participants</head><p>Eight groups took part in this pilot question answering track, and a total of seventeen runs were submitted, three using 50 bytes long strings as answers and the other fourteen, in compliance with last year's TREC conditions, returning exact answers. The fact that most participants chose to retrieve exact answers shows that many have made the transition from more or less long strings to precise responses. Figure <ref type="figure" coords="3,99.85,142.43,4.92,10.89">1</ref> below shows the name of the participants, the task in which they participated and the filename of their runs. It is interesting to notice that all the participants except the DFKI group had already participated in some previous TREC QA campaigns. Bilingual French udemst031bf udemex032bf</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GROUP</head><p>Figure <ref type="figure" coords="3,225.24,444.86,3.08,8.77">1</ref>: participants in the CLEF question answering track Note that the fifth and sixth letters in the run names show whether the responses are exact answers (ex) or 50 bytes long strings (st).</p><p>Three teams took part in the monolingual tasks, submitting a total of six runs. We had only one participant in each language, which is quite disappointing because no comparison can be made between similar runs. Anyway, since the question set for all the monolingual tasks was the same (except the NIL questions), the monolingual runs can be compared to some extent. Four teams initially registered for the monolingual Italian task, but unfortunately only one, the ITC-irst group, actually participated. Similarly, only the University of Alicante took part in the monolingual Spanish task submitting two runs of exact answers, although three other groups expressed their intention of participation. As for the monolingual Dutch task, the University of Amsterdam with its two runs of exact answers was the only participant. Six groups participated in the cross-language tasks, submitting eleven runs. The challenging novelty of the crosslanguage question answering attracted more participants than the monolingual tasks: the bilingual French task was chosen by three groups, while no one tested their system in the bilingual Dutch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Test Sets</head><p>From a potential user's point of view, a question answering system should be able to process natural language queries and return precise and unambiguous responses, drawn from a large reference corpus. Thus, in every evaluation campaign like the one we conducted, a set of well formulated questions is required. Since they should reflect real requests posed by humans, such questions must sound spontaneous and realistic. On the other hand, they must be clear, simple and factoid, i.e. related to facts, events, physical situations, so that the answers can be retrieved without inference. All the necessary information to answer the questions must be straightforwardly available and consequently included in the document collection searched by the systems. For this reason no external knowledge of the world should be required and the queries should deal with practical, concrete matters, rather than with abstract notions, that depend on personal opinion or reasoning.</p><p>The creation of the question sets for both the tasks -thoroughly described in the article "Creating the DISEQuA Corpus: a Test Set for Multilingual Question Answering" in these proceedings -entailed much work in terms of queries selection and answers verification. In order to establish some common criteria of comparison between the several languages involved, we decided to provide the participants, independently from the language, with the same queries. Thus, we created two collections of two hundred questions each, translated into different languages: one for the monolingual tasks and the other one for the cross-language tasks. As a result, we put together two reusable linguistic resources that can be useful for the QA community but also for other NLP fields, such as Machine Translation. The test set for the monolingual tasks in particular represents a multilingual collection of queries with their answers in different corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Gold Standard for the Monolingual Tasks</head><p>The benchmark collection of queries and responses for the Dutch, Italian and Spanish monolingual tasks was the result of a joint effort between the co-ordinators, who decided to share the test sets in the three languages. Our activity can be roughly divided into four steps:</p><p>1. Production of a pool of 200 candidate questions with their answers in each language. These queries were formulated on the basis of the topics released by CLEF for the retrieval tasks of the year 2000, 2001 and/or 2002. The CLEF topics, i.e. a set of concepts chosen with the aim of covering the main events occurred in the years 1994 and/or 1995, allowed us to pose questions independently from the document collection. In this way we avoided any influence in the contents and in the formulation of the queries. Questions were posed according to common guidelines: they had to be generally short and factbased, unrelated to subjective opinions. They could not ask for definitions (i.e. "Who is Bill Clinton") and they had to have just one unique and unambiguous item as response, which means that we avoided questions asking for multiple items like those used in the TREC list task. Three groups of native speakers, one for each language, were involved in this work and searched the correct answers. A question has an answer in the reference corpus if a document contains the correct response without any inference implying knowledge outside the document itself. 2. Selection of 150 questions from each monolingual set. Since our aim was to build a test set of shared queries that would find answers in all the monolingual corpora, each group chose 150 questions from its candidate pool and translated them into English, thus a larger collection of 450 queries was put together. English constituted a sort of inter-language we used to shift from one language to another, but in this phase we were aware that there was the risk of changing unwarily the content of the questions during the translation. Each group chose its 150 questions taking into consideration that they would be processed by the other two, so the most general queries, that were likely to find a response in the other two corpora, were selected. Those that were too strictly related to the specific issues of a country were discarded. 3. Processing of the shared questions. Once we had collected a pool of 450 questions that had response in one of the corpora, we detected the duplicates and eliminated them. Quite surprisingly, we found thirteen couples of queries that had an identical meaning, although the formulation could be slightly different. Then each group translated back from English the 300 questions provided by the other coordinators and verified whether they had an answer in its corpus. 4. Selection of the final 200 questions. At this point, about 450 different questions had been formulated and translated into Dutch, English, Italian and Spanish. All of them had at least one answer in at least one language (other than English), and more than 200, obtained by merging the data of the second cross-verification, proved to have at least one answer in all the three monolingual document collections. Our goal was to provide the QA participants with 200 questions, including a small rate of NIL queries, i.e. questions that do not have any known answer in the corpus. We agreed that the 10% of the test set was a reasonable amount of NIL questions, that were first introduced in QA evaluation at TREC-10 ( <ref type="formula" coords="4,152.60,625.55,16.58,10.89">2001</ref>). So we selected 180 questions from those that had a response in all the three corpora, and each group completed its monolingual test set adding 20 NIL questions, that were necessarily different for each task. Taking into consideration seven general classes of questions, we tried to balance the final test set of 180 questions, that is composed of: 45 entries that ask for the name or role of a PERSON, 40 that pertain a LOCATION, 31 a MEASURE, 23 an ORGANISATION, 19 a DATE, 9 a concrete OBJECT, while 13, due to their vagueness, can be labelled with OTHER. The result of the question development phase is a useful and reusable multilingual question set, whose entries are structured in a XML format, as shown in the example of Figure <ref type="figure" coords="4,331.13,705.95,3.71,10.89">2</ref>. More details are given in the paper "Creating the DISEQuA Corpus". &lt;qa cnt="1" type="DATE"&gt; &lt;language val="ITA" original="TRUE"&gt; &lt;question assessor="ALE"&gt; Quando e' avvenuta la riunificazione delle due Germanie? &lt;/question&gt; &lt;answer n="1" idx="SDA19941115.00073"&gt; nel 1989 &lt;/answer&gt; &lt;/language&gt; &lt;language val="SPA" original="FALSE"&gt; &lt;question assessor="Anselmo"&gt; ¿Cuándo se produjo la reunificación de Alemania? &lt;/question&gt; &lt;answer n="1" idx="EFE19941108-04388"&gt; 1989 &lt;/answer&gt; &lt;answer n="2" idx="EFE19941108-04508"&gt; 1989 &lt;/answer&gt; &lt;/language&gt; &lt;language val="DUT" original="FALSE"&gt; &lt;question assessor="LIT"&gt; Wanneer vond de Duitse hereniging plaats? &lt;/question&gt; &lt;answer n="1" idx="NH19940128-0161"&gt; in 1989 &lt;/answer&gt; &lt;/language&gt; &lt;language val="ENG" original="FALSE"&gt; &lt;question assessor=""&gt; When did the reunification of East and West Germany take place? &lt;/question&gt; &lt;answer n="1" idx="-1"&gt; SEARCH[in 1989] &lt;/answer&gt; &lt;/language&gt; &lt;/qa&gt; 1 Figure <ref type="figure" coords="5,208.92,488.54,3.08,8.77">2</ref>: gold standard format of a question for the monolingual tasks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gold standard for the Cross-Language Tasks</head><p>While in the monolingual tasks we had three different document collections and three sets of questions, all the bilingual tasks had one English target corpus. For this reason we generated 200 English queries and verified manually that each of them (except 15 NIL) had at least an answer. Then the questions were translated into each language. As in the monolingual test sets, translators were asked to be as faithful as possible to the original English version, in fact we were aware that every translation could be different from the source. Because of organisational problems encountered shortly before the test set creation deadline, three Italian native speakers at ITC-irst had to take on the job, even though there was a high risk of inconsistencies that may have affected the quality of the question set as a resource. Due to time constraints we could not compile a large pool of general questions independently from the corpus and then verify them. Instead, we chose an alternative approach: we randomly selected a document from the collection (while trying to select news with a worldwide importance, avoiding sections that deal with local politics or issues too strictly related to Los Angeles counties) and picked up a text snippet that was relevant, long and interesting enough to get a question out of it. For instance, from the following passage 1 In the files downloaded by participants, the questions for each monolingual QA task were written in another format, and numbered differently. The questions above appeared in the following way in the three monolingual test sets: M DUT 0097 Wanneer vond de Duitse hereniging plaats? M ITA 0077 Quando è avvenuta la riunificazione delle due Germanie? M SPA 0013 ¿Cuándo se produjo la reunificación de Alemania?</p><p>The government has banned foods containing intestine or thymus from calves because a new scientific study suggested that they might be contaminated with the infectious agent of bovine spongiform encephalopathy, commonly called "mad cow disease".</p><p>we drew the question 'What is another name for the "mad cow disease"?'. Finally, we obtained a benchmark corpus in which each question appears in six languages, as the tag attribute &lt;language val&gt; in figure <ref type="figure" coords="6,170.46,148.19,4.92,10.89" target="#fig_0">3</ref> shows: &lt;qa cnt="4" type="OTHER"&gt; &lt;language val="ENG" original="TRUE"&gt; &lt;question assessor="ALE"&gt; What is another name for the "mad cow disease"? &lt;/question&gt; &lt;answer n="1" idx="LA091194.0096"&gt; bovine spongiform encephalopathy &lt;/answer&gt; &lt;/language&gt; &lt;language val="ITA" original="FALSE"&gt; &lt;question assessor="ALE"&gt; Qual è un altro nome per la "malattia della mucca pazza"? &lt;/question&gt; &lt;answer n="1" idx=""&gt; SEARCH[bovine spongiform encephalopathy] &lt;/answer&gt; &lt;/language&gt; &lt;language val="SPA" original="FALSE"&gt; &lt;question assessor=""&gt; ¿Qué otro nombre recibe la enfermedad de las vacas locas? &lt;/question&gt; &lt;answer n="1" idx=""&gt; SEARCH[bovine spongiform encephalopathy] &lt;/answer&gt; &lt;/language&gt; &lt;language val="DUT" original="FALSE"&gt; &lt;question assessor=""&gt; Wat is een andere naam voor "gekke-koeienziekte"? &lt;/question&gt; &lt;answer n="1" idx=""&gt; SEARCH[bovine spongiform encephalopathy] &lt;/answer&gt; &lt;/language&gt; &lt;language val="GER" original="FALSE"&gt; &lt;question assessor=""&gt; Was ist ein anderer Name für "Rinderwahnsinn"? &lt;/question&gt; &lt;answer n="1" idx=""&gt; SEARCH[bovine spongiform encephalopathy] &lt;/answer&gt; &lt;/language&gt; &lt;language val="FRE" original="FALSE"&gt; &lt;question assessor=""&gt; Quel autre nom donne-t-on à la "maladie de la vache folle"? &lt;/question&gt; &lt;answer n="1" idx=""&gt; SEARCH[bovine spongiform encephalopathy] &lt;/answer&gt; &lt;/language&gt; &lt;/qa&gt; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Participants had one week to process the questions. Since no manual intervention of any kind was allowed, we asked participants to freeze their systems before downloading the queries from our "QA @ CLEF" website. 2 Before the start of the evaluation exercise, we released detailed guidelines with the necessary information about the required format of the submissions. We also put online a checking routine with which participants could make sure that their responses were in compliance with that.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Response Format</head><p>Since we allowed to submit both exact answers and 50 bytes long strings, we could not evaluate these two formats together. For this reason, we divided our track into two subtasks with separated evaluations. The required format of the answers in both subtasks was the same, but we decided to draw up two separate results. Figure <ref type="figure" coords="7,99.58,236.75,4.92,10.89" target="#fig_1">4</ref> shows an example of a participant's submissions, where the first column indicates the question number, provided by the organisers, and the string in the second one represents the unique identifier for a system and a run: the last two characters in this case show that the task is the bilingual Italian, and the fifth and sixth characters give information about the kind of responses retrieved in this run, i.e. exact answers. The third field in the response format was the answer rank, which was crucially important for the evaluation of the system accuracy. Participants had to return the questions in the same order in which they had been downloaded, i.e. unranked. On the contrary, they had to rank their responses by confidence, putting in the first place the surest answer. The integer or floating point score number of the fourth column justified the answer ranking. This field was not compulsory, and the systems that had no scoring strategies could set the value to default 0 (zero). The docid, i.e. the unique identifier of the document that supports the given answer, is placed in the fifth column. If the system maintained that there was no answer in the corpus or if it could not find one, the docid was replaced by the string "NIL". The answer string had to be given in the last field of the response, that was left empty when the docid was substituted by "NIL". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Judgements and Evaluation Measures</head><p>Each single answer was judged by human assessors, who assigned to each response a unique label: either right, or wrong, or unsupported or inexact. Assessors were told to judge the submissions from a potential user's point of view, because the evaluation should take into consideration the future portability of QA systems. They analysed both the answers themselves and the context, i.e. the document that supported the answer, in which they appeared. Answers were judged to be incorrect (W) when the answer-string did not contain the answer or when the answer was not responsive. In contrast, a response was considered to be correct (R) when the answer-string consisted of nothing more than the exact, minimal answer (or contained the correct answer within the 50 bytes long string) and when the document returned supported the response. Unsupported answers (U) were correct but it was impossible to infer that they were responsive from the retrieved document. Answers were judged as non-exact (X) when the answer was correct and supported by the document, but the answer string missed bits of the response or contained more than just the exact answer. In addition, we outlined some common criteria to distinguish and properly evaluate exact answers. We outlined general rules to apply in several cases: as regards the date of specific events that ended in the past, both day and year are normally required (unless the question refers only to the year), but if the day cannot be retrieved, the year is normally sufficient. For instance, if a system answered the question "When did Napoleon die?" returning "5th May", it would be judged as incorrect. On the other hand, both "May 5, 1821" and "1821" could be correct exact answers. Actually, no clear definitions of exact answer have been formalised, yet. Discussing the issue, we noticed that, generally speaking, articles and prepositions do not invalidate an "exact" answer. So, both "July, 9" and "on the 9th of July" are exact answers. Similarly, appositions should not represent a problem, as well. So for instance, "1957", "year 1957" and "in the year 1957" should be exact answers, though someone could object that (with dates) "year" is redundant. When a query asks for a measure, the unit of measure can be accepted, too. So, both "30" and "30 degrees" are exact.</p><p>Concerning NIL answers, they are correct if neither human assessors nor systems have found any answer before or after the assessment process. If there is an answer in the collection, NIL is evaluated as incorrect. A NIL answer means that the system believes that there is not an answer for that question in the collection. There is no way for systems to explicitly indicate that they do not know or cannot find the answer for a question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question and judged responses Comment</head><p>What museum is directed by Henry Hopkins? W 1 irstex031bi 1 3253 LA011694-0094 Modern Art U 1 irstex031bi 2 1776 LA011694-0094 UCLA X 1 irstex031bi 3 1251 LA042294-0050 Cultural Center</p><p>The second answer was correct but the document retrieved was not relevant. The third response missed bits of the name, and was judged non-exact.</p><p>Where did the Purussaurus live before becoming extinct? W 2 irstex031bi 1 9 NIL</p><p>The system erroneously "believed" that the query had no answer in the corpus, or could not find one. When did Shapour Bakhtiar die? R 3 irstex031bi 1 484 LA012594-0239 1991 W 3 irstex031bi 2 106 LA012594-0239 Monday</p><p>In the questions that asked for the date of an event, the year was often regarded as sufficient.</p><p>Who is John J. Famalaro accused of having killed? W 4 irstex031bi 1 154 LA072294-0071 Clark R 4 irstex031bi 2 117 LA072594-0055 Huber W 4 irstex031bi 3 110 LA072594-0055 Department</p><p>The second answer, that returned the victim's last name, was considered sufficient and correct, since in the document retrieved no other people named "Huber" were mentioned. In strict evaluation, only correct answers (R) scored points, while in lenient evaluation the unsupported responses (U) were considered to be correct, too. The score of each question was the reciprocal of the rank for the first answer to be judged correct, which means that each query could receive either 1, or 0, or 0.333, or 0.5 points, depending on the confidence ranking. The basic evaluation measure was the Mean Reciprocal Rank (MRR), that represents the mean score over all questions. MRR takes into consideration both recall and precision of the systems' performance, and can range between 0 (no correct responses) and 1 (all the 200 queries have a correct answer at position one). Figures <ref type="figure" coords="8,502.31,549.23,4.92,10.89">6</ref> and<ref type="figure" coords="8,70.80,560.75,4.92,10.89">7</ref> below summarise the QA track results and show that the systems achieved better results in the monolingual than in the bilingual tasks, where the drop in performance is possibly due to the cross-language step. The QA system developed by ITC-irst proved to be the most accurate among those that participated, and the mean reciprocal rank scored in the monolingual Italian using 50 bytes long strings as answers was the highest of the whole QA track. Answer responsiveness and exactness were in the opinion of human assessors, whose judgement could be different, as in everyday life we have different criteria to determine whether a response is good or not. During the evaluation of most of the runs, two different assessors judged each single question (each question of the bilingual runs were judged by three NIST assessors) and in case of discrepancies, they discussed their opinion and tried to reach an agreement. Whenever they could not agree, another person took the final decision.</p><p>After the submission deadline had passed, we detected some mistakes in the questions. In particular, a blunder persisted in the Italian queries: we wrongly put an apostrophe after the contraction of the question word "quale" ("which"/"what"). We found 21 cases in the monolingual test set and 17 cases in the bilingual one. In the TREC campaigns the questions that contain mistakes are excluded from the evaluation, but, considering that the form "qual'e'/era" is quite common in Italian and that a QA system should be robust enough to recognise variant spellings, we decided to keep those queries. For the sake of completeness, we calculated precision and recall without the questions with that mistake, and we obtained just a very minor variation of the values (around 1%). Translation could be the source of mistakes, as well. In the monolingual Spanish questions collection, "minister of Foreign Affairs" was erroneously translated as "president of Foreign Affairs" during the question sharing between the Italian and the Spanish co-ordinators. As can be noticed in graphs 1 and 2, strict and lenient evaluation results do not differ much. More strikingly, the performance of the cross-language systems turned out to be quite low, which suggests that multilinguality is a field that requires much more attention and investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXACT ANSWERS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Perspectives</head><p>The first European evaluation of non-English QA systems has given rise to useful resources for future multilingual QA developments. It has allowed us to establish and test a methodology and criteria for both the test suit production and the assessment procedure. Unfortunately, the CLEF QA Track did not receive the expected attention in terms of participation, and in most tasks just one group submitted its results. Actually, twelve research groups registered and were interested into participating, but some of them could not adjust their system on time. This suggests that the debate and the activities on multilingual QA have a certain appeal on the community, even though much challenging work remains to be done. We can be pleased of the outcome of this pilot QA evaluation exercise, and we hope that the results and the resources we developed will encourage many other groups to participate in future campaigns. Cross-linguality has always been out of the scope of the TREC QA tracks, and our pilot QA at CLEF hopefully represents a first step in the direction of more sophisticated evaluation campaigns of multilingual systems. In our track, we provided five non-English question sets but just one English target document collection: in the future we could have several reference corpora in different languages, many different question sets and answers translated into different languages. Multilinguality provides us with the opportunity to experiment with different approaches, exploring many potential applications: for instance, we could think about developing intelligent systems that taking into consideration the language and the text coverage, select the most useful target corpus to search the answer for a particular question posed in a particular language. The possibilities are manifold, and our cross-language tasks can be considered just a starting point.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,192.24,710.54,210.62,8.77"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: gold standard format of a question for the bilingual tasks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,157.20,544.94,281.08,8.77"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: examples of responses drawn from the first bilingual run submitted by ITC-irst</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,145.44,459.74,304.62,8.77"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: examples of judged responses drawn from the first bilingual run submitted by ITC-irst</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,75.47,163.31,434.00,591.52"><head></head><label></label><figDesc>Figure 7: summary statistics of the 50 bytes answer runs Graph 2: Mean Reciprocal Rank score of the 50 bytes long answer runs</figDesc><table coords="9,75.47,163.31,434.00,591.52"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">50 BYTES LONG ANSWER RUNS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>No. Q with at</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">GROUP</cell><cell>TASK</cell><cell></cell><cell cols="2">RUN NAME</cell><cell></cell><cell>MRR</cell><cell></cell><cell>least one Right answer</cell><cell>NIL Questions</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">strict lenient strict lenient returned correctly</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>returned</cell></row><row><cell cols="6">MONO-LINGUAL TASKS GROUP ITC-irst DFKI CROSS-LANGUAGE TASKS RALI</cell><cell cols="3">Monolingual Italian TASK Bilingual German Bilingual French</cell><cell cols="6">RUNS MRR .449 .471 strict lenient strict lenient returned correctly 99 104 5 2 No. of Q. with at least one right answer NIL Questions returned dfkist031bg .098 .103 irstst032mi RUN NAME 29 30 18 0 udemst031bf .213 .220 56 58 4 1</cell></row><row><cell>MONO-</cell><cell cols="2">LINGUAL</cell><cell>TASKS</cell><cell cols="2">DLSI-UA ITC-irst UVA</cell><cell cols="2">monolingual Spanish monolingual Italian monolingual Dutch</cell><cell cols="5">alicex031ms alicex032ms irstex031mi uamsex031md .298 .317 .307 .320 .296 .317 .422 .442 uamsex032md .305 .335</cell><cell cols="2">80 70 97 78 82</cell><cell>87 77 101 82 89</cell><cell>21 21 4 200 200</cell><cell>5 5 2 17 17</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ISI</cell><cell></cell><cell cols="2">bilingual Spanish</cell><cell></cell><cell cols="2">isixex031bs isixex032bs</cell><cell cols="2">.302 .328 .271 .307</cell><cell cols="2">69 68</cell><cell>77 78</cell><cell>4 4</cell><cell>0 0</cell></row><row><cell>CROSS-</cell><cell cols="2">LANGUAGE</cell><cell>TASKS</cell><cell cols="2">ITC-irst CS-CMU DLTG</cell><cell cols="2">bilingual Italian bilingual French bilingual French</cell><cell></cell><cell cols="2">irstex031bi irstex032bi lumoex031bf lumoex032bf dltgex031bf dltgex032bf</cell><cell cols="2">.322 .334 .393 .400 .153 .170 .131 .149 .115 .120 .110 .115</cell><cell cols="2">77 90 38 31 23 22</cell><cell>81 92 42 35 24 23</cell><cell>49 28 92 91 119 119</cell><cell>6 5 8 7 10 10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RALI</cell><cell cols="2">bilingual French</cell><cell></cell><cell cols="4">udemex032bf .140 .160</cell><cell cols="2">38</cell><cell>42</cell><cell>3</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">Figure 6: summary statistics of the exact answer runs</cell></row><row><cell></cell><cell></cell><cell cols="2">0,6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">0,5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">0,4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MRR</cell><cell cols="2">0,3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">0,2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">0,1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>alicex031ms</cell><cell>alicex032ms</cell><cell>irstex031mi</cell><cell>uamsex031md</cell><cell>uamsex032md</cell><cell>isixex031bs</cell><cell>isixex032bs</cell><cell>irstex031bi</cell><cell>irstex032bi</cell><cell>lumoex031bf</cell><cell>lumoex032bf</cell><cell>dltgex031bf</cell><cell>dltg032bf</cell><cell>udemex032bf</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>run</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">monolingual strict</cell><cell cols="4">monolingual lenient</cell><cell cols="2">bilingual strict</cell><cell>bilingual lenient</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">Graph 1: Mean Reciprocal Rank score of the exact answer runs</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>The work described in this paper has been supported by the <rs type="funder">Autonomous Province of Trento</rs>, in the framework of the <rs type="projectName">WebFAQ Project</rs>, by the <rs type="funder">Spanish Government</rs> (<rs type="projectName">MCyT</rs>, <rs type="grantNumber">TIC-2002-10597-E</rs>) and by the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">365-20-005</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">220-80-001</rs>, and <rs type="grantNumber">612.000.207</rs>. The authors wish to acknowledge the contribution and support given by <rs type="person">Franca Rossi</rs>, <rs type="person">Elisabetta Fauri</rs>, <rs type="person">Pamela Forner</rs> and <rs type="person">Manuela Speranza</rs> at <rs type="affiliation">ITC-irst</rs>, who helped us in the generation and verification of the questions for both the monolingual and bilingual tasks.</p><p><rs type="person">Stephan Busemann</rs> at <rs type="affiliation">DFKI</rs> and <rs type="person">Jian-Yun Nie</rs> at the <rs type="affiliation">University of Montreal</rs> took on the job of translating into German and French the questions for the bilingual tasks, and their contribution was fundamental. We wish to thank the <rs type="institution">NIST</rs>, and in particular <rs type="person">Ellen Voorhees</rs> and <rs type="person">Donna Harman</rs>, for judging all the bilingual runs and for providing us with the necessary resources and feedback we needed to organise this pilot track. We are also grateful to <rs type="person">Carol Peters</rs> (<rs type="affiliation">ISTI-CNR</rs>) and <rs type="person">Charles Callaway</rs> (<rs type="affiliation">ITC</rs>-irst) who, as English native speakers, edited the questions for the cross-language tasks. We wish to thank <rs type="person">Henry Chinaski</rs>, <rs type="person">Vera Hollink</rs>, and <rs type="person">Valentin Jijkoun</rs> for their help in the development and assessment of the monolingual Dutch task.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_RKpv4xZ">
					<orgName type="project" subtype="full">WebFAQ Project</orgName>
				</org>
				<org type="funded-project" xml:id="_qAwRsS5">
					<idno type="grant-number">TIC-2002-10597-E</idno>
					<orgName type="project" subtype="full">MCyT</orgName>
				</org>
				<org type="funding" xml:id="_eSde73J">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_8bXTsXf">
					<idno type="grant-number">365-20-005</idno>
				</org>
				<org type="funding" xml:id="_cG39Zu3">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_cZYEfK4">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_bjVucha">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_GHfWCMW">
					<idno type="grant-number">612.000.207</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,74.90,363.23,105.55,10.89" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Peters</forename><surname>Braschler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.80,374.75,453.58,10.89;11,70.80,386.27,453.63,10.89;11,70.80,397.79,99.82,10.89;11,70.80,409.31,119.45,10.89;11,448.67,409.31,2.46,10.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,203.27,374.75,321.11,10.89;11,70.80,386.27,31.34,10.89">The CLEF Campaigns: Evaluation of Cross-Language Information Retrieval Systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,125.60,386.27,282.43,10.89">UPGRADE (The European Online Magazine for the IT Professional)</title>
		<imprint>
			<biblScope unit="volume">III</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="78" to="81" />
			<date type="published" when="2002-06">June 2002</date>
		</imprint>
	</monogr>
	<note>CEPIS. Electronic version available at</note>
</biblStruct>

<biblStruct coords="11,75.18,432.35,73.27,10.89" xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Burger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.80,443.87,453.51,10.89;11,70.80,455.39,453.66,10.89;11,70.80,466.91,453.70,10.89;11,70.80,478.43,112.14,10.89;11,70.80,489.71,412.89,10.89" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Israel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jacquemin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maiorano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ogden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shrihari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weishedel</surname></persName>
		</author>
		<ptr target="http://www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc" />
		<title level="m" coord="11,214.23,466.91,310.27,10.89;11,70.80,478.43,70.88,10.89">Issues Tasks and Program Structures to Roadmap Research in Question &amp; Answering (Q&amp;A)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,76.27,512.75,134.53,10.89;11,70.80,524.27,377.31,10.89;11,73.26,535.79,256.42,10.89" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,76.49,524.27,363.11,10.89">CLEF 2003 Question Answering Track: Guidelines for the Monolingual and Bilingual Tasks</title>
		<author>
			<orgName type="collaboration" coords="11,76.27,512.75,21.88,10.89;11,122.76,512.75,83.87,10.89">CLEF ; QA Track Guidelines</orgName>
		</author>
		<ptr target="http://clef-qa.itc.it/guidelines.htm" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,75.09,558.83,68.44,10.89" xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Knott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.80,570.35,453.63,10.89;11,70.80,581.87,453.60,10.89;11,70.80,593.39,285.23,10.89" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,388.49,570.35,135.95,10.89;11,70.80,581.87,73.27,10.89">A question-answering system for English and Ma_ri</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Knott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bayard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>De Jager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Moorfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>O'keefe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,166.92,581.87,357.48,10.89;11,70.80,593.39,71.93,10.89">Proceedings of the Fifth Biannual Conference on Artificial Neural Networks and Expert Systems (ANNES)</title>
		<meeting>the Fifth Biannual Conference on Artificial Neural Networks and Expert Systems (ANNES)</meeting>
		<imprint>
			<date type="published" when="2001-11">November 2001</date>
			<biblScope unit="page" from="223" to="228" />
		</imprint>
		<respStmt>
			<orgName>University of Otago</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,75.27,616.19,47.75,10.89" xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Liddy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.80,627.71,453.62,10.89;11,70.80,639.23,453.65,10.89;11,70.80,650.75,109.36,10.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,130.12,627.71,394.30,10.89;11,70.80,639.23,74.17,10.89">Why are People Asking these Questions? A Call for Bringing Situation into Question-Answering System Evaluation</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Liddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,167.33,639.23,324.80,10.89">LREC Workshop Proceedings on Question Answering -Strategy and Resources</title>
		<meeting><address><addrLine>Grand Canary Island, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,75.38,673.79,79.08,10.89" xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Magnini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.80,685.31,453.59,10.89;11,70.80,696.83,431.41,10.89" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,268.01,685.31,235.75,10.89">Multilingual Question/Answering: the DIOGENE System</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,70.80,696.83,261.76,10.89">Proceedings of the Tenth Text REtrieval Conference (TREC 2001)</title>
		<meeting>the Tenth Text REtrieval Conference (TREC 2001)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="500" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,75.38,719.87,57.49,10.89" xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Magnini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.80,731.39,453.56,10.89;11,70.80,742.91,92.14,10.89;12,70.80,71.15,382.21,10.89" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="11,130.64,731.39,246.77,10.89">Evaluation of Cross-Language Question Answering Systems</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<ptr target="http://clef.iei.pi.cnr.it:2002/workshop2002/presentations/q-a.pdf" />
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>proposal presentation held at the CLEF Workshop</note>
</biblStruct>

<biblStruct coords="12,75.72,94.19,59.88,10.89" xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Maybury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.80,105.71,257.00,10.89;12,70.80,117.23,122.18,10.89;12,70.80,128.75,367.19,10.89;12,70.80,151.79,66.45,10.89" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="12,135.68,105.71,157.62,10.89">Toward a Question Answering Roadmap</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maybury</surname></persName>
		</author>
		<ptr target="www.mitre.org/work/tech_papers/tech_papers_02/maybury_toward/maybury_toward_qa.pdf" />
		<imprint>
			<date type="published" when="1999">2002. Voorhees 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.80,163.31,453.72,10.89;12,70.80,174.83,255.64,10.89" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,147.18,163.31,190.14,10.89">The TREC-8 Question Answering Track Report</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,359.12,163.31,165.41,10.89;12,70.80,174.83,85.51,10.89">Proceedings of the Eighth Text REtrieval Conference (TREC-8)</title>
		<meeting>the Eighth Text REtrieval Conference (TREC-8)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="500" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,75.36,197.63,61.89,10.89" xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Voorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.80,209.15,453.63,10.89;12,70.80,220.67,309.76,10.89" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,148.86,209.15,228.84,10.89">Overview of the TREC 2001 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,400.61,209.15,123.83,10.89;12,70.80,220.67,140.10,10.89">Proceedings of the Tenth Text REtrieval Conference (TREC 2001)</title>
		<meeting>the Tenth Text REtrieval Conference (TREC 2001)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="500" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,75.36,243.71,61.89,10.89" xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Voorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.80,255.23,453.49,10.89;12,70.80,266.75,330.26,10.89" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,150.61,255.23,233.21,10.89">Overview of the TREC 2002 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,407.71,255.23,116.58,10.89;12,70.80,266.75,160.60,10.89">Proceedings of the Eleventh Text REtrieval Conference (TREC 2002)</title>
		<meeting>the Eleventh Text REtrieval Conference (TREC 2002)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="500" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,75.36,289.79,98.51,10.89" xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Tice</forename><surname>Voorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.80,301.31,453.64,10.89;12,70.80,312.83,140.84,10.89" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,200.07,301.31,191.29,10.89">Building a Question Answering Test Collection</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,413.20,301.31,106.40,10.89">Proceedings of SIGIR2000</title>
		<meeting>SIGIR2000<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
