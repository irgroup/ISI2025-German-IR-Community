<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,418.35,106.49,27.90,12.55">2003</title>
				<funder ref="#_jNp6FQY #_8T5NRYG">
					<orgName type="full">NWO</orgName>
				</funder>
				<funder ref="#_AkfJdjB">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_KYxzeZm #_kgu64Dy #_egAr6Ae #_BPGT5QM">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,159.94,136.74,77.92,10.76;1,261.77,136.74,26.56,10.76"><forename type="first">Valentin</forename><forename type="middle">Jijkoun</forename><surname>Gilad</surname></persName>
							<email>gilad@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,291.32,136.74,35.87,10.76;1,351.10,136.74,82.88,10.76"><forename type="first">Mishne</forename><surname>Maarten De Rijke</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">CLEF</orgName>
								<orgName type="institution">The University of Amsterdam at QA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Language &amp; Inference Technology Group</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Nieuwe ; Achtergracht 166</addrLine>
									<postCode>1018 WV</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,418.35,106.49,27.90,12.55">2003</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DE6C046E9D75C7F253181C20E4EA87A0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the official runs of our team for QA@CLEF 2003. We took part in the monolingual Dutch Question Answering task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this year's CLEF evaluation exercise we participated in the Dutch Question Answering task, new on the CLEF agenda, building on and extending our earlier work on question answering at TREC <ref type="bibr" coords="1,408.69,317.53,10.58,8.97" target="#b5">[6]</ref>. We experimented with a multi-stream architecture for question answering, in which the different independent streams, each a complete QA system in its own right, compete with each other to provide the system's final answer.</p><p>The paper is organized as follows. In Section 2 we describe the architecture of our system. Section 3 describes our official runs. In Section 4 we discuss the results we have obtained. Finally, in Section 5 we offer some preliminary conclusions regarding our Dutch question answering efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>The general architecture of a question answering (QA) system, shared by many systems, can be summed up as follows. A question is first associated with a question type, out of a predefined set such as DATE-OF-BIRTH or CURRENCY. Then a query is formulated based on the question, and an information retrieval engine is used to identify a list of documents that are likely to contain the answer. Those documents are sent to an answer extraction module, which identifies candidate answers, ranks them, and selects the final answer. On top of this basic architecture, numerous add-ons have been devised, ranging from logic-based methods <ref type="bibr" coords="1,443.68,488.99,11.62,8.97" target="#b4">[5]</ref> to ones that rely heavily on the redundancy of information available on the World Wide Web <ref type="bibr" coords="1,374.52,500.94,10.58,8.97" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-Stream Architecture</head><p>During the design of our QA system, it became evident that there are a number of distinct approaches for the task; some are beneficial for all question types, and others only for a subset. For instance, abbreviations are often found enclosed in brackets, following the multi-word string they abbreviate, as in "Verenigde Naties <ref type="bibr" coords="1,477.48,572.05,17.06,8.97">(VN)</ref>." This suggests that for abbreviation questions the text corpus can be mined to extract multi-word strings with leading capitals followed by capitalized strings in brackets; the results can then be stored in a table to be consulted when an abbreviation (or an expansion of an abbreviation) is being asked for. Similar table-creation strategies are applicable for questions that ask for capitals, dates-of-birth, etc., whereas the approach seems less appropriate for definition questions, why-questions, or how-to questions. It was therefore decided to implement a multi-stream system: a system that includes a number of separate and independent subsystems, each of which is a complete standalone QA system that produces ranked answers, but not necessarily for all types of questions; the system's answer is then taken from the combined pool of candidates.</p><p>Scientifically, it is interesting to understand the performance of each stream on specific question types and in general. On the practical side, our multi-stream architecture allows us to modify and test a stream without affecting the rest of the system. A general overview of our system is given in Figure <ref type="figure" coords="1,376.79,703.56,3.74,8.97" target="#fig_0">1</ref>. The system consists of 5 separate QA streams and a final answer selection module that combines the results of all streams and produces the final answers. Question Answering Streams. We now provide a brief description of the five streams of our QA system: Table <ref type="table" coords="2,70.19,321.77,28.70,8.97">Lookup</ref>, Pattern Match, English Tequesta, Dutch Tequesta, and Web Answer.</p><p>The Table Lookup stream uses specialized knowledge bases constructed by preprocessing the collection, exploiting the fact that certain information types (such as country capitals, abbreviations, and names of political leaders) tend to occur in the document collection in a small number of fixed patterns. When a question type indicates that the question might potentially have an answer in these tables, a lookup is performed in the appropriate knowledge base and answers which are found there are assigned high confidence. For example, to collect abbreviation-expansion pairs we searched the document collection for strings of capitals in brackets; upon finding one, we extracted sequences of capitalized non-stopwords preceding it, and stored it in the "abbreviation knowledge base." This approach answered question such as: For a detailed overview of this stream, see <ref type="bibr" coords="2,240.75,487.74,10.58,8.97" target="#b2">[3]</ref>.</p><p>In the Pattern Match stream, zero or more Perl regular patterns are generated for each question according to its type and structure. These patterns indicate strings which contain the answer with high probability, and are then matched against the entire document collection. Here's a brief example: The English Tequesta stream translates the questions to English using Worldlingo's free translation service at http://www.worldlingo.com/. The auto-translated questions are then fed to Tequesta, an existing QA system for English developed at the University of Amsterdam <ref type="bibr" coords="2,297.13,617.86,10.58,8.97" target="#b5">[6]</ref>. The system uses the English CLEF corpus, and is extended with an Answer Justification module to anchor the answer in the Dutch collection.</p><p>The Dutch Tequesta is an adaptation of English Tequesta to Dutch and used as an independent stream, provided with the original Dutch newspaper corpus. The modifications to the original system included replacing (English) language specific components by Dutch counterparts; for instance, we trained TNT <ref type="bibr" coords="2,404.12,665.68,11.62,8.97" target="#b0">[1]</ref> to provide us with Part-of-Speech tags using the Corpus Gesproken Nederlands <ref type="bibr" coords="2,284.09,677.63,10.79,8.97" target="#b7">[7]</ref>; a named entity tagger for Dutch was also developed.</p><p>The Web Answer stream looks for an answer to a question on the World Wide Web, and then attempts to find justification for this answer in the collection. First, the question is converted to a web query, by leaving only meaningful keywords and (optionally) using lexical information from EuroWordNet. The query is sent to a web search engine (for the experiments reported here we used Google); if no relevant Web documents are found, the query is translated to English and sent again. Next, if the query yields some results, words and phrases appearing in the snippets of the top results are considered as possible answers, and ranked according to their relative frequency over all snippets. The Dutch named entity tagger and some heuristics were used to enhance the simple counts for the terms (e.g., terms that matched a TIME named entity were given a higher score if the expected answer type was a date). Finally, justifications for the answer candidates are found in the local Dutch corpus.</p><p>While each of the above streams is a "small" QA system in itself, many components are shared between the streams, including, for instance, an Answer Justification module that tries to ground externally found facts in the Dutch CLEF corpus, and a Web Ranking module that uses search engine hit counts to rank the candidate answers from our streams in a uniform way, similar to <ref type="bibr" coords="3,253.72,145.08,10.58,8.97" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs</head><p>We submitted two runs for the Dutch question answering task: uamsex031md and uamsex032md. Both runs returned exact answers, and both combined answers from all streams, but differed slightly in the method of using the search engine hit counts for ranking the answers. The score of an answer was the product of the confidence measure produced by the stream generating the answer and the "Web Hit Count" measure, which equals the number of hit counts produced by Google for a query made up of the answer and keywords from the question. To prefer queries with words that do not occur frequently, we also calculated a "Query Value" measure: in uamsex031md, the query value was calculated using the word frequencies of the query words in the CLEF English and Dutch corpora, and in uamsex032md it was calculated using the Web hit count of the answer alone. Query values were used to normalize the Web Hit Count measure.</p><p>Here is a simplified example, in which the method used for uamsex031md produced better results (stream confidence level not displayed): Shortly after the submission, we discovered a couple of implementation bugs that caused some of the Table Lookup stream answers to be incorrect. Below we also discuss two post-submission runs, uamsex031md.fixed and uamsex032md.fixed, which are identical to the submitted runs but with these implementation bugs fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>The following table shows the evaluation results of our CLEF 2002 submissions and the two post-submission runs described above. Beside the standard Strict and Lenient measures, we also evaluated our runs using more "generous" Lenient, Non-exact measure that accepts non-exact answers as correct. The run uamsex032md scored better than uamsex031md: as expected, normalizing web hit counts according to the distribution of words on the web yielded a more accurate ranking than normalization using corpus word frequencies. Also, the two runs with the fixed Table Lookup stream outperformed our official runs. An error analysis of the questions which had a correct answer with incorrect document ID (i.e. those separating Strict and Lenient scores) revealed that answers with incorrect justifications did not necessarily come from external resources (the Web and English Tequesta streams); this suggests a local problem in our justification mechanism, rather than an inherent inability to justify externally found answers in the local corpus. Taking this into account, our 53.5% score in the table seems quite realistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strict</head><p>It is interesting to see the increase in performance with the Lenient, Non-exact measure. Most of the non-exact answers that the system produced contained noise around the correct answer strings, e.g. "Jacques Delors. Met", "Kim Il Sung. Japan" or "1989, heeft vooral in het oostelijke deel van Berl", due to named entity extraction errors.</p><p>An initial analysis of the contribution of the different answering streams to the system's overall performance suggests that every stream has its own strengths, that is, specific question types for which it provides correct answers with higher probability than other streams. The Web Answer stream, for example, seemed to perform better than other streams on questions for which the answer was a date; the Pattern and Table Lookup streams had very good performance on the specific (5-6) question types for which they were used. Every stream contributed some correct answers, so the total combined output of the system was better than any subsystem alone. E.g., out of the 200 questions, 54 (27%) were answered by the Table Lookup stream; of these, 26 answers (13% of the total answers) came solely from this stream. A further analysis of the performance of our streams on different question types will allow us to give each stream a confidence weight conditioned on question type, and thus to make the answer selection more informed, in ways similar to the approach adopted by BBN for TREC 2002 <ref type="bibr" coords="4,464.38,216.81,10.58,8.97" target="#b9">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We presented our multi-stream question answering system and the runs it produced for CLEF 2003. Running in parallel several subsystems that approach the QA task from different angles proved successful, as some approaches seem better fit to answer certain types of questions than others.</p><p>Our current ongoing work on the system is focused on extensions of the Table Lookup stream and the Web Answer stream. Future plans also include improvements of the voting mechanism between the answers provided by the different streams, and enhancing the system to support definition and list questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,141.34,278.37,311.23,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The University of Amsterdam's Dutch Question Answering System.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We want to thank <rs type="person">Christine Foeldesi</rs> for her work on developing a named entity tagger for Dutch. <rs type="person">Valentin Jijkoun</rs> and <rs type="person">Gilad Mishne</rs> were supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs> under project number <rs type="grantNumber">220-80-001</rs>. Maarten de Rijke was supported by grants from <rs type="funder">NWO</rs>, under project numbers <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">365-20-005</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">220-80-001</rs>, and <rs type="grantNumber">612.000.207</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AkfJdjB">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_jNp6FQY">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_8T5NRYG">
					<idno type="grant-number">365-20-005</idno>
				</org>
				<org type="funding" xml:id="_KYxzeZm">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_kgu64Dy">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_egAr6Ae">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_BPGT5QM">
					<idno type="grant-number">612.000.207</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,86.78,465.75,436.95,8.97;4,86.78,477.70,75.00,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,134.61,465.75,163.62,8.97">TnT -a statistical part-of-speech tagger</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,321.84,465.75,197.53,8.97">Proceedings of the 6th Applied NLP Conference</title>
		<meeting>the 6th Applied NLP Conference</meeting>
		<imprint>
			<publisher>ANLP-2000</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,86.78,495.84,436.95,8.97;4,86.78,507.79,22.42,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,158.03,495.84,230.74,8.97">AskMSR: Question answering using the Worldwide Web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,412.37,495.84,106.88,8.97">Proceedings EMNLP 2002</title>
		<meeting>EMNLP 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,86.78,525.92,436.95,8.97;4,86.78,537.88,348.62,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,244.84,525.92,212.47,8.97">Preprocessing Documents to Answer Dutch Questions</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,474.38,525.92,49.36,8.97;4,86.78,537.88,300.24,8.97">Proceedings of the 15th Belgian-Dutch Conference on Artificial Intelligence (BNAIC&apos;03)</title>
		<meeting>the 15th Belgian-Dutch Conference on Artificial Intelligence (BNAIC&apos;03)</meeting>
		<imprint/>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="4,86.78,556.01,436.95,8.97;4,86.78,567.97,436.94,8.97;4,86.78,579.92,225.54,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,383.66,556.01,140.07,8.97;4,86.78,567.97,155.37,8.97">Is it the right answer? exploiting web redundancy for answer validation</title>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hristo</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,264.97,567.97,258.76,8.97;4,86.78,579.92,131.08,8.97">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="425" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,86.78,598.06,436.95,8.97;4,86.78,610.01,264.36,8.97" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="4,86.78,610.01,140.41,8.97">LCC Tools for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Morarescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lacatusu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Novischi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Badulescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bolohan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>Voorhees and Harman</note>
</biblStruct>

<biblStruct coords="4,86.78,628.15,436.95,8.97;4,86.78,640.10,436.94,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,197.75,628.15,308.45,8.97">Tequesta: The University of Amsterdam&apos;s textual question answering system</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,255.44,640.10,175.56,8.97">The Tenth Text REtrieval Conference (TREC</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,86.78,652.06,360.87,8.97" xml:id="b6">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j" coord="4,282.73,652.06,101.84,8.97">NIST Special Publication</title>
		<imprint>
			<biblScope unit="page" from="500" to="250" />
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="4,86.78,670.19,436.94,8.97;4,86.78,682.15,62.27,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,141.97,670.19,232.51,8.97">The Spoken Dutch Corpus: Overview and first evaluation</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Oostdijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,395.42,670.19,97.87,8.97">Proceedings LREC 2000</title>
		<meeting>LREC 2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="887" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,86.78,700.28,436.94,8.97;4,86.78,712.24,306.36,8.97" xml:id="b8">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="4,258.15,700.28,265.57,8.97;4,86.78,712.24,134.50,8.97">The Tenth Text REtrieval Conference (TREC 2002)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="500" to="251" />
		</imprint>
	</monogr>
	<note>National Institute for Standards and Technology</note>
</biblStruct>

<biblStruct coords="4,86.78,730.37,436.95,8.97;4,86.78,742.33,208.97,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="4,329.82,730.37,193.91,8.97;4,86.78,742.33,85.66,8.97">TREC 2002 QA at BBN: Answer selection and confidence estimation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Licuanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,190.82,742.33,88.34,8.97">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
