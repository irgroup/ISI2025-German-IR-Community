<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,97.03,148.86,408.92,15.15;1,199.29,170.78,204.38,15.15">A Cross-Language Question/Answering-System for German and English</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,212.55,204.67,75.93,8.74"><forename type="first">Günter</forename><surname>Neumann</surname></persName>
							<email>neumann@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="department">LT-Lab</orgName>
								<orgName type="institution">DFKI</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.16,204.67,79.30,8.74"><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
							<email>bogdan@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="department">LT-Lab</orgName>
								<orgName type="institution">DFKI</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,97.03,148.86,408.92,15.15;1,199.29,170.78,204.38,15.15">A Cross-Language Question/Answering-System for German and English</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1F043CFF628C933A0B09879BC47703C7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the work done by the QA group of the Language Technology Lab at DFKI, for the 2003 edition of the Cross-Language Evaluation Forum (CLEF). We have participated in the new track "Multiple Language Question Answering (QAat-CLEF)" that offers tasks to test monolingual and cross-language QA-systems. In particular we developed an open-domain bilingual QA-System for German source language queries and English target document collections. Since it was our very first participation at such kind of competition, the focus was on system implementation rather than system tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The basic functionality of an open-domain cross-language question/answering (QA) system is simple: given a Natural Language query in one language (say German) find answers for that query in textual documents written in another language (say English), and eventually express the found answers in the query language (German). <ref type="foot" coords="1,314.78,450.27,3.97,6.12" target="#foot_0">1</ref> In contrast to a standard cross-language IR system, the NL queries are usually well-formed NL-query clauses (instead of a set of keywords), and the identified answers should be textual fragments representing the answer (instead of complete documents containing the answer). Thus, for a question like "Welches Pseudonym nahm Norma Jean Baker an?" (Which pseudonym did Norma Jean Baker use?) the answer should be "Marilyn Monroe" rather than an English document containing this name.</p><p>At the Language Technology Lab of DFKI we have begun the development of large-scale opendomain cross-language QA systems, currently with a focus on German and English. In <ref type="bibr" coords="1,468.35,535.53,30.45,8.74" target="#b9">[NX03]</ref> we have described a first prototype of a monolingual Web-based QA-system that processes German queries and Web pages (using Google for initial web page retrieval). On basis of this initial prototype we have implemented BiQue a German-English bilingual textual QA-system. BiQue receives a German language query, parses and translates it into English, and searches for answers in a large English text collection maintained by the full-text search engine MG <ref type="bibr" coords="1,439.06,595.31,39.12,8.74" target="#b11">[WMB99]</ref>.</p><p>The main motivation for our participation at this year's CLEF was to foster development of an initial end-to-end cross-language QA-system enforced by external evaluation. Since, we also plan to extend the system for English query and German document analysis (and to support mixed language mode), we have focused on the development of common LT-core components for bilingual query and answering processing that enable us to easily improve the system in the future. Thus we also focused on the development of generic APIs (based on XML) and knowledge formalisms that helps us to systematically improve our system in next development cycles.</p><p>We start with an overview of the whole system, highlight some technical aspects, followed by a more detailed description of the methods we used for query translation and expansion. Finally, we present the results we have obtained for the task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System overview</head><p>The picture in Figure <ref type="figure" coords="2,187.84,457.58,4.98,8.74" target="#fig_0">1</ref> displays the control flow between the major components of BiQue. The major control flow is basically state-of-the-art and -from a coarse-grained point of viewnot novel. However, we think that we have realized a number of interesting "sub-issues" and an interesting "translation approach" (with hopefully fruitful future impacts, at least for us ;-) which is motivation enough to give some more details here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document retrieval</head><p>We are using the MG system -a public-domain full-text retrieval engine, cf. [WMB99] -for the selection of relevant paragraphs. MG is an easy useable software package that can handle text corpora of several Gigabytes very efficiently. In order to make use of the MG system in the context of the QAatCLEF track, we actually had to solve two problems:</p><p>• How can we keep track of the document identifier?</p><p>• How can we use MG for the selection of relevant short text passages?</p><p>The first issue is important because for each answer candidate one has to indicate the document from which the answer was extracted. Secondly, only a small fragment of the documents need to be processed more deeply in order to identify possible answer candidates. In order to fulfill both requirements when using MG, we performed a simple preprocessing of the text corpus: we attached to the front of each paragraph (identified by means of the P SGML tag) of a document a status line which represents the document identifier and the number of the paragraph in the document. Each such extended paragraph is then treated as a single document by MG. Thus, given a set of keywords as input to MG it will return a set of paragraphs where each paragraph encodes its location within the original text document. <ref type="foot" coords="3,327.97,110.45,3.97,6.12" target="#foot_1">2</ref> Here is example of a paragraph returned by MG for the query "leader, india":</p><formula xml:id="formula_0" coords="3,114.91,139.25,169.46,7.47">######## LA110594-0041 10 ######## .</formula><p>The official Indian position has changed a few times but basically has been that all the stones, or the most remarkable among them, should stay in India. India's leaders, however, haven't had the cash to purchase them.</p><p>MG supports document retrieval by either using a boolean query or a ranked query. We decided to use the ranked query because MG should only return paragraphs (see above), and hence a boolean query would be too restrictive already in an early processing phase. Currently, we use the first 100 paragraphs returned by MG, basically because of performance reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Shallow syntactic processing</head><p>NL queries and documents are linguistically analyzed using ShProT, a shallow processing tool that consists of several integrated components: SPPC for tokenization and analysis of compound words (cf. <ref type="bibr" coords="3,141.66,294.79,27.30,8.74" target="#b8">[NP02]</ref>), TnT for part-of-speech tagging (cf. <ref type="bibr" coords="3,351.05,294.79,29.62,8.74" target="#b0">[Bra00]</ref>), Mmorph for morphological analysis (cf. <ref type="bibr" coords="3,149.99,306.74,29.83,8.74" target="#b2">[DG94]</ref>) and Chunkie for phrase recognition (cf. <ref type="bibr" coords="3,372.65,306.74,26.05,8.74" target="#b10">[SB98]</ref>). TnT and Chunkie are statistical based components which derive the linguistic entities, rules and generalizations from annotated corpora. The language models are based on the Penn treebank (for English) and the Negra treebank (for German). ShProT receives as input an ascii text and returns a stream of sentences each consisting of a sequence of tagged phrases and tagged wordforms. The tagged phrases actually define the type of the phrase (either NP or PP) and consists of a sequence of tagged wordforms. A tagged wordform contains the POS, and the lemma as determined by Mmorph. For unknown words (which also includes proper names) TnT tries to guess the POS. In case of a proper name, these are tagged with generic tags like NNP (for singular proper noun). Figure <ref type="figure" coords="3,121.43,414.34,4.98,8.74">2</ref> shows the XML-representation of the shallow analysis of an example sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Named Entity Recognition</head><p>Our Named Entity Recognition (NER) method is based on the unsupervised learning approach of <ref type="bibr" coords="3,101.19,472.57,26.58,8.74" target="#b1">[CS99]</ref>. A decision list of NER-rules (also represented in XML) is applied on the XML-output of ShProT and performs an additional annotation of relevant NPs with corresponding NE-type information (currently, we consider the NE-types person, organization, location, time, and date). Currently, only NPs that contain at least one word recognized by ShProT as a proper noun or time/date expression will be considered as candidates for NE-typing.<ref type="foot" coords="3,403.09,518.82,3.97,6.12" target="#foot_2">3</ref> All these NE candidate phrases are then further processed by the decision list matcher. Each element of the decision list is a simple if-then rule. If a NP-candidate fulfills some spelling or contextual conditions (these are based on generic syntactic criteria of adjacent phrases like "capitol of XXX") then it receives the NE-type as indicated by the rule (e.g., in the example case XXX is typed as location).</p><p>Our current NER-learner is still under development. Usually the decision list is automatically learned. However, for the QAatCLEF track we have not been able to perform a complete training phase because preprocessing of the QA-corpus turned out to be too expensive (it will be one future research issue to explore more efficient learning methods). For that reason a number of rules of the decision list were specified manually. In order to compensate possible (and actual) recall problems we combined the decision list with external Gazetters.</p><p>&lt;SENTENCE id="S4"&gt; &lt;CHUNK id="H26" cat="NP"&gt; &lt;W id="W99" PoS="DT" tclass="25" mclass="24" stems="[a]"&gt; &lt;WORDFORM string="An" /&gt; &lt;READINGS&gt; &lt;R id="R0" subtype="art_indef" category="Det" /&gt; &lt;/READINGS&gt; &lt;/W&gt; &lt;W id="W100" PoS="NNP" tclass="22" mclass="-1" stems="[]"&gt; &lt;WORDFORM string="FBI" /&gt; &lt;READINGS /&gt; &lt;/W&gt; &lt;W id="W101" PoS="NN" tclass="24" mclass="29" stems="[informant]"&gt; &lt;WORDFORM string="informant" /&gt; &lt;READINGS&gt; &lt;R id="R0" subtype="char" category="Abbr" /&gt; &lt;/READINGS&gt; &lt;/W&gt; &lt;/CHUNK&gt; &lt;W id="W102" PoS="VBN" tclass="24" mclass="205" stems="[claim]"&gt; &lt;WORDFORM string="claimed" /&gt; &lt;READINGS&gt; &lt;R id="R0" subtype="main" tense="past" verbclass="intrans" category="Verb" vform="psp" /&gt; &lt;/READINGS&gt; &lt;/W&gt; &lt;W id="W103" PoS="IN" tclass="24" mclass="-1" stems="[that]"&gt; &lt;WORDFORM string="that" /&gt; &lt;READINGS /&gt; &lt;/W&gt; &lt;W id="W104" PoS="NNS" tclass="25" mclass="-1" stems="[]"&gt; &lt;WORDFORM string="Wilkins" /&gt; &lt;READINGS /&gt; &lt;/W&gt; &lt;W id="W105" PoS="VBD" tclass="24" mclass="-1" stems="[be]"&gt; &lt;WORDFORM string="was" /&gt; &lt;READINGS /&gt; &lt;/W&gt; &lt;CHUNK id="H27" cat="NP"&gt; &lt;W id="W106" PoS="DT" tclass="24" mclass="399" stems="[the]"&gt; &lt;WORDFORM string="the" /&gt; &lt;READINGS&gt; &lt;R id="R0" wh="no" subtype="gen" number="plural" category="Det" /&gt; &lt;/READINGS&gt; &lt;/W&gt; &lt;W id="W107" PoS="NN" tclass="24" mclass="1" stems="[trig, German]"&gt; &lt;WORDFORM string="triggerman" /&gt; &lt;READINGS&gt; &lt;R id="R0" gender="neutrum" number="singular" category="Noun" /&gt; &lt;/READINGS&gt; &lt;/W&gt; &lt;/CHUNK&gt; &lt;W id="W108" PoS="$." tclass="1" mclass="-1" stems="[$PUNCTUATION]"&gt; &lt;WORDFORM string="." /&gt; &lt;READINGS /&gt; &lt;/W&gt; &lt;/SENTENCE&gt; Figure 2: The XML-representation of the shallow syntactic analysis for the sentence An FBI informant claimed that Wilkins was the triggerman .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Internal query and document representation</head><p>Internally, queries and documents are uniformly represented as weighted sets of structured (possibly linked) objects in order to facilitate a robust and efficient comparison between queries and answer candidates. More formally, we call the set B := {O 1 , . . . , O n ; α} a Bag-of-Objects or short BoO consisting of n objects O i and weight α. Each object O i is a tuple of the form W F, Stem, P oS, N E, α i , i.e., a structured object consisting of a word form, a lemma, part-ofspeech, named entity and weight α i (note that for all elements but W F and α i the actual value can be empty).</p><p>The weight of a BoO is determined during the matching phase of the query with a candidate answer sentence. The actual approach we are exploiting for comparing and merging two different BoOs is a variant of the word overlap method described in <ref type="bibr" coords="5,351.18,238.01,42.69,8.74" target="#b6">[LMRB01]</ref>. A word overlap (which is also a BoO in our case) is the subset of objects a query and an answer candidate have in common, i.e., the word overlap of two sentences s 1 and s 2 is Ov s1,s2 := B s1 ∩ B s2 , where B i is the BoO of s i . The weight β of a word overlap Ov is determined as the sum over the weights α i of the overlapping words. <ref type="foot" coords="5,171.40,284.25,3.97,6.12" target="#foot_3">4</ref> After Ov s1,s2 has been computed, the B i obtain β as their weight, i.e., BoO with same word overlap have equal weight (however, this weight will later be updated using the expected answer type, see below).</p><p>We also define the overlap set Os q of a query q as the set of all BoOs of all candidate answer sentences which have the same word overlap with q, i.e., Os q := {B s1 , . . . , B sn }, with: Ov q,si = Ov q,sj for i = j. This means that the overlap sets define equivalence classes over the set of possible answer candidates wrt. the set of objects each answer has in common with the query, i.e., query and sentences with same word overlap (and hence, with equal weight, but see 2.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Query processing</head><p>The main tasks of the query processor are the 1. parsing of a German (or English) NL query, and the 2. translation and expansion of the German query object to an English one.</p><p>A query object is a tuple EAT, BoO, Keys, L consisting of the expected answer type EAT, the BoO representation of the question, the set of relevant keywords used as query for the full-text retrieval engine MG (see 2.1), and the language identifier. We will now describe very briefly the different steps, and will only say a bit more on parsing in the subsection that follows.</p><p>The main goal of parsing a NL query in the context of open-domain QA is the identification of the question focus and the expected type (or concept) of the potential answer phrase (Expected Answer Type (EAT), cf. [HMP + 00]). The question focus is a phrase or word in the question that can help to disambiguate it and -together with the question stem (e.g., who, how much, where) -can help to deduce the EAT.</p><p>After the parser has determined the EAT for the current question, a BoO representation is constructed on basis of all content words of the question. (In principle, it is also possible to link the elements of the BoO based on the derivation tree computed during parsing (which corresponds roughly to a dependency tree, see next paragraph). However, we have not been able to finish implementation of this further step in due time which would have helped us to define more clever strategies for the identification of exact answers, see below 2.6.) So we currently have to live with a quite flat internal representation of the query. The set of relevant keywords is determined very simply from the BoO by collecting all stems of the content words (or word forms if no such stem could have been computed).</p><p>Finally, query translation and expansion takes place in order to perform retrieval of English paragraphs and to allow for computation of overlap sets on basis of word overlap between the query and answer candidates. A description of details of this third step during the analysis of a question is postponed until section 3. The only thing worth to mention here, is that the German query BoO is basically translated to an English one (by keeping the EAT determined for the German query). Translation is basically realized by means of "merging" results from EuroWordNet with the results of externally available translation services which we are using as a means for performing word sense disambiguation. Query expansion is performed simultaneously with query translation.</p><p>Query parsing Before going on in describing how answer processing is performed on basis of the translated query object, we describe some details of our parsing methods.</p><p>In our current system, we have specified manually a German and a English query grammar in form of lexicalized tree substitution grammars (LTSG). A query LTSG consists of set of syntax/semantics oriented tree patterns which express mutual constraints for the identification of a question focus and an EAT. Here is an example of such an elementary tree: &lt;tree id="6a" label="F-Wo" eat="LOCATION" freq="" prob=""&gt; &lt;node label="PWAV"&gt; &lt;node label="wo" type="TERM" anchor="YES"/&gt; &lt;/node&gt; &lt;node label="VVFIN"&gt; &lt;node label="schliessen" type="TERM" anchor="YES"/&gt; &lt;/node&gt; &lt;node label="NE" nclass="PERSON" type="SUBST"/&gt; &lt;node label="NP" type="SUBST"/&gt; &lt;node label="PTKVZ"&gt; &lt;node label="ab" type="TERM"/&gt; &lt;/node&gt; &lt;/tree&gt; which would be applicable for a question like Wo schloss Hillary Clinton das College ab? (Where did Hillary Clinton graduate college?). A query grammar is applied on top of the shallow chunk analysis computed by first applying ShProT on the NL question. Note that nodes of type term are lexical anchors and nodes of type subst have to be expanded by substituting the node with a consistent (complete) phrase. Parsing of a query LTSG is performed along the line of the method described in <ref type="bibr" coords="6,181.96,466.75,31.24,8.74" target="#b7">[Neu03]</ref>. <ref type="foot" coords="6,217.66,465.18,3.97,6.12" target="#foot_4">5</ref>In some sense, the elementary trees of a LTSG define clause-level patterns using lexical information about the question type and focus to constraint their applicability. Linguistically, an elementary tree of a LTSG also describes a head-modifier relationship between the lexical anchors and the modifiers (basically the substitution nodes). Hence a derivation of a query analysis can also be used to uncover the dependency structure. <ref type="foot" coords="6,315.30,524.95,3.97,6.12" target="#foot_5">6</ref> The current grammars (together with the possible supported EAT) have been defined on the basis of a manually translation of the QA-Trec 8 and 9 question corpus. Actually, it turned out that the current grammars have been defined a bit too Trec-8/9 specific concerning supported subcategorization. Hence, future work will focus on improving generalization without loosing the benefits of lexicalized tree structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Answer processing</head><p>Paragraph selection The keywords of the translated query object are used to build a query expression for the MG system. Currently, we use the whole set of keywords (including the expanded terms) to form one MG-query. The ranked query mode of MG is then used to retrieve the N best paragraphs (see also 2.1). In the ranked query mode, MG actually ranks all documents according to some similarity measure applied on each document which specifies how close the document matches with the query. Thus seen, MG returns the N most relevant documents with respect to the query.</p><p>Candidate answer selection All retrieved paragraphs are analysed by ShProT (see 2.2) which maps a paragraph into a sequence of sentence objects. A sentence object consists of the shallow syntactic XML-structure, the sentence BoO (constructed from it) and additional bookkeeping information (e.g., pointer to document identifier).</p><p>All sentence objects of every paragraph are collected into one container from which the overlap sets are constructed along the line described in 2.4. This means that a word overlap Ov q,si is computed by merging the BoO of the query q with the BoO of every sentence object s i , which is then used to construct and rank the overlap sets. In a next step, all sentence objects from the top five equivalence classes are collected into one list of answer candidate sentences. For each such sentence object it is then check, whether it contains one element which is type-compatible with the expected answer type EAT of the query object. If so, the weight of the sentence is increased. Note that this means that a sentence whose corresponding word overlap weight is smaller than that of another sentence (which means that it is less similar wrt. to the query) might now receive a higher rank.</p><p>In a final step, each sentence is searched for an NP phrase which can serve as the exact answer of the question. The method that we have exploited so far, actually constructs a ranked list of all NPs (extracted from every sentences) that do not contain any element from the sentence's word overlap. Ranking is performed by taking into account the type of the NP (e.g., EAT-compatible, containing other NEs), and the number and distance of elements from the sentence's word overlap wrt. the NP. By doing so we determine exact answer and 50bytes answer strings. Note that the underlying assumption made by our current method is that the strings of NPs serve as exact answers. Generally, this view is surely too restricted (and might only apply for certain kind of questions), and hence will be improved in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Query translation and expansion</head><p>In this section we are going to describe in more detail how question translation and expansion is performed.</p><p>Background Traditional approaches for cross-language information management systems can be classified as follows:</p><p>1. systems that translate the queries into the target language, or 2. the document collection into the source language, or both,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">queries and documents into an intermediate representation (inter-lingua).</head><p>Two types of translation services are well known within this context which are based on • lexical resources (e.g., dictionaries, aligned wordnets), or</p><p>• machine translation (e.g., example-based translation).</p><p>Each translation method has to deal with the following issues: word sense disambiguation (WSD) and coverage. WSD accounts for translating the appropriate meaning of a word, as suggested by its context, while coverage guarantees that source language words have a chance to be translated, to the extent to which it is intended (e.g., not all named entities should be translated). In retrieving the documents related to a formulated query, it is often useful to take into consideration words related to the query words. This query expansion method can be achieved either through syntactic or semantic variations. A query of the form "presidential election" could be extended with "election of the president", "the president was elected", "presidential vote", "presidency vote"", etc. An issue in query expansion is the word sense disambiguation, too. As query words may be ambiguous, only the intended meanings of them should be targets of the expansion task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>The system BiQue as used in this competition translates the German language question to the English language of the document collection by means of machine translation techniques. The system accounts for the above-mentioned coverage issue by using three different translation services: FreeTranslation, Altavista and Logos. The results of translating the original German question are used in generation of bag-of-object (BoO) collections of English open-class words, which are further on target of the query expansion module. Expansion is being achieved only through semantic variations using WordNet-like resources, whereby a pseudo word-sensedisambiguation task using the German original question and its English translations is being applied. Following we will describe the functioning of the question translation and expansion module by means of the example question:</p><p>Wo wurde das Militärflugzeug Strike Eagles 1990 eingesetzt? Question Translation Three different translation services have been considered for this purpose:</p><p>• FreeTranslation (via http://www.freetranslation.com/) yields:</p><p>"Where did the military airplane become would strike used Eagles 1990?"</p><p>• Altavista (via http://babel.altavista.com/) yields: "Where was the military aircraft Strike Eagle used 1990?"</p><p>• Logos (off-line) yields: "Where was the soldier airplane Strike Eagles installed in 1990?"</p><p>Initial experiments using only one translation service unveiled the limitation imposed by the coverage problem: inadequate or no translations (e.g., some name of countries that were different in German and English). Extending the translation module with two further services, the results improved and pointed out the advantage of indirectly using it for question expansion as well, as different translations can generate synonym words. Moreover, the original German question and its English translations were used for question expansion too, as building blocks for our pseudo-WSD module.</p><p>Given the above-listed translations, a BoO collection of open-class normalized words has been created, with the following content (for convenience we abbreviate the object elements by means of their lexemes): {soldier, airplane, strike, eagle, install, 1990, military, become, strike, use, aircraft, Eagle} This BoO is obtained as follows: from each English version of the question a corresponding BoO is constructed by applying ShProT and the English question grammar. The resulting different BoO's are then merged into one BoO which represents the translated query object (re-using the expected answer type EAT as computed for the German question analysis).</p><p>Question Expansion For the expansion task we have used the German and English wordnets aligned within the EuroWordNet lexical resource. Our goal was to extend the English BoO collection with synonyms for the words that are present in the wordnet.</p><p>Considering the ambiguity of words, a WSD module was required as part of the expansion task. For this purpose we have used both the original question and its translations, leveraging the reduction in ambiguity gained through translation. Our devised pseudo-WSD algorithm works as following:</p><p>1. look up every word from the translated BoO collection (see example above) in the lexical resource;</p><p>2. if the word is not ambiguous (which is, for example, the case for airplane, aircraft) then extend the BoO collection with its synonyms, e. Following the question expansion task, the BoO collection has been enriched with new words that are synonyms of the un-ambiguous English words and by synonyms of those ambiguous words, whose meaning(s) have been found in the original German question. Thus our expanded example looks as follows:</p><p>{soldier, airplane, strike, eagle, install, 1990, military, become, strike, use, aircraft, Eagle, aeroplane, plane, apply, employ, make use of, put to use, use, utilise, utilize}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and conclusion</head><p>We have participated for the first time in a QA track, and hence had to build BiQue from scratch, so the focus was on system implementation, rather than on system tuning (and actually we had no time to test different settings of critical system parameters, like the weighting values). We submitted only one run for the 50byte run, and obtained as result for the strict statistics 14.5% correct answers, and 15% for the lenient statistics.<ref type="foot" coords="9,307.83,599.58,3.97,6.12" target="#foot_6">7</ref> This is surely a result that should and can be improved.<ref type="foot" coords="9,132.92,611.54,3.97,6.12" target="#foot_7">8</ref> Besides evaluation of the performance of the system wrt. different parametrization, important next steps for system improvement are, among others, the unsupervised online learning of more fine-grained NE rules, Machine Learning of query grammars, methods for determining the utility of answer candidates, development of ontology based answer validation methods, and more controlled query expansion by using fine-grained ontologies (following [HGH + 00]).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,199.48,403.94,204.04,9.30;2,121.50,108.86,360.00,270.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A blueprint of BiQue's architecture.</figDesc><graphic coords="2,121.50,108.86,360.00,270.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,102.18,123.98,410.83,280.28"><head></head><label></label><figDesc>for every possible reading of it, get its aligned German correspondent reading (if it exists) and look up that reading in the German original question (i.e., in the BoO representation of the original German question "Wo wurde das Militärflugzeug Strike Eagles 1990 eingesetzt?"), e.g., Reading-697925: EN: handle, use, wield , DE: handhaben, hantieren Reading-1453934: EN: behave toward, use , DE: not aligned Reading-661760: EN: be a user of, use, use regularly , DE: not aligned Reading-658041: EN: expend, use , DE: aufwenden Reading-658243: EN: apply, employ, make use of, put to use, use, utilise, utilize , DE: anbringen, anwenden, bedienen, benutzen, einsetzen, . . . (b) if an aligned reading is found (e.g., Reading-658243) retain it and add the English synonyms of it to the BoO collection, i.e., expand it with: apply, employ, make use of, put to use, use, utilise, utilize</figDesc><table coords="9,102.18,123.98,313.18,72.50"><row><cell></cell><cell>g.,</cell></row><row><cell cols="2">airplane =⇒ aeroplane, plane</cell></row><row><cell>aircraft =⇒</cell><cell>(i.e., in case of aircraft there are no synonyms);</cell></row><row><cell cols="2">3. if the word is ambiguous (e.g., use) then</cell></row><row><cell>(a)</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,731.85,407.75,6.99;1,90.00,741.31,137.24,6.99"><p>The translation of answers into the query language is currently not part of the QAatCLEF track, hence we will say nothing about this problem here.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.24,656.94,407.75,6.99;3,90.00,666.40,423.00,6.99;3,90.00,675.86,80.31,6.99"><p>By way: since the status line is part of a paragraph it can also be specified as part of the query. Hence, we also can use the status line information for reconstructing the whole document as well as for performing corpus navigation using MG.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,105.24,685.37,407.75,6.99;3,90.00,694.83,423.00,6.99;3,90.00,704.30,423.00,6.99;3,90.00,713.76,66.49,6.99"><p>Note that this means that we perform NER after shallow parsing. Hence the accuracy of the NER depends on the accuracy of shallow parsing. In some sense the approach can also be viewed as a top-down classification approach, since NER-module actually performs a sub-typing of those generic NE types already recognized by the shallow processor.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,105.24,743.62,398.80,6.99"><p>The weight of an individual object is currently specified a priori and is based on the word's part-of-speech.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,105.24,709.11,407.75,6.99;6,90.00,718.57,423.00,6.99;6,90.00,728.04,140.03,6.99"><p>One of the reasons why we have chosen an LTSG approach is our future goal, to automatically extract a linguistically expressive but specific query subgrammar form a large-scale general HPSG-source grammar following the approach described in that paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,105.24,736.09,407.75,8.44;6,90.00,747.00,294.19,6.99"><p>Following the approach of [HMP + 00], it would then be possible to construct a quasi logical form from the dependency relation in order to support theorem proving for answer validation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="9,105.24,677.93,407.75,6.99;9,90.00,687.39,423.00,6.99;9,90.00,696.86,423.00,6.99;9,90.00,706.32,81.06,6.99"><p>We had also planned to submit a second run by first preprocessing the whole corpus with ShProT, but it turned out that this was too time consuming. The major motivation was, that we wanted to perform a stemming of the complete corpus by using ShProT instead of the build in stemmer of MG which turned out to cause too much trouble in some cases.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="9,105.24,715.82,407.75,6.99;9,90.00,725.29,290.89,6.99"><p>For example we were not able to process questions containing quoted terms, because we simply have not foreseen such questions. However, 20% of the test set contained such kind of questions.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,140.48,133.84,372.52,8.74;10,140.48,145.80,256.48,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,220.77,133.84,173.72,8.74">Tnt -a statistical part-of-speech tagger</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,417.05,133.84,95.95,8.74;10,140.48,145.80,163.58,8.74">Proceedings of the 6th Applied NLP Conference, ANLP-2000</title>
		<meeting>the 6th Applied NLP Conference, ANLP-2000<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,140.48,165.72,372.52,8.74;10,140.48,177.68,372.52,8.74;10,140.48,189.63,372.53,8.74;10,140.48,201.59,75.53,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,299.00,165.72,214.00,8.74;10,140.48,177.68,15.94,8.74">Unsupervised models for named entity classification</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,195.61,177.68,317.38,8.74;10,140.48,189.63,227.90,8.74">Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora</title>
		<meeting>the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,140.48,221.51,372.52,8.74;10,140.48,233.47,188.67,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,272.22,221.51,190.50,8.74">Mmorph -the multext morphology program</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Petitpierre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>ISSCO, University of Geneva</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="10,96.38,253.39,19.14,8.74" xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j" coord="10,96.38,253.39,19.14,8.74">HGH</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,140.47,253.39,372.53,8.74;10,140.48,265.35,372.52,8.74;10,140.48,277.30,73.87,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,488.03,253.39,24.97,8.74;10,140.48,265.35,130.12,8.74">Question answering in Webclopedia</title>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurie</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Junk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,290.95,265.35,222.04,8.74;10,140.48,277.30,42.54,8.74">Proceedings of the Ninth Text REtrieval Conference (TREC-9)</title>
		<meeting>the Ninth Text REtrieval Conference (TREC-9)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,140.48,297.23,372.53,8.74;10,140.48,306.82,372.53,11.10;10,140.48,321.14,372.52,8.74;10,140.48,333.09,73.87,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,427.33,309.18,85.67,8.74;10,140.48,321.14,126.49,8.74">FALCON: Boosting knowledge for answer engines</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Paşca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rȃzvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roxana</forename><forename type="middle">G</forename><surname>Îrju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Morȃrescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,288.12,321.14,224.88,8.74;10,140.48,333.09,42.54,8.74">Proceedings of the Ninth Text REtrieval Conference (TREC-9)</title>
		<meeting>the Ninth Text REtrieval Conference (TREC-9)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,140.48,353.02,372.52,8.74;10,140.48,364.97,357.93,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,408.10,353.02,104.90,8.74;10,140.48,364.97,164.61,8.74">Analysis for elucidating current question answering technology</title>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gideon</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Rilo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,313.90,364.97,130.16,8.74">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,140.48,384.90,372.52,8.74;10,140.48,396.85,372.53,8.74;10,140.48,408.81,260.14,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,223.57,384.90,284.16,8.74">Data-driven approaches to head-driven phrase structure grammar</title>
		<author>
			<persName coords=""><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,393.43,396.85,119.58,8.74;10,140.48,408.81,16.29,8.74">DATA-ORIENTED PARS-ING</title>
		<editor>
			<persName><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Remko</forename><surname>Scha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Khalil</forename><surname>Sima'an</surname></persName>
		</editor>
		<imprint>
			<publisher>University of Chicago Press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,140.48,428.74,372.52,8.74;10,140.48,440.69,181.27,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,316.23,428.74,153.25,8.74">Shallow text processing core engine</title>
		<author>
			<persName coords=""><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakub</forename><surname>Piskorski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,478.63,428.74,34.37,8.74;10,140.48,440.69,84.83,8.74">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="476" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,140.48,460.62,372.52,8.74;10,140.48,472.57,372.53,8.74;10,140.48,484.53,61.19,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,280.08,460.62,164.64,8.74">Mining Answers in German Web Pages</title>
		<author>
			<persName coords=""><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Feiyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,463.34,460.62,49.66,8.74;10,140.48,472.57,287.99,8.74">Proceedings of The International Conference on Web Intelligence (WI 2003)</title>
		<meeting>The International Conference on Web Intelligence (WI 2003)<address><addrLine>Halifax, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10">October 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,140.47,504.45,372.53,8.74;10,140.48,516.41,372.52,8.74;10,140.48,528.36,22.70,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,312.65,504.45,200.35,8.74;10,140.48,516.41,53.22,8.74">A maximum entropy partial parser for unrestricted text</title>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Skut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,218.56,516.41,166.65,8.74">6th Workshop on Very Large Corpora</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-08">August 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,140.47,548.29,372.53,8.74;10,140.48,560.24,372.52,8.74;10,140.48,572.20,90.16,8.74" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,390.25,548.29,122.74,8.74;10,140.48,560.24,202.79,8.74">Managing Gigabytes: Compressing and Indexing Documents and Images</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
