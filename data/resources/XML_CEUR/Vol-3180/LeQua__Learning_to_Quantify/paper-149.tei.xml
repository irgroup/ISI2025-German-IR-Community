<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,381.49,15.42;1,89.29,106.66,270.63,15.42">UniOviedo(Team1) at LeQua 2022: Sample-based quantification using deep learning</title>
				<funder ref="#_3aTQjzj">
					<orgName type="full">FEDER (Fondo Europeo de Desarrollo Regional)</orgName>
				</funder>
				<funder ref="#_ePpEQHW">
					<orgName type="full">MINECO</orgName>
				</funder>
				<funder>
					<orgName type="full">MINECO (Ministerio de EconomÃ­a y Competitividad)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,134.97,74.79,11.96"><forename type="first">Pablo</forename><surname>GonzÃ¡lez</surname></persName>
							<email>gonzalezgpablo@uniovi.es</email>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence Center at GijÃ³n</orgName>
								<orgName type="institution">University of Oviedo</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,381.49,15.42;1,89.29,106.66,270.63,15.42">UniOviedo(Team1) at LeQua 2022: Sample-based quantification using deep learning</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">DD979B0DF3FFFA46BBC3CF3049546AC6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>quantification</term>
					<term>prevalence estimation</term>
					<term>deep neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks (DNNs) have become very popular in recent years, being applied to a wide variety of problems. In the field of quantification, DNNs also show a promising future as an alternative to conventional quantification methods with some properties that can be useful in certain problems. In this paper we propose a deep learning architecture for quantification problems based on differentiable histograms in order to obtain invariant sample representations. This approach has obtained competitive results in each of the four subtasks of the LeQua 2022 competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There are real-world applications where predicting the class of each individual example in a dataset has no real utility, and the goal is to be able to estimate the prevalences of the classes in a data sample (i.e. set of examples). Quantification is a supervised machine learning method that tackles this particular problem and has become a task on its own in recent years <ref type="bibr" coords="1,469.12,398.31,11.43,10.91" target="#b0">[1]</ref>.</p><p>Since the recognition of the quantification problem as a self-standing problem and different from classification, several methods to solve it have been devised <ref type="bibr" coords="1,377.83,425.41,11.33,10.91" target="#b1">[2]</ref>. The LeQua 2022 competition <ref type="bibr" coords="1,109.20,438.96,12.68,10.91" target="#b2">[3]</ref> was launched with the aim of taking a step further the quantification field and stimulate the creation of new quantification methods. In this competition, the standard quantification methods are to be considered as baselines and the task of the competitors is to obtain solutions to improve those.</p><p>The approach proposed by this team is a DNN architecture designed to learn from samples (i.e. sets of unlabelled examples), that can optimize a specific error measure. The four subtasks of the competition: T1A, T1B, T2A and T2B were tackled using basically the same setup that will be described in the following section. This solution has led to pretty consistent results in the four tasks, achieving, two silver medals, one bronze medal and one gold medal in the four subtasks respectively. The solution also obtains state-of-the-art results compared with the standard baselines. The method used for this competition is a DNN designed for tackling quantification problems. The architecture (see Figure <ref type="figure" coords="2,218.18,339.46,4.23,10.91" target="#fig_0">1</ref>) is based on obtaining the representation of the samples using a differentiable histogram layer. This representation is invariant to the order of the examples in the sample. The network consists of a feature extraction layer, that is dependent on the subtask; a histogram layer, that computes a histogram per input feature creating a numeric vector representation of each sample; and a quantification module formed by dense linear layers, able to learn the relationships existent between sample histograms and the sample prevalences. Lastly, one softmax activation function translates the outputs of the last fully connected layer (with size equal to the number of classes and specific to the task) into prevalences adding up to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>The principal advantages of this architecture versus the other approaches are threefold:</p><p>â€¢ The network can optimize any loss function. In this case, the competition rules established the relative absolute error (RAE) as the official loss function for the competition, making other possible quantification loss functions <ref type="bibr" coords="2,308.67,509.70,12.75,10.91" target="#b3">[4]</ref> not important for this particular problem. â€¢ There is no need for labelled examples to train the network. Samples and their associated prevalences are sufficient to train the network. â€¢ The ability to use any existing network architecture in the feature extraction layer (for instance, transformers for natural language processing problems).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature extraction layer</head><p>The feature extraction layer depends on the subtask. For tasks T1A and T1B, in which vector features were already computed for each document, fully connected layers were used. The number of layers and their size were considered as hyperparameters dependent on the subtask. For subtasks T2A and T2B, raw text documents were the input of the network. In this case, the decision taken was to plug-in BERT <ref type="bibr" coords="2,267.47,669.58,11.42,10.91" target="#b4">[5]</ref>, a model pretrained on English language, from the transformers library <ref type="bibr" coords="3,183.65,86.97,11.55,10.91" target="#b5">[6]</ref>. The input of this network is tokenized text (default BERT tokenizer was used, with a sequence length limit of 200 tokens), and the network outputs a vector of 768 features per each example that is used to compute the histograms that represent the sample. Note that using BERT in a network architecture like this is not exempt from problems, as all the examples in a sample must pass through the network before doing a backward pass. As BERT has 110M parameters this operation is not feasible from a memory usage point of view.</p><p>The solution adopted was to use a technique called gradient checkpointing <ref type="bibr" coords="3,419.33,168.26,12.80,10.91" target="#b6">[7]</ref> that reduces the memory footprint at the cost of having a 30% increase in computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Differentiable histograms</head><p>One of the problems to overcome is that histograms are not differentiable thus, they can not be used directly in a neural network. The solution is to use an approximation that can be computed with basic and differentiable functions as the sigmoid function, provided by all deep learning frameworks.</p><p>For computing a histogram of a feature ğ‘“ ğ‘˜ for each example ğ‘–, first, the fixed bin centres ğœ‡ ğ‘ are computed as well as the values ğ‘“ ğ‘˜,ğ‘– distance to each bin centre. Then, in a second step, two logistic functions are used to approximate which values fall in each bin (see Figure <ref type="figure" coords="3,459.56,312.83,3.57,10.91">2</ref>). </p><formula xml:id="formula_0" coords="3,210.23,337.36,175.32,26.93">1 ğœ(ğ‘“ ğ‘˜,ğ‘– -ğ‘¤ 2 ) 1 -ğœ(ğ‘“ ğ‘˜,ğ‘– -ğ‘¤ 2 )</formula><p>Figure <ref type="figure" coords="3,119.94,491.00,3.74,8.93">2</ref>: Histogram bin approximation using two sigmoid functions ğœ(ğ‘“ ğ‘˜,ğ‘– ) = 1 1+ğ‘’ ğ›¾ğ‘“ ğ‘˜,ğ‘– . In this example, the bin center is fixed and equal to ğœ‡ = 0.75. Bin width is also fixed with ğ‘¤ = 0.5. ğ›¾ is a constant with a high enough value to make sigmoid functions sharp and closer to a step function.</p><p>This method for computing histograms is fast, differentiable (so it can be used with backpropagation), and given a big enough ğ›¾ value, provides a good-enough approximation to the real histograms. An example of a approximated histogram for a single feature using sigmoid functions can be seen in Figure <ref type="figure" coords="3,239.90,587.14,3.81,10.91" target="#fig_2">3</ref>. In this example, the error made by the approximation compared with the true one is close to zero (absolute error: 0.000123).</p><p>The number of bins of the histograms is parameter that can be optimized for each problem. For this competition the optimal values were always close to 32, so for simplicity, this is the value that was used for all four tasks (more information in Section 3.1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Sample generation</head><p>For the training process, the network needs training samples, so they can be fed forward through the layers, compute the loss and make a backward pass to learn the network weights. For each of the four subtasks, participants were given a labelled dataset to train the quantification algorithms plus a set of dev samples associated with their prevalences, but without individual example labels, intended to be used as validation samples. As example labels are not needed at all in this DNN architecture, dev samples were used as the base of the training process. From the 1000 thousand dev samples given for each task, 500 were used for training, 200 for validation and early stopping, and the remaining 300 were used for testing, to make sure that the solution was not overfitting. In the subtasks where labelled training data was used, the APP protocol was used to generate random samples.</p><p>Even though the number of samples may seem high, 500 instances for training the network is not enough and would cause overfitting. The solution adopted was to create a bag generator that can generate artificial samples that are a mixture of two real samples, the prevalence of which is aproximated to the average of the prevalences of the two real samples. To also feed the network with real samples, the sample generator was parameterized to control the generated proportion of real samples versus mix samples. This parameter was considered a hyperparameter to optimize for each subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In order to train the DNN, the bag generator described in the previous section was used to feed the network with samples. After finishing one epoch with a fixed number of samples, validation loss was computed using the 200 samples reserved for validation. For validation no mix samples were generated, only real ones.</p><p>The loss function for the network was Relative absolute error (RAE) is defined as,</p><formula xml:id="formula_1" coords="5,229.41,109.31,276.58,30.37">ğ‘…ğ´ğ¸(ğ‘, ğ‘ ^) = 1 |ğ¶| âˆ‘ï¸ ğ‘âˆˆğ¶ |ğ‘ ^-ğ‘| ğ‘ ,<label>(1)</label></formula><p>where ğ‘ and ğ‘ ^are the real and predicted prevalences and ğ¶ are the classes of the problem. RAE may be undefined because of undefined denominator so we can take the smoothed version ğ‘ ğ‘  of both ğ‘ and ğ‘ ^ <ref type="bibr" coords="5,156.03,178.25,14.17,10.91" target="#b3">[4]</ref>:</p><formula xml:id="formula_2" coords="5,248.89,187.73,257.10,27.82">ğ‘ ğ‘  = ğœ– + ğ‘ ğœ–|ğ¶| + âˆ‘ï¸€ ğ‘âˆˆğ¶ ğ‘ ,<label>(2)</label></formula><p>where ğœ– = 1 2*ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ ğ‘–ğ‘§ğ‘’ . To determine the number of training epochs, an early stopping criterion was established, taking the model with the best validation loss and stopping learning after a number of epochs without any improvement in the validation loss. The learning rate was considered a hyperparameter and was reduced by a factor of 0.5 after a certain number of epochs without improvement in the validation loss. Dropout and weight decay were used to prevent overfitting which was indeed a problem in the training process of the DNN.</p><p>After completing the training process, the 300 test samples (not seen during training or validation) were evaluated in order to get a realistic estimation of the performance of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hyperparameter search</head><p>Exploring the different architectures and hyperparameters in a deep learning problem can be a daunting experience. For tasks T1A and T1B, the Optuna hyperparameter optimization framework was used <ref type="bibr" coords="5,185.84,422.00,11.55,10.91" target="#b7">[8]</ref>. For tasks T2A and T2B the BERT module slowed down the training process to a degree that made impossible an automated hyperparameter search due to the lack of computational resources (check Table <ref type="table" coords="5,275.94,449.09,5.17,10.91" target="#tab_0">1</ref> for a list of the most important hyperparameters considered and their values for each task). Table <ref type="table" coords="5,299.67,462.64,4.97,10.91" target="#tab_1">2</ref> illustrates the optimization process conducted for task T1A using Optuna. For each hyperparameter, the search space considered and the best value found are presented. Note that an exhaustive search would not be plausible due to the high number of hyperparameters and possible values. As an alternative, Optuna uses a sampler to pick hyperparameter values using TPE (Tree-structured Parzen Estimator) algorithm <ref type="bibr" coords="5,481.44,516.84,12.85,10.91" target="#b8">[9]</ref> to explore the search space and find the best values that optimize the objetive function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training process differences by subtask</head><p>Even though every subtask was tackled with a similar training process, it is worth noting the subtle differences between them. In subtask T1A, only dev samples were used in the training process. Best hyperparameters were found using Optuna and the final model was trained with those. The approach for T1B was similar to T1A but training the network was more difficult. Due to the high number of classes, the network tended to overfit. In this case, using the samples generated with the labelled train data in a second stage helped reduce overfitting to a certain degree. For task T2A the training process was divided into two stages. In the first one, BERT weights were frozen, allowing the network to update the weights only for the quantification layers. In a second stage, all the weights were unfrozen and the network was allowed to converge freely. For subtask T2B, BERT was finetuned for a few epochs to the 28 classes of the problem using the training data. Then the process was the same as for T2A. This additional step helped the network to converge faster as the feature extraction layer is already optimized to distinguish between the 28 classes of the problem. In Table <ref type="table" coords="6,139.05,546.27,3.75,10.91">3</ref>, training and inference times are shown for each subtask. Computation for tasks T1A and T1B was done in a NVidia Titan Xp card, with 12Gb of RAM. For tasks T2A and T2B a NVidia GeForce RTX 3080 with 24Gb of RAM was used. Note that T2A and T2B training and inference times are hugely conditioned by the feature extracture layer (BERT). Difference in training times between T1A and T1B is due to the higher number of epochs needed in T1B for the network to converge and the bigger sample size (250 for T1A vs 1000 examples for T1B). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Training/inference times for each of the subtasks. Inference time represent the time required to evaluate the 5000 test samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head><p>After selecting the best model for each subtask, based on the MRAE (mean RAE across samples), the solution was submitted to the competition website. The following table shows the results for each subtask in MRAE and MAE (mean absolute error, defined as the mean by sample of ğ‘ğ‘ğ‘ (ğ‘ -ğ‘ ^)), compared with the best baseline method, which was consistently the EM method <ref type="bibr" coords="7,89.29,294.95,16.35,10.91" target="#b9">[10]</ref>. Results for the rest of the competitors and the other baselines methods can be found on the website of the competition 1 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>The results obtained in the final test data, released after the end of the competition, improve MRAE for the existing baselines in three out of four subtasks. For subtasks T2A and T2B the validation result is better than the final result. This may have been caused by overfitting to validation data. Note that these subtasks were solved using a BERT transformer which is a huge model with millions of parameters. Maybe the results using a slightly simpler method as a feature extraction layer could have led to better results.</p><p>1 https://codalab.lisn.upsaclay.fr/competitions/4134#results</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,272.73,416.69,8.96;2,89.29,284.74,416.70,8.87;2,89.29,296.69,48.03,8.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: DNN architecture. For tasks T2A and T2B the feature extraction module uses a BERT transformer pretrained. For T1A and T1B this layer is removed. Dense layers number and size depend on the task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,242.87,416.70,8.93;4,89.29,254.87,385.87,8.87;4,89.29,84.19,416.71,152.09"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: True histogram (left) and sigmoid differentiable histogram (right) with 8 bins for a feature normally distributed with 100 values. Difference between the two is 0.000123 in absolute error.</figDesc><graphic coords="4,89.29,84.19,416.71,152.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.99,89.95,381.59,306.96"><head>Table 1</head><label>1</label><figDesc>Summary of the most important hyperparameters used for each task.</figDesc><table coords="6,124.70,89.95,345.88,306.96"><row><cell>Hyperparameter</cell><cell>T1A</cell><cell>T1B</cell><cell>T2A</cell><cell></cell><cell>T2B</cell></row><row><cell>starting lr</cell><cell>0.00027</cell><cell>0.0005</cell><cell cols="2">0.0001</cell><cell>0.001</cell></row><row><cell>optimizer</cell><cell>AdamW</cell><cell>AdamW</cell><cell cols="2">AdamW</cell><cell>AdamW</cell></row><row><cell>batch size</cell><cell>21</cell><cell>500</cell><cell>20</cell><cell></cell><cell>200</cell></row><row><cell>weight decay</cell><cell>0.0018</cell><cell>0</cell><cell>0</cell><cell></cell><cell>0</cell></row><row><cell>histogram bins</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell></cell><cell>32</cell></row><row><cell>dropout</cell><cell>0.52</cell><cell>0.5</cell><cell>0.5</cell><cell></cell><cell>0.1</cell></row><row><cell>real bags proportion</cell><cell>0.35</cell><cell>0.5</cell><cell>0.5</cell><cell></cell><cell>0.5</cell></row><row><cell cols="2">quantification linear layers size [906, 1554, 38]</cell><cell>[4096]</cell><cell cols="2">[2048, 512]</cell><cell>[4096]</cell></row><row><cell>Hyperparameter</cell><cell cols="2">Values considered</cell><cell cols="3">Selected value</cell></row><row><cell>starting lr</cell><cell cols="2">[0.00001, 0.01]</cell><cell></cell><cell cols="2">0.00027</cell></row><row><cell>optimizer</cell><cell cols="3">AdamW,Adam,RMSprop</cell><cell cols="2">AdamW</cell></row><row><cell>batch size</cell><cell></cell><cell>[1, 100]</cell><cell></cell><cell>21</cell></row><row><cell>weight decay</cell><cell cols="2">[0.00001, 0.1]</cell><cell></cell><cell cols="2">0.0018</cell></row><row><cell>histogram bins</cell><cell></cell><cell>[2, 64]</cell><cell></cell><cell>32</cell></row><row><cell>dropout</cell><cell></cell><cell>[0, 0.8]</cell><cell></cell><cell cols="2">0.52</cell></row><row><cell>real bags proportion</cell><cell></cell><cell>[0, 1]</cell><cell></cell><cell cols="2">0.35</cell></row><row><cell cols="2">feature extraction hidden layer size</cell><cell>[2, 1024]</cell><cell></cell><cell cols="2">798</cell></row><row><cell cols="2">feature extraction output layer size</cell><cell>[2, 256]</cell><cell></cell><cell cols="2">118</cell></row><row><cell>quantification linear layers</cell><cell></cell><cell>[1, 3]</cell><cell></cell><cell>3</cell></row><row><cell>quantification linear layer size</cell><cell></cell><cell>[2, 2048]</cell><cell cols="3">[906, 1554, 38]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,405.46,416.99,32.83"><head>Table 2</head><label>2</label><figDesc>Hyperparameters optimized for task T1A. Note that the values tested (except for the optimizer) represent ranges from which Optuna will sample values trying to optimize the RAE over the validation set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.97,336.97,417.01,130.20"><head>Table 4</head><label>4</label><figDesc>Validation: results over the last 300 dev samples that were not seen by the network in the training process; Test: final results with the competition test data; Baseline (EM): results of the best baseline method over the competition test data</figDesc><table coords="7,124.79,336.97,345.69,74.13"><row><cell>Task</cell><cell></cell><cell>MRAE</cell><cell></cell><cell></cell><cell>MAE</cell><cell></cell></row><row><cell></cell><cell>Validation</cell><cell>Test</cell><cell cols="2">Baseline (EM) Validation</cell><cell>Test</cell><cell>Baseline (EM)</cell></row><row><cell>T1A</cell><cell>0.1194</cell><cell>0.1090</cell><cell>0.113823</cell><cell>0.0229</cell><cell>0.0233</cell><cell>0.02518</cell></row><row><cell>T1B</cell><cell>0.9093</cell><cell>0.8842</cell><cell>1.182070</cell><cell>0.0280</cell><cell>0.0280</cell><cell>0.01976</cell></row><row><cell>T2A</cell><cell>0.0834</cell><cell>0.1070</cell><cell>0.087029</cell><cell>0.0184</cell><cell>0.0192</cell><cell>0.01952</cell></row><row><cell>T2B</cell><cell>1.1911</cell><cell>1.2309</cell><cell>1.309778</cell><cell>0.0319</cell><cell>0.0321</cell><cell>0.01552</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was funded by <rs type="funder">MINECO (Ministerio de EconomÃ­a y Competitividad)</rs> and <rs type="funder">FEDER (Fondo Europeo de Desarrollo Regional)</rs>, grant <rs type="grantNumber">PID2019-110742RB-I00</rs> (<rs type="funder">MINECO</rs><rs type="grantNumber">/FEDER</rs>).</p><p>Also, I would like to thank the organization of the LeQua 2022 competition, for making possible this research and bringing us the opportunity to learn more about new and existing methods, pushing a little further the quantification field. Lastly, thanks to <rs type="person">Juan JosÃ© del Coz</rs> for his insightful comments and the enriching discussions about the methods and the competition.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3aTQjzj">
					<idno type="grant-number">PID2019-110742RB-I00</idno>
				</org>
				<org type="funding" xml:id="_ePpEQHW">
					<idno type="grant-number">/FEDER</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,237.65,393.32,10.91;8,112.66,251.20,394.04,10.91;8,112.66,264.75,256.69,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,308.66,237.65,197.32,10.91;8,112.66,251.20,38.71,10.91">Why is quantification an interesting learning problem?</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>GonzÃ¡lez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>DÃ­ez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Del Coz</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13748-016-0103-3</idno>
		<ptr target="https://doi.org/10.1007/s13748-016-0103-3.doi:10.1007/s13748-016-0103-3" />
	</analytic>
	<monogr>
		<title level="j" coord="8,160.14,251.20,146.86,10.91">Progress in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="53" to="58" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,278.30,394.52,10.91;8,112.28,291.85,218.36,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,343.00,278.30,159.72,10.91">A review on quantification learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>GonzÃ¡lez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>CastaÃ±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J D</forename><surname>Coz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,112.28,291.85,149.64,10.91">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,305.40,394.53,10.91;8,112.66,318.95,22.69,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moreo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sperduti</surname></persName>
		</author>
		<title level="m" coord="8,308.60,305.40,194.30,10.91">Overview of lequa 2022: Learning to quantify</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,332.50,393.33,10.91;8,112.66,346.05,395.01,10.91;8,112.66,359.59,174.74,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,173.01,332.50,271.90,10.91">Evaluation measures for quantification: an axiomatic approach</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10791-019-09363-y</idno>
		<ptr target="https://doi.org/10.1007/s10791-019-09363-y.doi:10.1007/s10791-019-09363-y" />
	</analytic>
	<monogr>
		<title level="j" coord="8,453.55,332.50,52.44,10.91;8,112.66,346.05,77.79,10.91">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="255" to="288" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,373.14,393.33,10.91;8,112.66,386.69,363.59,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="8,353.43,373.14,152.55,10.91;8,112.66,386.69,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,400.24,394.53,10.91;8,112.66,413.79,394.53,10.91;8,112.66,427.34,395.17,10.91;8,112.66,440.89,393.33,10.91;8,112.66,454.44,394.53,10.91;8,112.66,467.99,385.60,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,315.63,427.34,192.20,10.91;8,112.66,440.89,72.82,10.91">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m" coord="8,207.25,440.89,298.74,10.91;8,112.66,454.44,390.37,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,481.54,394.53,10.91;8,112.66,495.09,173.79,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<title level="m" coord="8,289.23,481.54,213.83,10.91">Training deep nets with sublinear memory cost</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,508.64,395.17,10.91;8,112.66,522.18,393.33,10.91;8,112.66,535.73,271.56,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,340.16,508.64,167.68,10.91;8,112.66,522.18,140.77,10.91">Optuna: A next-generation hyperparameter optimization framework</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,276.87,522.18,229.11,10.91;8,112.66,535.73,241.18,10.91">Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,549.28,394.52,10.91;8,112.28,562.83,275.03,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,303.61,549.28,199.10,10.91">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>KÃ©gl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,112.28,562.83,230.24,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,576.38,393.73,10.91;8,112.66,589.93,394.62,10.91;8,112.66,603.48,373.25,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,301.09,576.38,205.30,10.91;8,112.66,589.93,188.80,10.91">Adjusting the Outputs of a Classifier to New a Priori Probabilities: A Simple Procedure</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Latinne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Decaestecker</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976602753284446</idno>
		<ptr target="https://doi.org/10.1162/089976602753284446.doi:10.1162/089976602753284446" />
	</analytic>
	<monogr>
		<title level="j" coord="8,311.66,589.93,93.95,10.91">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="21" to="41" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
