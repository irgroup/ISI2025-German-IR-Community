<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.19,201.82,15.42;1,89.29,106.11,357.70,15.42">KULeuven at LeQua 2022: Model Calibration in Quantification Learning</title>
				<funder ref="#_CAnudAU">
					<orgName type="full">Flemish Government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.93,134.42,109.87,11.96"><forename type="first">Teodora</forename><surname>Popordanoska</surname></persName>
							<email>popordanoska@kuleuven.</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering (ESAT)</orgName>
								<orgName type="laboratory">Processing Speech and Images</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.36,134.42,103.32,11.96"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
							<email>matthew.blaschko@esat.kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering (ESAT)</orgName>
								<orgName type="laboratory">Processing Speech and Images</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.19,201.82,15.42;1,89.29,106.11,357.70,15.42">KULeuven at LeQua 2022: Model Calibration in Quantification Learning</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">AC39D22D81F079AA6C7B72513BC65461</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>quantification learning</term>
					<term>prevalence estimation</term>
					<term>uncertainty calibration</term>
					<term>bias .lisn.upsaclay.fr/competitions/</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many decision-making applications it is of particular interest to obtain aggregated properties about the data sample. Quantification learning (or prevalence estimation) is the task of training predictors via supervised learning to estimate the class distribution of an unlabelled test set. In this paper, we describe our perspective on the "LeQua 2022: Learning to Quantify" challenge, and our entry in "the vector task -T1A, " which achieved 1st place on the leaderboard in terms of relative absolute error, and 2nd place in terms of absolute error. We explored the relationship between probabilistic calibration and quantification, concluding that calibration can help with quantification, but this typically requires an i.i.d. assumption or known distribution shift.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We explore the task of class prevalence estimation from the perspective of uncertainty calibration. Calibrated uncertainty estimates of the predictions from probabalistic classifiers are important in many applications. Numerous techniques have been proposed to address the issue of miscalibration, and they generally fall under two categories. One category is trainable calibration strategies, which modify the training objective, usually by integrating a differentiable calibration measure in the form of a regularizer. Some examples include KDE-XE <ref type="bibr" coords="1,456.11,446.78,11.52,10.91" target="#b0">[1]</ref>, MMCE <ref type="bibr" coords="1,89.29,460.33,12.69,10.91" target="#b1">[2]</ref> and Mix-n-Match <ref type="bibr" coords="1,183.17,460.33,11.28,10.91" target="#b2">[3]</ref>. A second category are post-hoc calibration methods, which rescale the predictions from the classifier after training. The most notable examples of this cateogry are Platt scaling <ref type="bibr" coords="1,163.21,487.42,12.84,10.91" target="#b3">[4]</ref> and its single-parameter version called temperature scaling <ref type="bibr" coords="1,445.11,487.42,11.43,10.91" target="#b4">[5]</ref>.</p><p>A few works have reported the effects of calibration in quantification learning. On the one hand, calibration has been shown to be crucial for a well-known and highly competitive method for estimating class probabilities on unlabelled sets -namely, the Saerens-Latinne-Decaestecker (SLD) method <ref type="bibr" coords="1,215.08,541.62,11.40,10.91" target="#b5">[6]</ref>. Through large scale experimentation across multiple learners and varying amounts of distribution shift, the authors concluded that SLD is ineffective, and often detrimental, when the classifier has not been calibrated. On the other hand, calibration has been shown to deteriorate the results for other probabalistic quantifiers <ref type="bibr" coords="1,434.77,582.27,11.58,10.91" target="#b6">[7]</ref>. Both works used a simple post-hoc method to calibrate the classifier -namely, Platt scaling <ref type="bibr" coords="1,446.59,595.82,11.42,10.91" target="#b3">[4]</ref>. However, Karandikar et al. <ref type="bibr" coords="2,164.75,86.97,12.69,10.91" target="#b7">[8]</ref> demonstrate that under dataset shift, using calibration regularized training objectives result in better uncertainty estimates compared to post-hoc calibration methods. Since the main challenge in quantification stems from the fact that the data distribution changes between the training and testing phase, we investigate empirically whether KDE-XE, as a trainable calibration strategy, can provide further improvements on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>Machine learning is based on the notion of empirical risk minimization (ERM). This principle minimizes risk over a training set of data:</p><formula xml:id="formula_0" coords="2,224.87,234.59,281.11,33.71">ğ‘“ * := arg min ğ‘“ âˆˆâ„± 1 ğ‘› ğ‘› âˆ‘ï¸ ğ‘–=1 â„“(ğ‘“ (ğ‘¥ ğ‘– ), ğ‘¦ ğ‘– )<label>(1)</label></formula><p>for some loss function â„“ such as hinge loss or logistic loss. In the case of binary labels (i.e. ğ‘¦ âˆˆ {0, 1}) and in the limit that ğ‘› â†’ âˆ it is well known <ref type="bibr" coords="2,349.46,292.36,13.00,10.91" target="#b8">[9]</ref> that nearly all standard convex loss functions are equivalent and statistically consistent, i.e. ğ‘“ * converges in probability to the Bayes optimal classifier that is the minimizer of the risk functional:</p><formula xml:id="formula_1" coords="2,223.15,341.27,282.84,18.52">â„› ğ‘ƒ (ğ‘“ ) := âˆ«ï¸ â„“(ğ‘“ (ğ‘¥), ğ‘¦)ğ‘‘ğ‘ƒ (ğ‘¥, ğ‘¦).<label>(2)</label></formula><p>It is key to note that this implies the assumption that the data distribution ğ‘ƒ exists and that the samples are drawn i.i.d. from it. Although learning under data shift is also a well studied field, it is essentially unavoidable that a specific form of data shift must be assumed <ref type="bibr" coords="2,438.37,404.01,16.25,10.91" target="#b9">[10]</ref>. Our analysis of quantification learning is based on the analysis of the bias of a probabilistic predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1 (Bias).</head><p>Let ğ‘“ be a function that predicts from an input space ğ’³ the probability that the label ğ‘¦ âˆˆ {0, 1} is 1. The bias of ğ‘“ with respect to a data distribution ğ‘ƒ is:</p><formula xml:id="formula_2" coords="2,223.33,490.82,282.65,11.95">Bias ğ‘ƒ (ğ‘“ ) := E (ğ‘¥,ğ‘¦)âˆ¼ğ‘ƒ [ğ‘“ (ğ‘¥) -ğ‘¦].<label>(3)</label></formula><p>It is straightforward to see that, for a given distribution ğ‘ƒ , we would like the magnitude of the bias to be minimized w.r.t. the empirical distribution of a data sample in order to ensure that âˆ‘ï¸€ ğ‘– ğ‘“ (ğ‘¥ ğ‘– ) is equal to the true number of positive elements within the sample. It is in general difficult to ensure that the bias of a predictor is minimized, but there is a related notion of calibration error that can be used to assess if under a certain distribution ğ‘ƒ the confidence of ğ‘“ (ğ‘¥) âˆˆ [0, 1] is equal to the probability that ğ‘¦ = 1.</p><p>Definition 2 (ğ¿ ğ‘ calibration error <ref type="bibr" coords="2,244.88,604.74,15.88,10.91" target="#b10">[11]</ref>). The ğ¿ ğ‘ calibration error of ğ‘“ : ğ’³ â†’ [0, 1] is:</p><formula xml:id="formula_3" coords="2,216.01,627.37,289.97,15.99">CE ğ‘ (ğ‘“ ) := E [|E[ğ‘¦|ğ‘“ (ğ‘¥)] -ğ‘“ (ğ‘¥)| ğ‘ ] 1 ğ‘ (4)</formula><p>It has recently been noted that calibration error can be used to control the bias of a predictor due to the following result: Theorem 1 (Proposition 1 in <ref type="bibr" coords="3,219.49,86.97,15.70,10.91" target="#b11">[12]</ref>). ğ¿ ğ‘ calibration error provides an upper bound on the bias of a predictor of the form:</p><formula xml:id="formula_4" coords="3,249.64,125.02,256.35,11.36">CE 1 (ğ‘“ ) â‰¥ | Bias(ğ‘“ )|.<label>(5)</label></formula><p>We refer the reader to <ref type="bibr" coords="3,187.56,149.53,17.76,10.91" target="#b11">[12]</ref> for the proof. <ref type="foot" coords="3,266.02,147.78,3.71,7.97" target="#foot_0">1</ref> Furthermore, as Figure <ref type="figure" coords="3,375.86,149.53,4.97,10.91">1</ref> in <ref type="bibr" coords="3,394.96,149.53,17.76,10.91" target="#b11">[12]</ref> shows, in most cases the empirical values for | Bias | and CE 1 are equal. Thus, if we have a means to find a calibrated classifier, e.g. by trainable calibration methods [2, 1], we will simultaneously minimize the calibration error (with respect to the data distribution) and the bias of a predictor, thereby improving the accuracy of the quantification. An additional result governing calibration error is that it is itself upper bounded by the MSE <ref type="bibr" coords="3,89.29,230.83,16.43,10.91" target="#b12">[13,</ref><ref type="bibr" coords="3,108.45,230.83,14.03,10.91" target="#b13">14]</ref> in the following sense:</p><formula xml:id="formula_5" coords="3,169.42,253.57,332.71,13.13">MSE(ğ‘“ ) -CE 2 (ğ‘“ ) 2 = E[(1 -E[ğ‘¦|ğ‘“ (ğ‘¥)])E[ğ‘¦|ğ‘“ (ğ‘¥)]] â‰¥ 0. (<label>6</label></formula><formula xml:id="formula_6" coords="3,502.13,255.34,3.86,10.91">)</formula><p>The inequality is due to the fact that the expectation is over a variance of Bernoulli random variables.</p><p>A simple consequence of this inequality is that minimizing MSE also minimizes calibration error, which in turn minimizes bias. It is therefore an empirical question whether simple maximization of accuracy is sufficient, or whether an additional calibration operation will yield the lowest possible bias. It is this question that we also sought to answer in our experiments.</p><p>A final important point is that all results involve an expectation with respect to a data distribution. Importantly, the competition performance metric appears to average over a number of distribution shifts, thereby making the evaluation more robust in a sense, but also limiting the ability to have the method specialize to a given distribution. This makes the appropriate notion of MSE, CE, and bias not entirely well defined, though we can of course use the training distribution as a proxy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data</head><p>In task T1A, we tackle a binary quantification problem, where vector representations of the training, development, and test documents were provided. The training set consists of 5000 instances sampled from Amazon product reviews. The development and test set contain 1000 and 5000 samples, respectively, and each sample has 250 documents. The samples are generated with the so-called artificial prevalence protocol (APP), which manipulates the class distribution by sub-sampling, and creates a uniform distribution of class prevalence across the samples. More details about the experimental setting can be found in <ref type="bibr" coords="3,358.13,589.77,16.25,10.91" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Metrics</head><p>The official measure for the challenge is relative absolute error (RAE), defined as:</p><formula xml:id="formula_7" coords="4,209.25,130.98,296.73,29.64">ğ‘…ğ´ğ¸(ğ‘ ğœ , ğ‘ Ë†ğœ) = 1 ğ‘› âˆ‘ï¸ ğ‘¦âˆˆğ’´ |ğ‘ Ë†ğœ(ğ‘¦) -ğ‘ ğœ (ğ‘¦)| ğ‘ ğœ (ğ‘¦)<label>(7)</label></formula><p>where ğ‘ ğœ is the true distribution on sample ğœ and ğ‘ Ë†ğœ is the predicted distribution. The problem that occurs when at least one of the classes ğ‘¦ is zero is resolved by additive smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Baselines</head><p>As part of the QuaPy library <ref type="bibr" coords="4,227.51,236.42,11.58,10.91" target="#b6">[7]</ref>, several advanced quantification methods were provided as baselines, with Logistic Regression (LR) as the underlying classifier. We found the best performing quantification method on the development set, for several choices of a classifier, to be the expectation-maximization-based SLD method. Therefore, our analysis will focus on this method. Esuli et al. <ref type="bibr" coords="4,197.10,290.61,12.83,10.91" target="#b5">[6]</ref> reported a large improvement of the SLD method when used with calibrated classifiers and proposed calibration with Platt scaling (PS) <ref type="bibr" coords="4,400.22,304.16,11.53,10.91" target="#b3">[4]</ref>. We will denote this quantification method as SLD_PS. Following the observation in <ref type="bibr" coords="4,230.29,331.26,12.79,10.91" target="#b7">[8]</ref> that uncertainty estimates of trainable calibration methods generalize better under dataset shift, we use KDE-XE <ref type="bibr" coords="4,325.70,344.81,11.34,10.91" target="#b0">[1]</ref>, where a consistent and differentiable estimator of calibration error is used as a regularizer alongside cross-entropy (XE) loss to train a classifier. KDE refers to Kernel Density Estimation, which is used to estimate the conditional expectation E[ğ‘¦|ğ‘“ (ğ‘¥)] in Equation <ref type="formula" coords="4,241.95,385.46,3.66,10.91">4</ref>. Since SLD was used as a quantification method because of its superior performance, this baseline will be referred to as SLD_KDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results</head><p>The empirical evaluation consists of two parts: in the first experiment (I) we consider a trainable calibration strategy and compare it with an already-known method, whereas the second experiment (II) aims at maximizing the accuracy of the underlying classifier.</p><p>In particular, in the first experiment we explore whether the trainable KDE-XE method has an advantage over the post-hoc Platt scaling (PS) strategy. As an underlying classifier, we consider a Multi Layer Perceptron (MLP) and a Logistic Regression (LR) model, implemented in PyTorch. From the results reported in the top four rows of Table <ref type="table" coords="4,335.83,530.03,3.74,10.91" target="#tab_0">1</ref>, we notice that both for MLP and LR the PS calibration works better in combination with SLD. Therefore, for the next experiment and the final predictions we only consider PS calibration.</p><p>In the second experiment, we investigated several options for the classifier, such as LR, MLP, Random Forrest, Support Vector Machines (SVM), and classifiers based on gradient boosting (XGBoost and LightGBM). For each of the considered classifiers, we performed a randomized hyperparameter search to optimize for accuracy. Random search defines the search space as a bounded domain of hyperparameters and randomly samples points in that domain. The top-three best performing classifiers in terms of accuracy on a validation set are shown in the bottom three rows of Table <ref type="table" coords="4,231.13,651.97,3.78,10.91" target="#tab_0">1</ref>. For the final predictions, we chose the classifier (SVM) and quantification method (SLD) that minimized the mean RAE across samples (MRAE) on the development set. More specifically, the optimal hyperparameters for the chosen SVM classifier were found to be an RBF kernel, with ğ¶ = 5.73437 and ğ›¾ = 0.00146, which were sampled from a loguniform distribution in the range [1 Ã— 10 -5 , 1 Ã— 10 5 ].</p><p>We note that in the first experiment, no hyperparameter optimization was performed for the respective classifiers, hence the difference in accuracy, for instance, between SLD_PS_LR baselines between the two experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion and Conclusions</head><p>The SLD quantification method, with an underlying Platt re-scaled SVM classifier yielded the lowest MRAE on the development set and was used to generate the predictions on the test set. We believe that the reasons why this combination worked well are: (i) SLD with calibrated classifiers has been show to be a very strong baseline, especially for binary settings <ref type="bibr" coords="5,452.38,427.31,11.23,10.91" target="#b6">[7,</ref><ref type="bibr" coords="5,466.08,427.31,8.88,10.91" target="#b5">6]</ref> (ii) the specific type of post-hoc calibration method used assumes that the calibration error can be corrected by applying a sigmoid function to the predictions, which has been empirically justified for SVMs with common kernel functions [4, Section 2.1] (iii) The SVM model achieved highest accuracy across different classifiers.</p><p>To conclude, we compared the performance of calibration regularized training and post-hoc Platt scaling in the task of quantification and found that KDE-XE does not perform very well under severe dataset shift. As a future work, it would be interesting to investigate further the role of calibration in prevalence estimation for defined classes of distribution shift.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,183.72,417.00,139.18"><head>Table 1</head><label>1</label><figDesc>MRAE and Accuracy for the baseline quantification methods and different choices of the underlying classifier. The accuracy is reported on a validation set. Best MRAE and Accuracy is marked in bold.</figDesc><table coords="5,187.99,227.01,219.30,95.89"><row><cell>Experiment</cell><cell>Method</cell><cell cols="2">MRAE â†“ Accuracy â†‘</cell></row><row><cell></cell><cell>SLD_PS_LR</cell><cell>0.133</cell><cell>0.861</cell></row><row><cell>I</cell><cell>SLD_KDE_LR SLD_PS_MLP</cell><cell>0.272 0.279</cell><cell>0.861 0.850</cell></row><row><cell></cell><cell>SLD_KDE_MLP</cell><cell>0.900</cell><cell>0.855</cell></row><row><cell></cell><cell>SLD_PS_SVM</cell><cell>0.117</cell><cell>0.881</cell></row><row><cell>II</cell><cell>SLD_PS_MLP</cell><cell>0.137</cell><cell>0.870</cell></row><row><cell></cell><cell>SLD_PS_LR</cell><cell>0.139</cell><cell>0.868</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,108.93,659.98,397.05,8.97;3,89.29,670.94,77.73,8.97;3,167.02,669.77,3.84,5.24;3,171.36,670.94,276.41,8.97"><p>We note that the proof in<ref type="bibr" coords="3,202.84,659.98,14.65,8.97" target="#b11">[12]</ref> is straightforwardly generalized to other ğ¿ğ‘ norms, for ğ‘ â‰¥ 1, by replacing the absolute value by | â€¢ | ğ‘ , which is also convex and Jensen's inequality can therefore also be applied.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research received funding from the <rs type="funder">Flemish Government</rs> under the "<rs type="programName">Onderzoeksprogramma ArtificiÃ«le Intelligentie (AI) Vlaanderen" programme</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CAnudAU">
					<orgName type="program" subtype="full">Onderzoeksprogramma ArtificiÃ«le Intelligentie (AI) Vlaanderen&quot; programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,111.28,393.33,10.91;6,112.66,124.83,283.42,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Popordanoska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sayer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<title level="m" coord="6,291.46,111.28,214.53,10.91;6,112.66,124.83,188.69,10.91">Calibration Regularized Training of Deep Neural Networks using Kernel Density Estimation</title>
		<imprint>
			<publisher>Openreview</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,138.38,393.33,10.91;6,112.66,151.93,185.56,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,252.71,138.38,253.27,10.91;6,112.66,151.93,108.90,10.91">Trainable calibration measures for neural networks from kernel mean embeddings</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,245.20,151.93,22.08,10.91">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,165.48,393.33,10.91;6,112.66,179.03,393.33,10.91;6,112.66,192.57,67.73,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,272.85,165.48,233.13,10.91;6,112.66,179.03,196.36,10.91">Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,334.51,179.03,171.47,10.91;6,112.66,192.57,37.61,10.91">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,206.12,393.33,10.91;6,112.66,219.67,395.01,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,156.93,206.12,349.06,10.91;6,112.66,219.67,82.08,10.91">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,218.20,219.67,163.03,10.91">Advances in Large Margin Classifiers</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,233.22,394.53,10.91;6,112.66,246.77,122.77,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,311.49,233.22,190.70,10.91">On calibration of modern neural networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04599</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,260.32,395.17,10.91;6,112.66,273.87,395.17,10.91;6,112.66,287.42,249.92,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,289.99,260.32,217.84,10.91;6,112.66,273.87,261.56,10.91">A critical reassessment of the saerens-latinnedecaestecker algorithm for posterior probability adjustment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Molinari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
		<idno type="DOI">10.1145/3433164</idno>
	</analytic>
	<monogr>
		<title level="j" coord="6,382.87,273.87,124.96,10.91;6,112.66,287.42,70.50,10.91">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,300.97,394.53,10.91;6,112.28,314.52,354.36,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,259.83,300.97,242.90,10.91">QuaPy: A Python-Based Framework for Quantification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moreo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="4534" to="4543" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,328.07,394.53,10.91;6,112.66,341.62,292.01,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,112.66,341.62,203.96,10.91">Soft calibration objectives for neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Roelofs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,355.17,394.53,10.91;6,112.48,368.71,297.29,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,313.04,355.17,189.07,10.91">Convexity, classification, and risk bounds</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,112.48,368.71,208.29,10.91">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="138" to="156" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,382.26,393.33,10.91;6,112.66,395.81,179.29,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<title level="m" coord="6,434.67,382.26,71.31,10.91;6,112.66,395.81,78.76,10.91">Dataset Shift in Machine Learning</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,409.36,394.53,10.91;6,112.28,422.91,393.71,10.91;6,112.66,436.46,273.51,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,238.30,409.36,134.43,10.91">Verified uncertainty calibration</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="6,363.07,422.91,142.92,10.91;6,112.66,436.46,84.47,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>AlchÃ©-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,450.01,395.16,10.91;6,112.66,463.56,393.33,10.91;6,112.66,477.11,243.82,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,435.16,450.01,72.67,10.91;6,112.66,463.56,303.70,10.91">On the relationship between calibrated predictors and unbiased volume estimation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Popordanoska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bertels</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,439.77,463.56,66.22,10.91;6,112.66,477.11,213.94,10.91">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,490.66,393.61,10.91;6,112.41,504.21,81.21,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,164.58,490.66,197.39,10.91">A new vector partition of the probability score</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,369.06,490.66,137.21,10.91">Journal of Applied Meteorology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="595" to="600" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,517.76,393.33,10.91;6,112.66,531.30,71.06,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="6,224.33,517.76,200.16,10.91">The comparison and evaluation of forecasters</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Degroot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fienberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,436.62,517.76,69.36,10.91">The Statistician</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="12" to="22" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,544.85,394.53,10.91;6,112.66,558.40,394.52,10.91;6,112.66,571.95,79.06,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="6,308.97,544.85,193.94,10.91">Overview of lequa 2022: Learning to quantify</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moreo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,127.14,558.40,375.78,10.91">Working Notes of the 2022 Conference and Labs of the Evaluation Forum (CLEF 2022)</title>
		<meeting><address><addrLine>Bologna, IT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
