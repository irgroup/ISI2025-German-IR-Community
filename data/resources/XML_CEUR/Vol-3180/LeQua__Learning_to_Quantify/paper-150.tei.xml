<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,85.05,329.12,15.39;1,89.29,106.97,395.16,15.39;1,89.29,128.89,270.44,15.39;1,359.73,125.85,6.25,10.68">UniLeiden at LeQua 2022: The first step in understanding the behaviour of the median sweep quantifier using continuous sweep ⋆</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,158.21,58.86,10.68"><forename type="first">Kevin</forename><surname>Kloos</surname></persName>
							<email>k.kloos@fsw.leidenuniv.nl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Social Sciences</orgName>
								<orgName type="department" key="dep2">Institute of Psychology</orgName>
								<orgName type="department" key="dep3">Department Methodology and Statistics</orgName>
								<orgName type="institution">Leiden University</orgName>
								<address>
									<addrLine>Wassenaarseweg 52</addrLine>
									<postCode>2333 AK</postCode>
									<settlement>Leiden</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Statistics Netherlands</orgName>
								<address>
									<addrLine>Henri Faasdreef 312</addrLine>
									<postCode>2492 JP Den</postCode>
									<settlement>Haag</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,171.79,158.21,102.49,10.68"><forename type="first">Quinten</forename><forename type="middle">A</forename><surname>Meertens</surname></persName>
							<email>q.a.meertens@uva.nl</email>
							<affiliation key="aff1">
								<orgName type="department">Statistics Netherlands</orgName>
								<address>
									<addrLine>Henri Faasdreef 312</addrLine>
									<postCode>2492 JP Den</postCode>
									<settlement>Haag</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">School of Economics</orgName>
								<orgName type="department" key="dep2">Center for Nonlinear Dynamics in Economics and Finance</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Roetersstraat 11</addrLine>
									<postCode>1018 WB</postCode>
									<settlement>Amsterdam, Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.58,158.21,73.98,10.68"><forename type="first">Julian</forename><forename type="middle">D</forename><surname>Karch</surname></persName>
							<email>j.d.karch@fsw.leidenuniv.nl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Social Sciences</orgName>
								<orgName type="department" key="dep2">Institute of Psychology</orgName>
								<orgName type="department" key="dep3">Department Methodology and Statistics</orgName>
								<orgName type="institution">Leiden University</orgName>
								<address>
									<addrLine>Wassenaarseweg 52</addrLine>
									<postCode>2333 AK</postCode>
									<settlement>Leiden</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,85.05,329.12,15.39;1,89.29,106.97,395.16,15.39;1,89.29,128.89,270.44,15.39;1,359.73,125.85,6.25,10.68">UniLeiden at LeQua 2022: The first step in understanding the behaviour of the median sweep quantifier using continuous sweep ⋆</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">10EBB02A44235851B72CC49B2F9B88D1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>quantification learning</term>
					<term>learning to quantify</term>
					<term>classification</term>
					<term>machine learning</term>
					<term>median sweep</term>
					<term>continuous sweep</term>
					<term>LeQua 2022</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the continuous sweep quantifier, a smoothed adaptation of the median sweep quantifier. Previous research has shown that the median sweep quantifier is a good quantifier. However, it is not well understood why it performs well because it is hard to derive its theoretical properties. The continuous sweep quantifier is a modification of the median sweep quantifier that enables computing theoretical results. The continuous sweep quantifier 1) uses kernel estimates instead of the empirical distribution, 2) constructs decision boundaries instead of applying discrete decision rules, and 3) uses the mean instead of the median. We show that a simplified adaptation of the continuous sweep quantifier performs similarly to the median sweep quantifier in terms of bias and variance on the LeQua 2022 dataset. The continuous sweep quantifier can therefore be used to provide insights into the median sweep quantifier by computing theoretical expressions for bias and variance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Quantification Learning, also known as learning to quantify or quantification, is a machine learning task with the aim to compute the class prevalences from an unlabeled test set <ref type="bibr" coords="1,492.22,497.06,11.58,9.74" target="#b0">[1]</ref>. Quantification used to be seen as a side product of classification: a good classifier should also produce good prevalence estimates. However, Forman has objected against this statement and showed that simply classifying and counting the estimated labels from a classifier may lead to a severe bias <ref type="bibr" coords="1,140.43,551.26,11.43,9.74" target="#b1">[2]</ref>. Therefore, more advanced techniques are needed.</p><p>Over the past decades, specific techniques for quantification learning called quantifiers have been developed. Binary quantifiers can be categorized into three groups <ref type="bibr" coords="1,415.18,578.36,11.63,9.74" target="#b0">[1]</ref>: the group based on Classify, Count and Correct, the group based on direct learners, and the group based on distribution matching <ref type="bibr" coords="2,186.59,101.64,11.36,9.74" target="#b2">[3,</ref><ref type="bibr" coords="2,200.67,101.64,7.57,9.74" target="#b3">4]</ref>.</p><p>Currently, there is no consensus in the academic literature about which group of techniques performs best. According to Vapnik's principle <ref type="bibr" coords="2,296.47,128.74,11.28,9.74" target="#b4">[5]</ref>, a problem should be solved directly without solving a more general problem as an intermediate step. Quantification is a more generalized task than classification. Therefore, Vapnik's principle implies that quantifiers should be created without the intermediate step of constructing a classifier <ref type="bibr" coords="2,339.28,169.38,11.24,9.74" target="#b4">[5,</ref><ref type="bibr" coords="2,353.25,169.38,7.49,9.74" target="#b5">6]</ref>. Schumacher compared quantification techniques empirically using an extensive simulation study <ref type="bibr" coords="2,378.43,182.93,11.28,9.74" target="#b6">[7]</ref>. They conclude that some techniques based on Classify, Count and Correct, that is, quantifiers that construct a classifier as an intermediate step, performed best. Especially the median sweep method from Forman performs well among all popular quantifiers <ref type="bibr" coords="2,284.95,223.58,11.28,9.74" target="#b2">[3]</ref>. These two approaches are rather different. An open question is when and why median sweep is such a good quantifier <ref type="bibr" coords="2,412.35,237.13,11.43,9.74" target="#b6">[7]</ref>.</p><p>In this paper, we take the first step in understanding why median sweep is a good quantifier. We propose to perform a theoretical analysis. More specifically, we aim to derive the mean squared error of the median sweep method as a quantifier for the prevalence of the positive class (𝛼) in a binary classification setting. Fortunately, theoretical results of several threshold-based quantifiers have already been derived <ref type="bibr" coords="2,254.56,304.88,11.24,9.74" target="#b7">[8,</ref><ref type="bibr" coords="2,268.23,304.88,7.42,9.74" target="#b8">9,</ref><ref type="bibr" coords="2,278.09,304.88,12.50,9.74" target="#b9">10,</ref><ref type="bibr" coords="2,293.02,304.88,12.50,9.74" target="#b10">11,</ref><ref type="bibr" coords="2,307.96,304.88,12.23,9.74" target="#b11">12]</ref>. We aim to extend these results to median sweep, which is, in fact, an ensemble of threshold-based quantifiers. The key challenge in the theoretical analysis is the discrete nature of median sweep.</p><p>Therefore, this paper introduces the new continuous sweep quantifier. Continuous sweep is a quantifier that is empirically similar to median sweep. It is constructed to have similar empirical performance as median sweep and to allow for easier analytical derivations. Since continuous sweep and median sweep are closely related, we anticipate that thoroughly understanding the theoretical properties of continuous sweep will also provide insight into the properties of median sweep. In this paper, we construct the continuous sweep quantifier, study its empirical performance, and specify a research agenda for the theoretical analysis of this new quantifier.</p><p>The remainder of the paper is organized as follows. In Section 2, we will introduce the mathematical notation and reiterate the mathematical expressions for the common quantifiers from the group Classify, Count and Correct, including median sweep. Moreover, we will introduce the continuous sweep quantifier and we will show how it is related to the median sweep quantifier. In Section 3, we will evaluate and compare the performance of median sweep and continuous sweep using data of the LeQua2022 Task <ref type="bibr" coords="2,324.12,508.11,11.39,9.74" target="#b5">[6]</ref>. In Section 4, we will discuss our new continuous sweep quantifier and provide suggestions for feature research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>In this section, we introduce the continuous sweep quantifier and explain the differences between the continuous sweep quantifier and the median sweep quantifier. First, we introduce the notation and reiterate the definition of the median sweep. Second, we present three theoretical difficulties in analyzing the median sweep quantifier and introduce the continuous sweep quantifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Notation and median sweep</head><p>Consider a population of observations where each observation consists of a feature vector 𝑥 ∈ 𝒳 = ℝ 𝑝 and class label 𝑦 ∈ 𝒴 = {+, -}. Importantly, the class label 𝑦 is only observed in 𝐷 train and 𝐷 val . The class label 𝑦 is unobserved in 𝐷 test . The aim of quantification in a binary setting is to estimate the proportion of observations with a positive label in 𝐷 test using the available data and machine learning techniques.</p><p>We denote the probability density functions of the feature vector for observations in the positive and negative class by 𝑓 (+) (𝑥) and 𝑓 (-) (𝑥), respectively. The probability density functions of the feature vector for the training, validation and test set are each a mixture of 𝑓 (+) (𝑥) and 𝑓 (-) (𝑥), but with different mixture parameters 𝛼 train , 𝛼 val and 𝛼 test , respectively. So, we assume</p><formula xml:id="formula_0" coords="3,89.29,282.12,416.69,27.72">𝑓 train (𝑥) = 𝛼 train ⋅ 𝑓 (+) (𝑥) + (1 -𝛼 train ) ⋅ 𝑓 (-) (𝑥), 𝑓 val (𝑥) = 𝛼 val ⋅ 𝑓 (+) (𝑥) + (1 -𝛼 val ) ⋅ 𝑓 (-) (𝑥), and 𝑓 test (𝑥) = 𝛼 test ⋅ 𝑓 (+) (𝑥) + (1 -𝛼 test ) ⋅ 𝑓 (-) (𝑥).</formula><p>In other words, we assume that the distributions of the positive class in the training, validation, and test set are identical (and we make the same assumption for the negative class). Moreover, we assume that the mixture parameters differ across the data sets. The combination of these assumptions is referred to as prior-probability shift <ref type="bibr" coords="3,111.60,352.54,16.25,9.74" target="#b12">[13]</ref>.</p><p>We consider a soft-classifier δ that maps each feature vector 𝑥 to an estimate of 𝑃(𝑌 = +|𝑋 = 𝑥). The soft-classifier δ can be obtained from a machine learning algorithm which is trained using the training data 𝐷 train . Then, we compute probability estimates δ (𝑥) for all feature vectors in the validation set 𝐷 val . Note that these values can only be interpreted as classification probabilities if the classifier is properly calibrated. Otherwise, we interpret these values as scores. With those scores, we can estimate marginal densities for δ (𝑥) for both classes. We define f (𝑖) as the estimated marginal probability density function for δ (𝑥) given that 𝑦 = 𝑖. The true positive rate and false positive rate can be computed by integrating 𝑓 (𝑖) . Hence, 𝐹 (+) (𝑥) denotes the true positive rate and 𝐹 (-) (𝑥) denotes the false positive rate.</p><p>Quantifiers of type Classify, Count and Correct use a threshold to make an initial guess of the prevalence. The threshold value is based on the estimated score that an observation in 𝐷 test has a positive label. Usually, classifiers use a threshold with a score/probability of 1  2 to classify an observation. Observations with an estimated score larger than or equal to 1  2 are labeled as positive and observations with an estimated score smaller than 1  2 are labeled as negative. Other score values could also be chosen as the threshold value. We will define the threshold value by 𝜃, where we assume 𝜃 ∈ [0, 1] for convenience. Then, observations with an estimated score larger than 𝜃 are positively labelled and observations with an estimated score smaller than 𝜃 are negatively labelled.</p><p>There are several ways to estimate the prevalence of 𝐷 test using 𝐷 train and 𝐷 val , which we will discuss in the next subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classify-and-count ( α CC )</head><p>The most straightforward technique to estimate the prevalence 𝛼 is by simply counting the number of observations that have a score larger than a certain threshold 𝜃 ∈ [0, 1] in 𝐷 test and dividing it by the total number of observations in 𝐷 test . This technique is more commonly known as the classify-and-count quantifier α CC . The classify-and-count quantifier α CC is not a good quantifier for 𝛼, even when the underlying classifier performs well. Good classification performance is not sufficient enough for reliable quantification <ref type="bibr" coords="4,363.05,176.40,11.28,9.74" target="#b0">[1]</ref>. The most common threshold for 𝜃 is 1  2 , which makes sense for classification but is, in general, suboptimal for quantification. For a biased soft-classifier δ (𝑥) and/or when the prevalences differ across the training, validation and test set, a threshold of 𝜃 = 0.5 is suboptimal for quantification. Given the notation from the previous paragraphs, we define the classify-and-count quantifier as</p><formula xml:id="formula_1" coords="4,224.53,255.09,281.46,28.66">α CC (𝐷 test , 𝜃) = 1 𝑛 test ∑ 𝑥∈𝐷 test 𝟙 { δ (𝑥)≥𝜃} .<label>(1)</label></formula><p>In the next subsection, we use the classify-and-count quantifier to define the adjusted-count quantifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adjusted count ( α AC )</head><p>The adjusted-count quantifier ( α AC ) corrects the classify-and-count quantifier α CC using estimated classification rates. The adjusted-count quantifier uses the true positive rate and false positive rate for the classifier δ (𝑥) ≥ 𝜃 to adjust the classify-and-count estimate. The two classification rates are estimated from the validation set. The classification rates of class 𝑖 are computed by counting the proportion of observations in 𝐷 val with a label 𝑦 = 𝑖 that have a score δ (𝑥) larger than 𝜃. Then, the classification rates are defined as</p><formula xml:id="formula_2" coords="4,214.46,454.61,291.52,42.55">F (𝑖) (𝐷 val , 𝜃) = ∑ (𝑥,𝑦)∈𝐷 val ∶𝑦=𝑖 {𝟙 { δ (𝑥)≥𝜃} } ∑ 𝑦∈𝐷 val {𝟙 {𝑦=𝑖} } .<label>(2)</label></formula><p>The adjusted-count quantifier is then derived as</p><formula xml:id="formula_3" coords="4,194.42,534.74,311.57,31.67">α AC (𝐷 test , 𝐷 val , 𝜃) = α CC (𝐷 test , 𝜃) -F (-) (𝐷 val , 𝜃) F (+) (𝐷 val , 𝜃) -F (-) (𝐷 val , 𝜃) .<label>(3)</label></formula><p>In contrast to classify-and-count, the adjusted-count quantifier has been proven to be asymptotically unbiased <ref type="bibr" coords="4,157.98,590.43,11.31,9.74" target="#b7">[8,</ref><ref type="bibr" coords="4,172.02,590.43,12.53,9.74" target="#b9">10,</ref><ref type="bibr" coords="4,187.28,590.43,12.28,9.74" target="#b11">12]</ref>. The adjusted-count quantifier does not compute reliable prevalence estimates for each threshold value 𝜃. If 𝜃 is such that the difference between true positive rate F (+) (𝐷 val , 𝜃) and false positive rate F (-) (𝐷 val , 𝜃) is small, then the numerator of Eq. ( <ref type="formula" coords="4,461.26,617.52,3.86,9.74" target="#formula_3">3</ref>) is small, which, in turn, leads to a large variance of the quantifier <ref type="bibr" coords="4,342.55,631.07,11.36,9.74" target="#b7">[8,</ref><ref type="bibr" coords="4,356.63,631.07,12.32,9.74" target="#b11">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Median sweep ( α MS )</head><p>The median sweep quantifier uses the adjusted-count quantifier to compute prevalence estimates for a range of threshold values. Then, it takes the median value of the computed range of prevalence estimates as the final estimate <ref type="bibr" coords="5,270.50,135.76,11.28,9.74" target="#b2">[3]</ref>. As a remedy for the large variance of the adjustedcount quantifier, Forman advised to only compute the adjusted-count quantifier for those threshold values 𝜃 for which the difference between F (+) (𝐷 val , 𝜃) and F (-) (𝐷 val , 𝜃) is bigger than 1 4 <ref type="bibr" coords="5,98.47,176.40,11.43,9.74" target="#b2">[3]</ref>. In notation, the median sweep is</p><formula xml:id="formula_4" coords="5,129.60,200.41,376.38,23.70">α MS (𝐷 test , 𝐷 val ) = med ({ α AC (𝐷 test , 𝐷 val , 𝜃) ∶ F (+) (𝐷 val , 𝜃) -F (-) (𝐷 val ) &gt; 1 4 }) .<label>(4)</label></formula><p>This can be simplified by only considering thresholds 𝜃 ∈ { δ (𝑥) ∶ 𝑥 ∈ 𝐷 test }, which can be computed easily. Now, we have a finite set of prevalence estimates, which makes it easy to compute the median. We implement median sweep by fitting the estimated probabilities/scores of the validation set 𝐷 val to an empirical cumulative density function (ecdf ). The empirical cumulative density function is computed similarly to the classify-and-count quantifier α 𝐶𝐶 , but then using the validation data 𝐷 val conditional on the labels. Hence, F (+) MS (𝑥) defines the function of the true positive rate using the median sweep paradigm and F (-) MS (𝑥) defines the function of the false positive rate using the median sweep paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Continuous sweep</head><p>In this section, we first explain why it is difficult to derive the mean square error for the median sweep quantifier. Second, we introduce the continuous sweep quantifier. We introduce two variants of the continuous sweep quantifier: the original continuous sweep quantifier and the simplified continuous sweep quantifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Difficulties median sweep</head><p>In Section 2.1, we explained how the median sweep quantifier works. The median sweep quantifier has a few properties that make it difficult to derive the mean square error. We present the three most important reasons.</p><p>First, the classify-and-count quantifier α 𝐶𝐶 and classification rates F (-) and F (+) , are interpreted as step functions in 𝜃.</p><p>Step functions are not differentiable, so are therefore difficult to study analytically. Second, outliers are removed using a complicated data-dependent function, see Eq. ( <ref type="formula" coords="5,127.08,571.43,3.49,9.74" target="#formula_4">4</ref>). Every test set has a different number of observations that pass the data-dependent function and therefore computations grow fast since we need to compute the variance for every number of observations that pass the data-dependent function. Third, it is in general difficult to compute the mean and variance of the median as a function, especially for complex algorithms and distributions. Even for proper densities, we need to invert the cumulative density function to compute the median analytically, which is often unavailable.</p><p>In the next subsection, we propose solutions to the problems that occur with median sweep and we introduce the continuous sweep quantifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous sweep quantifier</head><p>The continuous sweep quantifier is a smoothed adaptation of the median sweep quantifier. The continuous sweep quantifier provides solutions for the problems that occur for the median sweep regarding computing theoretical results.</p><p>Instead of using step functions for the classify-and-count quantifier α 𝐶𝐶 and the classification rates F (-) and F (+) , the continuous sweep quantifier uses continuous functions. If we know the type of distribution, estimating the classify-and-count quantifier α 𝐶𝐶 and the classification rates F (-) and F (+) can be done parametrically with maximum likelihood estimation. If we do not know the type of distribution, we use kernel methods to estimate the marginal densities. In this paper, we use kernel estimates to compute the continuous functions for the classifyand-count quantifier α 𝐶𝐶 and the classification rates F (-) and F (+) . These functions are now continuous instead of discrete. Then, classification rates F (-) and F (+) are kernel cumulative density functions given a soft-classifier δ (𝑥) and validation data 𝐷 val , and where the classifyand-count quantifier α CC is a kernel cumulative density function given a soft-classifier δ (𝑥) and test data 𝐷 test . Figures <ref type="figure" coords="6,194.20,284.80,26.33,9.74" target="#fig_0">1a, 1b</ref> and<ref type="figure" coords="6,243.10,284.80,9.94,9.74" target="#fig_0">1c</ref> show some examples. The black dots in Figures <ref type="figure" coords="6,476.07,284.80,10.26,9.74" target="#fig_0">1a</ref> and<ref type="figure" coords="6,89.04,298.35,10.46,9.74" target="#fig_0">1b</ref> show the observations in 𝐷 𝑣𝑎𝑙 where from we construct the empirical density functions for the true positive rate and the false positive rate. The red lines show the continuous function of the classification rates using a kernel. The black dots in Figure <ref type="figure" coords="6,368.82,325.45,9.68,9.74" target="#fig_0">1c</ref> show the classify-and-count estimate for each observation in 𝐷 test and the red line shows the continuous function of the classify-and-count quantifier for each threshold value 𝜃. Figure <ref type="figure" coords="6,387.02,352.54,10.80,9.74" target="#fig_0">1d</ref> shows two things: the continuous function of the adjusted-count quantifier using the functions in Figures <ref type="figure" coords="6,459.11,366.09,27.85,9.74" target="#fig_0">1a, 1b,</ref> and<ref type="figure" coords="6,89.04,379.64,8.09,9.74" target="#fig_0">1c</ref>, and the prevalence estimates of each observation in 𝐷 test that we need to compute median sweep. All continuous functions seem to resemble their discrete equivalent.</p><p>With continuous sweep, we should still consider that prevalence estimates for extreme values of 𝜃 have large variances. With median sweep, we discard every prevalence estimate where the difference between the classification rates is smaller than 1  4 . In order to keep the differences between continuous sweep and median sweep as small as possible, we propose to apply the same decision rule to continuous sweep as to median sweep. Consider two decision boundaries 𝜃 𝑙 and 𝜃 𝑟 , where 𝜃 𝑙 is the lower (left) threshold value where F (+) (𝐷 val , 𝜃) -F (-) (𝐷 val , 𝜃) = 1  4 and 𝜃 𝑟 is the upper (right) threshold value where F (+) (𝐷 val , 𝜃) -F (-) (𝐷 val , 𝜃) = 1  4 . In Figure <ref type="figure" coords="6,437.45,491.28,8.55,9.74" target="#fig_0">1d</ref>, the decision boundaries 𝜃 𝑙 and 𝜃 𝑟 are showed with a vertical orange line. Then we integrate to compute the area between 𝜃 𝑙 and 𝜃 𝑟 , where F (+) (𝐷 val , 𝜃) -F (-) (𝐷 val , 𝜃) ≥ 1  4 , and divide it by the difference between 𝜃 𝑙 and 𝜃 𝑟 . In Figure <ref type="figure" coords="6,214.92,531.93,8.77,9.74" target="#fig_0">1d</ref>, we see a slight difference between the decision boundaries of the continuous sweep quantifier and the decision rule of the median sweep quantifier. In this example, the median sweep quantifier allows observations with more extreme threshold values 𝜃 than the continuous sweep quantifier in their calculations. This can be seen by the blue dots that lay at the outside of the orange decision boundaries. Therefore, the kernels do not exactly match the discrete observations.</p><p>Using the estimated continuous distributions, we can estimate the adjusted-count quantifier for any threshold. Hence, instead of computing the median of discrete data points, we propose to use integration across the whole probability range to compute the expected value of ( α CS ). Finding the median is more complex since we need to find the quantile function of α 𝐴𝐶 . In Figure <ref type="figure" coords="6,119.69,667.42,8.49,9.74" target="#fig_0">1d</ref>, we see that the function of the adjusted-count quantifier against the threshold values is not bijective. This property makes it hard to find the inverse function, which enables to compute the median. Therefore, we propose to compute the (weighted) mean instead of the median. Even though the median is a more robust estimator, we think that the mean should give similar estimates because the outliers are discarded using the decision rule. The mean can be computed by computing the area under the curve using integrals of the continuous functions.</p><p>In order to make the continuous sweep quantifier as similar as the median sweep quantifier, we should weight areas with many observations in 𝐷 test more than areas with little observations in 𝐷 test . The probability density function of the observations' threshold values in 𝐷 test ( f𝛿(𝑥) (𝜃)) defines the weights of the continuous sweep quantifier. In fact, this is the (negative value of the) derivative of the classify-and-count quantifier with respect to 𝜃. We have already computed the function of the classify-and-count quantifier and can use its derivative with respect to 𝜃 to compute the weights. Taking into account the decision boundaries, the expected value of the continuous sweep quantifier α CS is given by</p><formula xml:id="formula_5" coords="7,98.11,266.89,407.87,111.42">α CS (𝐷 test , 𝐷 val , 𝜃 𝑙 , 𝜃 𝑟 ) = 1 F (𝜃 𝑟 ) -F (𝜃 𝑙 ) ∫ 𝜃 𝑟 𝜃=𝜃 𝑙 f𝛿(𝑥) (𝜃) ⋅ α 𝐴𝐶 (𝐷 test , 𝐷 val , 𝜃) 𝑑𝜃 = 1 α CC (𝐷 test , 𝜃 𝑟 ) -α AC (𝐷 test , 𝜃 𝑙 ) ∫ 𝜃 𝑟 𝜃=𝜃 𝑙 -( 𝑑 𝑑𝜃 α 𝐶𝐶 (𝐷 test , 𝜃)) α 𝐶𝐶 (𝐷 test , 𝐷 val , 𝜃) 𝑑𝜃 = 1 α AC (𝐷 test , 𝜃 𝑟 ) -α CC (𝐷 test , 𝜃 𝑙 ) ∫ 𝜃 𝑟 𝜃=𝜃 𝑙 -( 𝑑 𝑑𝜃 α 𝐶𝐶 (𝐷 test , 𝜃)) α 𝐶𝐶 (𝐷 test , 𝜃) -F (-) (𝐷 val , 𝜃) F (+) (𝐷 val , 𝜃) -F (-) (𝐷 val , 𝜃) 𝑑𝜃.<label>(5)</label></formula><p>The integral of Eq. ( <ref type="formula" coords="7,184.65,386.47,3.86,9.74" target="#formula_5">5</ref>) is numerically tedious because it contains many estimates from the data. In order to reduce numerical complexity, we introduce the simplified continuous sweep quantifier α SCS . The simplified continuous sweep quantifier does not contain f𝛿(𝑥) (𝜃) in the integral. The interpretation of leaving out this density is that we no longer weight areas with many observations in 𝐷 test more than areas with little observations in 𝐷 test . We believe that the impact of this omission on the theoretical properties of the quantifier are limited. We include a brief explanation, as an elaborate theoretical analysis is out of scope of this paper. First, we note that the adjusted count estimator is asymptotically unbiased for every threshold value 𝜃 <ref type="bibr" coords="7,123.28,494.87,11.48,9.74" target="#b7">[8,</ref><ref type="bibr" coords="7,137.67,494.87,12.59,9.74" target="#b9">10,</ref><ref type="bibr" coords="7,153.18,494.87,12.42,9.74" target="#b11">12]</ref>. Hence, the continuous sweep quantifier can be interpreted as a weighted average of asymptotically unbiased estimators and the simplified continuous sweep quantifier can be interpreted as an unweighted average of asymptotically unbiased estimators. Both quantifiers are therefore asymptotically unbiased estimators. The difference between the two is the asymptotic variance. A more detailed theoretical comparison between median sweep, continuous sweep, and simplified continuous will be included in a future paper. The key take home message is that the simplified continuous sweep quantifier is theoretically similar to the continuous sweep quantifier and has more appealing numerical properties. The simplified continuous sweep quantifier α SCS that can be computed as</p><formula xml:id="formula_6" coords="7,154.51,619.43,351.47,65.85">α SCS (𝐷 test , 𝐷 val , 𝜃 𝑙 , 𝜃 𝑟 ) = 1 𝜃 𝑟 -𝜃 𝑙 ∫ 𝜃 𝑟 𝜃=𝜃 𝑙 α 𝐴𝐶 (𝐷 test , 𝐷 val , 𝜃) 𝑑𝜃 = 1 𝜃 𝑟 -𝜃 𝑙 ∫ 𝜃 𝑟 𝜃=𝜃 𝑙 α 𝐶𝐶 (𝐷 test , 𝜃) -F (-) (𝐷 val , 𝜃) F (+) (𝐷 val , 𝜃) -F (-) (𝐷 val , 𝜃) 𝑑𝜃.<label>(6)</label></formula><p>Concluding, the continuous sweep quantifiers are continuous adaptations of median sweep, but makes it easier to compute theoretical results. In the next section, we compare the continuous sweep quantifiers with the median sweep quantifier with the data provided by the LeQua2022 Task. This figure shows the strong numerical similarity between median sweep as in Eq. 4 and our continuous sweep method as in Eqs. 5 and 6. In subfigures (a)-(c), the red curves are the continuous version of the discrete median sweep estimates (black dots). In subfigure (d), the black line shows the estimated adjusted-count value for every threshold value 𝜃 using the curve from subfigures (a)-(c). The vertical, orange lines show the decision boundaries 𝜃 𝑙 and 𝜃 𝑟 . The blue dots shows the adjusted-count estimates from median sweep that pass the criterion that the difference between the true positive rate and the false positive rate is larger than 1  4 , the red dots are the estimates that fail the criterion. The median sweep quantifier is computed by taking the median of the blue dots in subfigure (d). The simplified continuous sweep quantifier can be computed by integrating the area between the decision boundaries of subfigure (d) and divide it by the distance between the decision boundaries. The original continuous sweep quantifier can be computed by integrating the weighted area between the decision boundaries of subfigure (d) and divide it by the weighted distance between the decision boundaries. These weights are based on the classify-and-count quantifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation</head><p>In this section, we evaluate the continuous sweep quantifiers and the median sweep quantifier. In short, the objective is to quantify the prevalence 𝛼 of positive product reviews (from a webshop) as accurate as possible across 5, 000 test sets. For more information on the quantification task, we refer to the paper of the LeQua 2022 Task <ref type="bibr" coords="10,292.29,153.05,11.45,9.74" target="#b5">[6]</ref>. First, we explain the technical details of our study. Second, we show the results of the quantifiers on the test datasets. Third, we explain the similarities and differences between the continuous sweep quantifiers and the median sweep quantifier regarding the quantification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Technical setup</head><p>The analysis is performed using statistical software R version 4.1.3 <ref type="bibr" coords="10,403.44,243.42,16.42,9.74" target="#b13">[14]</ref>. Besides the core packages, we used t i d y v e r s e and t i d y m o d e l s <ref type="bibr" coords="10,299.53,256.97,16.56,9.74" target="#b14">[15,</ref><ref type="bibr" coords="10,319.98,256.97,12.42,9.74" target="#b15">16]</ref>. The training data consists of 5, 000 observations, each with 300 covariates and a label on whether the review is positive or negative. The training set is imbalanced: 3, 870 reviews are positive and 1, 130 reviews are negative. We randomly split this dataset in two parts: a training set 𝐷 train containing 4, 000 observations and validation set 𝐷 val containing 1, 000 observations from the complete training data. The training data 𝐷 train was balanced, which means that some of the negatively labelled observations are replicated to match the number of positively labelled observations.</p><p>Our classification model was a support vector machine (SVM) <ref type="bibr" coords="10,371.72,351.82,16.09,9.74" target="#b16">[17]</ref>, denoted by δ . The SVM is trained with the training data 𝐷 train . The model had a linear kernel boundary and a regularisation parameter 𝐶 = 1. We converted the decision values of the SVM to probabilities/scores using Platt scaling <ref type="bibr" coords="10,146.65,392.47,16.25,9.74" target="#b17">[18]</ref>, such that we could use the theory of the previous section.</p><p>We computed the classify-and-count estimator α CC and classification rates 𝐹 We computed the classify-and-count estimator α CC and classification rates 𝐹 CS (𝑥) for the continuous sweep quantifiers using the k c d e function from the k s package <ref type="bibr" coords="10,439.58,464.53,16.09,9.74" target="#b18">[19]</ref>. Moreover, we computed f𝛿(𝑥) (𝜃) using the k d e function from the same k s package. We added no additional arguments for both functions, except the boundaries for the estimated probabilities, which are set to 0 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head><p>In this section, we evaluated the median sweep quantifier and the continuous sweep quantifiers on the test sets of the LeQua2022 task. First, we compared the median sweep quantifier and the continuous sweep quantifiers with the true prevalences. Second, we compared the median sweep quantifier with the continuous sweep quantifiers.</p><p>First, we evaluated the median sweep quantifier on the test sets. Figures <ref type="figure" coords="10,439.83,609.10,12.71,9.74" target="#fig_4">2a,</ref><ref type="figure" coords="10,456.15,609.10,10.66,9.74" target="#fig_4">2b</ref> plot the estimated prevalence by the median sweep quantifier against the true prevalence, and the residuals. Obviously, the error of very small estimated prevalences is positive and the error of the very large estimated prevalence is negative. Moreover, it seems that there is a small positive bias among the estimated prevalences. Second, we evaluate the continuous sweep quantifiers on the test sets. Figure <ref type="figure" coords="11,441.05,198.17,9.56,9.74" target="#fig_4">2c</ref> and 2e plots the estimated prevalence by the continuous sweep quantifiers against the true prevalence, Figure <ref type="figure" coords="11,89.29,225.27,10.38,9.74" target="#fig_4">2d</ref> and 2f plot the estimated prevalence by the continuous sweep quantifiers against the residuals. We see different results between the continuous sweep quantifiers. The continuous sweep quantifier performs worse than the simplified continuous sweep quantifier: the continuous sweep quantifier has a large bias for large prevalence values and it has more variance than the simplified continuous sweep quantifier. It is clear that the simplified continuous sweep quantifier performs better than the original continuous sweep quantifier and therefore, we will now only compare the simplified continuous sweep quantifier with the median sweep quantifier.</p><p>When we compare the median sweep quantifier with the simplified continuous sweep quantifier, we see some similarities and differences. The two quantifiers seem to have only little bias across the range of prevalences, however, the direction of the bias is different. Moreover, the pattern of the bias is different. The bias of the median sweep quantifier is monotonically increasing (Figure <ref type="figure" coords="11,169.80,374.31,8.95,9.74" target="#fig_4">2b</ref>) while the bias of the simplified continuous sweep quantifier seems to have a local minimum and a local maximum (Figure <ref type="figure" coords="11,301.12,387.86,7.12,9.74" target="#fig_4">2f</ref>). The variance of the simplified continuous sweep quantifier is slightly larger than the variance of the median sweep quantifier, see Table <ref type="table" coords="11,499.86,401.40,3.66,9.74" target="#tab_1">1</ref>, hence the mean absolute error (MAE) of the simplified continuous sweep quantifier is slightly larger than the MAE of the median sweep quantifier.</p><p>The simplified continuous sweep quantifier has more variance than the median sweep quantifier. A reason could be that the mean is more sensitive to extreme values than the median. Figure <ref type="figure" coords="11,120.43,469.15,5.08,9.74" target="#fig_5">3</ref> shows nine examples of the adjusted-count integral and the median sweep estimates. Remarkable is that the continuous sweep function is close to the discrete estimates over the whole range of 𝜃, except around the value of 𝜃 𝑟 . This difference can be a possible cause of the small difference between the continuous sweep quantifier and the median sweep quantifier.</p><p>Concluding, the simplified continuous sweep quantifier is a quantifier that performs slightly worse than the median sweep quantifier using the procedure described in this section. The original continuous sweep quantifier performs much worse than the other two quantifiers. The results for the simplified continuous sweep quantifier and the median sweep quantifier are similar and we believe that we can use the (simplified) continuous sweep quantifier to compute theoretical results that are related to the median sweep quantifier.   The blue dots show the adjusted-count estimates from the median sweep that pass the criterion that the difference between the true positive rate and the false positive rate is larger than 1  4 , and the red dots are the estimates that fail the criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and Discussion</head><p>The goal of this paper was to design the continuous sweep quantifier, study its empirical performance, and specify a research agenda for the theoretical analysis of this new quantifier.</p><p>In this paper, we constructed the continuous sweep quantifier. We provided two versions of the continuous sweep quantifier: the original continuous sweep quantifier where every threshold is weighted with the classify-and-count quantifier and the simplified continuous sweep quantifier without weights. The continuous sweep quantifiers are based on the wellknown median sweep quantifier. Previous research has shown that the median sweep quantifier is a good quantifier. However, it is not well understood why it performs well because it is hard to derive its theoretical properties. The median sweep quantifier uses empirical distributions for the classify-and-count quantifier α AC and the classification rates 𝐹 (+) (𝑥) and 𝐹 (-) (𝑥) which makes it hard to do proper calculations on, like differentiating and integrating. Moreover, median sweep uses discrete decision rules to remove outliers, which makes the calculations more complicated. Last, the median is hard to compute analytically since the functions of the prevalence 𝛼 against threshold 𝜃 is non-bijective. Therefore, we proposed a new quantifier named the continuous sweep. The continuous sweep quantifier is a modification of the median sweep quantifier that enables computing theoretical results. The continuous sweep quantifier 1) used kernel estimates instead of the empirical distribution, 2) constructed decision boundaries instead of applying discrete decision rules, and 3) used the mean instead of the median. Figure <ref type="figure" coords="14,89.04,356.29,5.07,9.74" target="#fig_0">1</ref> showed that the continuous functions are closely related to the empirical functions.</p><p>The simplified continuous sweep quantifier performed similar to the median sweep quantifier in terms of bias and variance. The original continuous sweep quantifier performed much worse than the simplified continuous sweep quantifier. Both continuous sweep quantifiers can be further optimized by choosing better kernels and other hyper-parameters.</p><p>The outline for the theoretical agenda is separated into two parts: defining the assumptions of the continuous distributions, and second we discuss how to compute the theoretical results.</p><p>First, we make assumptions about the continuous distributions. In this paper, the continuous distributions are kernels with default parameters estimated from the training and validation data. Deriving theoretical results from default kernels is still a cumbersome task. Therefore, we can make assumptions on the distributions. We start using basic distributional distributions such as the uniform. Later on, we can extend it to more complex distributional distributions such as the beta.</p><p>Second, we discuss how to compute the theoretical results. In the first step, we assume that the classification rates follow a uniform distribution where the limits are given. Then, we can compute the expected value of the classify-and-count quantifier for each prevalence 𝛼 over each threshold value 𝜃. Adding the information of the distributions of the classification rates, we can compute the expected value of the adjusted-count quantifier using <ref type="bibr" coords="14,420.54,586.62,12.83,9.74" target="#b7">[8]</ref> and iterate over the whole range of 𝜃 to compute the expected value of the continuous sweep quantifier. We can apply a similar strategy for the variance. Combining the expected value and the variance results in a value for the mean square error for the continuous sweep quantifier. Having the mean square error of the continuous sweep quantifier, we can compare it with the mean square error for other quantifiers like the adjusted count, calibration or mixed quantifier <ref type="bibr" coords="14,453.00,654.37,11.36,9.74" target="#b7">[8,</ref><ref type="bibr" coords="14,467.08,654.37,7.47,9.74" target="#b8">9,</ref><ref type="bibr" coords="14,477.28,654.37,12.32,9.74" target="#b9">10]</ref>.</p><p>After computing theoretical results for the continuous sweep quantifier, we can further im-prove the continuous sweep quantifier. The continuous sweep quantifier has been constructed to compute theoretical results for the median sweep quantifier. With innovative techniques regarding kernel estimates and handling large variances, we can improve the predictive performance of the continuous sweep quantifier.</p><p>In conclusion, the continuous sweep quantifier can be used to understand median sweep more thoroughly. It enables us to compute theoretical results for bias and variance in future papers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,89.29,511.53,416.87,8.91;9,89.29,523.49,416.69,8.91;9,89.06,535.44,416.92,8.91;9,89.29,547.40,416.69,8.91;9,89.06,559.35,217.13,8.91;9,306.19,563.39,1.35,6.23;9,310.46,559.35,21.86,8.91;9,332.32,563.39,2.01,6.23;9,334.76,559.35,171.22,8.91;9,89.29,571.31,416.70,8.91;9,89.29,583.26,186.48,8.91;9,279.71,580.42,3.24,6.23;9,279.71,589.99,3.24,6.23;9,284.15,583.26,223.37,8.91;9,89.02,595.22,416.95,8.91;9,89.29,607.17,416.69,8.91;9,89.29,619.13,416.68,8.91;9,89.29,631.08,416.69,8.91;9,89.29,643.04,418.22,8.91;9,89.02,654.99,251.08,8.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1:This figure shows the strong numerical similarity between median sweep as in Eq. 4 and our continuous sweep method as in Eqs. 5 and 6. In subfigures (a)-(c), the red curves are the continuous version of the discrete median sweep estimates (black dots). In subfigure (d), the black line shows the estimated adjusted-count value for every threshold value 𝜃 using the curve from subfigures (a)-(c). The vertical, orange lines show the decision boundaries 𝜃 𝑙 and 𝜃 𝑟 . The blue dots shows the adjusted-count estimates from median sweep that pass the criterion that the difference between the true positive rate and the false positive rate is larger than1  4 , the red dots are the estimates that fail the criterion. The median sweep quantifier is computed by taking the median of the blue dots in subfigure (d). The simplified continuous sweep quantifier can be computed by integrating the area between the decision boundaries of subfigure (d) and divide it by the distance between the decision boundaries. The original continuous sweep quantifier can be computed by integrating the weighted area between the decision boundaries of subfigure (d) and divide it by the weighted distance between the decision boundaries. These weights are based on the classify-and-count quantifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,428.29,403.84,11.87,7.79;10,427.03,412.84,11.55,7.79;10,440.63,408.20,38.85,9.74;10,480.75,403.84,11.87,7.79;10,479.49,412.84,11.55,7.79;10,493.09,408.20,13.67,9.74;10,89.29,421.75,416.69,9.74;10,89.29,435.30,129.25,9.74"><head></head><label></label><figDesc>for the median sweep quantifier using the e c d f function. The e c d f function fits a empirical step function from the input data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,103.09,236.92,9.31,6.19;12,103.09,208.44,9.31,6.19;12,103.09,179.95,9.31,6.19;12,103.09,151.47,9.31,6.19;12,103.09,122.98,9.31,6.19;12,103.09,94.49,9.31,6.19;12,118.53,250.12,9.31,6.19;12,147.01,250.12,9.31,6.19;12,175.50,250.12,9.31,6.19;12,203.98,250.12,9.31,6.19;12,232.47,250.12,9.31,6.19;12,260.96,250.12,9.31,6.19;12,155.73,258.44,77.34,7.57;12,93.37,181.93,7.57,14.23;12,93.37,140.07,7.57,39.58;12,96.71,279.61,172.38,8.90;12,332.27,220.25,13.22,6.19;12,336.18,171.90,9.31,6.19;12,336.18,123.53,9.31,6.19;12,351.45,251.63,9.31,6.19;12,379.22,251.63,9.31,6.19;12,406.99,251.63,9.31,6.19;12,434.76,251.63,9.31,6.19;12,462.54,251.63,9.31,6.19;12,490.31,251.63,9.31,6.19;12,386.87,259.95,77.34,7.57;12,322.55,188.76,7.57,35.70;12,322.55,170.12,7.57,16.37;12,322.55,153.62,7.57,14.23;12,322.55,111.76,7.57,39.58;12,344.14,279.61,135.87,8.90;12,103.09,414.88,9.31,6.19;12,103.09,386.40,9.31,6.19;12,103.09,357.91,9.31,6.19;12,103.09,329.42,9.31,6.19;12,103.09,300.94,9.31,6.19;12,118.53,456.56,9.31,6.19;12,147.01,456.56,9.31,6.19;12,175.50,456.56,9.31,6.19;12,203.98,456.56,9.31,6.19;12,232.47,456.56,9.31,6.19;12,260.96,456.56,9.31,6.19;12,155.73,464.88,77.34,7.57;12,93.37,388.37,7.57,14.23;12,93.37,346.51,7.57,39.58;12,88.99,486.05,187.73,8.90;12,332.27,432.94,13.22,6.19;12,332.27,403.43,13.22,6.19;12,336.18,373.92,9.31,6.19;12,336.18,344.42,9.31,6.19;12,336.18,314.91,9.31,6.19;12,351.45,458.07,9.31,6.19;12,379.22,458.07,9.31,6.19;12,406.99,458.07,9.31,6.19;12,434.76,458.07,9.31,6.19;12,462.54,458.07,9.31,6.19;12,490.31,458.07,9.31,6.19;12,386.87,466.39,77.34,7.57;12,322.55,395.21,7.57,35.70;12,322.55,376.56,7.57,16.37;12,322.55,360.06,7.57,14.23;12,322.55,318.20,7.57,39.58;12,336.56,486.05,151.04,8.90;12,103.09,649.81,9.31,6.19;12,103.09,621.33,9.31,6.19;12,103.09,592.84,9.31,6.19;12,103.09,564.36,9.31,6.19;12,103.09,535.87,9.31,6.19;12,103.09,507.38,9.31,6.19;12,118.53,663.00,9.31,6.19;12,147.01,663.00,9.31,6.19;12,175.50,663.00,9.31,6.19;12,203.98,663.00,9.31,6.19;12,232.47,663.00,9.31,6.19;12,260.96,663.00,9.31,6.19;12,155.73,671.33,77.34,7.57;12,93.37,594.82,7.57,14.23;12,93.37,552.96,7.57,39.58;12,88.99,692.49,187.80,8.90;12,102.00,704.45,47.77,8.90;12,332.27,662.11,16.94,6.19;12,332.27,636.27,16.94,6.19;12,332.27,610.45,16.94,6.19;12,336.18,584.61,13.03,6.19;12,336.18,558.78,13.03,6.19;12,336.18,532.95,13.03,6.19;12,355.00,676.47,9.31,6.19;12,382.10,676.47,9.31,6.19;12,409.20,676.47,9.31,6.19;12,436.29,676.47,9.31,6.19;12,463.39,676.47,9.31,6.19;12,490.48,676.47,9.31,6.19;12,388.73,684.79,77.34,7.57;12,322.55,613.61,7.57,35.70;12,322.55,594.96,7.57,16.37;12,322.55,578.46,7.57,14.23;12,322.55,536.60,7.57,39.58;12,318.18,704.45,187.80,8.90"><head></head><label></label><figDesc>prevalence (f) Fitted residuals simplified continuous sweep</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="12,89.29,721.95,416.69,8.91;12,88.93,733.91,417.04,8.91;12,89.29,745.86,184.82,8.91"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Quantifiers against true prevalence among 5, 000 test sets. The fitted red lines plot the line where the estimated prevalence is equal to the true prevalence. The blue lines plot a fitted GAM-model representing the bias among the prevalences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="13,89.29,452.73,416.69,8.91;13,89.29,464.68,416.69,8.91;13,89.29,476.64,51.88,8.91;13,141.17,480.67,1.35,6.23;13,145.44,476.64,21.72,8.91;13,167.16,480.67,2.01,6.23;13,172.10,476.64,333.88,8.91;13,89.29,488.59,416.68,8.91;13,89.29,500.55,323.50,8.91;13,416.38,497.71,3.24,6.23;13,416.38,507.27,3.24,6.23;13,420.82,500.55,85.17,8.91;13,89.29,512.50,145.33,8.91"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3:Nine examples of the adjusted-count integral. The black line denotes the estimated adjusted count quantifier at threshold 𝜃 for a development set. The orange vertical lines are the two decision boundaries 𝜃 𝑙 and 𝜃 𝑟 and the grey horizontal lines denote the prevalence of each development set. The blue dots show the adjusted-count estimates from the median sweep that pass the criterion that the difference between the true positive rate and the false positive rate is larger than1  4 , and the red dots are the estimates that fail the criterion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,89.02,122.21,417.62,65.69"><head></head><label></label><figDesc>The feature vector 𝑥 consists of 𝑝 (numeric) covariate values. Denote a training set of size 𝑛 train by 𝐷 train where the feature vectors are independent and identically distributed (i.i.d.) with density 𝑓 train . Moreover, we denote a validation set of size 𝑛 val by 𝐷 val with corresponding density 𝑓 val . Last, denote the test set of size 𝑛 test by 𝐷 test with density 𝑓 test .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,88.99,90.67,416.99,82.54"><head>Table 1</head><label>1</label><figDesc>Comparing summary statistics between the median sweep and continuous sweep quantifiers with the test sets.</figDesc><table coords="11,173.64,128.03,247.99,45.17"><row><cell>Quantifier</cell><cell cols="2">Bias Variance</cell><cell>MAE</cell></row><row><cell>Continuous sweep</cell><cell>0.02565</cell><cell cols="2">0.00302 0.0473</cell></row><row><cell cols="2">Simplified continuous sweep -0.00916</cell><cell cols="2">0.00151 0.0317</cell></row><row><cell>Median sweep</cell><cell>0.00650</cell><cell cols="2">0.00129 0.0289</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="15,112.66,228.01,393.33,9.74;15,112.66,241.56,182.59,9.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,317.76,228.01,156.62,9.74">A review on quantification learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Castaño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Coz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,482.67,228.01,23.32,9.74;15,112.66,241.56,88.43,9.74">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="74" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,255.11,393.33,9.74;15,112.66,268.66,394.61,9.74;15,112.31,282.21,395.51,9.74;15,112.66,295.76,141.70,9.74" xml:id="b1">
	<analytic>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Forman</surname></persName>
		</author>
		<idno type="DOI">10.1007/11564096_55</idno>
		<idno>DOI: 10.1007/11564096_55</idno>
		<ptr target="http://link.springer.com/10.1007/11564096_55" />
	</analytic>
	<monogr>
		<title level="m" coord="15,168.90,255.11,294.67,9.74">Counting Positives Accurately Despite Inaccurate Classification</title>
		<title level="s" coord="15,361.47,282.21,146.35,9.74;15,112.66,295.76,20.33,9.74">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg, Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3720</biblScope>
			<biblScope unit="page" from="564" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,309.31,393.32,9.74;15,112.66,322.86,395.01,9.74;15,112.66,336.40,141.37,9.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,165.78,309.31,204.56,9.74">Quantifying counts and costs via classification</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Forman</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-008-0097-y</idno>
		<ptr target="http://link.springer.com/10.1007/s10618-008-0097-y.doi:10.1007/s10618-008-0097-y" />
	</analytic>
	<monogr>
		<title level="j" coord="15,378.24,309.31,127.74,9.74;15,112.66,322.86,45.86,9.74">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="164" to="206" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,349.95,393.32,9.74;15,112.66,363.50,129.34,9.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="15,441.78,349.95,64.20,9.74;15,112.66,363.50,19.92,9.74">Quantification trees</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Milli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rossetti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="528" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,377.05,205.71,9.74" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m" coord="15,174.58,377.05,112.69,9.74">Statistical learning theory</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,390.60,394.61,9.74;15,112.31,404.15,262.04,9.74" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moreo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2111.11249</idno>
		<ptr target="https://arxiv.org/abs/2111.11249.doi:10.48550/ARXIV.2111.11249" />
		<title level="m" coord="15,258.50,390.60,166.95,9.74">Lequa@clef2022: Learning to quantify</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,417.70,395.16,9.74;15,112.66,431.25,394.61,9.74;15,112.66,444.80,50.45,9.74" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schumacher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Strohmaier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lemmerich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03223[cs</idno>
		<idno>arXiv: 2103.03223</idno>
		<ptr target="http://arxiv.org/abs/2103.03223" />
		<title level="m" coord="15,325.64,417.70,182.19,9.74;15,112.66,431.25,56.83,9.74">A comparative evaluation of quantification methods</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,458.35,393.32,9.74;15,112.66,471.90,356.43,9.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="15,317.17,458.35,188.81,9.74;15,112.66,471.90,92.11,9.74">Comparing correction methods to reduce misclassification bias</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kloos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Meertens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Scholtus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Karch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="64" to="90" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,485.45,393.32,9.74;15,112.66,498.99,395.01,9.74;15,112.66,512.54,182.13,9.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,159.91,485.45,346.07,9.74;15,112.66,498.99,38.01,9.74">A new generic method to improve machine learning applications in official statistics</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kloos</surname></persName>
		</author>
		<idno type="DOI">10.3233/sji-210885</idno>
		<ptr target="http://dx.doi.org/10.3233/SJI-210885.doi:10.3233/sji-210885" />
	</analytic>
	<monogr>
		<title level="j" coord="15,158.38,498.99,135.53,9.74">Statistical Journal of the IAOS</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1181" to="1196" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,526.09,393.32,9.74;15,112.66,539.64,356.07,9.74" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="15,392.91,526.09,113.07,9.74;15,112.66,539.64,324.87,9.74">Understanding the output quality of official statistics that are based on machine learning algorithms</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">A</forename><surname>Meertens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G H</forename><surname>Diks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">W</forename><surname>Herik</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Takes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,553.19,393.32,9.74;15,112.66,566.74,133.99,9.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,161.16,553.19,191.81,9.74">Fisher consistency for prior probability shift</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tasche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,360.52,553.19,145.46,9.74;15,112.66,566.74,39.91,9.74">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="3338" to="3369" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,580.29,394.61,9.74;15,112.31,593.84,262.04,9.74" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="15,160.78,580.29,264.67,9.74">Minimising quantifier variance under prior probability shift</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tasche</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2107.08209</idno>
		<ptr target="https://arxiv.org/abs/2107.08209.doi:10.48550/ARXIV.2107.08209" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,607.39,393.32,9.74;15,112.39,620.94,340.77,9.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,457.32,607.39,48.66,9.74;15,112.39,620.94,162.32,9.74">A unifying view on dataset shift in classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Moreno-Torres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Raeder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alaiz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,282.63,620.94,86.59,9.74">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="521" to="530" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,634.49,393.32,9.74;15,112.66,648.04,366.26,9.74" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="15,176.11,634.49,329.87,9.74;15,112.66,648.04,109.22,9.74">R: A Language and Environment for Statistical Computing, R Foundation for Statistical Computing</title>
		<author>
			<persName coords=""><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<ptr target="https://www.R-project.org/" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<pubPlace>Vienna, Austria</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,661.58,394.53,9.74;16,112.28,88.09,394.91,9.74;16,112.66,101.64,394.53,9.74;16,112.14,115.19,395.53,9.74;16,112.66,128.74,189.67,9.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,112.14,115.19,111.71,9.74">Welcome to the tidyverse</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wickham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Averick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mcgowan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>François</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Grolemund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ooms</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Spinu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wilke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yutani</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.01686</idno>
		<ptr target="http://dx.doi.org/10.21105/joss.01686.doi:10.21105/joss.01686" />
	</analytic>
	<monogr>
		<title level="j" coord="16,232.13,115.19,95.95,9.74">J. Open Source Softw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">1686</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,142.29,393.32,9.74;16,112.66,155.84,340.35,9.74" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="16,218.85,142.29,287.13,9.74;16,112.66,155.84,151.27,9.74">Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wickham</surname></persName>
		</author>
		<ptr target="https://www.tidymodels.org" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,169.38,394.53,9.74;16,112.66,182.93,216.33,9.74" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="16,310.41,169.38,150.67,9.74">The elements of statistical learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-84858-7</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,196.48,393.32,9.74;16,112.66,210.03,222.85,9.74" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="16,318.03,196.48,187.95,9.74;16,112.66,210.03,16.00,9.74">kernlab -an S4 package for kernel methods in R</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zeileis</surname></persName>
		</author>
		<ptr target="http://www.jstatsoft.org/v11/i09/" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,223.58,393.52,9.74;16,112.66,237.13,102.05,9.74" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="16,160.02,223.58,95.99,9.74">ks: Kernel Smoothing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Duong</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=ks,rpackageversion1.13.4" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
