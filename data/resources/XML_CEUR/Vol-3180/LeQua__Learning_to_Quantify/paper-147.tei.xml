<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,389.19,15.42;1,89.29,106.66,396.68,15.42;1,89.29,128.58,127.65,15.43">UniOviedo(Team2) at LeQua 2022: Comparison of traditional quantifiers and a new method based on Energy Distance</title>
				<funder ref="#_gPnTe7J">
					<orgName type="full">MINECO</orgName>
				</funder>
				<funder>
					<orgName type="full">MINECO (Ministerio de Economía y Competitividad)</orgName>
				</funder>
				<funder ref="#_3NSMY7z">
					<orgName type="full">FEDER (Fondo Europeo de Desarrollo Regional)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.10,156.89,84.36,11.96"><forename type="first">Juan</forename><surname>José Del Coz</surname></persName>
							<email>juanjo@uniovi.es</email>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence Center at Gijón</orgName>
								<orgName type="institution">University of Oviedo</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,389.19,15.42;1,89.29,106.66,396.68,15.42;1,89.29,128.58,127.65,15.43">UniOviedo(Team2) at LeQua 2022: Comparison of traditional quantifiers and a new method based on Energy Distance</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">02146110C82288997FD81C61DA4B401C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>T E X quantification</term>
					<term>prevalence estimation</term>
					<term>energy distance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The idea of this team was to compare the performance of some of the most important quantification methods and a new approach based on the Energy Distance that has been proposed by our group recently. This paper describes this method, called 𝐸𝐷𝑦, and the experimentation carried out to tackle the vector subtasks (T1A and T1B) of LeQua 2022 quantification competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Motivation</head><p>Our main intention in this competition was to analyze the behavior of a new quantification algorithm devised by our group. This method, called 𝐸𝐷𝑦, is unpublished yet and will be briefly described in Section 2.2. To assess its performance, we compare it with some of the most popular quantification algorithms, see Section 2.1.</p><p>We just focus in vector subtasks (T1A and T1B) because we are not experts on deep learning that is more or less required to tackle the subtasks using raw documents (T2A and T2B). According to our previous studies using 𝐸𝐷𝑦 over benchmark data, our hopes of being truly competitive were centered on T1B, because 𝐸𝐷𝑦 usually provides better results for multiclass quantification tasks. In fact, we only submitted the scores of 𝐸𝐷𝑦 for T1B. For the binary subtask T1A we employed 𝐻𝐷𝑦 <ref type="bibr" coords="1,162.35,479.15,12.69,10.91" target="#b0">[1]</ref> with some customization. We achieved a broze medal in both competitions, but as we will see later, our results could easily have been better in subtask T1B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>Before describing the methods used, we introduce here some notation. In the general setting, quantification methods learn from a training set, 𝐷 = {(𝑥 𝑖 , 𝑦 𝑖 )} 𝑛 𝑖=1 , in which 𝑥 𝑖 is the description of an instance using the features of the input space, and 𝑦 𝑖 is its class. In the tasks of LeQua competition 𝑦 𝑖 ∈ {𝑐 1 , . . . , 𝑐 𝑘 } being 𝑘 = 2 for binary tasks TA and 𝑘 = 28 for multiclass quantification tasks TB. The goal of quantification learning is to automatically obtain models able to predict the prevalence of all classes, 𝑝 ^= {𝑝 ^1, . . . , 𝑝 ^𝑘}, given a set of unlabeled examples, 𝑇 = {𝑥 𝑗 } 𝑚 𝑗=1 , ensuring that 𝑝 𝑙 ^≥ 0 and ∑︀ 𝑘 𝑙=1 𝑝 𝑙 ^= 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">SOTA quantifiers</head><p>There are several quantification methods that can be considered state-of-the-art, see <ref type="bibr" coords="2,472.70,163.43,11.56,10.91" target="#b1">[2]</ref>. We chose the best performing methods according to our experience, namely:</p><p>• 𝐸𝑀 <ref type="bibr" coords="2,140.26,199.75,11.58,10.91" target="#b2">[3]</ref>. This method is based on expectation-maximization algorithm in which the parameters to be estimated are the class prior probabilities. This method is denoted as 𝐸𝑀 𝑄 in QuaPy <ref type="bibr" coords="2,192.65,226.84,12.84,10.91" target="#b3">[4]</ref> and 𝑆𝐿𝐷 in the baseline results given by the organizers<ref type="foot" coords="2,458.32,225.09,3.71,7.97" target="#foot_0">1</ref> . • 𝐻𝐷𝑦 <ref type="bibr" coords="2,144.24,241.09,11.28,10.91" target="#b0">[1]</ref>. This is a matching distribution quantifier that uses histograms to represent the distributions and the Hellinger Distance to measure histogram similarity.</p><p>However, while we used the 𝐸𝑀 method without any major customization, we tested two possible improvements for the 𝐻𝐷𝑦 method:</p><p>1. A different way of computing the histograms. The original method is based on equal-width bins. We tested also equal-count bins, considering the examples of all the classes. 2. Taken into account the results reported in <ref type="bibr" coords="2,299.86,340.23,11.28,10.91" target="#b4">[5]</ref>, we also tested Topsøe as similarity measure.</p><p>We improved the 𝐻𝐷𝑦 results provided by the organizers using these modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">EDy</head><p>𝐸𝐷𝑦 is based on the method presented in <ref type="bibr" coords="2,277.51,411.45,11.35,10.91" target="#b5">[6]</ref>. It is also a matching distribution algorithm, like 𝐻𝐷𝑦, but the distributions are represented by the complete sets of examples (the sets with the training examples for each class, denoted as 𝐷 𝑐 𝑙 , and the testing set 𝑇 ), and the metric is the Energy Distance (ED). Formally, 𝐸𝐷𝑦 minimizes the ED between 𝑇 and the weighted mixture distribution,</p><formula xml:id="formula_0" coords="2,206.94,477.43,299.05,12.68">𝐷 ′ = 𝐷 𝑐 1 • 𝑝 ^1 + 𝐷 𝑐 2 • 𝑝 ^2 + . . . + 𝐷 𝑐 𝑘 • 𝑝 ^𝑘.<label>(1)</label></formula><p>with respect to 𝑝 ^. That is:</p><formula xml:id="formula_1" coords="2,174.46,520.22,331.52,17.08">min 𝑝 ^1,...,𝑝 ^𝑘 2 • E 𝑥 𝑖 ∽𝐷 ′ ,𝑥 𝑗 ∽𝑇 𝛿(𝑥 𝑖 , 𝑥 𝑗 )<label>(2)</label></formula><formula xml:id="formula_2" coords="2,223.40,541.86,197.41,16.07">-E 𝑥 𝑖 ,𝑥 ′ 𝑖 ∽𝐷 ′ 𝛿(𝑥 𝑖 , 𝑥 ′ 𝑖 ) -E 𝑥 𝑗 ,𝑥 ′ 𝑗 ∽𝑇 𝛿(𝑥 𝑗 , 𝑥 ′ 𝑗 ),</formula><p>where 𝛿 is a distance. The last term can be removed (it does not depend on 𝑝 ^), so we have:</p><formula xml:id="formula_3" coords="2,179.12,588.64,326.86,33.98">min 𝑝 ^1,...,𝑝 ^𝑘 2 𝑘 ∑︁ 𝑙=1 𝑝 ^𝑙 E 𝑥 𝑖 ∽𝐷 𝑐 𝑙 ,𝑥 𝑗 ∽𝑇 𝛿(𝑥 𝑖 , 𝑥 𝑗 )<label>(3)</label></formula><formula xml:id="formula_4" coords="2,228.06,627.33,188.10,34.03">- 𝑘 ∑︁ 𝑙=1 𝑘 ∑︁ 𝑙 ′ =1 𝑝 ^𝑙 𝑝 ^𝑙′ E 𝑥 𝑖 ∽𝐷 𝑐 𝑙 ,𝑥 ′ 𝑖 ∽𝐷 𝑐 𝑙 ′ 𝛿(𝑥 𝑖 , 𝑥 ′ 𝑖 ).</formula><p>The difference between 𝐸𝐷𝑦 and the method introduced in [6] is how to compute 𝛿(𝑥 𝑖 , 𝑥 𝑗 ). The authors in <ref type="bibr" coords="3,153.99,100.52,12.68,10.91" target="#b5">[6]</ref> propose to use the actual features of the input space. We denote such approach as 𝐸𝐷𝑋. Our proposal is to use the predictions of a classifier, ℎ. In symbols, 𝛿(ℎ(𝑥 𝑖 ), ℎ(𝑥 𝑗 )), the same predictions used by 𝐸𝑀 and 𝐻𝐷𝑦. As function 𝛿 we used the Manhattan distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>The first aspect of our experiments<ref type="foot" coords="3,246.07,183.62,3.71,7.97" target="#foot_1">2</ref> was to select the best classifier because all the compared algorithms require a classifier. We tested several classifiers using just the training set of subtask T1A, including Logistic Regression, Random Forest, Support Vector Machines, XGboost, Naive Bayes and Gaussian processes. The best one was Logistic Regression (LR) rather clearly. Then we adjusted its regularization parameter resulting than the best value was 𝐶 = 0.01. We employed this classifier for the rest of the experiments including subtask 𝑇 1𝐵.</p><p>Another important factor according to our experience is how to estimate the distributions. It is well-described in the literature, for instance for 𝐴𝐶 method <ref type="bibr" coords="3,388.19,280.22,11.58,10.91" target="#b6">[7]</ref>, that it is better to use some sort of cross-validation (CV). Our approach in our recent papers is to use such CV to estimate both, the distributions of the training data, but also for the testing sets, averaging the predictions of the classifiers that compose the CV model. This works better that learning a separated classifier to estimate any value of the testing bags. We used 20 folds for subtask T1A and 10 for T1B.</p><p>The only compared method that has hyperparameters is 𝐻𝐷𝑦:</p><p>• Similarity Measure. Two alternatives: HD, as the original method 𝐻𝐷𝑦, and Topsøe (method 𝐷𝑦𝑆-𝑇 𝑆 in <ref type="bibr" coords="3,211.70,392.04,11.10,10.91" target="#b4">[5]</ref>). We will denoted this last method as 𝑃 𝐷𝐹 𝑦𝑇 , because it uses histograms (𝑃 𝐷𝐹 ), the predictions from a classifier (𝑦) and Topsøe (𝑇 ). • Number of bins. We tried the following group of values {30, 40, 50}.</p><p>• Method used for computing the cut points for the histograms: equal-width or equal-count.</p><p>We just tried these six choices to select the best combination for 𝐻𝐷𝑦 and 𝑃 𝐷𝐹 𝑦𝑇 .</p><p>Recall that the target performance measure is the Mean of the Relative Absolute Error (MRAE):</p><formula xml:id="formula_5" coords="3,221.76,482.10,284.23,33.98">𝑀 𝑅𝐴𝐸(𝑝, 𝑝 ^) = 1 |𝑘| 𝑘 ∑︁ 𝑙=1 |𝑝 𝑙 ^-𝑝 𝑙 | 𝑝 𝑙 ,<label>(4)</label></formula><p>where 𝑝 𝑙 and 𝑝 𝑙 ^are the real and the predicted prevalences for class 𝑙. RAE may be undefined when 𝑝 𝑙 = 0, so both prevalences are smoothed before computing it <ref type="bibr" coords="3,394.41,535.06,11.56,10.91" target="#b7">[8]</ref>:</p><formula xml:id="formula_6" coords="3,222.23,551.81,283.76,24.43">𝑠𝑚𝑜𝑜𝑡ℎ(𝑝) = 𝜖 + 𝑝 𝜖𝑘 + 1 , 𝜖 = 1 2𝑚 .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Subtask T1A</head><p>For this task the equal-count method works better. The results using LR over the validation set are those in Table <ref type="table" coords="3,172.93,624.60,3.81,10.91" target="#tab_0">1</ref>. The first conclusion is that 𝐻𝐷𝑦 and 𝑃 𝐷𝐹 𝑦𝑇 are the best performers.</p><p>There is no much difference between them but 𝑃 𝐷𝐹 𝑦𝑇 seems slightly better. This is in line with the conclusions in <ref type="bibr" coords="3,195.58,651.69,11.43,10.91" target="#b4">[5]</ref>. 𝐸𝐷𝑦 is clearly outperformed in terms of MRAE, but its performance is similar to 𝐻𝐷𝑦 in terms of MAE. Moreover, it is pretty clear from the results in Table <ref type="table" coords="4,396.36,436.21,5.17,10.91" target="#tab_0">1</ref> that the scores of 𝐸𝑀 are rather bad because it requires well-calibrated posterior probabilities. Thus, we used the CalibratedCV object of sklearn to obtain calibrated probabilities. The scores of such experiment are in Table <ref type="table" coords="4,142.84,476.85,3.66,10.91">2</ref>. 𝐸𝑀 clearly improves but it performs worse than 𝑃 𝐷𝐹 𝑦𝑇 . Notice that the score of 𝐸𝑀 is just slightly better than that provided by the organizers (0.13775 vs. 0.1393).</p><p>Taking into account all these results, we finally selected 𝑃 𝐷𝐹 𝑦𝑇 with 40 bins of equal-count using Calibrated Logistic Regression with 𝐶 = 0.01. Notice that 𝑃 𝐷𝐹 𝑦𝑇 obtains better results that the original version of 𝐻𝐷𝑌 provided by the organizers (0.12701 vs. 0.1767).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Subtask T1B</head><p>Due to the lack of time, we just tried here basically the same configuration of the classifier selected for subtask T1A in combination with the OneVsRestClassifier provided by sklearn. The only changes were: i) we had to reduce the number of folds for the cross-validation used to 10 folds because the smallest class has 14 examples, and ii) for 𝐻𝐷𝑦 the best bin strategy was equal-width and the number of bins tested were {4, 8, 16} because the performance tended to decrease as the number of bins increased in this case. Notice that 𝑃 𝐷𝐹 𝑦𝑇 could not be employed here because it uses search (not optimization) for computing the final prevalences (exhaustive search is not suitable because the searching space is [0, 1] 28 and other methods that should have been implemented, such as genetic algorithms, do not guarantee to find the optimal solution). When we performed this experiment we committed a terrible mistake: the value used for the parameter 𝜖 of MRAE was 0.002 (the one for substask T1A), instead of the correct value 0.0005. The results of such incorrect experiment are in Table <ref type="table" coords="5,366.66,533.12,3.79,10.91" target="#tab_1">3</ref>. In such circumstances, 𝐸𝐷𝑦 seemed the best method: its performance was much better than the rest of approaches, including the baselines provided by the organizers and the results of HistNet (the method of the other team from the U. of Oviedo). Thus we submitted the predictions of 𝐸𝐷𝑦 to the competition. But the problem was of course the value of 𝜖. The results over the validation set using the correct value are in Table <ref type="table" coords="5,202.06,600.86,3.66,10.91" target="#tab_2">4</ref>. In this case, 𝐸𝑀 performs better in terms of MRAE but worse than 𝐸𝐷𝑦 for MAE. In both cases, their results are worse than those of the two best competitors.</p><p>After exchanging some emails with TU Dortmund University team, we did one last experiment. Instead of using OneVsRestClassifier and Calibrated Logistic Regression we just applied a plain Logistic Classifier in combination with the same cross-validation estimation procedure (10 folds). The results of 𝐸𝐷𝑌 would improve significantly, see Table <ref type="table" coords="5,388.01,668.61,3.76,10.91" target="#tab_3">5</ref>. Also the scores of 𝐻𝐷𝑦 are very competitive, while those of 𝐸𝑀 are much worse as it occurred in subtask T1A when the posteriors were not calibrated. If we had sent the predictions of this version of 𝐸𝐷𝑦 the scores over the test set would have been MRAE 0.864787, MAE 0.00994 which are better than those of the winning team of the competition (MRAE 0.879870, MAE 0.011733).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Conclusions</head><p>We have drawn several interesting conclusions from our participation in LeQua:</p><p>1. To obtain good results with quantification algorithms that rely on the use of a classifier it is crucial to select the best classifier-quantifier combination. Obviously, not always the same classifier is the most appropriate one for all quantification algorithms. 2. This implies that quantification competitions are even more complex than classification ones. There are more elements to be adjusted: select a combination of a classifier and a quantifier and adjust their hyperparameters. The search space is sometimes doubled. 3. 𝐸𝑀 is a very good quantification algorithm but is very sensitive to the classifier calibration. Other methods are more robust in this sense and work well with more classifiers. 4. 𝐸𝐷𝑦 seems a good approach for multiclass quantification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,304.25,305.46"><head>Table 1</head><label>1</label><figDesc>Results over the validation set of subtask T1A using Logistic Regression</figDesc><table coords="4,88.99,122.10,304.25,273.85"><row><cell>Method</cell><cell>MRAE</cell><cell>MAE</cell></row><row><cell>𝐸𝑀</cell><cell>1.19731</cell><cell>0.22649</cell></row><row><cell>𝐻𝐷𝑦 (30 bins)</cell><cell>0.15273</cell><cell>0.02570</cell></row><row><cell>𝐻𝐷𝑦 (40 bins)</cell><cell>0.13941</cell><cell>0.02639</cell></row><row><cell>𝐻𝐷𝑦 (50 bins)</cell><cell>0.14917</cell><cell>0.02748</cell></row><row><cell cols="3">𝑃 𝐷𝐹 𝑦𝑇 (30 bins) 0.13542 0.02411</cell></row><row><cell cols="2">𝑃 𝐷𝐹 𝑦𝑇 (40 bins) 0.13225</cell><cell>0.02480</cell></row><row><cell cols="3">𝑃 𝐷𝐹 𝑦𝑇 (50 bins) 0.13112 0.02469</cell></row><row><cell>𝐸𝐷𝑦</cell><cell>0.21878</cell><cell>0.02676</cell></row><row><cell>Table 2</cell><cell></cell><cell></cell></row><row><cell cols="3">Results over the validation set of T1A using Calibrated Logistic Regression</cell></row><row><cell>Method</cell><cell>MRAE</cell><cell>MAE</cell></row><row><cell>𝐸𝑀</cell><cell>0.13775</cell><cell>0.02374</cell></row><row><cell>𝐻𝐷𝑦 (30 bins)</cell><cell>0.18334</cell><cell>0.03077</cell></row><row><cell>𝐻𝐷𝑦 (40 bins)</cell><cell>0.19601</cell><cell>0.03561</cell></row><row><cell>𝐻𝐷𝑦 (50 bins)</cell><cell>0.20383</cell><cell>0.04044</cell></row><row><cell cols="3">𝑃 𝐷𝐹 𝑦𝑇 (30 bins) 0.13025 0.02425</cell></row><row><cell cols="3">𝑃 𝐷𝐹 𝑦𝑇 (40 bins) 0.12701 0.02470</cell></row><row><cell cols="2">𝑃 𝐷𝐹 𝑦𝑇 (50 bins) 0.12825</cell><cell>0.02552</cell></row><row><cell>𝐸𝐷𝑦</cell><cell>0.21586</cell><cell>0.02692</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,90.49,370.78,105.75"><head>Table 3</head><label>3</label><figDesc>Results over the validation set of T1B using OVR(Calibrated LR) with 𝜖 = 0.002 (incorrect)</figDesc><table coords="5,222.10,122.10,151.07,74.13"><row><cell>Method</cell><cell>MRAE</cell><cell>MAE</cell></row><row><cell>𝐸𝑀</cell><cell>0.74921</cell><cell>0.01637</cell></row><row><cell>𝐻𝐷𝑦 (4 bins)</cell><cell>0.86716</cell><cell>0.01527</cell></row><row><cell>𝐻𝐷𝑦 (8 bins)</cell><cell>0.80127</cell><cell>0.01402</cell></row><row><cell cols="2">𝐻𝐷𝑦 (16 bins) 0.85876</cell><cell>0.01586</cell></row><row><cell>𝐸𝐷𝑦</cell><cell cols="2">0.68223 0.01173</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.99,218.48,367.63,105.75"><head>Table 4</head><label>4</label><figDesc>Results over the validation set of T1B using OVR(Calibrated LR) with 𝜖 = 0.0005 (correct)</figDesc><table coords="5,222.10,250.10,151.07,74.13"><row><cell>Method</cell><cell>MRAE</cell><cell>MAE</cell></row><row><cell>𝐸𝑀</cell><cell cols="2">1.12322 0.01637</cell></row><row><cell>𝐻𝐷𝑦 (4 bins)</cell><cell>1.47463</cell><cell>0.01527</cell></row><row><cell>𝐻𝐷𝑦 (8 bins)</cell><cell>1.33846</cell><cell>0.01402</cell></row><row><cell cols="2">𝐻𝐷𝑦 (16 bins) 1.39885</cell><cell>0.01586</cell></row><row><cell>𝐸𝐷𝑦</cell><cell cols="2">1.16777 0.01173</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,88.99,346.48,363.53,105.75"><head>Table 5</head><label>5</label><figDesc>Results over the validation set of T1B using Logistic Regression with 𝜖 = 0.0005 (correct)</figDesc><table coords="5,222.10,378.09,151.07,74.13"><row><cell>Method</cell><cell>MRAE</cell><cell>MAE</cell></row><row><cell>𝐸𝑀</cell><cell>2.35675</cell><cell>0.02811</cell></row><row><cell>𝐻𝐷𝑦 (4 bins)</cell><cell>0.95555</cell><cell>0.01158</cell></row><row><cell>𝐻𝐷𝑦 (8 bins)</cell><cell>1.07063</cell><cell>0.01257</cell></row><row><cell cols="2">𝐻𝐷𝑦 (16 bins) 1.19310</cell><cell>0.01494</cell></row><row><cell>𝐸𝐷𝑦</cell><cell cols="2">0.89837 0.00996</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,671.04,232.53,8.97"><p>https://github.com/HLT-ISTI/QuaPy/tree/lequa2022/LeQua2022</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,108.93,671.04,178.80,8.97"><p>Source code: https://github.com/jjdelcoz/QU-Ant</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was funded by <rs type="funder">MINECO (Ministerio de Economía y Competitividad)</rs> and <rs type="funder">FEDER (Fondo Europeo de Desarrollo Regional)</rs>, grant <rs type="grantNumber">PID2019-110742RB-I00</rs> (<rs type="funder">MINECO</rs><rs type="grantNumber">/FEDER</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3NSMY7z">
					<idno type="grant-number">PID2019-110742RB-I00</idno>
				</org>
				<org type="funding" xml:id="_gPnTe7J">
					<idno type="grant-number">/FEDER</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,107.59,439.25,398.40,10.91;6,107.59,452.79,285.00,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,334.89,439.25,171.10,10.91;6,107.59,452.79,94.00,10.91">Class distribution estimation based on the hellinger distance</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>González-Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alaiz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alegre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,209.84,452.79,93.74,10.91">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="page" from="146" to="164" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,466.34,399.60,10.91;6,107.20,479.89,208.72,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,342.46,466.34,160.26,10.91">A review on quantification learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Castaño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Del Coz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,107.20,479.89,114.57,10.91">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="74" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,493.44,398.40,10.91;6,107.59,506.99,349.83,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,294.02,493.44,211.96,10.91;6,107.59,506.99,175.54,10.91">Adjusting the Outputs of a Classifier to New a Priori Probabilities: A Simple Procedure</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Latinne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Decaestecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,291.82,506.99,91.81,10.91">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="21" to="41" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,520.54,399.60,10.91;6,107.59,534.09,398.40,10.91;6,107.59,547.64,154.07,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,260.26,520.54,242.54,10.91">Quapy: A python-based framework for quantification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moreo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,122.35,534.09,383.63,10.91;6,107.59,547.64,55.11,10.91">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4534" to="4543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,561.19,398.40,10.91;6,107.59,574.74,341.32,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,325.25,561.19,180.74,10.91;6,107.59,574.74,60.26,10.91">Dys: A framework for mixture models in quantification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Maletzke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cherman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,190.70,574.74,108.08,10.91">Proceedings of the AAAI</title>
		<meeting>the AAAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4552" to="4560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,588.29,400.24,10.91;6,107.59,601.84,398.40,10.91;6,107.26,615.39,68.33,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,318.76,588.29,189.06,10.91;6,107.59,601.84,253.62,10.91">Computationally efficient class-prior estimation under class balance change using energy distance</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kawakubo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Du Plessis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,369.47,601.84,123.61,10.91">IEICE Tran. on Inf. and Sys</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="176" to="186" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,628.93,398.40,10.91;6,107.59,642.48,128.93,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,161.36,628.93,207.24,10.91">Quantifying counts and costs via classification</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Forman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,376.56,628.93,129.43,10.91;6,107.59,642.48,45.00,10.91">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="164" to="206" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,656.03,398.40,10.91;6,107.59,669.58,159.13,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,168.70,656.03,275.41,10.91">Evaluation measures for quantification: an axiomatic approach</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,452.85,656.03,53.13,10.91;6,107.59,669.58,75.20,10.91">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="255" to="288" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
