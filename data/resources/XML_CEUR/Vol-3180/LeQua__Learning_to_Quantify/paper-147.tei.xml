<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,389.19,15.42;1,89.29,106.66,396.68,15.42;1,89.29,128.58,127.65,15.43">UniOviedo(Team2) at LeQua 2022: Comparison of traditional quantifiers and a new method based on Energy Distance</title>
				<funder ref="#_gPnTe7J">
					<orgName type="full">MINECO</orgName>
				</funder>
				<funder>
					<orgName type="full">MINECO (Ministerio de EconomÃ­a y Competitividad)</orgName>
				</funder>
				<funder ref="#_3NSMY7z">
					<orgName type="full">FEDER (Fondo Europeo de Desarrollo Regional)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.10,156.89,84.36,11.96"><forename type="first">Juan</forename><surname>JosÃ© Del Coz</surname></persName>
							<email>juanjo@uniovi.es</email>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence Center at GijÃ³n</orgName>
								<orgName type="institution">University of Oviedo</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,389.19,15.42;1,89.29,106.66,396.68,15.42;1,89.29,128.58,127.65,15.43">UniOviedo(Team2) at LeQua 2022: Comparison of traditional quantifiers and a new method based on Energy Distance</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">02146110C82288997FD81C61DA4B401C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>T E X quantification</term>
					<term>prevalence estimation</term>
					<term>energy distance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The idea of this team was to compare the performance of some of the most important quantification methods and a new approach based on the Energy Distance that has been proposed by our group recently. This paper describes this method, called ğ¸ğ·ğ‘¦, and the experimentation carried out to tackle the vector subtasks (T1A and T1B) of LeQua 2022 quantification competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Motivation</head><p>Our main intention in this competition was to analyze the behavior of a new quantification algorithm devised by our group. This method, called ğ¸ğ·ğ‘¦, is unpublished yet and will be briefly described in Section 2.2. To assess its performance, we compare it with some of the most popular quantification algorithms, see Section 2.1.</p><p>We just focus in vector subtasks (T1A and T1B) because we are not experts on deep learning that is more or less required to tackle the subtasks using raw documents (T2A and T2B). According to our previous studies using ğ¸ğ·ğ‘¦ over benchmark data, our hopes of being truly competitive were centered on T1B, because ğ¸ğ·ğ‘¦ usually provides better results for multiclass quantification tasks. In fact, we only submitted the scores of ğ¸ğ·ğ‘¦ for T1B. For the binary subtask T1A we employed ğ»ğ·ğ‘¦ <ref type="bibr" coords="1,162.35,479.15,12.69,10.91" target="#b0">[1]</ref> with some customization. We achieved a broze medal in both competitions, but as we will see later, our results could easily have been better in subtask T1B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>Before describing the methods used, we introduce here some notation. In the general setting, quantification methods learn from a training set, ğ· = {(ğ‘¥ ğ‘– , ğ‘¦ ğ‘– )} ğ‘› ğ‘–=1 , in which ğ‘¥ ğ‘– is the description of an instance using the features of the input space, and ğ‘¦ ğ‘– is its class. In the tasks of LeQua competition ğ‘¦ ğ‘– âˆˆ {ğ‘ 1 , . . . , ğ‘ ğ‘˜ } being ğ‘˜ = 2 for binary tasks TA and ğ‘˜ = 28 for multiclass quantification tasks TB. The goal of quantification learning is to automatically obtain models able to predict the prevalence of all classes, ğ‘ ^= {ğ‘ ^1, . . . , ğ‘ ^ğ‘˜}, given a set of unlabeled examples, ğ‘‡ = {ğ‘¥ ğ‘— } ğ‘š ğ‘—=1 , ensuring that ğ‘ ğ‘™ ^â‰¥ 0 and âˆ‘ï¸€ ğ‘˜ ğ‘™=1 ğ‘ ğ‘™ ^= 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">SOTA quantifiers</head><p>There are several quantification methods that can be considered state-of-the-art, see <ref type="bibr" coords="2,472.70,163.43,11.56,10.91" target="#b1">[2]</ref>. We chose the best performing methods according to our experience, namely:</p><p>â€¢ ğ¸ğ‘€ <ref type="bibr" coords="2,140.26,199.75,11.58,10.91" target="#b2">[3]</ref>. This method is based on expectation-maximization algorithm in which the parameters to be estimated are the class prior probabilities. This method is denoted as ğ¸ğ‘€ ğ‘„ in QuaPy <ref type="bibr" coords="2,192.65,226.84,12.84,10.91" target="#b3">[4]</ref> and ğ‘†ğ¿ğ· in the baseline results given by the organizers<ref type="foot" coords="2,458.32,225.09,3.71,7.97" target="#foot_0">1</ref> . â€¢ ğ»ğ·ğ‘¦ <ref type="bibr" coords="2,144.24,241.09,11.28,10.91" target="#b0">[1]</ref>. This is a matching distribution quantifier that uses histograms to represent the distributions and the Hellinger Distance to measure histogram similarity.</p><p>However, while we used the ğ¸ğ‘€ method without any major customization, we tested two possible improvements for the ğ»ğ·ğ‘¦ method:</p><p>1. A different way of computing the histograms. The original method is based on equal-width bins. We tested also equal-count bins, considering the examples of all the classes. 2. Taken into account the results reported in <ref type="bibr" coords="2,299.86,340.23,11.28,10.91" target="#b4">[5]</ref>, we also tested TopsÃ¸e as similarity measure.</p><p>We improved the ğ»ğ·ğ‘¦ results provided by the organizers using these modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">EDy</head><p>ğ¸ğ·ğ‘¦ is based on the method presented in <ref type="bibr" coords="2,277.51,411.45,11.35,10.91" target="#b5">[6]</ref>. It is also a matching distribution algorithm, like ğ»ğ·ğ‘¦, but the distributions are represented by the complete sets of examples (the sets with the training examples for each class, denoted as ğ· ğ‘ ğ‘™ , and the testing set ğ‘‡ ), and the metric is the Energy Distance (ED). Formally, ğ¸ğ·ğ‘¦ minimizes the ED between ğ‘‡ and the weighted mixture distribution,</p><formula xml:id="formula_0" coords="2,206.94,477.43,299.05,12.68">ğ· â€² = ğ· ğ‘ 1 â€¢ ğ‘ ^1 + ğ· ğ‘ 2 â€¢ ğ‘ ^2 + . . . + ğ· ğ‘ ğ‘˜ â€¢ ğ‘ ^ğ‘˜.<label>(1)</label></formula><p>with respect to ğ‘ ^. That is:</p><formula xml:id="formula_1" coords="2,174.46,520.22,331.52,17.08">min ğ‘ ^1,...,ğ‘ ^ğ‘˜ 2 â€¢ E ğ‘¥ ğ‘– âˆ½ğ· â€² ,ğ‘¥ ğ‘— âˆ½ğ‘‡ ğ›¿(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )<label>(2)</label></formula><formula xml:id="formula_2" coords="2,223.40,541.86,197.41,16.07">-E ğ‘¥ ğ‘– ,ğ‘¥ â€² ğ‘– âˆ½ğ· â€² ğ›¿(ğ‘¥ ğ‘– , ğ‘¥ â€² ğ‘– ) -E ğ‘¥ ğ‘— ,ğ‘¥ â€² ğ‘— âˆ½ğ‘‡ ğ›¿(ğ‘¥ ğ‘— , ğ‘¥ â€² ğ‘— ),</formula><p>where ğ›¿ is a distance. The last term can be removed (it does not depend on ğ‘ ^), so we have:</p><formula xml:id="formula_3" coords="2,179.12,588.64,326.86,33.98">min ğ‘ ^1,...,ğ‘ ^ğ‘˜ 2 ğ‘˜ âˆ‘ï¸ ğ‘™=1 ğ‘ ^ğ‘™ E ğ‘¥ ğ‘– âˆ½ğ· ğ‘ ğ‘™ ,ğ‘¥ ğ‘— âˆ½ğ‘‡ ğ›¿(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— )<label>(3)</label></formula><formula xml:id="formula_4" coords="2,228.06,627.33,188.10,34.03">- ğ‘˜ âˆ‘ï¸ ğ‘™=1 ğ‘˜ âˆ‘ï¸ ğ‘™ â€² =1 ğ‘ ^ğ‘™ ğ‘ ^ğ‘™â€² E ğ‘¥ ğ‘– âˆ½ğ· ğ‘ ğ‘™ ,ğ‘¥ â€² ğ‘– âˆ½ğ· ğ‘ ğ‘™ â€² ğ›¿(ğ‘¥ ğ‘– , ğ‘¥ â€² ğ‘– ).</formula><p>The difference between ğ¸ğ·ğ‘¦ and the method introduced in [6] is how to compute ğ›¿(ğ‘¥ ğ‘– , ğ‘¥ ğ‘— ). The authors in <ref type="bibr" coords="3,153.99,100.52,12.68,10.91" target="#b5">[6]</ref> propose to use the actual features of the input space. We denote such approach as ğ¸ğ·ğ‘‹. Our proposal is to use the predictions of a classifier, â„. In symbols, ğ›¿(â„(ğ‘¥ ğ‘– ), â„(ğ‘¥ ğ‘— )), the same predictions used by ğ¸ğ‘€ and ğ»ğ·ğ‘¦. As function ğ›¿ we used the Manhattan distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>The first aspect of our experiments<ref type="foot" coords="3,246.07,183.62,3.71,7.97" target="#foot_1">2</ref> was to select the best classifier because all the compared algorithms require a classifier. We tested several classifiers using just the training set of subtask T1A, including Logistic Regression, Random Forest, Support Vector Machines, XGboost, Naive Bayes and Gaussian processes. The best one was Logistic Regression (LR) rather clearly. Then we adjusted its regularization parameter resulting than the best value was ğ¶ = 0.01. We employed this classifier for the rest of the experiments including subtask ğ‘‡ 1ğµ.</p><p>Another important factor according to our experience is how to estimate the distributions. It is well-described in the literature, for instance for ğ´ğ¶ method <ref type="bibr" coords="3,388.19,280.22,11.58,10.91" target="#b6">[7]</ref>, that it is better to use some sort of cross-validation (CV). Our approach in our recent papers is to use such CV to estimate both, the distributions of the training data, but also for the testing sets, averaging the predictions of the classifiers that compose the CV model. This works better that learning a separated classifier to estimate any value of the testing bags. We used 20 folds for subtask T1A and 10 for T1B.</p><p>The only compared method that has hyperparameters is ğ»ğ·ğ‘¦:</p><p>â€¢ Similarity Measure. Two alternatives: HD, as the original method ğ»ğ·ğ‘¦, and TopsÃ¸e (method ğ·ğ‘¦ğ‘†-ğ‘‡ ğ‘† in <ref type="bibr" coords="3,211.70,392.04,11.10,10.91" target="#b4">[5]</ref>). We will denoted this last method as ğ‘ƒ ğ·ğ¹ ğ‘¦ğ‘‡ , because it uses histograms (ğ‘ƒ ğ·ğ¹ ), the predictions from a classifier (ğ‘¦) and TopsÃ¸e (ğ‘‡ ). â€¢ Number of bins. We tried the following group of values {30, 40, 50}.</p><p>â€¢ Method used for computing the cut points for the histograms: equal-width or equal-count.</p><p>We just tried these six choices to select the best combination for ğ»ğ·ğ‘¦ and ğ‘ƒ ğ·ğ¹ ğ‘¦ğ‘‡ .</p><p>Recall that the target performance measure is the Mean of the Relative Absolute Error (MRAE):</p><formula xml:id="formula_5" coords="3,221.76,482.10,284.23,33.98">ğ‘€ ğ‘…ğ´ğ¸(ğ‘, ğ‘ ^) = 1 |ğ‘˜| ğ‘˜ âˆ‘ï¸ ğ‘™=1 |ğ‘ ğ‘™ ^-ğ‘ ğ‘™ | ğ‘ ğ‘™ ,<label>(4)</label></formula><p>where ğ‘ ğ‘™ and ğ‘ ğ‘™ ^are the real and the predicted prevalences for class ğ‘™. RAE may be undefined when ğ‘ ğ‘™ = 0, so both prevalences are smoothed before computing it <ref type="bibr" coords="3,394.41,535.06,11.56,10.91" target="#b7">[8]</ref>:</p><formula xml:id="formula_6" coords="3,222.23,551.81,283.76,24.43">ğ‘ ğ‘šğ‘œğ‘œğ‘¡â„(ğ‘) = ğœ– + ğ‘ ğœ–ğ‘˜ + 1 , ğœ– = 1 2ğ‘š .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Subtask T1A</head><p>For this task the equal-count method works better. The results using LR over the validation set are those in Table <ref type="table" coords="3,172.93,624.60,3.81,10.91" target="#tab_0">1</ref>. The first conclusion is that ğ»ğ·ğ‘¦ and ğ‘ƒ ğ·ğ¹ ğ‘¦ğ‘‡ are the best performers.</p><p>There is no much difference between them but ğ‘ƒ ğ·ğ¹ ğ‘¦ğ‘‡ seems slightly better. This is in line with the conclusions in <ref type="bibr" coords="3,195.58,651.69,11.43,10.91" target="#b4">[5]</ref>. ğ¸ğ·ğ‘¦ is clearly outperformed in terms of MRAE, but its performance is similar to ğ»ğ·ğ‘¦ in terms of MAE. Moreover, it is pretty clear from the results in Table <ref type="table" coords="4,396.36,436.21,5.17,10.91" target="#tab_0">1</ref> that the scores of ğ¸ğ‘€ are rather bad because it requires well-calibrated posterior probabilities. Thus, we used the CalibratedCV object of sklearn to obtain calibrated probabilities. The scores of such experiment are in Table <ref type="table" coords="4,142.84,476.85,3.66,10.91">2</ref>. ğ¸ğ‘€ clearly improves but it performs worse than ğ‘ƒ ğ·ğ¹ ğ‘¦ğ‘‡ . Notice that the score of ğ¸ğ‘€ is just slightly better than that provided by the organizers (0.13775 vs. 0.1393).</p><p>Taking into account all these results, we finally selected ğ‘ƒ ğ·ğ¹ ğ‘¦ğ‘‡ with 40 bins of equal-count using Calibrated Logistic Regression with ğ¶ = 0.01. Notice that ğ‘ƒ ğ·ğ¹ ğ‘¦ğ‘‡ obtains better results that the original version of ğ»ğ·ğ‘Œ provided by the organizers (0.12701 vs. 0.1767).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Subtask T1B</head><p>Due to the lack of time, we just tried here basically the same configuration of the classifier selected for subtask T1A in combination with the OneVsRestClassifier provided by sklearn. The only changes were: i) we had to reduce the number of folds for the cross-validation used to 10 folds because the smallest class has 14 examples, and ii) for ğ»ğ·ğ‘¦ the best bin strategy was equal-width and the number of bins tested were {4, 8, 16} because the performance tended to decrease as the number of bins increased in this case. Notice that ğ‘ƒ ğ·ğ¹ ğ‘¦ğ‘‡ could not be employed here because it uses search (not optimization) for computing the final prevalences (exhaustive search is not suitable because the searching space is [0, 1] 28 and other methods that should have been implemented, such as genetic algorithms, do not guarantee to find the optimal solution). When we performed this experiment we committed a terrible mistake: the value used for the parameter ğœ– of MRAE was 0.002 (the one for substask T1A), instead of the correct value 0.0005. The results of such incorrect experiment are in Table <ref type="table" coords="5,366.66,533.12,3.79,10.91" target="#tab_1">3</ref>. In such circumstances, ğ¸ğ·ğ‘¦ seemed the best method: its performance was much better than the rest of approaches, including the baselines provided by the organizers and the results of HistNet (the method of the other team from the U. of Oviedo). Thus we submitted the predictions of ğ¸ğ·ğ‘¦ to the competition. But the problem was of course the value of ğœ–. The results over the validation set using the correct value are in Table <ref type="table" coords="5,202.06,600.86,3.66,10.91" target="#tab_2">4</ref>. In this case, ğ¸ğ‘€ performs better in terms of MRAE but worse than ğ¸ğ·ğ‘¦ for MAE. In both cases, their results are worse than those of the two best competitors.</p><p>After exchanging some emails with TU Dortmund University team, we did one last experiment. Instead of using OneVsRestClassifier and Calibrated Logistic Regression we just applied a plain Logistic Classifier in combination with the same cross-validation estimation procedure (10 folds). The results of ğ¸ğ·ğ‘Œ would improve significantly, see Table <ref type="table" coords="5,388.01,668.61,3.76,10.91" target="#tab_3">5</ref>. Also the scores of ğ»ğ·ğ‘¦ are very competitive, while those of ğ¸ğ‘€ are much worse as it occurred in subtask T1A when the posteriors were not calibrated. If we had sent the predictions of this version of ğ¸ğ·ğ‘¦ the scores over the test set would have been MRAE 0.864787, MAE 0.00994 which are better than those of the winning team of the competition (MRAE 0.879870, MAE 0.011733).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Conclusions</head><p>We have drawn several interesting conclusions from our participation in LeQua:</p><p>1. To obtain good results with quantification algorithms that rely on the use of a classifier it is crucial to select the best classifier-quantifier combination. Obviously, not always the same classifier is the most appropriate one for all quantification algorithms. 2. This implies that quantification competitions are even more complex than classification ones. There are more elements to be adjusted: select a combination of a classifier and a quantifier and adjust their hyperparameters. The search space is sometimes doubled. 3. ğ¸ğ‘€ is a very good quantification algorithm but is very sensitive to the classifier calibration. Other methods are more robust in this sense and work well with more classifiers. 4. ğ¸ğ·ğ‘¦ seems a good approach for multiclass quantification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,304.25,305.46"><head>Table 1</head><label>1</label><figDesc>Results over the validation set of subtask T1A using Logistic Regression</figDesc><table coords="4,88.99,122.10,304.25,273.85"><row><cell>Method</cell><cell>MRAE</cell><cell>MAE</cell></row><row><cell>ğ¸ğ‘€</cell><cell>1.19731</cell><cell>0.22649</cell></row><row><cell>ğ»ğ·ğ‘¦ (30 bins)</cell><cell>0.15273</cell><cell>0.02570</cell></row><row><cell>ğ»ğ·ğ‘¦ (40 bins)</cell><cell>0.13941</cell><cell>0.02639</cell></row><row><cell>ğ»ğ·ğ‘¦ (50 bins)</cell><cell>0.14917</cell><cell>0.02748</cell></row><row><cell cols="3">ğ‘ƒ ğ·ğ¹ ğ‘¦ğ‘‡ (30 bins) 0.13542 0.02411</cell></row><row><cell cols="2">ğ‘ƒ ğ·ğ¹ ğ‘¦ğ‘‡ (40 bins) 0.13225</cell><cell>0.02480</cell></row><row><cell cols="3">ğ‘ƒ ğ·ğ¹ ğ‘¦ğ‘‡ (50 bins) 0.13112 0.02469</cell></row><row><cell>ğ¸ğ·ğ‘¦</cell><cell>0.21878</cell><cell>0.02676</cell></row><row><cell>Table 2</cell><cell></cell><cell></cell></row><row><cell cols="3">Results over the validation set of T1A using Calibrated Logistic Regression</cell></row><row><cell>Method</cell><cell>MRAE</cell><cell>MAE</cell></row><row><cell>ğ¸ğ‘€</cell><cell>0.13775</cell><cell>0.02374</cell></row><row><cell>ğ»ğ·ğ‘¦ (30 bins)</cell><cell>0.18334</cell><cell>0.03077</cell></row><row><cell>ğ»ğ·ğ‘¦ (40 bins)</cell><cell>0.19601</cell><cell>0.03561</cell></row><row><cell>ğ»ğ·ğ‘¦ (50 bins)</cell><cell>0.20383</cell><cell>0.04044</cell></row><row><cell cols="3">ğ‘ƒ ğ·ğ¹ ğ‘¦ğ‘‡ (30 bins) 0.13025 0.02425</cell></row><row><cell cols="3">ğ‘ƒ ğ·ğ¹ ğ‘¦ğ‘‡ (40 bins) 0.12701 0.02470</cell></row><row><cell cols="2">ğ‘ƒ ğ·ğ¹ ğ‘¦ğ‘‡ (50 bins) 0.12825</cell><cell>0.02552</cell></row><row><cell>ğ¸ğ·ğ‘¦</cell><cell>0.21586</cell><cell>0.02692</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,90.49,370.78,105.75"><head>Table 3</head><label>3</label><figDesc>Results over the validation set of T1B using OVR(Calibrated LR) with ğœ– = 0.002 (incorrect)</figDesc><table coords="5,222.10,122.10,151.07,74.13"><row><cell>Method</cell><cell>MRAE</cell><cell>MAE</cell></row><row><cell>ğ¸ğ‘€</cell><cell>0.74921</cell><cell>0.01637</cell></row><row><cell>ğ»ğ·ğ‘¦ (4 bins)</cell><cell>0.86716</cell><cell>0.01527</cell></row><row><cell>ğ»ğ·ğ‘¦ (8 bins)</cell><cell>0.80127</cell><cell>0.01402</cell></row><row><cell cols="2">ğ»ğ·ğ‘¦ (16 bins) 0.85876</cell><cell>0.01586</cell></row><row><cell>ğ¸ğ·ğ‘¦</cell><cell cols="2">0.68223 0.01173</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.99,218.48,367.63,105.75"><head>Table 4</head><label>4</label><figDesc>Results over the validation set of T1B using OVR(Calibrated LR) with ğœ– = 0.0005 (correct)</figDesc><table coords="5,222.10,250.10,151.07,74.13"><row><cell>Method</cell><cell>MRAE</cell><cell>MAE</cell></row><row><cell>ğ¸ğ‘€</cell><cell cols="2">1.12322 0.01637</cell></row><row><cell>ğ»ğ·ğ‘¦ (4 bins)</cell><cell>1.47463</cell><cell>0.01527</cell></row><row><cell>ğ»ğ·ğ‘¦ (8 bins)</cell><cell>1.33846</cell><cell>0.01402</cell></row><row><cell cols="2">ğ»ğ·ğ‘¦ (16 bins) 1.39885</cell><cell>0.01586</cell></row><row><cell>ğ¸ğ·ğ‘¦</cell><cell cols="2">1.16777 0.01173</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,88.99,346.48,363.53,105.75"><head>Table 5</head><label>5</label><figDesc>Results over the validation set of T1B using Logistic Regression with ğœ– = 0.0005 (correct)</figDesc><table coords="5,222.10,378.09,151.07,74.13"><row><cell>Method</cell><cell>MRAE</cell><cell>MAE</cell></row><row><cell>ğ¸ğ‘€</cell><cell>2.35675</cell><cell>0.02811</cell></row><row><cell>ğ»ğ·ğ‘¦ (4 bins)</cell><cell>0.95555</cell><cell>0.01158</cell></row><row><cell>ğ»ğ·ğ‘¦ (8 bins)</cell><cell>1.07063</cell><cell>0.01257</cell></row><row><cell cols="2">ğ»ğ·ğ‘¦ (16 bins) 1.19310</cell><cell>0.01494</cell></row><row><cell>ğ¸ğ·ğ‘¦</cell><cell cols="2">0.89837 0.00996</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,671.04,232.53,8.97"><p>https://github.com/HLT-ISTI/QuaPy/tree/lequa2022/LeQua2022</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,108.93,671.04,178.80,8.97"><p>Source code: https://github.com/jjdelcoz/QU-Ant</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was funded by <rs type="funder">MINECO (Ministerio de EconomÃ­a y Competitividad)</rs> and <rs type="funder">FEDER (Fondo Europeo de Desarrollo Regional)</rs>, grant <rs type="grantNumber">PID2019-110742RB-I00</rs> (<rs type="funder">MINECO</rs><rs type="grantNumber">/FEDER</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3NSMY7z">
					<idno type="grant-number">PID2019-110742RB-I00</idno>
				</org>
				<org type="funding" xml:id="_gPnTe7J">
					<idno type="grant-number">/FEDER</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,107.59,439.25,398.40,10.91;6,107.59,452.79,285.00,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,334.89,439.25,171.10,10.91;6,107.59,452.79,94.00,10.91">Class distribution estimation based on the hellinger distance</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>GonzÃ¡lez-Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alaiz-RodrÃ­guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alegre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,209.84,452.79,93.74,10.91">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="page" from="146" to="164" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,466.34,399.60,10.91;6,107.20,479.89,208.72,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,342.46,466.34,160.26,10.91">A review on quantification learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>GonzÃ¡lez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>CastaÃ±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Del Coz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,107.20,479.89,114.57,10.91">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="74" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,493.44,398.40,10.91;6,107.59,506.99,349.83,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,294.02,493.44,211.96,10.91;6,107.59,506.99,175.54,10.91">Adjusting the Outputs of a Classifier to New a Priori Probabilities: A Simple Procedure</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Latinne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Decaestecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,291.82,506.99,91.81,10.91">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="21" to="41" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,520.54,399.60,10.91;6,107.59,534.09,398.40,10.91;6,107.59,547.64,154.07,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,260.26,520.54,242.54,10.91">Quapy: A python-based framework for quantification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moreo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,122.35,534.09,383.63,10.91;6,107.59,547.64,55.11,10.91">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4534" to="4543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,561.19,398.40,10.91;6,107.59,574.74,341.32,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,325.25,561.19,180.74,10.91;6,107.59,574.74,60.26,10.91">Dys: A framework for mixture models in quantification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Maletzke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cherman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,190.70,574.74,108.08,10.91">Proceedings of the AAAI</title>
		<meeting>the AAAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4552" to="4560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,588.29,400.24,10.91;6,107.59,601.84,398.40,10.91;6,107.26,615.39,68.33,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,318.76,588.29,189.06,10.91;6,107.59,601.84,253.62,10.91">Computationally efficient class-prior estimation under class balance change using energy distance</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kawakubo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Du Plessis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,369.47,601.84,123.61,10.91">IEICE Tran. on Inf. and Sys</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="176" to="186" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,628.93,398.40,10.91;6,107.59,642.48,128.93,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,161.36,628.93,207.24,10.91">Quantifying counts and costs via classification</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Forman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,376.56,628.93,129.43,10.91;6,107.59,642.48,45.00,10.91">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="164" to="206" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,656.03,398.40,10.91;6,107.59,669.58,159.13,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,168.70,656.03,275.41,10.91">Evaluation measures for quantification: an axiomatic approach</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,452.85,656.03,53.13,10.91;6,107.59,669.58,75.20,10.91">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="255" to="288" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
