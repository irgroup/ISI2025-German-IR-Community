<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,417.53,15.42;1,89.29,106.66,232.17,15.42">Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF</title>
				<funder>
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Compute Ontario and Compute Canada</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.72,134.97,54.12,11.96"><forename type="first">Wei</forename><surname>Zhong</surname></persName>
							<email>w32zhong@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,160.77,134.97,53.93,11.96"><forename type="first">Yuqing</forename><surname>Xie</surname></persName>
							<email>yuqing.xie@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,250.98,134.97,51.16,11.96"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,92.44,173.92,63.71,7.99"><forename type="first">David</forename><forename type="middle">R</forename><surname>Cheriton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,417.53,15.42;1,89.29,106.66,232.17,15.42">Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">EB0F064D300442E975515917419DB2F8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Mathematics information retrieval</term>
					<term>structure search</term>
					<term>dense retrieval</term>
					<term>math-aware search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work describes the participation of our team in the ARQMath 2022 Lab, where we have applied two highly complementary methods for effective math answer and formula retrieval. More specifically, a lexical sparse retriever (Approach Zero) capable of first-stage structure matching is combined with a fine-tuned bi-encoder dense retriever (ColBERT) to capture contextual similarity and semantic matching. The dense retrieval model is further pretrained to adapt to math domain content containing L A T E X tokens. In the Open Domain QA task, we take an extractive approach and filter sentences using heuristic rules applied to top-ranked answers returned from our retrievers. We provide an analysis of both the effectiveness and efficiency of our models. In this contest, our effectiveness is ranked at the top among all three tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The steady growth of scientific publications and the need to retrieve math-related content by formulas have attracted the attention of many researchers into the Mathematics Information Retrieval (MIR) field recently. The core task in MIR is to retrieve relevant information from documents that contain math formulas. However, the heterogeneous data presented in math content (including rich-structured formulas and their textual context) require special treatment to create a truly effective search engine. The difficulty arises not only because the usage of math notation demands a special tokenizer for processing, but also because of the rich structures and special semantic properties implied by math languages such as expression commutativity and symbol substitution equivalence. Furthermore, the general challenge in understanding math content in MIR imposes another hurdle compared to other IR tasks.</p><p>The ARQMath Labs have been one of the few tasks facing this challenge. The ARQMath-1 (2020) <ref type="bibr" coords="1,118.71,551.46,12.90,10.91" target="#b0">[1]</ref> and ARQMath-2 (2021) <ref type="bibr" coords="1,240.38,551.46,12.90,10.91" target="#b1">[2]</ref> include two tasks: Task 1 is a Community Question and Answer (CQA) task that asks to retrieve relevant answer posts from a limited set of Math StackExchange (MSE) 1 corpus between the year 2010 to 2018, given queries of real-world questions sampled from later-year MSE threads. Task 2 is a formula-centered task where it asks to return relevant formulas (considering their context) in the documents given one specified query formula appeared in a Task 1 topic. To encourage formula diversity, this task requires at most 5 visually distinct formulas to be returned, otherwise, the result will not be judged. This time, i.e., ARQMath-3, an additional Open Domain QA task (Task 3) has been introduced. Task 3 requires participants to return a single answer for each topic in Task 1, and the answer can be automatically generated or extracted from existing data, potentially outside the ARQMath collection.</p><p>To deal with formulas in math content, not only do the structured expressions need to be paid special attention to but also the context where a math formula occurs needs to be considered for a better understanding of the formula itself. Moreover, the context helps to discover math formula similarities even if formulas look different in structure. This resembles the synonym issue in regular full-text retrieval, where exact lexical matching prevents the return of semantically relevant documents having only synonyms to the query keywords. In both cases, recent studies using bi-encoder dense retrieval models <ref type="bibr" coords="2,276.07,249.56,11.48,10.91" target="#b2">[3,</ref><ref type="bibr" coords="2,290.96,249.56,7.52,10.91" target="#b3">4,</ref><ref type="bibr" coords="2,301.88,249.56,7.52,10.91" target="#b4">5,</ref><ref type="bibr" coords="2,312.81,249.56,7.52,10.91" target="#b5">6,</ref><ref type="bibr" coords="2,323.73,249.56,7.52,10.91" target="#b6">7,</ref><ref type="bibr" coords="2,334.66,249.56,9.03,10.91" target="#b7">8]</ref> have shown success for in-domain effectiveness and the ability to discover semantic similarities even if content lacks of lexical agreement. On the other hand, the structure of math formulas itself is an important aspect of similarity in MIR. For example, being a large substructure of another math expression is a good indicator of similarity.</p><p>In this work, we combine a structure-aware search engine with a bi-encoder dense retriever (See Lin <ref type="bibr" coords="2,125.29,330.85,12.84,10.91" target="#b8">[9]</ref> for this classification) to capture both the important structure similarity and semantic similarity. According to our recent findings <ref type="bibr" coords="2,288.51,344.40,16.41,10.91" target="#b9">[10]</ref>, we adopt the ColBERT model <ref type="bibr" coords="2,448.32,344.40,12.99,10.91" target="#b3">[4]</ref> due to its high effectiveness demonstrated in our evaluation of previous MIR tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Early work on MIR simply applies specialized tokenizers to handle math formulas <ref type="bibr" coords="2,458.56,416.58,16.31,10.91" target="#b10">[11]</ref>. Later, different intermediate tree representations are utilized to extract features for capturing structure similarities.</p><p>The Operator Trees (OPT) representation represents a math formula by identifying operators and operands in the expression, and constructing a tree recursively where each internal node is the operator of its children's operands. To our best knowledge, Hijikata et al. <ref type="bibr" coords="2,438.21,484.32,16.27,10.91" target="#b11">[12]</ref>, Yokoi and Aizawa <ref type="bibr" coords="2,123.96,497.87,17.91,10.91" target="#b12">[13]</ref> are the first to use the OPT representation for math retrieval. They extract leaf-root paths from OPT (alternatively in representational MathML) as features that are invariant to operand positional mutation (e.g., due to commutativity) for retrieval. Later, Zhong et al. <ref type="bibr" coords="2,473.47,524.97,16.30,10.91" target="#b13">[14,</ref><ref type="bibr" coords="2,492.03,524.97,13.95,10.91" target="#b14">15]</ref> extend OPT leaf-path matching to more strict structure matching with real-time efficiency. Their system (i.e., Approach Zero<ref type="foot" coords="2,236.94,550.31,3.71,7.97" target="#foot_0">2</ref> ) defines a meaningful metric for formula structure similarity by finding the maximum common subtree(s) between two math formula OPTs. This approach has achieved the best effectiveness for the formula search task in the ARQMath 2021 <ref type="bibr" coords="2,468.38,579.17,11.43,10.91" target="#b1">[2]</ref>.</p><p>On the other hand, the Symbol Layout Tree (SLT) <ref type="bibr" coords="2,322.56,592.72,16.39,10.91" target="#b15">[16,</ref><ref type="bibr" coords="2,341.68,592.72,14.01,10.91" target="#b16">17]</ref> represents a lower level structure semantics for math formulas. Similar to the L A T E X representation, it only captures the layout or the topology of a formula. This creates an advantage of little ambiguity in parsing. SLT is adopted as the main representation by a line of MIR works, e.g., the Tangent and Tangent-S systems <ref type="bibr" coords="2,127.83,646.91,16.52,10.91" target="#b17">[18,</ref><ref type="bibr" coords="2,147.07,646.91,12.58,10.91" target="#b18">19,</ref><ref type="bibr" coords="2,162.37,646.91,12.39,10.91" target="#b19">20]</ref>, and the Tangent-L or the MathDowsers system <ref type="bibr" coords="2,396.94,646.91,16.52,10.91" target="#b20">[21,</ref><ref type="bibr" coords="2,416.18,646.91,12.58,10.91" target="#b21">22,</ref><ref type="bibr" coords="2,431.48,646.91,12.58,10.91" target="#b22">23,</ref><ref type="bibr" coords="2,446.78,646.91,12.58,10.91" target="#b23">24,</ref><ref type="bibr" coords="2,462.09,646.91,12.39,10.91" target="#b24">25]</ref>. Local features such as symbols on adjacent nodes or nodes within a distance window and their spatial relations are together tokenized into math tuples<ref type="foot" coords="3,309.83,98.76,3.71,7.97" target="#foot_1">3</ref> and used for retrieval. As an example, the up-to-date MathDowsers system extracts more than five types of features from SLT <ref type="bibr" coords="3,463.75,114.06,16.25,10.91" target="#b24">[25]</ref>.</p><p>There are other systems such as the MCAT system <ref type="bibr" coords="3,322.99,127.61,17.76,10.91" target="#b25">[26]</ref> and the Tangent-S system <ref type="bibr" coords="3,458.69,127.61,17.76,10.91" target="#b26">[27]</ref> which incorporate both SLT and OPT representations. Both use linear regression to interpolate scores contributed from SLT and OPT features. The Tangent-CFTED system <ref type="bibr" coords="3,393.97,154.71,16.09,10.91" target="#b27">[28]</ref>, an upgraded version of Tangent-S, further applies the FastText algorithm <ref type="bibr" coords="3,323.93,168.26,17.94,10.91" target="#b28">[29]</ref> to learn structural embeddings from SLT and OPT local features for candidate retrieval, then it reranks them by tree edit distance.</p><p>More recently, Transformer models for MIR have entered the MIR domain. Although it has been shown Transformer-based language model may still be relatively weak at math tasks <ref type="bibr" coords="3,113.20,222.46,16.30,10.91" target="#b29">[30,</ref><ref type="bibr" coords="3,131.32,222.46,12.23,10.91" target="#b30">31]</ref>, these Transformer models have nevertheless demonstrated their good effectiveness at MIR. The MathBERT model <ref type="bibr" coords="3,232.39,236.01,16.41,10.91" target="#b31">[32]</ref>, evaluated on the NTCIR-12 collection <ref type="bibr" coords="3,433.88,236.01,16.42,10.91" target="#b32">[33]</ref>, introduces structure mask pretraining on top of the pretraining objectives used in BERT <ref type="bibr" coords="3,438.16,249.56,16.50,10.91" target="#b33">[34,</ref><ref type="bibr" coords="3,457.39,249.56,12.38,10.91" target="#b34">35]</ref>. It uses the last two layers' feature vectors for reranking. The previous ARQMath tasks <ref type="bibr" coords="3,439.66,263.11,11.26,10.91" target="#b0">[1,</ref><ref type="bibr" coords="3,453.64,263.11,7.51,10.91" target="#b1">2]</ref>, however, have witnessed more widespread use of deep models in MIR. Among them, Novotnỳ et al. <ref type="bibr" coords="3,89.29,290.20,16.48,10.91" target="#b35">[36,</ref><ref type="bibr" coords="3,109.08,290.20,14.03,10.91" target="#b36">37]</ref> use a SentenceBERT-based Transformer (i.e., CompuBERT) <ref type="bibr" coords="3,400.21,290.20,18.06,10.91" target="#b37">[38]</ref> to regress QA pair scores based on user-generated data in the original MSE thread. And Rohatgi et al. <ref type="bibr" coords="3,472.34,303.75,16.48,10.91" target="#b38">[39,</ref><ref type="bibr" coords="3,491.96,303.75,14.03,10.91" target="#b39">40]</ref> reranks search results using the full token embeddings generated from a pretrained RoBERTa Transformer. The DPRL QASim method <ref type="bibr" coords="3,265.89,330.85,17.76,10.91" target="#b40">[41]</ref> uses two Transformers as similarity assessors, one question-question SentenceBERT <ref type="bibr" coords="3,237.58,344.40,17.77,10.91" target="#b37">[38]</ref> assessor pretrained on the Quora website and fine-tuned using related/duplicate links on the MSE website, as well as a question-answer TinyBERT <ref type="bibr" coords="3,488.13,357.95,17.85,10.91" target="#b41">[42]</ref> assessor pretrained on the MS-MARCO dataset <ref type="bibr" coords="3,297.62,371.50,17.76,10.91" target="#b42">[43]</ref> and fine-tuned on the ARQMath-1 training data. The similarity produced by QASim is a product of these two assessor scores where the question-question model evaluates the topic question and the question to which the document answer is given. Similarly, the TU_DBS systems <ref type="bibr" coords="3,298.13,412.15,16.30,10.91" target="#b43">[44,</ref><ref type="bibr" coords="3,316.61,412.15,13.95,10.91" target="#b44">45]</ref> uses a cross encoder as a major model in the ARQMath 2021, moreover, they apply the deep model to the Task 2 of formula retrieval. For the backbone model, they use an ALBERT Transformer <ref type="bibr" coords="3,330.71,439.25,17.76,10.91" target="#b45">[46]</ref> further pretrained on the ARQMath corpus directly with a 512 maximum token input.</p><p>However, the aforementioned models are either mostly cross encoders that require a full pass through the Transformer model to evaluate a pair of query and document, or they only use pretrained model embeddings without finetuning for better QA similarity assessment. As a result, they have to either only evaluate partial collections (e.g., considering only answers shared at least one tag with the topic), or use a simplified model architecture (e.g., using the TinyBERT or ALBERT instead) for fast inference on the million-scale ARQMath dataset. Prior to this ARQMath-3 Lab, to our best knowledge, only the CompuBERT model <ref type="bibr" coords="3,412.30,547.64,16.31,10.91" target="#b35">[36,</ref><ref type="bibr" coords="3,431.08,547.64,12.23,10.91" target="#b36">37]</ref>, the ColBERT model <ref type="bibr" coords="3,118.95,561.19,12.69,10.91" target="#b3">[4]</ref> explored by the TU_DBS system <ref type="bibr" coords="3,279.05,561.19,16.09,10.91" target="#b44">[45]</ref>, and our recent work <ref type="bibr" coords="3,393.97,561.19,17.76,10.91" target="#b9">[10]</ref> have used fine-tuned bi-encoder Transformer models (which are practically efficient) in the MIR domain. In this work, we will incorporate the ColBERT model into our structure-aware search system Approach Zero <ref type="bibr" coords="3,112.95,601.84,16.43,10.91" target="#b13">[14,</ref><ref type="bibr" coords="3,132.11,601.84,14.03,10.91" target="#b14">15]</ref> for the ARQMath-3 Lab. Operator and leaf (i.e., operand) are denoted by circle and box respectively. In order to improve recall, operands with or without subscript (sub) or superscript (sup) are represented canonically under a subsup token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Lexical and Structural Search</head><p>Similar to our approach in the ARQMath 2021 <ref type="bibr" coords="4,296.12,383.97,16.22,10.91" target="#b46">[47]</ref>, we use the Approach Zero system <ref type="bibr" coords="4,472.85,383.97,16.40,10.91" target="#b13">[14,</ref><ref type="bibr" coords="4,491.97,383.97,14.01,10.91" target="#b14">15]</ref> for searching for lexical text words and structural math formulas. Approach Zero constructs a customized OPT representation internally to extract structure features, specifically, it uses leafroot path extracted from OPT and their path prefixes as "keywords". Figure <ref type="figure" coords="4,418.98,424.61,4.97,10.91" target="#fig_0">1</ref> shows an example of OPT we use for representing an ARQMath topic formula. Compared to the representation we use in 2021 <ref type="bibr" coords="4,140.37,451.71,16.08,10.91" target="#b46">[47]</ref>, it no longer requires a "sign" node on top of an operand, instead, we downplay its importance and include the operand sign into the "fingerprint" of the path where symbols are hashed into a single value to be scored and counted for symbol similarity rather than for structure similarity. Moreover, we have fixed a few inconsistencies in the grammar reduction rules which we use to construct the OPT tree. These engineering aspects of improvements, including a few bug fixes, turn out to greatly impact the effectiveness. Given extracted leaf-root paths (and their prefixes), we tokenize all the nodes along each path to boost search recall. These paths are used as the vocabulary keys in a specialized inverted index <ref type="bibr" coords="4,116.69,560.11,17.95,10.91" target="#b14">[15]</ref> for mapping any query path (extracted from the OPTs of query formulas using the same way) to the corresponding posting lists which store relevant document path information. More specifically, each posting list item includes document ID, root-end node IDs, formula ID, formula length, the frequencies of paths under each subtree, leaf-end symbols (i.e., operand symbols), and path fingerprints. The matched query and document paths are evaluated one formula at a time at query processing.</p><p>For structure similarity, we calculate a common structure score 𝑤(𝑄 (𝑚) , 𝐷 (𝑛) ) of node 𝑚 in query formula 𝑄 and node 𝑛 in document formula 𝐷 by summing the number of common paths under each substructure and weighted by a path idf <ref type="bibr" coords="5,348.44,86.97,16.39,10.91" target="#b46">[47]</ref>:</p><formula xml:id="formula_0" coords="5,250.72,109.20,255.92,25.50">PathIDF 𝑝 = log 𝑁 𝑑𝑓 𝑝 (1)</formula><p>where 𝑁 is the total number of paths in the index and 𝑑𝑓 𝑝 is the document frequency of path 𝑝. The overall structure score 𝑤 * (𝑄, 𝐷) is given by the maximum common structure score between query and document formulas:</p><formula xml:id="formula_1" coords="5,175.28,195.49,331.36,49.93">𝑤 * (𝑄, 𝐷) = max 𝑚,𝑛 𝑤(𝑄 (𝑚) , 𝐷 (𝑛) ) = max 𝑚,𝑛 ∑︁ 𝑡 min( ⃒ ⃒ ⃒𝑄 (𝑚) 𝑡 ⃒ ⃒ ⃒ , ⃒ ⃒ ⃒𝐷 (𝑛) 𝑡 ⃒ ⃒ ⃒) • PathIDF 𝑡<label>(2)</label></formula><p>where</p><formula xml:id="formula_2" coords="5,119.36,256.00,81.17,20.07">⃒ ⃒ ⃒𝑄 (𝑚) 𝑡 ⃒ ⃒ ⃒ and ⃒ ⃒ ⃒𝐷 (𝑛) 𝑡 ⃒ ⃒</formula><p>⃒ are the number of paths which have the same tokens (or vocabulary key) 𝑡 under query node 𝑚 and document node 𝑛 respectively, and we look up them at retrieval time by grouping hit document paths by their root-end nodes and their (indexed) frequency of path 𝑡. The modeled structure similarity 𝑤 * (𝑄, 𝐷) can also be viewed as the sum of path idf in the largest common subtree between query and document formula OPTs. This resembles the tf-idf scoring except for the "term frequency" here counts for common structure "width", i.e., the number of matched leaves in the common subtrees.</p><p>In addition to the aforementioned structure weight, we multiply a few other factors to the path idf for the final similarity score. First, paths in the maximum common structure are paired by their symbols (if they have the freedom to match another path of a different symbol in the counterpart). Then, the operand symbol and the fingerprint value associated with each path will determine the symbol score by summing the match points by the following rules:</p><p>• 1 point if both the operand symbol and the fingerprint match • a lower point 𝑏 1 if only operand symbol match • a nonzero base point 𝑏 2 otherwise (𝑏 2 &lt; 𝑏 1 )</p><p>where the fingerprint is a hash value of the symbols of up to 4 operator nodes on top of the path leaf, it also takes into account the sign value (i.e., 1, -1) induced for each operator (this includes the sign of the operand itself since the sign of an operand is induced into the subsup node which is always placed on top of an operand). A greedy matching algorithm MarkAndCross <ref type="bibr" coords="5,477.97,526.36,18.00,10.91" target="#b47">[48]</ref> is used to pair them such that a higher number of points is likely achieved, then we normalize it with the number of matched paths and produce the (normalized) symbol score 𝑆 ′ 𝑠𝑦𝑚 . For query formula 𝑄 and document formula 𝐷, the final symbol score factor we multiply is a rescaled version:</p><formula xml:id="formula_3" coords="5,214.10,589.44,292.54,26.56">𝑆 𝑠𝑦𝑚 (𝑄, 𝐷 | 𝑤 * ) = 1 1 + (1 -𝑆 ′ 𝑠𝑦𝑚 ) 2<label>(3)</label></formula><p>Second, we add penalties to long formulas in a document. Assume the original formula length is 𝐿 𝐷 , the penalty 𝑃 (𝐷; 𝜂) is parameterized by 𝜂 ∈ [0, 1]:</p><formula xml:id="formula_4" coords="5,216.63,658.20,290.01,25.55">𝑃 (𝐷; 𝜂) = 1 -𝜂 + 𝜂 • 1 log(1 + 𝐿 𝐷 )<label>(4)</label></formula><p>Finally, the overall formula score for math formula similarity 𝑆(𝑄, 𝐷) is given by</p><formula xml:id="formula_5" coords="6,184.48,109.71,322.16,13.13">𝑆(𝑄, 𝐷) = 𝑤 * (𝑄, 𝐷) • 𝑆 𝑠𝑦𝑚 (𝑄, 𝐷 | 𝑤 * ) • 𝑃 (𝐷; 𝜂)<label>(5)</label></formula><p>This scoring process is accelerated by the GBP-LEN dynamic pruning strategy <ref type="bibr" coords="6,438.65,135.98,16.25,10.91" target="#b14">[15]</ref>.</p><p>To handle text keywords, we adopt the BM25+ scoring schema <ref type="bibr" coords="6,390.53,149.53,16.42,10.91" target="#b48">[49]</ref>. The overall score in Approach Zero is a weighted sum (using a math path weight <ref type="bibr" coords="6,360.54,163.08,17.91,10.91" target="#b46">[47]</ref> to weigh a math match over text-word match) of all partial scores obtained by BM25+ in normal text keywords and those by formula scoring in formula keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ColBERT</head><p>The ColBERT model <ref type="bibr" coords="6,180.66,239.91,12.69,10.91" target="#b3">[4]</ref> is a dense retrieval model based on a BERT backbone. It is considered a bi-encoder model because it has two independent encoders, i.e., a query encoder and a document encoder. The interaction between them is deferred until the similarity scoring takes place, and this similarity is only dependent on their encoded embeddings and it is calculated using the MaxSim operation <ref type="bibr" coords="6,173.96,294.10,11.42,10.91" target="#b3">[4]</ref>. In addition, unlike other dense retrievers that use passage-level <ref type="bibr" coords="6,476.31,296.55,29.67,7.90">[CLS]</ref> embedding, the ColBERT model preserves all output embeddings associated with each token. However, to reduce space footprint and speed up indexing, ColBERT pools each BERT output into a smaller dimension embedding (𝑑 = 128 by default). Because each output embedding is pretrained for the MLM objective <ref type="bibr" coords="6,237.30,348.30,16.09,10.91" target="#b34">[35]</ref>, they represent fine-grained contextualized semantics for individual tokens. This also makes ColBERT easier to visualize the contribution of similarity by tokens.</p><p>For a query in token sequence 𝑞 = 𝑞 0 , 𝑞 1 , ...𝑞 𝑙 and a passage in token sequence 𝑝 = 𝑑 1 , 𝑑 2 , ...𝑑 𝑛 , the ColBERT model calculates either dot product or L2 distance for the tokenlevel score 𝑠(𝑞 𝑖 , 𝑑 𝑗 ) over the normalized output embeddings between the corresponding tokens 𝑖 ∈ [𝐸(𝑞)], 𝑗 ∈ [𝐸(𝑝)] of the query and passage. The overall scoring of query 𝑞 and passage 𝑝 is conducted by the MaxSim operation which locates the maximum matched token 𝑑 𝑗 in the passage for each query token 𝑞 𝑖 , and then it sums over their token-level scores:</p><formula xml:id="formula_6" coords="6,221.81,480.06,284.82,26.14">𝑆(𝑞, 𝑝) = ∑︁ 𝑖∈[𝐸(𝑞)] max 𝑗∈[𝐸(𝑝)] 𝑠(𝑞 𝑖 , 𝑑 𝑗 ).<label>(6)</label></formula><p>During the training, a triple of query and a contrastive passage pair, i.e., (𝑞, 𝑝 + , 𝑝 -) is fed to the model to optimize a pairwise cross-entropy loss. At encoding, the model always prepends an unused token <ref type="bibr" coords="6,166.19,549.83,17.80,7.90">[Q]</ref> or <ref type="bibr" coords="6,199.05,549.83,17.80,7.90">[D]</ref> to differentiate the encoding of a query or a passage. In practice, the authors also use query augmentation by rewriting the padding query tokens [PAD]s to [MASK] tokens before query encoding. This has been demonstrated to boost effectiveness and it gets rid of the need to mask query tokens in batch processing.</p><p>In order to perform end-to-end retrieval, the index of ColBERT needs to include all encoded passage tokens as well as document IDs and their lengths to locate the offset of passage tokens. And for efficient query processing, a two-stage retrieval is done: (1) An approximate nearest neighbors (ANN) search (e.g., using the product quantization method <ref type="bibr" coords="6,394.71,642.23,16.78,10.91" target="#b49">[50]</ref>) is first performed to filter a pool of top candidate tokens for each query token individually. (2) Then it locates unique documents associated with the top candidate tokens and loads their entire passage embeddings into GPU for fast MaxSim operation. Notice that the above candidate selection stage comes at a cost, it has rendered the end-to-end retrieval an approximate version of what is originally defined in Eq. 6.</p><p>Our implementation of the ColBERT model and its backbone are based on the Hugging Face Transformer <ref type="bibr" coords="7,146.33,141.16,17.76,10.91" target="#b50">[51]</ref> package and we use the Faiss package <ref type="bibr" coords="7,333.66,141.16,17.76,10.91" target="#b51">[52]</ref> to do first-stage ANN with product quantization. We use dot product for token-level scoring in both reranking and end-to-end retrieval. To accommodate the memory limit in our experimental environment, we choose to split the index into multiple shards and load them sequentially into memory and GPU. We adjust the division of shards to make sure each shard will not generate candidate embeddings that exceed the capacity of memory and GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Handling ARQMath Tasks</head><p>Except for minor corpus fix, format changes, and topic renewal, Task 1 and Task 2 remain mostly unchanged compared to ARQMath 2021. However, the ARQMath Lab 2022 adds another open-domain QA task (i.e., Task 3) where it re-uses the same topics from Task 1 but only one single answer should be returned for each topic (potentially through extraction or generation).</p><p>In the following, we will focus more on our newly introduced model ColBERT and our handling of Task 3. For the Approach Zero system, only changes to the previous-year system <ref type="bibr" coords="7,488.22,335.08,17.76,10.91" target="#b46">[47]</ref> are described in detail. Both the Approach Zero and the ColBERT model are described in the beginning of this section because they are applied uniformly to all the tasks.<ref type="foot" coords="7,431.50,360.43,3.71,7.97" target="#foot_2">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Approach Zero Changes</head><p>The version of Approach Zero we use in this ARQMath Lab is basically the same as what we have used in ARQMath 2021 <ref type="bibr" coords="7,194.88,425.26,16.24,10.91" target="#b46">[47]</ref>. We do notice a leap in effectiveness when we evaluate Approach Zero on previous-year tasks. However, except for a list of major engineering improvements below, the model remains unchanged.</p><p>• Improved OPT: This includes unwrapping parentheses if it is redundant and directly under a fraction operator. And we unify some malformed L A T E X tokens with their correct forms, such as sin and \sin. • Simplified OPT: Merge sign node into symbol hash value, delete unnecessary ADD nodes, and fix inconsistent grammar reduction. (See Section 3.1) • Bug fix: Fixed a bug that prevents many paths from being indexed.</p><p>In Section 5.2, we will further evaluate the impact of effectiveness as a result of these changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Data</head><p>For pretraining, we use a corpus made by ourselves. <ref type="foot" coords="7,324.15,612.58,3.71,7.97" target="#foot_3">5</ref>  token where sentences are extracted by the following strategy: We set an input threshold for two sentences to at most 1/4 of the maximum tokens of the Transformer 10% of the time, and the other 90% of the time we use the default maximum number of tokens in Transformer. Then the length threshold for the first sentence is randomly selected between 1 and the input threshold, the second sentence is selected by filling up the rest input space. Sentences are concatenated until it exceeds their length threshold for the first time, and are truncated if it exceeds the maximum number of tokens in the Transformer. Compared to our previous work <ref type="bibr" coords="8,447.07,181.81,16.09,10.91" target="#b9">[10]</ref>, we have improved the sentence splitting algorithm to include fewer short and meaningless sentences.</p><p>For training ColBERT, we use the ARQMath corpus which contains questions, answers, the number of upvotes by users, and links to the accepted answer as well as duplicate questions. The training triples only sample question-answer pairs. In particular, we use the accepted answer, or the accepted answer in a duplicate question, or any answer posts receiving more than 7 upvotes for a question as positive answers; while we mine hard negatives from the corpus by sampling random answers related to the same tags of the question. Finally, we have extracted 607K triplets for ColBERT training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Preprocessing for Math</head><p>In this ARQMath Lab, we follow a similar way to handle math tokens: First, we add L A T E X math tokens as additional vocabulary before further pretraining. This helps to reduce the number of tokens for the input of the Transformer, which further allows more information to be considered for assessing similarity. Second, the existing L A T E X lexer used in Approach Zero is utilized (by calling a Python binding PyA0 <ref type="bibr" coords="8,245.31,394.13,16.99,10.91" target="#b52">[53]</ref>) to determine the added vocabulary. This additionally reduces the newly added vocabulary size as Approach Zero focuses on semantically relevant tokens and ignores unimportant tokens such as color and spaces in L A T E X commands.</p><p>Different from our previous work <ref type="bibr" coords="8,251.87,434.78,16.22,10.91" target="#b9">[10]</ref>, we have adjusted our lexer slightly and pretrained a new backbone: We used to create a new token for each number less than 2 digits and create an extra special token BIGNUM for any other number tokens. However, this prevents decimals to be meaningfully represented and it disables the Transformer to learn any differences in larger numbers. We modified the way to tokenize a number by using the CHARACTER scheme proposed by Nogueira et al. <ref type="bibr" coords="8,219.68,502.52,16.29,10.91" target="#b53">[54]</ref>, this orthography is robust to handle various user-created content and it has shown to help certain simple arithmetic downstream tasks to accurately calculate numbers to the 5th digit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training</head><p>We further pretrain a bert-base model and then we fine-tune a ColBERT model based on it. In a recent work <ref type="bibr" coords="8,155.80,592.90,16.41,10.91" target="#b9">[10]</ref>, we have demonstrated the benefits for downstream MIR tasks to create a further pretrained backbone. Therefore, we pretrain the bert-base checkpoints with math vocabulary using the MLM and NSP objectives <ref type="bibr" coords="8,293.86,619.99,16.09,10.91" target="#b34">[35]</ref>. Instead of treating math and text separately as seen in <ref type="bibr" coords="8,136.04,633.54,16.36,10.91" target="#b44">[45]</ref>, we generate sentence pairs by splitting passages containing math tokens just like normal full text, this creates a more diverse mixture of text and math tokens, presumably covering more heterogeneous data distributions.</p><p>On top of our pretrained backbone, we train the ColBERT model directly using the triples described in Section 4.2. In addition, we mask out punctuation symbols in a sentence to make more space for meaningful tokens. We apply the training procedure described in Section 3.2 for the ColBERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Task 1: Answer Retrieval</head><p>Task 1 is about retrieving answers relevant to a question in full text.</p><p>For the Approach Zero pass, similar to our system in ARQMath 2021 <ref type="bibr" coords="9,407.84,190.89,16.23,10.91" target="#b46">[47]</ref>, we prepend each answer post with its original question text before indexing to improve recall. In addition, we switch from the Lancaster stemmer to mainly using the Porter stemmer as we observe minor effectiveness gain for the latter one in the up-to-date Approach Zero. Furthermore, we use manually extracted text and formula keywords <ref type="foot" coords="9,305.93,243.33,3.71,7.97" target="#foot_4">6</ref> for Approach Zero in Task 1, this not only mimics the ad-hoc search queries but also avoids hurting the effectiveness of a system based on strict matches when a misleading keyword or formula in the original topic is used. However, our manual topic for Task 1 is available <ref type="foot" coords="9,275.12,283.98,3.71,7.97" target="#foot_5">7</ref> for anyone who wants to compare to our results directly.</p><p>In contrast to the Approach Zero pass, we use the complete topic content as the input for the ColBERT model as it can automatically encode tokens for similarity and this aligns with the way it is trained.</p><p>Finally, we combine the two systems by linear interpolation using the best parameters based on our previous K-fold cross validation on ARQMath-2 <ref type="bibr" coords="9,335.54,367.03,16.22,10.91" target="#b46">[47]</ref>. We also use ColBERT to rerank a base run produced by Approach Zero with no stemmer because reranking has generated the highest precision in one of our previous evaluations <ref type="bibr" coords="9,323.35,394.13,16.25,10.91" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Task 2: Formula Retrieval</head><p>Task 2 asks to retrieve formulas relevant to the specified topic formula in a corresponding Task 1 question, given the formula's context.</p><p>For formula retrieval, we directly query the specified formula without considering the context in the Approach Zero pass. Similar to our approach in ARQMath 2021 <ref type="bibr" coords="9,415.29,484.50,16.41,10.91" target="#b46">[47]</ref>, we rewrite the topic manually 6 to ensure the L A T E X can be correctly parsed into a clear OPT. For example, we manually insert a comma in B.336 to separate two conditions in a set expression (the original expression uses a long space to separate them), and we replace the text mode "m++" to "m+1" so that our parser can handle the rare increment expression correctly. If not manually corrected, we believe these changes are going to be difficult for being automatically suggested so that every user-generated formula can be converted to construct a clear OPT. In total, we have manually refined 20 topics in Task 2 for ARQMath 2022, and our rewritten topics for Task 2 are also made available. <ref type="foot" coords="9,133.66,591.14,3.71,7.97" target="#foot_6">8</ref>We use two approaches in the ColBERT pass to handle Task 2. The first one does not consider the context of the query formula even though the ColBERT is trained from both math and surrounding text. Thus it only passes an isolated formula to both the query encoder and document encoder, considering only the visual appearance of formulas. On the other hand, the second approach (we name it colbert_ctx) adds the dependency to the query formula context in the following way: We use the query formula ID (i.e., qid) to identify the specified formula in the full-text question, and mask every other formula except the query formula by rewriting each one to the special [MASK] token. Then, we feed ColBERT the modified tokens of the entire topic question.</p><p>Similarly, we generate a few fusion runs to combine the two passes by linear interpolation using the best parameters which are the same as what we choose to use in Task 1. However, we do not generate reranked results in Task 2.</p><p>To make sure we do not return more than 5 visually distinct formulas for a topic, we simply index at most 5 document formulas. For the colbert_ctx method, we index the full-text embeddings of a document, each formula in the document will get indexed with its formula ID and the complete document, masking out all other formulas. To reduce the number of embeddings to be indexed, we only consider formulas with original string lengths greater than 2 or those in the visually distinct formula set of our Task 1 index. For other methods (except the colbert_ctx), we only index formulas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Task 3: Open Domain Question Answering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.1.">Model Selection</head><p>There are many choices for handling the open-domain QA problem. Parametric generative models are trained with questions as input and it outputs question-answer pairs without access to external knowledge. Such models store the required knowledge in the model parameters. On the other hand, non-parametric models mostly adopt a retrieve-and-read framework: they first retrieve relevant documents from the corpus given a question, and then produce the final answer based on these documents <ref type="bibr" coords="10,238.73,441.80,16.09,10.91" target="#b54">[55]</ref>. We adopt non-parametric models, which require smaller model sizes and less training data.</p><p>In non-parametric models, there are generative or extractive readers. Generally speaking, generative readers either need much training data to transfer themselves to the target domain or at least need a few examples to familiarize themselves with the answer formatting. Unfortunately, we do not have a good amount of training data for MIR, furthermore, generative models tend to recall non-overlapping spans in the training data, which leads to non-logical answers that are not desired for math question answering. Therefore, we adopt extractive readers in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.2.">Generating Candidate Answers</head><p>We use our Task 1 runs for producing a set of candidate answers. As a principle, we choose to base our answer on a single post because different answer posts together will introduce different notations and dialects which create difficulties to align them in the final answer.</p><p>We design three strategies for post candidates:</p><p>• Original: A straightforward way to narrow down answer posts is to take the top-1 result from Task 1, we will use this as our first strategy</p><p>• Re-rank: The next strategy is simply considering a larger set of top results -here we use the top-20 results from Task 1 -and select one of them using the methods in the next subsection (Section 4.7.3). • Re-map: Another strategy to look at the problem is to trust the community: we first try to retrieve the most similar corpus question post given a topic question (through the methods we use in Task 1), then we pick the accepted answer post, if non-exists, the top voted answer post as our candidate answer post.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.3.">Snippet Selection</head><p>A candidate post might be either inappropriately long or exceeding the length limit imposed by the Task 3 requirements, therefore we add a snippet selection step based on the candidate answer post(s). We split the answer post(s) into sentences with the PyA0 sentence splitting utility which takes care of the punctuations and avoids cutting them in the middle of a L A T E X string. Then a window of varied sizes (from a minimum of 5 up to 10 sentences) will go through the beginning of a post to its end to select a combination of sentence spans to form candidate snippets.</p><p>It is worth noting the beginning of a post usually contains some conclusive answers, or it serves as a good start for smooth reasoning, therefore we also add a selection strategy that always starts the window from the beginning of candidate answer post(s), but removes the limit of the window size unless it hits the end of an answer post.</p><p>We will generate candidate snippets according to all the above strategies, at the meantime, we also filter out a snippet if it:</p><p>• is shorter than 20 tokens, • is longer than 1200 characters, • has an odd number of "$"s, meaning that some math delimiters are very likely not placed correctly (admittedly, this is only a heuristic rule).</p><p>Finally, we use the same ColBERT model to score the snippets and pick the top 1 snippet as the final answer. Lastly, we also double-check the produced answers manually, if there are cases the final selected snippet is still too short, or there is no candidate available, or the math delimiters are still incorrect but not detected by the odd-"$" checking, we will randomly copy an answer from a parallel run among our (manually selected) 5 best runs for submissions. As a result, we mark our Task 3 runs as manual runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Setup</head><p>For Approach Zero, we uniformly apply the same configuration as we choose for the primary run in 2021 (See Table <ref type="table" coords="11,190.37,628.93,3.57,10.91" target="#tab_1">1</ref>). Rather than exploring different configurations, we fix the Approach Zero pass for all three tasks in 2022.</p><p>We have further pretrained a bert-base checkpoint with 5.8 M sentence pairs extracted from the MSE and AoPS corpus. The pretraining takes 9 epochs with only ∼1100 added math vocabulary on three A6000 GPUs using a batch size of 114. Based on our backbone, we train ColBERT for 7 epochs also using three A6000 GPUs but with a batch size of 48. Following Reusch et al. <ref type="bibr" coords="12,113.61,342.16,16.28,10.91" target="#b44">[45]</ref>, we set the maximum number of tokens to 512 for both pretraining and finetuning.</p><p>The training, inference, and scoring are all done in half-precision. In all the experiments, we use the AdamW optimizer <ref type="bibr" coords="12,334.36,369.26,17.78,10.91" target="#b55">[56]</ref> with a weight decay of 0.01, and a fixed learning rate of 1 × 10 -6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Approach Zero Improvements</head><p>We have investigated the impact of major improvements after our previously published system run, results are shown in Table <ref type="table" coords="12,230.91,446.08,3.77,10.91" target="#tab_3">2</ref>. It shows the most impactful changes are representationalafter two major OPT improvements, we boost the official MAP ranking metric and precision metric by at least 28% and 20% over the result we have reported in 2021 -this has demonstrated that the design of representation in our structure search method is crucial for effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">ARQMath-1 and 2 Results</head><p>Table <ref type="table" coords="12,116.55,536.46,5.17,10.91">3</ref> and 4 show the evaluation results on ARQMath-1 and 2 topics using our systems in 2022. We compare ours to the official runs of the most effective systems in ARQMath 2021. We exclude comparing to this-year systems on previous topics because other systems may train on previous labels. Some of these systems are briefly mentioned in Section 2, however, here we describe relevant runs shown in these tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 1 Runs:</head><p>The TU_DBS_P primary run from the TU_DBS team uses an official ALBERT cross encoder which is further trained for 750k steps and fine-tuned for 125k steps on the ARQMath data pairs split by sentence <ref type="bibr" coords="12,260.80,637.28,16.28,10.91" target="#b44">[45]</ref>. The DPRL_RRF is a Reciprocal Rank Fusion (RRF) between a run relying on MSE upvote and the DPRL_QASim run produced by the QASim method <ref type="bibr" coords="12,124.78,664.38,16.09,10.91" target="#b40">[41]</ref>. The MathDowsers_P is the primary run generated from the Tangent-L system <ref type="bibr" coords="12,487.08,664.38,16.09,10.91" target="#b23">[24]</ref>, Table <ref type="table" coords="13,116.06,112.48,5.12,8.93">3</ref> Effectiveness evaluation for previous ARQMath Labs (Task 1). Top-5 most effective systems for the year 2021 are compared to ours. In addition to the official measurements, we have also reported the BPref metric as well as the average number of judged hits per topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>ARQMath-1 ARQMath-2 NDCG' MAP' P'@10 BPref Judged NDCG' MAP' P'@10 BPref Judged </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Effectiveness evaluation for previous ARQMath Labs (Task 2). Top-5 most effective systems for the year 2021 are compared to ours. In addition to the official measurements, we have also reported the BPref metric as well as the average number of judged hits per topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>ARQMath-1 ARQMath-2 NDCG' MAP' P'@10 BPref Judged NDCG' MAP' P'@10 BPref Judged 1 "♭": unsubmitted runs, " †": using unsupervised method, and " ‡": supervised only in reranking fusion.</p><p>it additionally captures repeated symbols and commutative operands based on manipulating the features extracted from the SLT representation, e.g., by enumerating the combination of all possible repeated symbol pairs and indexing all of them. The A0-B60 is our best previous-year submission for Task 1, produced by a combination of Approach Zero with Lucene/Anserini <ref type="bibr" coords="14,487.56,127.61,16.08,10.91" target="#b46">[47]</ref>. Our a0none and a0porter runs are Approach Zero runs using no stemmer and a Porter stemmer respectively. The rerank_nostemmer run is produced by reranking a0none using the ColBERT scorer. Finally, the fusion_alpha* runs are a linear fusion interpolated by a convex combination where 𝛼 = 0.2, 0.3 and 0.5 respectively:</p><formula xml:id="formula_7" coords="14,237.48,202.96,76.15,10.77">𝑆 𝑓 = 𝛼 • 𝑆 𝑑 + (1</formula><p>where 𝑆 𝑑 , 𝑆 𝑎 are the scores produced from the dense retriever and Approach Zero respectively, and 𝑆 𝑓 is the fusion score.</p><p>Task 2 Runs: The Tangent-S <ref type="bibr" coords="14,228.34,254.37,17.88,10.91" target="#b26">[27]</ref> is a formula search engine using both SLT and OPT, it has been used as a baseline for formula search in ARQMath-1 and 2. Tangent-S makes no use of the question text in Task 2. The DPRL_ltrall reranks a list of 6 signals using SVM-rank <ref type="bibr" coords="14,486.67,281.47,16.41,10.91" target="#b56">[57]</ref>, the training data includes all ARQMath-1 judgments (77 queries). The DPRL_CFTED reranks the results from Tangent-CFT using tree-edit distance, the latter learns fastText <ref type="bibr" coords="14,451.80,308.57,18.07,10.91" target="#b28">[29]</ref> n-gram embedding from SLT, SLT operators, and OPT structure features. The MathDowsers_P is generated by the same system from the MathDowsers team <ref type="bibr" coords="14,356.23,335.66,16.42,10.91" target="#b22">[23,</ref><ref type="bibr" coords="14,375.38,335.66,14.02,10.91" target="#b23">24]</ref> in Task 1, except it adds a strategy to rank a set of visually distinct formula candidates by utilizing their Task 1 result. Our approach0 run is the formula-only retrieval for Task 2 by the up-to-date Approach Zero system. The fusion02_ctx and fusion_alpha* runs are the convex combination (See Equation <ref type="formula" coords="14,473.87,376.31,4.25,10.91">7</ref>) with the approach0 run by the colbert_ctx run (𝛼 = 0.2) and the colbert run (𝛼 = 0.2, 0.3 and 0.5) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">ARQMath-3 Results</head><p>We present the results for ARQMath-3 in Table <ref type="table" coords="14,308.22,452.50,3.81,10.91">5</ref>, 6, and 7. In this ARQMath Lab, a total of 9 teams have participated, and we achieve the best results in all three tasks. Here we briefly describe some of the top systems we are comparing in this paper. For a complete overview of other participant systems, please refer to Mansouri, Novotný, Agarwal, Oard, and Zanibbi <ref type="bibr" coords="14,487.38,493.14,16.23,10.91" target="#b57">[58]</ref>.</p><p>Task 1 Baselines: According to Geletka et al. <ref type="bibr" coords="14,297.97,511.31,16.21,10.91" target="#b58">[59]</ref>, the selected MSM run is produced from an ensemble model in which each method is mainly developed as part of an Information Retrieval course taught at the Faculty of Informatics, Masaryk University, Brno, Czech Republic. And the MIRMU run is produced by a dense retriever pipeline that uses a miniLM as a bi-encoder (first-stage) retriever and a RoBERTa model as a cross-encoder reranker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 2 Baselines:</head><p>The DPRL Tangent-CFTED uses tree-edit distance to rerank a set of candidates retrieved by formula FastText embeddings trained on structure features, see Section 2 for detail. And the latex_L8_a040 from MathDowsers is the default configuration for a newly rewritten and improved system on their previous Tangent-L system, with a relative weight of 0.40 on math tuples (over text terms).</p><p>Task 3 Baselines: In Task3, text-davinci-002, the most capable model of GPT-3 <ref type="bibr" coords="14,454.32,656.03,18.03,10.91" target="#b59">[60]</ref> is used as the baseline system. Another generative run amps3_se1_hints by the TU_DBS team uses</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Results for the ARQMath-3 (2022) main task. Our baseline and submission runs are compared to a selected set of the best results from other participating teams. We are ranked at the top in terms of all effectiveness metrics. In addition to the official measurements, we have also reported the BPref metric as well as the average number of judged hits per topic. Our baseline runs using structure search without learning on data can still be competitive among top systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>ARQMath-3 the GPT-2 model <ref type="bibr" coords="15,169.10,370.14,18.07,10.91" target="#b60">[61]</ref> but is further fine-tuned on the AMPS dataset <ref type="bibr" coords="15,401.63,370.14,16.41,10.91" target="#b30">[31]</ref>. They additionally prepend the prompt word "HINT" to the beginning during decoding. On the other hand, the DPRL run, SBERT-SVMRank, uses an extractive approach based on SVM and Sentence-BERT models.</p><p>As required, we apply the same methods that generate our runs for Task 1 and Task 2 in all evaluations. For Task 3 in ARQMath-3, our methods are described in Section 4.7.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Effectiveness Discussion</head><p>Analysis: Our system has advanced substantially from 2021, this comes in with two folds, i.e., the Approach Zero improvements and the introduction of ColBERT: <ref type="bibr" coords="15,431.07,506.99,11.81,10.91" target="#b0">(1)</ref> The boost in Approach Zero scores is mainly attributed to our engineering improvements, this includes the enhancement of our OPT representation (see Section 3.1 and 4.1). These changes make our Approach Zero base runs alone very effective, especially in formula retrieval, outperforming other systems in almost every metric without using any training data. <ref type="bibr" coords="15,412.24,561.19,11.81,10.91" target="#b1">(2)</ref> Because this year we have introduced a strong dense retriever and it is demonstrated to be a complementary component to the structure search. Although combining a much more efficient DPR dense retriever <ref type="bibr" coords="15,128.95,601.84,12.69,10.91" target="#b2">[3]</ref> may still be very effective according to our previous study <ref type="bibr" coords="15,394.04,601.84,16.08,10.91" target="#b9">[10]</ref>, we find the ColBERT model more effective and robust as it provides fine-grained contextualized embeddings.</p><p>Our implementation of the ColBERT is among the few bi-encoder dense retrievers being able to achieve effective results in the formula-centered retrieval task, i.e., Task 2, despite the fact that we are using the same ColBERT model from Task 1, trained on math QA posts with text around formulas. However, we notice the ranking on structure search does not provide such</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>Results for the ARQMath-3 (2022) formula retrieval task. Our baseline and submission runs are compared to a selected set of the best results from other participating teams. We are ranked at the top in terms of all effectiveness metrics. In addition to the official measurements, we have also reported the BPref metric as well as the average number of judged hits per topic. Our baseline run using structure search without learning on data can also outperform other systems except for NDCG'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>ARQMath 2 "♭": unsubmitted runs, " †": using unsupervised method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head><p>Effectiveness evaluation for ARQMath-3 Labs (Task 3). The Posts column lists the candidate selection strategy described in Section 4.7.2. The Start column lists whether we select the answer sentence from the beginning of the answer posts or not, as described in Section 4.7.3. The Type column lists the type of the QA model: 'E' for extractive and 'G' for generative. benefits as it is compared to fusion results with first-stage ColBERT search. This indicates that ColBERT provides fewer benefits in boosting the precision of a structure search system than it provides through adding recall to the base run. Interestingly, the powerful GPT-3 model, i.e., text-davinci-002, can answer math questions very well, even better than our extractive approach based on a highly effective retriever. Although it is unclear to us whether this GPT-3 model simply recalls some holdout answers in the ARQMath dataset from its training data, its performance is surprising to us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization:</head><p>To illustrate the benefits that ColBERT can provide, we visualize its final scoring matrix during the MaxSim operation (See Figure <ref type="figure" coords="17,348.40,491.62,3.71,10.91">5</ref>.5). The heatmap in Figure <ref type="figure" coords="17,476.29,491.62,4.27,10.91">5</ref>.5 are ColBERT partial scores of each pair of tokens in two similar but heterogeneous sentences, i.e., "Inequality between norm 1, norm 2 and norm ∞ of matrices" and "‖𝐴‖ 2 ≤ √︀ ‖𝐴‖ 1 ‖𝐴‖ ∞ ". The ColBERT model is able to (1) identify the same math entity even if they are from different modes, (2) handle formulas even if they are malformed and cannot be handled correctly by our existing parser, and (3) capture the similarity between relevant formulas even if they are structurally different. For example, it can associate text entity inequality, norm, and matrices to the correct "≤", "|" math tokens, and the variable "𝑎" (it actually associates to an uppercase "𝐴" in the original topic, this is because the Huggingface tokenizer changes the input to lowercase by default. However, this is unintended behavior, we may need to fix it in the future). In addition, the [CLS] embedding may also capture high-level passage semantics. These strengths will offset some of the most important weaknesses in our structure search system. In the ColBERT case, we need to load 14 shards into GPU memory subsequently for each query topic due to our GPU memory limit.  In practice, we need to load multiple shards into GPU when GPU memory is not sufficient to perform the matrix multiplication for all candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Efficiency Discussion</head><p>We report our efficiency for both Approach Zero and our implementation of ColBERT in Figure <ref type="figure" coords="18,500.34,604.47,3.66,10.91" target="#fig_2">3</ref>. The Approach Zero run times are reported on a personal workstation with Intel Core i5-8600K CPU, 32 GiB memory, and Toshiba HDWD110 hard drive. The ColBERT is running on a server with Xeon(R) 4210 CPU, 370 GiB memory and A6000 GPUs. Both passes are running in a single-thread experimental environment.</p><p>As illustrated in Figure <ref type="figure" coords="19,203.57,86.97,3.67,10.91" target="#fig_2">3</ref>, the Approach Zero system is able to finish a query in a sub-second on average even if it is configured to return top-1000 results. Compared to a 3 times higher average run time we have reported in 2021 <ref type="bibr" coords="19,292.40,114.06,18.07,10.91" target="#b46">[47]</ref> where we run evaluation on a potentially heavily loaded shared server, we believe our evaluation this time on a personal workstation in a low workload environment reflects the efficiency more accurately. In addition, we find that the run times become much smaller after one "warm-up" run. To investigate this, we use the Linux strace -cw command to summarize the wall clock times for the query processing of the first 10 ARQMath-3 topics. We find that the first-time run spends 62.10% of the time on the read system calls which are invoked about 300K times in total and they mainly reflect the disk IO wait times. After the first run, only 16.46% of the time is spent on the read system calls. This indicates that the index is implicitly cached into memory (e.g., by the OS or filesystem) during the first run, and this first-time run could cause a high variance if not explicitly caching the index into memory. Because our run times are reported after running queries a few times, they should not be considered as entirely on-disk runs.</p><p>For the ColBERT model, because we use 512 maximum number of embeddings and the ARQMath topics contain a lot of math tokens and are generally long enough, it is suboptimal in efficiency. Furthermore, the way our ColBERT was initially implemented considers this large GPU memory consumption (quadratic in query length) and low memory resource constraints imposed on the cluster we were running. As a result, it has to load and spill multiple shards of the index in sequence for each query to cope with different resource limits, resulting in the notable inefficiency in our ColBERT pass. In our case, it takes more than 20 seconds to run a single topic on average, each loading 14 shards of the 312 million embeddings in total. In fact, the majority of the query processing in latencies is to locate candidates in the main memory and load them to GPU (See Figure <ref type="figure" coords="19,246.48,398.60,3.65,10.91" target="#fig_3">4</ref>). Obviously, the resource requirement and time cost for our current ColBERT model implementation are impractical. However, recent developments of ColBERT <ref type="bibr" coords="19,131.94,425.70,11.23,10.91" target="#b6">[7,</ref><ref type="bibr" coords="19,145.80,425.70,8.88,10.91" target="#b7">8]</ref> have shown the potential to lower the order of magnitude of the space footprint and query latencies. Additionally, other effective sparse retrieval systems based on learned dense representations <ref type="bibr" coords="19,188.44,452.79,16.38,10.91" target="#b61">[62,</ref><ref type="bibr" coords="19,207.56,452.79,12.53,10.91" target="#b62">63,</ref><ref type="bibr" coords="19,222.83,452.79,14.00,10.91" target="#b63">64]</ref> also show promising results. These could be the alternatives to be studied by us while keeping our system at the same effectiveness level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>It is shown that a structure-match search method, when combined with a dense retriever in an end-to-end pipeline, can be very effective in the math answer retrieval tasks. This is because the ColBERT model can discover related connections and overcome the issue of imposing too many lexical or structural constraints over search candidates. However, the need to keep multiple embedding vectors makes it expensive in resources. We believe an important direction to continuously advance MIR in the future is to efficiently and effectively capture semantic similarities lacking lexical agreements and, additionally, to capture math transformations beyond structure matching. Lastly, we are truly at the dawn of an exciting time where large language models like the GPT-3 and others <ref type="bibr" coords="19,239.34,633.36,16.38,10.91" target="#b64">[65,</ref><ref type="bibr" coords="19,258.44,633.36,14.00,10.91" target="#b65">66]</ref> keep surprising us with their capabilities. It remains to be seen whether a generative approach in the future can serve as a dominant role to directly and fully handle queries in the domain of math IR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,276.29,228.84,9.96;4,318.13,276.95,25.14,9.65;4,343.27,275.38,3.97,6.12;4,350.04,276.29,155.94,9.96;4,88.99,288.24,417.17,9.96;4,88.93,300.20,392.38,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Operator Tree representation for formula "𝑈 𝑛 = 𝑛 2 + 𝑛" (Topic B.285). Operator and leaf (i.e., operand) are denoted by circle and box respectively. In order to improve recall, operands with or without subscript (sub) or superscript (sup) are represented canonically under a subsup token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="17,89.29,386.41,418.36,9.96;17,89.29,398.36,416.94,9.96;17,89.29,410.32,416.94,12.11;17,89.29,422.27,254.38,9.96;17,146.04,98.23,331.68,226.32"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization for the ColBERT token matching between two heterogeneous tokenized sentences extracted from an example topic (A.301). Tokens wrapped in dollar signs are those that originally appear inside math-mode L A T E X. The highlighted grids in red demonstrate the model is capable to link the same math entities from math mode and non-math mode.</figDesc><graphic coords="17,146.04,98.23,331.68,226.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="18,89.29,250.21,416.69,9.96;18,88.98,262.16,417.00,9.96;18,89.29,274.12,416.94,9.96;18,89.29,286.07,267.14,9.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Query run times of Approach Zero and our implementation of ColBERT evaluated by the ARQMath-3 Task 1 topics. Approach Zero latencies are shown for different threshold values, and the distribution is averaged over 5 runs.In the ColBERT case, we need to load 14 shards into GPU memory subsequently for each query topic due to our GPU memory limit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="18,89.29,521.51,416.69,9.96;18,89.29,533.46,416.70,9.96;18,89.29,545.42,326.98,9.96"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Latency decomposition for our ColBERT implementation. This graph only reflects single-shard latencies in the MaxSim operation.In practice, we need to load multiple shards into GPU when GPU memory is not sufficient to perform the matrix multiplication for all candidates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,89.29,614.33,416.70,24.46"><head></head><label></label><figDesc>It is made of 1.69M documents crawled from the MSE and the Art of Problem Solving community (AoPS) website. During training, we exclude MSE posts after 2018. Sentence pairs are generated and concatenated with a [SEP]</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,88.99,90.49,138.11,40.45"><head>Table 1</head><label>1</label><figDesc>Configuration for Approach Zero.</figDesc><table coords="12,95.27,122.02,23.73,8.92"><row><cell>Math</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,121.49,121.83,392.90,26.73"><head>path weight Formula length penalty</head><label></label><figDesc>𝜂 BM25+ (𝑏, 𝑘 1 ) Symbol match points (𝑏 1 , 𝑏 2 )</figDesc><table coords="12,129.51,138.60,337.10,9.96"><row><cell>2.5</cell><cell>0.3</cell><cell>0.75, 2.0</cell><cell>0.94, 0.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,88.99,170.66,417.00,117.85"><head>Table 2</head><label>2</label><figDesc>Effectiveness impact evaluated by the ARQMath-2 topics for major changes in Approach Zero since 2021. Changes are shown in chronological order. (see Section 4.1 for detailed descriptions)</figDesc><table coords="12,147.85,214.15,299.59,74.36"><row><cell>Changes</cell><cell cols="4">Checkpoint SHA1 NDCG' MAP' P'@10</cell></row><row><cell>(Our 2021 run)</cell><cell>eae6690e</cell><cell>0.351</cell><cell>0.137</cell><cell>0.189</cell></row><row><cell>Improved OPT</cell><cell>14d311b2</cell><cell>0.365</cell><cell>0.174</cell><cell>0.216</cell></row><row><cell>Simplified OPT</cell><cell>77e3571b</cell><cell>0.372</cell><cell>0.176</cell><cell>0.227</cell></row><row><cell>Bug fix</cell><cell>35afeb50</cell><cell>0.374</cell><cell>0.178</cell><cell>0.231</cell></row><row><cell>Switch stemmer etc.</cell><cell>(up to date)</cell><cell>0.383</cell><cell>0.190</cell><cell>0.235</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,108.61,671.03,396.04,8.97"><p>Approach Zero or approach0 gets its name from the word "asymptotics", the core concept to define a limit.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,108.93,670.94,328.91,8.97"><p>See an open-source parser package for examples: https://github.com/fwtompa/mathtuples</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="7,108.93,649.12,397.64,8.97;7,89.29,660.08,113.33,8.97"><p>For reproducibility, our system pipelines and model checkpoints are made available: https://github.com/ approach0/pya0/tree/arqmath3</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="7,108.66,671.04,300.39,8.97"><p>To download our raw corpus: https://vault.cs.uwaterloo.ca/s/G36Mjt55HWRSNRR</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="9,108.93,649.11,397.03,8.97"><p>Our only manual intervention occurs at keywords extraction from official topics, other phases are automatic.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="9,108.93,660.06,391.28,8.97"><p>https://github.com/approach0/pya0/blob/arqmath3/topics-and-qrels/topics.arqmath-2022-task1-manual.txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="9,108.93,671.02,389.78,8.97"><p>https://github.com/approach0/pya0/blob/arqmath3/topics-and-qrels/topics.arqmath-2022-task2-refined.txt</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>As ARQMath has come to an end in the CLEF Lab, we greatly appreciate task organizers and the <rs type="funder">National Science Foundation (NSF)</rs> for hosting and funding the CLEF ARQMath tasks for the past three years. Without these opportunities, we could not identify our weaknesses and advance our system continuously.</p><p>This research was supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council (NSERC) of Canada</rs>. Computational resources were provided by <rs type="funder">Compute Ontario and Compute Canada</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="20,112.66,251.20,394.61,10.91;20,112.33,264.75,394.85,10.91;20,112.66,278.30,394.62,10.91;20,112.66,291.85,295.17,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="20,315.08,251.20,192.19,10.91;20,112.33,264.75,150.71,10.91">Finding old answers to new math questions: The ARQMath Lab at CLEF 2020</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<ptr target="https://www.cs.rit.edu/~rlaz/files/ARQMATH_Lab_overview_.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="20,292.48,278.30,157.37,10.91">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Castells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Martins</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,305.40,393.32,10.91;20,112.66,318.95,393.33,10.91;20,112.66,332.50,394.62,10.91;20,112.31,346.05,166.60,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="20,330.36,305.40,109.68,10.91;20,474.74,305.40,31.24,10.91;20,112.66,318.95,226.82,10.91">Second CLEF lab on answer retrieval for questions on math</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-01.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="20,467.61,318.95,38.37,10.91;20,112.66,332.50,311.63,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Overview of ARQMath-2. working notes version</note>
</biblStruct>

<biblStruct coords="20,112.66,359.59,394.53,10.91;20,112.66,373.14,395.01,10.91;20,112.66,389.13,97.35,7.90" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Oğuz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04906</idno>
		<idno>arXiv:2004.04906</idno>
		<title level="m" coord="20,112.66,373.14,276.11,10.91">Dense passage retrieval for open-domain question answering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,400.24,393.33,10.91;20,112.66,413.79,394.04,10.91;20,112.66,427.34,75.82,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="20,217.69,400.24,288.30,10.91;20,112.66,413.79,120.44,10.91">ColBERT: Efficient and effective passage search via contextualized late interaction over BERT</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401075</idno>
		<ptr target="https://dl.acm.org/doi/abs/10.1145/3397271.3401075" />
	</analytic>
	<monogr>
		<title level="m" coord="20,263.70,413.79,23.84,10.91">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,440.89,86.45,10.91;20,216.85,440.89,290.33,10.91;20,112.66,454.44,207.25,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08253</idno>
		<idno>arXiv:2104.08253</idno>
		<title level="m" coord="20,216.85,440.89,286.27,10.91">Condenser: A pre-training architecture for dense retrieval</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,467.99,393.33,10.91;20,112.66,481.54,394.04,10.91;20,112.41,495.09,112.18,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="20,362.05,467.99,143.93,10.91;20,112.66,481.54,224.66,10.91">Efficiently teaching an effective dense retriever with balanced topic aware sampling</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hofstätter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3462891</idno>
		<ptr target="https://doi.org/10.1145/3404835.3462891" />
	</analytic>
	<monogr>
		<title level="m" coord="20,360.44,481.54,23.07,10.91">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,508.64,395.17,10.91;20,112.66,522.18,395.01,10.91;20,112.66,538.18,97.35,7.90" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Saad-Falcon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01488</idno>
		<idno>arXiv:2112.01488</idno>
		<title level="m" coord="20,420.91,508.64,86.93,10.91;20,112.66,522.18,269.33,10.91">ColBERTv2: Effective and efficient retrieval via lightweight late interaction</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,549.28,395.17,10.91;20,112.66,562.83,271.52,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.09707</idno>
		<idno>arXiv:2205.09707</idno>
		<title level="m" coord="20,324.99,549.28,182.84,10.91;20,112.66,562.83,56.36,10.91">Plaid: An efficient engine for late interaction retrieval</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,576.38,393.33,10.91;20,112.66,589.93,165.29,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="20,141.07,576.38,364.91,10.91;20,112.66,589.93,35.82,10.91">A proposed conceptual framework for a representational approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01529</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,603.48,393.33,10.91;20,112.66,617.03,297.01,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="20,247.63,603.48,258.35,10.91;20,112.66,617.03,167.53,10.91">Evaluating token-level and passage-level dense retrieval models for math information retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11163</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,630.58,394.53,10.91;20,112.66,644.13,357.00,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="20,218.55,630.58,284.26,10.91">Technical aspects of the digital library of mathematical functions</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Youssef</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1022967814992</idno>
		<ptr target="https://link.springer.com/article/10.1023/A:1022967814992" />
	</analytic>
	<monogr>
		<title level="j" coord="20,127.29,644.13,23.97,10.91">AMAI</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,657.68,393.33,10.91;21,112.66,86.97,394.04,10.91;21,112.66,100.52,210.32,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="20,288.90,657.68,217.08,10.91;21,112.66,86.97,36.70,10.91">Search mathematical formulas by mathematical formulas</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hijikata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nishida</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-02556-3_46.pdf</idno>
		<ptr target="https://link.springer.com/content/pdf/10.1007/978-3-642-02556-3_46.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="21,172.30,86.97,166.46,10.91">SHI (Symposium on Human Interface)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,114.06,393.33,10.91;21,112.66,127.61,395.01,10.91;21,112.66,141.16,62.67,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="21,205.48,114.06,300.51,10.91;21,112.66,127.61,34.66,10.91">An approach to similarity search for mathematical expressions using MathML</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yokoi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<ptr target="https://dml.cz/handle/10338.dmlcz/702557" />
	</analytic>
	<monogr>
		<title level="m" coord="21,171.55,127.61,154.56,10.91">DML (Digital Mathematics Library)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,154.71,393.33,10.91;21,112.66,168.26,363.95,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="21,215.31,154.71,290.68,10.91;21,112.66,168.26,75.84,10.91">Structural similarity search for formulas using leaf-root paths in operator subtrees</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<ptr target="https://par.nsf.gov/servlets/purl/10124342" />
	</analytic>
	<monogr>
		<title level="m" coord="21,211.43,168.26,20.13,10.91">ECIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,181.81,393.60,10.91;21,112.66,195.36,395.01,10.91;21,112.41,208.91,124.35,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="21,342.10,181.81,164.16,10.91;21,112.66,195.36,123.92,10.91">Accelerating substructure similarity search for formula retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-45439-5_47</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-3-030-45439-5_47" />
	</analytic>
	<monogr>
		<title level="m" coord="21,261.15,195.36,20.54,10.91">ECIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,222.46,394.52,10.91;21,112.66,236.01,316.34,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="21,217.24,222.46,237.03,10.91">Recognition and retrieval of mathematical expressions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blostein</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10032-011-0174-4</idno>
		<ptr target="https://link.springer.com/article/10.1007/s10032-011-0174-4" />
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IJDAR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,249.56,196.32,10.91;21,336.14,249.56,171.69,10.91;21,112.66,263.11,257.22,10.91;21,395.65,263.11,111.63,10.91;21,112.66,276.66,363.53,10.91;21,112.66,290.20,393.05,10.91;21,112.41,303.75,141.33,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="21,336.14,249.56,171.69,10.91;21,112.66,263.11,252.61,10.91">Layout-based substitution tree indexing and retrieval for mathematical expressions</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schellenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.912502.short?SSO=1</idno>
		<ptr target="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/8297/82970I/Layout-based-substitution-tree-indexing-and-retrieval-for-mathematical-expressions/10.1117/12.912502.short?SSO=1" />
	</analytic>
	<monogr>
		<title level="m" coord="21,421.74,263.11,17.48,10.91">DRR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,317.30,393.53,10.91;21,112.66,330.85,394.62,10.91;21,112.66,344.40,394.04,10.91;21,112.66,357.95,236.57,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="21,233.68,317.30,272.51,10.91;21,112.66,330.85,367.85,10.91">Combining tf-idf text retrieval with an inverted index over symbol pairs in math expressions: The tangent math search engine at ntcir 2014</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Pattaniyil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<ptr target="https://research.nii.ac.jp/ntcir/workshop/OnlineProceedings11/pdf/NTCIR/Math-2/08-NTCIR11-MATH-PattaniyilN.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="21,112.66,344.40,28.53,10.91">NTCIR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,371.50,393.33,10.91;21,112.66,385.05,395.01,10.91;21,112.66,401.04,97.35,7.90" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Tompa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06235</idno>
		<idno>arXiv:1507.06235</idno>
		<title level="m" coord="21,323.63,371.50,182.35,10.91;21,112.66,385.05,269.57,10.91">The Tangent search engine: Improved similarity metrics and scalability for math formula search</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,412.15,393.33,10.91;21,112.66,425.70,394.04,10.91;21,112.66,439.25,130.32,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="21,317.70,412.15,188.28,10.91;21,112.66,425.70,192.63,10.91">Multi-stage math formula search: Using appearance-based similarity metrics at scale</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Tompa</surname></persName>
		</author>
		<idno type="DOI">10.1145/2911451.2911512</idno>
		<ptr target="https://dl.acm.org/doi/abs/10.1145/2911451.2911512" />
	</analytic>
	<monogr>
		<title level="m" coord="21,327.72,425.70,23.14,10.91">SIGIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,452.79,394.53,10.91;21,112.66,466.34,335.31,10.91" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="21,244.52,452.79,257.82,10.91">Choosing math features for BM25 ranking with Tangent-L</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Tompa</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209280.3209527</idno>
		<ptr target="https://dl.acm.org/doi/abs/10.1145/3209280.3209527" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>DocEng</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,479.89,393.61,10.91;21,112.66,493.44,331.69,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="21,153.19,479.89,232.18,10.91">Math Information Retrieval using a Text Search Engine</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Dallas</surname></persName>
		</author>
		<ptr target="https://uwspace.uwaterloo.ca/handle/10012/13329" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="21,112.66,506.99,393.32,10.91;21,112.66,520.54,394.03,10.91;21,112.41,534.09,136.90,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kassaie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Labahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Marzouk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Tompa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-85251-1_16</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-3-030-85251-1_16" />
		<title level="m" coord="21,466.30,506.99,39.68,10.91;21,112.66,520.54,144.94,10.91">Dowsing for math answers with Tangent-L</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,547.64,393.33,10.91;21,112.39,561.19,395.27,10.91;21,112.66,574.74,17.97,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="21,289.69,547.64,216.30,10.91;21,112.39,561.19,128.19,10.91">Dowsing for answers to math questions: Ongoing viability of traditional MathIR</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kassaie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Tompa</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-05.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="21,263.79,561.19,20.63,10.91">CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,588.29,393.33,10.91;21,112.66,601.84,394.04,10.91;21,112.66,615.39,90.22,10.91" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="21,155.49,588.29,350.50,10.91;21,112.66,601.84,28.35,10.91">Dowsing for Math Answers: Exploring MathCQA with a Math-aware Search Engine</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="https://uwspace.uwaterloo.ca/handle/10012/17696" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="21,112.66,628.93,393.33,10.91;21,112.66,642.48,394.03,10.91;21,112.66,656.03,254.57,10.91" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="21,279.23,628.93,226.76,10.91;21,112.66,642.48,16.21,10.91">MCAT math retrieval system for NTCIR-12 MathIR task</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">Y</forename><surname>Kristianto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Topic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<ptr target="http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings12/pdf/ntcir/MathIR/04-NTCIR12-MathIR-KristiantoGY.pdf" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>NTCIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,669.58,393.33,10.91;22,112.66,86.97,395.01,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="21,209.02,669.58,296.97,10.91;22,112.66,86.97,63.96,10.91">Layout and semantics: Combining representations for mathematical formula search</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080748</idno>
		<ptr target="https://dl.acm.org/doi/abs/10.1145/3077136.3080748" />
	</analytic>
	<monogr>
		<title level="m" coord="22,199.44,86.97,23.16,10.91">SIGIR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,100.52,394.61,10.91;22,112.66,114.06,266.32,10.91" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<ptr target="https://par.nsf.gov/servlets/purl/10198749" />
		<title level="m" coord="22,280.54,100.52,207.26,10.91">DPRL systems in the CLEF 2020 ARQMath lab</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,127.61,393.33,10.91;22,112.66,141.16,266.80,10.91" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<idno>arXiv:1607.04606</idno>
		<title level="m" coord="22,331.70,127.61,174.29,10.91;22,112.66,141.16,50.99,10.91">Enriching word vectors with subword information</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,154.71,341.59,10.91;22,112.66,168.26,393.33,10.91;22,112.66,181.81,282.76,10.91" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<idno>arXiv:2112.11446</idno>
		<title level="m" coord="22,228.68,168.26,277.30,10.91;22,112.66,181.81,66.85,10.91">Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,195.36,395.17,10.91;22,112.66,208.91,393.33,10.91;22,112.66,222.46,107.17,10.91" xml:id="b30">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03874</idno>
		<title level="m" coord="22,143.47,208.91,288.94,10.91">Measuring mathematical problem solving with the math dataset</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="22,112.66,236.01,393.33,10.91;22,112.66,249.56,316.12,10.91" xml:id="b31">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.00377</idno>
		<idno>arXiv:2105.00377</idno>
		<title level="m" coord="22,274.93,236.01,231.06,10.91;22,112.66,249.56,100.16,10.91">MathBERT: A pre-trained model for mathematical formula understanding</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,263.11,331.36,10.91;22,460.09,263.11,45.90,10.91;22,112.66,276.66,394.04,10.91;22,112.66,290.20,375.58,10.91" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Topic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<ptr target="http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings12/pdf/ntcir/OVERVIEW/01-NTCIR12-OV-MathIR-ZanibbiR.pdf" />
		<title level="m" coord="22,460.09,263.11,45.90,10.91;22,112.66,276.66,97.37,10.91">NTCIR-12 MathIR task overview</title>
		<imprint>
			<publisher>NTCIR</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,303.75,395.17,10.91;22,112.66,317.30,394.04,10.91;22,112.66,330.85,278.51,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="22,151.08,317.30,110.79,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="22,287.11,317.30,19.96,10.91">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,344.40,393.33,10.91;22,112.66,357.95,395.01,10.91" xml:id="b34">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<idno>arXiv:1810.04805</idno>
		<title level="m" coord="22,323.15,344.40,182.83,10.91;22,112.66,357.95,179.85,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,371.50,393.33,10.91;22,112.66,385.05,393.86,10.91;22,112.66,398.60,35.59,10.91" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="22,314.63,371.50,191.35,10.91;22,112.66,385.05,127.87,10.91">Three is better than one: Ensembling math information retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Novotnỳ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stefánik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lupták</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_235.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="22,268.07,385.05,21.16,10.91">CLEF</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,412.15,393.32,10.91;22,112.66,425.70,395.01,10.91;22,112.66,439.25,17.97,10.91" xml:id="b36">
	<monogr>
		<title level="m" type="main" coord="22,411.22,412.15,94.76,10.91;22,112.66,425.70,127.09,10.91">Ensembling ten math information retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Novotnỳ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Štefánik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lupták</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Geletka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zelina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-06.pdf" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,452.79,394.53,10.91;22,112.66,466.34,207.25,10.91" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<idno>arXiv:1908.10084</idno>
		<title level="m" coord="22,219.74,452.79,282.85,10.91">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,479.89,393.33,10.91;22,112.66,493.44,373.57,10.91" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="22,248.08,479.89,257.91,10.91;22,112.66,493.44,73.63,10.91">Psu at clef-2020 arqmath track: Unsupervised re-ranking using pretraining</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_121.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="22,213.31,493.44,21.05,10.91">CLEF</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,506.99,395.17,10.91;22,112.66,520.54,353.40,10.91" xml:id="b39">
	<monogr>
		<title level="m" type="main" coord="22,247.84,506.99,259.99,10.91;22,112.66,520.54,106.47,10.91">Ranked list fusion and re-ranking with pre-trained transformers for arqmath lab</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-08.pdf" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,534.09,394.62,10.91;22,112.66,547.64,395.01,10.91;22,112.66,561.19,212.28,10.91" xml:id="b40">
	<analytic>
		<title level="a" type="main" coord="22,289.37,534.09,217.91,10.91;22,112.66,547.64,322.84,10.91">DPRL systems in the CLEF 2021 ARQMath lab: Sentence-BERT for answer retrieval, learning-to-rank for formula retrieval</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-04.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="22,457.17,547.64,20.62,10.91">CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,574.74,393.33,10.91;22,112.66,588.29,314.16,10.91" xml:id="b41">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10351</idno>
		<title level="m" coord="22,410.87,574.74,95.12,10.91;22,112.66,588.29,183.88,10.91">TinyBERT: Distilling BERT for natural language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,601.84,394.53,10.91;22,112.66,615.39,393.71,10.91;22,112.66,628.93,388.45,10.91" xml:id="b42">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<title level="m" coord="22,448.57,615.39,57.79,10.91;22,112.66,628.93,258.71,10.91">Ms marco: A human generated machine reading comprehension dataset</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,642.48,393.32,10.91;22,112.66,656.03,395.01,10.91" xml:id="b43">
	<analytic>
		<title level="a" type="main" coord="22,260.04,642.48,245.94,10.91;22,112.66,656.03,69.09,10.91">An ALBERT-based similarity measure for mathematical answer retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Reusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lehner</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3463023</idno>
		<ptr target="https://dl.acm.org/doi/abs/10.1145/3404835.3463023" />
	</analytic>
	<monogr>
		<title level="m" coord="22,203.45,656.03,22.91,10.91">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,669.58,395.01,10.91;23,112.66,86.97,212.28,10.91" xml:id="b44">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Reusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lehner</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-07.pdf" />
		<title level="m" coord="22,257.59,669.58,148.43,10.91">TU_DBS in the ARQMath lab 2021</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,100.52,393.53,10.91;23,112.66,114.06,361.04,10.91" xml:id="b45">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<title level="m" coord="23,407.44,100.52,98.74,10.91;23,112.66,114.06,231.08,10.91">Albert: A lite bert for self-supervised learning of language representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,127.61,393.57,10.91;23,112.28,141.16,395.00,10.91;23,112.66,154.71,266.74,10.91" xml:id="b46">
	<analytic>
		<title level="a" type="main" coord="23,307.49,127.61,198.74,10.91;23,112.28,141.16,375.49,10.91">Approach Zero and Anserini at the CLEF-2021 ARQMath track: Applying substructure search and BM25 on operator tree path tokens</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-09.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="23,112.66,154.71,21.05,10.91">CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,168.26,393.33,10.91;23,112.66,181.81,395.01,10.91;23,112.66,195.36,131.48,10.91" xml:id="b47">
	<monogr>
		<title level="m" type="main" coord="23,161.59,168.26,344.39,10.91;23,112.66,181.81,100.80,10.91">A novel similarity-search method for mathematical content in LaTeX markup and its implementation</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<ptr target="https://udspace.udel.edu/handle/19716/17656" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>University of Delaware</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="23,112.66,208.91,393.33,10.91;23,112.66,222.46,395.01,10.91;23,112.41,236.01,23.60,10.91" xml:id="b48">
	<analytic>
		<title level="a" type="main" coord="23,181.89,208.91,213.47,10.91">Lower-bounding term frequency normalization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,421.05,208.91,84.93,10.91;23,112.66,222.46,348.72,10.91">Proceedings of the 20th ACM international conference on Information and knowledge management</title>
		<meeting>the 20th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="7" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,249.56,393.61,10.91;23,112.26,263.11,395.41,10.91;23,112.66,276.66,128.13,10.91" xml:id="b49">
	<analytic>
		<title level="a" type="main" coord="23,321.21,249.56,185.06,10.91;23,112.26,263.11,82.19,10.91">Searching in one billion vectors: Re-rank with source coding</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tavenard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Amsaleg</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5946540" />
	</analytic>
	<monogr>
		<title level="m" coord="23,217.57,263.11,55.05,10.91">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,290.20,394.53,10.91;23,112.66,303.75,394.53,10.91;23,112.66,317.30,393.33,10.91;23,112.66,330.85,361.77,10.91" xml:id="b50">
	<analytic>
		<title level="a" type="main" coord="23,296.48,317.30,209.50,10.91;23,112.66,330.85,45.44,10.91">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m" coord="23,181.20,330.85,30.55,10.91">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,344.40,395.17,10.91;23,112.66,357.95,388.71,10.91" xml:id="b51">
	<analytic>
		<title level="a" type="main" coord="23,255.31,344.40,179.93,10.91">Billion-scale similarity search with GPUs</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/8733051" />
	</analytic>
	<monogr>
		<title level="j" coord="23,444.73,344.40,63.11,10.91;23,112.66,357.95,77.55,10.91">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,371.50,394.53,10.91;23,112.66,385.05,280.22,10.91" xml:id="b52">
	<analytic>
		<title level="a" type="main" coord="23,194.68,371.50,258.87,10.91">PyA0: A Python toolkit for accessible math-aware search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3462794</idno>
		<ptr target="https://dl.acm.org/doi/abs/10.1145/3404835.3462794" />
	</analytic>
	<monogr>
		<title level="m" coord="23,478.58,371.50,23.84,10.91">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,398.60,393.33,10.91;23,112.66,412.15,284.72,10.91" xml:id="b53">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.13019</idno>
		<idno>arXiv:2102.13019</idno>
		<title level="m" coord="23,246.03,398.60,259.96,10.91;23,112.66,412.15,69.38,10.91">Investigating the limitations of transformers with simple arithmetic tasks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,425.70,393.33,10.91;23,112.66,439.25,395.00,10.91;23,112.66,455.24,97.35,7.90" xml:id="b54">
	<monogr>
		<title level="m" type="main" coord="23,309.25,425.70,196.73,10.91;23,112.66,439.25,85.73,10.91">Challenges in generalization in open domain question answering</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">S H</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2109.01156" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,466.34,395.01,10.91;23,112.66,482.34,97.35,7.90" xml:id="b55">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<idno>arXiv:1711.05101</idno>
		<title level="m" coord="23,219.57,466.34,172.66,10.91">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,493.44,393.33,10.91;23,112.66,506.99,394.61,10.91;23,112.66,520.54,179.39,10.91" xml:id="b56">
	<analytic>
		<title level="a" type="main" coord="23,176.90,493.44,161.15,10.91">Training linear svms in linear time</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="DOI">10.1145/1150402.1150429</idno>
		<ptr target="https://doi.org/10.1145/1150402.1150429" />
	</analytic>
	<monogr>
		<title level="m" coord="23,368.05,493.44,137.93,10.91;23,112.66,506.99,340.02,10.91">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,534.09,395.17,10.91;23,112.66,547.64,393.33,10.91;23,112.30,561.19,394.89,10.91;23,112.66,574.74,22.69,10.91" xml:id="b57">
	<analytic>
		<title level="a" type="main" coord="23,398.52,534.09,109.32,10.91;23,112.66,547.64,5.17,10.91;23,155.30,547.64,274.87,10.91">Third CLEF lab on Answer Retrieval for Questions on Math</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Novotný</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,169.15,561.19,332.87,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of ARQMath-3. Working Notes Version</note>
</biblStruct>

<biblStruct coords="23,112.66,588.29,393.33,10.91;23,112.66,601.84,256.49,10.91" xml:id="b58">
	<monogr>
		<title level="m" type="main" coord="23,360.02,588.29,145.97,10.91;23,112.66,601.84,181.68,10.91">Diverse semantics representation is king: Mirmu and msm at arqmath 2022</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Geletka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kalivoda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Štefánik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,615.39,395.17,10.91;23,112.66,628.93,244.37,10.91;23,376.60,628.93,131.23,10.91;23,112.66,642.48,394.04,10.91;23,112.41,656.03,207.45,10.91" xml:id="b59">
	<analytic>
		<title level="a" type="main" coord="23,376.60,628.93,131.23,10.91;23,112.66,642.48,59.37,10.91">Language models are fewshot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="j" coord="23,189.88,642.48,37.52,10.91">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,669.58,393.33,10.91;24,112.66,86.97,394.03,10.91;24,112.66,100.52,112.55,10.91" xml:id="b60">
	<monogr>
		<title level="m" type="main" coord="23,426.38,669.58,79.61,10.91;24,112.66,86.97,224.06,10.91">Language models are unsupervised multitask learners, OpenAI blog</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/better-language-models/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,114.06,395.17,10.91;24,112.66,127.61,387.27,10.91" xml:id="b61">
	<monogr>
		<title level="m" type="main" coord="24,433.46,114.06,74.37,10.91;24,112.66,127.61,257.79,10.91">Sparterm: Learning term-based sparse representation for fast text retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,141.16,393.53,10.91;24,112.66,154.71,294.79,10.91" xml:id="b62">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Formal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05720</idno>
		<idno>arXiv:2107.05720</idno>
		<title level="m" coord="24,293.42,141.16,212.77,10.91;24,112.66,154.71,78.95,10.91">Splade: Sparse lexical and expansion model for first stage ranking</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,168.26,393.33,10.91;24,112.66,181.81,395.01,10.91" xml:id="b63">
	<monogr>
		<title level="m" type="main" coord="24,361.89,168.26,144.10,10.91;24,112.66,181.81,182.12,10.91">SPLADE v2: Sparse lexical and expansion model for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Formal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lassance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10086</idno>
		<idno>arXiv:2109.10086</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,195.36,394.53,10.91;24,112.28,208.91,393.71,10.91;24,112.66,222.46,370.73,10.91" xml:id="b64">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bieber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00114</idno>
		<idno>arXiv:2112.00114</idno>
		<title level="m" coord="24,298.76,208.91,207.23,10.91;24,112.66,222.46,154.68,10.91">Show your work: Scratchpads for intermediate computation with language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,236.01,395.16,10.91;24,112.66,249.56,393.58,10.91;24,112.33,263.11,129.27,10.91" xml:id="b65">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<idno>arXiv:2203.11171</idno>
		<title level="m" coord="24,487.33,236.01,20.50,10.91;24,112.66,249.56,308.73,10.91">Selfconsistency improves chain of thought reasoning in language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
