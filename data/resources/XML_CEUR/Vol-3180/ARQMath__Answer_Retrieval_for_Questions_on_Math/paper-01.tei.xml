<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,250.97,15.42;1,88.78,106.66,417.21,15.42;1,89.29,128.58,238.28,15.43">Overview of ARQMath-3 (2022): Third CLEF Lab on Answer Retrieval for Questions on Math (Working Notes Version)</title>
				<funder ref="#_2AmeM6R">
					<orgName type="full">Alfred P. Sloan Foundation</orgName>
				</funder>
				<funder ref="#_ZuTVVaJ">
					<orgName type="full">National Science Foundation (USA)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,160.45,90.05,5.43"><forename type="first">Behrooz</forename><surname>Mansouri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,191.98,160.45,60.39,5.43"><forename type="first">Vít</forename><surname>Novotný</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Informatics</orgName>
								<orgName type="institution">Masaryk University</orgName>
								<address>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.01,160.45,81.80,5.43"><forename type="first">Anurag</forename><surname>Agarwal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,359.45,160.45,83.20,5.43"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,174.40,78.09,5.43"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,250.97,15.42;1,88.78,106.66,417.21,15.42;1,89.29,128.58,238.28,15.43">Overview of ARQMath-3 (2022): Third CLEF Lab on Answer Retrieval for Questions on Math (Working Notes Version)</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">A59156692D92BA6F1AF81700C1591797</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Community Question Answering</term>
					<term>Open Domain Question Answering</term>
					<term>Mathematical Information Retrieval</term>
					<term>Math-aware Search</term>
					<term>Math Formula Search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper provides an overview of the third and final year of the Answer Retrieval for Questions on Math (ARQMath-3) lab, run as part of CLEF 2022. ARQMath has aimed to introduce test collections for math-aware information retrieval. ARQMath-3 has two main tasks, Answer Retrieval (Task 1) and Formula Search (Task 2), along with a new pilot task Open Domain Question Answering (Task 3). Nine teams participated in ARQMath-3, submitting 33 runs for Task 1, 19 runs for Task 2, and 13 runs for Task 3. Tasks, topics, evaluation protocols, and results for each task are presented in this lab overview.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Math information retrieval (Math IR) aims at facilitating the access, retrieval and discovery of math resources, and is needed in many scenarios <ref type="bibr" coords="1,313.63,460.30,11.58,4.95" target="#b0">[1]</ref>. For example, many traditional courses and Massive Open Online Courses (MOOCs) release their resources (books, lecture notes and exercises, etc.) as digital files in HTML or XML. However, due to the specific characteristics of math formulae, classic search engines do not work well for indexing and retrieving math.</p><p>Math-aware search systems can be beneficial for learning activities. Students can search for references to help solve problems, increase knowledge, reduce doubts, and clarify concepts. Instructors might also benefit from these systems by creating learning communities within a classroom. For example, a teacher can pool different digital resources to create the subject matter and then let students search through them for mathematical notation and terminology. Math-aware search engines can also help researchers identify potentially useful systems, fields, and collaborators. Good examples of this interdisciplinary approach benefiting physics include the AdS/CFT correspondence and holographic duality theories. the effectiveness of individual systems was measured by manually assessing a set of topics). The Cambridge University MathIR Test Collection (CUMTC) <ref type="bibr" coords="3,361.69,103.77,12.84,4.95" target="#b5">[6]</ref> subsequently built on MREC, adding 160 test queries derived from 120 MathOverflow discussion threads (although not all queries contained math). CUMTC relevance judgments were constructed using citations to MREC documents cited in MathOverflow answers.</p><p>To the best of our knowledge, ARQMath's Task 1 is the first Math IR test collection to focus directly on answer retrieval. ARQMath's Task 2 (formula search) extends earlier work on formula search, with several improvements:</p><p>• Scale. ARQMath has an order of magnitude more assessed topics than prior formula search test collections. There are 22 topics in NTCIR-10, and 20 in NTCIR-12 WFB (+20 variants with wildcards). • Contextual Relevance. In the NTCIR-12 WFB task <ref type="bibr" coords="3,364.44,246.92,11.58,4.95" target="#b3">[4]</ref>, there was less attention to context. ARQMath Task 2, by contrast, has evolved as a contextualized formula search task, where relevance is defined both by the query and retrieved formulae and also the contexts in which those formulae appear. • Deduplication. NTCIR collections measured effectiveness using formula instances. In ARQMath we clustered visually identical formulae to avoid rewarding retrieval of multiple instances of the same formula. • Balance. ARQMath balances formula query complexity, whereas prior collections were less balanced (reannotation shows low complexity topics dominate NTCIR-10 and high complexity topics dominate NTCIR-12 WFB <ref type="bibr" coords="3,315.09,370.40,11.09,4.95" target="#b6">[7]</ref>).</p><p>In ARQMath-3, we introduced a new pilot task, Open Domain Question Answering. The most similar prior work is the SemEval 2019 <ref type="bibr" coords="3,294.89,404.73,12.99,4.95" target="#b7">[8]</ref> math question answering task, which used question sets from Math SAT practice exams in three categories: Closed Algebra, Open Algebra and Geometry. A majority of the Math SAT questions were multiple choice, with some having numeric answers.</p><p>While we have focused on search and question answering tasks in ARQMath, there are other math information processing tasks that can be considered for future work. For example, extracting definitions for identifiers, math word problem solving, and informal theorem proving are active areas of research: for a survey of recent work in these areas, see Meadows and Ferentes <ref type="bibr" coords="3,89.29,513.13,11.58,4.95" target="#b8">[9]</ref>. Summarization of mathematical texts, text/formula co-referencing, and the multimodal representation and linking of information in documents are some other examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The ARQMath Stack Exchange Collection</head><p>For ARQMath-3, we reused the collection<ref type="foot" coords="3,283.28,582.34,3.71,3.62" target="#foot_1">2</ref> from ARQMath-1 and -2. <ref type="foot" coords="3,407.90,582.34,3.71,3.62" target="#foot_2">3</ref> The collection was constructed using the March 1st, 2020 Math Stack Exchange snapshot from the Internet Archive. <ref type="foot" coords="3,501.96,595.89,3.71,3.62" target="#foot_3">4</ref>Questions and answers from 2010-2018 are included in the collection. The ARQMath test collection contains roughly 1 million questions and 28 million formulae. Formulae in the collection are annotated using &lt;span&gt; XML elements with the class attribute math-container, and a unique integer identifier given in the id attribute. Formulae are also provided separately in three index files for different formula representations (L A T E X, Presentation MathML, and Content MathML), which we describe in more detail below.</p><p>During ARQMath-2021, participants identified three issues with the ARQMath collection that had not been noticed and corrected earlier. In 2022, we have made the following improvements to the collection:</p><p>1. Formula Representations. We found and corrected 65,681 formulae with incorrect Symbol Layout Tree (SLT) and Operator Tree (OPT) representations. This resulted from incorrect handling of errors generated by the L A T E XML tool that had been used for generating those representations. 2. Clustering Visually Distinct Formulae. Correcting SLT representations resulted in a need to adjust the clustering of formula instances. Each cluster of visually identical formulae was assigned a unique 'Visual ID'. Clustering had been performed using SLT where possible, and L A T E X otherwise. To correct the clustering, we split any cluster that now included formulae with different representations. In such cases, the partition with the largest number of instances retained its Visual ID; remaining formulae were assigned to another existing Visual ID (with the same SLT or L A T E X) or, if necessary, to a new Visual ID. To break ties, the partition with the largest cumulative ARQMath-2 relevance score retained its Visual ID or, failing that, choosing the partition with the lowest Formula ID. 29,750 new Visual IDs resulted. 3. XML Errors. In the XML files for posts and comments, the L A T E X for each formula is encoded as a &lt;span&gt; XML element with the class attribute math-container. We found and corrected 108,242 formulae that had not been encoded in that way. 4. Spurious Formula Identifiers. The ARQMath collection includes an index file that includes Formula ID, Visual ID, Post ID, SLT, OPT, and L A T E X for each formula instance. However, there were also formulae in the index file that did not actually occur in any post or comment in the collection. This happened because formula extraction was initially done on the Post History file, which also contained some content that had later been removed. We added a new annotation to the formula index file to mark such cases.</p><p>The Math Stack Exchange collection was distributed to participants as XML files on Google Drive. <ref type="foot" coords="4,117.00,552.32,3.71,3.62" target="#foot_4">5</ref> To facilitate local processing, the organizers provided python code on GitHub <ref type="foot" coords="4,485.37,552.32,3.71,3.62" target="#foot_5">6</ref> for reading and iterating over the XML data, and for generating the HTML question threads. All of the code to generate the corrected ARQMath collection is available from that same GitHub repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Task 1: Answer Retrieval</head><p>The goal of Task 1 is to find and rank relevant answers to math questions. Topics are constructed from questions posted to Math Stack Exchange in 2021, and the collection to search is only the answers to earlier questions (from 2010-2018) in the ARQMath collection. System results ('runs') are evaluated using measures that characterize the extent to which answers judged by relevance assessors as having higher relevance come before answers with lower relevance in the system results (e.g., using nDCG ′ ). In this section, we describe the Task 1 search topics, participant runs, baselines, pooling, relevance assessment, and evaluation measures, and we briefly summarize the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Topics</head><p>ARQMath-3 Task 1 topics were selected from questions posted to Math Stack Exchange in 2021. There were two strict criteria for selecting candidate topics: (1) any candidate must have at least one formula in the title or the body of the question, (2) any candidate must have at least one known duplicate question (from 2010 to 2018) in the ARQMath collection. Duplicates have been annotated by Math Stack Exchange moderators as part of their ongoing work, and we chose to limit our candidates to topics for which a known duplicate question existed. We did this to avoid assessing topics with no relevant answers in the assessment pools or even the collection itself. In ARQMath-2 we had included 11 topics for which there were no known duplicates on an experimental basis. Of those 11, 9 had turned out to have no relevant answers found by any participating system or baseline.</p><p>We selected 139 candidate topics from among the 3313 questions that satisfied both of our strict criteria by applying additional soft criteria based on the number of terms and formulae in the title and body of the question, the question score that Math Stack Exchange users had assigned to the question, and the number of answers, comments, and views for the question. From those 139, we manually selected 100 topics in a way that balanced three desiderata: (1) A similar topic should not already be present in the ARQMath-1 or ARQMath-2 test collections, <ref type="bibr" coords="5,495.30,462.34,11.34,4.95" target="#b1">(2)</ref> we expected that our assessors would have (or be able to easily acquire) the expertise to judge relevance to the topic, and (3) the set of topics maximized diversity across four dimensions (question type, difficulty, dependence, and complexity).</p><p>In prior years, we had manually categorized topic type as computation, concept or proof and we did so again for ARQMath-3. A disproportionately large fraction of Math Stack Exchange questions ask for proofs, so we sought to stratify the ARQMath-3 topics in a way that was somewhat better balanced. Of the 100 ARQMath-3 topics, 49 are categorized as proof, 28 as computation, and 23 as concept. Question difficulty also benefited from restratification. Our insistence that topics have at least one duplicate question in the collection injects a bias in favor of easier questions, and such a bias is indeed evident in the ARQMath-1 and ARQMath-2 test collections. We made an effort to better balance (manually estimated) topic difficulty for the ARQMath-3 test collection, ultimately resulting in 24 topics categorized as hard, 55 as medium, and 21 as easy. We also paid attention to the (manually estimated) dependency of topics on text, formulae, or both, but we did not restratify on that factor. Of the 100 ARQMath-3 topics, 12 are categorized as dependent to text, 28 on formulae, and 60 on both. New this year, we  also paid attention to whether a topic actually asks several questions rather than just one. For these multi-part topics, our relevance criteria require that a highly relevant answer provide relevant information for all parts of the question. Among ARQMath-3 topics, 14 are categorized as multi-part questions.</p><formula xml:id="formula_0" coords="6,119.68,367.64,163.60,33.87">d =``q_898 ' ' &gt; $ $ f _ 3 ( n ) = \ binom n2$$ &lt;/ span &gt; &lt;/ Q u e s t i o n &gt; &lt; Tags &gt; d i s c</formula><p>The topics were published in the XML file format illustrated in Figure <ref type="figure" coords="6,420.78,571.29,3.81,4.95" target="#fig_0">1</ref>. Each topic has a unique Topic ID, a Title, a Question (which is the body of the question post), and Tags provided by the asker of the question on the Math Stack Exchange. Notably, links to duplicate or related questions are not included. To facilitate system development, we provided python code that participants could use to load the topics. As in the collection, the formulae in the topic file are placed in &lt;span&gt; XML elements, with each formula instance represented by a unique identifier and its L A T E X representation. Similar to the collection, there are three Tab Separated Value (TSV) files, for the L A T E X, OPT and SLT representations of the formulae, in the same format as the collection's TSV files. The Topic IDs in ARQMath-3 start from 301 and continue to 400. In ARQMath-1, Topic IDs were numbered from 1 to 200, and in ARQMath-2, from 201 to 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Participant Runs</head><p>ARQMath Participants submitted their runs on Google Drive. As in previous years, we expect all runs to be publicly available. <ref type="foot" coords="7,233.03,550.11,3.71,3.62" target="#foot_6">7</ref> A total of 33 runs were received from 7 teams. Of these, 28 runs were declared to be automatic, with no human intervention at any stage of generating the ranked list for each query. The remaining 5 runs were declared to be manual, meaning that there was some type of human involvement in at least one stage of retrieving answers. Manual runs were invited in ARQMath to increase the quality and diversity of the pool of documents that are judged for relevance, but it is important to note that they might not be fairly compared to automatic runs. The teams and submissions are shown in Table <ref type="table" coords="7,380.03,634.04,3.66,4.95" target="#tab_1">1</ref>. For the details of each run, please see the participant papers in the working notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Baseline Runs</head><p>For Task 1, five baseline systems were provided by the organizers. <ref type="foot" coords="8,387.13,108.16,3.71,3.62" target="#foot_7">8</ref> This year, the organizers included a new baseline system using PyTerrier <ref type="bibr" coords="8,301.00,124.34,17.81,4.95" target="#b9">[10]</ref> for the TF-IDF model. The other baselines were also run for ARQMath 2020 and 2021. Here is a description of our baseline runs.</p><p>1. TF-IDF. We provided two TF-IDF baselines . The first uses Terrier <ref type="bibr" coords="8,428.78,159.40,18.07,4.95" target="#b10">[11]</ref> with default parameters and raw L A T E X strings, as in prior years of the lab. One problem with this baseline is that Terrier removes some L A T E X symbols during tokenization. The second uses PyTerrier <ref type="bibr" coords="8,182.12,200.05,16.13,4.95" target="#b9">[10]</ref>, with symbols in L A T E X strings first mapped to English words to avoid tokenization problems. 2. Tangent-S. This baseline is an isolated formula search engine that uses both SLT and OPT representations <ref type="bibr" coords="8,212.67,241.52,16.40,4.95" target="#b11">[12]</ref>. The target formula was selected from the question title if at least one existed, otherwise from the question body. If there were multiple formulae in the field, a formula with the largest number of symbols (nodes) in its SLT representation was chosen; if more than one had the largest number of symbols, we chose randomly between them. 3. TF-IDF + Tangent-S. Averaging normalized similarity scores from the TF-IDF (only from PyTerrier) and Tangent-S baselines. The relevance scores from both systems were normalized in [0,1] using min-max normalization, and then combined using an unweighted average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Linked Math Stack Exchange Posts. Using duplicate post links from 2021 in Math</head><p>Stack Exchange, this oracle system returns a list of answers from posts in the ARQMath collection that had been given to questions marked in Math Stack Exchange as duplicates to ARQMath-3 topics. These answers are ranked by descending order of their vote scores. Note that the links to duplicate questions were not available to the participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Relevance Assessment</head><p>Relevance judgments for Tasks 1 and 3 were performed together, with the results for the two tasks intermixed in the judgment pools.</p><p>Pooling. For each topic, participants were asked to rank up to 1,000 answer posts. We created pools for relevance judgments by taking the top-𝑘 retrieved answer posts from every participating system or baseline in Tasks 1 or 3. For Task 1 primary runs, the top 45 answer posts were included; for alternate runs the top 20 were included. These pooling depths were chosen based on assessment capacity, with the goal of identifying as many relevant answer posts as possible. Two Task 1 baseline runs, PyTerrier TF-IDF+Tangent-S. and Linked Math Stack Exchange Posts, were pooled as primary runs (i.e, to depth 45); other baselines were pooled as alternate runs (i.e., to depth 20). All Task 3 run results (each of which is a single answer; see section 5.6) were also included in the pools. After merging these top-ranked results, duplicate posts were deleted and the resulting pools were sorted randomly for display to assessors. On average, the judgment pools for Tasks 1 and 3 contain 464 answer posts per topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Relevance Assessment Criteria for Tasks 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Score Rating Definition</head><p>Task 1: Answer Retrieval 3 High Sufficient to answer the complete question on its own 2 Medium Provides some path towards the solution. This path might come from clarifying the question, or identifying steps towards a solution 1 Low Provides information that could be useful for finding or interpreting an answer, or interpreting the question 0 Not Relevant Provides no information pertinent to the question or its answers. A post that restates the question without providing any new information is considered nonrelevant Task 2: Formula Retrieval 3 High Just as good as finding an exact match to the query formula would be 2 Medium Useful but not as good as the original formula would be 1 Low There is some chance of finding something useful 0 Not Relevant Not expected to be useful</p><p>Relevance definition. The relevance definitions were the same those defined for ARQMath-1 and -2. The assessors were asked to consider an expert (modeling a math professor) judging the relevance of each answer to the topics. This was intended to avoid the ambiguity that might result from guessing the level of math knowledge of the actual posters of the original Math Stack Exchange question. The definitions of the four levels of relevance are shown in Table <ref type="table" coords="9,89.29,371.41,3.81,4.95">2</ref>. In judging relevance, ARQMath assessors were asked not to consider any link outside the ARQMath collection. For example, if there is a link to a Wikipedia page, which provides relevant information, the information in the Wikipedia page should not be considered to be a part of the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Assessor Selection</head><p>Paid ARQMath-3 assessors were recruited over email at the Rochester Institute of Technology. 44 students expressed interest, 11 were invited to perform 3 sample assessment tasks, and 9 students specializing in mathematics or computer science were then selected, based on an evaluation of their judgments by an expert mathematician. Of those, 6 were assigned to Tasks 1 and 3; the others performed assessment for Task 2.</p><p>Assessment tool. As with ARQMath-1 and ARQMath-2, we used Turkle, a system similar to Amazon Mechanical Turk. As shown in Figure <ref type="figure" coords="9,325.58,543.08,3.81,4.95">2</ref>, there are two panes, one having the question topic (left pane) and the other having a candidate answer from the judgment pool (right panel). For each topic, the title and question body are provided for the assessors. To familiarize themselves with the topic question, assessors can click on the Thread link for the question, which shows the question and the answers given to it (i.e., answers posted in 2021, which were not available to task participants), along with other information such as tags and comments. Another Thread link is also available for the answer post being assessed. By clicking on that link, the assessor can see a copy of the original question thread on Math Stack Exchange in which the candidate answer was given, as recorded in the March 2020 snapshot used for the ARQMath test collection.</p><p>Note that these Thread links are provided to help the assessors gain just-in-time knowledge that they might need for unfamiliar concepts, but the content of the threads is neither a part of the topic nor of the answer being assessed, and thus it should have no effect on their judgement beyond serving as reference information.</p><p>In the right pane, below the candidate answer, assessors can indicate the relevance degree. In addition to four relevance degrees, there are two additional choices: 'System failure' to indicate system issues such as unintelligible rendering of formulae, and 'Do not know' which can be used if after possibly consulting external sources such as Wikipedia or viewing the Threads the assessor is simply not able to decide the relevance degree. We asked the assessors to leave a comment in the event of a 'System failure' or 'Do not know' selection.</p><p>Assessor Training. All training was done remotely, over Zoom, in four sessions, with some individual assessment practice between each Zoom session. As in ARQMath-1 and -2, in the first session the task and relevance criteria were explained. A few examples were then shown to the assessors and they were asked for their opinions on relevance, which were then discussed with an expert assessor (a math professor). Then, three rounds of training were conducted, with each round consisting of assessment of small judgment pools for four sample topics from ARQMath-2. For each topic, 5-6 answers with different ground truth relevance degrees (from the ARQMath-2 qrels) were chosen. After each round, we held a Zoom session to discuss their relevance judgements, with the specific goal of clarifying their understanding of the relevance criteria. The assessors discussed the reasoning for their choices, with organizers (always including the math professor) sharing their own judgments and their supporting reasoning. The primary Project: ARQMath3_Gregory / Batch: B.301</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accept Task Skip Task Stop Preview</head><p>Instructions: Select the Relevance of the highlighted formula within each post to the query formula (shown at bottom-left).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Go Back</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inequality between norm 1,norm 2 and norm of Matrices</head><p>Thread Suppose is a matrix.</p><p>Then Prove that, I have proved the following relations: Also I feel that somehow Holder's inequality for the special case when and might be useful.But I couldn't prove that.</p><p>Edit: I would like to have a prove that do not use the information that Usage of inequalities like Cauchy Schwartz or Holder is fine.</p><formula xml:id="formula_1" coords="10,109.68,391.73,154.12,86.07">∞ A m × n ∥A ≤ ∥2 ∥A ∥A ∥1 ∥∞ - - ------- √ ∥A ≤ ∥A ≤ ∥A 1 n -- √ ∥∞ ∥2 m -- √ ∥∞ ∥A ≤ ∥A ≤ ∥A 1 m -- √ ∥1 ∥2 n -- √ ∥1 p = 1 q = ∞ ∥A = ∥2 ρ( A) A T - - ---- √</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Post</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thread</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer:</head><p>As the answer you looked at indicates, the key here is that a unitary transition of basis preserves the matrix norm. In particular, if has SVD , then we'll have . So indeed, .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thread</head><p>Answer: Write as the matrix with the columns and as the matrix with the columns , then notice that . Notice we are searching for and trying to show that it is less than . Now and since , we have and finally yield .</p><formula xml:id="formula_2" coords="10,283.52,421.26,204.93,177.49">A A = UDV T ∥A∥ = ∥D∥ ∥A∥ = ( A) λmax A T - - ------- √ High Medium Low Not Relevant System failure Do not know Annotator comment ME ej MF fj A = M T E MF ∥A∥ = ( A) λmax A T - - ------- √ 1 ∥A∥ ≤ ∥ ∥ ∥ ∥ ME MF = M T E ME Ik = M T F MF Il ∥ ∥ = 1, ∥ ∥ = 1 ME MF ∥A∥ ≤ 1 High Medium Low Not Relevant System failure</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Do not know Annotator comment</head><p>You must ACCEPT the Task before you can submit the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Turkle Assessment Interface. Shown are hits for Formula Retrieval (Task 2). In the left pane, the formula query is highlighted. In the right pane, two answer posts containing the same retrieved formula are shown. For Task 1, the same interface was used, but without formula highlighting, and presenting only one answer post at a time.</p><p>-  goal of training was to help assessors make self-consistent annotations, as topic interpretations will vary across individuals. Some of the topics involve issues that are not typically covered in regular undergraduate courses, and some such cases required the assessors to get a basic understanding of those issue before they could do the assessment. The assessors found the question Threads made available in the Turkle interface helpful in this regard (see Figure <ref type="figure" coords="11,487.60,302.55,3.57,4.95">2</ref>). Figure <ref type="figure" coords="11,130.72,316.10,4.98,4.95" target="#fig_1">3</ref> shows average Cohen's kappa coefficients for agreement between each assessor and all others during training. Collapsing relevance to binary by considering only high and medium as relevant (henceforth "H+M binarization") yielded better agreement among the assessors. <ref type="foot" coords="11,501.78,340.56,3.71,3.62" target="#foot_8">9</ref>The agreement values in the second round are unusually low, but the third round agreement is in line with what we had seen at the end of training in prior years.</p><p>Assessment Results. Among 80 topics assessed, two (A.335 and A.367) had only one answer assessed as high or medium; these two topics were removed from the collection as score quantization for MAP ′ can be quite substantial when only a single relevant document contributes to the computation. For the remaining 78 topics, an average of 446.8 answers were assessed, with an average assessment time of 44.1 seconds per answer post. The average number of answers labeled with any degree of relevance (high, medium, or low; henceforth "H+M+L binarization") over those 78 topics was 100.8 per question (twice as high as that seen in ARQMath-2), with the highest number being 295 (for topic A.317) and the lowest being 11 (for topic A.385).</p><p>Post Assessment. After assessments of 80 topics for Task 1 were done, each of the assessors for this task, assessed one topic assessed by another assessor. <ref type="foot" coords="11,356.19,516.70,7.41,3.62" target="#foot_9">10</ref> With Cohen's kappa coefficient, a kappa of 0.24 was achieved on the four-way assessment task, and with H+M binarization, the average kappa value was 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Evaluation Measures</head><p>While this is the third year of the ARQMath lab, with several relatively mature systems participating, it is still possible that many relevant answers may remain undiscovered. To support fair comparisons with future systems that may find different documents, we have adopted evaluation measures that ignore unjudged answers, rather than adopting the more traditional convention of treating unjudged answers as not relevant. Specifically, the primary evaluation measure for Task 1 is the nDCG ′ (read as "nDCG-prime") introduced by Sakai and Kando <ref type="bibr" coords="12,409.04,117.32,16.27,4.95" target="#b12">[13]</ref>. nDCG ′ is simply the nDCG@1000 that would be computed after removing unjudged documents from the ranked list. This measure has shown better discriminative power and somewhat better system ranking stability (with judgement ablation) compared to the bpref <ref type="bibr" coords="12,347.65,157.96,17.82,4.95" target="#b13">[14]</ref> measure that had been adopted for experiments using the NTCIR Math IR collections for similar reasons <ref type="bibr" coords="12,421.86,171.51,16.56,4.95" target="#b11">[12,</ref><ref type="bibr" coords="12,441.24,171.51,12.42,4.95" target="#b14">15]</ref>. Moreover, nDCG ′ yields a single-valued measure with graded relevance, whereas bpref, Precision@k, and Mean Average Precision (MAP) all require binarized relevance judgments. As secondary measures, we compute Mean Average Precision (MAP@1000) with unjudged posts removed (MAP ′ ) and Precision at 10 with unjudged posts removed (P ′ @10). For MAP ′ and P ′ @10 we used H+M binarization. Note that the answers assessed as "System failure" or "Do not know" were not considered for evaluation, thus can be viewed as answers that are not assessed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Results</head><p>Progress Testing. In addition to their submissions on the ARQMath-3 topics, we asked each participating team to also submit results from exactly the same systems on ARQMath-1 and ARQMath-2 topics for progress testing. Note, however, that ARQMath-3 systems could be trained on topics from ARQMath-1 and -2; Together, there were 158 topics (77 from ARQMath-1, 81 from ARQMath-2) that could be used for training. The progress test results thus need to be interpreted with this train-on-test potential in mind. Progress test results are provided in Table <ref type="table" coords="12,115.79,383.83,3.74,4.95">3</ref>.</p><p>ARQMath-3 Results. Table <ref type="table" coords="12,232.25,397.38,5.11,4.95">3</ref> also shows results for ARQMath-3 Task 1. This table shows baselines first, followed by teams, and within teams their systems, ranked by nDCG ′ . As seen in the table, the manual primary run of the approach0 team achieved the best results, with 0.508 nDCG ′ . Among automatic runs, nDCG ′ , 0.504, was achieved by the MSM team. Note that the highest possible nDCG ′ and MAP ′ values are 1.0, but because fewer than 10 assessed relevant answers (with H+M binarization) were found in the pools for some topics, the highest possible P ′ @10 value in ARQMath-3 Task 1 is 0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Task 2: Formula Search</head><p>The goal of the formula search task is to find a ranked list of formula instances from both questions and answers in the collection that are relevant to a formula query. The formula queries are selected from the questions in Task 1. One formula was selected from each Task 1 question topic to produce Task 2 topics. For cases in which suitable formulae were present in both the title and the body of the Task 1 question, we selected the Task 2 formula query from the title. For each query, a ranked list of 1,000 formulae instances were returned by their identifiers in the &lt;span&gt; XML elements and the accompanying TSV L A T E X formula index file, along with their associated post identifiers.</p><p>While in Task 1, the goal was to find relevant answers for the questions, in Task 2, the goal is to find relevant formulae that are associated with information that can help to satisfy an information need. The post in which a formula is found need not be relevant to the question post in which the formula query originally appeared for a formula to be relevant to a formula query, but those post contexts inform the interpretation of each formula (e.g., by defining operations and identifying variable types). A second difference is that the retrieved formulae instances in Task 2 can be found in either question posts or answer posts, whereas in Task 1, only answer posts were retrieved.</p><p>Finally, in Task 2, we distinguish visually distinct formulae from instances of those formulae, and systems are evaluated by the ranking of the visually distinct formulae they return. The same formula can appear in different posts, and we call these individual occurrences formula instances. A visually distinct formula is a formula associated with a set of instances that are visually identical when viewed in isolation. For example, 𝑥 2 is a formula, 𝑥 • 𝑥 is a different (i.e., visually distinct) formula, and each time 𝑥 2 appears, it is an instance of the visually distinct formula 𝑥 2 . Although systems in Task 2 rank formula instances in order to support the relevance judgment process, the evaluation measure for Task 2 is based on the ranking of visually distinct formulae. As shown by <ref type="bibr" coords="13,191.84,266.36,93.74,4.95">Mansouri et al. (2021)</ref>  <ref type="bibr" coords="13,287.82,266.36,11.28,4.95" target="#b6">[7]</ref>, using visually-distinct formulae for evaluation can result in a different preference order between systems than would evaluation on formula instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Topics</head><p>Each formula query was selected from a Task 1 topic. Similarly to Task 1, Task 2 topics were provided in XML in the format shown in Figure <ref type="figure" coords="13,304.11,356.73,3.74,4.95" target="#fig_0">1</ref>. Differences are:</p><p>1. Topic Id. Task 2 topic ids are in the form "B.x" where x is the topic number. There is a correspondence between topic id in tasks 1 and 2. For instance, topic id "B.384" indicates the formula is selected from topic "A.384" in Task 1, and both topics include the same question post (see Figure <ref type="figure" coords="13,230.18,417.70,3.57,4.95" target="#fig_0">1</ref>). 2. Formula Id. This added field specifies the unique identifier for the query formula instance.</p><p>There may be other formulae in the Title or Body of the same question post, but the formula query is only the formula instance specified by this Formula_Id. 3. L A T E X. This added field is the L A T E X representation of the query formula instance, as found in the question post.</p><p>As the query formulae are selected from Task 1 questions, the same L A T E X, SLT and OPT TSV files that were provided for the Task 1 topics can be used when SLT or OPT representations for a query formula are needed. Formulae for Task 2 were manually selected using a heuristic approach to stratified sampling over two criteria: complexity and elements. Formula complexity was labeled low, medium or high by the third author. For example, [𝑥, 𝑦] = 𝑥 is low complexity, ∫︀ 1 (𝑥 2 +1) 𝑛 𝑑𝑥 is medium complexity, and</p><formula xml:id="formula_3" coords="13,194.60,581.73,27.83,16.74">√ 1-𝑝 2</formula><p>2𝜋(1-2𝑝 sin(𝜙) cos(𝜙)) is high complexity. These annotations, available in an auxiliary file, can be useful as a basis for fine-grained result analysis, since formula queries of differing complexity may result in different preference orders between systems <ref type="bibr" coords="13,466.18,626.20,16.41,4.95" target="#b15">[16]</ref>. For elements, our intuition was to make sure that we have formula queries that contain different elements and math phenomena such as integral, limit, and matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Participant Runs</head><p>A total of 19 runs were received for Task 2 from a total of five teams, as shown in Table <ref type="table" coords="14,500.04,110.79,3.81,4.95" target="#tab_1">1</ref>. Among the participating runs, 5 were annotated as manual and the others were automatic. Each run retrieved up to 1,000 formula instances for each formula query, ranked by relevance to that query. For each retrieved formula instance, participating teams provided the formula_id and the associated post_id for that formula. Please see the participant papers in the working notes for descriptions of the systems that generated these runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Baseline Run: Tangent-S</head><p>Tangent-S <ref type="bibr" coords="14,138.07,228.26,18.05,4.95" target="#b11">[12]</ref> is the baseline system for ARQMath-3 Task 2. That system accepts a formula query without using any associated text from its associated question post. Since a single formula is specified for each Task 2 query, the formula selection step in the Task 1 Tangent-S baseline is not needed for Task 2. Timing was similar to that of Tangent-S in ARQMath-1 and -2 (i.e., with an average retrieval time of around six seconds per query).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Assessment</head><p>Pooling. For each topic, participants were asked to rank up to 1,000 formula instances. However, the pooling was done using visually distinct formulae. The visual ids, which were provided beforehand for the participants, were used for clustering formula instances. Pooling was done by going down each ranked list until 𝑘 visually distinct formulae were found. For primary runs (and the baseline system), the first 25 visually distinct formulae were pooled; for alternate runs, the first 15 visually distinct formulae were pooled.</p><p>The visual Ids used for clustering retrieval results were determined by the SLT representation when possible, and the L A T E X representation otherwise. When SLT was available, we used Tangent-S <ref type="bibr" coords="14,137.99,440.58,18.03,4.95" target="#b11">[12]</ref> to create a string representation using a depth-first traversal of the SLT, with each SLT node and edge generating a single item in the SLT string. Formula instances with identical SLT strings were then considered to be the same formula. For formula instances with no Tangent-S SLT string available, we removed the white space from their L A T E X strings and grouped formula instances with identical L A T E X strings. This process is simple and appears to be reasonably robust, but it is possible that some visually identical formula instances were not captured due to L A T E XML conversion failures, or where different L A T E X strings produce visually identical formulae (e.g., if subscripts and superscripts appear in a different order in L A T E X).</p><p>Task 2 assessment was done on formula instances. For each visually distinct formula at most five instances were selected for assessment. As in ARQMath-2 Task 2, formula instances to be assessed were chosen in a way that prefers highly-ranked instances and that prefers instances returned in multiple runs. This was done using a simple voting protocol, where each instance votes by the sum of its reciprocal ranks within each run, breaking ties randomly. For each query, on average there were 154.35 visually distinct formulae to be assessed, and only 6% of visually distinct formulae had more than 5 instances.</p><p>Relevance definition. To distinguish between different relevance degrees, we relied on the definitions in Table <ref type="table" coords="14,175.76,657.36,3.66,4.95">2</ref>. The usefulness is defined as the likelihood of the candidate formula being  associated with information (text) that can help a to accomplish their task. In our case, the task is answering the question from which a query formula is taken.</p><p>To judge the relevance of a candidate formula instance, the assessor was given the candidate formula (highlighted) along with the (question or answer) post in which it had appeared. They were then asked to decide on relevance by considering the definitions provided. For each visually distinct formula, up to 5 instances were shown to assessors and they would assess the instances individually. For assessment, they could look at the formula's associated post in an effort to understand factors such as variable types, the interpretation of specific operators, and the area of mathematics it concerns. As in Task 1, assessors could also follow Thread links to increase their knowledge by examining the thread in which the query formula had appeared, or in which a candidate formula had appeared.</p><p>Assessment tool. As in Task 1, we used Turkle for the Task 2 assessment process, as illustrated in Figure <ref type="figure" coords="15,179.43,403.16,3.72,4.95">2</ref>. There are two panes, the left pane showing the formula query (‖𝐴‖ 2 = √︀ 𝜌(𝐴 𝑇 𝐴) in this case) highlighted in yellow inside its question post, and the right pane showing the (in this case, two) candidate formula instances of a single visually distinct formula. For each topic, the title and question body are provided for the assessors. Thread links can be used by the assessors just for learning more about mathematical concepts in the posts. For each formula instance, the assessment is done separately. As in Task 1, the assessors can choose between different relevance degrees, they can choose 'System failure' for issues with Turkle, or they can choose 'Do not know' if they are not able to decide on a relevance degree.</p><p>Assessor Training. Three paid undergraduate and graduate mathematics and computer science students from RIT were selected to perform relevance judgments. As in Task 1, all training sessions were done remotely, over Zoom.</p><p>There were four Task 2 training sessions. In the first meeting, the task and relevance criteria were explained to assessors and then a few examples were shown, followed by discussion about relevance level choices. In each subsequent training round, assessors were asked to first assess four ARQMath-2 Task 2 topics, each with 5-6 visually distinct formula candidates with a variety of relevance degrees. Organizers then met with the assessors to discuss their choices and clarify relevance criteria. Figure <ref type="figure" coords="15,207.03,619.94,5.17,4.95" target="#fig_2">4</ref> shows the average agreement (kappa) of each assessor with the others during training. As can be seen, agreement had improved considerably by round three, reaching levels comparable to that seen in prior years of ARQMath.</p><p>Assessment Results. Among 76 assessed topics, all have at least two relevant visually distinct formulae with H+M binarization, so all 76 topics were retained in the ARQMath- Post Assessment. After Task 2 assessments were done, each of the three assessors, assessed two topics, each assessed by the other two assessors. Using Cohen's kappa coefficient, a kappa of 0.44 was achieved on the four-way assessment task (higher than ARQMath-1 and -2), and with H+M binarization the average kappa value was 0.51.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Evaluation Measures</head><p>As in Task 1, the primary evaluation measure for Task 2 is nDCG ′ , with MAP ′ and P ′ @10 also reported. Participants submitted ranked lists of formula instances used for pooling, but with evaluation measures computed over visually distinct formulae. The ARQMath-2 Task 2 evaluation script replaces each formula instance with its associated visually distinct formula, and then deduplicates from the top of the list downward, producing a ranked list of visually distinct formulae, from which our "prime" evaluation measures are then computed using trec_eval, after removing unjudged visually distinct formulae. For the visually distinct formulae with multiple instances, the maximum relevance score of any judged instance was used as the relevance visually distinct formula's relevance score. This reflects a goal of having at least one instance that provides useful information. Similar to Task 1, formulas assessed as "System failure" or "Do not know" were treated as not being assessed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Results</head><p>Progress Testing. As with Task 1, we asked Task 2 teams to run their ARQMath-3 systems on ARQMath-1 and -2 Topics for progress testing (see Table <ref type="table" coords="16,350.33,447.11,3.65,4.95">4</ref>). Some progress test results may represent a train-on-test condition: there were 70 topics from ARQMath-2 and 74 topics from ARQMath-1 available for training. Note also that while the relevance definition stayed the same for ARQMath-1, -2, and -3, the assessors were instructed differently in ARQMath-1 on how to handle the specific case in which two formulae were visually identical. In ARQMath-1 assessors were told such cases are always highly relevant, whereas ARQMath-2 and ARQMath-3 assessors were told that from context they might recognize cases in which a visually identical formula would be less relevant, or not relevant at all (e.g., where identical notation is used with very different meaning). Assessor instruction did not change between ARQMath-2 and -3.</p><p>ARQMath-3 Results. Table <ref type="table" coords="16,238.11,569.05,5.17,4.95">4</ref> also shows results for ARQMath-3 Task 2. In that table, the baseline is shown first, followed by teams and then their systems ranked by nDCG ′ on ARQMath-3 Task 2 topics. As shown, the highest nDCG ′ was achieved by the manual primary run from the approach0 team, with an nDCG ′ value of 0.720. Among automatic runs, the highest nDCG ′ value was the DPRL primary run, with an NDCG ′ of 0.694. Note that 1.0 is a possible score for nDCG ′ and MAP ′ , but that the highest possible P ′ @10 value is 0.93 because (with H+M binarization) 10 visually distinct formulae were not found in the pools for some topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Task 3: Open Domain Question Answering</head><p>The new pilot task developed for ARQMath-3 (Task 3) is Open Domain Question Answering. Unlike Task 1, system answers are not limited to content from any specific source. Rather, answers can be extracted from anywhere, automatically generated, or even written by a person. For example, suppose that we ask a Task 3 system the question "What does it mean for a matrix to be Hermitian?" An extractive system might first retrieve an article about Hermitian matrices from Wikipedia and then extract the following excerpt as the answer: "In mathematics, a Hermitian matrix (or self-adjoint matrix) is a complex square matrix that is equal to its own conjugate transpose. " By contrast, a generative system such as GPT-3 can directly construct an answer such as: "A matrix is Hermitian if it is equal to its transpose conjugate. " For a survey of open-domain question answering, see Zhu et al. <ref type="bibr" coords="17,310.03,236.47,16.41,4.95" target="#b16">[17]</ref>. In this section, we describe the Task 3 search topics, runs from participant and baseline systems, assessment and evaluation procedures, and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Topics and Participant Runs</head><p>The topics for Task 3 are the Task 1 topics, with the same content provided (title, question body, and tags). A total of 13 runs were received from 3 teams. Each run consists of a single result for each topic. 9 runs from the TU_DBS and DPRL teams were declared to be automatic and 5 runs from the approach0 team were declared as manual. The 4 automatic runs from the TU_DBS team used generative systems, whereas the remaining 9 runs from the DPRL and approach0 teams used extractive systems. The teams and their submissions are listed in Table <ref type="table" coords="17,459.56,380.59,3.74,4.95" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Baseline Run: GPT-3</head><p>The ARQMath organizers provided one baseline run for this task using GPT-3. This baseline system uses the text-davinci-002 model of GPT-3 <ref type="bibr" coords="17,307.71,443.41,17.92,4.95" target="#b17">[18]</ref> from OpenAI. First, the system prompts the model with the text Q: followed by the text and the L A T E X formulae of the question, two newline characters, and the text A: as follows:</p><p>Q: What does it mean for a matrix to be Hermitian? A:</p><p>Then, GPT-3 completes the text and produces an answer of up to 570 tokens:</p><p>Q: What does it mean for a matrix to be Hermitian? A: A matrix is Hermitian if it is equal to its transpose conjugate.</p><p>If the answer is longer than the maximum of 1,200 Unicode characters, the system retries until the model has produced a sufficiently short answer.</p><p>To provide control over how creative an answer is, GPT-3 resmooths the output layer 𝐿 using the temperature 𝜏 as follows: softmax(𝐿/𝜏 ) <ref type="bibr" coords="17,320.11,645.73,16.38,4.95" target="#b18">[19]</ref>. A temperature close to zero ensures deterministic outputs on repeated prompts, whereas higher temperatures allow the model's decoder to consider many different answers. Our system uses the default temperature 𝜏 = 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Evaluation Measures</head><p>In this section, we first describe the measures we used to evaluate participating systems. Then, we describe additional evaluation measures that we have developed with the goal of providing a fair comparison between participating systems and future systems that return answers from outside Math Stack Exchange, or that are generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1.">Manual Evaluation Measures</head><p>As described in Section 4.4, the assessors produced a relevance score between 0 and 3 for most answers from each participating system. The exceptions were 'System failure' and 'Do not know' assessments, which we interpreted as relevance score 0 ('Not relevant') in our evaluation of Task 3. To evaluate participating systems, we report the Average Relevance (AR) score and Precision@1 (P@1). AR is equivalent to the unnormalized Discounted Cumulative Gain at position 1 (DCG@1).<ref type="foot" coords="18,181.99,265.88,7.41,3.62" target="#foot_10">11</ref> P@1 is computed using H+M binarization.</p><p>Task 1 systems approximate a restricted class of Task 3 systems. For this reason, we also report AR and P@1 for ARQMath-3 Task 1 systems in order to extend the range of system comparisons that can be made. To do this, we truncate the Task 1 result lists after the first result. Note, however, that Task 3 answers were limited to a maximum of 1,200 Unicode characters, whereas Task 1 systems had no such limitation. Approximately 15% of all answer posts in the collection are longer than 1,200 Unicode characters when represented as text and L A T E X. Therefore, the Task 3 measures that we report for Task 1 systems should be treated as somewhat optimistic estimates of what might have been achieved by an extractive system that was limited to the ARQMath collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2.">Automatic Evaluation Measures</head><p>In Task 1, systems pick answers from a fixed collection of potential answers. When evaluated with measures that differentiate between relevant, non-relevant, and unjudged answers, reasonable comparisons can be made between participating systems that contributed to the judgement pools and future systems that did not. By contrast, the open-ended nature of Task 3 means that relevance judgements on results from participating systems can not be used in the same way to evaluate future systems that might (and hopefully will!) generate different answers.</p><p>The problem lies in the way AR and P@1 are defined; they rely on our ability to match new answers with judged answers. For future systems, however, the best we might reasonably hope for is similarity between the new answers and the judged answers. If we are to avoid the need to keep assessors around forever, we need automatic evaluation measures that can be used to compare participating Task 3 systems with future Task 3 systems. With that goal in mind, we also report Task 3 results using the following evaluation measures:</p><p>1. Lexical Overlap (LO) Following SQuAD and CoQA [20, Section 6.1], we represent answers as a bag of tokens, where tokens are produced by the MathBERTa<ref type="foot" coords="18,452.72,623.05,7.41,3.62" target="#foot_11">12</ref> tokenizer.</p><p>For every topic, we compute the token 𝐹 1 score between the system's answer and each known relevant Task 3 answer (using H+M binarization). The score for a topic is the maximum across these 𝐹 1 scores. The final score is the average across all topics of those per-topic maximum 𝐹 1 scores. 2. Contextual Similarity (CS) Although lexical overlap can account for answers with high surface similarity, it cannot recognize answers that use different tokens with similar meaning. For context similarity, we use BERTScore <ref type="bibr" coords="19,348.25,172.87,17.96,4.95" target="#b20">[21]</ref> with the MathBERTa language model. As with our computation of lexical overlap, for BERTScore we also compute a token 𝐹 1 score, but instead of exact matches, we match tokens with the most similar contextual embeddings and interpret their similarity as fractional membership. For every topic, we compute 𝐹 1 score between the system's answer and each known relevant answer (with H+M binarization). The score for a topic is the maximum across these 𝐹 1 scores. The final score is the average across all topics of those per-topic maximum 𝐹 1 scores.</p><p>When computing the automatic measures for a participating system, we exclude relevant answers uniquely contributed to the pools by systems from the same team. This ablation avoids the perfect overlap scores that systems contributing to the pools would otherwise get from matching their own results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Results</head><p>Task 3 runs were assessed together with Task 1 runs, using the same relevance definitions, although after that assessment was complete, we also did some additional annotation that was specific to Task 3. Here we present results for the baseline and submitted runs using manual and automatic measures, along with additional analysis that we performed using the additional annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1.">Manual Evaluation Measures</head><p>Table <ref type="table" coords="19,115.35,471.38,4.98,4.95">5</ref> shows ARQMath-3 results for Task 3 systems. This table shows baselines first, followed by teams ordered by their best Average Recall (AR), and within teams their runs are ordered by AR. As seen in the table, the automatic generative baseline run using GPT-3 achieved the best results, with 1.346 AR. Note that uniquely among ARQMath evaluation measures, AR is not bounded between 0 and 1; rather, it is bounded between 0 and 3. <ref type="foot" coords="19,378.08,522.94,7.41,3.62" target="#foot_12">13</ref> Among manual extractive non-baseline runs, the highest AR was achieved by a run from the approach0 team, with 1.282 AR. Among automatic extractive non-baseline runs, the highest AR was achieved by a run from the DPRL team, with 0.462 AR. Among automatic generative non-baseline runs, the highest AR was achieved by the TU_DBS team, with 0.325 AR. No manual generative non-baseline runs were submitted to ARQMath-3 Task 3.</p><p>Table <ref type="table" coords="19,128.09,606.87,5.17,4.95">7</ref> shows ARQMath-3 Task 3 results for Task 1 systems. Similarly to Table <ref type="table" coords="19,470.34,606.87,3.81,4.95">5</ref>, Table <ref type="table" coords="19,89.04,620.42,5.17,4.95">7</ref> shows baselines first, followed by teams ordered by their best AR, and within teams their runs are ordered by AR. As seen in the table, the Linked MSE posts baseline achieved the best result, with 1.608 AR. Among non-baseline runs, the highest AR was achieved by a run from the approach0 team, with 1.377 AR. Among automatic runs, the highest AR was achieved by a run from the TU_DBS team, with 1.192 AR. Compared to ARQMath-3 Task 1 results in Table <ref type="table" coords="20,499.80,117.32,3.69,4.95">3</ref>, the TU_DBS team's best run did relatively better, swapping order with the best runs from the MSM and MIRMU. Within teams, the fusion_alpha05 run from approach0, which achieved the highest nDCG ′ on Task 1, did not do as well as that team's rerank_nostemmer system when both were scored using Task 3 measures. The RRF-AMR-SVM run from DPRL, which achieved the second highest nDCG ′ score among DPRL runs on Task 1, received the lowest AR and P@1 among Task 1 systems. These differences result from the exclusive focus of Task 3 measures on the single highest-ranked result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2.">Automatic Evaluation Measures</head><p>At least one participating system produced a relevant answer (with H+M binarization) for 66 of the 78 Task 3 topics. However, automated evaluation can only be computed with ablation of each team's contributions if two or more of the three teams produced a relevant answer; there were only 35 such topics. We therefore expanded the set of references for automatic Task 3 measures to also include relevant answers (with H+M binarization) that were produced for ARQMath-3 topics by Task 1 systems, but only for relevant answers that were no longer than 1,200 Unicode characters.</p><p>As one measure of the suitability of our automatic evaluation measures for the evaluation of future systems, we report paired pointwise correlation measures between our automatic measures and manual measures, using Pearson's 𝑟 to characterize the linear relationship between the measures, and Kendall's 𝜏 to characterize differences in how the evaluation measures rank systems.</p><p>Table <ref type="table" coords="20,128.09,424.08,5.17,4.95">5</ref> also shows results for automatic evaluation measures. The automatic generative baseline run using GPT-3, which achieved the best result using manual measures, scored below extractive runs from the approach0 and DPRL teams on both automatic measures. We theorize that this is because we used relevant answers produced by Task 1 systems in our automatic measures, which favors extractive systems over generative systems, because identical hits may be retrieved by extractive systems. Both automatic measures maintained the ordering of teams given by the manual measures.</p><p>Table <ref type="table" coords="20,127.75,518.92,5.17,4.95">6</ref> shows pointwise correlations between the manual and automatic measures. Both automatic measures show a strong linear relationship to the manual measures, with lexcial overlap (LO) and average relevance (AR) having Pearson's 𝑟 of 0.837, and contextual similarity (CS) and AR having Pearson's 𝑟 of 0.839. LO is better able to maintain the ordering of results given by the manual measures, having Kendall's 𝜏 with AR of 0.736, compared to CS, which has Kendall's 𝜏 with AR of 0.670. Furthermore, LO is also more easily interpretable than CS, because it only considers exact matches between tokens, and is independent of a specific BERT language model, which may have to be replaced in the future. This suggests that LO may be preferable as an automatic measure to evaluate future Math OpenQA systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3.">Characterizing Answers</head><p>The answers for Task 3 were assessed together with the Task 1 results, using the same relevance definitions. We also provided a sample of Task 1 and Task 3 answers to assessors, and asked them to annotate:</p><p>1. Whether answers were machine-generated 2. Whether answers contained information unrelated to the topic question In Tasks 1 and 3, answers are considered relevant if any part of the answer is relevant to the question. Annotating unrelated information allows us to determine whether extractive systems stuff answers with unrelated information, perhaps in the hope that some of it will be relevant, and whether generative systems generate off-topic content together with on-topic content. To support that analysis, assessors were asked to differentiate between undesirable answer stuffing and the possibly desirable inclusion of background information that is related to the question or relevant part(s) of the answer.</p><p>We report the answers to these questions using two measures:</p><p>1. Machine-Generated (MG). The fraction of answers assessed as machine-generated. Ideally this would always be zero, but in practice we are interested in whether it is larger for generative systems than for extractive systems. 2. Unrelated Information (UI). The fraction of answers assessed as containing information unrelated to the question. Again, ideally this would be zero.</p><p>We report these measures as averages over 73 of the 78 Task 3 topics because one assessor was unable to complete this post-evaluation assessment process. <ref type="foot" coords="21,356.18,395.40,7.41,3.62" target="#foot_13">14</ref>Table <ref type="table" coords="21,126.10,411.58,4.97,4.95">5</ref> includes results for these measures. The manual extractive run of approach0 produced the smallest fraction of answers annotated as machine-generated (11%). Among generative runs, the automatic baseline run using GPT-3 produced the fewest answers annotated as machine-generated (28.8%). With the exception of the automatic extractive SBERT-QQ-AMR run from DPRL, which had 34.2% of answers annotated as machine-generated, the generative runs are linearly separable from the extractive runs (by MG &gt; 0.26). This suggests that even though people would perform worse than chance at identifying answers as machine generated for systems such as GPT-3, they would often be able to differentiate between extractive and generative systems after seeing many answers from a system.</p><p>We also see that UI has a strong inverse correlation with AR, with Pearson's 𝑟 of -0.97 and Kendell's 𝜏 of -0.88. Moreover, we also see that 90.43% of answers that were annotated as containing information unrelated to the question had been assessed as not relevant (with H+M binarization), whereas only 79.03% of all answers were annotated as not relevant (with H+M binarization). That suggests that answer stuffing does not seem to have been a serious problem in our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Over the course of three years, ARQMath has created test collections for three tasks that together include relevance judgments for hundreds of topics for two of those tasks, and 78 topics for the third. Coming as it did at the dawn of the neural age in information retrieval, considerable innovation in methods has been evident throughout the three years of the lab. ARQMath has included substantial innovation in evaluation design as well, including better contextualized definitions for graded relevance, and piloting a new task on open domain question answering. Having achieved our twin goals of building a new test collection from Math Stack Exchange posts and bringing together a research community around that test collection, the time as now come to end this lab at CLEF. We expect, however, that both that collection and that community will continue to contribute to advancing the state of the art in Math IR for years to come.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>ARQMath 2022 Task 1 (CQA) results. P: primary run, M: manual run, (✓): baseline pooled as a primary run. For MAP ′ and P ′ @10, H+M binarization was used. (D)ata indicates use of (T)ext, (M)ath, (B)oth text and math, or link structure (*L).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARQMath-1</head><p>ARQMath- </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,89.29,451.45,417.79,9.96;6,88.98,463.40,417.00,9.96;6,89.29,475.36,195.48,12.11"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example XML Topic Files. Formula queries in Task 2 are taken from questions for Task 1. Here, ARQMath-3 formula topic B.384 is a copy of ARQMath-3 question topic A.384 with two additional fields for the query formula (1) identifier and (2) L A T E X.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="11,89.29,182.70,416.69,9.96;11,89.29,194.66,418.35,9.96;11,89.29,206.61,175.20,9.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Inter-annotator agreement for 6 assessors during training sessions for Task 1 (mean Cohen's kappa), with four-way classification in gray, and two-way classification (H+M binarized) in black. Leftto-right: agreements for rounds 1, 2, and 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="15,89.29,174.92,418.36,9.96;15,88.93,186.87,418.15,9.96;15,89.29,198.83,138.46,9.96"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Annotator agreement for 3 assessors during training for Task 2 (mean Cohen's kappa). Fourway classification is shown in gray, and two-way (H+M binarized) classification in black. Left-to-right: agreements for rounds 1, 2, and 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.98,90.49,417.00,362.71"><head>Table 1</head><label>1</label><figDesc>ARQMath-3: Submitted Runs. Baselines for Task 1 (5), Task 2 (1) and Task 3 (1) were generated by the organizers. Primary and alternate runs were pooled to different depths, as described in Section 4.4.</figDesc><table coords="7,128.45,133.98,335.88,319.22"><row><cell></cell><cell cols="2">Automatic</cell><cell cols="2">Manual</cell></row><row><cell></cell><cell>Primary</cell><cell>Alternate</cell><cell>Primary</cell><cell>Alternate</cell></row><row><cell>Task 1: Answer Retrieval</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baselines</cell><cell>2</cell><cell>3</cell><cell></cell><cell></cell></row><row><cell>Approach0</cell><cell></cell><cell></cell><cell>1</cell><cell>4</cell></row><row><cell>DPRL</cell><cell>1</cell><cell>4</cell><cell></cell><cell></cell></row><row><cell>MathDowsers</cell><cell>1</cell><cell>2</cell><cell></cell><cell></cell></row><row><cell>MIRMU</cell><cell>1</cell><cell>4</cell><cell></cell><cell></cell></row><row><cell>MSM</cell><cell>1</cell><cell>4</cell><cell></cell><cell></cell></row><row><cell>SCM</cell><cell>1</cell><cell>4</cell><cell></cell><cell></cell></row><row><cell>TU_DBS</cell><cell>1</cell><cell>4</cell><cell></cell><cell></cell></row><row><cell>Totals (38 runs)</cell><cell>8</cell><cell>25</cell><cell>1</cell><cell>4</cell></row><row><cell>Task 2: Formula Retrieval</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Approach0</cell><cell></cell><cell></cell><cell>1</cell><cell>4</cell></row><row><cell>DPRL</cell><cell>1</cell><cell>4</cell><cell></cell><cell></cell></row><row><cell>MathDowsers</cell><cell>1</cell><cell>2</cell><cell></cell><cell></cell></row><row><cell>JU_NITS</cell><cell>1</cell><cell>2</cell><cell></cell><cell></cell></row><row><cell>XY_PHOC_DPRL</cell><cell>1</cell><cell>2</cell><cell></cell><cell></cell></row><row><cell>Totals (20 runs)</cell><cell>5</cell><cell>10</cell><cell>1</cell><cell>4</cell></row><row><cell>Task 3: Open Domain QA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Approach0</cell><cell></cell><cell></cell><cell>1</cell><cell>4</cell></row><row><cell>DPRL</cell><cell>1</cell><cell>3</cell><cell></cell><cell></cell></row><row><cell>TU_DBS</cell><cell>1</cell><cell>3</cell><cell></cell><cell></cell></row><row><cell>Totals (14 runs)</cell><cell>3</cell><cell>6</cell><cell>1</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="16,89.04,90.22,417.23,59.15"><head></head><label></label><figDesc>3 Task 2 test collection. An average of 152.3 visually distinct formulae were assessed per topic, with an average assessment time of 26.6 seconds per formula instance. The average number of visually distinct formulae with H+M+L binarization was 63.2 per query, with the highest number being 143 (topic B.305) and the lowest being 2 (topic B.333).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,673.71,120.89,4.07"><p>https://math.stackexchange.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,108.93,617.08,397.06,7.99;3,89.29,628.04,205.60,7.99"><p>By collection we mean the content to be searched. That content together with topics and relevance judgments is a test collection. There is only one ARQMath collection</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,108.93,640.83,397.23,4.07;3,89.29,651.79,416.69,4.07;3,89.29,662.75,293.85,4.07"><p>ARQMath-1 was built for CLEF 2020, ARQMath-2 was built for CLEF 2021. We refer to submitted runs or evaluation results by year, as AQRMath-2020 or ARQMath-2021. This distinction is important because ARQMath-2022 participants also submitted runs for both the ARQMath-1 and -2 test collections.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,108.93,673.71,162.78,4.07"><p>https://archive.org/download/stackexchange</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,108.93,662.65,309.80,4.07"><p>https://drive.google.com/drive/folders/1ZPKIWDnhMGRaPNVLi1reQxZWTfH2R4u3</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,108.93,673.61,166.38,4.07"><p>https://github.com/ARQMath/ARQMathCode</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="7,108.93,673.70,304.82,4.07"><p>https://drive.google.com/drive/u/1/folders/1l1c2O06gfCk2jWOixgBXI9hAlATybxKv</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="8,108.93,651.79,398.43,4.07;8,89.29,662.75,418.07,4.07;8,89.29,673.71,237.08,4.07"><p>Source code and instructions for running the baselines are available from GitLab (Tangent-S: https://gitlab. com/dprl/tangent-s, PyTerrier: https://gitlab.com/dprl/pt-arqmath/) and GoogleDrive (Terrier: https://drive.google. com/drive/u/0/folders/1YQsFSNoPAFHefweaN01Sy2ryJjb7XnKF)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="11,108.93,651.78,397.05,4.07;11,89.02,662.74,28.72,4.07"><p>H+M binarization corresponds to the definition of relevance usually used in the Text Retrieval Conference (TREC).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="11,108.93,673.70,224.35,4.07"><p>One assessor (with id 8) was not able to continue assessment.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="18,108.93,651.78,397.05,4.07;18,89.29,662.74,90.89,4.07"><p>For ranked lists of depth 1 there is no discounting or accumulation, and in ARQMath the relevance value is used directly as the gain.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11" coords="18,108.93,673.70,148.75,4.07"><p>https://huggingface.co/witiko/mathberta</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12" coords="19,108.93,662.74,397.05,4.07;19,89.29,673.69,20.62,4.07"><p>Because some topics have no highly relevant answers, the actual maximum value of AR on the Task 3 topics is 2.346.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_13" coords="21,108.93,673.60,396.96,4.07"><p>The five topics for which results were not characterized in this way are A.301, A.314, A.322, A.324, and A.350.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.0.1.">Acknowledgements</head><p>We thank our student assessors from <rs type="institution">RIT</rs>: <rs type="person">Duncan Brickner</rs>, <rs type="person">Jill Conti</rs>, <rs type="person">James Hanby</rs>, <rs type="person">Gursimran Lnu</rs>, <rs type="person">Megan Marra</rs>, <rs type="person">Gregory Mockler</rs>, <rs type="person">Tolu Olatunbosun</rs>, and <rs type="person">Samson Zhang</rs>. This material is based upon work supported by the <rs type="funder">National Science Foundation (USA)</rs> under Grant No. <rs type="grantNumber">IIS-1717997</rs> and the <rs type="funder">Alfred P. Sloan Foundation</rs> under Grant No. <rs type="grantNumber">G-2017-9827</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZuTVVaJ">
					<idno type="grant-number">IIS-1717997</idno>
				</org>
				<org type="funding" xml:id="_2AmeM6R">
					<idno type="grant-number">G-2017-9827</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>ARQMath 2022 Task 2 (Formula Retrieval) results. P: primary run, M: manual run, (✓): baseline pooled as a primary run. MAP ′ and P ′ @10 use H+M binarization. Baseline results in parentheses. Data indicates sources used by systems: (M)ath, or (B)oth math and text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARQMath-1</head><p>ARQMath-  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="22,112.66,385.08,394.52,4.95;22,112.66,398.62,240.72,4.95" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="22,276.50,385.08,225.87,4.95">Characterizing Searches for Mathematical Concepts</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,127.29,398.62,195.86,4.95">Joint Conference on Digital Libraries (JCDL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,412.17,393.33,4.95;22,112.66,425.72,109.71,4.95" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="22,265.79,412.17,162.16,4.95">NTCIR-10 Math Pilot Task Overview</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,451.70,412.17,54.28,4.95;22,112.66,425.72,78.70,4.95">Proceedings of the 10th NTCIR</title>
		<meeting>the 10th NTCIR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,439.27,393.32,4.95;22,112.66,452.82,98.10,4.95" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<title level="m" coord="22,266.94,439.27,148.70,4.95;22,439.50,439.27,66.48,4.95;22,112.66,452.82,67.09,4.95">NTCIR-11 Math-2 Task Overview</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Proceedings of the 11th NTCIR</note>
</biblStruct>

<biblStruct coords="22,112.66,466.37,393.61,4.95;22,112.66,479.92,231.04,4.95" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="22,402.62,466.37,103.65,4.95;22,112.66,479.92,40.82,4.95">NTCIR-12 MathIR Task Overview</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Topic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,177.14,479.92,135.55,4.95">Proceedings of the 12th NTCIR</title>
		<meeting>the 12th NTCIR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,493.47,393.32,4.95;22,112.66,507.02,167.45,4.95" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Líška</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Růžička</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mravec</surname></persName>
		</author>
		<title level="m" coord="22,298.96,493.47,207.02,4.95;22,112.66,507.02,135.53,4.95">Web Interface and Collection for Mathematical Retrieval WebMIaS and MREC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,520.57,394.61,4.95;22,112.28,534.12,393.71,4.95;22,112.28,547.67,393.70,4.95;22,112.48,561.21,363.44,4.95" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="22,232.75,520.57,274.52,4.95;22,112.28,534.12,260.34,4.95">Retrieval of Research-level Mathematical Information Needs: A Test Collection and Technical Terminology Experiment</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Teufel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,398.16,534.12,107.82,4.95;22,112.28,547.67,393.70,4.95;22,112.48,561.21,222.22,4.95">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct coords="22,112.66,574.76,393.33,4.95;22,112.66,588.31,393.57,4.95;22,112.33,601.86,29.19,4.95" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="22,347.18,574.76,158.81,4.95;22,112.66,588.31,235.47,4.95">Effects of context, complexity, and clustering on evaluation for math formula retrieval</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10504</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="22,112.66,615.41,395.17,4.95;22,112.66,628.96,393.33,4.95;22,112.66,642.51,242.89,4.95" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="22,164.96,628.96,215.30,4.95">SemEval-2019 Task 10: Math Question Answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">Le</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Petrescu-Prahova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,403.40,628.96,102.59,4.95;22,112.66,642.51,212.89,4.95">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>the 13th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,656.06,393.33,4.95;22,112.66,669.61,107.17,4.95" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Meadows</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.15231</idno>
		<title level="m" coord="22,218.95,656.06,213.70,4.95">A Survey in Mathematical Language Processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="23,112.66,90.22,393.33,4.95;23,112.66,103.77,393.61,4.95;23,112.66,117.32,135.49,4.95" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="23,242.72,90.22,263.26,4.95;23,112.66,103.77,38.97,4.95">Declarative Experimentation in Information Retrieval using PyTerrier</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,174.24,103.77,332.03,4.95;23,112.66,117.32,105.85,4.95">Proceedings of the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval</title>
		<meeting>the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,130.87,393.32,4.95;23,112.66,144.41,382.52,4.95" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="23,417.94,130.87,88.04,4.95;23,112.66,144.41,79.27,4.95">Terrier Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,215.07,144.41,207.71,4.95">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,157.96,393.33,4.95;23,112.66,171.51,393.33,4.95;23,112.66,185.06,259.54,4.95" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="23,207.26,157.96,298.73,4.95;23,112.66,171.51,69.16,4.95">Layout and Semantics: Combining Representations for Mathematical Formula Search</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,208.36,171.51,297.62,4.95;23,112.66,185.06,229.90,4.95">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,198.61,393.33,4.95;23,112.66,212.16,290.73,4.95" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="23,210.16,198.61,295.83,4.95;23,112.66,212.16,153.91,4.95">On Information Retrieval Metrics Designed for Evaluation with Incomplete Relevance Assessments</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,275.40,212.16,96.07,4.95">Information Retrieval</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,225.71,395.17,4.95;23,112.66,239.26,395.16,4.95;23,112.66,252.81,172.22,4.95" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="23,240.53,225.71,224.27,4.95">Retrieval Evaluation with Incomplete Information</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,488.38,225.71,19.45,4.95;23,112.66,239.26,395.16,4.95;23,112.66,252.81,142.58,4.95">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,266.36,393.33,4.95;23,112.66,279.91,393.33,4.95;23,112.66,293.46,335.29,4.95" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="23,425.90,266.36,80.08,4.95;23,112.66,279.91,206.09,4.95">Tangent-CFT: An Embedding Model for Mathematical Formulas</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,342.69,279.91,163.29,4.95;23,112.66,293.46,305.45,4.95">Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval (ICTIR)</title>
		<meeting>the 2019 ACM SIGIR International Conference on Theory of Information Retrieval (ICTIR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,307.00,395.17,4.95;23,112.66,320.55,393.32,4.95;23,112.66,334.10,197.76,4.95" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="23,286.67,307.00,221.17,4.95;23,112.66,320.55,26.62,4.95">Learning to Rank for Mathematical Formula Retrieval</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,161.51,320.55,344.48,4.95;23,112.66,334.10,168.12,4.95">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,347.65,395.17,4.95;23,112.66,361.20,393.33,4.95;23,112.33,374.75,29.19,4.95" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="23,351.39,347.65,156.44,4.95;23,112.66,361.20,235.32,4.95">Retrieving and Reading: A Comprehensive Survey on Open-Domain Question Answering</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00774v3</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="23,112.66,388.30,394.53,4.95;23,112.66,401.85,374.64,4.95" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="23,277.24,401.85,178.86,4.95">Language Models are Few-Shot Learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,415.40,394.53,4.95;23,112.66,428.95,393.33,4.95;23,112.66,442.50,76.23,4.95" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="23,210.14,415.40,292.41,4.95">Controlling linguistic style aspects in neural language generation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ficler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,127.80,428.95,230.81,4.95">Proceedings of the Workshop on Stylistic Variation</title>
		<meeting>the Workshop on Stylistic Variation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,456.05,394.53,4.95;23,112.33,469.60,308.63,4.95" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="23,260.12,456.05,242.44,4.95">CoQA: A Conversational Question Answering Challenge</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,112.33,469.60,276.71,4.95">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,483.14,393.33,4.95;23,112.66,496.69,281.40,4.95" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09675</idno>
		<title level="m" coord="23,375.83,483.14,130.16,4.95;23,112.66,496.69,98.12,4.95">BERTScore: Evaluating Text Generation with BERT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
