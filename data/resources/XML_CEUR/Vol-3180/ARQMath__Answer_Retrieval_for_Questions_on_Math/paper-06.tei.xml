<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,405.51,15.42;1,89.29,110.65,404.04,5.42">Combining Sparse and Dense Information Retrieval Soft Vector Space Model and MathBERTa at ARQMath-3 Task 1 (Answer Retrieval)</title>
				<funder ref="#_Q73aZDn">
					<orgName type="full">Ministry of Education</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.90,136.56,60.39,5.42"><forename type="first">V√≠t</forename><surname>Novotn√Ω</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Informatics</orgName>
								<orgName type="institution">Masaryk University</orgName>
								<address>
									<addrLine>Botanick√° 554/68a</addrLine>
									<postCode>602 00</postCode>
									<settlement>Brno</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,161.93,136.56,76.36,5.42"><forename type="first">Michal</forename><surname>≈†tef√°nik</surname></persName>
							<email>stefanik.m@mail.muni.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Informatics</orgName>
								<orgName type="institution">Masaryk University</orgName>
								<address>
									<addrLine>Botanick√° 554/68a</addrLine>
									<postCode>602 00</postCode>
									<settlement>Brno</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,405.51,15.42;1,89.29,110.65,404.04,5.42">Combining Sparse and Dense Information Retrieval Soft Vector Space Model and MathBERTa at ARQMath-3 Task 1 (Answer Retrieval)</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">527F6E54C4B306FF2E961500CAEF1070</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>information retrieval</term>
					<term>sparse retrieval</term>
					<term>dense retrieval</term>
					<term>soft vector space model</term>
					<term>math representations</term>
					<term>word embeddings</term>
					<term>constrained positional weighting</term>
					<term>decontextualization</term>
					<term>word2vec</term>
					<term>transformers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sparse retrieval techniques can detect exact matches, but are inadequate for mathematical texts, where the same information can be expressed as either text or math. The soft vector space model has been shown to improve sparse retrieval on semantic text similarity, text classification, and machine translation evaluation tasks, but it has not yet been properly evaluated on math information retrieval.</p><p>In our work, we compare the soft vector space model against standard sparse retrieval baselines and state-of-the-art math information retrieval systems from Task 1 (Answer Retrieval) of the ARQMath-3 lab. We evaluate the impact of different math representations, different notions of similarity between key words and math symbols ranging from Levenshtein distances to deep neural language models, and different ways of combining text and math.</p><p>We show that using the soft vector space model consistently improves effectiveness compared to using standard sparse retrieval techniques. We also show that the Tangent-L math representation achieves better effectiveness than LaTeX, and that modeling text and math separately using two models improves effectiveness compared to jointly modeling text and math using a single model. Lastly, we show that different math representations and different ways of combining text and math benefit from different notions of similarity between tokens. Our best system achieves NDCG' of 0.251 on Task 1 of the ARQMath-3 lab.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>State-of-the-art math information retrieval systems use sparse retrieval techniques that can detect exact key word matches with high precision, but fail to retrieve texts that are semantically similar but use different terminology. This shortcoming is all the more apparent with mathematical texts, where the same information can be expressed in two completely different systems of writing and thought: the natural language and the language of mathematics.</p><p>Recently, the soft vector space model of Sidorov et al. <ref type="bibr" coords="1,356.51,547.23,18.27,4.94" target="#b31">[25]</ref> made it possible to retrieve documents according to both exact and fuzzy key word matches and has outperformed standard sparse retrieval techniques on semantic text similarity <ref type="bibr" coords="1,330.04,574.33,11.28,4.94" target="#b1">[2]</ref>, text classification <ref type="bibr" coords="1,426.89,574.33,16.04,4.94" target="#b22">[17]</ref>, and machine translation evaluation <ref type="bibr" coords="1,192.11,587.87,18.27,4.94" target="#b32">[26]</ref> tasks. The soft vector space has been used for math information retrieval in the ARQMath-1 and 2 labs <ref type="bibr" coords="1,264.64,601.42,16.74,4.94" target="#b23">[18,</ref><ref type="bibr" coords="1,284.12,601.42,12.56,4.94" target="#b20">16]</ref>. However, it has not been properly compared to sparse retrieval baselines. Furthermore, the soft vector space model makes it possible to use different representations of math, different notions of similarity between key words and symbols, and different ways to combine text and math. However, neither of these possibilities has been previously explored.</p><p>In our work, we aim to answer the following four research questions:</p><p>1. Does the soft vector space model outperform sparse information retrieval baselines on the math information retrieval task?</p><p>2. Which math representation works best with the soft vector space model?</p><p>3. Which notion of similarity between key words and symbols works best?</p><p>4. Is it better to use a single soft vector space model to represent both text and math or to use two separate models?</p><p>The rest of our paper is structured as follows: In Section 2, we describe our system and our experimental setup. In Section 3, we report and discuss our experimental results. We conclude in Section 4 by answering our research questions and summarizing our contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>In this section, we describe the datasets we used to train our tokenizers and language models. We also describe how we used our language models to measure similarity between text and math tokens, how we used our similarity measures to find answers to math questions, and how we evaluated our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Datasets</head><p>In our experiments, we used the Math StackExchange and ArXMLiv corpora:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Math StackExchange</head><p>The Math StackExchange collection v1.2 (MSE) <ref type="foot" coords="2,412.82,484.62,3.71,3.61" target="#foot_0">1</ref> provided by the organizers of the ARQMath-2 lab [11, Section 3] contains 2,466,080 posts from the Math StackExchange question answering website in HTML5 with math formulae in LaTeX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ArXMLiv</head><p>The ArXMLiv 2020 corpus <ref type="bibr" coords="2,263.88,538.84,13.10,4.94" target="#b2">[3]</ref> contains 1,571,037 scientific preprints from ArXiv in the HTML5 format with math formulae in MathML. Documents in the dataset were converted from LaTeX sources and are divided into the following subsets according to the severity of errors encountered during conversion: no-problem (10%), warning (60%), and error (30%).</p><p>From the corpora, we produced a number of datasets<ref type="foot" coords="2,339.11,614.88,3.71,3.61" target="#foot_1">2</ref> in different formats that we used to train our tokenizers and language models: </p><formula xml:id="formula_0" coords="3,89.29,87.94,62.96,9.77">Text + LaTeX</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text</head><p>To train text language models, we used the same combinations of MSE and ArXMLiv as in the previous dataset, but now our dataset only contains text with math formulae removed.</p><p>Example: (Graphs of residually finite groups) Assume that and are satisfied. Let be a graph of groups. If is infinite then assume that is continuous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LaTeX</head><p>To train math language models, we used the same combinations of MSE and ArXMLiv subsets as in the previous datasets, but now our dataset only contains formulae in the LaTeX format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example: \begin{pmatrix}1&amp;n\0&amp;1\end{pmatrix}\begin{pmatrix}1&amp;p\0&amp;1\end{pmatrix}</head><p>Tangent-L To train math language models, we used the same combinations of MSE and ArXMLiv subsets as in the previous datasets, but now our dataset only contains formulae in the format used by the state-of-the-art Tangent-L search engine from UWaterloo<ref type="foot" coords="3,480.73,358.28,3.71,3.61" target="#foot_2">3</ref>  <ref type="bibr" coords="3,487.66,360.91,15.93,4.94" target="#b17">[14]</ref>.</p><p>Example:</p><formula xml:id="formula_1" coords="3,159.80,376.70,345.70,9.72">#(start)# #(v! ‚óÅ ,/,n,-)# #(v! ‚óÅ ,/,n)# #(/,v!l,n,n)# #(/,v!l,n)# #(v!l,!0,nn)# #(v!l,!0)# #(end)#</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Tokenization</head><p>In our system, we used several tokenizers:</p><p>‚Ä¢ To tokenize text, we used the BPE tokenizer of the roberta-base language model<ref type="foot" coords="3,485.62,451.54,3.71,3.61" target="#foot_3">4</ref>  <ref type="bibr" coords="3,492.55,454.18,11.34,4.94" target="#b9">[9]</ref>.</p><p>‚Ä¢ To tokenize math, we used two different tokenizers for the LaTeX and Tangent-L formats:</p><p>-To tokenize LaTeX, we trained a BPE tokenizer<ref type="foot" coords="3,345.91,495.08,3.71,3.61" target="#foot_4">5</ref> with a vocabulary of size 50,000 on our LaTeX dataset. -To tokenize Tangent-L, we strip leading and trailing hash signs (#) from a formula representation and then split it into tokens using the #\s+# Perl regex. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Language Modeling</head><p>In our experiments, we used two different types of language models: Shallow log-bilinear models We trained shallow word2vec language models<ref type="foot" coords="4,448.84,431.67,3.71,3.61" target="#foot_5">6</ref>  <ref type="bibr" coords="4,455.77,434.30,18.04,4.94" target="#b16">[13]</ref> on our text + LaTeX, text, LaTeX, and Tangent-L datasets.</p><p>On text documents, a technique known as constrained positional weighting has been shown to improve the performance of word2vec models on analogical reasoning and causal language modeling <ref type="bibr" coords="4,207.32,492.94,16.58,4.94" target="#b24">[19]</ref>. To evaluate the impact of constrained positional weighting on math information retrieval, we trained word2vec models both with and without constrained positional weighting for every dataset. For brevity, we refer to word2vec with and without constrained positional weighting as positional word2vec and nonpositional word2vec in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep transformer models</head><p>To model text, we used pre-trained roberta-base model<ref type="foot" coords="4,481.22,566.94,3.71,3.61" target="#foot_6">7</ref>  <ref type="bibr" coords="4,488.15,569.57,11.43,4.94" target="#b9">[9]</ref>.</p><p>Related work shows that accurate domain-specialized representations can be obtained by continuous training, i.e. adaptation, using masked language modeling (MLM) on domain-specific unlabeled texts, in medicine <ref type="bibr" coords="4,318.31,614.66,16.36,4.94" target="#b26">[21]</ref>, biology <ref type="bibr" coords="4,377.73,614.66,11.51,4.94" target="#b8">[8]</ref>, and other scientific texts <ref type="bibr" coords="4,116.56,628.21,11.35,4.94" target="#b0">[1]</ref>. Previous work <ref type="bibr" coords="4,201.67,628.21,16.31,4.94" target="#b25">[20,</ref><ref type="bibr" coords="4,220.70,628.21,13.93,4.94" target="#b30">24]</ref> performs continuous MLM training on scientific texts, or the math formulae thereof <ref type="bibr" coords="4,220.12,641.76,11.48,4.94" target="#b4">[5]</ref>. However, all the aforementioned works treats math as plain text and only few <ref type="bibr" coords="5,200.32,90.23,13.10,4.94" target="#b3">[4]</ref> promote math-specific representations in the model adaptation. This motivated us to experiment with adaptation incorporating specific encodings for non-textual expressions.</p><p>To model text and math in the LaTeX format, we replaced the tokenizer of roberta-base with our text and math tokenizer. Then, we extended the vocabulary of our model with the [MATH] and [/MATH] special tokens and with the tokens recognized by our LaTeX tokenizer, and we randomly initialized weights for the new tokens. We fine-tuned our model on our text + LaTeX dataset for one epoch using the MLM objective of RoBERTa<ref type="foot" coords="5,501.78,186.91,3.71,3.61" target="#foot_7">8</ref>  <ref type="bibr" coords="5,116.56,203.09,13.10,4.94" target="#b9">[9]</ref> and a learning rate of 10 -5 with a linear decay to zero, see the learning curves in Figure <ref type="figure" coords="5,147.63,216.64,3.74,4.94">1</ref>. We called our model MathBERTa and released it to the HF Model Hub.<ref type="foot" coords="5,472.26,214.01,3.71,3.61" target="#foot_8">9</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Token Similarity</head><p>To determine the similarity of text and math tokens, we first extracted their global representations from our language models: Shallow log-bilinear models We extracted token vectors from the input and output matrices of our word2vec models and averaged them to produce global token embeddings.</p><p>Deep transformer models Unlike word2vec, transformer models do not contain global representations of tokens, but produce representations of tokens in the context of a sentence.</p><p>To extract global token embeddings from roberta-base and MathBERTa, we decontextualized their contextual token embeddings<ref type="foot" coords="5,307.35,379.37,7.41,3.61" target="#foot_9">10</ref> [26, Section 3.2] on the sentences from our text + LaTeX dataset.</p><p>Then, we produced dictionaries of all tokens in our text + LaTeX, text, LaTeX, and Tangent-L datasets, <ref type="foot" coords="5,128.23,431.88,7.41,3.61" target="#foot_10">11</ref> removing all tokens that occurred less than twice in a dataset and keeping only 100,000 most frequent tokens from every dataset. For each dictionary, we produced two types of token similarity matrices <ref type="foot" coords="5,211.61,458.98,7.41,3.61" target="#foot_11">12</ref> that capture the surface-level lexical similarity and the semantic similarity between tokens, respectively:</p><p>Lexical similarity We used the method of Charlet and Damnati [2, Section 2.2] to produce similarity matrices using the Levenshtein distance between the tokens.</p><p>Semantic similarity We used the method of Charlet and Damnati [2, Section 2.1] to produce similarity matrices using the cosine similarity between the global token embeddings.</p><p>For all dictionaries, we produced two matrices using the token embeddings of the positional and non-positional word2vec models. For the text and text + LaTeX dictionaries, we also produced an additional matrix using the token embeddings of the roberta-base and MathBERTa models, respectively.</p><p>To ensure sparsity and symmetry of the matrices, we considered only the 100 most similar tokens for each token and we used the greedy algorithm of Novotn√Ω <ref type="bibr" coords="6,389.01,103.78,16.10,4.94" target="#b19">[15,</ref><ref type="bibr" coords="6,407.72,103.78,43.43,4.94">Section 3]</ref> to construct the matrices. For semantic similarity matrices, we also enforced strict diagonal dominance, which has been shown to improve performance on the semantic text similarity task <ref type="bibr" coords="6,452.04,130.88,16.10,4.94" target="#b22">[17,</ref><ref type="bibr" coords="6,470.58,130.88,33.38,4.94">Table 2]</ref>.</p><p>Finally, to produce token similarity matrices that capture both lexical and semantic similarity between tokens, we combined every semantic similarity matrix with a corresponding lexical similarity matrix as follows:</p><formula xml:id="formula_2" coords="6,111.97,193.50,394.67,9.97">Combined similarity = ùõº ‚Ä¢ Lexical similarity + (1 -ùõº) ‚Ä¢ Semantic similarity (1)</formula><p>In our system, we only used the combined token similarity matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Soft Vector Space Modeling</head><p>In order to find answers to math questions, we used sparse retrieval with the soft vector space model of Sidorov et al. <ref type="bibr" coords="6,186.91,283.82,15.93,4.94" target="#b31">[25]</ref>, using Lucene BM25 [6, Table <ref type="table" coords="6,334.59,283.82,4.39,4.94">1</ref>] as the vector space and our combined similarity matrices as the token similarity measure. To address the bimodal nature of math questions and answers, we used the following two approaches: <ref type="foot" coords="6,369.01,308.28,7.41,3.61" target="#foot_12">13</ref>Joint models To allow users to query math information using natural language and vise versa, we used single soft vector space models to jointly represent both text and math.</p><p>As our baselines, we used 1) Lucene BM25 with the text dictionary and no token similarities and 2) Lucene BM25 with the text + LaTeX dictionary and no token similarities.</p><p>We also used four soft vector space models with the text + LaTeX dictionary and the token similarity matrices from the positional and non-positional word2vec models, the roberta-base model, and the MathBERTa model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpolated models</head><p>To properly represent the different frequency distributions of text and math tokens, we used separate soft vector space models for text and math. The final score of an answer is determined by linear interpolation of the scores assigned by the two soft vector space models:</p><formula xml:id="formula_3" coords="6,131.87,511.82,374.77,9.97">Interpolated similarity = ùõΩ ‚Ä¢ Text similarity + (1 -ùõΩ) ‚Ä¢ Math similarity (2)</formula><p>As our baselines, we used Lucene BM25 with the text dictionary and no token similarities interpolated with 1) Lucene BM25 with the LaTeX dictionary and no token similarities and with 2) Lucene BM25 with the Tangent-L dictionary and no token similarities.</p><p>We also used four pairs of soft vector space models: two pairs with the text and LaTeX dictionaries and two pairs with the text and Tangent-L dictionaries. In each of the two pairs, one used the token similarity matrices from the positional word2vec model and the other used the token similarity matrices from non-positional word2vec model.</p><p>For our representation of questions in the soft vector space model, we used the tokens in the title and in the body text. To represent an answer in the soft vector space model, we used the tokens in the title of its parent question and in the body text of the answer. To give greater weight to tokens in the title, we repeated them ùõæ times, which proved useful in ARQMath-2 [16, Section 3.2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Evaluation</head><p>To evaluate our system, we searched for answers to sets of topics provided by the ARQMath organizers for Task 1 (Answer Retrieval) <ref type="bibr" coords="7,267.88,207.70,16.10,4.94" target="#b35">[28,</ref><ref type="bibr" coords="7,286.61,207.70,12.29,4.94" target="#b12">11,</ref><ref type="bibr" coords="7,301.53,207.70,8.20,4.94" target="#b14">12</ref>, Section 4.1]. As our retrieval units, we used answers from the MSE dataset.</p><p>Effectiveness To determine how well the answers retrieved by our system satisfied the information needs of users, we used the normalized discounted cumulative gain prime (NDCG') evaluation measure <ref type="bibr" coords="7,249.02,273.85,18.11,4.94" target="#b29">[23]</ref> on the top 1,000 answers retrieved by our system for each topic. As our ground truth, we used the relevance judgements provided by the ARQMath organizers <ref type="bibr" coords="7,213.41,300.95,16.43,4.94" target="#b35">[28,</ref><ref type="bibr" coords="7,232.57,300.95,12.55,4.94" target="#b12">11,</ref><ref type="bibr" coords="7,247.84,300.95,12.55,4.94" target="#b14">12,</ref><ref type="bibr" coords="7,263.12,300.95,50.52,4.94">Section 4.3]</ref>.</p><p>To select the optimal values for parameters ùõº, ùõΩ, and ùõæ, we used the 148 topics from ARQMath-1 and 2, Task 1 and performed a grid search over values ùõº ‚àà {0.0, 0.1, . . . , 1.0}, ùõΩ ‚àà {0.0, 0.1, . . . , 1.0}, and ùõæ ‚àà {1, 2, 3, 4, 5}. To estimate the effectiveness of our system, we used the 78 topics from ARQMath-3 Task 1.</p><p>Due to time constraints, we hand-picked the parameter values ùõº = 0.1, ùõΩ = 0.5, and ùõæ = 5 for our submissions to the ARQMath-3 lab. We report effectiveness for both hand-picked and optimal parameter values, and discuss the robustness of our system to parameter variations.</p><p>Efficiency Our system is a prototype written in a high-level programming language with emphasis on correctness over efficiency. Furthermore, we computed our evaluation on a non-dedicated computer cluster with heterogeneous hardware, which made it difficult to meaningfully measure the efficiency of our system. Therefore, we have not measured and do not report the efficiency of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>In tables 1 and 2, we list effectiveness results with hand-picked parameter values submitted to the ARQMath-3 lab for our joint and interpolated soft vector space models. In tables 3 and 4, we list post-competition effectiveness results with optimized parameter values for our joint and interpolated models. In all tables 1-4, we also list the parameter values the we used.</p><p>In Figure <ref type="figure" coords="7,143.21,607.85,3.74,4.94" target="#fig_1">2</ref>, we visualize the effectiveness of our baseline models with optimized parameter values and how it is affected by our various extensions.</p><p>In Table <ref type="table" coords="7,138.57,634.95,3.70,4.94" target="#tab_3">5</ref>, we compare our post-competition effectiveness results with optimized parameter values to the baselines and the best results from other teams on ARQMath-3 Task 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Results with hand-picked parameter values submitted to the ARQMath-3 lab for joint soft vector space models on ARQMath-3 Task 1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Results with hand-picked parameter values submitted to the ARQMath-3 lab for interpolated soft vector space models on ARQMath-3 Task 1</p><formula xml:id="formula_4" coords="8,111.99,260.87,371.30,9.93">Model ùõº 1 ùõæ 1 ùõº 2 ùõæ 2 ùõΩ NDCG'</formula><p>Interpolated text + Tangent-L (positional word2vec) 0.1 5 0.1 5 0.5 0.257</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Robustness to Parameter Variations</head><p>In tables 1-4, the differences between hand-picked and optimized parameter values for joint models are within 0.002 NDCG' except Joint text (roberta-base), which improves effectiveness by 0.041 NDCG' by placing more weight on the lexical similarity of tokens (ùõº: 0.1 ‚Üí 0.6) and by placing less weight on question titles (ùõæ: 5 ‚Üí 2). This shows that our joint vector space models are relatively robust to parameter variations. By contrast, optimizing parameter values for the Interpolated text + Tangent-L (positional word2vec) model improves effectiveness by 0.098 NDCG'. Compared to the hand-picked parameter values, the optimized parameter values place more weight on the lexical similarity for text tokens (ùõº 1 : 0.1 ‚Üí 0.7), use only semantic similarity for math tokens (ùõº 2 : 0.1 ‚Üí 0.0), place less weight on the text in question titles (ùõæ 1 : 5 ‚Üí 2), and place more weight on math over text (ùõΩ: 0.5 ‚Üí 0.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Effectiveness of Baselines and Their Extensions</head><p>Figure <ref type="figure" coords="8,119.69,523.19,4.97,4.94" target="#fig_1">2</ref> shows that the Joint text (no token similarities) baseline receives NDCG' of 0.235. Using roberta-base as the source of semantic similarity between text tokens improves effectiveness by 0.012 NDCG', reaching NDCG' of 0.247. By contrast, including also LaTeX math tokens reduces effectiveness by 0.011 NDCG', reaching NDCG' of 0.224, which we attribute to the difficulty to properly represent the different frequency distributions of text and math tokens in a single joint model. However, when we also use either positional word2vec or MathBERTa as the source of semantic similarity between text and math tokens, effectiveness improves by 0.025 NDCG', reaching NDCG' of 0.249. Removing the positional weighting from word2vec further improves effectiveness by 0.002 NDCG', reaching NDCG' of 0.251, which is the best result among our joint models.</p><p>Figure <ref type="figure" coords="8,130.83,658.69,4.99,4.94" target="#fig_1">2</ref> also shows that the Interpolated text + LaTeX (no token similarities) baseline receives NDCG' of 0.257. Using non-positional word2vec as the source of similarity between text and The Interpolated text + Tangent-L (no token similarities) baseline receives NDCG' of 0.349. Using non-positional word2vec as the source of similarity between text and math tokens improves effectiveness by 0.002 NDCG', reaching NDCG' of 0.251. Enabling the positional weighting of word2vec further improves effectiveness by 0.004 NDCG', reaching NDCG' of 0.355, the best result among all our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimized Parameter Values</head><p>Tables <ref type="table" coords="9,119.20,529.89,4.97,4.94" target="#tab_2">3</ref> and<ref type="table" coords="9,145.18,529.89,4.97,4.94">4</ref> show that all joint models and the interpolated models for text place more weight on the lexical similarity of tokens (ùõº and ùõº 1 of either 0.6 or 0.7). Furthermore, all joint and interpolated models for text place equal weight on question titles (ùõæ and ùõæ 1 of 2). By contrast, all joint models for text and math and the interpolated models for math place comparatively higher weight on the math in question titles (ùõæ and ùõæ 2 between 3 and 5). This indicates that math in question titles is more informative than text in question titles. Additionally, all interpolated models for LaTeX math only used the lexical similarity of tokens (ùõº 2 : 1.0). By contract, all interpolated models for Tangent-L math only used the semantic similarity of tokens (ùõº 2 : 0.0). Lastly, all interpolated models place more weight on text over math (ùõΩ of either 0.6 or 0.7). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Comparison to Results from Other Teams</head><p>Our submission to the ARQMath-3 lab with hand-picked parameter values placed last in effectiveness among the teams that participated in Task 1. However Table <ref type="table" coords="10,428.25,408.76,5.17,4.94" target="#tab_3">5</ref> shows that our Interpolated text + Tangent-L (positional word2vec) model with optimized parameter values achieves better effectiveness than the best SVM-Rank system from the DPRL team <ref type="bibr" coords="10,450.11,435.86,17.59,4.94" target="#b10">[10]</ref> by 0.011 NDCG'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we aimed to answer the following research questions:</p><p>1. Does the soft vector space model outperform sparse information retrieval baselines on the math information retrieval task?</p><p>2. Which math representation works best with the soft vector space model?</p><p>3. Which notion of similarity between key words and symbols works best?</p><p>4. Is it better to use a single soft vector space model to represent both text and math or to use two separate models?</p><p>Using our experimental results, we can answer our research questions as follows:  <ref type="bibr" coords="11,265.33,162.53,16.46,8.87" target="#b33">[27]</ref> 0.504 MiniLM+RoBERTa from MIRMU <ref type="bibr" coords="11,292.14,174.48,16.46,8.87" target="#b33">[27]</ref> 0.498 L8_a018 from MathDowsers <ref type="bibr" coords="11,272.34,186.44,11.83,8.87" target="#b6">[7]</ref> 0.474 math_10 from TU_DBS <ref type="bibr" coords="11,254.51,198.39,16.46,8.87" target="#b27">[22]</ref> 0  <ref type="bibr" coords="11,314.91,365.77,16.46,8.87" target="#b14">[12]</ref> 0.229 Joint Text + LaTeX (no token similarities) 0.224 TF-IDF (PyTerrier) baseline <ref type="bibr" coords="11,268.37,389.68,16.46,8.87" target="#b14">[12]</ref> 0.190 Tangent-S baseline <ref type="bibr" coords="11,235.83,401.63,16.46,8.87" target="#b14">[12]</ref> 0.159 Linked MSE Posts baseline <ref type="bibr" coords="11,266.48,413.59,16.46,8.87" target="#b14">[12]</ref> 0.106 1. Yes, using the soft vector space model to capture the semantic similarity between tokens consistently improves effectiveness on ARQMath-3 Task 1, both for just text and for text combined with different math representations.</p><p>2. Among LaTeX and Tangent-L, our soft vector space models using Tangent-L achieve the highest effectiveness on ARQMath-3 Task 1.</p><p>3. Among lexical and semantic similarity, all joint models and the interpolated models for text reach their highest effectiveness on ARQMath-3 Task 1 by combining both lexical and semantic similarity, but place slightly more weight on lexical similarity. The interpolated models for math gave mixed results: The model for Tangent-L reaches the highest efficiency by using only semantic similarity, whereas the model for LaTeX reaches the highest efficiency by using only lexical similarity.</p><p>Among sources of semantic similarity, joint models achieve comparable effectiveness on ARQMath-3 Task 1 with non-positional word2vec, positional word2vec, and Math-BERTa, and interpolated models achieved comparable effectiveness with non-positional word2vec and positional word2vec. This may indicate that the soft vector space model does not fully exploit the semantic information provided by the sources of semantic similarity and therefore does not benefit from their improvements after a certain threshold.</p><p>4. All our interpolated models achieved higher effectiveness on ARQMath-3 Task 1 than our joint models. This shows that it is generally better to use two separate models to represent text and math even at the expense of losing the ability to model the similarity between text and math tokens.</p><p>Our answers to research questions 2 and 3 also provide the following new questions:</p><p>2. Are there other math representations besides LaTeX and Tangent-L that may work better with the soft vector space model?</p><p>3. How can the soft vector space model be parametrized or improved, so that it can benefit from improved measures of similarity between tokens?</p><p>These questions should provide a fruitful venue for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,361.06,132.87,6.37,8.74;8,384.47,132.87,48.45,9.14;8,162.35,150.57,131.07,8.87;8,361.06,150.57,61.80,8.87;8,162.35,162.53,260.50,8.87;8,162.35,174.48,167.71,8.87;8,361.06,174.48,61.80,8.87;8,162.35,186.44,111.26,8.87;8,361.06,186.44,61.80,8.87"><head></head><label></label><figDesc>text + LaTeX (non-positional word2vec) 0.1 5 0.249 Joint text + LaTeX (positional word2vec)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,89.29,321.82,416.70,8.93;10,88.93,333.82,134.95,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2:The extensions of the baseline soft vector space models and their impact on the effectiveness with optimized parameter values</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,116.56,89.41,389.42,80.52"><head></head><label></label><figDesc>To train text &amp; math language models, we combined MSE with the no-problem and warning subsets of ArXMLiv. The dataset contains text and mathematical formulae in the LaTeX format surrounded by[MATH]  and [/MATH] tags. To validate our language models, we used a small part of the error subset of ArXMLiv and no data from MSE.</figDesc><table /><note coords="3,116.56,146.66,389.42,9.72;3,116.56,160.21,208.98,9.72"><p>Example: We denote the set of branches with [MATH] B_{0},B_{1},\ldots,B{n} [/MATH] where [MATH] n [/MATH] are the number of branches.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,88.98,90.49,418.67,329.01"><head>Table 3</head><label>3</label><figDesc>Post-competition results with optimized parameter values for joint soft vector space models on ARQMath-3 Task 1</figDesc><table coords="9,88.98,131.57,417.00,287.93"><row><cell>Model</cell><cell>ùõº</cell><cell>ùõæ NDCG'</cell><cell></cell></row><row><cell cols="3">Joint text + LaTeX (non-positional word2vec) 0.6 5 0.251</cell><cell></cell></row><row><cell>Joint text + LaTeX (positional word2vec)</cell><cell cols="2">0.7 5 0.249</cell><cell></cell></row><row><cell>Joint text + LaTeX (MathBERTa)</cell><cell cols="2">0.6 4 0.249</cell><cell></cell></row><row><cell>Joint text (roberta-base)</cell><cell cols="2">0.6 2 0.247</cell><cell></cell></row><row><cell>Joint text (no token similarities)</cell><cell></cell><cell>2 0.235</cell><cell></cell></row><row><cell>Joint text + LaTeX (no token similarities)</cell><cell></cell><cell>3 0.224</cell><cell></cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Post-competition results with optimized parameter values for interpolated soft vector space models on</cell></row><row><cell>ARQMath-3 Task 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">ùõº 1 ùõæ 1 ùõº 2 ùõæ 2 ùõΩ</cell><cell>NDCG'</cell></row><row><cell>Interpolated text + Tangent-L (positional word2vec)</cell><cell>0.7 2</cell><cell>0.0 5</cell><cell cols="2">0.7 0.355</cell></row><row><cell cols="2">Interpolated text + Tangent-L (non-positional word2vec) 0.6 2</cell><cell>0.0 5</cell><cell cols="2">0.7 0.351</cell></row><row><cell>Interpolated text + Tangent-L (no token similarities)</cell><cell>2</cell><cell>4</cell><cell cols="2">0.6 0.349</cell></row><row><cell>Interpolated text + LaTeX (positional word2vec)</cell><cell>0.7 2</cell><cell>1.0 5</cell><cell cols="2">0.6 0.288</cell></row><row><cell>Interpolated text + LaTeX (non-positional word2vec)</cell><cell>0.6 2</cell><cell>1.0 5</cell><cell cols="2">0.6 0.288</cell></row><row><cell>Interpolated text + LaTeX (no token similarities)</cell><cell>2</cell><cell>5</cell><cell cols="2">0.6 0.257</cell></row><row><cell cols="5">math tokens improves effectiveness by 0.031 NDCG', reaching NDCG' of 0.288. Using positional</cell></row><row><cell>word2vec does not further improve effectiveness.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,88.99,90.49,417.00,80.95"><head>Table 5</head><label>5</label><figDesc>Comparison of our post-competition effectiveness results with the baselines and the best results from other teams on ARQMath-3 Task 1</figDesc><table coords="11,158.99,133.14,26.18,8.87"><row><cell>Model</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.61,651.80,397.37,4.06;2,89.29,662.76,223.21,4.06"><p>An improved Math Stack Exchange collection v1.3 was made available by the organizers of the ARQMath-3 lab [12, Section 3], which we did not use due to time constraints.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,108.93,673.72,285.57,4.06"><p>See https://github.com/witiko/scm-at-arqmath3, file 01-prepare-dataset.ipynb.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,108.93,651.72,322.57,4.06"><p>See https://github.com/fwtompa/mathtuples, git commit 888b3d5 from October 25, 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,108.93,662.67,147.50,4.06"><p>See https://huggingface.co/roberta-base.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,108.93,673.63,287.16,4.06"><p>See https://github.com/witiko/scm-at-arqmath3, file 02-train-tokenizers.ipynb.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,108.93,662.76,284.68,4.06"><p>See https://github.com/witiko/scm-at-arqmath3, file 04-train-word2vec.ipynb.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="4,108.93,673.72,147.50,4.06"><p>See https://huggingface.co/roberta-base.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="5,108.93,629.88,288.64,4.06"><p>See https://github.com/witiko/scm-at-arqmath3, file 03-finetune-roberta.ipynb.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="5,108.93,640.84,165.39,4.06"><p>See https://huggingface.co/witiko/mathberta.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="5,108.93,651.80,392.28,4.06"><p>See https://github.com/witiko/scm-at-arqmath3, file 05-produce-decontextualized-word-embeddings.ipynb.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="5,108.93,662.76,304.18,4.06"><p>See https://github.com/witiko/scm-at-arqmath3, file 06-produce-dictionaries.ipynb.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11" coords="5,108.93,673.72,350.93,4.06"><p>See https://github.com/witiko/scm-at-arqmath3, file 07-produce-term-similarity-matrices.ipynb.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12" coords="6,108.93,673.66,311.56,4.06"><p>See https://github.com/witiko/scm-at-arqmath3, file 08-produce-arqmath-runs.ipynb.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been partly supported by the <rs type="funder">Ministry of Education</rs> of CR within the <rs type="projectName">LINDAT-CLARIAH-CZ</rs> project <rs type="grantNumber">LM2018101</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Q73aZDn">
					<idno type="grant-number">LM2018101</idno>
					<orgName type="project" subtype="full">LINDAT-CLARIAH-CZ</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,118.11,414.57,388.08,4.94;12,118.11,425.87,387.88,9.72;12,118.11,441.67,389.16,4.94;12,118.11,455.22,154.96,4.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,307.17,414.57,199.02,4.94;12,118.11,428.12,60.94,4.94">SciBERT. A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName coords=""><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1371</idno>
		<ptr target="https://aclanthology.org/D19-1371" />
	</analytic>
	<monogr>
		<title level="m" coord="12,204.88,425.87,153.60,9.72">Proceedings of EMNLP-IJCNLP 2019</title>
		<meeting>EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">Nov. 2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,118.11,473.25,387.87,4.94;12,118.11,484.55,388.96,9.72;12,118.11,498.10,389.07,9.72;12,118.11,513.90,338.92,4.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,309.82,473.25,196.16,4.94;12,118.11,486.80,343.37,4.94">Simbow at SemEval-2017 Task 3. Soft-Cosine Semantic Similarity Between Questions for Community Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Delphine</forename><surname>Charlet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geraldine</forename><surname>Damnati</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/S17-2051/(visitedon10/16/2021" />
	</analytic>
	<monogr>
		<title level="m" coord="12,489.08,484.55,17.99,9.72;12,118.11,498.10,354.68,9.72">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="315" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,118.11,529.68,389.07,9.72;12,118.11,545.48,388.58,4.94;12,118.11,559.03,146.16,4.94" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Deyan</forename><surname>Ginev</surname></persName>
		</author>
		<ptr target="https://sigmathling.kwarc.info/resources/arxmliv-dataset-2020/" />
		<title level="m" coord="12,182.25,529.68,231.65,9.72">arXMLiv:2020 dataset, an HTML5 conversion of arXiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>SIGMathLing -Special Interest Group on Math Linguistics</note>
</biblStruct>

<biblStruct coords="12,118.11,577.06,389.71,4.94;12,118.11,588.36,387.87,9.72;12,118.11,601.91,389.07,9.72;12,118.11,617.71,389.55,4.94;12,117.86,631.26,336.14,4.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,207.38,577.06,300.44,4.94;12,118.11,590.61,224.28,4.94">Continual Pre-training of Language Models for Math Problem Understanding with Syntax-Aware Memory Network</title>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Gong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.408</idno>
		<ptr target="https://aclanthology.org/2022.acl-long.408" />
	</analytic>
	<monogr>
		<title level="m" coord="12,370.12,588.36,135.86,9.72;12,118.11,601.91,244.81,9.72">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5923" to="5933" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="13,118.11,90.23,389.55,4.94;13,118.11,101.53,389.07,9.72;13,118.11,117.33,389.55,4.94;13,118.11,130.88,389.21,4.94;13,118.11,144.43,47.86,4.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,204.82,90.23,293.59,4.94">Modeling Mathematical Notation Semantics in Academic Papers</title>
		<author>
			<persName coords=""><forename type="first">Jo</forename><surname>Hwiyeol</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.266</idno>
		<ptr target="https://aclanthology.org/2021.findings-emnlp.266" />
	</analytic>
	<monogr>
		<title level="m" coord="13,132.86,101.53,311.91,9.72">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">Nov. 2021</date>
			<biblScope unit="page" from="3102" to="3115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,118.11,162.04,388.15,4.94;13,118.11,173.34,389.07,9.72;13,118.11,189.14,45.01,4.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,223.72,162.04,282.54,4.94;13,118.11,175.59,83.52,4.94">Which BM25 do you mean? A large-scale reproducibility study of scoring variants</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Kamphuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,228.57,173.34,203.52,9.72">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,118.11,206.75,389.55,4.94;13,118.11,218.05,389.08,9.72;13,118.11,233.85,25.94,4.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,323.20,206.75,184.46,4.94;13,118.11,220.30,103.01,4.94">Dowsing for Answers to Math Questions. Doing Better with Less</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yin</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ki</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Tompa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,247.85,218.05,208.51,9.72">Proceedings of the Working Notes of CLEF 2022</title>
		<meeting>the Working Notes of CLEF 2022</meeting>
		<imprint>
			<date type="published" when="2022-05-08">Sept. 5-8, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,146.78,233.85,282.47,4.94" xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>CEUR-WS</publisher>
			<pubPlace>Italy</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,118.11,251.46,387.88,4.94;13,118.11,262.77,389.55,9.72;13,118.11,278.56,230.17,4.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,204.86,251.46,301.12,4.94;13,118.11,265.01,116.76,4.94">BioBERT. A pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
	</analytic>
	<monogr>
		<title level="j" coord="13,300.93,262.77,64.00,9.72">Bioinformatics</title>
		<idno type="ISSN">1367-4803</idno>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020-02">Feb. 2020</date>
		</imprint>
	</monogr>
	<note>English</note>
</biblStruct>

<biblStruct coords="13,118.11,293.93,389.16,9.72;13,118.11,309.73,257.79,4.94" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="13,239.17,293.93,215.74,9.72">A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1907.11692v1" />
		<imprint>
			<date type="published" when="2019-05-27">2019. 05/27/2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,118.11,327.34,387.87,4.94;13,117.73,338.64,388.26,9.72;13,118.11,352.19,203.11,9.72" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,367.80,327.34,138.18,4.94;13,117.73,340.89,281.52,4.94">DPRL Systems in the CLEF 2022 ARQMath Lab. Introducing MathAMR for Math-Aware Search</title>
		<author>
			<persName coords=""><forename type="first">Behrooz</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,426.45,338.64,79.54,9.72;13,118.11,352.19,124.09,9.72">Proceedings of the Working Notes of CLEF 2022</title>
		<meeting>the Working Notes of CLEF 2022</meeting>
		<imprint>
			<date type="published" when="2022-05-08">Sept. 5-8, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,323.95,354.44,183.23,4.94;13,118.11,367.99,98.85,4.94" xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>CEUR-WS</publisher>
			<pubPlace>Italy</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,118.11,385.60,387.87,4.94;13,118.11,396.90,387.87,9.72;13,118.11,410.45,147.22,9.72" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,228.14,385.60,277.84,4.94;13,118.11,399.15,99.87,4.94">Overview of ARQMath-2. Second CLEF Lab on Answer Retrieval for Questions on Math</title>
		<author>
			<persName coords=""><forename type="first">Behrooz</forename><surname>Mansouri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,358.25,396.90,147.73,9.72;13,118.11,410.45,57.52,9.72">Proceedings of the Working Notes of CLEF 2021</title>
		<meeting>the Working Notes of CLEF 2021</meeting>
		<imprint>
			<date type="published" when="2021">Sept. 22-23, 2021</date>
		</imprint>
	</monogr>
	<note>Working Notes Version</note>
</biblStruct>

<biblStruct coords="13,268.19,412.70,238.99,4.94;13,118.11,426.25,386.95,4.94;13,117.79,439.80,102.90,4.94" xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-01.pdf" />
		<imprint>
			<date type="published" when="2021-10-23">2936. 2021. 10/23/2021</date>
			<publisher>CEUR-WS</publisher>
			<biblScope unit="page" from="1" to="24" />
			<pubPlace>Bucharest, Romania</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,118.11,457.41,388.08,4.94;13,118.11,468.71,387.88,9.72;13,118.11,482.26,169.04,9.72" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,234.26,457.41,112.50,4.94;13,381.93,457.41,124.27,4.94;13,118.11,470.96,137.78,4.94">Third CLEF Lab on Answer Retrieval for Questions on Math</title>
		<author>
			<persName coords=""><forename type="first">Behrooz</forename><surname>Mansouri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,390.26,468.71,115.73,9.72;13,118.11,482.26,86.89,9.72">Proceedings of the Working Notes of CLEF 2022</title>
		<meeting>the Working Notes of CLEF 2022</meeting>
		<imprint>
			<date type="published" when="2022-05-08">2022. Sept. 5-8, 2022</date>
		</imprint>
	</monogr>
	<note>Overview of ARQMath-3. Working Notes Version</note>
</biblStruct>

<biblStruct coords="13,290.79,484.51,216.48,4.94;13,118.11,498.06,73.37,4.94" xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>CEUR-WS</publisher>
			<pubPlace>Italy</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,118.11,515.67,388.07,4.94;13,118.11,526.97,389.55,9.72;13,118.11,542.77,389.55,4.94" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,225.44,515.67,280.75,4.94;13,118.11,529.22,74.42,4.94">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName coords=""><forename type="first">Tom√°≈°</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://papers" />
	</analytic>
	<monogr>
		<title level="m" coord="13,218.98,526.97,220.74,9.72">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,118.11,587.48,387.87,4.94;13,118.11,598.79,356.68,9.72" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,188.97,587.48,317.01,4.94;13,118.11,601.03,30.29,4.94">Dowsing for Answers to Math Questions. Ongoing Viability of Traditional MathIR</title>
		<author>
			<persName coords=""><forename type="first">Ki</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,176.43,598.79,208.27,9.72">Proceedings of the Working Notes of CLEF 2021</title>
		<meeting>the Working Notes of CLEF 2021</meeting>
		<imprint>
			<date type="published" when="2021">Sept. 22-23, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,477.79,601.03,28.48,4.94;13,118.11,614.58,389.16,4.94;13,118.11,628.13,290.84,4.94" xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-05.pdf" />
		<imprint>
			<date type="published" when="2021-10-13">2936. 2021. 10/13/2021</date>
			<publisher>CEUR-WS</publisher>
			<biblScope unit="page" from="63" to="81" />
			<pubPlace>Bucharest, Romania</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,118.11,643.50,387.87,9.72;13,118.11,657.05,389.07,9.72;13,118.11,672.84,201.21,4.94" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,185.52,645.75,228.93,4.94">Implementation Notes for the Soft Cosine Measure</title>
		<author>
			<persName coords=""><forename type="first">V√≠t</forename><surname>Novotn√Ω</surname></persName>
		</author>
		<idno type="DOI">10.1145/3269206.3269317</idno>
	</analytic>
	<monogr>
		<title level="m" coord="13,442.90,643.50,63.08,9.72;13,118.11,657.05,358.59,9.72">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1639" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,118.11,90.23,387.87,4.94;14,118.11,101.53,389.07,9.72;14,118.11,117.33,26.38,4.94" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,206.59,90.23,299.40,4.94;14,118.11,103.78,82.13,4.94">Ensembling Ten Math Information Retrieval Systems. MIRMU and MSM at ARQMath</title>
		<author>
			<persName coords=""><forename type="first">V√≠t</forename><surname>Novotn√Ω</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,246.73,101.53,201.71,9.72">Proceedings of the Working Notes of CLEF 2021</title>
		<meeting>the Working Notes of CLEF 2021</meeting>
		<imprint>
			<date type="published" when="2021-09-22">2021. Sept. 22-23, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,147.23,117.33,359.96,4.94;14,118.11,130.88,365.51,4.94" xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-06.pdf" />
		<imprint>
			<date type="published" when="2021-10-13">2936. 2021. 10/13/2021</date>
			<publisher>CEUR-WS</publisher>
			<biblScope unit="page" from="82" to="106" />
			<pubPlace>Bucharest, Romania</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,118.11,146.66,387.87,9.72;14,118.11,160.21,346.13,9.72" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="14,196.66,146.66,309.32,9.72;14,118.11,160.21,33.54,9.72">Text classification with word embedding regularization and soft similarity measure</title>
		<author>
			<persName coords=""><forename type="first">V√≠t</forename><surname>Novotn√Ω</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2003.05019v1" />
		<imprint>
			<date type="published" when="2020-10-15">2020. 10/15/2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,118.11,180.49,387.87,4.94;14,118.11,191.79,357.22,9.72" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,207.38,180.49,298.61,4.94;14,118.11,194.04,34.00,4.94">Three is Better than One. Ensembling Math Information Retrieval Systems</title>
		<author>
			<persName coords=""><forename type="first">V√≠t</forename><surname>Novotn√Ω</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,179.22,191.79,206.40,9.72">Proceedings of the Working Notes of CLEF 2020</title>
		<meeting>the Working Notes of CLEF 2020</meeting>
		<imprint>
			<date type="published" when="2020">Sept. 22-25, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,118.11,239.17,389.71,4.94;14,118.11,250.47,387.87,9.72;14,118.11,264.02,389.50,9.72" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="14,206.94,239.17,300.89,4.94;14,118.11,252.72,225.95,4.94">When FastText Pays Attention. Efficient Estimation of Word Representations using Constrained Positional Weighting</title>
		<author>
			<persName coords=""><forename type="first">V√≠t</forename><surname>Novotn√Ω</surname></persName>
		</author>
		<idno type="DOI">10.3897/jucs.69619</idno>
	</analytic>
	<monogr>
		<title level="j" coord="14,371.18,250.47,134.81,9.72;14,118.11,264.02,65.07,9.72">Journal of Universal Computer Science (J.UCS)</title>
		<idno type="ISSN">0948-6968</idno>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="201" />
			<date type="published" when="2022-02-28">Feb. 28, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,118.11,284.30,389.71,4.94;14,118.11,295.60,189.11,9.72" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="14,202.27,284.30,305.55,4.94;14,118.11,297.85,35.95,4.94">MathBERT. A Pre-Trained Model for Mathematical Formula Understanding</title>
		<author>
			<persName coords=""><forename type="first">Shuai</forename><surname>Peng</surname></persName>
		</author>
		<idno>ArXiv abs/2105.00377</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,118.11,315.88,387.87,4.94;14,118.11,327.18,388.12,9.72;14,117.79,342.98,71.86,4.94" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="14,209.26,315.88,296.73,4.94;14,118.11,329.43,256.53,4.94">Med-BERT. Pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction</title>
		<author>
			<persName coords=""><forename type="first">Laila</forename><surname>Rasmy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,400.63,327.18,89.99,9.72">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,118.11,361.01,388.08,4.94;14,118.11,372.31,389.07,9.72;14,118.11,388.11,36.14,4.94" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="14,348.55,361.01,157.64,4.94;14,118.11,374.56,127.68,4.94">Transformer-Encoder and Decoder Models for Questions on Math</title>
		<author>
			<persName coords=""><forename type="first">Anja</forename><surname>Reusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maik</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wolfgang</forename><surname>Lehner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,271.51,372.31,195.53,9.72">Proceedings of the Working Notes of CLEF 2022</title>
		<meeting>the Working Notes of CLEF 2022</meeting>
		<imprint>
			<date type="published" when="2022-05-08">Sept. 5-8, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,156.98,388.11,282.47,4.94" xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>CEUR-WS</publisher>
			<pubPlace>Italy</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,118.11,406.14,389.71,4.94;14,118.11,417.45,389.07,9.72;14,118.11,433.24,55.16,4.94" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="14,277.85,406.14,229.97,4.94;14,118.11,419.69,209.95,4.94">On information retrieval metrics designed for evaluation with incomplete relevance assessments</title>
		<author>
			<persName coords=""><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,356.58,417.45,95.29,9.72">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="447" to="470" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,118.11,451.27,387.87,4.94;14,118.11,462.58,267.11,9.72" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="14,214.46,451.27,291.52,4.94;14,118.11,464.82,113.60,4.94">MathBERT. A Pre-trained Language Model for General NLP Tasks in Mathematics Education</title>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Tracy Shen</surname></persName>
		</author>
		<idno>ArXiv abs/2106.07340</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,118.11,482.86,387.87,4.94;14,117.84,494.16,325.17,9.72" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="14,219.73,482.86,286.26,4.94;14,117.84,496.41,82.12,4.94">Soft similarity and soft cosine measure. Similarity of features in vector space model</title>
		<author>
			<persName coords=""><forename type="first">Grigori</forename><surname>Sidorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,226.84,494.16,105.76,9.72">Computaci√≥n y Sistemas</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="491" to="504" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,118.11,514.44,387.87,4.94;14,117.79,527.99,388.20,4.94;14,118.11,541.54,389.16,4.94;14,118.11,555.09,285.71,4.94" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="14,320.29,514.44,185.69,4.94;14,117.79,527.99,131.07,4.94">RegEMT. Regressive Ensemble for Machine Translation Quality Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Michal</forename><surname>≈†tef√°nik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V√≠t</forename><surname>Novotn√Ω</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petr</forename><surname>Sojka</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.wmt-1.112(visitedon05/27/2022" />
	</analytic>
	<monogr>
		<title level="m" coord="14,273.04,527.99,232.94,4.94;14,118.11,541.54,158.07,4.94">The 2021 Conference on Empirical Methods in Natural Language Processing EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021-11-10">Nov. 10, 2021. Nov. 10, 2021</date>
			<biblScope unit="page" from="1041" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,118.11,573.12,389.55,4.94;14,118.11,584.42,387.87,9.72;14,117.79,600.22,74.85,4.94" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="14,317.11,573.12,190.55,4.94;14,118.11,586.67,159.70,4.94">Diverse Semantics Representation is King. MIRMU and MSM at ARQMath 2022</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Geletka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vojtƒõch</forename><surname>Kalivoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,303.84,584.42,202.14,9.72">Proceedings of the Working Notes of CLEF 2022</title>
		<meeting>the Working Notes of CLEF 2022</meeting>
		<imprint>
			<date type="published" when="2022-05-08">Sept. 5-8, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,195.36,600.22,282.47,4.94" xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>CEUR-WS</publisher>
			<pubPlace>Italy</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,118.11,618.25,388.08,4.94;14,118.11,629.55,387.87,9.72;14,118.11,643.10,146.17,9.72" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="14,221.95,618.25,102.32,4.94;14,352.54,618.25,153.65,4.94;14,118.11,631.80,78.91,4.94">CLEF Lab on Answer Retrieval for Questions on Math</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,366.96,629.55,139.02,9.72;14,118.11,643.10,56.93,9.72">Proceedings of the Working Notes of CLEF 2020</title>
		<meeting>the Working Notes of CLEF 2020</meeting>
		<imprint>
			<date type="published" when="2020-09-22">2020. Sept. 22-25, 2020</date>
		</imprint>
	</monogr>
	<note>Overview of ARQMath. Updated Working Notes Version</note>
</biblStruct>

<biblStruct coords="15,118.11,90.23,389.72,4.94;15,118.11,101.53,387.87,9.72;15,117.79,117.33,74.85,4.94" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="15,298.08,90.23,209.74,4.94;15,118.11,103.78,160.32,4.94">Applying Structural and Dense Semantic Matching for the ARQMath Lab 2021, CLEF</title>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,305.57,101.53,200.42,9.72">Proceedings of the Working Notes of CLEF 2022</title>
		<meeting>the Working Notes of CLEF 2022</meeting>
		<imprint>
			<date type="published" when="2022-05-08">Sept. 5-8, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,195.36,117.33,282.47,4.94" xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>CEUR-WS</publisher>
			<pubPlace>Italy</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
