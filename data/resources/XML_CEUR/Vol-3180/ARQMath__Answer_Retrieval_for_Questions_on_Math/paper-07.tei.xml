<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.78,84.74,362.07,15.42;1,89.29,106.66,148.59,15.42">Transformer-Encoder and Decoder Models for Questions on Math</title>
				<funder ref="#_SMNqgZP">
					<orgName type="full">DFG</orgName>
				</funder>
				<funder>
					<orgName type="full">Cluster of Excellence &quot;Physics of Life&quot; of TU Dresden</orgName>
				</funder>
				<funder>
					<orgName type="full">Center for Information Services and HPC (ZIH)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,138.55,61.41,5.42"><forename type="first">Anja</forename><surname>Reusch</surname></persName>
							<email>anja.reusch@tu-dresden.de</email>
							<affiliation key="aff0">
								<orgName type="department">Database Systems Group</orgName>
								<orgName type="institution">Technische Universität Dresden</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,162.93,138.55,58.50,5.42"><forename type="first">Maik</forename><surname>Thiele</surname></persName>
							<email>maik.thiele@htw-dresden.de</email>
							<affiliation key="aff1">
								<orgName type="department">Hochschule für Wirtschaft und Technik</orgName>
								<address>
									<settlement>Dresden</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.42,138.55,84.55,5.42"><forename type="first">Wolfgang</forename><surname>Lehner</surname></persName>
							<email>wolfgang.lehner@tu-dresden.de</email>
							<affiliation key="aff0">
								<orgName type="department">Database Systems Group</orgName>
								<orgName type="institution">Technische Universität Dresden</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.78,84.74,362.07,15.42;1,89.29,106.66,148.59,15.42">Transformer-Encoder and Decoder Models for Questions on Math</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">6209DBE23213BC8FFC292E64F9579487</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Mathematical Language Processing</term>
					<term>Information Retrieval</term>
					<term>Transformer-based Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work summarizes our submission to ARQMath-3. We pre-trained Transformer-Encoder-based Language Models for the task of mathematical answer retrieval and employed a Transformer-Decoder Model for the generation of answers given a question from a mathematical domain. In comparison to our submission to ARQmath-2, we could improve the performance of our models regarding all three metrics nDGC', mAP' and p'@10 by refined pre-training and enlarged fine-tuning data. In addition, we improved our p'@10 results even further by additionally fine-tuning on annotated test data from ARQMath-2. In summary, our findings confirm that Transformer-based models benefit from domain adaptive pre-training in the mathematical domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With a rising number of scientific publications, retrieval of information from documents containing mathematical notation has recently received more attention. The task of Mathematical Information Retrieval (MIR) deals with finding relevant documents for a query, where both document and query may include mathematical notation such as L A T E X expressions beside natural language. As for many text-based tasks, Transformer-based models have demonstrated great potential for MIR. They could even be applied as a stand-alone model when adapted to the domain, since models like BERT <ref type="bibr" coords="1,234.98,477.09,12.84,4.94">[1]</ref> or ALBERT <ref type="bibr" coords="1,304.35,477.09,12.84,4.94" target="#b1">[2]</ref> were originally pre-trained on documents that did not contain mathematical notation. Recent research has therefore focused on adapting these models to the domain of mathematics by additional pre-training on the Mathematics StackExchange (MathSE). However, several base models such as BERT and ALBERT were further pre-trained and evaluated using different methods or data sets <ref type="bibr" coords="1,255.98,544.84,11.48,4.94" target="#b2">[3,</ref><ref type="bibr" coords="1,270.46,544.84,7.52,4.94" target="#b3">4,</ref><ref type="bibr" coords="1,280.98,544.84,7.65,4.94" target="#b4">5]</ref>. Hence, a fair comparison which base model is best suited for MIR is not possible. In order to evaluate their impact under the same conditions, we start our submission by pre-training and fine-tuning three Transformer-Encoder models, namely ALBERT, BERT and RoBERTa, on MIR.</p><p>In the core of this work, we refine our ALBERT-based approach from ARQMath-2. Here, the model was pre-trained on MathSE serving as in-domain data and fine-tuned using a classification task with the objective to predict whether a given post answers the question. The resulting classification probability assigned by the model was used to rank the answers. The success of models like BERT is attributed to pre-training on large diverse corpora. To evaluate the influence of the corpus, we experiment with pre-training using the AMPS corpus <ref type="bibr" coords="2,89.29,171.52,12.69,4.94" target="#b5">[6]</ref> which consists of 23GByte of question and answer pairs. Compared to 2021, we also enlarged our fine-tuning corpus by 152%. Furthermore, we also rely on the annotated test data of last year which we are leveraging for more informed training data for MIR. We will also study the impact of this new training data on our models. Finally, ARQMath-3 includes the new task of generating answers instead of retrieving them from the corpus. Our team fine-tuned a GPT-2 model <ref type="bibr" coords="2,320.27,239.27,12.69,4.94" target="#b6">[7]</ref> on the AMPS as well as on the provided MathSE corpora to generate solutions for the questions from the retrieval task. In summary, our submission to the ARQMath Lab 2022 Task 1 Answer Retrieval and Task 3 Answer Generation focuses on three areas:</p><p>• A comparison of three pre-trained Transformer-Encoder models: BERT, ALBERT, RoBERTa, • The impact of pre-training and fine-tuning data on MIR, • The application of Transformer-Decoder Models to Mathematical Answer Generation.</p><p>Our evaluation shows that all our submitted models could outperform the best models of ARQMath-2. Our ALBERT model trained on three different versions of the MathSE data and the enlarged fine-tuning data demonstrated the best performance. We could improve p'@10 even more by fine-tuning using annotated data from the run of 2021. Training models based on RoBERTa also shows to result in a promising approach, even though longer computing time is needed. All our models are made publicly available as part of the Huggingface Model Hub <ref type="foot" coords="2,490.08,429.57,3.71,3.61" target="#foot_0">1</ref> . The remainder of this submission document is structured as follows: Section 2 will review related work in the field of MIR and related generation tasks. We will introduce the tasks of the lab in Section 3 and the models in Section 4. Section 5 describes our model setup and the results of Task 1, while Section 6 will summarize our efforts for Task 3. The final section concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep learning models based on Encoders or Decoders of the Transformer architecture <ref type="bibr" coords="2,469.79,558.33,12.73,4.94" target="#b7">[8]</ref> have been widely introduced in several Natural Language Processing and Information Retrieval Tasks in recent years. Encoder-Models like BERT <ref type="bibr" coords="2,276.95,585.43,11.28,4.94">[1]</ref>, ALBERT <ref type="bibr" coords="2,334.48,585.43,11.28,4.94" target="#b1">[2]</ref>, and RoBERTa <ref type="bibr" coords="2,413.26,585.43,12.68,4.94" target="#b8">[9]</ref> have been applied to various domains including scientific literature <ref type="bibr" coords="2,313.34,598.97,16.41,4.94" target="#b9">[10]</ref>, medical documents <ref type="bibr" coords="2,427.37,598.97,16.55,4.94" target="#b10">[11,</ref><ref type="bibr" coords="2,446.77,598.97,14.11,4.94" target="#b11">12]</ref> or source code <ref type="bibr" coords="2,112.86,612.52,16.49,4.94" target="#b12">[13,</ref><ref type="bibr" coords="2,132.07,612.52,12.37,4.94" target="#b13">14]</ref>. Decoder-Models are typically used to generate text, where the most prominent examples being GPT-1, GPT-2, and GPT-3 <ref type="bibr" coords="2,276.99,626.07,16.43,4.94" target="#b14">[15,</ref><ref type="bibr" coords="2,296.15,626.07,7.47,4.94" target="#b6">7,</ref><ref type="bibr" coords="2,306.35,626.07,12.32,4.94" target="#b15">16]</ref>. Transformer-Encoder-based models for mathematical domains have also been studied with one example being MathBERT <ref type="bibr" coords="2,231.50,653.17,16.41,4.94" target="#b16">[17]</ref>. Here, mathematical formulas in form of operator trees are used as an input for pre-training. During the ARQMath Lab in 2020 and 2021, five teams submitted systems based on BERT, RoBERTa, and SentenceBERT <ref type="bibr" coords="3,381.09,103.78,16.45,4.94" target="#b17">[18,</ref><ref type="bibr" coords="3,400.27,103.78,12.55,4.94" target="#b18">19,</ref><ref type="bibr" coords="3,415.55,103.78,12.55,4.94" target="#b19">20,</ref><ref type="bibr" coords="3,430.83,103.78,12.55,4.94" target="#b20">21,</ref><ref type="bibr" coords="3,446.11,103.78,12.55,4.94" target="#b21">22,</ref><ref type="bibr" coords="3,461.39,103.78,14.04,4.94" target="#b22">23]</ref> where the models were used without domain adaption for downstream tasks. Only <ref type="bibr" coords="3,419.24,117.33,12.69,4.94" target="#b3">[4]</ref> pre-trained their submissions on mathematical documents. <ref type="bibr" coords="3,275.87,130.88,12.76,4.94" target="#b2">[3]</ref> fine-tuned a BERT model for notation prediction tasks based on scientific documents by enlarging the vocabulary of BERT with additional L A T E X tokens. <ref type="bibr" coords="3,124.45,157.97,12.84,4.94" target="#b4">[5]</ref> followed a similar procedure for mathematical documents. Generative Models have also found their way into the mathematical domain with GPT-𝑓 , a Transformer-based proof-solver <ref type="bibr" coords="3,233.31,185.07,16.22,4.94" target="#b23">[24]</ref>. <ref type="bibr" coords="3,256.77,185.07,12.81,4.94" target="#b5">[6]</ref> introduced two new data sets, one for measuring the performance of generative mathematical language models and one for pre-training. Along with it, benchmarks based on GPT-2 and GPT-3 were published. All of these only used an exercise-level data set and not a community-data set like the MathSE data in the task at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ARQMath 2022 Lab</head><p>The overall goal of ARQMath Lab 2022 (ARQMath-3) <ref type="bibr" coords="3,341.26,284.35,18.07,4.94" target="#b24">[25]</ref> is to accelerate the research in mathematical Information Retrieval. The lab consists of three tasks offering three different scenarios. Task 1 of the lab involves mathematical answer retrieval for a question asked on the Mathematics StackExchange<ref type="foot" coords="3,236.80,322.36,3.71,3.61" target="#foot_1">2</ref> , which is a platform for users to post questions related to mathematical topics to be answered by the community. The goal of this task is the retrieval of an answer post from 2010 -2018 to questions that were posted in 2019. The evaluation data of ARQMath-1 contain 99, while ARQMath-2 and 3 provided 100 query topics each, which are question posts including title, text and tags. In the 2020 test set, 77 queries were evaluated for Task 1, while its evaluation in 2021 included 71 queries. ARQMath-3 evaluated 78 queries. The optimal answers retrieved by the participants are expected to answer the complete question on their own. The relevance of the question-answer pairs was assessed by reviewers during the evaluation process. This relevance assessment was performed by pooling after the teams submitted their results. For each topic the participating teams submitted a ranked list of 1,000 documents retrieved by their systems, which were scored by Normalized Discounted Cumulative Gain, but with unjudged documents removed before assessment (nDCG'). The graded relevance scale used for scoring ranged from 0 (not relevant) to 3 (highly relevant). Two additional measures, mAP' and p@10, were also reported using binarized relevance judgments (0 and 1: not relevant, 2 and 3: relevant). Task 2 of the ARQMath is built on top of the same data as Task 1, but with a different goal in mind: Participants are expected to retrieve relevant formulas given a query formula in the context of its post. This task is related to the formula browsing task of NTCIR-12 <ref type="bibr" coords="3,452.38,568.88,16.25,4.94" target="#b25">[26]</ref>. ARQMath-3's new Task 3 presents an open-domain question/answering scenario instead of finding the most relevant answers for a given question. For each of the 100 topics of Task 1 in the 2022 test set the participants are asked to extract or generate a single answer. These answers contributed to the pool of answers which were judged for Task 1. Any knowledge source -except for the MathSE data from 2019 to today -was allowed as training data. The evaluation is carried out by evaluating the average relevance (AR) of the answers and the Precision at 1 (p@1) for each topic. Apart from the task definitions and the evaluation data, ARQMath provides data from the Mathematics StackExchange including question and answer posts from 2010 to 2018. In total, the collection contains 1M questions and 1.4M answers. Furthermore, users may use mathematical formulas to clarify their posts. These formulas written in L A T E X notation were extracted and parsed into Symbol Layout Trees and Operator Trees. Apart from this corpus of posts and formulas that are available for training and evaluating models, the organizers of ARQMath also released a test set of queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Transformer-based Models</head><p>The Transformer Architecture as introduced by <ref type="bibr" coords="4,309.42,257.25,12.99,4.94" target="#b7">[8]</ref> consists of Encoder and Decoder layers. Encoder models in Natural Language Processing typically apply several of these encoder layers on top of each other resulting in a model that reads a sequence of tokens and outputs contextualized embeddings for each token as well as for the entire input. These embeddings can then be further processed for classification tasks among others. In contrast, decoder models are designed to generate the next output token given a sequence of previous tokens (context tokens). Training both types of models usually includes two phases: A pre-training phase and a fine-tuning phase. While pre-training consists of training the model on relatively simple self-supervised tasks on a large amount of data, fine-tuning does not necessarily need large annotated data. In the following sections we will describe the pre-training of both model types, differences in the concrete model instances we applied and our fine-tuning for the respective task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Encoder Models</head><p>Encoder models are trained to capture the meaning of natural language by self-supervised pre-training tasks. The most important and widely applied task is the Masked Language Model. The model is presented with the embeddings 𝐸 𝑖 for each token 𝑖 from the input sentence:</p><formula xml:id="formula_0" coords="4,196.36,507.68,202.56,10.69">𝐶𝑈 1 𝑈 2 • • • 𝑈 𝑁 = BERT(𝐸 𝐶𝐿𝑆 𝐸 1 𝐸 2 • • • 𝐸 𝑁 )</formula><p>, where 𝐸 𝐶𝐿𝑆 and 𝐶 are the input and output embeddings of the ⟨𝐶𝐿𝑆⟩ token. A classifier is then applied to predict the original word given the input:</p><formula xml:id="formula_1" coords="4,196.09,568.35,203.09,10.68">𝑃 (𝑤 𝑗 |𝑆) = softmax(𝑈 𝑖 • 𝑊 𝑀 𝐿𝑀 + 𝑏 𝑀 𝐿𝑀 ) 𝑗 ,</formula><p>where 𝑤 𝑗 is the 𝑗-th word from the vocabulary. This determines the probability that the 𝑖-th input word was 𝑤 𝑗 given the input sentence 𝑆. The weight matrix 𝑊 𝑀 𝐿𝑀 and its bias 𝑏 𝑀 𝐿𝑀 are only used for this pre-training task and are not reused afterwards. RoBERTa uses only the MLM task, while BERT and ALBERT also employ a second pre-training task on the same input data, which is a sequence classification task applied on top of the contextualized embeddings of the ⟨𝐶𝐿𝑆⟩ token:</p><formula xml:id="formula_2" coords="5,188.35,101.25,218.57,10.69">𝑃 (𝑙𝑎𝑏𝑒𝑙 = 𝑖|𝑆) = softmax(𝐶 • 𝑊 𝑆𝑂𝑃 + 𝑏 𝑆𝑂𝑃 ) 𝑖 ,</formula><p>where the matrix 𝑊 𝑆𝑂𝑃 and the bias 𝑏 𝑆𝑂𝑃 are only used for pre-training and are not re-used otherwise later. In practice, this task is used to learn coherence between two input sequences given to the model. In this work, we pre-train using the Sentence Order Prediction (SOP), where 𝑙𝑎𝑏𝑒𝑙 = 1 denotes that the two input sequences are in correct order, while 𝑙𝑎𝑏𝑒𝑙 = 0 denotes that they were swapped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-Tuning</head><p>In order to predict whether an answer</p><formula xml:id="formula_3" coords="5,89.29,237.98,416.90,37.78">𝐴 = 𝐴 1 𝐴 2 • • • 𝐴 𝑀 is relevant to a question 𝑄 = 𝑄 1 𝑄 2 • • • 𝑄 𝑁 a classifier is trained on top of the pre-trained Transformer-Encoder model. The input string ⟨𝐶𝐿𝑆⟩𝑄 1 𝑄 2 • • • 𝑄 𝑁 ⟨𝑆𝐸𝑃 ⟩𝐴 1 𝐴 2 • • • 𝐴 𝑀 ,</formula><p>with ⟨𝐶𝐿𝑆⟩ being the classification token and ⟨𝑆𝐸𝑃 ⟩ the separation token, is presented to the model:</p><formula xml:id="formula_4" coords="5,193.73,305.72,207.83,10.69">𝐶𝑈 1 𝑈 2 • • • 𝑈 𝑁 = LM(𝐸 𝐶𝐿𝑆 𝐸 1 𝐸 2 • • • 𝐸 𝑁 +𝑀 ),</formula><p>where 𝐸 𝑖 and 𝐸 𝐶𝐿𝑆 are the input embeddings for each input token and the ⟨𝐶𝐿𝑆⟩ token, respectively, calculated as explained in the previous section. After the forward pass through the model, the output vector of the ⟨𝐶𝐿𝑆⟩ token 𝐶 is given into a classification layer:</p><formula xml:id="formula_5" coords="5,180.28,379.66,234.71,10.68">𝑃 (𝑙𝑎𝑏𝑒𝑙 = 𝑖|𝑄, 𝐴) = softmax(𝐶 • 𝑊 𝑀 𝐼𝑅 + 𝑏 𝑀 𝐼𝑅 ) 𝑖 ,</formula><p>where the label 1 stands for a matching or correct answer for the query and label 0 otherwise. During evaluation, the resulting probability of the classification layer for label 1 is the assigned a similarity score 𝑠 for the answer 𝐴 to a question 𝑄 and is then used to rank all answers in the corpus: 𝑠(𝑄, 𝐴) = 𝑝(𝑙𝑎𝑏𝑒𝑙 = 1|𝑄, 𝐴).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Decoder Models</head><p>Decoder models are trained on the casual language modeling objective, i.e., given some input tokens, generate the most probable next token. In other words, the objective is to maximize the probability over the corpus consisting of a sequence of 𝑛 tokens 𝐶 = {𝑡 0 , 𝑡 1 , ..., 𝑡 𝑛 }:</p><formula xml:id="formula_6" coords="5,226.77,537.44,141.73,33.71">𝑃 (𝐶) = 𝑛 ∏︁ 𝑖=0 𝑃 (𝑡 𝑖 |𝑡 𝑖-𝑘 , ..., 𝑡 𝑖-1 ),</formula><p>where the conditional probability 𝑃 (𝑠 𝑡 |𝑡 𝑖-𝑘 , ..., 𝑡 𝑖-1 ), ranging over a context window of size 𝑘, is estimated by the Decoder model. The input is embedded and given to the Transformer Decoder layers resulting in the last layer's output ℎ last , which is used to calculate the probability:</p><formula xml:id="formula_7" coords="5,231.94,634.52,131.40,14.19">𝑃 (𝑢) = softmax(ℎ last • 𝑊 𝑇 𝑒 ),</formula><p>with 𝑊 𝑇 𝑒 being the embedding matrix. Decoder Models such as GPT or GPT-2 are also fine-tuned in a supervised fashion to predict labels from an annotated corpus. Since we only use GPT-2 to generate tokens and not to predict labels, we will omit the details of this fine-tuning here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-Tuning</head><p>To generate answers given a question, we provide the model with the question tokens and prompt it to complete the text by filling in the answer. During training, the model is prompted using the following pattern:</p><formula xml:id="formula_8" coords="6,211.57,355.91,172.13,10.69">PROBLEM: 𝑄 1 𝑄 2 • • • 𝑄 𝑁 SOLUTION: ,</formula><p>where 𝑄 𝑖 are the question tokens. The model is then optimized to complete the prompt by generating the answer tokens 𝐴 𝑖 :</p><formula xml:id="formula_9" coords="6,181.65,416.92,231.98,10.69">PROBLEM: 𝑄 1 𝑄 2 • • • 𝑄 𝑁 SOLUTION: 𝐴 1 𝐴 2 • • • 𝐴 𝑀 .</formula><p>During evaluation, the model is presented with the same pattern to generate an answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Contribution to Task 1</head><p>Task 1 deals with retrieving the most relevant answers from the MathSE corpus given 100 questions that were not seen during training. For this task we pre-trained and fine-tuned several models using the base models BERT, ALBERT, and RoBERTa, applying different corpora for pre-training and fine-tuning on three different sets of question-answer pairs. An overview of our approach for Task 1 is depicted in Figure <ref type="figure" coords="6,289.60,555.89,3.70,4.94" target="#fig_0">1</ref>. In the following, we will first describe the data we used, then our experiments including hyper-parameter settings, and finally present our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-Training Data</head><p>Prior to pre-training, we applied the official tool provided by ARQMath to read the posts, wrapped formulas in $ and removed other HTML markup, yielding a list of paragraphs for each post. BERT and ALBERT models rely on data which is separated into sentences during pre-processing for the SOP task. We combined three different strategies: (1) split the text</p><formula xml:id="formula_10" coords="7,100.46,105.85,397.99,40.13">frac \\ {\\ exp ( x _ i ) exp ( x _ i ) } } {\\ s um _ j \\ \\frac { \\exp ( x _ i ) ( x _ i ) } } { \\sum _ j \\exp</formula><p>ALBERT-base Tokenizer ALBERT-base Tokenizer with additional Math Tokens into sentences, ( <ref type="formula" coords="7,164.57,225.27,3.94,4.94" target="#formula_11">2</ref>) split text into chunks of natural language and formulas and (3) split the mathematical equations on relation symbols (e.g., =, ∈) into parts. The SOP task is designed to work on sentences level granularity to facilitate the modeling of inter-sentence-coherency. Hence, ( <ref type="formula" coords="7,127.63,265.91,3.94,4.94">1</ref>) is usually used in various NLP tasks. At the same time, our goal was to increase the model's understanding of formulas. Therefore, strategy (2) splits a paragraph first into sentences. These sentences are then further split at a formula (with more than three L A T E X tokens to avoid splitting at e.g., definitions of symbols). In case the remaining text is too short (less than ten characters), it is concatenated to the formula before, separated by a $ sign. Strategy (3) only uses formula data without natural language. The three strategies will be denoted by MathSE (1), MathSE (2), and MathSE (3), respectively. Apart from the MathSE corpus provided by the ARQMath Lab, we also pre-processed the Auxiliary Mathematics Problems and Solutions (AMPS) corpus containing questions and answers relating to mathematical problem-solving <ref type="bibr" coords="7,324.90,387.86,11.58,4.94" target="#b5">[6]</ref>. Since the data was already split in chunks, we used these data sets as the base for the sentence order task of ALBERT and BERT. The data set contains two parts: the Khan data set consisting of 100,000 exercise questions and answers from the Khan Academy and the Mathematica data set containing 5 M similar questions that are generated using Mathematica Scripts. The questions from both data sets range from topics like simple geometry to multivariate calculus. Both questions and answers use L A T E X to convey mathematical notation. We used this data only for pre-training, but not for fine-tuning due to its structure. Tokenizing, creating the pre-training data for each task, i.e., masking tokens and assembling pairs of sentences, and further pre-processing was performed using Huggingface's libraries transformers and datasets <ref type="bibr" coords="7,215.59,523.35,16.56,4.94" target="#b26">[27,</ref><ref type="bibr" coords="7,236.98,523.35,12.42,4.94" target="#b27">28]</ref>. For our models, we used the released sentencepiece vocabulary, but added 501 additional tokens <ref type="foot" coords="7,288.68,534.26,3.71,3.61" target="#foot_2">3</ref> to the tokenizer to cover L A T E X <ref type="bibr" coords="7,435.78,536.90,16.41,4.94" target="#b28">[29]</ref>. The list of tokens was taken from the L A T E X parser by Approach0<ref type="foot" coords="7,330.39,547.81,3.71,3.61" target="#foot_3">4</ref> . An example of the impact of the new tokenizer on the expression exp(𝑥 𝑖 ) ∑︀ 𝑗 exp(𝑥 𝑗 ) can be seen in Figure <ref type="figure" coords="7,355.49,565.37,3.66,4.94" target="#fig_1">2</ref>. After we added the L A T E X tokens to the vocabulary, typical tokens like \\sum or \\frac did not get torn apart into multiple tokens, but remain together. Input sequences whose length after tokenization exceeded the maximum number of input tokens were truncated to the maximum length of 512 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-Tuning Data</head><p>In order to fine-tune our models, we paired each question with up to 𝑁 correct answers and the same number of incorrect answers. Up to 𝑁 correct answers were randomly chosen from the answers of the question. Each question in the corpus comes along with tags, i.e. categories indicating the topic of a question such as sequences-and-series or limits. As an incorrect answer for each question, we picked a random answer from one question sharing at least one tag with the original question by chance. This way, we chose up to 𝑁 incorrect answers independently from another. This procedure yields 1.9M examples for 𝑁 = 1 and 2.8M examples for 𝑁 = 10, of which 90% were used as training data for the fine-tuning task. We presented to the model the entire text of the questions and answers using the structure introduced in the previous section. In addition, we pre-trained an ALBERT Model on MathSE <ref type="bibr" coords="8,344.28,246.29,11.81,4.94">(1)</ref> and fine-tuned it on 𝑁 = 1. We then let this model predict 1,000 answers to the 2021 test set. We evaluated the answers against the publicly available test set from last year and paired each correct answer with a randomly selected incorrect answer from the model's results. These question-answer pairs were used as an additional fine-tuning set which we denote by Annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Data</head><p>To evaluate the trained models, we paired each question of the ARQMath-1 to 3 test sets with each of the answer posts from 2010 to 2018. The question-answer pairs are pre-processed in the same way as the fine-tuning data. Note, that we do not apply pre-filtering or a first-ranking stage as it would be usually done for this kind of cross-encoder design. Instead, we are ranking the entire set of answers. This is possible because we are using a GPU with a greater memory size compared to our submission to ARQMath-2. For the longest queries, ranking the entire set of answers takes around 3h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>In the previous sections, we have introduced several base models, pre-training, and fine-tuning data sets which lead to many combinations for MIR. A summary of our devised models can be found in Table <ref type="table" coords="8,172.91,507.94,3.81,4.94" target="#tab_0">1</ref>. Our submission includes five models which were fine-tuned using the 𝑁 = 10 fine-tuning data set. For these models, we added their official identifiers to the table. The other models are used as baselines and for comparison of our setup. The models Math_10 and Math_10_Add were first pre-trained on MathSE (1), then on MathSE (2), and finally, on MathSE (3). We refer to this pre-training as mathematical pre-training. Six other models did not incorporate these two additional data sets for pre-training but were only pre-trained using the first strategy MathSE (1). One model was trained only on the Khan part of the AMPS data set, while two models used a mix of samples from Khan and MathSE. Here, both corpora were combined into a single data set and shuffled. We experimented with the same approach on the entire AMPS corpus and MathSE. For fine-tuning, we denoted on which data set each model was trained. Two models where trained first on the 𝑁 = 10 data. After this training was completed, a second fine-tuning was conducted using Annotated. All twelve models were trained using eight A100 GPUs with 40 GB GPU memory each. For pre-training, a batch size of 16 samples per GPU was used. We pre-trained the models for 13 epochs using MathSE (1) and 9 epochs on MathSE (2). MathSE (3) added additional 20 epochs to the model. Fine-tuning on 𝑁 = 1 and 𝑁 = 10 used a batch size of 32 examples per device, and 200 warm-up steps with a learning rate of 2𝑒 -05 . Annotated used the same hyperparameters, but a batch size of 32 in total. Pre-training and fine-tuning were performed using Huggingface's library transformers <ref type="bibr" coords="9,182.25,377.63,16.25,4.94" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation</head><p>This section summarizes our results using the different setups. We start by presenting our overall results of the models submitted to the lab and then discuss the details of choosing the base model, the pre-training, and fine-tuning data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Overall Results</head><p>The results of our runs submitted to Task 1 of the ARQMath Lab 2022 are presented in Tables <ref type="table" coords="9,501.00,503.78,4.99,4.94">2</ref> and<ref type="table" coords="9,112.41,517.33,3.81,4.94">3</ref>. Regarding nDCG' and mAP', Math_10, our model using mathematical pre-training performs the best in all three years. The model which was fine-tuned using Annotated received the highest scores for p'@10, but its performance on the other two metrics degraded. Since it was fine-tuned on the ARQMath 2021 test set, the scores on this set are naturally much higher than the models which were not fine-tuned on this data. The other three models of our submission are on par even though they were trained on different data and with a different base architecture. Nevertheless, our models for the submission to ARQMath-3 outperform even the best models from ARQMath-2 in all three metrics. In comparison to other participants of ARQMath-3, our Math_10_Add received the highest p'@10 scores among all automatic runs. In the following, we will analyze different aspects of improvements of our submission. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Results of Task 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Base Model</head><p>We evaluated models trained on three base architectures: BERT, ALBERT, and RoBERTa. The results can be found in Table <ref type="table" coords="10,227.36,466.55,3.81,4.94" target="#tab_2">4</ref>. Even though ALBERT and RoBERTa are considered to be advancements over BERT, their performance on our downstream task is not necessarily higher. RoBERTa receives the highest scores for nDCG' and p'@10, while BERT scores highest using the metric mAP'. Overall, the improvements of the three architectures over each other are rather minimal. However, the training time should also be considered: Pre-training ALBERT on (1) MathSE took 24h, while BERT and RoBERTa needed on average 25% more time. To fine-tune each model, ALBERT was the fastest with 8h on the 𝑁 = 1 data set. The fine-tuning of BERT and RoBERTa on the same data set took 11h. Evaluation takes the same time on average for each of the three models because the data is processed by the same number of layers since ALBERT's layer sharing is only beneficial during training and BERT and RoBERTa share the same underlying architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Additional Pre-Training Data</head><p>Transformer-Encoder models are known to benefit from more pre-training data which is why we evaluate the ALBERT model on four different data set configurations whose results are presented in Table <ref type="table" coords="11,171.27,213.50,3.66,4.94">5</ref>. Interestingly, the model trained on a mixed data set consisting of data from the Khan Academy and the MathSE scores best, receiving slightly better scores on nDCG' and mAP'. For p'@10 the model trained only on MathSE outperforms the other models, indicating that it is able to place relevant documents better within the top 10 documents, while the first model ranked relevant documents better in the long run. The model trained only on data from Khan Academy scored worst in this evaluation demonstrating the shortcomings of out-of-domain data. A reason for this behavior could be that the questions from Khan are designed to serve as exercises. Therefore, each sentence is relevant for solving the question and does not contain any irrelevant information that could be included by question authors on MathSE (e.g. "Dear community, I have a question . . . "). After training on Khan data only, it could be harder for the model to deal with these irrelevant information in questions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Comparison of results for pre-training using different data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">Fine-Tuning Data</head><p>When comparing the amount of fine-tuning data needed for the answer retrieval task, we can see in Table <ref type="table" coords="11,146.34,564.45,5.17,4.94" target="#tab_4">6</ref> that more data is clearly beneficial. In both cases, for ALBERT and RoBERTa, we see an increase on all three metrics when fine-tuning on 𝑁 = 10 instead of 𝑁 = 1 is applied. With an additional training on Annotated, only p'@10 is increased, while the other two metrics deteriorate. This indicates that the model can differentiate better between relevant and non-relevant answers in the top 10, but fails to place other relevant documents in good positions afterwards.</p><p>We also report the scores on nDCG' for the three categories 'Both', 'Math', and 'Text' indicating which of these parts are most crucial for answering the question. For example, a question based in the category 'Text' would require to understand the written text of the question over the Comparison of results for fine-tuning using different data sets.</p><p>mathematical formulas. Models which were trained using 𝑁 = 10 data showed improvements in all three categories, but the 'Math' category benefited the most. An explanation for this observation could be that for 𝑁 = 1 the model saw fewer irrelevant examples that shared the same notation (same mathematical symbols) as the question, but which were used in a different and therefore irrelevant way for the question. In the 𝑁 = 10 data set, it was more probable that an irrelevant question still used the same mathematical symbol. Therefore, using this data set, the model needed to learn the semantics of the usage of the symbols, rather than their mere appearance. However, a similar observation should be possible for fine-tuning on Annotated, but by adding this fine-tuning, the model's performance on all three categories degrades.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Contribution to Task 3</head><p>Task 3 was first introduced to the ARQMath Lab in 2022 and has the goal of generating answers given a question instead of retrieving them from a corpus. The questions are identical to the ones for Task 1. All include at least one formula.</p><p>In the following, we will introduce our approach of generating answers using GPT-2 by finetuning it on two corpora. An overview of the approach is illustrated in Figure <ref type="figure" coords="12,443.58,464.82,3.81,4.94">3</ref>. We start by describing the data which we used, then our experimental setup and finally, present our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Data</head><p>For fine-tuning GPT-2, we use the same two data sets as for Task 1: the MathSE data set as provided by the ARQMath Lab and the AMPS data set consisting of questions-answer pairs from the Khan Academy and generated questions with step-by-step answers using Mathematica.</p><p>In total, we fine-tuned our models on 1,445,487 question-answer pairs from MathSE, where for each question a single answer was chosen by chance. In addition, the AMPS data set consists of 627,795 question-answer pairs. For pre-processing, we used the tokenizer provided by the authors of AMPS, which is based on the original GPT-2 tokenizer, but separated compounds of digits into single digits. We also experimented with the original GPT-2 tokenizer, but found the adapted one to perform better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experimental Setup</head><p>A summary of our experiments can be found in Table <ref type="table" coords="13,336.66,381.13,3.81,4.94" target="#tab_5">7</ref>. We experimented with fine-tuning the models on two data sets for a different number of epochs. We tested to train only on the MathSE data for three epochs, while another model was first fine-tuned on the AMPS data set for three epochs and afterwards for one epoch on MathSE. In addition, also smaller numbers of epochs were tested but did not yield better results. For training on AMPS, we sub-sampled the amount of training data for the Mathematica part to 0.5 and for Khan to 5 following the procedure of <ref type="bibr" coords="13,148.61,462.42,11.43,4.94" target="#b5">[6]</ref>.</p><p>For decoding, we varied the length penalty between 1 and 2 and applied beam search with a beam size of 5, 10, and 20. We also experimented with top-k sampling. Apart from these modifications, we followed the training and evaluation recommendations reported by <ref type="bibr" coords="13,492.22,503.07,11.58,4.94" target="#b5">[6]</ref>.</p><p>Because the length of the generated answers exceeded the allowed maximum length for the submission to the ARQMath Lab 2022 in several cases, we tried to force the model to generate shorter, but still relevant answers by adding the word 'HINT' to the beginning of the solution during decoding. The data set for the training was not altered. These combinations in total led to three experiments. The fourth one is a combined run which includes the shortest generated answer of each of the three runs. This run is denoted by 'shortest'.</p><p>For all experiments for Task 3, we adapted the code by <ref type="bibr" coords="13,338.15,597.91,12.96,4.94" target="#b5">[6]</ref> for our data set which is based on Huggingface Transformers <ref type="bibr" coords="13,211.90,611.46,16.25,4.94" target="#b26">[27]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results</head><p>Table <ref type="table" coords="14,115.26,232.59,4.97,4.94" target="#tab_6">8</ref> displays our efforts for Task 3 of ARQMath-3. Out best model was trained on the AMPS data and afterward on MathSE. During decoding, we use the word 'HINT' to force the model to generate shorter answers. Surprisingly, this run scored better in both metrics than the one that was trained in the same way but used a higher length penalty, sampling, and a beam size of 20.</p><p>The model which was trained on MathSE only ranks in second place for both metrics. The lowest scores received the run that consists of the shortest answers of our models for each topic. This indicates that shorter answers may be insufficient to convey enough relevant information in the post. Since an automatic evaluation of answer generation is challenging, we will not analyze the impact of different aspects of our submission but instead report in the following some results in the context of a qualitative evaluation. Below, we present two examples of questions from the test set with their generated answers by our primary submission. In general, the model is able to pick up the topic from the questions and generate meaningful, syntactically correct answers in most cases. However, whether the answers are relevant for solving the questions needs further evaluation. For example, the generated answer for Question 322 is simply a copy of a part of the question which does not seem to provide any additional information. This behavior of copied information can also be seen in other answers. Especially, when there was an edit of the author where they already provided a correct answer, the generated answer would only copy this solution without commenting. A similar issue can be seen in the generated solution for Question 340. The model only arranges the question differently in several steps but does not comment on the process. Whether and why this equation holds, can not directly be inferred from the model's answer. More examples of model outputs can be seen in the Appendix.</p><p>Post ID 322: Title: How do I calculate the sum of sum of triangular numbers? Question: As we know, triangular numbers are a sequence defined by 𝑛(𝑛+1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>. And it's first few terms are 1, 3, 6, 10, 15.... Now I want to calculate the sum of the sum of triangular numbers. Let's define</p><formula xml:id="formula_11" coords="14,263.28,641.12,67.51,24.43">𝑎 𝑛 = 𝑛(𝑛 + 1)<label>2</label></formula><formula xml:id="formula_12" coords="15,270.90,93.19,52.97,73.26">𝑏 𝑛 = 𝑛 ∑︁ 𝑥=1 𝑎 𝑥 𝑐 𝑛 = 𝑛 ∑︁ 𝑥=1 𝑏 𝑥</formula><p>And I want an explicit formula for 𝑐 𝑛 . After some research, I found the explicit formula for 𝑏 𝑛 = 𝑛(𝑛+1)(𝑛+2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head><p>. Seeing the patterns from 𝑎 𝑛 and 𝑏 𝑛 , I figured the explicit formula for 𝑐 𝑛 would be 𝑛(𝑛+1)(𝑛+2)(𝑛+3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>24</head><p>or 𝑛(𝑛+1)(𝑛+2)(𝑛+3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12</head><p>.</p><p>Then I tried to plug in those two potential equations, </p><formula xml:id="formula_13" coords="15,104.88,229.80,254.85,16.13">If 𝑛 = 1, 𝑐 𝑛 = 1, 𝑛(𝑛+1)(𝑛+2)(𝑛+3) 24 = 1, 𝑛(𝑛+1)(𝑛+2)(𝑛+3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>24</head><p>= 360 24 = 15 Overall, from the terms that I tried, the formula above seems to have worked. However, I cannot prove, or explain, why that is. Can someone prove (or disprove) my result above? Generated Solution: HINT: 𝑛(𝑛+1)(𝑛+2)(𝑛+3) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In our contribution to this lab, we demonstrated the use of Transformer-Encoders and Transformer-Decoders for two mathematical question-answering tasks. We pre-trained and fine-tuned twelve models based on BERT, RoBERTa, and ALBERT for the retrieval of answers given a mathematical question for the ARQMath Lab 2022. Our results show significant improvements compared to the 2021 edition of this lab, which can be attributed to better pre-training and the enlarged fine-tuning data. In addition, we analyzed several pre-training data sets and found that the Khan data set showed slight improvements in two out of three metrics. Finally, we improved our p'@10 results even further by additionally fine-tuning on annotated test data from ARQMath-2. For Task 3, a GPT-2 model was fine-tuned on two data sets. The results for this task are not yet published, but first analyses showed that the model is able to capture the topic of the question and can generate syntactically correct answers. The limited length of the generated answers is still an issue, which should be addressed in future research.</p><p>Generated Solution: HINT: If 𝛼 is an infinite ordinal, then there exists an injection from 𝛼 + to 𝛼.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated Solution:</head><p>If 𝑛 is not a perfect square, then √ 𝑛 is irrational. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,89.29,192.55,416.69,8.93;6,89.29,206.33,132.92,4.79"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our approach for Task 1 -Mathematical Answer Retrieval including examples for training and evaluation data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,180.09,225.84,8.93;7,325.48,177.08,26.03,6.12;7,318.47,184.54,8.37,4.53;7,326.84,185.35,31.68,7.97;7,359.72,181.92,147.80,4.79"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of tokenizing the L A T E X expression exp(𝑥𝑖) ∑︀ 𝑗 exp(𝑥𝑗 ) , important changes are highlighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="15,319.11,238.93,8.47,6.99;15,364.43,233.16,126.17,9.57;15,104.88,249.24,175.64,4.94;15,104.88,261.64,112.57,10.63;15,221.37,258.27,72.77,6.99;15,253.53,267.41,8.47,6.99;15,298.37,261.64,116.66,9.57;15,104.88,277.75,142.27,10.63;15,251.07,274.38,72.77,6.99"><head>12 = 2 . 24 = 5 .</head><label>122245</label><figDesc>Thus we can know for sure that the second equation is wrong. If 𝑛 = 2, 𝑐 𝑛 = 1 + 4 = 5, 𝑛(𝑛+1)(𝑛+2)(𝑛+3) Seems correct so far.If 𝑛 = 3, 𝑐 𝑛 = 1 + 4 + 10 = 15, 𝑛(𝑛+1)(𝑛+2)(𝑛+3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,88.99,91.72,378.01,185.90"><head>Table 1</head><label>1</label><figDesc>Model Configurations for Task 1.</figDesc><table coords="9,128.27,91.72,338.73,155.63"><row><cell cols="3">Official Identifier Base Model Pre-Training Data</cell><cell>Fine-Tuning Data</cell></row><row><cell></cell><cell>BERT</cell><cell>MathSE (1)</cell><cell>𝑁 = 1</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>MathSE (1)</cell><cell>𝑁 = 1</cell></row><row><cell>roberta_10</cell><cell>RoBERTa</cell><cell>MathSE (1)</cell><cell>𝑁 = 10</cell></row><row><cell></cell><cell>ALBERT</cell><cell>MathSE (1)</cell><cell>𝑁 = 1</cell></row><row><cell>base_10</cell><cell>ALBERT</cell><cell>MathSE (1)</cell><cell>𝑁 = 10</cell></row><row><cell></cell><cell>ALBERT</cell><cell>MathSE (1)</cell><cell>𝑁 = 10 + Annotated</cell></row><row><cell>math_10</cell><cell>ALBERT</cell><cell>MathSE (1) -(3)</cell><cell>𝑁 = 10</cell></row><row><cell>math_10_add</cell><cell>ALBERT</cell><cell>MathSE (1) -(3)</cell><cell>𝑁 = 10 + Annotated</cell></row><row><cell></cell><cell>ALBERT</cell><cell>Khan</cell><cell>𝑁 = 1</cell></row><row><cell></cell><cell>ALBERT</cell><cell>Khan + MathSE mixed</cell><cell>𝑁 = 1</cell></row><row><cell>Khan_SE_10</cell><cell>ALBERT</cell><cell>Khan + MathSE mixed</cell><cell>𝑁 = 10</cell></row><row><cell></cell><cell>ALBERT</cell><cell cols="2">AMPS + MathSE mixed 𝑁 = 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,88.99,91.72,289.49,90.26"><head>Table 4</head><label>4</label><figDesc>Comparison of results of BERT, ALBERT and RoBERTa as base models.</figDesc><table coords="11,218.09,91.72,159.09,60.40"><row><cell></cell><cell cols="2">ARQMath Lab 2020</cell></row><row><cell></cell><cell cols="2">nDCG' mAP'</cell><cell>p'@10</cell></row><row><cell>BERT</cell><cell>0.4068</cell><cell cols="2">0.2411 0.3560</cell></row><row><cell>ALBERT</cell><cell>0.4122</cell><cell cols="2">0.2335 0.3587</cell></row><row><cell cols="4">RoBERTa 0.4157 0.2328 0.3676</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,88.99,91.72,401.12,110.00"><head>Table 6</head><label>6</label><figDesc></figDesc><table coords="12,105.17,91.72,384.94,89.79"><row><cell></cell><cell></cell><cell cols="2">ARQMath Lab 2020</cell><cell></cell></row><row><cell cols="2">Base Model Fine-Tuning Data</cell><cell cols="2">nDCG' mAP'</cell><cell cols="2">p'@10 Both</cell><cell>Math</cell><cell>Text</cell></row><row><cell>ALBERT</cell><cell>𝑁 = 1</cell><cell>0.4122</cell><cell cols="2">0.2335 0.3587</cell><cell>0.4202 0.4033 0.4140</cell></row><row><cell>ALBERT</cell><cell>𝑁 = 10</cell><cell cols="3">0.4377 0.2519 0.3693</cell><cell>0.4391 0.4437 0.4184</cell></row><row><cell>ALBERT</cell><cell cols="2">𝑁 = 10+Annotated 0.3988</cell><cell cols="3">0.2435 0.3853 0.3837 0.4220 0.3789</cell></row><row><cell>RoBERTa</cell><cell>𝑁 = 1</cell><cell>0.4157</cell><cell cols="2">0.2328 0.3676</cell><cell>0.4175 0.4200 0.3999</cell></row><row><cell>RoBERTa</cell><cell>𝑁 = 10</cell><cell cols="4">0.4376 0.2543 0.3720 0.4293 0.4511 0.4246</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,89.29,574.85,416.70,94.63"><head>Table 7</head><label>7</label><figDesc>Model Configurations for Task 3, ep. denotes the number of epochs.</figDesc><table coords="12,89.29,574.85,416.70,94.63"><row><cell>Base Model</cell><cell>How are you? He replied that …</cell></row><row><cell>Fine-Tuning</cell><cell>Problem: Find $x$ such Solution: …</cell></row><row><cell>Evaluation</cell><cell>Problem: Greatest lower Solution: …</cell></row><row><cell cols="2">Figure 3: Overview of our approach for Task 3 -Mathematical Answer Generation including examples</cell></row><row><cell>for fine-tuning and evaluation data.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="14,88.99,91.72,317.03,90.26"><head>Table 8</head><label>8</label><figDesc>Results of Task 3.</figDesc><table coords="14,189.25,91.72,216.77,58.09"><row><cell>Official Identifier</cell><cell>AR</cell><cell>p@1</cell></row><row><cell>amps3_se1_hints</cell><cell cols="2">0.325 0.078</cell></row><row><cell>se3_len_pen_10</cell><cell cols="2">0.244 0.064</cell></row><row><cell cols="3">amps3_se1_len_pen_20_sample_hint 0.231 0.051</cell></row><row><cell>shortest</cell><cell cols="2">0.205 0.026</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="15,104.36,317.48,386.04,227.09"><head></head><label></label><figDesc>We must prove that this is a Cauchy sequence. I know that an Cauchy sequence follows the definition: given 𝜖 &gt; 0, exists 𝑛 0 &gt; 0 , such that 𝑚, 𝑛 &gt; 𝑛 𝑜 ⇒ |𝑥 𝑚 -𝑥 𝑛 | &lt; 𝜖 But I don't know how to use both informations to prove the exercise. If someone please may help me, I'd be very thankful. Generated Solution: |𝑥 𝑛+1 -𝑥 𝑛 | =</figDesc><table coords="15,104.88,317.48,310.90,227.09"><row><cell>24</cell><cell></cell><cell>= 360 24 = 15</cell><cell></cell></row><row><cell>Post ID 340:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>⃒ ⃒ ⃒</cell><cell>𝑥 𝑛+1 -𝑥𝑛 3 𝑛+1</cell><cell>⃒ ⃒ ⃒ = |𝑥 𝑛+1 -𝑥𝑛| 3 𝑛+1</cell><cell>&lt; 1 3 𝑛+1 .</cell></row></table><note coords="15,104.52,380.93,279.16,10.63;15,383.68,378.98,5.14,6.99;15,392.75,380.93,97.64,10.63;15,104.88,397.01,79.82,4.94;15,104.88,408.03,385.51,10.63;15,104.88,424.11,20.18,4.94;15,255.13,437.11,65.47,10.63;15,327.65,429.73,5.45,9.57;15,324.83,443.45,10.59,10.71;15,337.12,437.11,3.03,9.57"><p>Title: I have the following problem: Let |𝑥 𝑛+1 -𝑥 𝑛 | &lt; 1/3 𝑛 . Show that (𝑥 𝑛 ) is a Cauchy sequence. Question: We have that (𝑥 𝑛 ) is a sequence of real numbers. And the relation on the title: |𝑥 𝑛+1 -𝑥 𝑛 | &lt; 1 3 𝑛 .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="18,89.29,558.29,416.69,124.85"><head></head><label></label><figDesc>Solution: HINT: If 𝑓 is Riemann-integrable, then ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡-</figDesc><table coords="18,89.29,558.29,416.69,124.85"><row><cell>∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 +</cell><cell>∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -</cell><cell>∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 =</cell><cell>∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 +</cell><cell>∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -</cell></row><row><cell>∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 +</cell><cell cols="4">∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 ∫︀ 𝑥 ∫︀ 𝑥 ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 = ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 + ∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 -∫︀ 𝑥 𝑎 𝑓 (𝑡) 𝑑𝑡 =</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,673.72,190.01,4.06"><p>https://huggingface.co/AnReu/albert-for-arqmath-3</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,108.93,673.71,117.99,4.06"><p>https://math.stackexchange.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="7,108.93,651.78,397.64,4.06;7,89.29,662.74,127.45,4.06"><p>Our list of additional tokens can be found here: https://github.com/AnReu/ALBERT-for-Math-AR/blob/main/ untrained_models/latex_tokens.txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="7,108.93,673.70,308.57,4.06"><p>https://github.com/approach0/search-engine/blob/master/tex-parser/lexer.template.l</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">DFG</rs> under <rs type="programName">Germany's Excellence Strategy</rs>, Grant No. <rs type="grantNumber">EXC-2068-390729961</rs>, <rs type="funder">Cluster of Excellence "Physics of Life" of TU Dresden</rs>. Furthermore, the authors are grateful for the GWK support for funding this project by providing computing time through the <rs type="funder">Center for Information Services and HPC (ZIH)</rs> <rs type="institution">at TU Dresden</rs>. We would also like to thank the reviewers for their helpful comments and recommendations.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SMNqgZP">
					<idno type="grant-number">EXC-2068-390729961</idno>
					<orgName type="program" subtype="full">Germany&apos;s Excellence Strategy</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="16,112.66,342.97,393.33,4.94;16,112.66,356.52,363.59,4.94" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="16,353.43,342.97,152.55,4.94;16,112.66,356.52,181.08,4.94">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,370.07,393.53,4.94;16,112.66,383.62,393.33,4.94;16,112.33,397.17,29.19,4.94" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<title level="m" coord="16,408.01,370.07,98.18,4.94;16,112.66,383.62,237.41,4.94">Albert: A lite bert for self-supervised learning of language representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,410.72,393.33,4.94;16,112.66,424.27,393.33,4.94;16,112.66,437.82,116.14,4.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,295.51,410.72,210.47,4.94;16,112.66,424.27,71.56,4.94">Modeling mathematical notation semantics in academic papers</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Head</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,207.39,424.27,298.60,4.94;16,112.66,437.82,18.15,4.94">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3102" to="3115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,451.37,393.33,4.94;16,112.66,464.92,272.27,4.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,256.44,451.37,113.43,4.94">Tu_dbs in the arqmath lab</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Reusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lehner</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-07.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="16,432.40,451.37,73.59,4.94;16,112.66,464.92,51.81,4.94">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,478.46,393.33,4.94;16,112.66,492.01,349.23,4.94" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="16,248.80,478.46,257.18,4.94;16,112.66,492.01,167.53,4.94">Evaluating token-level and passage-level dense retrieval models for math information retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11163</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,505.56,395.17,4.94;16,112.66,519.11,393.33,4.94;16,112.66,532.66,107.17,4.94" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03874</idno>
		<title level="m" coord="16,143.47,519.11,288.94,4.94">Measuring mathematical problem solving with the math dataset</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,546.21,393.33,4.94;16,112.66,559.76,253.81,4.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,412.10,546.21,93.89,4.94;16,112.66,559.76,141.16,4.94">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,262.00,559.76,56.95,4.94">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,573.31,395.17,4.94;16,112.66,586.86,393.32,4.94;16,112.33,600.41,78.48,4.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,484.04,573.31,23.79,4.94;16,112.66,586.86,142.26,4.94">Polosukhin, Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,263.51,586.86,229.78,4.94">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,613.96,394.53,4.94;16,112.30,627.51,393.68,4.94;16,112.66,641.05,107.17,4.94" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="16,173.53,627.51,256.77,4.94">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,654.60,394.62,4.94;16,112.66,668.15,393.33,4.94;17,112.66,90.23,395.17,4.94;17,112.66,103.78,132.19,4.94" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,239.01,654.60,248.29,4.94">Scibert: A pretrained language model for scientific text</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,112.66,668.15,393.33,4.94;17,112.66,90.23,395.17,4.94;17,112.66,103.78,33.90,4.94">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3606" to="3611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,117.33,395.00,4.94;17,112.66,130.88,379.73,4.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,170.40,130.88,186.98,4.94">Publicly available clinical bert embeddings</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Redmond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,366.39,130.88,58.19,4.94">NAACL HLT</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,144.43,393.33,4.94;17,112.66,157.97,272.20,4.94" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05342</idno>
		<title level="m" coord="17,276.78,144.43,229.21,4.94;17,112.66,157.97,89.80,4.94">Clinicalbert: Modeling clinical notes and predicting hospital readmission</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,112.66,171.52,394.53,4.94;17,112.66,185.07,393.33,4.94;17,112.66,198.62,107.17,4.94" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="17,112.66,185.07,319.87,4.94">Codebert: A pre-trained model for programming and natural languages</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08155</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,112.66,212.17,393.33,4.94;17,112.66,225.72,394.53,4.94;17,112.66,239.27,90.72,4.94" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="17,342.15,212.17,163.84,4.94;17,112.66,225.72,117.15,4.94">Learning and evaluating contextual embedding of source code</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Maniatis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,255.79,225.72,213.13,4.94">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5110" to="5121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,252.82,393.33,4.94;17,112.66,266.37,394.03,4.94;17,112.66,279.92,119.97,4.94" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf" />
		<title level="m" coord="17,349.67,252.82,156.32,4.94;17,112.66,266.37,119.27,4.94">Improving language understanding by generative pre-training</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,293.47,394.53,4.94;17,112.66,307.02,393.32,4.94;17,112.66,320.56,266.90,4.94" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="17,274.48,307.02,169.72,4.94">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,452.29,307.02,53.69,4.94;17,112.66,320.56,172.82,4.94">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,334.11,393.33,4.94;17,112.66,347.66,244.72,4.94" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.00377</idno>
		<title level="m" coord="17,258.59,334.11,247.40,4.94;17,112.66,347.66,62.22,4.94">Mathbert: A pre-trained model for mathematical formula understanding</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,112.66,361.21,393.33,4.94;17,112.66,374.76,394.62,4.94;17,112.31,388.31,172.79,4.94" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="17,248.08,361.21,257.91,4.94;17,112.66,374.76,77.35,4.94">Psu at clef-2020 arqmath track: Unsupervised re-ranking using pretraining</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_121.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="17,216.74,374.76,133.62,4.94">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<pubPlace>Thessaloniki, Greece</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,401.86,393.32,4.94;17,112.66,415.41,376.04,4.94" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="17,308.30,401.86,101.72,4.94">Three is better than one</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Novotnỳ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Štefánik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lupták</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_235.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="17,432.32,401.86,73.66,4.94;17,112.66,415.41,51.81,4.94">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,428.96,395.17,4.94;17,112.66,442.51,328.72,4.94" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="17,247.84,428.96,259.99,4.94;17,112.66,442.51,106.47,4.94">Ranked list fusion and re-ranking with pre-trained transformers for arqmath lab</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-08.pdf" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,456.06,393.32,4.94;17,112.66,469.61,353.65,4.94" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="17,411.22,456.06,94.76,4.94;17,112.66,469.61,131.40,4.94">Ensembling ten math information retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Novotnỳ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Štefánik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lupták</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Geletka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zelina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-06.pdf" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,483.16,394.53,4.94;17,112.66,496.70,239.64,4.94" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="17,288.61,483.16,214.68,4.94">Bert-based embedding model for formula retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dadure</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-03.pdf" />
	</analytic>
	<monogr>
		<title level="j" coord="17,112.66,496.70,21.05,4.94">CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,510.25,395.17,4.94;17,112.66,523.80,394.04,4.94;17,112.30,537.35,102.63,4.94" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-04.pdf" />
		<title level="m" coord="17,276.55,510.25,231.28,4.94;17,112.66,523.80,276.48,4.94">Dprl systems in the clef 2021 arqmath lab: Sentencebert for answer retrieval, learning-to-rank for formula retrieval</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,550.90,393.60,4.94;17,112.66,564.45,146.44,4.94" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="17,203.20,550.90,270.79,4.94">Generative language modeling for automated theorem proving</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Polu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03393</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,112.66,578.00,395.17,4.94;17,112.66,591.55,393.33,4.94;17,112.30,605.10,393.95,4.94;17,112.66,618.65,261.72,4.94" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="17,398.52,578.00,109.32,4.94;17,112.66,591.55,5.17,4.94;17,155.30,591.55,350.69,4.94;17,112.30,605.10,34.53,4.94">Third CLEF lab on Answer Retrieval for Questions on Math (Working Notes Version)</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Novotný</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,401.82,605.10,104.42,4.94;17,112.66,618.65,231.02,4.94">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of ARQMath-3</note>
</biblStruct>

<biblStruct coords="17,112.66,632.20,393.61,4.94;17,112.66,645.75,360.07,4.94" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="17,413.82,632.20,92.45,4.94;17,112.66,645.75,36.27,4.94">Ntcir-12 mathir task overview</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Topic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<ptr target="https://www.cs.rit.edu/~rlaz/files/ntcir12-mathir.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="17,176.55,645.75,27.98,4.94">NTCIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,659.29,394.53,4.94;17,112.66,672.84,394.53,4.94;18,112.66,90.23,395.17,4.94;18,112.66,103.78,393.33,4.94;18,112.66,117.33,394.53,4.94;18,112.66,130.88,385.60,4.94" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="18,315.63,90.23,192.20,4.94;18,112.66,103.78,72.82,4.94">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m" coord="18,207.25,103.78,298.74,4.94;18,112.66,117.33,390.37,4.94">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,144.43,394.53,4.94;18,112.66,157.97,394.53,4.94;18,112.33,171.52,394.86,4.94;18,112.66,185.07,394.53,4.94;18,112.66,198.62,394.62,4.94;18,112.66,212.17,394.61,4.94;18,112.66,225.72,395.01,4.94;18,112.66,239.27,229.14,4.94" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="18,179.62,198.62,303.37,4.94">Datasets: A Community Library for Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Villanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Del Moral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Šaško</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Brandeis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Le</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Canwen</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Patry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Raw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lesage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Carrigan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Matussière</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Werra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bekman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Delangue</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.emnlp-demo.21" />
	</analytic>
	<monogr>
		<title level="m" coord="18,112.66,212.17,394.61,4.94;18,112.66,225.72,105.99,4.94">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,252.82,393.33,4.94;18,112.66,266.37,393.33,4.94;18,112.33,279.92,29.19,4.94" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="18,225.16,252.82,280.83,4.94;18,112.66,266.37,237.64,4.94">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="18,89.29,318.79,172.21,12.85;18,89.29,345.13,388.31,10.63" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="18,110.05,318.79,151.45,12.85;18,89.29,345.13,326.16,10.63">Sample Results -Task 3 Generated Solution: HINT: If 𝑝 ≡ 2(mod 3) and 𝑎 ∈ F 𝑝 , then gcd</title>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,487.78,345.13,18.20,9.57;18,89.29,358.68,416.69,10.63;18,89.29,372.23,73.47,9.57" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="18,499.23,347.66,6.75,4.94;18,89.29,358.68,350.43,10.63">If 𝑝 ≡ 2(mod 3) and 𝑎 ∈ F 𝑝 , then gcd(3, 𝑝 -1) = 1. If 𝑝 ≡ 2(mod 3) and</title>
		<imprint/>
	</monogr>
	<note>𝑎 ∈ F 𝑝 , then gcd(3, 𝑝 -1) =</note>
</biblStruct>

<biblStruct coords="18,171.15,372.23,252.30,10.63" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="18,180.17,372.23,131.09,10.63">If 𝑝 ≡ 2(mod 3) and 𝑎 ∈ F 𝑝</title>
		<imprint/>
	</monogr>
	<note>, then gcd(3, 𝑝 -1) = 1</note>
</biblStruct>

<biblStruct coords="18,423.45,372.23,83.82,9.57;18,89.29,385.78,406.22,10.63" xml:id="b32">
	<monogr>
		<title level="m" type="main" coord="18,432.47,372.23,74.79,9.57;18,89.29,385.78,267.16,10.63">If 𝑝 ≡ 2(mod 3) and 𝑎 ∈ F 𝑝 , then gcd(3, 𝑝 -1) = 1. If 𝑝 ≡ 2(mod 3) and</title>
		<imprint/>
	</monogr>
	<note>𝑎 ∈ F 𝑝 , then gcd(3, 𝑝 -1) =</note>
</biblStruct>

<biblStruct coords="18,503.71,385.78,3.95,9.57;18,89.29,399.33,416.70,10.63;18,89.29,412.88,73.47,9.57" xml:id="b33">
	<monogr>
		<title level="m" type="main" coord="18,89.29,399.33,352.37,10.63">If 𝑝 ≡ 2(mod 3) and 𝑎 ∈ F 𝑝 , then gcd(3, 𝑝 -1) = 1. If 𝑝 ≡ 2(mod 3) and</title>
		<imprint/>
	</monogr>
	<note>𝑎 ∈ F 𝑝 , then gcd(3, 𝑝 -1) =</note>
</biblStruct>

<biblStruct coords="18,171.15,412.88,252.30,10.63" xml:id="b34">
	<monogr>
		<title level="m" type="main" coord="18,180.17,412.88,131.09,10.63">If 𝑝 ≡ 2(mod 3) and 𝑎 ∈ F 𝑝</title>
		<imprint/>
	</monogr>
	<note>, then gcd(3, 𝑝 -1) = 1</note>
</biblStruct>

<biblStruct coords="18,423.45,412.88,83.82,9.57;18,89.29,426.43,406.22,10.63" xml:id="b35">
	<monogr>
		<title level="m" type="main" coord="18,432.47,412.88,74.79,9.57;18,89.29,426.43,267.16,10.63">If 𝑝 ≡ 2(mod 3) and 𝑎 ∈ F 𝑝 , then gcd(3, 𝑝 -1) = 1. If 𝑝 ≡ 2(mod 3) and</title>
		<imprint/>
	</monogr>
	<note>𝑎 ∈ F 𝑝 , then gcd(3, 𝑝 -1) =</note>
</biblStruct>

<biblStruct coords="18,503.71,426.43,3.95,9.57;18,89.29,439.98,416.70,10.63;18,89.29,453.53,71.28,9.57" xml:id="b36">
	<monogr>
		<title level="m" type="main" coord="18,89.29,439.98,352.37,10.63">If 𝑝 ≡ 2(mod 3) and 𝑎 ∈ F 𝑝 , then gcd(3, 𝑝 -1) = 1. If 𝑝 ≡ 2(mod 3) and</title>
		<imprint/>
	</monogr>
	<note>𝑎 ∈ F 𝑝 , then gcd(3, 𝑝 -1) =</note>
</biblStruct>

<biblStruct coords="18,167.74,453.53,239.50,10.63" xml:id="b37">
	<monogr>
		<title level="m" type="main" coord="18,175.13,453.53,124.57,10.63">If 𝑝 ≡ 2(mod 3) and 𝑎 ∈ F 𝑝</title>
		<imprint/>
	</monogr>
	<note>, then gcd(3, 𝑝 -1) = 1</note>
</biblStruct>

<biblStruct coords="18,407.24,453.53,98.75,9.57;18,89.29,467.08,191.20,10.63" xml:id="b38">
	<monogr>
		<title level="m" type="main" coord="18,414.63,453.53,91.36,9.57;18,89.29,467.08,82.73,10.63">If 𝑝 ≡ 2(mod 3) and 𝑎 ∈ F 𝑝 , then gcd(3</title>
		<imprint/>
	</monogr>
	<note>, 𝑝 -1) = 1. If 𝑝 ≡ 2(m</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
