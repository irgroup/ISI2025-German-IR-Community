<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,369.48,15.42;1,89.29,106.66,363.26,15.42">DPRL Systems in the CLEF 2022 ARQMath Lab: Introducing MathAMR for Math-Aware Search</title>
				<funder ref="#_RaAtgbY">
					<orgName type="full">National Science Foundation (USA)</orgName>
				</funder>
				<funder ref="#_98ETbQK">
					<orgName type="full">Alfred P. Sloan Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,138.54,90.05,5.43"><forename type="first">Behrooz</forename><surname>Mansouri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,191.98,138.54,83.20,5.43"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.17,138.54,78.09,5.43"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,369.48,15.42;1,89.29,106.66,363.26,15.42">DPRL Systems in the CLEF 2022 ARQMath Lab: Introducing MathAMR for Math-Aware Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">04D87CEEE0620A1FBC2A42CB27C2A73C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Community Question Answering (CQA)</term>
					<term>Mathematical Information Retrieval (MIR)</term>
					<term>Math-aware search</term>
					<term>Math formula search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There are two main tasks defined for ARQMath: (1) Question Answering, and (2) Formula Retrieval, along with a pilot task (3) Open Domain Question Answering. For Task 1, five systems were submitted using raw text with formulas in LaTeX and/or linearized MathAMR trees. MathAMR provides a unified hierarchical representation for text and formulas in sentences, based on the Abstract Meaning Representation (AMR) developed for Natural Language Processing. For Task 2, five runs were submitted: three of them using isolated formula retrieval techniques applying embeddings, tree edit distance, and learning to rank, and two using MathAMRs to perform contextual formula search, with BERT embeddings used for ranking. Our model with tree-edit distance ranking achieved the highest automatic effectiveness. Finally, for Task 3, four runs were submitted, which included the Top-1 results for two Task 1 runs (one using MathAMR, the other SVM-Rank with raw text and metadata features), each with one of two extractive summarizers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ARQMath-3 lab <ref type="bibr" coords="1,181.03,426.65,12.79,4.95" target="#b0">[1]</ref> at CLEF has three tasks. Answer retrieval (Task 1) and formula search (Task 2) are the tasks performed in ARQMath-1 <ref type="bibr" coords="1,307.54,440.20,12.99,4.95" target="#b1">[2]</ref> and -2 <ref type="bibr" coords="1,354.59,440.20,11.59,4.95" target="#b2">[3]</ref>. The ARQMath test collection contains Math Stack Exchange (MathSE) 1 question and answer posts. In the answer retrieval task, the goal is to return a ranked list of relevant answers for new math questions. These questions are taken from posts made in 2021 on MathSE. The questions are not included in the collection (which has only posts from 2010 to 2018). In the formula search task, a formula is chosen from each question in Task 1 as the formula query. The goal in Task 2 is to find relevant formulas from both question and answer posts in the collection, with relevance defined by the likelihood of finding materials associated with a candidate formula that fully or partially answers the question that a formula query is taken from. The formula-specific context is used in making relevance determinations for candidate formulas (e.g., variable and constant types, and operation definitions), so that formula semantics are taken into account. This year a new pilot Open-Domain Question Answering task was also introduced, where for the same questions as in the Answer Retrieval task (Task 1), the participants were asked to extract or generate answers using data from any source.</p><p>The Document and Pattern Recognition Lab (DPRL) from the Rochester Institute of Technology (RIT, USA) participated in all three tasks. For Task 1, we have two categories of approaches. In the first approach, we search for relevant answers using Sentence-BERT <ref type="bibr" coords="2,425.17,144.41,12.84,4.95" target="#b3">[4]</ref> embeddings of raw text that includes the L A T E X representation for formulas given in the MathSE posts. In the second approach, we use a unified tree representation for text and formulas for search. For this, we consider the Abstract Meaning Representation (AMR) <ref type="bibr" coords="2,351.37,185.06,13.00,4.95" target="#b4">[5]</ref> for text, representing formulas by identifiers as placeholders, and then integrating the Operator Tree (OPT) representation of formulas into our AMR trees, forming MathAMR. The MathAMR representations are then linearized as a sequence, and Sentence-BERT is used for retrieval. We trained Sentence-BERT on pairs of (query, candidate) formulas with known relevance, and ranked (query, candidate) pairs with unknown relevance to perform the search.</p><p>Our runs in Task 1 are motivated by a common user behavior on community question answering websites such as MathSE. When there is a new question posted, the moderators can mark the question as duplicate if similar question(s) exist. We would expect that good answers to a similar question are likely to be relevant to a newly posted question. Our goal is use this strategy and to make this process automatic. First, we aim to find similar questions for a given topic in Task 1, and then rank the answers given to those similar questions.</p><p>For Task 2, we submitted two types of approaches. For the first type, we consider only isolated formulas during search: the context in which formulas occur are ignored for both query and candidates, with similarity determined by comparing formulas directly. In the second type of approach, we use contextual formula search. In contextual formula search, not only is formula similarity important, but also the context in which formulas appear. As in Task 1, we make use of AMR for text and then integrate the OPT into the AMR.</p><p>Finally, for Task 3, we select the first answers retrieved by two of our Task 1 runs, and then apply two extractive summarization models to each. These two summarizers select at most 3 sentences from each answer post returned by a Task 1 system.</p><p>In this paper, we first introduce the MathAMR representation, as it is used in our runs for all the tasks. We then explain our approaches for formula search, answer retrieval, and open-domain question answering tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MathAMR</head><p>Formula Representations: SLTs and OPTs. Previously, math-aware search systems primarily used two representation types for formulas: Symbol Layout Trees (SLTs) capture the appearance of the formula, while Operator Trees (OPTs) capture formula syntax <ref type="bibr" coords="2,408.04,582.42,11.58,4.95" target="#b5">[6]</ref>. In an SLT, nodes represent formula elements (including variable, operator, number, etc.), whereas the edge labels capture their spatial relationships. In an OPT, nodes are again the formula elements, but the the edge labels indicate the order of the operands. For commutative operators such as '=', for which the order is not important, the edge labels are identical. Figure <ref type="figure" coords="2,393.74,636.61,4.97,4.95" target="#fig_0">1</ref> shows the SLT and OPT representations for formula ğ‘¥ ğ‘› + ğ‘¦ ğ‘› + ğ‘§ ğ‘› .</p><p>In both representations, formula symbol types are given in the nodes (e.g., V! indicates that the type is a variable). In our SLT represenation, there is no explicit node type for operators. SLT edge labels show the spatial relationship between symbols. For example, variable ğ‘› is located above variable ğ‘¥, and operator + is located next to variable ğ‘¥. As with a SLT, in an OPT the nodes represent the formula symbols. The difference is that in OPT representation, operators have an explicit node type. Unordered and ordered operators are shown with 'U!' and 'O!'. For further details refer to Davila et al. <ref type="bibr" coords="3,305.23,376.04,12.84,4.95" target="#b6">[7]</ref> and Mansouri et al. <ref type="bibr" coords="3,409.78,376.04,11.43,4.95" target="#b7">[8]</ref>.</p><p>Operator trees capture the operation syntax in a formula. The edge labels provide the argument order for operands. By looking at the operator tree, one can see what operator is being applied on what operands. This is very similar to the representation of text with Abstract Meaning Representations (AMR), which can roughly be understood as representing "who is doing what to whom". 2  Abstract Meaning Representation (AMR). AMRs are rooted Directed Acyclic Graphs (DAGs). AMR nodes represent two core concepts in a sentence: words (typically adjectives or stemmed nouns/adverbs), or frames extracted from Propbank <ref type="bibr" coords="3,383.36,484.43,11.59,4.95" target="#b8">[9]</ref>. 3 For example in Figure <ref type="figure" coords="3,89.29,497.98,3.80,4.95" target="#fig_3">3</ref>, nodes such as 'you' and 'thing' are English words, while 'find-01' and 'solve-01' represent Propbank framesets. Labeled edges between a parent node and a child node indicate a semantic relationship between them. AMRs are commonly used in summarization <ref type="bibr" coords="3,427.17,525.08,16.56,4.95" target="#b9">[10,</ref><ref type="bibr" coords="3,447.02,525.08,12.42,4.95" target="#b10">11]</ref>, question answering <ref type="bibr" coords="3,138.73,538.63,16.46,4.95" target="#b11">[12,</ref><ref type="bibr" coords="3,157.92,538.63,12.35,4.95" target="#b12">13]</ref>, and information extraction <ref type="bibr" coords="3,300.65,538.63,16.46,4.95" target="#b13">[14,</ref><ref type="bibr" coords="3,319.84,538.63,12.35,4.95" target="#b14">15]</ref>. For example, Liu et al <ref type="bibr" coords="3,438.59,538.63,16.29,4.95" target="#b9">[10]</ref>, generated AMRs for sentences in a document, and then merged them by collapsing named and date entities. Next, a summary sub-graph was generated using integer linear programming, and finally summary text was generated from that sub-graph using JARM <ref type="bibr" coords="3,400.95,579.27,16.25,4.95" target="#b15">[16]</ref>. Figure <ref type="figure" coords="3,131.99,592.82,5.17,4.95" target="#fig_1">2</ref> shows an example summary of two sentences in their AMR representations <ref type="bibr" coords="3,487.15,592.82,16.41,4.95" target="#b9">[10]</ref>. There are two sentences:</p><p>(a) I saw Joe's dog, which was running in the garden.   (b) The dog was chasing a cat.</p><formula xml:id="formula_0" coords="4,225.39,298.05,116.43,49.68">O!SUP O!SUP U!Plus O!SUP V!x V!n V!y V!n V!z V!n 0 0 0 0 0 0 1<label>1</label></formula><p>Figure <ref type="figure" coords="4,120.36,510.24,9.74,4.95" target="#fig_1">2c</ref> shows the summary AMR generated for sentences (a) and (b).</p><p>To generate AMRs from text, different parsers have been proposed. There are Graph-based parsers that aim to build the AMR graph by treating AMR parsing as a procedure for searching for the Maximum Spanning Connected Subgraphs (MSCGs) from an edge-labeled, directed graph of all possible relations. JAMR <ref type="bibr" coords="4,256.94,564.44,17.99,4.95" target="#b15">[16]</ref> was the first AMR parser, developed in 2014, and it used that approach. Transition-based parsers such as CAMR <ref type="bibr" coords="4,360.94,577.99,16.26,4.95" target="#b16">[17]</ref>, by contrast, first generate a dependency parse from a sentence and then transform it into an AMR graph using transition rules. Neural approaches instead view the problem as a sequence translation task, learning to directly convert raw text to linearized AMR representations. For example, the SPRING parser <ref type="bibr" coords="4,119.46,632.19,17.78,4.95" target="#b17">[18]</ref> uses depth-first linearization of AMR, and views the problem of AMR generation as translation problem, translation raw text to linearized AMRs with a BART transformer model <ref type="bibr" coords="4,89.29,659.28,17.91,4.95" target="#b18">[19]</ref> by modifying its tokenizer to handle AMR tokens.</p><p>While AMRs can capture the meaning (semantics) of text, current AMR parsers fail to correctly parse math formulas. Therefore, in this work, we introduced MathAMR. Considering the text "Find ğ‘¥ ğ‘› + ğ‘¦ ğ‘› + ğ‘§ ğ‘› general solution" (the question title for topic A.289 in ARQMath-2 Task 1), Figure <ref type="figure" coords="5,135.10,117.32,5.17,4.95" target="#fig_3">3</ref> shows the steps to generate the MathAMR for this query. First, each formula is replaced with a placeholder node that includes the identifier for that formula. In our example, we show this as "EQ:ID", where in practice ID would be the formula's individual identifier in the ARQMath collection.</p><p>The modified text is then presented to an AMR parser. For our work we used the python-based AMRLib,<ref type="foot" coords="5,128.28,182.43,3.71,3.62" target="#foot_0">4</ref> using the model "model_parse_xfm_bart_large". Figure <ref type="figure" coords="5,383.16,185.06,4.10,4.95" target="#fig_3">3</ref>(a) shows the output AMR. Nodes are either words or concepts (such as 'find-01') from the PropBank framesets <ref type="bibr" coords="5,469.85,198.61,11.56,4.95" target="#b8">[9]</ref>. The edge labels show the relationship between the nodes. In this instance, 'arg0' indicates the subject and 'arg1' the object of the sentence. We introduce a new edge label 'math' to connect a formula's placeholder node to its parent. For further information on AMR notation, see <ref type="bibr" coords="5,480.36,239.26,16.25,4.95" target="#b19">[20]</ref>.</p><p>Figure <ref type="figure" coords="5,131.78,252.81,4.35,4.95" target="#fig_3">3</ref>(b) is the OPT representation of the formula, which is integrated into the AMR by replacing the placeholder with the root of the OPT, thus generating what we call MathAMR. This is shown in Figure <ref type="figure" coords="5,200.70,279.91,3.77,4.95" target="#fig_3">3</ref>(c). To follow AMR conventions, we rename the edge labels from numbers to 'opX' where 'X' is the edge label originally used in the OPT. We use the edge label 'op' as in AMRs edge labels capture the relation and its ordering. In the next sections, we show how MathAMR is used for search.</p><p>This is an early attempt to introduce a unified representation of text and formula using AMRs. Therefore, we aim to keep our model simple and avoid other information related to the formula that could have been used. For example, our current model only uses the OPT formula representation, where as previous researches have shown using SLT representation can be helpful as well <ref type="bibr" coords="5,189.70,388.30,11.47,4.95" target="#b7">[8,</ref><ref type="bibr" coords="5,203.90,388.30,12.59,4.95" target="#b20">21,</ref><ref type="bibr" coords="5,219.22,388.30,12.41,4.95" target="#b21">22]</ref>. In our current model we only use OPTs. Also, we are using the AMR parser that is trained on general text not specific for math which is a limitation of our work. For other domains such as biomedical research, there exist pre-trained AMR parsers <ref type="bibr" coords="5,487.36,415.40,16.11,4.95" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task 2: Formula Retrieval</head><p>Because of our focus on formula representation in AMRs, we start by describing our Task 2 runs. In Section 4 we then draw on that background to describe our Task 1 runs.</p><p>In the formula retrieval task, participating teams were asked to return a set of relevant formulas for a given formula query. Starting in ARQMath-2 <ref type="bibr" coords="5,368.73,514.67,16.41,4.95" target="#b23">[24]</ref>, the relevance criteria for Task 2 were defined in such a way that a formula's context has a role in defining its relevance. Therefore, in our ARQMath-3 Task 2 runs we use athAMR to create a unified representation of formulas and text. In addition to the MathAMR model, we also report results from some of our previous isolated formula search models for this task, as they yielded promising results in ARQMath-1 and -2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Isolated Formula Search Runs</head><p>For isolated formula search, we created three runs. These runs are almost identical to what we had in ARQMath-2. Therefore, we provide a brief summary of the systems, along with differences compared to our previous year's systems. Please eefer to Mansouri et al. <ref type="bibr" coords="6,472.18,90.22,18.06,4.95" target="#b24">[25]</ref> for more information.</p><p>TangentCFT-2: Tangent-CFT <ref type="bibr" coords="6,240.91,117.32,13.00,4.95" target="#b7">[8]</ref> is an embedding model for mathematical formulas that considers SLT and OPT representations of the formulas. In addition to these representations, two unified representations are considered where only types are represented when present in SLT and OPT nodes, referred to as SLT TYPE and OPT TYPE.</p><p>Tangent-CFT uses Tangent-S <ref type="bibr" coords="6,239.20,171.51,18.06,4.95" target="#b20">[21]</ref> to linearize the tree representations of the formulas. Tangent-S represents formulas using tuples comprised of symbol pairs along with the labeled sequence of edges between them. These tuples are generated separately for SLT and OPT trees. In Tangent-CFT we linearize the path tuples using depth-first traversals, tokenize the tuples, and then embed each tuple using an n-gram embedding model, implemented using fastText <ref type="bibr" coords="6,89.29,239.26,16.41,4.95" target="#b25">[26]</ref>. The final embedding of a formula SLT or OPT is the average of its constituent tuple embeddings. Our fastText models were trained on formulas in the ARQMath collection. Using the same pipeline for training, each formula tree is linearized with Tangent-S, then tokenized with Tangent-CFT, and their vector representations are extracted using trained models.</p><p>In our run, we use the MathFIRE<ref type="foot" coords="6,243.34,290.82,3.71,3.62" target="#foot_1">5</ref> (Math Formula Indexing and Retrieval with Elastic Search) system. In this system, formula vector representations are extracted with Tangent-CFT, then loaded in OpenSearch<ref type="foot" coords="6,185.44,317.92,3.71,3.62" target="#foot_2">6</ref> where dense vector retrieval was performed by approximate k-NN search using nmslib and Faiss <ref type="bibr" coords="6,190.06,334.10,16.09,4.95" target="#b26">[27]</ref>. We used the default parameters. Note that in our previous Tangent-CFT implementations we had used exhaustive rather than approximate nearest neighbor search.</p><p>As there are four retrieval results from four different representations, we combined the results using modified Reciprocal Rank Fusion <ref type="bibr" coords="6,265.87,374.75,17.91,4.95" target="#b27">[28]</ref> as:</p><formula xml:id="formula_1" coords="6,207.35,393.41,298.64,29.64">ğ‘…ğ‘…ğ¹ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘“ âˆˆ ğ¹ ) = âˆ‘ï¸ ğ‘šâˆˆğ‘€ ğ‘  ğ‘š (ğ‘“ ) 60 + ğ‘Ÿ ğ‘š (ğ‘“ )<label>(1)</label></formula><p>where the ğ‘  ğ‘š is the similarity score and ğ‘Ÿ ğ‘š is the rank of the candidate. As all the scores from the retrieval with different representations are cosine similarity scores with values in the interval [0, 1], we did not apply score normalization.</p><p>TangentCFT2TED (Primary Run): Previous experiments have shown that Tangent-CFT can find partial matches better than it can find full-tree matches <ref type="bibr" coords="6,393.56,490.43,16.41,4.95" target="#b28">[29]</ref>. As it is an n-gram embedding model, it focuses on matching of n-grams. Also, vectors aim to capture features from the n-gram appearing frequently next to each other, and this approach has less of a focus on structural matching of formulas. Therefore, in TangentCFT2TED, we rerank the top retrieval results from TangentCFT-2 using tree edit distance (TED). We considered three edit operations: deletion, insertion, and substitution. Note that in our work, we only consider the node values and ignore the edge labels. For each edit operation, we use weights learnt on the NTCIR-12 <ref type="bibr" coords="6,89.29,585.28,17.91,4.95" target="#b21">[22]</ref> collection. <ref type="foot" coords="6,155.70,582.65,3.71,3.62" target="#foot_3">7</ref> We use an inverse tree-edit distance score as the similarity score:</p><formula xml:id="formula_2" coords="6,220.53,602.72,285.45,25.50">ğ‘ ğ‘–ğ‘š(ğ‘‡ 1 , ğ‘‡ 2 ) = 1 ğ‘‡ ğ¸ğ·(ğ‘‡ 1 , ğ‘‡ 2 ) + 1 .<label>(2)</label></formula><p>The tree edit distance was used on both SLT and OPT representations, and the results were combined using modified Reciprocal Rank Fusion as in equation 1.</p><p>Because in ARQMath-2 this run had the highest nDCG â€² , this year we annotated it as our primary run, for which the hits are pooled to a greater depth.</p><p>Learning to Rank: Our third isolated formula search model is a learning to rank approach for formula search that we introduced in <ref type="bibr" coords="7,270.82,157.96,16.15,4.95" target="#b28">[29]</ref>. In this model, sub-tree, full-tree, and embedding similarity scores are used to train an SVM-rank model <ref type="bibr" coords="7,332.90,171.51,16.25,4.95" target="#b29">[30]</ref>. Our features are:</p><p>â€¢ Maximum Sub-tree Similarity (MSS) <ref type="bibr" coords="7,280.34,197.02,12.84,4.95" target="#b6">[7]</ref> â€¢ Tuple and node matching scores <ref type="bibr" coords="7,263.24,212.71,12.84,4.95" target="#b6">[7]</ref> â€¢ Weighted and Unweighted tree edit distance scores <ref type="bibr" coords="7,347.18,228.40,17.91,4.95" target="#b28">[29]</ref> â€¢ Cosine similarity from the Tangent-CFT model All features, with the exception of MSS, were calculated using both OPT and SLT representations, both with and without unification of node values to types. The MSS features were calculated only on the unified OPT and SLT representations. MSS is computed from the largest connected match between the formula query and a candidate formula obtained using a greedy algorithm, evaluating pairwise alignments between trees using unified node values. Tuple matching scores are calculated by considering the harmonic mean of the ratio of matching symbol pair tuples between a query and the candidates. The tuples are generated using the Tangent-S <ref type="bibr" coords="7,138.16,364.44,18.07,4.95" target="#b20">[21]</ref> system, which traverses the formula tree depth-first and generates tuples for pairs of symbols and their associated paths.</p><p>For training, we used all topics from ARQMath-1 and -2, a total of about 32K pairs. Following our original proposed approach in <ref type="bibr" coords="7,244.86,405.09,16.28,4.95" target="#b28">[29]</ref>, we re-rank Tangent-S results using linearly weighted features, with weights defined using SVM-rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contextual Formula Search Runs</head><p>Some previous approaches to combined formula and text search combined separate search results from isolated formula search and text retrieval models. For example, Zhong et al. <ref type="bibr" coords="7,488.00,481.92,17.99,4.95" target="#b30">[31]</ref> combined retrieval results from a isolated formula search engine, Approach0 <ref type="bibr" coords="7,431.38,495.47,16.21,4.95" target="#b31">[32]</ref>, and for text used the Anserini toolkit. Similarly, Ng et al. <ref type="bibr" coords="7,283.33,509.01,17.76,4.95" target="#b32">[33]</ref> combined retrieval results from Tangent-L <ref type="bibr" coords="7,488.22,509.01,17.76,4.95" target="#b33">[34]</ref> and BM25+. In the work of Krstovski et al. <ref type="bibr" coords="7,283.52,522.56,16.34,4.95" target="#b34">[35]</ref>, by contrast, equation embeddings generated unified representations by linearizing formulas as tuples and then treated them as tokens in the text. These equation embeddings utilized a context window around formulas and used a word embedding model <ref type="bibr" coords="7,171.51,563.21,17.91,4.95" target="#b35">[36]</ref> to construct vector representations for formulas.</p><p>While our first category of runs focused on isolated formula search, in our second category we made use of the formula's context. Our reasoning is based in part on the potential for complementary between different sources of evidence for meaning, and in part on the relevance definition for Task 2, where a candidate formula's interpretation in the context of its post matters. In particular, it is possible for a formula identical to the query be considered not relevant. As an example from ARQMath-2, for the formula query ğ‘¥ ğ‘› + ğ‘¦ ğ‘› + ğ‘§ ğ‘› (B.289), some exact matches were considered irrelevant, as for that formula query, x, y, and z could (according to the question text) be any real numbers. The assessors thus considered all exact matches in the pooled posts in which x, y, and z referred not to real numbers but specifically to integers as not relevant.</p><p>Therefore, in our contextual formula search model we use MathAMR for search. For each candidate formula, we considered the sentence in which the formula appeared along with a sentence before and after that (if available) as the context. We then generated MathAMR for each candidate. To generate MathAMR for query formulas, we used the same procedure, using the same context window. For matching, we made use of Sentence-BERT Cross-Encoders <ref type="bibr" coords="8,492.33,171.51,11.50,4.95" target="#b3">[4]</ref>. To use Sentence-BERT, we traversed the MathAMR depth-first. For simplicity, we ignored the edge labels in the AMRs. For Figure <ref type="figure" coords="8,248.51,198.61,11.82,4.95" target="#fig_3">3(c</ref> To train a Sentence-BERT cross encoder, we used the pre-trained 'all-distilroberta-v1' model and trained our model with 10 epochs using a batch size of 16 and a maximum sequence size of 256. Note that our final model is what is trained after 10 epochs on the training set, and we did not use a separate validation set. For training, we made use of all the available pairs from ARQMath-1 and -2 topics. For labeling the data, high and medium relevance formula instances were labeled 1, low relevance instances were labeled 0.5, and non-relevant instances 0. For retrieval, we re-rank the candidates retrieved by the Tangent-CFTED system (top-1000 results). Note that candidates are ranked only by the similarity score from our Sentence-BERT model.</p><p>Our fifth run combined the search results from MathAMR and Tangent-CFT2TED systems. This was motivated by MathAMR embeddings for sentences with multiple formulas having the same representation, meaning that all formulas in that sentence receive the same matching score. Because Tangent-CFT2TED performs isolated formula matching, combining these results helps avoid this issue. For combining the results, we normalize the scores to the range 0 to 1 with Min-Max normalization and use modified Reciprocal Rank Fusion as given in Equation <ref type="formula" coords="8,500.21,452.86,3.73,4.95" target="#formula_0">1</ref>.</p><p>Additional Unofficial Post Hoc Run. For our MathAMR model in our official submission we used three sentences as the context window: the sentence in which the candidate formula appears in, a sentence before and as sentence after that. We made a change to the context window size and considered only the sentence in which the formula appears as the context. Also, in our previous approach with context window of three sentences, we simply split the question post containing the query formula at periods (.) in the body text, and then choose the sentence with the candidate formula. However, a sentence can end with other punctuation such as '?'. Also, formulas are delimited within L A T E X by '$'; these formula regions commonly contain sentence punctuation. To address these two issues, in our additional run, we first move any punctuation (. , ! ?) from the end of formula regions to after final delimiter. Then, we use Spacy <ref type="foot" coords="8,115.99,599.27,3.71,3.62" target="#foot_4">8</ref> to split paragraphs into sentences and choose the sentence that a formula appears in. After getting the results with using a context window size of one, we also consider the modified reciprocal rank fusion of this system with Tangent-CFT2ED as another additional post-hoc run. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experiment Results</head><p>This section describes the results of our runs on the ARQMath-1, -2 and -3 Task 2 topics. ARQMath 1 and -2 Progress Test Results. Table <ref type="table" coords="9,343.38,293.36,5.17,4.95" target="#tab_0">1</ref> shows the results of our progress test runs on ARQMath-1 and -2 topics. Note that because some of our systems are trained using relevance judgments for ARQMath-1 and -2 topics, those results should be interpreted as training results rather than as a clean progress test since some models (and in particular MathAMR) may be over-fit to this data. <ref type="foot" coords="9,264.83,344.92,3.71,3.62" target="#foot_5">9</ref>To compare isolated vs contextual formula search, we look at results from Tangent-CFT2TED and MathAMR runs. Using MathAMR can be helpful specifically for formulas for which context is important. For example, in query ğ‘“ (ğ‘¥) = 1 1+ln 2 ğ‘¥ (B.300 from ARQMath-2), the formula is described as "is uniformly continuous on ğ¼ = (0, âˆ)". Similar formulas such as ğ‘”(ğ‘¥) = 1 ğ‘¥ ln 2 ğ‘¥ , that in isolation are less similar to the query are not ranked in the top-10 results from Tangent-CFT2ED. However, with MathAMR, as this formula in its context has the text "is integrable on [2, âˆ)"; it was ranked 4th by MathAMR. The P â€² @10 for this query is 0.1 for Tangent-CFT2TED and 0.7 for MathAMR.</p><p>In contrast to this, there were cases where P â€² @10 was lower for MathAMR compared to Tangent-CFT2TED. As an example, for formula query, B.206, appearing in the title of a question as: "I'm confused on the limit of (ï¸€ 1 + 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘›</head><p>)ï¸€ ğ‘› ", a low relevant formula appearing in the same context, "Calculate limit of (1 + 1 ğ‘› 2 ) ğ‘› " gets higher rank in MathAMR than Tangent-CFT2ED. Both query and candidate formula appear in the questions' title and there is no additional useful information in the text other then the word 'limit' is not providing any new information. Therefore, we can consider this another limitation in our current model, that we are not distinguishing between formula queries that are or are not dependent on the surrounding text, and also there is no pruning applied to MathAMR to remove information that is not necessary helpful.</p><p>ARQMath-3 Results. Table <ref type="table" coords="9,230.63,594.00,5.04,4.95" target="#tab_1">2</ref> shows the Task 2 results on ARQMath-3. Tangent-CFT2TED achieved the highest nDCG â€² among our models, significantly better than other representations, except Tangent-CFT2TED+MathAMR (p &lt; 0.05, ğ‘¡-test with Bonferroni correction). We compare our Tangent-CFT2TED model with MathAMR, looking at the effect of using context. One obvious pattern is that using MathAMR can help with topics for which variables are important. For example, for topic ğ¹ = ğ‘ƒ âŠ• ğ‘‡ . (Topic B.326), P is a projective module and F is a free module. There are instances retrieved in the top-10 results by TangentCFT2ED, such as ğ‘‰ = ğ´ âŠ• ğµ, where variables are referring to different concepts; in this a formula k-dimensional subspace. With MathAMR, formulas such as ğ‘ƒ âŠ• ğ‘„ = ğ¹ appearing in a post that specifically says: "If P is projective, then ğ‘ƒ âŠ• ğ‘„ = ğ¹ for some module P and some free module F. " (similar text to the topic) are ranked in the top-10 results.</p><p>For cases that Tangent-CFT2ED has better effectiveness, two patterns are observed. In the first pattern, the formula is specific and variables do not have specifications. In the second pattern, the context is not helpful (not providing any useful information) for retrieval. For instance, topic B.334, "logarithm proof for ğ‘ ğ‘™ğ‘œğ‘”ğ‘(ğ‘) = ğ‘" the formula on its own is informative enough. Low relevant formulas appearing in a context such as "When I tried out the proof, the final answer I ended up with was ğ‘ ğ‘™ğ‘œğ‘” ğ‘ ğ‘› " are ranked in the top-10 results because of having proof and part of formula.</p><p>Combining the results on Tangent-CFT2ED and MathAMR with our modified RRF provided better P â€² @10 than one of the individual system results for only 10% of the topics. For the topic (B.338) appearing in the a title of a question as "Find all integer solutions of equation ğ‘¦ = ğ‘+ğ‘ğ‘¥ ğ‘-ğ‘¥ ", both Tangent-CFT2ED and MathAMR had P â€² @10 of 0.6. However combining the results with modified RRF increases the P â€² value to 0.9. Table <ref type="table" coords="10,370.81,538.42,5.17,4.95" target="#tab_2">3</ref> shows the top-10 results for Tangent-CFT2ED+MathAMR, along with the original ranked lists for the Tangent-CFT2ED and MathAMR systems. As can be seen, there are relevant formula that Tangent-CFT2ED or MathAMR model gave lower rank to, but the other system provided a better ranking and combining the systems with our modified RRF improved the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Task 1: Answer Retrieval</head><p>The goal of the ARQMath answer retrieval task is to find relevant answers to the mathematical questions in a collection of MathSE answer posts. These are new questions that were asked after the posts in the ARQMath collection (i.e., not during 2010-2018). Our team provided 5 runs<ref type="foot" coords="11,483.11,334.70,7.41,3.62" target="#foot_6">10</ref> for this task. Two of our runs considered only text and L A T E X representation of the formulas. Two other runs used strings created by depth-first MathAMR tree traversals. Our fifth run combined the retrieval results from the two runs, one from each of the approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Raw Text Approaches</head><p>We submitted two runs that use the raw text, with formulas being represented with L A T E X representations. In both runs, we first find similar questions for the given question in Task 1 and then compile all the answers given to those questions and re-rank them based on the similarity to the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Candidate Selection by Question-Question Similarity</head><p>To identify questions similar to a topic, we started with a model pre-trained on the Quora question pairs dataset, <ref type="foot" coords="11,185.12,528.60,7.41,3.62" target="#foot_7">11</ref> and then fine-tuned that model to recognize question-question similarity in actual ARQMath questions. To obtain similar training questions we used the links in the ARQMath collection (i.e., the data from 2010-2018 that predates the topics that we are actually searching) to related and duplicate questions. Duplicate question(s) are marked by MathSE moderators as having been asked before, whereas related questions are marked by MathSE moderators as similar to, but not exactly the same as, the given question. We applied two-step fine-tuning: first using both related and duplicate questions, and then fine-tuning more strictly using only duplicate questions. We used 358,306 pairs in the first round, and 57,670 pairs in the second round.</p><p>For training, we utilized a multi-task learning framework provided by Sentence-BERT, used previously for detecting duplicate Quora questions in the 'distilbert-base-nli-stsb-quora-ranking' model. This framework combines two loss functions. First, the contrastive loss <ref type="bibr" coords="12,447.05,117.32,18.04,4.95" target="#b36">[37]</ref> function minimizes the distance between positive pairs and maximizes the distance between for negative pairs, making it suitable for classification tasks. The second loss function is the multiple negatives ranking loss <ref type="bibr" coords="12,144.83,157.96,16.09,4.95" target="#b37">[38]</ref>, which considers only positive pairs, minimizing the distance between positive pairs out of a large set of possible candidates, making it suitable for ranking tasks. We expected that with these two loss functions we could distinguish well between relevant and not-relevant candidates, and also rank the relevant candidates well by the order of their relevance degrees.</p><p>We set the batch size to 64 and the number of training epochs to 20. The maximum sequence size was set to 128. In our training, half of the samples were positive and the other half were negative, randomly chosen from the collection. In the first fine-tuning, a question title and body are concatenated. In the second fine-tuning, however, we considered the same process for training, with three different inputs:</p><p>â€¢ Using the question title, with a maximum sequence length of 128 tokens.</p><p>â€¢ Using the first 128 tokens of the question body.</p><p>â€¢ Using the last 128 tokens of the question body.</p><p>To find a similar question, we used the three models to separately retrieve the top-1000 most similar questions. The results were combined by choosing the maximum similarity score for each candidate question across the three models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Candidate Ranking by Question-Answer Similarity</head><p>Having a set of candidate answers given to similar questions, we re-rank them differently in our two runs, as explained below.</p><p>QQ-QA-RawText. In our first run, we used QASim (Question-Answer Similarity) <ref type="bibr" coords="12,458.69,456.05,17.76,4.95" target="#b24">[25]</ref> which achieved our highest nDCG â€² value in ARQMath-2. Our training procedure is the same as for our ARQMath-2 system, but this time we added ARQMath-2 training pairs to those from ARQMath-1. For questions, we used the concatenation of title and body, and for the answer we choose only the answer body. For both questions and answers, the first 256 tokens are chosen. For ranking, we compute the similarity score between the topic question and the answer, and the similarity score between the topic question and the answer's parent question. We multiplied those two similarity scores to get the final similarity score. Our pre-trained model is Tiny-BERT, with 6 layers trained on the "MS Marco Passage Reranking" <ref type="bibr" coords="12,353.24,564.44,17.78,4.95" target="#b38">[39]</ref> task. The inputs are triplets of (Question, Answer, Relevance), where the relevance is a number between 0 and 1. In ARQMath, high and medium relevance degrees were considered as relevant for precision-based measures. Based on this, for training, answers from ARQMath-1 and -2 assessed as high or medium got a relevance label of 1, a label of 0.5 was given to those with low relevance, and 0 was given for non-relevant answers. For the system details refer to <ref type="bibr" coords="12,326.81,632.19,16.25,4.95" target="#b24">[25]</ref>.</p><p>SVM-Rank (Primary Run). Previous approaches for the answer retrieval task have shown that information such as question tags and votes can be useful in finding relevant answers <ref type="bibr" coords="12,89.29,672.83,16.30,4.95" target="#b32">[33,</ref><ref type="bibr" coords="12,107.62,672.83,12.23,4.95" target="#b30">31]</ref>. We aimed to make use of these features and study their effect for retrieval. In this second run (which we designated as our primary Task 1 run), we considered 6 features: Question-Question similarity (QQSim) score, Question-Answer similarity (QASim) score, number of comments on the answer, the answer's MathSE score (i.e, upvotes-downvotes), a binary field showing if the answer is marked as selected by the asker (as the best answerto their question), and the percentage of topic question post tags that the question associated with an answer post also contains (which we refer to as question tag overlap). Note that we did not apply normalization to feature value ranges. We trained a ranking SVM model <ref type="bibr" coords="13,427.53,171.51,18.07,4.95" target="#b29">[30]</ref> using all the assessed pairs from ARQMath-1 and -2, calling the result SVM-Rank. After training, we found that QQSim, QASim, and overlap between the tags were the most important features, with weights 0.52, 2.42 and 0.05, respectively, while the weights for other features were less than 0.01.</p><p>Both out QQ-QA-RawText and SVM-Rank models have the same first stage retrieval, using Sentence-BERT to find similar questions. Then the candidates are ranked differently. While both approaches make use of Question-Question and Question-Answer similarity scores (using Sentence-BERT), the second approach considers additional features and learns weights for the features using ARQMath-1 and -2 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MathAMR Approaches</head><p>In our second category of approaches, we made use of our MathAMR representation, providing two runs. As in our raw text-based approach, retrieval is comprised of two stages: identifying candidates from answers to questions similar to a topic question, and then ranking candidate answers by comparing them with the topic question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Candidate Selection by Question-Question Similarity</head><p>In our first step, we find similar questions for a given question in Task 1. For this, we only focus on the question title. Our intuition is that AMR was designed to capture meaning from a sentence. As the question titles are usually just a sentence, we assume that similar questions can be found by comparing AMR representations of their titles.</p><p>Following our approach in Task 2, we generated MathAMR for each question's title. Then the MathAMRs are linearized using a depth-first traversal. We used a model that we trained on RawText for question-question similarity as our pretrained model, although in this case we trained on the question titles. We used the known duplicates from the ARQMath collection (2010-2018) to fine tune our model on the linearized AMR of questions, using a similar process as for raw text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Candidate Ranking by Question-Answer Similarity</head><p>Answers to similar questions are ranked in two ways for our two Task 1 AMR runs.</p><p>QQ-MathSE-AMR. Using a question title's MathAMR, we find the top-1000 similar questions for each topic. Starting from the most similar question and moving down the list, we compile the answers given to the similar questions. The answers for each similar question are ranked based on their MathSE score (i.e., upvotes-downvotes). To determine the similarity score of topic and an answer, we used the reciprocal of the rank after getting the top-1000 answers. Note that this approach does not use any topics from ARQMath-1 or -2 for training.</p><p>QQ-QA-AMR. This run is similar to our QQ-QA-RawText run, but instead of raw text representations, we use MathAMR representations. For similarity of questions, we only use the question titles, while for similarity of a question and an answer we use the first 128 tokens of the linearized MathAMR from the post bodies of the question and the answer. We trained a Sentence-BERT model, and did retrieval, similarly to our QAsim model with two differences: <ref type="bibr" coords="14,495.30,171.51,11.34,4.95" target="#b0">(1)</ref> we used 'all-distilroberta-v1' as the pre-trained model (2) instead of raw text we use linearized MathAMR. The parameters for Sentence-BERT such as number of epochs, batch size and loss function are the same. Our Sentence-BERT design is similar to the QAsim we had used for raw text in ARQMath-2 <ref type="bibr" coords="14,177.24,225.71,16.25,4.95" target="#b24">[25]</ref>. We used both ARQMath-1 and -2 topics from Task 1 for training.</p><p>For our fifth run, we combined the results from our SVM-Rank model (from raw text approaches) and QQ-QA-AMR (from MathAMR approaches) using modified reciprocal rank fusion, naming that run RRF-AMR-SVM.</p><p>Additional Unofficial Post Hoc Runs. In ARQMath-2021, we had two other runs using raw text representations that we also include here for ARQMath-3 topics, using post hoc scoring (i.e., without these runs having contributed to the judgement pools). One is our 'QQ-MathSE-RawText' run, which uses question-question (QQ) similarity to identify similar questions and then ranks answers associated with similar question using MathSE scores (upvotes-downvotes). The similarity score was defined as:</p><formula xml:id="formula_3" coords="14,162.21,367.83,343.77,10.69">ğ‘…ğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘ğ‘’(ğ‘„ ğ‘‡ , ğ´) = ğ‘„ğ‘„ğ‘†ğ‘–ğ‘š(ğ‘„ ğ‘‡ , ğ‘„ ğ´ ) â€¢ ğ‘€ ğ‘ğ‘¡â„ğ‘†ğ¸ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ (ğ´)<label>(3)</label></formula><p>where the ğ‘„ ğ‘‡ is the topic question, ğ´ is a candidate answer and ğ‘„ ğ´ is the question to which answer ğ´ was given. The other is our 'RRF-QQ-MathSE-QA-RawText' run, which combines retrieval results from two systems, 'QQ-MathSE-RawText' and 'QQ-QA-RawText', using our modified reciprocal rank fusion.</p><p>A third additional unofficial post hoc run that we scored locally is 'QQ-MathSE(2)-AMR'. To find similar questions, this model uses the exact same model as 'QQ-MathSE-AMR'. However, for ranking the answers, instead of the ranking function used for 'QQ-MathSSE-AMR', we use the ranking function in equation <ref type="bibr" coords="14,237.46,487.90,10.48,4.95" target="#b2">(3)</ref>.</p><p>For corrected runs, we fixed an error for 'QQ-QA-RawText' model and report the results. This model affects two other models, "SVM-rank model" and "RRF-AMR-SVM". Therefore, we report the results on these systems as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiment Results</head><p>ARQMath 1 and -2 Results. Table <ref type="table" coords="14,256.45,577.99,5.17,4.95" target="#tab_3">4</ref> shows the results of our progress test runs for Task 1 on ARQMath-1 and -2 topics. As with our Task 2 progress test results, those results should be interpreted as training results rather than as a clean progress test since some models may be over-fit to this data. Note that the runs in each category of Raw Text and MathAMR have the same set of candidates to rank, which may lead to similar effectiveness measures.</p><p>ARQMath-3 Results. Table <ref type="table" coords="14,232.23,645.73,5.11,4.95" target="#tab_4">5</ref> shows the RPDL Task 1 results on ARQMath-3 topics along with baseline Linked MSE post that our models aim to automate. Our highest nDCG â€² and mAP â€² are achieved by our additional unofficial 'QQ-MathSE-RawText' run, while our highest P â€² @10 is for the our unofficial corrected "QQ-QA-RawText" run. Comparing the QQ-QA models using MathAMR or raw text, in 41% of topics raw text provided better P â€² @10, while with MathAMR a higher P â€² @10 was achieved for 21% of topics. In all categories of dependencies (text, formula, or both), using raw text was on average more effective than MathAMR. The best effectiveness for MathAMR was when questions were text dependent, with an average P â€² @10 of 0.12, over the 10 assessed topics dependent on text. Considering topic types, for both computation and proof topics, P â€² @10 was 0.10 and 0.06 higher, respectively, using raw text than MathAMR. For concept topics, P â€² @10 was almost the same for the two techniques. Considering topic difficulty, only for hard questions did MathAMR do even slightly better numerically than raw text by P â€² @10, with just a 0.01 difference. Among those topics that did better at P â€² @10 using MathAMR, 94% were hard or medium difficulty topics.</p><p>To further analyze our approaches, we look at the effect of different representations on individual topics. With both raw text and MathAMR, selecting candidates is done by first finding similar questions. Considering the titles of questions to find similar questions, there are cases where MathAMR can be more effective due to considering OPT representations. For  <ref type="table" coords="16,127.73,254.33,5.17,4.95" target="#tab_5">6</ref> shows the titles of the top-5 similar questions for that topic. As seen in this table, MathAMR representations retrieved two similar questions (at ranks 3 and 4) that have similar formulas, whereas raw text failed to retrieve those formulas in its top-5 results. The P â€² @10 on that topic for the QASim model using MathAMR was 0.5, whereas with raw text it was 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Task 3: Open Domain Question Answering</head><p>Open domain question answering is a new pilot task introduced in ARQMath-3. The goal of this task is to provide answers to the math questions in any way, based on any sources. The Task 3 topics are the same as those used for Task 1. Our team created four runs for this task, each having the same architecture. All our four runs use extractive summarization, where a subset of sentences are chosen from the answer to form a summary of the answer. This subset hopefully contains the important section of the answer. The organizers provided one run using GPT-3 <ref type="bibr" coords="16,120.02,434.90,17.91,4.95" target="#b39">[40]</ref> from OpenAI as the baseline system.</p><p>We made two runs, "SVM-Rank" and "QQ-QA-AMR" from those two Task 1 runs by simply truncating the result set for each topic after the first post, then applying one of two BERT-based summarizers to the top-1 answer for each question for each run. For summarizers, we used one that we call BERT that uses 'bert-large-uncased' <ref type="bibr" coords="16,296.73,489.09,17.75,4.95" target="#b40">[41]</ref> and a second called Sentence-BERT (SBERT) <ref type="bibr" coords="16,89.29,502.64,12.69,4.95" target="#b3">[4]</ref> that is implemented using an available python library, <ref type="foot" coords="16,339.93,500.01,7.41,3.62" target="#foot_8">12</ref> with its 'paraphrase-MiniLM-L6-v2' model.</p><p>Both summarizers split answers into sentences, and then embed each sentence using BERT or Sentence-BERT. Sentence vectors are then clustered into k groups using k-means clustering, after which the k sentences closest to each cluster centroid are returned unaltered, in-order, in the generated response. We set ğ‘˜ to 3, meaning that all sentences for posts with up to three sentences are returned, and exactly three sentences are returned for posts with four or more sentences.</p><p>Results. The results of our Task 3 are reported in Table <ref type="table" coords="16,352.53,611.04,3.76,4.95">7</ref>. As seen in this table, our results are not comparable to the baseline system. The highest Average Relevance (AR) and P@1 are achieved using the Sentence-BERT summarizer to summarize the top-1 answered retrieved with the SVM-Rank model for Task 1. Answers extracted by BERT and Sentence-BERT from top-1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head><p>DPRL Runs for Open Domain Question Answering (Task 3) on ARQMath-3 (78) topics. For this task, we submitted the top hit from each run (i.e., a MathSE answer post) that was then passed through a BERT-based summarizer. All runs use both math and text to retrieve answers. GPT-3 is provided by the organizers as the baseline system.</p><p>Open Domain QA Run Avg. Rel. P@1 GPT-3 (Baseline) SVM-Rank answers were only different for 13 of the 78 assessed topics. However, P@1 was identical for each topic. For models using AMR, p@1 differed beteen BERT and Sentence-BERT for 3 topics, although 19 topics had different sentences extracted. In two of those three cases, Sentence-BERT included examples in the extracted answer, resulting in a higher P@1 in both cases compared to BERT. Table <ref type="table" coords="17,116.76,505.18,5.17,4.95" target="#tab_6">8</ref> shows the answers extracted for Topic A.325, which has the title "Find consecutive composite numbers", with the BERT and Sentence-BERT summarizers, where the answers are highly relevant and low relevant, respectively. The only case in hich P@1 for the Sentence-BERT summarizer was lower than that of the BERT summarizer with the "QQ-QA-AMR" model was a case in which the answer extracted by Sentence-BERT was not rendered correctly, and thus was not assessed, which in Task 3 was scored as non-relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper has described the DPRL runs for the ARQMath lab at CLEF 2022. Five runs were submitted for the Formula Retrieval task. These runs used isolated or contextual formula search models. Our models with tree-edit distance ranking had the highest effectiveness among the automatic runs. For the Answer Retrieval task, five runs were submitted using raw text or new unified representation of text and math that we call MathAMR. While for model provided better effectiveness compared to the baseline model we were aiming to automate, there results were less effective compared to our participating teams. For the new Open Domain Question Answering task, four runs were submitted, each of which summarizes the first result from an Answer Retrieval run using extractive summarization on models with MathAMR and raw text. The models using raw text were more effective.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,219.40,205.16,9.15;3,294.45,217.82,4.92,6.12;3,301.22,219.40,13.97,8.74;3,315.55,217.82,4.92,6.12;3,322.31,219.40,13.72,8.74;3,336.47,217.82,4.92,6.12;3,341.89,219.67,164.09,8.87;3,89.29,231.57,416.69,8.96;3,89.29,243.58,417.29,8.87;3,89.29,255.48,416.69,8.96;3,89.29,267.49,50.49,8.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SLT (a) and OPT (b) representations for ğ‘¥ ğ‘› + ğ‘¦ ğ‘› + ğ‘§ ğ‘› . The nodes in SLT show the symbols and their types (with exception of operators). The edge labels above and next show the spatial relationship between symbols. Nodes in the OPT show symbols and their type (U! for unordered (commutative) operator, O! for ordered operator, and V! for variable identifiers). OPT edge labels indicate the ordering of operands.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,233.16,416.69,8.93;4,89.29,245.17,417.76,8.87;4,88.99,257.12,310.10,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: AMR summarization example adapted from Liu et al. [10]. (a) AMR for sentence 'I saw Joe's dog, which was running in the garden. ' (b) AMR for a following sentence, 'The dog was chasing a cat. ' (c) Summary AMR generated from the sentence AMRs shown in (a) and (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,89.29,413.46,219.16,9.15;4,308.45,411.88,4.92,6.12;4,315.91,413.46,14.67,8.74;4,330.94,411.88,4.92,6.12;4,338.41,413.46,14.42,8.74;4,353.27,411.88,4.92,6.12;4,361.12,413.73,144.87,8.87;4,88.98,425.68,417.00,8.87;4,89.29,437.64,416.70,8.87;4,89.29,449.59,397.50,8.87"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Generating MathAMR for the query "Find ğ‘¥ ğ‘› + ğ‘¦ ğ‘› + ğ‘§ ğ‘› general solution" (ARQMath-2 topic A.289). (a) AMR tree is generated with formulas replaced by single tokens having ARQMath formula ids. (b) OPT formula representation is generated for formulas. (c) Operator tree root node replaces the formula place holder node. Note that in (c) the rest of OPT is not shown due to space limitations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,260.33,198.61,245.65,4.95;8,89.29,212.16,87.92,4.95;8,116.56,237.35,362.15,7.22;8,116.56,250.90,97.55,7.21"><head></head><label></label><figDesc>) (included the full OPT from Figure 3(b)), the linearized MathAMR string is: find-01 you thing general-02 solve-01 equal-01 plus SUP z n SUP y n SUP x n imperative</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,88.99,90.49,418.08,139.21"><head>Table 1</head><label>1</label><figDesc>DPRL Runs for Formula Retrieval (Task 2) on ARQMath-1 (45 topics) and ARQMath-2 (58 topics) for topics used in training (i.e., test-on-train). The Data column indicates whether isolated math formulas, or both math formulas and surrounding text are used in retrieval.</figDesc><table coords="9,334.03,147.26,85.86,4.07"><row><cell>Evaluation Measures</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,88.99,90.49,417.24,160.51"><head>Table 2</head><label>2</label><figDesc>DPRL Runs for Formula Retrieval (Task 2) on ARQMath-3 (76) topics. Tangent-CFT2TED is our primary run.</figDesc><table coords="10,149.90,133.92,292.97,117.08"><row><cell>Formula Retrieval</cell><cell></cell><cell cols="3">Evaluation Measures</cell></row><row><cell>Run</cell><cell>Data</cell><cell>nDCG â€²</cell><cell>MAP â€²</cell><cell>P â€² @10</cell></row><row><cell>Tangent-CFT2TED</cell><cell>Math</cell><cell>0.694</cell><cell>0.480</cell><cell>0.611</cell></row><row><cell>Tangent-CFT2</cell><cell>Math</cell><cell>0.641</cell><cell>0.419</cell><cell>0.534</cell></row><row><cell>Tangent-CFT2TED+MathAMR</cell><cell>Both</cell><cell>0.640</cell><cell>0.388</cell><cell>0.478</cell></row><row><cell>LtR</cell><cell>Math</cell><cell>0.575</cell><cell>0.377</cell><cell>0.566</cell></row><row><cell>MathAMR</cell><cell>Both</cell><cell>0.316</cell><cell>0.160</cell><cell>0.253</cell></row><row><cell cols="2">Additional Unofficial Post Hoc</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tangent-CFT2TED+MathAMR</cell><cell>Both</cell><cell>0.681</cell><cell>0.471</cell><cell>0.617</cell></row><row><cell>MathAMR</cell><cell>Both</cell><cell>0.579</cell><cell>0.367</cell><cell>0.549</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,88.99,90.49,418.65,218.66"><head>Table 3</head><label>3</label><figDesc>Top-10 Formulas Retrieved by Tangent-CFT2ED+MathAMR along with their ranks in original Tangent-CFT2ED and MathAMR runs for topic (B.338), appearing in a question post title as "Find all integer solutions of equation ğ‘¦ = ğ‘+ğ‘ğ‘¥ ğ‘-ğ‘¥ ". For space, sentences for formula hits (used by MathAMR) are omitted.</figDesc><table coords="11,137.44,147.94,320.40,161.20"><row><cell cols="2">TangentCFT+MathAMR Relevance</cell><cell>Tangent-CFT2TED</cell><cell>MathAMR</cell></row><row><cell>Top-10 Formula Hits</cell><cell>Score</cell><cell>Rank</cell><cell>Rank</cell></row><row><cell>1. ğ‘¦ = ğ‘+ğ‘ğ‘¥ ğ‘+ğ‘¥ 2. ğ‘¦ = ğ‘+ğ‘ğ‘¥ ğ‘¥+ğ‘ 3. ğ‘¦ = ğ‘+ğ‘¥ ğ‘+ğ‘ğ‘¥ 4. ğ‘¦ = ğ‘+ğ‘ğ‘¥ ğ‘+ğ‘‘ğ‘¥ 5. ğ‘¦ = ğ‘ğ‘¥ ğ‘¥ -ğ‘ 6. ğ‘¦ = ğ‘ğ‘¥+ğ‘ ğ‘ğ‘¥+ğ‘‘ 7. ğ‘¦ = ğ‘+ğ‘‘ğ‘¥ 1-ğ‘-ğ‘‘ğ‘¥ 8. ğ‘”(ğ‘¥) = ğ‘+ğ‘ğ‘¥ ğ‘+ğ‘ğ‘¥ , 9. ğ‘¦ = | ğ‘+ğ‘ğ‘¥ ğ‘+ğ‘¥ | 10. ğ‘¦ = ğ‘-ğ‘¥ 1-ğ‘ğ‘¥</cell><cell>2 2 2 2 1 3 2 2 2 2</cell><cell>1 3 8 2 29 53 4 7 27 19</cell><cell>10 88 8 30 5 2 42 31 9 14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="15,88.98,90.49,417.00,125.64"><head>Table 4</head><label>4</label><figDesc>DPRL progress test Runs for Answer Retrieval (Task 1) progress test on ARQMath-1 (71 topics) and ARQMath-2 (77 topics) for topics used in training (test-on-train). All runs use both text and math information. Stage-1 selects answers candidates that are then ranked in Stage-2. SVM-Rank is the primary run.</figDesc><table coords="15,95.53,157.95,402.38,58.18"><row><cell cols="2">Answer Retrieval Stage-1</cell><cell>Stage-2</cell><cell></cell><cell>ARQMath-1</cell><cell></cell><cell></cell><cell>ARQMath-2</cell><cell></cell></row><row><cell>Run</cell><cell>Selection</cell><cell>Ranking</cell><cell>nDCG â€²</cell><cell>MAP â€²</cell><cell>P â€² @10</cell><cell>nDCG â€²</cell><cell>MAP â€²</cell><cell>P â€² @10</cell></row><row><cell>QQ-QA-AMR</cell><cell cols="2">QQ-MathAMR QQSIM x QASIM (MathAMR)</cell><cell>0.276</cell><cell>0.180</cell><cell>0.295</cell><cell>0.186</cell><cell>0.103</cell><cell>0.237</cell></row><row><cell>QQ-MathSE-AMR</cell><cell cols="2">QQ-MathAMR MathSE</cell><cell>0.231</cell><cell>0.114</cell><cell>0.218</cell><cell>0.187</cell><cell>0.069</cell><cell>0.138</cell></row><row><cell>QQ-QA-RawText</cell><cell>QQ-RawText</cell><cell>QQSIM x QASIM (RawText)</cell><cell>0.511</cell><cell>0.467</cell><cell>0.604</cell><cell>0.532</cell><cell>0.460</cell><cell>0.597</cell></row><row><cell>SVM-Rank</cell><cell>QQ-RawText</cell><cell>SMV-Rank</cell><cell>0.508</cell><cell>0.467</cell><cell>0.604</cell><cell>0.533</cell><cell>0.460</cell><cell>0.596</cell></row><row><cell>RRF-AMR-SVM</cell><cell>-</cell><cell>-</cell><cell>0.587</cell><cell>0.519</cell><cell>0.625</cell><cell>0.582</cell><cell>0.490</cell><cell>0.618</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="15,88.99,237.32,417.00,204.57"><head>Table 5</head><label>5</label><figDesc>DPRL Runs for Answer Retrieval (Task 1) on ARQMath-3 (78) topics along with the Linked MSE posts baseline. SVM-Rank is the primary run. For Post Hoc runs, (C) indicates corrected run, and (A) indicates additional run. Linked MSE posts is a baseline system provided by ARQMath organizers.</figDesc><table coords="15,96.42,293.65,400.35,148.24"><row><cell>Answer Retrieval</cell><cell>Stage-1</cell><cell>Stage-2</cell><cell cols="3">Evaluation Measures</cell></row><row><cell>Run</cell><cell>Selection</cell><cell>Ranking</cell><cell>nDCG â€²</cell><cell>MAP â€²</cell><cell>P â€² @10</cell></row><row><cell>Linked MSE posts</cell><cell>-</cell><cell>-</cell><cell>0.106</cell><cell>0.051</cell><cell>0.168</cell></row><row><cell>SVM-Rank</cell><cell>QQ-RawText</cell><cell>SVM-Rank</cell><cell>0.283</cell><cell>0.067</cell><cell>0.101</cell></row><row><cell>QQ-QA-RawText</cell><cell>QQ-RawText</cell><cell>QQSIM x QASIM (RawText)</cell><cell>0.245</cell><cell>0.054</cell><cell>0.099</cell></row><row><cell>QQ-MathSE-AMR</cell><cell cols="2">QQ-MathAMR MathSE</cell><cell>0.178</cell><cell>0.039</cell><cell>0.081</cell></row><row><cell>QQ-QA-AMR</cell><cell cols="2">QQ-MathAMR QQSIM x QASIM (MathAMR)</cell><cell>0.185</cell><cell>0.040</cell><cell>0.091</cell></row><row><cell>RRF-AMR-SVM</cell><cell>-</cell><cell>-</cell><cell>0.274</cell><cell>0.054</cell><cell>0.022</cell></row><row><cell>Post Hoc Runs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>QQ-QA-RawText (C)</cell><cell>QQ-RawText</cell><cell>QQSIM x QASIM (RawText)</cell><cell>0.241</cell><cell>0.030</cell><cell>0.151</cell></row><row><cell>SVM-Rank (C)</cell><cell>QQ-RawText</cell><cell>SVM-Rank</cell><cell>0.296</cell><cell>0.070</cell><cell>0.101</cell></row><row><cell>RRF-AMR-SVM (C)</cell><cell>-</cell><cell>-</cell><cell>0.269</cell><cell>0.059</cell><cell>0.106</cell></row><row><cell>QQ-MathSE-RawText (A)</cell><cell>QQ-RawText</cell><cell>MathSE</cell><cell>0.313</cell><cell>0.147</cell><cell>0.087</cell></row><row><cell cols="2">RRF-QQ-MathSE-QA-RawText (A) -</cell><cell>-</cell><cell>0.250</cell><cell>0.067</cell><cell>0.110</cell></row><row><cell>QQ-MathSE(2)-AMR (A)</cell><cell cols="2">QQ-MathAMR MathSE(2)</cell><cell>0.200</cell><cell>0.044</cell><cell>0.100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="16,88.93,90.49,417.05,168.79"><head>Table 6</head><label>6</label><figDesc>Titles of the top-5 most similar questions found with MathAMR and raw text, for the topic question with title "Proving âˆ‘ï¸€ ğ‘› ğ‘˜=1 cos 2ğœ‹ğ‘˜ ğ‘› = 0".</figDesc><table coords="16,89.29,134.64,411.01,124.64"><row><cell cols="2">Rank MathAMR</cell><cell></cell><cell>RawText</cell><cell></cell></row><row><cell>1 2 3 4 5</cell><cell>Prove that How to prove ğ‘ âˆ‘ï¸€ ğ‘›=1 âˆ‘ï¸€ ğ‘› cos(2ğœ‹ğ‘›/ğ‘ ) = 0 ğ‘˜=1 cos( 2ğœ‹ğ‘˜ ğ‘› ) = 0 for any ğ‘› &gt; 1? Proving that âˆ‘ï¸€ ğ‘›-1 ğ‘¥=0 cos (ï¸€ ğ‘˜ + ğ‘¥ 2ğœ‹ ğ‘› )ï¸€ = âˆ‘ï¸€ ğ‘›-1 ğ‘¥=0 sin (ï¸€ ğ‘˜ + ğ‘¥ 2ğœ‹ ğ‘› âˆ‘ï¸€ ğ‘›-1 ğ‘˜=0 cos (ï¸€ 2ğœ‹ğ‘˜ ğ‘› )ï¸€ = 0 = âˆ‘ï¸€ ğ‘›-1 ğ‘˜=0 sin (ï¸€ 2ğœ‹ğ‘˜ ğ‘› )ï¸€ âˆ‘ï¸€ cos when angles are in arithmetic progression</cell><cell>)ï¸€</cell><cell>How to prove How to prove that âˆ‘ï¸€ ğ‘› ğ‘˜=1 cos( 2ğœ‹ğ‘˜ ğ‘› ) = 0 for any ğ‘› &gt; 1? âˆ‘ï¸€ ğ‘›-1 ğ‘˜=0 cos (ï¸€ 2ğœ‹ğ‘˜ )ï¸€ = 0 ğ‘› + ğœ‘ ğ‘ = 0. Prove that âˆ‘ï¸€ cos(2ğœ‹ğ‘›/ğ‘ ) = 0 ğ‘›=1 Understanding a step in applying deMoivre's Theorem to âˆ‘ï¸€ cos when angles are in arithmetic progression</cell><cell>âˆ‘ï¸€ ğ‘› ğ‘˜=0 cos(ğ‘˜ğœƒ)</cell></row><row><cell cols="4">example, this happens for topic A.328 with the title:</cell><cell></cell></row><row><cell></cell><cell>"Proving</cell><cell cols="2">âˆ‘ï¸€ ğ‘› ğ‘˜=1 cos 2ğœ‹ğ‘˜ ğ‘› = 0"</cell><cell></cell></row><row><cell></cell><cell>Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="17,88.99,182.28,417.17,224.85"><head>Table 8</head><label>8</label><figDesc>Sentence-BERT vs. BERT extracted summaries on the first answer retrieved by QQ-QA-AMR model for topic A.325. Title Find consecutive composite numbers (A.325) BERT "No, we can find consecutive composites that are not of this form. The point of ğ‘›! is just that it is a ""very divisible number"". " SBERT No, we can find consecutive composites that are not of this form. For example the numbers ğ‘›! 2 + 2, ğ‘›! 2 + 4 â€¢ â€¢ â€¢ + ğ‘›! 2 + ğ‘› or ğ‘›! 3 + 2, ğ‘›! 3 + 3 . . . ğ‘›! 3 + 2. Also ğ‘˜ğ‘›! + 2, ğ‘˜ğ‘›! + 3 . . . ğ‘˜!ğ‘› + ğ‘› works for all ğ‘˜ &gt; 0 âˆˆ Z You can also get a smaller examples if instead of using ğ‘›! we use the least common multiple of the numbers between 1 and ğ‘›.</figDesc><table coords="17,98.49,182.28,278.82,130.54"><row><cell></cell><cell>1.346</cell><cell>0.500</cell></row><row><cell>SBERT-SVMRank</cell><cell>0.462</cell><cell>0.154</cell></row><row><cell>BERT-SVMRank</cell><cell>0.449</cell><cell>0.154</cell></row><row><cell>SBER-QQ-AMR</cell><cell>0.423</cell><cell>0.128</cell></row><row><cell>BERT-QQ-AMR</cell><cell>0.385</cell><cell>0.103</cell></row><row><cell>Topic</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="5,108.93,673.70,124.29,4.07"><p>https://github.com/bjascob/amrlib</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="6,108.93,640.83,116.23,4.07"><p>https://gitlab.com/dprl/mathfire</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="6,108.93,651.79,85.39,4.07"><p>https://opensearch.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="6,108.93,662.75,397.05,4.07;6,89.29,673.71,37.61,4.07"><p>We have also trained weights on ARQMath-1, and those weights were similar to those trained on the NTCIR-12 collection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4" coords="8,108.93,673.66,58.54,4.07"><p>https://spacy.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5" coords="9,108.93,651.77,397.06,4.07;9,88.96,662.73,417.03,4.07;9,89.29,673.69,23.74,4.07"><p>This progress-test-on-train condition was the condition requested by the ARQMath organizers; all systems were to be run on ARQMath-1 and ARQMath-2 topics in the same configuration as they were run on ARQMath-3 topics.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6" coords="11,108.93,662.74,203.84,4.07"><p>Note that ARQMath teams are limited to 5 submissions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7" coords="11,108.93,673.70,270.26,4.07"><p>https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8" coords="16,108.93,673.71,192.73,4.07"><p>https://pypi.org/project/bert-extractive-summarizer/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Heng Ji</rs> and <rs type="person">Kevin Knight</rs> for helpful discussions about AMR and multimodal text representations. This material is based upon work supported by the <rs type="funder">Alfred P. Sloan Foundation</rs> under Grant No. <rs type="grantNumber">G-2017-9827</rs> and the <rs type="funder">National Science Foundation (USA)</rs> under Grant No. <rs type="grantNumber">IIS-1717997</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_98ETbQK">
					<idno type="grant-number">G-2017-9827</idno>
				</org>
				<org type="funding" xml:id="_RaAtgbY">
					<idno type="grant-number">IIS-1717997</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="18,112.66,315.86,393.33,4.95;18,112.28,329.41,394.91,4.95;18,112.66,342.96,22.69,4.95" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="18,340.22,315.86,165.76,4.95;18,112.28,329.41,126.70,4.95">Advancing Math-Aware Search: The ARQMath-3 Lab at CLEF 2022</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,260.39,329.41,201.22,4.95">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,356.51,393.58,4.95;18,112.66,370.06,393.33,4.95;18,112.66,383.61,336.62,4.95" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="18,344.81,356.51,105.97,4.95;18,481.58,356.51,24.66,4.95;18,112.66,370.06,219.80,4.95">CLEF Lab on Answer Retrieval for Questions on Math</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,360.78,370.06,145.21,4.95;18,112.66,383.61,263.46,4.95">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Overview of ARQMath</note>
</biblStruct>

<biblStruct coords="18,112.66,397.16,393.33,4.95;18,112.28,410.71,394.91,4.95;18,112.66,424.26,22.69,4.95" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="18,336.40,397.16,169.59,4.95;18,112.28,410.71,104.77,4.95">Advancing Math-Aware Search: The ARQMath-2 lab at CLEF</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,259.15,410.71,202.26,4.95">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,437.81,395.17,4.95;18,112.66,451.35,393.33,4.95;18,112.66,464.90,393.33,4.95;18,112.66,478.45,156.16,4.95" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="18,231.26,437.81,276.57,4.95;18,112.66,451.35,41.53,4.95">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,183.50,451.35,322.49,4.95;18,112.66,464.90,393.33,4.95;18,112.66,478.45,125.40,4.95">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,492.00,394.53,4.95;18,112.66,505.55,395.17,4.95;18,112.66,519.10,394.53,4.95;18,112.66,532.65,22.69,4.95" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="18,225.04,505.55,219.88,4.95">Abstract Meaning Representation for Sembanking</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,468.64,505.55,39.19,4.95;18,112.66,519.10,389.81,4.95">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,546.20,395.16,4.95;18,112.66,559.75,395.01,4.95;18,112.66,573.30,168.81,4.95" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="18,216.69,546.20,235.67,4.95">Recognition and retrieval of mathematical expressions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blostein</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10032-011-0174-4</idno>
		<ptr target="https://doi.org/10.1007/s10032-011-0174-4.doi:10.1007/s10032-011-0174-4" />
	</analytic>
	<monogr>
		<title level="j" coord="18,460.70,546.20,47.12,4.95;18,112.66,559.75,98.40,4.95">Int. J. Document Anal. Recognit</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="331" to="357" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,586.85,394.61,4.95;18,112.66,600.40,395.17,4.95;18,112.66,613.95,393.33,4.95;18,112.41,627.49,394.28,4.95;18,112.26,641.04,378.67,4.95" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="18,311.50,586.85,176.26,4.95">Tangent-3 at the NTCIR-12 MathIR task</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">W</forename><surname>Tompa</surname></persName>
		</author>
		<ptr target="http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings12/pdf/ntcir/MathIR/06-NTCIR12-MathIR-DavilaK.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="18,284.54,600.40,223.29,4.95;18,112.66,613.95,182.35,4.95">Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Kando</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<meeting>the 12th NTCIR Conference on Evaluation of Information Access Technologies<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 7-10, 2016. 2016</date>
		</imprint>
		<respStmt>
			<orgName>National Center of Sciences ; National Institute of Informatics (NII)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,654.59,393.33,4.95;19,112.66,90.22,393.33,4.95;19,112.66,103.77,299.62,4.95" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="18,425.90,654.59,80.08,4.95;19,112.66,90.22,206.09,4.95">Tangent-CFT: An Embedding Model for Mathematical Formulas</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,342.69,90.22,163.29,4.95;19,112.66,103.77,269.98,4.95">Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<meeting>the 2019 ACM SIGIR International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,117.32,328.49,4.95" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="19,237.26,117.32,122.48,4.95">From TreeBank to PropBank</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,388.05,117.32,22.15,4.95">LREC</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,130.87,393.32,4.95;19,112.66,144.41,393.33,4.95;19,112.28,157.96,393.71,4.95;19,112.33,171.51,86.46,4.95" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="19,350.84,130.87,155.14,4.95;19,112.66,144.41,144.49,4.95">Toward Abstractive Summarization Using Semantic Representations</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,282.56,144.41,223.43,4.95;19,112.28,157.96,393.71,4.95;19,112.33,171.51,56.35,4.95">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,185.06,395.17,4.95;19,112.66,198.61,393.33,4.95;19,112.66,212.16,76.23,4.95" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="19,236.01,185.06,271.82,4.95;19,112.66,198.61,46.78,4.95">Abstract Meaning Representation for Multi-Document Summarization</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,189.47,198.61,316.52,4.95;19,112.66,212.16,46.58,4.95">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,225.71,394.52,4.95;19,112.66,239.26,395.17,4.95;19,112.66,252.81,393.53,4.95;19,112.66,266.36,231.26,4.95" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="19,339.60,239.26,168.23,4.95;19,112.66,252.81,229.59,4.95">Leveraging Abstract Meaning Representation for Knowledge Base Question Answering</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cornelio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fokoue-Nkoutche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,366.43,252.81,139.76,4.95;19,112.66,266.36,201.30,4.95">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,279.91,393.32,4.95;19,112.66,293.46,393.32,4.95;19,112.66,307.00,246.93,4.95" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="19,263.95,279.91,242.03,4.95;19,112.66,293.46,246.44,4.95">Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,383.03,293.46,122.95,4.95;19,112.66,307.00,216.97,4.95">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,320.55,393.33,4.95;19,112.66,334.10,393.32,4.95;19,112.66,347.65,79.69,4.95" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="19,336.64,320.55,169.35,4.95;19,112.66,334.10,190.42,4.95">Extracting Biomolecular Interactions Using Semantic Parsing of Biomedical Text</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,326.14,334.10,179.84,4.95;19,112.66,347.65,50.10,4.95">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,361.20,393.33,4.95;19,112.66,374.75,393.33,4.95;19,112.28,388.30,393.71,4.95;19,112.33,401.85,86.46,4.95" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="19,183.74,361.20,322.25,4.95;19,112.66,374.75,144.08,4.95">Abstract Meaning Representation Guided Graph Encoding and Decoding for Joint Information Extraction</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,282.24,374.75,223.75,4.95;19,112.28,388.30,393.71,4.95;19,112.33,401.85,56.35,4.95">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,415.40,395.17,4.95;19,112.66,428.95,393.33,4.95;19,112.66,442.50,395.00,4.95" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="19,395.21,415.40,112.62,4.95;19,112.66,428.95,232.30,4.95">A Discriminative Graphbased Parser for the Abstract Meaning Representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,367.13,428.95,138.85,4.95;19,112.66,442.50,255.65,4.95">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="19,112.66,456.05,395.17,4.95;19,112.66,469.60,393.53,4.95;19,112.66,483.14,292.05,4.95" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="19,252.04,456.05,210.46,4.95">A transition-based algorithm for AMR parsing</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,488.38,456.05,19.45,4.95;19,112.66,469.60,393.53,4.95;19,112.66,483.14,261.93,4.95">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,496.69,393.33,4.95;19,112.66,510.24,394.53,4.95;19,112.66,523.79,22.69,4.95" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="19,286.03,496.69,219.96,4.95;19,112.66,510.24,274.57,4.95">One SPRING to Rule them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Blloshmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,410.14,510.24,91.36,4.95">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,537.34,395.17,4.95;19,112.66,550.89,395.17,4.95;19,112.66,564.44,393.32,4.95;19,112.66,577.99,232.96,4.95" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="19,146.14,550.89,361.68,4.95;19,112.66,564.44,174.42,4.95">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,311.16,564.44,194.82,4.95;19,112.66,577.99,203.31,4.95">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,591.54,394.53,4.95;19,112.66,605.09,393.98,4.95;19,112.28,618.64,353.95,4.95" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="19,317.68,605.09,188.96,4.95;19,112.28,618.64,87.03,4.95">Abstract Meaning Representation (AMR) Annotation Release</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Badarau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Baranescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bardocz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>O'gorman</surname></persName>
		</author>
		<ptr target="https://catalog.ldc.upenn.edu/LDC2020T02" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,632.19,393.33,4.95;19,112.66,645.73,393.33,4.95;19,112.66,659.28,259.54,4.95" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="19,207.26,632.19,298.73,4.95;19,112.66,645.73,69.16,4.95">Layout and Semantics: Combining Representations for Mathematical Formula Search</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,208.36,645.73,297.62,4.95;19,112.66,659.28,229.90,4.95">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,672.83,393.61,4.95;20,112.66,90.22,125.24,4.95" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Topic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<title level="m" coord="19,402.62,672.83,103.65,4.95;20,112.66,90.22,38.15,4.95">NTCIR-12 MathIR Task Overview</title>
		<imprint>
			<publisher>NTCIR</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,103.77,393.33,4.95;20,112.66,117.32,393.33,4.95;20,112.33,130.87,96.45,4.95" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="20,209.43,103.77,296.55,4.95;20,112.66,117.32,47.59,4.95">Semeval-2017 Task 9: Abstract Meaning Representation Parsing and Generation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Priyadarshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,183.58,117.32,322.40,4.95;20,112.33,130.87,66.29,4.95">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,144.41,393.32,4.95;20,112.66,157.96,393.33,4.95;20,112.66,171.51,367.47,4.95" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="20,330.36,144.41,109.68,4.95;20,474.74,144.41,31.24,4.95;20,112.66,157.96,232.69,4.95">Second CLEF Lab on Answer Retrieval for Questions on Math</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,368.54,157.96,137.45,4.95;20,112.66,171.51,263.46,4.95">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Overview of ARQMath-2</note>
</biblStruct>

<biblStruct coords="20,112.66,185.06,394.62,4.95;20,112.66,198.61,378.89,4.95" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="20,286.99,185.06,124.65,4.95">DPRL Systems in the CLEF</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,438.91,185.06,68.37,4.95;20,112.66,198.61,346.97,4.95">ARQMath Lab: Sentence-BERT for Answer Retrieval, Learning-to-Rank for Formula Retrieval</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,212.16,393.33,4.95;20,112.66,225.71,376.27,4.95" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="20,329.11,212.16,176.88,4.95;20,112.66,225.71,51.25,4.95">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,172.50,225.71,276.71,4.95">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,239.26,395.17,4.95;20,112.66,252.81,109.47,4.95" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="20,254.26,239.26,181.52,4.95">Billion-Scale Similarity Search with GPUs</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>JÃ©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,445.26,239.26,62.57,4.95;20,112.66,252.81,77.55,4.95">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,266.36,393.33,4.95;20,112.66,279.91,393.32,4.95;20,112.66,293.46,355.80,4.95" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="20,295.97,266.36,210.02,4.95;20,112.66,279.91,173.23,4.95">Reciprocal Rank Fusion Outperforms Condorcet and Individual Rank Learning Methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buettcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,309.84,279.91,196.14,4.95;20,112.66,293.46,326.16,4.95">Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,307.00,395.16,4.95;20,112.66,320.55,393.32,4.95;20,112.66,334.10,135.48,4.95" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="20,275.51,307.00,190.40,4.95">Learning to Rank for Mathematical Formula</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,489.14,307.00,18.69,4.95;20,112.66,320.55,393.32,4.95;20,112.66,334.10,105.84,4.95">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,347.65,393.33,4.95;20,112.66,361.20,373.37,4.95" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="20,173.83,347.65,169.79,4.95">Training Linear SVMs in Linear Time</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,370.57,347.65,135.42,4.95;20,112.66,361.20,342.99,4.95">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,374.75,395.17,4.95;20,112.66,388.30,393.33,4.95;20,112.33,401.85,90.01,4.95" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="20,319.72,374.75,188.11,4.95;20,112.66,388.30,393.33,4.95;20,112.33,401.85,29.45,4.95">Approach Zero and Anserini at the CLEF-2021 ARQMath Track: Applying Substructure Search and BM25 on Operator Tree Path Tokens</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,415.40,395.17,4.95;20,112.66,428.95,393.33,4.95;20,112.66,442.50,123.88,4.95" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="20,190.14,415.40,256.22,4.95">PYA0: A Python Toolkit for Accessible Math-Aware Search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,469.23,415.40,38.60,4.95;20,112.66,428.95,393.33,4.95;20,112.66,442.50,94.24,4.95">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,456.05,393.33,4.95;20,112.66,469.60,394.53,4.95;20,112.66,483.14,22.69,4.95" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="20,309.64,456.05,116.98,4.95">Dowsing for Math Answers</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kassaie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">W</forename><surname>Tompa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,449.04,456.05,56.95,4.95;20,112.66,469.60,346.66,4.95">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,496.69,394.53,4.95;20,112.66,510.24,350.02,4.95" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="20,250.55,496.69,251.89,4.95">Choosing Math Features for BM25 Ranking with Tangent-L</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">W</forename><surname>Tompa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,127.29,510.24,284.55,4.95">Proceedings of the ACM Symposium on Document Engineering</title>
		<meeting>the ACM Symposium on Document Engineering</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,523.79,386.30,4.95" xml:id="b34">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Krstovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09123</idno>
		<title level="m" coord="20,222.40,523.79,93.75,4.95">Equation embeddings</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="20,112.66,537.34,393.32,4.95;20,112.14,550.89,393.85,4.95;20,112.66,564.44,68.28,4.95" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="20,368.21,537.34,137.77,4.95;20,112.14,550.89,198.47,4.95">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,318.00,550.89,187.99,4.95;20,112.66,564.44,36.36,4.95">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,577.99,393.33,4.95;20,112.28,591.54,393.92,4.95;20,112.30,605.09,242.65,4.95" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="20,269.12,577.99,236.87,4.95;20,112.28,591.54,135.56,4.95">Learning a Similarity Metric Discriminatively, with Application to Face Verification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,291.88,591.54,214.31,4.95;20,112.30,605.09,185.80,4.95">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,618.64,394.53,4.95;20,112.66,632.19,393.59,4.95;20,112.66,645.73,146.44,4.95" xml:id="b37">
	<monogr>
		<title level="m" type="main" coord="20,174.55,632.19,295.70,4.95">Efficient Natural Language Response Suggestion for Smart Reply</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>LukÃ¡cs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Miklos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00652</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="20,112.66,659.28,394.61,4.95;20,112.28,672.83,379.28,4.95" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="20,112.28,672.83,269.02,4.95">A human generated machine reading comprehension dataset</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Marco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,404.10,672.83,57.15,4.95">CoCo@ NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,90.22,394.53,4.95;21,112.66,103.77,374.64,4.95" xml:id="b39">
	<monogr>
		<title level="m" type="main" coord="21,277.24,103.77,178.86,4.95">Language Models are Few-Shot Learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,117.32,393.33,4.95;21,112.66,130.87,107.17,4.95" xml:id="b40">
	<monogr>
		<title level="m" type="main" coord="21,156.19,117.32,279.45,4.95">Leveraging BERT for Extractive Text Summarization on Lectures</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04165</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
