<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,384.68,15.42;1,89.29,106.66,103.45,15.42">Does Closed-Set Training Generalize to Open-Set Recognition?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,40.41,11.96"><forename type="first">Fan</forename><surname>Gao</surname></persName>
							<email>xsgaofan@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Beijing Key Laboratory of Network System and Network Culture</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,142.34,134.97,61.61,11.96"><forename type="first">Zining</forename><surname>Chen</surname></persName>
							<email>chenzn@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Beijing Key Laboratory of Network System and Network Culture</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,216.59,134.97,66.16,11.96"><forename type="first">Weiqiu</forename><surname>Wang</surname></persName>
							<email>wangweiqiu@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Beijing Key Laboratory of Network System and Network Culture</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.40,134.97,55.81,11.96"><forename type="first">Yinan</forename><surname>Song</surname></persName>
							<email>songyn@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Beijing Key Laboratory of Network System and Network Culture</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.85,134.97,29.52,11.96"><forename type="first">Fei</forename><surname>Su</surname></persName>
							<email>sufei@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Beijing Key Laboratory of Network System and Network Culture</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,406.01,134.97,74.37,11.96"><forename type="first">Zhicheng</forename><surname>Zhao</surname></persName>
							<email>zhaozc@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Beijing Key Laboratory of Network System and Network Culture</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,56.18,11.96"><forename type="first">Hong</forename><surname>Chen</surname></persName>
							<email>chenhongyj@chinamobile.com</email>
							<affiliation key="aff1">
								<orgName type="department">Mobile Research Institute</orgName>
								<orgName type="institution">China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,384.68,15.42;1,89.29,106.66,103.45,15.42">Does Closed-Set Training Generalize to Open-Set Recognition?</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">C4C63898FAE5D6BF73BDA8CB42DEB50C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fungi identification</term>
					<term>Fine-grained</term>
					<term>Open-set recognition</term>
					<term>Metadata</term>
					<term>Long-tailed</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic classification of fungi assists scientists in species identification and biodiversity protection. The FungiCLEF 2022 challenge provides a large-scale multi-modal fine-grained dataset to contribute to this issue. This paper proposes a novel open-set image classification method called Class-wise Weighted Prototype Classifier (CWPC) which decouples closed-set training and open-set inference. Thus, it can benefit from all existing closed-set advances and transfer to open-set without further modification. By using meta-vision models and two different vision-only models, an ensemble result achieves excellent performance with the mean F1 scores of 81.02% and 77.58% on public leaderboard and private leaderboard, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fungi contains many fine-grained classes of eukaryotic organisms that are widely distributed in nature and play an important role in human production and life. Automatic recognition of fungi species assists mycologists, citizen scientists and nature enthusiasts in species identification in the wild. However, fungi identification is difficult because of the high diversity of fungi, fine granularity of species and domain gap caused by observation tools. As a part of LifeCLEF-2022 <ref type="bibr" coords="1,170.28,485.00,11.48,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,184.50,485.00,9.03,10.91" target="#b1">2]</ref> which aims at biodiversity identification and prediction, FungiCLEF-2022 <ref type="bibr" coords="1,111.94,498.55,12.70,10.91" target="#b2">[3]</ref> searches for a robust open-set fungi identification system which is more practical than a closed-set recognition system in real-world scenario.</p><p>Thanks to the large-scale labeled dataset such as ImageNet <ref type="bibr" coords="1,354.69,525.64,12.68,10.91" target="#b3">[4]</ref> and iNaturalist <ref type="bibr" coords="1,436.35,525.64,11.28,10.91" target="#b4">[5]</ref>, convolution neural networks are the mainstream of vision recognition and outperform human experts in some fields <ref type="bibr" coords="1,144.40,552.74,11.49,10.91" target="#b3">[4,</ref><ref type="bibr" coords="1,159.54,552.74,7.65,10.91" target="#b5">6]</ref>. Due to the limitation of convolution structures, CNN only takes image information as input and can't benefit from rich metadata information. Meanwhile, most of existing methods and optimization are based on closed-set recognition which can't be applied to open-set recognition directly.</p><p>In this paper, we propose a novel open-set classification method called Class-wise Weighted Prototype Classifier (CWPC) by decoupling closed-set training and open-set inference. On the one hand, the open-set recognition can benefit from all the advances in closed-set image classification, such as large-scale pre-trained models, label smoothing and data augmentation. On the other hand, it is cost-saving to transfer closed-set recognition models to open-set scenarios with further modification and training. As for the closed-set training, firstly we elaborately design a text template to compensate the context of metadata which have more reasonable and complete semantic information than discrete and independent words. And then, we combine text and vision information with a meta-vision model where convolution is used to extra deep vision embedding and transformer is used to fuse image and metadata information. We also employ two different vision-only models to complement each other. A hard classes mining strategy and LDAM loss <ref type="bibr" coords="2,282.59,236.01,12.68,10.91" target="#b6">[7]</ref> are used to eliminate the long-tailed distribution of dataset. Finally, we got a result of 81.02% and 77.58% with our method respectively on public leaderboard and private leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Open-set Recognition</head><p>Unlike traditional closed-set recognition, open-set recognition is more suitable for real-world applications. This task was first proposed in <ref type="bibr" coords="2,282.19,356.25,11.27,10.91" target="#b7">[8]</ref>, in which the authors apply an 1-vs-Set machine to calculate an open-space risk as an indicator. When a sample is far from known samples, the increased risk suggests it is more likely from unknown classes. OpenMax <ref type="bibr" coords="2,436.99,383.35,12.89,10.91" target="#b8">[9]</ref> replaces the SoftMax layer in DNNs with an OpenMax layer to redistribute to get the class probability of unknown samples. PROSER <ref type="bibr" coords="2,214.35,410.45,17.79,10.91" target="#b9">[10]</ref> takes open-set problem into consideration during the training process. It generates data placeholders by fusing middle hidden layer features from different classes as the embedding of open-set classes and augments the output layer with an extra dummy classifier to well separate known and unknown. Although these methods make great progress on open-set recognition, they can't utilize metadata efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-Modality</head><p>Fine-grained classification methods with only images have been explored by many researchers. Besides visual information, additional information is used to improve the performance. CVL <ref type="bibr" coords="2,488.22,527.92,17.76,10.91" target="#b10">[11]</ref> proposes a two-branch network while the vision stream learns deep vision representations and the language stream learns text representations. The results of two streams are merged in later stage to combine vision and language. Geo-Aware Networks <ref type="bibr" coords="2,377.41,568.57,17.75,10.91" target="#b11">[12]</ref> incorporates geolocation information prior to fine-grained classification and examines various ways of geographic prior. MetaFormer <ref type="bibr" coords="2,173.91,595.66,17.89,10.91" target="#b12">[13]</ref> is a hybrid structure backbone where the convolution can extra image embedding and introduce the inductive bias of the convolution, and the transformer can fuse visual and meta-information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Long-tail Recognition</head><p>For long-tailed recognition, re-balancing methods including re-weighting <ref type="bibr" coords="3,438.31,107.54,16.55,10.91" target="#b13">[14,</ref><ref type="bibr" coords="3,457.97,107.54,14.11,10.91" target="#b14">15]</ref> and resampling <ref type="bibr" coords="3,133.67,121.08,16.56,10.91" target="#b15">[16,</ref><ref type="bibr" coords="3,153.06,121.08,14.11,10.91" target="#b16">17]</ref> are conventional methods to alleviate the imbalance of datasets. However, recent studies find that they may do harm to feature learning. Besides, re-balancing methods are easily over-fitting and under-fitting on the tail and head classes, respectively. Multi-experts models such as BBN <ref type="bibr" coords="3,184.91,161.73,18.06,10.91" target="#b17">[18]</ref> and RIDE <ref type="bibr" coords="3,252.53,161.73,16.41,10.91" target="#b18">[19]</ref>, are also designed to solve long-tailed problem, but these methods have high computation complexity and are hard to optimize when we choose a large-scale pretrained model as backbone. OLTR <ref type="bibr" coords="3,311.34,188.83,18.05,10.91" target="#b19">[20]</ref> is the first work proposed for open-set long-tailed recognition which utilizes extra attention module and memory bank. Therefore, considering the computation and memory cost, in our strategy, we apply LDAM loss on the large-scale pretrained models to fine-tune on the competition dataset, which assigns large margins on the high-frequency classes and small margins on the low-frequency ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Challenge Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The data of FungiCLEF is from Danish Fungi 2020 <ref type="bibr" coords="3,327.53,322.62,16.34,10.91" target="#b20">[21]</ref>, a novel fine-grained dataset which consists of 266,344 images for training and 29,594 images for validation. It contains 1,604 species mainly from the Fungi kingdom with a few visually similar species. While most of images are collected from natural scene, there remains some hand drawn drafts and microscope observations which have a huge domain gap with others, as shown in Figure <ref type="figure" coords="3,434.73,376.82,3.75,10.91" target="#fig_1">1</ref>. In addition to image information and class labels, this dataset provides rich observation metadata in csv files. There are more than 20 kinds covering basic time and geographic localities, full taxonomy labels, substrate and habitat, etc. For some images, not all meta information is available and some are missing. The class frequencies in the dataset follow an extremely unbalanced long-tailed distribution with a maximum 1,913 and a minimum 31, as illustrated in Figure <ref type="figure" coords="3,435.24,444.56,3.69,10.91" target="#fig_3">2</ref>. An additional set of 118,676 images from 3,134 species is used for testing. These images are provided with less metadata (e.g. time stamp, location, substrate, habitat).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Task</head><p>Being a part of LifeCLEF-2022 which aims at biodiversity identification and prediction, FungiCLEF-2022 is an automatic fungi recognition competition, as well as an open-set machine learning problem, which means unknown categories will emerge during test time. Under this circumstances, open-set recognition task is proposed to perform on known classes and reject unknown classes as one class. Meanwhile, Danish Fungi 2020 is a fine-grained dataset with 1,604 fungi species. Small inter-class variances and huge intra-class similarity make it more challenging. Contrast to traditional visual recognition, this task provides rich metadata acquired by citizen-scientists, i.e. only vision models are not sufficient, the combination of metadata and images must be considered. Here we conclude the main difficulties of this completion:</p><p>• Usage of rich metadata; • Extremely unbalanced long-tailed data distribution;</p><p>• Open-set recognition rather than closed-set; • Robust recognition with noise data, e.g. images, hand-drawn drafts and microscopic observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>In this section, we will introduce our    for test set: "eventDate", "month", "day", "countryCode", "Location_lvl0", "Location_lvl3", "Lo-cation_lvl2", "Location_lvl1", "Substrate" and "Habitat". Therefore, to keep the consistency of training and testing, we choose from the above ten metadata dropping time-related ones in consideration of the potential confusion caused by time. Also we replace "countryCode" with full country name. Instead of regarding these metadata as discrete and independent words, we design a description text template with all metadata and replace the missing ones with the word "unknown". For example, if the values of "countryCode", "Location_lvl0", "Location_lvl3", "Loca-tion_lvl2", "Location_lvl1", "Substrate" and "Habitat" are "US", "Mount Olive Baptist Church", "United States", "Texas", "Brazoria", "bark of living trees" and "None", respectively, with our template, the description is: "Its location is Mount Olive Baptist Church, Brazoria, Texas, United States. It lives in bark of living trees. Its habitat is unknown". And the description is taken as the caption of its corresponding image, which is used in Metadata Encoding. Our designed text template eliminates the distractions of missing metadata, adds contextual information and ensures that we can get fixed dimension features for later stages.</p><p>Metadata Encoding. To get deep text embedding efficiently, we employ pre-trained NLP models directly. Intuitively, we use a multilingual BERT <ref type="bibr" coords="5,334.87,620.46,17.99,10.91" target="#b21">[22]</ref>-base model because the location is recorded in Danish. It is pretrained on the top 104 languages with the largest Wikipedia using masked language modeling (MLM) objective. And for each designed template, it generates a 768-dimension feature. Further, we update the multilingual BERT model with RoBERTa <ref type="bibr" coords="5,487.92,661.10,18.06,10.91" target="#b22">[23]</ref>  large model. RoBERTa is a well-trained BERT with some modifications including more epochs, larger batches, more data, etc. It generates a 1,024-dimension feature for each text template which contains more information and can be more representative.</p><p>Meta-Vision Models. We use MetaFormer as our meta-vision backbone to add meta information to improve the fine-grained classification. Metaformer is a hybrid framework which uses convolution to extract deep vision features and uses transformer layers to fuse vision and meta information. The origin MetaFormer design multi-layered fully-connected networks for each metadata to get embedding vector. However, our meta information has been merged as one in a unified text template and encoded by pre-trained NLP models as described in Metadata Preprocessing and Metadta Encoding, respectively. After getting initial text embeddings with a pre-trained NLP model, we apply a single fully-connected layer on them, followed by an activation layer ReLU and layer normalization. Relative transformer layers in MetaFormer are used to fuse visual token, meta token and class token. Like ViT <ref type="bibr" coords="6,381.07,442.69,16.41,10.91" target="#b23">[24]</ref>, only the class token is used for the category prediction.</p><p>Vision-Only Models. While MetaFormer focuses on the fusion of multi-modal information, models only trained on images is of necessity to learn visual-representative deep features. Here we use convolution-based ConvNeXt <ref type="bibr" coords="6,257.99,496.89,17.96,10.91" target="#b24">[25]</ref> and transformer-based Swin Transformer <ref type="bibr" coords="6,467.49,496.89,11.49,10.91" target="#b5">[6]</ref>, both of which are pioneer works in their respective fields. We hope these two different network structures can pay attention to different image patterns, bring new views into learning process and complement each other in the final decision. We adopt vanilla Swin Transformer and ConvNeXt architecture for simplicity.</p><p>To sum up, during closed-set training, the pipeline of meta-vision models and vision-only models are illustrated in Figure <ref type="figure" coords="6,230.87,578.18,3.74,10.91" target="#fig_4">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Long-Tailed Solution</head><p>LDAM Loss <ref type="bibr" coords="6,161.26,628.45,12.87,9.76" target="#b6">[7]</ref>. As analyzed in Section3.1, the dataset shows an extremely unbalanced long-tailed distribution, which will deteriorate the network performance during testing. To alleviate this adverse effect, we train our models with LDAM loss rather than CE loss. LDAM loss enforces a class-aware margin for each class to optimize a uniform-label generalization error bound. It encourages larger margins for minority classes and smaller margins for majority classes. Meanwhile, the inputs of LDAM loss should be normalized by normalizing last hidden activation layer and the weight vectors of last fully-connected layer with L2 norm. We only use LDAM loss on our meta-vision model.</p><p>Hard Classes Mining. We design a hard class mining (HCM) strategy with the accuracy on train and validation set, then augment them with high-resolution data provided by the host. Specifically, we set the threshold on the train set to 80% and classes whose accuracy under 80% are defined as hard classes. Besides, based on the validation set, we only consider the classes whose samples are more than 50, and the threshold is set to 85%. Based on above two principles, we get 83 hard classes. We manually filter corresponding images and remove some dirty cases like too-small target or low-quality images, as shown in Figure <ref type="figure" coords="7,372.27,482.38,3.75,10.91" target="#fig_4">3</ref>. Finally, we complement the rest images with high-resolution ones as provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Data Augmentation</head><p>Including the traditional data augmentation <ref type="bibr" coords="7,299.39,545.26,20.30,10.91" target="#b25">[26]</ref> like random horizontal flip, we also use Mixup <ref type="bibr" coords="7,120.57,558.81,17.82,10.91" target="#b26">[27]</ref> and CutMix <ref type="bibr" coords="7,196.11,558.81,17.82,10.91" target="#b27">[28]</ref> for models' robustness at a probability of 0.4. It should be pointed out that these two data augmentation methods and LDAM loss are not compatible because of the mixed labels. Besides, we use random erase with a probability of 0.2 and Auto Augment (AA) <ref type="bibr" coords="7,113.33,599.46,16.39,10.91" target="#b28">[29]</ref>, which searches for improved data augmentation policies automatically, which are over ShearX/Y, TranslateX/Y, Rotate, AutoContrast, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Open-Set Inference Design</head><p>Our model is trained as a closed-set classification task described as above, therefore, inferencing methods are innovated and well-designed to tackle the open-set challenge that whether test images belong to the "unknown" class. All data used during inference stage are features and prediction scores of training and test sets extracted by our closed-set training models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Class-wise Weighted Prototype Classifier</head><p>Traditional open-set maximum softmax probability (MSP) method only utilizes test prediction scores to judge unknown class probability, lack of using information in train set. Thus, our paper considers a similarity-based method and proposes our Class-wise Weighted Prototype Classifier (CWPC) by constructing class centers using both features and prediction scores of train set. Specifically, we firstly extract features and prediction scores of all images from train set. Then, we utilize them to compute class centers assigning different weights on samples with the same label instead of taking the features of samples equally. We innovate to apply softmax on the maximum prediction scores of all images with the same label to compute the weight of each sample when computing the class center. The weights for samples of each class can be formulated as follows:</p><formula xml:id="formula_0" coords="8,239.27,341.75,266.71,60.25">𝑃 𝑖 = [𝑝 1 , 𝑝 2 , . . . , 𝑝 𝑐 ], 𝑚 𝑖 = 𝑀 𝑎𝑥(𝑃 𝑖 ), 𝑀 𝑐 = [𝑚 1 , 𝑚 2 , . . . , 𝑚 𝑁𝑐 ], 𝑊 𝑐 = 𝑆𝑜𝑓 𝑡𝑚𝑎𝑥(𝑀 𝑐 ) (1)</formula><p>where 𝑃 𝑖 is the prediction score on 𝑖 𝑡ℎ image after Softmax function, 𝑚 𝑖 denotes the maximum prediction score of 𝑖 𝑡ℎ image, 𝑁 𝑐 is the number of images of class 𝑐 in the training set, 𝑀 𝑐 denotes prediction scores of 𝑁 𝑐 images, and 𝑊 𝑐 denotes weights for images of class 𝑐.</p><p>CWPC improves the compactness within each class, resulting in more accurate class center representations and achieving powerful prerequisites on subsequent similarity measurements. Also, as an inference-stage strategy, CWPC consumes negligible computation resources, and can be applied on inference stage of all open-set image classification tasks with great generalization, which we consider as a universal algorithm in open-set challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">ObservationId-awared Weighted Similarity</head><p>To meet the requirements of submission on ObservationId, our paper designs an ObservationId-awared Weighted Similarity (OAWS) to make fusion on images with same Obser-vationId. As CWPC outputs all class centers, OAWS module aims at calculating the similarity between features of ObservationId and each class center. Thus, we first employ fusion strategy on images with the same ObservationId, where different weights are applied on different images.</p><p>The fusion weights for images with the same ObservationId are computed as follows,</p><formula xml:id="formula_1" coords="9,249.40,110.36,93.21,10.63">𝑃 𝑖 = [𝑝 1 , 𝑝 2 , . . . , 𝑝 𝑐 ],</formula><p>𝑚 𝑖 = 𝑀 𝑎𝑥(𝑃 𝑖 ),</p><formula xml:id="formula_2" coords="9,244.89,134.46,261.10,36.15">𝐾 𝑜 = [𝑘 1 , 𝑘 2 , . . . , 𝑘 𝑁𝑜 ], 𝑊 𝑜 = 𝑆𝑜𝑓 𝑡𝑚𝑎𝑥(𝐾 𝑜 ) (2)</formula><p>where 𝑃 𝑖 is prediction score on 𝑖 𝑡ℎ image after Softmax function, 𝑚 𝑖 denotes the maximum prediction score of 𝑖 𝑡ℎ image, 𝐾 𝑜 denotes prediction scores of 𝑁 𝑜 images with the same Obser-vationId, 𝑊 𝑜 denotes weights for images with the same ObservationId.</p><p>Then, cosine similarity is subsequently adopted to measure similarity for final results. Specifically, an adjustable threshold is set, the maximum similarity under which belongs to the "unknown" class on test set. OAWS module not only serves as a special technique on Fungi Challenge, but also has referenced significance for open-set challenge for its novelty on similarity and threshold design, which can be further adjusted to achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Comparsions</head><p>As CWPC and OAWS successfully generalize closed-set training to open-set recognition, achieving prominent improvements on Fungi Challenge, several comparative methods are proposed. First, based on MSP, we calculate the maximum prediction score and set a threshold to judge whether it belongs to the "unknown" class. It should be noted that as the particularity in Fungi Challenge is ObservationId-format result, we calculate the average test prediction score within each ObservationId as follows,</p><formula xml:id="formula_3" coords="9,223.39,415.56,282.59,43.71">𝑃 𝑖 = [𝑝 1 , 𝑝 2 , . . . , 𝑝 𝑐 ], 𝑚 𝑗 = 𝑀 𝑒𝑎𝑛([𝑃 1 , 𝑃 2 , . . . , 𝑃 𝑖 ]), 𝑃 𝑚𝑎𝑥 = 𝑀 𝑎𝑥(𝑆𝑜𝑓 𝑡𝑚𝑎𝑥(𝑚 𝑗 ))<label>(3)</label></formula><p>where 𝑃 𝑖 is prediction score on 𝑖 𝑡ℎ image after Softmax function, 𝑚 𝑗 represents the mean prediction score of 𝑗 𝑡ℎ ObservationId, 𝑃 𝑚𝑎𝑥 denotes the maximum prediction score of 𝑗 𝑡ℎ ObservationId.</p><p>Second, besides CWPC, class centers can be calculated by using three other selection strategies proposed as follows.</p><p>• Average Selection: use average features from all images in the train set to calculate class centers as follows,</p><formula xml:id="formula_4" coords="9,232.64,572.69,273.34,12.19">𝐹 𝑎𝑣𝑒𝑟𝑎𝑔𝑒 𝑗 = 𝑀 𝑒𝑎𝑛([𝑓 1 , 𝑓 2 , . . . , 𝑓 𝑖 ])<label>(4)</label></formula><p>where 𝑓 𝑖 is features of 𝑖 𝑡ℎ image in 𝑗 𝑡ℎ classes. • Filter Selection: use average features from images in the train set whose maximum prediction score is above threshold to calculate class centers. • GT Selection: use average features from images whose predictions are the same as GT to calculate class centers.</p><p>Third, besides OAWS, we apply two other different fusion strategies on test features.</p><p>• Average Fusion: test features per ObservationId are the average features of all images with the same ObservationId. • Filter Fusion: test features per ObservationId are the average features of images whose maximum prediction score is above threshold.</p><p>Fourth, we consider using features of every single image in train set instead of class centers. We calculate the similarity between test features of each ObservationId and features of every single image, and extract the top-1 or top-k prediction using model ensemble strategy in Section. 4.2.4, named as Single-Image Similarity Top1 and Single-Image Similarity Top9.</p><p>Fifth, we conduct OpenMax, an open-set inference strategy based on Extreme Value Theory (EVT), to estimate the probability of an input being from an unknown class. The key element of estimating the unknown probability is adapting Meta-Recognition concepts to the activation patterns in the penultimate layer of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Inference Augmentation</head><p>Test-time Augmentation (TTA). TTA aims at creating multiple enhanced copies of images on test sets, which allows models to make predictions on both original and augmented copies to improve the mean F1 score on the test set. Typical TTA methods such as crop, flip, color jitter are used in Fungi Challenge, where we use random crop with an extension rate of 1.15 on input size, five crop with an additional extension of 32 pixels, horizontal flip with a probability of 1, color jitter with a scope of 0.2, and conduct fusion TTA methods based on above.</p><p>Diverse Model Ensemble. As diverse network architectures, training strategies, data augmentations are proposed to improve the performance of models, the variability and diversity between models greatly differs. In order to take full advantage of the semantic information of different models, model ensemble methods are essential to make improvements on final results, which voting is considered as the easiest and most efficient way. We propose top-1 and top-5 voting strategy on diverse models in Fungi Challenge. Top-1 strategy follows "the minority obeys the majority" rule to find the majority class index as the final result. Top-5 strategy extracts predicted top-5 class index of each model and differently weigh them, and chooses the class index with maximum weight as the final result,</p><formula xml:id="formula_5" coords="10,229.04,509.85,276.95,11.36">𝑊 𝑣𝑜𝑡𝑒 = [1, 1/2, 1/3, 1/4, 1/5]<label>(5)</label></formula><p>where 𝑊 𝑣𝑜𝑡𝑒 is the weight of 𝑇 𝑜𝑝1 to 𝑇 𝑜𝑝5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>We fine-tune our MetaFormer-2 on ImageNet-21K pre-trained models with input resolution 224×224 on 4 Nvidia T4 GPUs and 384×384 input resolution on 4 Nvidia V100 GPUs. AdamW optimizer is employed with a cosine learning rate scheduler. The learning rate is initialized as 2 × 10 -4 for 30 epochs and the first 3 epochs are set for warm-up from 5 × 10 -8 . As for vision-only models, we fine-tune SwinTransformer-Base and ConvNeXt networks on 4 Nvidia </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Result</head><p>We totally train 7 models, two of which are vision-only models and five are meta-vision models. The evaluation metric for this competition is Mean F1-Score, denoted as Macro-F1, and the results are shown in Tab. 6. We conducted these 7 experiments with different training data, multi-scale input size and loss function. Particularly, test set images with pesudo-labels given by diverse classifiers are used to further fine-tune our trained model. These settings ensure the diversity of models during the model ensemble stage which can complement each other to a better result. Finally, we got 6-th place with a result of 81.02% on public leaderboard and 77.58% on private leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Studies</head><p>We conduct ablation studies to demonstrate the effectiveness of our strategies on selection, fusion, similarity, augmentation and ensemble. Tab. 1 proves that CWPC is the best selection strategy in Fungi Challenge. Tab. 2 proves that OAWS is the best fusion strategy in Fungi Challenge. Tab. 3 and Tab. 5 proves that CWPC and OAWS is the best open-set strategy in Fungi Challenge. Tab. 4 proves that fivecrop is the best test-time augmentation strategy in Fungi Challenge. Tab. 6 proves that Top5 voting is the best ensemble strategy in Fungi Challenge and our ensembled model achieves final result of 81.02% on public leaderboard and 77.58% on private leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we propose a novel open-set fine-grained image classification method called Class-wise Weighted Prototype Classifier (CWPC) using extra text information for FungiCLEF-2022 challenge. We decouple all the process into closed-set training and open-set testing. Thus, it can benefit from the numerous advances in closed-set image classification, such as large-scale pre-trained models, label smoothing and data augment. It is also cost-saving to generalize closed-set recognition models to open-set scenarios without any further modification with our methods. Besides, we add extra metadata to improve the performance of fine-grained classification using a hybrid structure where convolution is used to extract deep vision features and transformer is used to fuse vision and metadata embedding. With other long-tailed solution and data augmentation, we got 6-th place in this challenge with a final result of 81.02% on public leaderboard and 77.58% on private leaderboard. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,268.25,174.05,239.42,10.91;4,88.96,187.60,417.02,10.91;4,89.29,201.14,416.70,10.91;4,89.29,214.69,418.54,10.91;4,89.29,228.24,417.90,10.91;4,88.89,241.79,417.10,10.91;4,89.29,255.34,416.69,10.91;4,89.29,268.89,416.69,10.91;4,89.29,282.44,416.70,10.91;4,89.29,295.99,416.69,10.91;4,89.29,309.54,107.91,10.91"><head></head><label></label><figDesc>solution for the open-set fungi recognition challenge. The insight of our solution is to generalize models trained on the closed-set dataset to the open-set scenario without any additional trivial module or extra computation cost on open-set training. Therefore, we decouple the open-set recognition into closed-set training and openset inference, described in Section 4.1 and Section 4.2, respectively. For closed-set training, we utilize the existing closed-set advances and innovate to use metadata with a designed text template and merge multi-modal embeddings in feature space. For open-set inference, we design a Class-wise Weighted Prototype Classifier (CWPC) and the ObservationId-awared Weighted Similarity (OAWS) strategy to generalize closedo-set training models to open-set recognition challenge. Besides, we proposed a weighted Top-5 voting strategy to ensemble diverse models for better performances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,338.97,213.70,10.71;4,89.29,360.24,187.94,9.77;4,100.20,379.83,405.78,10.91;4,89.29,393.38,416.69,10.91;4,89.29,406.93,416.69,10.91"><head>4. 1 .</head><label>1</label><figDesc>Closed-Set Training Improvements 4.1.1. Multi-modal Information Usage Metadata Preprocessing. For training and validation data, more than 20 kinds of metadata are provided including time stamp, geographic localities, full taxonomy labels, substrate and habitat, etc. There are plenty of choices during training, while it only provides 10 metadata</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,444.04,187.06,8.93;4,116.99,459.94,361.30,216.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Samples of fungi challenge dataset.</figDesc><graphic coords="4,116.99,459.94,361.30,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,89.29,90.62,202.84,8.93;5,105.63,106.51,384.01,288.01"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of fungi challenge dataset.</figDesc><graphic coords="5,105.63,106.51,384.01,288.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,89.29,90.62,395.03,8.93;6,108.88,106.52,375.02,150.88"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The pipeline of Meta-vision Models and vision-only models during closed-set training.</figDesc><graphic coords="6,108.88,106.52,375.02,150.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,89.29,90.62,152.60,8.93;7,188.39,106.51,216.00,217.67"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Dirty cases of hard classes.</figDesc><graphic coords="7,188.39,106.51,216.00,217.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,88.93,90.49,417.26,209.51"><head>Table 1</head><label>1</label><figDesc>Results on selection strategies. V100 GPUs for 30 epochs. Both are pre-trained on ImageNet-21K and the pretraining weights are provided by the official. The optimizer, learning rate and scheduler is the same as MetaFormer-2 but no warm-up epochs. We choose SwinTransformer-base and ConvNeXt-base with input resolution 384×384 in balance of computational consumption. The weight decay is 10 -8 for SwinTransformer-base and 2 × 10 -5 for others.</figDesc><table coords="11,170.57,122.10,254.14,86.08"><row><cell>Method</cell><cell>Model</cell><cell>Test Input Macro-F1(%)</cell></row><row><cell>MSP</cell><cell cols="2">MetaFormer 224×224 75.39</cell></row><row><cell cols="3">Average Selection MetaFormer 224×224 75.74</cell></row><row><cell>Filter Selection</cell><cell cols="2">MetaFormer 224×224 75.58</cell></row><row><cell cols="3">Average Selection MetaFormer 384×384 77.39</cell></row><row><cell>GT Selection</cell><cell cols="2">MetaFormer 384×384 77.24</cell></row><row><cell>CWPC</cell><cell cols="2">MetaFormer 384×384 77.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,88.99,90.49,329.47,81.84"><head>Table 2</head><label>2</label><figDesc>Results on fusion strategies.</figDesc><table coords="12,174.32,122.10,244.15,50.22"><row><cell>Method</cell><cell>Model</cell><cell>Test Input Macro-F1(%)</cell></row><row><cell cols="3">Average Fusion MetaFormer 384×384 76.83</cell></row><row><cell>Filter Fusion</cell><cell cols="2">MetaFormer 384×384 76.36</cell></row><row><cell>OAWS</cell><cell cols="2">MetaFormer 384×384 77.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,88.99,194.57,378.64,81.84"><head>Table 3</head><label>3</label><figDesc>Results on similarity strategies.</figDesc><table coords="12,125.15,226.19,342.48,50.22"><row><cell>Method</cell><cell>Model</cell><cell>Test Input Macro-F1(%)</cell></row><row><cell cols="3">Single-Image Similarity Top1 SwinTransformer-Base 384×384 64.98</cell></row><row><cell cols="3">Single-Image Similarity Top9 SwinTransformer-Base 384×384 64.15</cell></row><row><cell>CWPC</cell><cell cols="2">SwinTransformer-Base 384×384 73.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,88.99,298.65,318.07,117.70"><head>Table 4</head><label>4</label><figDesc>Results on test-time augmentation strategies.</figDesc><table coords="12,185.73,330.27,221.33,86.08"><row><cell>Test-Time Augmentation</cell><cell>Macro-F1(%)</cell></row><row><cell>None</cell><cell>77.49%</cell></row><row><cell cols="2">Horizontal flip + Vertical Flip + Origin 77.69</cell></row><row><cell cols="2">Horizontal flip + Color jitter + Origin 77.81</cell></row><row><cell>CenterCrop</cell><cell>77.24</cell></row><row><cell>RandomCrop</cell><cell>77.71</cell></row><row><cell>FiveCrop</cell><cell>78.28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,88.99,438.60,336.62,69.88"><head>Table 5</head><label>5</label><figDesc>Results on overall strategies.</figDesc><table coords="12,167.17,470.22,258.44,38.26"><row><cell>Method</cell><cell>Model</cell><cell>Test Input Macro-F1(%)</cell></row><row><cell>OpenMax</cell><cell cols="2">Convnext-Base 384×384 77.18</cell></row><row><cell cols="3">CWPC + OAWS Convnext-Base 384×384 79.41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="13,88.99,90.49,440.01,221.33"><head>Table 6</head><label>6</label><figDesc>Final results.This work is supported by Chinese National Natural Science Foundation (62076033, U1931202) and MoE-CMCC "Artifical Intelligence" Project No. MCM20190701.</figDesc><table coords="13,89.29,119.88,439.71,154.72"><row><cell>Model</cell><cell cols="5">Input size Train set Val set Pseudo HCM</cell><cell>Loss</cell><cell>Macro-F1(%)</cell></row><row><cell cols="2">MetaFormer MetaFormer MetaFormer MetaFormer MetaFormer Convnext-Base SwinTransformer-Base 384×384 224×224 384×384 384×384 384×384 384×384 384×384</cell><cell>√ √ √ √ √ √ √</cell><cell>√ √ √ √</cell><cell>√ √</cell><cell>√</cell><cell>CE Loss LDAM Loss CE Loss LDAM Loss LDAM Loss CE Loss CE Loss</cell><cell>78.33 79.72 78.28 78.22 79.42 79.81 73.00</cell></row><row><cell>Ensemble Top5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>81.02</cell></row><row><cell cols="2">7. Acknowldgments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,112.66,359.54,394.53,10.91;13,112.66,373.09,393.33,10.91;13,112.66,386.64,393.33,10.91;13,112.66,400.18,168.28,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,185.62,373.09,320.37,10.91;13,112.66,386.64,208.65,10.91">Lifeclef 2022 teaser: An evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,343.19,386.64,162.80,10.91;13,112.66,400.18,38.01,10.91">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,413.73,394.53,10.91;13,112.66,427.28,394.53,10.91;13,112.66,440.83,393.33,10.91;13,112.66,454.38,393.33,10.91;13,112.66,467.93,353.54,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,198.52,440.83,307.47,10.91;13,112.66,454.38,247.50,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,383.00,454.38,122.99,10.91;13,112.66,467.93,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,481.48,393.33,10.91;13,112.66,495.03,395.17,10.91;13,112.66,508.58,232.29,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,339.05,481.48,110.41,10.91;13,480.07,481.48,25.92,10.91;13,112.66,495.03,223.69,10.91">Fungi recognition as an open set classification problem</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,365.49,495.03,142.34,10.91;13,112.66,508.58,201.59,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of FungiCLEF</note>
</biblStruct>

<biblStruct coords="13,112.66,522.13,393.33,10.91;13,112.66,535.68,393.33,10.91;13,112.66,549.23,349.88,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,346.64,522.13,159.35,10.91;13,112.66,535.68,67.31,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvprw.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m" coord="13,204.72,535.68,189.44,10.91;13,428.03,535.68,77.96,10.91;13,112.66,549.23,62.96,10.91">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct coords="13,112.66,562.78,394.53,10.91;13,112.66,576.32,393.33,10.91;13,112.66,589.87,383.04,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,167.27,576.32,251.61,10.91">The inaturalist species classification and detection dataset</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,441.39,576.32,64.60,10.91;13,112.66,589.87,285.12,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,603.42,393.33,10.91;13,112.39,616.97,393.60,10.91;13,112.66,630.52,250.30,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,370.89,603.42,135.10,10.91;13,112.39,616.97,181.28,10.91">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,317.36,616.97,188.63,10.91;13,112.66,630.52,142.26,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,644.07,395.17,10.91;14,112.66,86.97,394.53,10.91;14,112.66,100.52,90.72,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,323.58,644.07,184.25,10.91;14,112.66,86.97,135.85,10.91">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,271.00,86.97,231.34,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1567" to="1578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,114.06,394.52,10.91;14,112.66,127.61,395.01,10.91;14,112.66,141.16,149.51,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,378.28,114.06,124.49,10.91">Toward open set recognition</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Rezende Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2012.256</idno>
	</analytic>
	<monogr>
		<title level="j" coord="14,112.66,127.61,297.19,10.91">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,154.71,393.33,10.91;14,112.66,168.26,341.92,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,222.79,154.71,147.79,10.91">Towards open set deep networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,396.22,154.71,109.77,10.91;14,112.66,168.26,244.01,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1563" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,181.81,394.62,10.91;14,112.66,195.36,394.53,10.91;14,112.66,208.91,90.72,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,270.96,181.81,213.70,10.91">Learning placeholders for open-set recognition</title>
		<author>
			<persName coords=""><forename type="first">D.-W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-C</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,195.36,389.80,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,222.46,394.62,10.91;14,112.66,236.01,394.53,10.91;14,112.66,249.56,65.30,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,182.67,222.46,304.38,10.91">Fine-grained image classification via combining vision and language</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,236.01,364.29,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5994" to="6002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,263.11,393.33,10.91;14,112.66,276.66,393.33,10.91;14,112.66,290.20,262.63,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,458.38,263.11,47.61,10.91;14,112.66,276.66,171.00,10.91">Geo-aware networks for fine-grained recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Potetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,309.82,276.66,196.16,10.91;14,112.66,290.20,194.36,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,303.75,393.53,10.91;14,112.66,317.30,288.50,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02751</idno>
		<title level="m" coord="14,307.65,303.75,198.53,10.91;14,112.66,317.30,106.31,10.91">Metaformer: A unified meta framework for fine-grained recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,330.85,395.17,10.91;14,112.66,344.40,394.52,10.91;14,112.66,357.95,90.72,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,267.00,330.85,240.82,10.91;14,112.66,344.40,16.07,10.91">Learning deep representation for imbalanced classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,151.22,344.40,351.52,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,371.50,393.33,10.91;14,112.66,385.05,239.00,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,280.02,371.50,115.62,10.91">Learning to model the tail</title>
		<author>
			<persName coords=""><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,417.69,371.50,88.30,10.91;14,112.66,385.05,140.70,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,398.60,395.17,10.91;14,112.66,412.15,395.00,10.91;14,112.41,425.70,38.81,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,231.47,398.60,276.36,10.91;14,112.66,412.15,99.46,10.91">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,235.65,412.15,182.77,10.91">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,439.25,394.62,10.91;14,112.66,452.79,327.84,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,202.67,439.25,281.26,10.91">What is the effect of importance weighting in deep learning?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,452.79,207.49,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="872" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,466.34,393.33,10.91;14,112.66,479.89,393.33,10.91;14,112.66,493.44,297.29,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,292.05,466.34,213.94,10.91;14,112.66,479.89,185.56,10.91">Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,321.36,479.89,184.62,10.91;14,112.66,493.44,199.18,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9719" to="9728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,506.99,393.33,10.91;14,112.66,520.54,298.20,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01809</idno>
		<title level="m" coord="14,311.86,506.99,194.13,10.91;14,112.66,520.54,116.12,10.91">Long-tailed recognition by routing diverse distribution-aware experts</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,534.09,393.33,10.91;14,112.66,547.64,393.33,10.91;14,112.66,561.19,184.87,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,348.53,534.09,157.45,10.91;14,112.66,547.64,73.73,10.91">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,209.71,547.64,296.28,10.91;14,112.66,561.19,86.75,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,574.74,394.53,10.91;14,112.66,588.29,393.33,10.91;14,112.66,601.84,392.48,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,112.66,588.29,281.26,10.91">Danish fungi 2020-not just another image recognition dataset</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Laessøe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Frøslev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,420.58,588.29,85.40,10.91;14,112.66,601.84,294.59,10.91">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1525" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,615.39,393.33,10.91;14,112.66,628.93,363.59,10.91" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="14,353.43,615.39,152.55,10.91;14,112.66,628.93,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,642.48,394.53,10.91;14,112.30,656.03,393.68,10.91;14,112.66,669.58,107.17,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="14,173.53,656.03,256.77,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,86.97,395.16,10.91;15,112.66,100.52,395.17,10.91;15,112.66,114.06,349.55,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m" coord="15,330.08,100.52,177.76,10.91;15,112.66,114.06,167.84,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,127.61,394.61,10.91;15,112.66,141.16,394.53,10.91;15,112.66,154.71,100.87,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="15,383.60,127.61,103.81,10.91">A convnet for the 2020s</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,112.66,141.16,389.80,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,168.26,393.33,10.91;15,112.66,181.81,350.03,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="15,219.03,168.26,211.50,10.91">Torchvision the machine-vision package of torch</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,452.94,168.26,53.05,10.91;15,112.66,181.81,251.70,10.91">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1485" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,195.36,395.17,10.91;15,112.66,208.91,197.93,10.91" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m" coord="15,331.02,195.36,176.81,10.91;15,112.66,208.91,16.17,10.91">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,222.46,393.32,10.91;15,112.66,236.01,393.32,10.91;15,112.66,249.56,233.71,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="15,329.08,222.46,176.90,10.91;15,112.66,236.01,182.01,10.91">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,317.28,236.01,188.70,10.91;15,112.66,249.56,136.06,10.91">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,263.11,393.53,10.91;15,112.66,276.66,393.33,10.91;15,112.66,290.20,240.15,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="15,399.43,263.11,106.76,10.91;15,112.66,276.66,178.45,10.91">Online hyper-parameter learning for auto-augmentation strategy</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,313.96,276.66,192.03,10.91;15,112.66,290.20,142.26,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6579" to="6588" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
