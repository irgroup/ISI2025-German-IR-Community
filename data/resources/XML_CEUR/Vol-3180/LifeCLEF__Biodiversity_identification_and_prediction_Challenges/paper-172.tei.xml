<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,378.08,15.42;1,89.29,106.66,147.02,15.42">Image-based plant identification with taxonomy aware architecture</title>
				<funder ref="#_5yn6aZs">
					<orgName type="full">UM-IIRG</orgName>
				</funder>
				<funder>
					<orgName type="full">Universiti Malaya</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.10,134.97,67.13,11.96"><forename type="first">Jack</forename><forename type="middle">Min</forename><surname>Ong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CISiP</orgName>
								<orgName type="institution">Universiti Malaya</orgName>
								<address>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,168.87,134.97,61.81,11.96"><forename type="first">Sze</forename><forename type="middle">Jue</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CISiP</orgName>
								<orgName type="institution">Universiti Malaya</orgName>
								<address>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">CVSSP</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.33,134.97,65.53,11.96"><forename type="first">Kam</forename><forename type="middle">Woh</forename><surname>Ng</surname></persName>
						</author>
						<author role="corresp">
							<persName coords="1,339.85,134.97,80.60,11.96"><forename type="first">Chee</forename><forename type="middle">Seng</forename><surname>Chan</surname></persName>
							<email>cs.chan@um.edu.my</email>
							<affiliation key="aff0">
								<orgName type="department">CISiP</orgName>
								<orgName type="institution">Universiti Malaya</orgName>
								<address>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,378.08,15.42;1,89.29,106.66,147.02,15.42">Image-based plant identification with taxonomy aware architecture</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">588EBB18F0BEAA53F56950F654CE1F6A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>plant identification</term>
					<term>taxonomy aware</term>
					<term>convolutional neural networks</term>
					<term>fine-grained image recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our approach for LifeCLEF Plant Identification Task 2022. Chan's Temple's solution was able to achieve a mean reciprocal rank (MRR) of 0.5104 using an ensemble of ResNets and 0.4880 with a single ResNet34. Our work shows that taxonomy-aware training schemes can offer a better performance gains over training schemes that do not utilize the taxonomy information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It is estimated that there are about 374,000 described and documented plant species <ref type="bibr" coords="1,474.27,348.70,11.58,10.91" target="#b0">[1]</ref>. As such, a solution that is capable to automatically classify and retrieve specific plant species from images will greatly accelerate the work of ecologists working on important domains such as agriculture, construction or pharmacopoeia.</p><p>The LifeCLEF <ref type="bibr" coords="1,165.90,402.89,12.99,10.91" target="#b1">[2]</ref> Plant identification task 2022 aimed to encourage work towards such a solution by introducing a fine-grained image classification task containing 80,000 plant species with a well annotated dataset of more than 2 million images <ref type="bibr" coords="1,359.63,429.99,11.44,10.91" target="#b2">[3]</ref>. The images are also grouped into observations where each plant observation can contain multiple images of the plant, usually of different parts, to better mimic the images that would be taken by an ecologist in the field (behavior).</p><p>For our approach, we trained a few residual networks (i.e., ResNet <ref type="bibr" coords="1,385.25,484.19,11.90,10.91" target="#b3">[4]</ref>) using different methods of incorporating the taxonomy information into the learning objective. Our idea is that the taxonomy information will alleviate some of the class sparsity issues with optimization on large number of classes. It might also be reasonable to think that when ecologists are searching for observations or using a model for identification in the field, they would also look for those species that are close to the target species. If the model is unable to make a correct prediction, then predicting species that are closer in the taxonomy is thus desirable. As such, using training schemes that incorporate taxonomy into the loss function would yield models that are more likely to predict species that are closer in the taxonomy when making an incorrect prediction which is better than predicting a species that is far away in the taxonomy <ref type="bibr" coords="2,419.50,330.47,11.43,10.91" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The methodology that used multiple classification heads for hierarchical labelled data has been explored for standard computer vision image recognition benchmarks like CIFAR100 <ref type="bibr" coords="2,473.70,402.65,12.94,10.91" target="#b5">[6]</ref> and ImageNet <ref type="bibr" coords="2,135.00,416.20,12.82,10.91" target="#b6">[7]</ref> in <ref type="bibr" coords="2,162.10,416.20,11.41,10.91" target="#b4">[5]</ref>. However, our method use residual networks <ref type="bibr" coords="2,379.62,416.20,12.82,10.91" target="#b3">[4]</ref> as the backbone network in contrast with <ref type="bibr" coords="2,163.73,429.75,12.93,10.91" target="#b4">[5]</ref> which uses VGG <ref type="bibr" coords="2,256.66,429.75,12.93,10.91" target="#b7">[8]</ref> as their backbone network. It is shown that residual network has a better performance as well as being much easier to optimize <ref type="bibr" coords="2,425.55,443.30,11.36,10.91" target="#b8">[9,</ref><ref type="bibr" coords="2,439.64,443.30,12.32,10.91" target="#b9">10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>For all the trained models, we use ResNet models with the last fully connected layer replaced with a new fully connected layer initialized with kaiming initialization <ref type="bibr" coords="2,383.41,515.47,17.90,10.91" target="#b10">[11]</ref> with output dimension equal to the number of classes in the target taxonomy. The networks are initialized with weights pre-trained on ImageNet <ref type="bibr" coords="2,199.20,542.57,11.28,10.91" target="#b6">[7]</ref>. The learning process involves 2 steps. We first pretrain the models with classification heads from other taxonomical levels. The species classification head is then used to finalize the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Family Pre-training</head><p>Our first approach was to pre-train the model on classifying 483 plant families in the trusted dataset. We then swap out the last fully connected layer to a new one that has an output dimension of 80,000. This is to correspond to the number of species. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multihead Pre-training</head><p>The second approach was to pre-train the model on classifying all the plant taxonomies at the same time. We then remove the other classification heads, and only leave the head corresponding to the species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>There were 2 datasets provided for the task, a trusted dataset and a web dataset. The trusted dataset contained 2, 886, 761 images of 80, 000 plant species from academic sources with higher certainty of the class labels being correct. The web dataset contained 1, 071, 627 images of 57, 314 plant species from online sources. The web dataset was semi-automatically revised by the competition authors to reduce the number of errors and duplication.</p><p>For our approach, we only trained on the trusted dataset as it contains all the classes and is less noisy compared to the web dataset. We used splits of the web dataset, which were </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data Augmentation</head><p>For our submissions, two different pipelines of image augmentation were used. The first pipeline was a random resized crop of size 224 x 224. The second pipeline was a resize of size 224 x 224 with deformation of the content. The random resized crop was used for the pre-training epochs. The resized crop will then be used for the last epochs and for submission generation.</p><p>We did not use any normalization as our initial experiments showed that the normalization has no effect on the model performance after one epoch of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Target Mapping</head><p>In order to keep the class mappings consistent, the classes in each taxonomy of the trusted dataset were sorted and then being assigned an integer index based on their lexicographic ranking. These taxonomies to integer mappings are saved in a file and used by the data loaders and submission generation code to keep track of which integer corresponds to which class and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Hyperparameters</head><p>The hyperparameters used for training all the models are summarized in Table <ref type="table" coords="4,427.87,623.64,3.66,10.91" target="#tab_0">1</ref>. The parameters that differ are summarized in Table <ref type="table" coords="4,248.35,637.19,3.74,10.91">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Validation and Submission Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Web-1-per-species</head><p>In order to evaluate the performance of our models without using up the submission limits, we created a split of the web dataset containing 1 image from each species. The reason for this is that task will use a version of the mean reciprocal rank (MRR) that is weighted by the species.</p><p>We only took 1 because many of the species only have 1 image present in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Prediction aggregation by performance ranking</head><p>The test dataset contained multiple images of different parts of the same plant observation. However, the models were trained to predict on individual images. We thus need a method to aggregate the predictions made on the individual images for each observation.</p><p>Of the methods tried, we found that sorting the predictions based on the number of times they appear in the top 30 of the images and then by the part with the best individual performance (3) yielded the best performance on the competition evaluation.</p><p>The aggregation is also extended without needing any modifications to aggregate the predictions for the ensemble model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>The results of our internal evaluations and submitted results are summarized in Table <ref type="table" coords="5,477.72,512.06,3.79,10.91" target="#tab_2">4</ref>. The number of resize train epochs in Table <ref type="table" coords="5,262.85,525.61,5.07,10.91">2</ref> were determined by the epoch that achieved the best performance on the internal evaluations before the MRR began to drop.</p><p>Interestingly, the models with pretraining on the random resized crop did not benefit much from the training on the resized dataset for the species prediction task. They however do get an initial performance boost from the changes in augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we presented our approach of using the plant taxonomy information to train models for fine-grained image classification in the LifeCLEF Plant identification task 2022. Our results showed that incorporating the taxonomy information will improve the performance of the models on the plant species identification task. It also shows that combining the prediction from multiple models is able to minimize some of the variance in the models, as well as improving the performance but at the cost of increased computational cost.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,267.37,416.69,8.93;2,89.29,279.37,267.19,8.87;2,130.96,84.18,333.37,176.60"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Family pre-training procedure. We first pre-train the model on classifying the 483 plant families and then fine-tune the model to the 80,000 plant species.</figDesc><graphic coords="2,130.96,84.18,333.37,176.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,400.12,416.69,8.93;3,89.29,412.13,416.69,8.87;3,89.29,424.08,43.27,8.87;3,130.96,84.19,333.36,308.51"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Multihead pre-training architecture. We first pre-train the model on classifying all the plant taxonomies and then fine-tune the model to 80,000 plant species, similar to the family pre-train procedure.</figDesc><graphic coords="3,130.96,84.19,333.36,308.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,385.49,259.39"><head>Table 1</head><label>1</label><figDesc>Training hyperparameters used by all models trained</figDesc><table coords="4,88.99,122.10,385.49,227.78"><row><cell></cell><cell>Parameter</cell><cell>Value</cell><cell></cell></row><row><cell></cell><cell>Batch Size</cell><cell>128</cell><cell></cell></row><row><cell cols="2">Input Image Size</cell><cell>224 x 224 x 3</cell><cell></cell></row><row><cell></cell><cell>Optimizer</cell><cell>Adam [12]</cell><cell></cell></row><row><cell></cell><cell>Learning Rate</cell><cell>10 -4</cell><cell></cell></row><row><cell></cell><cell>Loss Function</cell><cell>Softmax Cross Entropy</cell><cell></cell></row><row><cell>Table 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Differing hyperparameters used by models trained</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Pretrain epochs Classid train epochs Resize train epochs</cell></row><row><cell>ResNet34 + Family</cell><cell>20</cell><cell>10</cell><cell>1</cell></row><row><cell>ResNet34 + Multihead</cell><cell>20</cell><cell>0</cell><cell>3</cell></row><row><cell>ResNet50</cell><cell>0</cell><cell>0</cell><cell>15</cell></row><row><cell>ResNet50-Wide</cell><cell>0</cell><cell>0</cell><cell>10</cell></row><row><cell cols="3">unobserved during training, for internal evaluation.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,90.49,353.18,105.74"><head>Table 3</head><label>3</label><figDesc>Performance by part with Resnet34+ Family Pretrain model</figDesc><table coords="5,153.10,122.10,289.07,74.12"><row><cell>Part</cell><cell cols="2">MRR on Web-1-per-species Occurrences in Trusted dataset</cell></row><row><cell>flower</cell><cell>0.2637</cell><cell>72,509</cell></row><row><cell>habit</cell><cell>0.1910</cell><cell>12,718</cell></row><row><cell>leaf</cell><cell>0.1722</cell><cell>42,414</cell></row><row><cell>fruit</cell><cell>0.1477</cell><cell>9,702</cell></row><row><cell>bark</cell><cell>0.1232</cell><cell>13,463</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,90.49,376.31,127.43"><head>Table 4</head><label>4</label><figDesc>Model results</figDesc><table coords="6,129.97,119.88,335.33,98.03"><row><cell>Model</cell><cell cols="2">Web-1-per-species Competition Submission</cell></row><row><cell>Resnet34 + Family</cell><cell>0.2458</cell><cell>0.49994</cell></row><row><cell>Resnet34 + Multihead</cell><cell>0.2370</cell><cell>0.47447</cell></row><row><cell>Resnet50</cell><cell>0.2089</cell><cell>NA</cell></row><row><cell>Resnet50-Wide</cell><cell>0.2019</cell><cell>NA</cell></row><row><cell>Ensemble of 4 models (3rd Place)</cell><cell></cell><cell>0.51043</cell></row><row><cell>Neuon AI's Model (2nd Place)</cell><cell></cell><cell>0.60781</cell></row><row><cell>Mingle Xu's Model (1st Place)</cell><cell></cell><cell>0.62692</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research is supported by the <rs type="funder">UM-IIRG</rs> (<rs type="grantNumber">IIRG006C-19FNW</rs>) from <rs type="funder">Universiti Malaya</rs>, while the resources for this project were provided by the <rs type="institution">Universiti Malaya Data Intensive Computing Centre (DICC)</rs>, as well as the <rs type="institution">Centre of Image and Signal Processing (CISiP) from Faculty of Computer Science and Information Technology, Universiti Malaya</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5yn6aZs">
					<idno type="grant-number">IIRG006C-19FNW</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,429.62,393.33,10.91;6,112.66,443.17,352.93,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,227.50,429.62,278.49,10.91;6,112.66,443.17,34.48,10.91">The number of known plant species in the world and its annual increase</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Christenhusz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Byng</surname></persName>
		</author>
		<idno type="DOI">10.11646/phytotaxa.261.3.1</idno>
	</analytic>
	<monogr>
		<title level="j" coord="6,155.38,443.17,45.23,10.91">Phytotaxa</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="page" from="201" to="217" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,456.72,394.53,10.91;6,112.66,470.27,394.53,10.91;6,112.66,483.81,393.33,10.91;6,112.66,497.36,393.33,10.91;6,112.66,510.91,353.54,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,198.52,483.81,307.47,10.91;6,112.66,497.36,247.50,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,383.00,497.36,122.99,10.91;6,112.66,510.91,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,524.46,393.33,10.91;6,112.66,538.01,393.32,10.91;6,112.66,551.56,57.08,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,237.09,524.46,268.90,10.91;6,112.66,538.01,60.82,10.91">Overview of PlantCLEF 2022: Image-based plant identification at global scale</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,195.99,538.01,309.99,10.91;6,112.66,551.56,26.38,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,565.11,393.32,10.91;6,112.66,578.66,361.84,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,254.09,565.11,198.80,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,484.43,565.11,21.56,10.91;6,112.66,578.66,290.78,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,592.21,395.17,10.91;6,112.66,605.76,393.32,10.91;6,112.66,619.31,320.03,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,435.13,592.21,72.70,10.91;6,112.66,605.76,339.77,10.91">Hd-cnn: Hierarchical deep convolutional neural networks for large scale visual recognition</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,484.21,605.76,21.77,10.91;6,112.66,619.31,238.82,10.91">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="2740" to="2748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,632.86,393.33,10.91;6,112.33,646.41,67.57,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<title level="m" coord="6,187.02,632.86,246.22,10.91">Learning multiple layers of features from tiny images</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,659.95,394.53,10.91;7,112.28,86.97,393.71,10.91;7,112.66,100.52,321.86,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,331.70,86.97,174.28,10.91;7,112.66,100.52,40.23,10.91">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,161.28,100.52,184.23,10.91">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,114.06,393.33,10.91;7,112.66,127.61,178.68,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,247.99,114.06,258.00,10.91;7,112.66,127.61,49.16,10.91">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,141.16,393.33,10.91;7,112.66,154.71,395.01,10.91;7,112.66,168.26,264.57,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,404.19,141.16,101.80,10.91;7,112.66,154.71,264.00,10.91">The shattered gradients problem: If resnets are the answer, then what is the question?</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Frean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1702.08591" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,181.81,393.33,10.91;7,112.66,195.36,393.33,10.91;7,112.41,208.91,91.35,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,293.70,181.81,212.29,10.91;7,112.66,195.36,104.67,10.91">Norm-preservation: Why residual networks can become extremely deep?</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zaeemzadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Rahnavard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,225.48,195.36,280.51,10.91">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="3980" to="3990" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,222.46,393.33,10.91;7,112.66,236.01,393.53,10.91;7,112.30,249.56,143.57,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,263.49,222.46,242.50,10.91;7,112.66,236.01,169.70,10.91">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,312.49,236.01,193.70,10.91;7,112.30,249.56,62.37,10.91">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,263.11,393.33,10.91;7,112.33,276.66,29.19,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="7,238.21,263.11,167.55,10.91">A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
