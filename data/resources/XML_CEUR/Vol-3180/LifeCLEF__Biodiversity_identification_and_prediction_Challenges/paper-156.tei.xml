<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,378.50,15.42;1,89.29,106.66,307.40,15.42">Overview of SnakeCLEF 2022: Automated Snake Species Identification on a Global Scale</title>
				<funder ref="#_PJ4DHRn">
					<orgName type="full">Ministry of Education, Youth and Sports of the Czech Republic</orgName>
				</funder>
				<funder ref="#_QtDH8ZH">
					<orgName type="full">Technology Agency of the Czech Republic</orgName>
				</funder>
				<funder>
					<orgName type="full">Florida Gulf Coast University Office of Scholarly Innovation and Student Research</orgName>
				</funder>
				<funder ref="#_umsqsAA">
					<orgName type="full">UWB</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,134.97,58.19,11.96"><forename type="first">Lukáš</forename><surname>Picek</surname></persName>
							<email>picekl@kky.zcu.cz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Cybernetics</orgName>
								<orgName type="department" key="dep2">Faculty of Applied Sciences</orgName>
								<orgName type="institution">University of West Bohemia</orgName>
								<address>
									<country>Czechia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,160.12,134.97,58.89,11.96"><forename type="first">Marek</forename><surname>Hrúz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Cybernetics</orgName>
								<orgName type="department" key="dep2">Faculty of Applied Sciences</orgName>
								<orgName type="institution">University of West Bohemia</orgName>
								<address>
									<country>Czechia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.66,134.97,87.88,11.96"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Biological Sciences</orgName>
								<orgName type="institution">Florida Gulf Coast University</orgName>
								<address>
									<settlement>Florida</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.54,134.97,68.40,11.96"><forename type="first">Isabelle</forename><surname>Bolon</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute of Global Health</orgName>
								<orgName type="department" key="dep2">Department of Community Health and Medicine</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,378.50,15.42;1,89.29,106.66,307.40,15.42">Overview of SnakeCLEF 2022: Automated Snake Species Identification on a Global Scale</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">05BEFDD2D10F16AFAFB3DD9200DF2582</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>SnakeCLEF</term>
					<term>fine grained visual categorization</term>
					<term>global health</term>
					<term>epidemiology</term>
					<term>snake bite</term>
					<term>snake</term>
					<term>reptile</term>
					<term>benchmark</term>
					<term>biodiversity</term>
					<term>species identification</term>
					<term>machine learning</term>
					<term>computer vision</term>
					<term>classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The main goal of the third year of the SnakeCLEF challenge was to provide an evaluation platform that helps track the performance of AI-driven methods for snake species recognition systems on a global scale and allows direct comparison with human experts. We ran two challenges separately for humansexperts and novices -and AI methods in order to lay the groundwork for future comparison between human and machine-based snake species identification. We have provided 187,129 snake observations with 318,532 photographs -270,251 for training and 48,281 for testing -of 1,572 snake species collected in 208 countries. The human performance evaluation was conducted on a tailored subset with 150 images derived from the full test set. We report (i) a description of the provided data, (ii) evaluation methodology and principles, (iii) an overview of the methods submitted by the participating teams, and (iv) a discussion of the obtained results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A robust image-based identification system for snake species is an important goal for biodiversity, conservation, and global health. With over half a million victims of death and disability from venomous snakebite annually, such a system could significantly improve eco-epidemiological data and treatment outcomes (e.g. selection of specific antivenoms) <ref type="bibr" coords="1,395.65,479.01,11.48,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,409.89,479.01,7.65,10.91" target="#b1">2]</ref>. Importantly, most herpetological expertise and most snake images are concentrated in developed countries where snake diversity is relatively low and snakebite is not a major public health concern. In contrast, remote parts of developing countries tend to lack expertise and images, even in areas where snake diversity is high and snakebites are common <ref type="bibr" coords="1,323.83,533.21,11.48,10.91" target="#b2">[3,</ref><ref type="bibr" coords="1,338.16,533.21,7.65,10.91" target="#b3">4]</ref>. Thus, snake species identification assistance has a bigger potential to save lives in areas with the least information.</p><p>A primary difficulty of snake species identification lies in the high intra-class and low interclass variance in appearance, which may depend on geographic location, color morph, sex, or age. At the same time, many species are visually similar to other species, i.e., mimicry (Figure <ref type="figure" coords="1,497.16,587.40,3.50,10.91" target="#fig_0">1</ref>). Furthermore, our knowledge of which snake species occur in which countries is incomplete, and it is common that most or all images of a given snake species might originate from a small handful of countries or even a single country. Furthermore, many snake species resemble species found on other continents, with which they are entirely allopatric. Incorporating metadata on the geographic origin of an unidentified snake almost always narrows down the possible correct identifications considerably because only about 125 of the approximately 3,900 snake species co-occur in any given location <ref type="bibr" coords="2,224.48,168.26,11.31,10.91" target="#b4">[5]</ref>. It is known that more widespread species with more images are over-predicted relative to rare species with few images <ref type="bibr" coords="2,356.61,181.81,11.56,10.91" target="#b5">[6]</ref>, and this can be a particularly vexing problem when trying to predict the identity of species that are widespread across areas of the world with few images.</p><p>The main goal of the SnakeCLEF 2022 competition was to provide a reliable evaluation ground for automatic snake species recognition. Like other LifeCLEF competitions, the SnakeCLEF 2022 competition was hosted on Kaggle<ref type="foot" coords="2,240.20,261.35,3.71,7.97" target="#foot_0">1</ref> primarily to attract machine learning experts to participate and present their ideas.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Dataset</head><p>For this year, the dataset used in previous editions <ref type="bibr" coords="3,325.43,494.68,11.48,10.91" target="#b6">[7,</ref><ref type="bibr" coords="3,340.49,494.68,9.03,10.91" target="#b7">8]</ref> has been extended with new and rare species. The number of species was doubled and the number of images from remote geographic areas with no or just a few samples was increased considerably, i.e., the uneven species distributions across all the countries was straightened. The SnakeCLEF 2022 dataset is based on 187,129 snake observations -including some instances of multiple images of the same individual (refer to Figure <ref type="figure" coords="3,228.32,562.42,4.08,10.91" target="#fig_1">2</ref>) -with 318,532 photographs belonging to 1,572 snake species and observed in 208 countries. The dataset has a heavy long-tailed class distribution, where the most frequent species (Natrix natrix) is represented by 6,472 images and the least frequent species just by 5 samples. The difference in the number of images between the species with the most and fewest was reduced by an order of magnitude relative to SnakeCLEF2021. All the data were gathered from the online biodiversity platform iNaturalist<ref type="foot" coords="3,365.20,628.41,3.71,7.97" target="#foot_1">2</ref> . Additional dataset parameters and comparison with previous editions are listed in Table <ref type="table" coords="3,346.55,643.72,3.74,10.91" target="#tab_0">1</ref>. Large countries in the tropics (Brazil, Mexico, Colombia, India, and Indonesia) have more than 300 species. Right: Percentage of snake species per country included in the SnakeCLEF 2022 dataset. The countries with adequate species coverage are those from Europe, Oceania, and North America, i.e., the countries with the smallest diversity.</p><p>For testing, two sets were created: (i) the full test set for a machine evaluation, with 48,280 images from 28,431 observations, and (ii) the subset from the full test set with 150 observations, tailored for the human performance evaluation. Unlike in other LifeCLEF competitions, where the final testing set remained undisclosed, we provided the test data without labels to the participants. To prevent over fitting to the leaderboard, the evaluation method was composed of two stages; the first being the public leaderboard where the user scores were calculated on an unknown 20% of the test set, and the second a private leaderboard where participants were scored on the remaining part of the test set. In addition to image data, we provide:</p><p>• human verified species labels that allow up-scaling to higher taxonomic ranks, • the country-species mapping file describing country-species presence to allow better regularization towards all geographical locations, based on The Reptile Database <ref type="bibr" coords="4,471.90,442.37,11.31,10.91" target="#b8">[9]</ref>, and • information about endemic species -species that occur only in one geographical region, e.g., Australia or Madagascar.</p><p>Geographical information, i.e., state/province and country labels, was included for approximately 95% of the training and test images. Additionally, we provide a mapping matrix (MM 𝑐𝑠 ) describing country-species presence to allow better worldwide regularization.</p><formula xml:id="formula_0" coords="4,206.71,543.16,299.27,30.47">MM 𝑐𝑠 = {︃ 1 if species S ∈ country C, 0 otherwise.<label>(1)</label></formula><p>Unlike last year's dataset, where the vast majority (77%) of all images came from the United States and Canada, the SnakeCLEF 2022 dataset includes just a fraction of the data (28.3%) from the United States and Canada. The rest of the data is distributed across remaining regions, e.g., Europe, Asia, Africa, Australia and Oceania. This was achieved by limiting the number of observations per species in a given country to 400. The estimated worldwide snake distribution and their coverage is visualized in Figure <ref type="figure" coords="4,273.55,650.58,3.74,10.91" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Timeline</head><p>The SnakeCLEF 2022 competition was announced together with the data in late February 2022 through the LifeCLEF, Kaggle, and FGVC challenge pages. Anyone with research ambitions was allowed to register and participate in the competition. The test data were provided jointly with the training data allowing continuous evaluation. Each team could submit up to 2 submissions a day. The competition deadline was May 16, setting the competition for roughly three months. Participants had to submit CSV files containing the Top1 prediction for each snake observation. Once the submission phase was closed (mid-May), the participants were allowed to submit post-competition submissions to evaluate their ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Evaluation Protocol</head><p>The main goal of this challenge was to build a system that is capable of recognizing 1,572 snake species based on the given snake observation -unseen set of images -and relevant geographical location. As a main metric, we use the macro F1 score (F 𝑚 1 ). The F 𝑚 1 is defined as the mean of class-wise F1 scores:</p><formula xml:id="formula_1" coords="5,205.01,316.58,297.11,33.58">F 𝑚 1 = 1 𝑁 𝑁 ∑︁ 𝑠=0 𝐹 1𝑠 , 𝐹 1𝑠 = 2 × 𝑃 𝑠 × 𝑅 𝑠 𝑃 𝑠 + 𝑅 𝑠 , (<label>2</label></formula><formula xml:id="formula_2" coords="5,502.13,327.47,3.86,10.91">)</formula><p>where 𝑠 is species index, 𝑁 equals to the number of classes in a training set. The F1 score for each class represents harmonic mean of the class precision 𝑃 𝑠 and recall 𝑅 𝑠 . This type of evaluation is suitable for data with long-tail distribution, since the quantity of samples from individual classes does not effect the outcome. For the additional evaluation of the performance of the teams we also compute the micro classification accuracy, which is a ratio between the number of correctly classified samples and all samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Working Notes</head><p>All participants were asked to provide code and a Working Note paper -a technical report with information needed to reproduce the results of all submissions. All submitted Working Notes were reviewed by 2-3 reviewers with a decent publication history in the field of Computer Vision and Machine Learning, ensuring a sufficient reproducibility and quality. The review process was single-blind and offered up to two rebuttals. The acceptance rate was 66.66%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Results</head><p>The best performing team achieved F 𝑚 1 of 86.47% on the private part of the test set and 94.01% classification accuracy on the full test set. Both scores are the best ones in the given category. We observe a steady decrease in the F 𝑚 1 score for the first three teams achieving 84.87% and 82.65% F 𝑚 1 score. Then there are several significant drops in performance. Similar behavior is present in the classification accuracy with the second team achieving 93.29% and the third one achieving 93.53%. It can be seen that the accuracy is not correlated with the F 𝑚 1 score which suggests that some teams did not cope well with the long tail distribution. Further performance evaluation for Top-25 teams is provided in Figure <ref type="figure" coords="6,310.67,100.52,3.74,10.91">4</ref>.</p><p>On the expert set, the best performing team -4 th in terms of F 𝑚 1 score -achieved impressive accuracy of 94.67%. Eight teams achieved satisfactory accuracy on the expert set, around 90%. Further performance evaluation for the Top-25 teams is provided in Figure <ref type="figure" coords="6,423.11,141.16,3.74,10.91" target="#fig_3">5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GG</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Participants and Methods</head><p>A total of 31 teams participated in the SnakeCLEF 2022 challenge and submitted 676 submissions. Everyone who submitted a solution better than baseline submission, i.e., random predictions, was considered a participant. The number of participants quadrupled since last year, primarily because Kaggle was used as an evaluation platform. More details can be found in the individual working notes of participants <ref type="bibr" coords="7,224.15,161.73,16.52,10.91" target="#b9">[10,</ref><ref type="bibr" coords="7,243.40,161.73,12.58,10.91" target="#b10">11,</ref><ref type="bibr" coords="7,258.72,161.73,12.58,10.91" target="#b11">12,</ref><ref type="bibr" coords="7,274.03,161.73,12.58,10.91" target="#b12">13,</ref><ref type="bibr" coords="7,289.35,161.73,12.58,10.91" target="#b13">14,</ref><ref type="bibr" coords="7,304.66,161.73,14.09,10.91" target="#b14">15]</ref> who passed the review process, ensuring a sufficient level of reproducibility and quality. The main outcomes we can derive from the achieved results are as follows:</p><p>GG <ref type="bibr" coords="7,107.40,215.93,16.56,10.91" target="#b9">[10]</ref>: The winner of the challenge. They have introduced a novel architecture CoLKA-Net based on VAN <ref type="bibr" coords="7,174.85,229.48,18.07,10.91" target="#b15">[16]</ref> (Visual Attention Network) and CoAtNet <ref type="bibr" coords="7,384.91,229.48,16.41,10.91" target="#b16">[17]</ref>. It is a combination of large kernel attention and vision transformer. In an ablation study, the model outperforms other tested models -ViT <ref type="bibr" coords="7,209.32,256.58,16.30,10.91" target="#b17">[18]</ref>, Swin <ref type="bibr" coords="7,256.69,256.58,16.30,10.91" target="#b18">[19]</ref>, VOLO <ref type="bibr" coords="7,309.60,256.58,16.30,10.91" target="#b19">[20]</ref>, and ConvNeXt <ref type="bibr" coords="7,401.09,256.58,17.96,10.91" target="#b20">[21]</ref> by around 2 points of F 𝑚 1 score (78.00% to 80.10%). In addition, the team used techniques such as Label Aware Smoothing <ref type="bibr" coords="7,140.70,283.68,16.42,10.91" target="#b21">[22]</ref>, Pseudo labelling for tail classes, FixRes mitigation <ref type="bibr" coords="7,394.22,283.68,16.41,10.91" target="#b22">[23]</ref>, and augmentations. When TrivialAugment <ref type="bibr" coords="7,190.22,297.22,17.83,10.91" target="#b23">[24]</ref> was deployed during the middle stage of experimentation the team observed a rise in F 𝑚 1 of around 0.5%. Progressively, Random Erasing <ref type="bibr" coords="7,407.90,310.77,16.42,10.91" target="#b24">[25]</ref>, CutMix <ref type="bibr" coords="7,467.97,310.77,18.07,10.91" target="#b25">[26]</ref> and Mixup <ref type="bibr" coords="7,120.59,324.32,18.07,10.91" target="#b26">[27]</ref> were added which helped with regularization. The final submission score was achieved by an ensemble of six models 2× ConvNeXt, VOLO, CoLKANet, Swin, and ViT. The novel CoLKANet is an interesting contribution with potential outside of the scope of this competition. <ref type="bibr" coords="7,165.42,392.07,16.55,10.91" target="#b10">[11]</ref>: The runner-up focused solely on ViT models, ensembling three different variants; two Large models trained on different resolutions (384; 432) and one Huge model (392). The observed trend from the ablation study is that larger models with higher input resolution perform better. The ViT models were pretrained as Masked autoencoders <ref type="bibr" coords="7,473.43,432.72,18.07,10.91" target="#b27">[28]</ref> on ImageNet-1K <ref type="bibr" coords="7,149.70,446.27,16.22,10.91" target="#b28">[29]</ref>. The team designed a new Effective Logit Adjustment Loss combining Logit adjustment loss <ref type="bibr" coords="7,160.95,459.81,17.96,10.91" target="#b29">[30]</ref> and Class-balanced Loss <ref type="bibr" coords="7,292.14,459.81,16.30,10.91" target="#b30">[31]</ref>. They observe a better overall performance when compared to Cross Entropy and further analyze that the improvement comes from the tail classes. The metadata was integrated by applying an estimated species a priori distribution in individual regions to the ViT estimates. Interestingly, when all metadata (code, endemic, country) were added the F 𝑚 1 score on validation data rose but dropped rapidly on the test data. The best combination on the test data was when the country code was omitted. This hints at a distribution gap between the training and test metadata. SAI <ref type="bibr" coords="7,108.88,568.21,16.48,10.91" target="#b11">[12]</ref>: The team used MetaFormer <ref type="bibr" coords="7,258.47,568.21,16.34,10.91" target="#b31">[32]</ref>. The metadata passes through an Embedding, individual tags (code, endemic, country) are concatenated and the final MLP is used to produce the Meta token. The Metaformer seems suitable for integrating categorical tokens. The team shows in an ablation study a significant raise in F 𝑚 1 score from 66.64% to 74.18%. The Logit adjustment loss function is used to optimize the model. It works better than Seesaw <ref type="bibr" coords="7,468.14,622.41,18.01,10.91" target="#b32">[33]</ref> loss but is outperformed by a post-hoc logit adjustment. Filtering the predictions based on possible locations of species raises the F 𝑚 1 score from 64.32% to 69.09%. SimCLR <ref type="bibr" coords="7,405.09,649.50,17.87,10.91" target="#b33">[34]</ref> with InfoLoss <ref type="bibr" coords="7,488.12,649.50,17.87,10.91" target="#b34">[35]</ref> is used to train the model. The team shows significant improvement when compared to only supervised models pre-trained on different datasets (eg. from ImageNet-1k pre-train F 𝑚 1 of 63.76% to 68.83% when SimCLR is used). In accordance with the findings of previous teams, a bigger resolution of input images leads to better scores. The final score was achieved by the multi-scale, multi-crop, and multi-resolution model ensemble. <ref type="bibr" coords="8,180.46,154.71,16.55,10.91" target="#b12">[13]</ref>: The team combines models of Swin Transformer, EfficientNet, and BEiT <ref type="bibr" coords="8,112.89,168.26,16.19,10.91" target="#b35">[36]</ref>. They experiment with several loss functions (CE Loss, Seesaw Loss, Focal Loss <ref type="bibr" coords="8,485.56,168.26,16.86,10.91" target="#b36">[37]</ref>) and show that Focal Loss performs the best, but only marginally (less than 1% F 𝑚 1 score improvement when compared to CE). From the individual models, the EfficientNet-L2 is the best one. Given the highest number of parameters (480M), it is expected. From this point of view, it is interesting that Swin with 197M parameters achieves the worst score (F 𝑚 1 of 61.70%) and is outperformed by EfficientNet-B7 (F 𝑚 1 of 64.99%) with only 66M parameters. The authors experimented with a fine-grained classification technique -PIM <ref type="bibr" coords="8,373.66,249.56,17.75,10.91" target="#b37">[38]</ref> -to find informative and discriminative features. They show the improvement on the Swin model (F 𝑚 1 from 62.38% to 63.80%) but then do not use it because of the relatively big computational burden when training the model. Finally, the models are ensembled by averaging their softmax outputs. The authors observe a steady increase in F 𝑚 1 score when adding the models to the ensemble one by one. But an unexpected drop is observed when the last model -Swin -is added. The drop is present only in the test data and not the validation data. This opens the question of how, when, and what models to add to the ensemble. <ref type="bibr" coords="8,153.91,371.50,16.36,10.91" target="#b13">[14]</ref>: The team adopted the two-stage principle of the best solution from Snake-CLEF2021 <ref type="bibr" coords="8,137.52,385.05,16.41,10.91" target="#b38">[39]</ref>. In the first stage, the snake is detected and in the second stage, the detected region is classified by a CNN. However, this year it was not the best choice, substantially lacking in performance when compared to the winning team (70.7% F 𝑚 1 versus 85.4% F 𝑚 1 ). Even so, the detection helps and the YOLOv5 <ref type="bibr" coords="8,230.22,425.70,17.75,10.91" target="#b39">[40]</ref> detection network improved the private F 𝑚 1 by 13% on average. The team experimented with two CNN models -EfficientNet and ConvNeXt. They provide an in-depth study of the behavior of the models when combined with different techniques of optimizing, adding metadata, types of augmentations, and so on. The best competition score was achieved by ensembling seven models -EfficientNet-B4 with and without object detection, EfficientNet-v2-m with no object detection, and EfficientNet-v2-m with and without object detection. However this score of 70.79% F 𝑚 1 is only a bit ahead of a single EfficientNet-v2-m model with score 70.23% F 𝑚 1 . This begs the question of how important it is to use ensemble in a real-life application when the improvement is limited. Lastly, the team experimented with different types of metadata representation. In all cases, the metadata were used as a priori distributions of snake species in different locations that were multiplied with the resulting softmax. The worst option was to use the estimated a priori distribution, followed by the binarized version (with a threshold of 0) and finally the post-competition best result (73.90% F 𝑚 1 ) when the binarized distribution was multiplied by the a priori country distribution of the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Secret;Weapon</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USTC-IAT-United</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FHDO-BCSG</head><p>anonymous_rice <ref type="bibr" coords="8,172.78,642.48,16.37,10.91" target="#b14">[15]</ref>: The team experimented with several CNN models -ResNet, ResNext, and EfficientNet to produce deep features of the images and concatenate them with the categorical representation of metadata. In the final solution, ResNet-101 and EfficientNet-B0 were used. The concatenated features are inputted into the XGBoost Ensemble Classifier <ref type="bibr" coords="9,475.43,86.97,18.06,10.91" target="#b40">[41]</ref> to produce the final classification. The nice property of the XGBoost algorithm is that the relative importance of the ensembled features is computed. Unfortunately, the team achieved low score of 3.6% F 𝑚 1 but after the competition when the backbones were trained further, the score raised significantly to 51.39% F 𝑚  1 . This shows some potential of the XGBoost algorithm that may be interesting to study in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Conclusions and Perspectives</head><p>This paper presents an overview and results evaluation of the third edition of the SnakeCLEF challenge organized in conjunction with the Conference and Labs of the Evaluation Forum (CLEF<ref type="foot" coords="9,499.22,225.13,3.71,7.97" target="#foot_2">3</ref> ) and LifeCLEF<ref type="foot" coords="9,149.05,238.68,3.71,7.97" target="#foot_3">4</ref> research platform <ref type="bibr" coords="9,235.82,240.44,16.18,10.91" target="#b41">[42]</ref>, and FGVC9 Workshop<ref type="foot" coords="9,357.30,238.68,3.71,7.97" target="#foot_4">5</ref> -The Ninth Workshop on Fine-Grained Visual Categorization organized within the CVPR conference. The main outcomes we can derive from the this year's evaluation are as follows.</p><p>Transformer-based architectures outperformed CNNs. This year various deep neural network architectures -Convolutional Neural Networks and Transformers -were evaluated; ConvNext <ref type="bibr" coords="9,162.34,321.73,16.41,10.91" target="#b20">[21]</ref>, EfficientNet <ref type="bibr" coords="9,241.89,321.73,16.41,10.91" target="#b42">[43]</ref>, Vision Transformer <ref type="bibr" coords="9,356.95,321.73,16.41,10.91" target="#b17">[18]</ref>, Swin Transformer <ref type="bibr" coords="9,465.50,321.73,16.41,10.91" target="#b18">[19]</ref>, and MetaFormer <ref type="bibr" coords="9,144.87,335.28,16.09,10.91" target="#b31">[32]</ref>. Unlike last year, where the CNN architectures overwhelmed the performance, Vision Transformer architectures were a vital asset for most methods submitted this year. The second best method with F 𝑚 1 score of 84.56% was based on an ensemble of exclusively ViT models and performed slightly worse (-0.9%) than the best performing system that used a combination of Transformer and CNN models. An ensemble of MetaFormer models achieved the third-best score of 82.65%. It seems that Transformers and CNNs benefit from each other in an ensemble, whereas a standalone Transformer ensemble performs better than a pure CNN ensemble which achieved an F 𝑚 1 score of "only" 70.80%</p><p>Loss Function matters. Several loss functions were evaluated: Label Aware Smoothing <ref type="bibr" coords="9,486.82,457.22,16.29,10.91" target="#b21">[22]</ref>, (modified) Categorical Cross-Entropy, Seesaw <ref type="bibr" coords="9,292.90,470.77,16.18,10.91" target="#b32">[33]</ref>, and Focal Loss <ref type="bibr" coords="9,382.18,470.77,16.17,10.91" target="#b36">[37]</ref>. Overall, any Loss function if used is better than standard CrossEntropy. The wining team used Label Aware Smoothing. The runner-up used an Effective Logit Adjustment Loss and showed an improvement of around 2% of F 𝑚 1 score when compared to Cross Entropy, reducing the error rate by 15%. The the third team used Logit adjustment to outperform the Seesaw loss from an F 𝑚 1 score of 76.49% to 78.57%. The team USTC-IAT-United compared CE Loss, Seesaw Loss, and Focal Loss with EfficientNet model. Focal Loss performed the best, but only with marginal improvement over CE Loss and Seesaw Loss.</p><p>Self-supervision has potential. Adding unlabeled data to the train set is a welcome option when not many observations of a species are available. The third team used the SimCLR <ref type="bibr" coords="9,487.98,606.27,18.00,10.91" target="#b33">[34]</ref> method with InfoNCE <ref type="bibr" coords="9,189.79,619.81,17.92,10.91" target="#b34">[35]</ref> loss function to increase the F 𝑚 1 score from 63.76% to 68.83% when compared to an ImageNet-1k pretrained models. Overall, performance on tail classes was higher this year.</p><p>Geographical metadata improves classification performance. Most teams report accuracy improvement when adding the metadata into the learning process. The second team achieved an improvement of 10.89% in terms of the F 𝑚 1 score using a simple location filtering approach. The third team described an absolute improvement of 7.54% when adding the metadata into the MetaFormer. Using the a priori country distribution from the training data outperformed other approaches tried by the fifth team, but in real life this will frustrate users in countries that lack many snake images to use as training data. Representation of many locations in both training and testing data remain important and challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble helps, but at what cost?</head><p>Most teams used ensembling to increase the accuracy of classification. The standard approach was to compute an average of the individual models' decisions. Some teams used a late fusion of deep features by concatenation as an ensemble technique. Even though the improvement in accuracy is observable (around 1 percentage point of F 𝑚 1 across the board), it would be interesting to measure the added computational complexity vs the added accuracy. In the case of snakebite, the system's inference time plays a crucial role.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,587.44,416.69,9.96;2,89.29,599.39,418.22,9.96;2,88.66,611.35,418.39,9.96;2,89.29,623.30,304.81,9.96;2,91.07,461.24,107.70,107.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Harmless mimic species Cemophora coccinea ssp. coccinea (top row) and poisonous lookalike species. Micrurus pyrrhocryptus, Micrurus ibiboboca, and Micrurus nigrocinctus (left to right, bot. row). ©roadmom-iNaturalist, ©Anthony Damiani-iNaturalist, ©Adam Cushen-iNaturalist, ©Alexander Guiñazu-iNaturalist, ©Tarik Câmara-iNaturalist, and ©Cristhian Banegas-iNaturalist.</figDesc><graphic coords="2,91.07,461.24,107.70,107.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,292.05,418.23,8.93;3,88.66,303.12,244.37,9.96;3,91.05,189.35,136.07,90.76"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two snake observations from SnakeCLEF2022 dataset -three images for each individual. ©André Giraldi -iNaturalist, ©Harshad Sharma -iNaturalist.</figDesc><graphic coords="3,91.05,189.35,136.07,90.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,211.13,418.23,8.93;4,89.29,223.14,416.70,8.87;4,89.29,235.04,416.69,8.93;4,89.29,247.05,416.70,8.87;4,89.29,259.00,150.57,8.87;4,90.13,84.19,206.26,119.52"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: Worldwide snake species distribution, i.e., The number of species found in each country.Large countries in the tropics (Brazil, Mexico, Colombia, India, and Indonesia) have more than 300 species. Right: Percentage of snake species per country included in the SnakeCLEF 2022 dataset. The countries with adequate species coverage are those from Europe, Oceania, and North America, i.e., the countries with the smallest diversity.</figDesc><graphic coords="4,90.13,84.19,206.26,119.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,89.29,602.64,418.23,8.93"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Identification accuracy on the full test set and the test set for human performance evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,336.18,404.98,111.21"><head>Table 1</head><label>1</label><figDesc>Details of the SnakeCLEF 2022 datasets and their comparison with previous editions.</figDesc><table coords="3,98.81,367.80,395.16,79.60"><row><cell>Dataset</cell><cell cols="6">Species Images Observation Countries min / max samples</cell></row><row><cell>SnakeCLEF 2020</cell><cell>783</cell><cell>259,214</cell><cell>×</cell><cell>145</cell><cell>19</cell><cell>14,433</cell></row><row><cell>SnakeCLEF 2021</cell><cell>772</cell><cell>386,006</cell><cell>×</cell><cell>188</cell><cell>10</cell><cell>22,163</cell></row><row><cell>SnakeCLEF 2022</cell><cell>1,572</cell><cell>318,532</cell><cell>187,129</cell><cell>208</cell><cell>5</cell><cell>6,472</cell></row><row><cell>SnakeCLEF 2022-Training</cell><cell>1,572</cell><cell>270,251</cell><cell>158,698</cell><cell>207</cell><cell>3</cell><cell>5,518</cell></row><row><cell>SnakeCLEF 2022-Test</cell><cell>1,572</cell><cell>48,281</cell><cell>28,431</cell><cell>183</cell><cell>2</cell><cell>954</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.12,173.53,411.86,413.02"><head></head><label></label><figDesc>Official SnakeCLEF 2022 competition results, sorted by performance on the private set.</figDesc><table coords="6,88.12,173.53,411.86,413.02"><row><cell>Macro averaged F1 [%]</cell><cell>20 40 60 80 100</cell><cell cols="23">86.47 90.28 84.87 89.96 82.65 87.55 76.47 80.23 72.93 78.2 72.01 77.92 71.97 78.26 70.8 75.43 68.56 70.87 59.5 65.64 57.02 62.13 44.28 49.04 42.3 45.84 41.87 47.38 39.55 44.08 39.16 44.39 36.51 41.19 32.18 40.52 31.92 36.38 26.31 33.72 22.38 26.78 21.04 27.41 17.14 23.33 15.84 21.62 9.9 12.01 Private Leaderboard Public Leaderboard</cell></row><row><cell></cell><cell>0</cell><cell>Secret;Weapon</cell><cell>SAI</cell><cell>WabuLabuDabu</cell><cell>LieDown</cell><cell>comfort break</cell><cell>USTC-IAT-United</cell><cell>FHDO-BCSG</cell><cell>vicii</cell><cell>DiamondH</cell><cell>BingXi</cell><cell>College Slytherin</cell><cell>jzsherlock</cell><cell>Got it !</cell><cell>dmitrykonovalov</cell><cell>Ritu Ahmed</cell><cell>MAGUS_YWX</cell><cell>YHT_MT1</cell><cell>SSN-CSE-Lekshmi</cell><cell>asdaas1</cell><cell>7aml</cell><cell>Rziting</cell><cell>plcrtkqrm</cell><cell>HaHaWork</cell></row><row><cell cols="25">Secret;Weapon 93.29 Figure 4: GG 20 40 60 80 100 Accuracy [%] 94.01 92 94 93.53 SAI 94 90.25 WabuLabuDabu 94.67 88.6 LieDown 91.33 90.04 comfort break 91.33 89.97 USTC-IAT-United 91.33 88.4 FHDO-BCSG 85.33 83.54 vicii 86.67 81.29 DiamondH 84 78.75 BingXi 82.67 58.32 College Slytherin 54 61.99 jzsherlock 60.67 65.89 Got it ! 68 67.46 dmitrykonovalov 70.67 67.85 Ritu Ahmed 64 62.24 MAGUS_YWX 65.33 46.85 YHT_MT1 52.67 52.74 SSN-CSE-Lekshmi 52 39.17 asdaas1 45.33 47.79 7aml 53.33 30.76 Rziting 31.33 25.08 plcrtkqrm 28 22.67 HaHaWork 26 29.19 28.67 Full Test Set Expert Set</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,671.02,192.05,8.97"><p>https://www.kaggle.com/competitions/fungiclef2022</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,108.93,671.03,106.21,8.97"><p>https://www.inaturalist.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="9,110.42,649.11,107.67,8.97"><p>http://www.clef-initiative.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="9,110.42,660.06,86.71,8.97"><p>http://www.lifeclef.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="9,110.42,671.02,152.86,8.97"><p>https://sites.google.com/view/fgvc9/home</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments LP was supported by the <rs type="funder">UWB</rs> grant, project No. <rs type="grantNumber">SGS-2022-017</rs>. LP and MH were supported by the <rs type="funder">Technology Agency of the Czech Republic</rs>, project No. <rs type="grantNumber">SS05010008</rs>. AMD was supported by the <rs type="funder">Florida Gulf Coast University Office of Scholarly Innovation and Student Research</rs>. The work described herein has been supported by the <rs type="funder">Ministry of Education, Youth and Sports of the Czech Republic</rs>, Project No. <rs type="grantNumber">LM2018101 LINDAT/CLARIAH-CZ</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_umsqsAA">
					<idno type="grant-number">SGS-2022-017</idno>
				</org>
				<org type="funding" xml:id="_QtDH8ZH">
					<idno type="grant-number">SS05010008</idno>
				</org>
				<org type="funding" xml:id="_PJ4DHRn">
					<idno type="grant-number">LM2018101 LINDAT/CLARIAH-CZ</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,515.85,394.53,10.91;10,112.66,529.40,393.32,10.91;10,112.66,542.95,378.58,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,112.66,529.40,393.32,10.91;10,112.66,542.95,235.47,10.91">Identifying the snake: First scoping review on practices of communities and healthcare providers confronted with snakebite across the world</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Botero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mesa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alcoba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chappuis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ruiz De Castañeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,356.79,542.95,46.54,10.91">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">229989</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,556.50,394.53,10.91;10,112.66,570.05,393.33,10.91;10,112.66,583.60,394.53,10.91;10,112.66,597.15,22.49,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,254.84,570.05,251.15,10.91;10,112.66,583.60,206.32,10.91">Snakebite and snake identification: empowering neglected communities and health-care providers with ai</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Alcoba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chappuis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Salathé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,326.39,583.60,114.94,10.91">The Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="202" to="e203" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,610.69,395.17,10.91;10,112.66,624.24,393.32,10.91;10,112.66,637.79,393.33,10.91;10,112.66,651.34,162.16,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,438.64,624.24,67.34,10.91;10,112.66,637.79,393.33,10.91;10,112.66,651.34,40.11,10.91">Citizen science and online data: Opportunities and challenges for snake ecology and action against snakebite</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Montalcini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Mondardini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Fernandez-Marques</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Grey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Uetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">M</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,161.15,651.34,33.43,10.91">Toxicon</title>
		<imprint>
			<biblScope unit="volume">X</biblScope>
			<biblScope unit="page">100071</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,86.97,395.17,10.91;11,112.48,100.52,393.50,10.91;11,112.66,114.06,393.33,10.91;11,112.66,127.61,277.77,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,406.43,100.52,99.56,10.91;11,112.66,114.06,393.33,10.91;11,112.66,127.61,76.42,10.91">Crowdsourcing snake identification with online communities of professional herpetologists and avocational snake enthusiasts</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kleinhesselink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mondardini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fernandez-Marquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gutsche-Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gwilliams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wüster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,197.38,127.61,120.16,10.91">Royal Society open science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">201273</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,141.16,395.17,10.91;11,112.66,154.71,393.53,10.91;11,112.66,168.26,349.52,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,268.89,154.71,237.30,10.91;11,112.66,168.26,126.30,10.91">The global distribution of tetrapods reveals a need for targeted reptile conservation</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Roll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Novosolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Böhm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Castro-Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chirio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Collen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,247.51,168.26,125.66,10.91">Nature Ecology &amp; Evolution</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1677" to="1682" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,181.81,394.53,10.91;11,112.66,195.36,393.33,10.91;11,112.66,208.91,393.33,10.91;11,112.66,222.46,104.57,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,112.66,195.36,393.33,10.91;11,112.66,208.91,281.21,10.91">Supervised learning computer vision benchmark for snake species identification from photographs: Implications for herpetology and global health</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Salathé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,407.34,208.91,98.64,10.91;11,112.66,222.46,51.98,10.91">Frontiers in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,236.01,394.61,10.91;11,112.28,249.56,395.00,10.91;11,112.66,263.11,378.53,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,369.26,236.01,138.01,10.91;11,112.28,249.56,220.27,10.91">Overview of the snakeclef 2020: Automatic snake species identification challenge</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Mohanty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,358.65,249.56,148.63,10.91;11,112.66,263.11,201.59,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">Sep. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>CLEF task overview 2020</note>
</biblStruct>

<biblStruct coords="11,112.66,276.66,394.62,10.91;11,112.28,290.20,393.70,10.91;11,112.66,303.75,288.62,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,368.26,276.66,139.01,10.91;11,112.28,290.20,288.08,10.91">Overview of SnakeCLEF 2021: Automatic snake species identification with country-level focus</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,425.01,290.20,80.97,10.91;11,112.66,303.75,257.93,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,317.30,395.00,10.91;11,112.66,330.85,137.88,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,257.37,317.30,87.40,10.91">The reptile database</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Uetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Freed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hošek</surname></persName>
		</author>
		<ptr target="https://reptile-database.reptarium.cz/advanced_search" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,344.40,393.53,10.91;11,112.66,357.95,393.33,10.91;11,112.66,371.50,107.76,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,229.73,344.40,276.46,10.91;11,112.66,357.95,95.23,10.91">When large kernel meets vision transformer: A solution for snakeclef &amp; fungiclef</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,234.83,357.95,271.16,10.91;11,112.66,371.50,77.06,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,385.05,393.33,10.91;11,112.66,398.60,393.33,10.91;11,112.66,412.15,57.08,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,280.27,385.05,225.72,10.91;11,112.66,398.60,61.14,10.91">Solution for snakeclef 2022 by tackling long-tailed categorization</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,196.66,398.60,309.33,10.91;11,112.66,412.15,26.38,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,425.70,393.32,10.91;11,112.66,439.25,393.33,10.91;11,112.66,452.79,159.39,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,295.02,425.70,210.96,10.91;11,112.66,439.25,141.35,10.91">Solutions for fine-grained and long-tailed snake species recognition in snakeclef</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,297.94,439.25,208.05,10.91;11,112.66,452.79,128.70,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,466.34,394.53,10.91;11,112.28,479.89,393.71,10.91;11,112.66,493.44,288.62,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,112.28,479.89,292.69,10.91">An efficient model integration-based snake classification algorithm</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Shuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,427.91,479.89,78.08,10.91;11,112.66,493.44,257.93,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,506.99,394.53,10.91;11,112.66,520.54,393.32,10.91;11,112.66,534.09,328.66,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,357.13,506.99,150.06,10.91;11,112.66,520.54,330.87,10.91">Combination of object detection, geospatial data, and feature concatenation for snake species identification</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-F</forename><surname>Böckmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bracke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,466.46,520.54,39.52,10.91;11,112.66,534.09,297.96,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,547.64,393.33,10.91;11,112.66,561.19,393.59,10.91;11,112.66,574.74,261.72,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,425.08,547.64,80.90,10.91;11,112.66,561.19,268.59,10.91">Deep learning and gradient boosting ensembles for classification of snake species</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palaniappan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Desingu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bharathi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Chodisetty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bhaskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,403.15,561.19,103.09,10.91;11,112.66,574.74,231.02,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,588.29,393.60,10.91;11,112.66,601.84,146.44,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09741</idno>
		<title level="m" coord="11,362.79,588.29,109.69,10.91">Visual attention network</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,615.39,393.33,10.91;11,112.66,628.93,354.88,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,254.27,615.39,251.72,10.91;11,112.66,628.93,19.47,10.91">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,139.95,628.93,233.51,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3965" to="3977" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,642.48,395.16,10.91;11,112.66,656.03,395.17,10.91;11,112.66,669.58,349.55,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m" coord="11,330.08,656.03,177.76,10.91;11,112.66,669.58,167.84,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,86.97,393.33,10.91;12,112.39,100.52,393.60,10.91;12,112.66,114.06,250.30,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,370.89,86.97,135.10,10.91;12,112.39,100.52,181.28,10.91">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,317.36,100.52,188.63,10.91;12,112.66,114.06,142.26,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,127.61,394.53,10.91;12,112.66,141.16,173.79,10.91" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13112</idno>
		<title level="m" coord="12,301.13,127.61,201.50,10.91">Volo: Vision outlooker for visual recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,154.71,394.61,10.91;12,112.66,168.26,394.53,10.91;12,112.66,181.81,100.87,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,383.60,154.71,103.81,10.91">A convnet for the 2020s</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,112.66,168.26,389.80,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,195.36,394.62,10.91;12,112.66,208.91,394.53,10.91;12,112.66,222.46,75.45,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,259.21,195.36,223.84,10.91">Improving calibration for long-tailed recognition</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,112.66,208.91,365.50,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16489" to="16498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,236.01,394.53,10.91;12,112.28,249.56,275.03,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,312.58,236.01,190.03,10.91">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,112.28,249.56,230.24,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,263.11,394.53,10.91;12,112.66,276.66,395.00,10.91;12,112.41,290.20,38.81,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,211.54,263.11,290.80,10.91">Trivialaugment: Tuning-free yet state-of-the-art data augmentation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">G</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,127.24,276.66,334.38,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="774" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,303.75,394.62,10.91;12,112.66,317.30,394.53,10.91;12,112.41,330.85,27.76,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,322.30,303.75,161.24,10.91">Random erasing data augmentation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,112.66,317.30,265.36,10.91">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,344.40,393.32,10.91;12,112.66,357.95,393.32,10.91;12,112.66,371.50,233.71,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="12,329.08,344.40,176.90,10.91;12,112.66,357.95,182.01,10.91">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,317.28,357.95,188.70,10.91;12,112.66,371.50,136.06,10.91">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,385.05,395.17,10.91;12,112.66,398.60,197.93,10.91" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m" coord="12,331.02,385.05,176.81,10.91;12,112.66,398.60,16.17,10.91">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,412.15,393.33,10.91;12,112.66,425.70,393.32,10.91;12,112.66,439.25,159.65,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="12,330.08,412.15,175.91,10.91;12,112.66,425.70,34.52,10.91">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,170.26,425.70,335.73,10.91;12,112.66,439.25,51.39,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,452.79,393.33,10.91;12,112.66,466.34,394.53,10.91;12,112.66,479.89,103.61,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="12,346.64,452.79,159.35,10.91;12,112.66,466.34,67.28,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,228.08,466.34,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,493.44,393.33,10.91;12,112.66,506.99,252.90,10.91" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07314</idno>
		<title level="m" coord="12,411.38,493.44,94.60,10.91;12,112.66,506.99,70.44,10.91">Long-tail learning via logit adjustment</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,520.54,393.53,10.91;12,112.66,534.09,393.33,10.91;12,112.66,547.64,147.08,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="12,305.50,520.54,200.69,10.91;12,112.66,534.09,44.86,10.91">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,180.82,534.09,325.16,10.91;12,112.66,547.64,49.16,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,561.19,393.53,10.91;12,112.66,574.74,288.50,10.91" xml:id="b31">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02751</idno>
		<title level="m" coord="12,307.65,561.19,198.53,10.91;12,112.66,574.74,106.31,10.91">Metaformer: A unified meta framework for fine-grained recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,588.29,394.53,10.91;12,112.66,601.84,393.58,10.91;12,112.66,615.39,351.04,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="12,112.66,601.84,228.32,10.91">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,370.86,601.84,135.39,10.91;12,112.66,615.39,252.92,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9695" to="9704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,628.93,393.33,10.91;12,112.66,642.48,394.53,10.91;12,112.66,656.03,65.30,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="12,311.65,628.93,194.34,10.91;12,112.66,642.48,107.45,10.91">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,243.09,642.48,201.97,10.91">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,669.58,393.33,10.91;13,112.66,86.97,184.16,10.91" xml:id="b34">
	<monogr>
		<title level="m" type="main" coord="12,272.35,669.58,233.64,10.91;13,112.66,86.97,27.85,10.91">Representation learning with contrastive predictive coding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>arXiv-1807</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct coords="13,112.66,100.52,393.33,10.91;13,112.66,114.06,107.17,10.91" xml:id="b35">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m" coord="13,226.97,100.52,203.67,10.91">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,127.61,394.61,10.91;13,112.66,141.16,395.01,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="13,327.53,127.61,159.84,10.91">Focal loss for dense object detection</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,112.66,141.16,299.48,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,154.71,395.17,10.91;13,112.66,168.26,197.93,10.91" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P.-Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-C</forename><surname>Kao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03822</idno>
		<title level="m" coord="13,258.65,154.71,249.17,10.91;13,112.66,168.26,16.17,10.91">A novel plug-in module for fine-grained visual classification</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,181.81,393.33,10.91;13,112.66,195.36,393.33,10.91;13,112.66,208.91,107.76,10.91" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="13,202.64,181.81,303.34,10.91;13,112.66,195.36,92.97,10.91">Incorporation of object detection models and location data into snake species classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Borsodi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Papp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,233.30,195.36,272.68,10.91;13,112.66,208.91,77.06,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,222.46,394.53,10.91;13,112.66,236.01,394.53,10.91;13,112.66,249.56,394.53,10.91;13,112.66,263.11,394.49,10.91;13,112.66,276.66,394.62,10.91;13,112.66,290.20,328.01,10.91" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="13,322.10,263.11,185.05,10.91;13,112.66,276.66,226.29,10.91">ultralytics/yolov5: v6.0 -YOLOv5n &apos;Nano&apos; models, Roboflow integration, TensorFlow export</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jocher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stoken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Borovec</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Nanocode012</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Taoxie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Changyu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">V</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Laughing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Skalski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Nadar</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mammana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Alexwang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hajek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Diaconu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oleg</forename><surname>Marc</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5563715</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5563715.doi:10.5281/zenodo.5563715" />
	</analytic>
	<monogr>
		<title level="m" coord="13,234.87,263.11,79.22,10.91">wanghaoyang</title>
		<imprint>
			<publisher>OpenCV DNN support</publisher>
			<date type="published" when="1900">1900. 0106. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,303.75,393.33,10.91;13,112.66,317.30,394.52,10.91;13,112.66,330.85,394.04,10.91;13,112.66,344.40,233.99,10.91" xml:id="b40">
	<analytic>
		<title level="a" type="main" coord="13,211.45,303.75,186.67,10.91">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="http://doi.acm.org/10.1145/2939672.2939785.doi:10.1145/2939672.2939785" />
	</analytic>
	<monogr>
		<title level="m" coord="13,421.80,303.75,84.19,10.91;13,112.66,317.30,394.52,10.91;13,112.66,330.85,36.69,10.91">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,357.95,394.53,10.91;13,112.66,371.50,394.53,10.91;13,112.66,385.05,393.33,10.91;13,112.66,398.60,393.33,10.91;13,112.66,412.15,353.54,10.91" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="13,198.52,385.05,307.47,10.91;13,112.66,398.60,247.50,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,383.00,398.60,122.99,10.91;13,112.66,412.15,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,425.70,394.53,10.91;13,112.66,439.25,352.61,10.91" xml:id="b42">
	<analytic>
		<title level="a" type="main" coord="13,178.42,425.70,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,127.29,439.25,207.49,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
