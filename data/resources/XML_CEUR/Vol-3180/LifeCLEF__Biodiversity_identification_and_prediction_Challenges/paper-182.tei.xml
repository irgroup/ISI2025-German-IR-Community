<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,354.24,15.42">Bag of Tricks and a Strong Baseline for FGVC</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.10,113.06,32.42,11.96"><forename type="first">Jun</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China)</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,134.17,113.06,55.28,11.96"><forename type="first">Hao</forename><surname>Chang</surname></persName>
							<email>changhaoustc@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China)</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,202.09,113.06,39.97,11.96"><forename type="first">Keda</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China)</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Ping An Technology Co., Ltd</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.99,113.06,63.48,11.96"><forename type="first">Guochen</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China)</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.11,113.06,64.77,11.96"><forename type="first">Liwen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China)</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,413.53,113.06,75.64,11.96"><forename type="first">Zhongpeng</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China)</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,127.00,64.69,11.96"><forename type="first">Shenshen</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China)</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,166.63,127.00,63.79,11.96"><forename type="first">Zhihong</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China)</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Ping An Technology Co., Ltd</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.35,127.00,55.54,11.96"><forename type="first">Zepeng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China)</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Ping An Technology Co., Ltd</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.82,127.00,46.39,11.96"><forename type="first">Fang</forename><surname>Gao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Guangxi University</orgName>
								<address>
									<settlement>Nanning</settlement>
									<region>Guangxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,399.20,127.00,63.09,11.96"><forename type="first">Feng</forename><surname>Shuang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Guangxi University</orgName>
								<address>
									<settlement>Nanning</settlement>
									<region>Guangxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,354.24,15.42">Bag of Tricks and a Strong Baseline for FGVC</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">254891D86F47DCE0B326ACF49006DB92</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>fine-grained</term>
					<term>class-imbalanced</term>
					<term>meta information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained visual classification (FGVC), as a subclass classification task under the superclass, brings more challenges. However, in addition to fine-grained features, the FungiCLEF 2022 dataset is also characterized by imbalance and rich meta-information. This motivates us to explore the impact of different methods and components in fine-grained classification on FungiCLEF 2022. We explore the impact of different data augmentations, backbones, loss functions, and attention mechanisms on classification performance. Additionally, we explore different metadata usage scenarios. In the end, we win second place in the CVPR2022 FGVC Workshop FungiCLEF 2022 challenge. Our code is available at https: //github.com/wujiekd/Bag-of-Tricks-and-a-Strong-Baseline-for-Fungi-Fine-Grained-Classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In contrast to general image classification, the goal of fine-grained image classification is to correctly classify subclasses that belong to the same superclass (birds, cars, etc.). FGVC has long been considered a challenging task due to small differences between classes and large differences within classes. Publicly available datasets and benchmarks accelerate machine learning research and allow quantitative comparisons of new approaches. In the fields of deep learning and computer vision, rapid progress over the past decade has been largely driven by the publication of large-scale image datasets. The same is true for the problem of FGVC, where datasets, such as iNaturalist <ref type="bibr" coords="1,211.74,492.36,11.28,10.91" target="#b0">[1]</ref>, have helped develop and evaluate new methods for fine-grained domain adaptation. But there has been a lack of research on the automatic classification of fungi.</p><p>Generally, the main methods of FGVC mainly focus on how to make the network focus on the most discriminative regions, such as part-based models and attention-based models. Inspired by human observational behavior, these methods introduce localization-induced biases to neural networks with complex structures. In addition, some data augmentation methods and loss functions can also make the model pay more attention to fine-grained feature regions. When some species are visually indistinguishable, some extra-visual information can assist fine-grained classification, such as spatiotemporal priors and textual descriptions. But none of these methods have been explored on fungi-related datasets. This motivates us to explore the combined effect of various modules of deep neural network(DNN) for fungi's fine-grained classification method.</p><p>To address the fungi FGVC problem, we experiment a bag of tricks for image classification. First, we do an exploratory data analysis on the data set of FungiCLEF 2022, and deal with the imbalance and fine-grained characteristics of the data set accordingly. We apply the current state-of-the-art(SOTA) data augmentation methods to expand the richness of the dataset, and use large-parameter Convolutional Neural Network(CNN) models and transformer models to extract image features. Furthermore, we explore various loss functions, attention mechanisms, and two-stage training approach to deal with indistinguishable features and data imbalance, respectively. Since the raw data contains rich metadata, we apply various methods to improve the accuracy of the model after processing the metadata of different modalities.</p><p>In terms of experiments, we first use SwinTransformer <ref type="bibr" coords="2,350.36,303.75,12.97,10.91" target="#b1">[2]</ref> as the baseline model, ùêπ 1 score of the Public Leaderboard is 75.01%, and then introduce various SOTA data augmentation and tricks for evaluation to improve the single mode by nearly 5%. After determining the optimal solutions for various settings, we use multiple CNN or Transformer-based models to extract features, and finally model fusion is used to obtain the final result. The ùêπ 1 score of the Public Leaderboard is 83.12%, while the final ùêπ 1 score of the Private Leaderboard is 79.6% in the fungi competition <ref type="bibr" coords="2,145.64,385.05,12.84,10.91" target="#b2">[3]</ref> of LifeCLEF 2022 Workshop <ref type="bibr" coords="2,288.36,385.05,11.36,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,302.44,385.05,7.57,10.91" target="#b4">5]</ref>.</p><p>The contribution of this paper are summarized as follows:</p><p>‚Ä¢ We explore the impact of different combinations of components in image classification on the FungiCLEF 2022 dataset. Appropriate processing methods are applied separately for imbalanced and fine-grained features to alleviate model bias. ‚Ä¢ We explore different ways of using metadata and achieve effective improvements on some backbones. ‚Ä¢ We achieve an ùêπ ùëö 1 score of 83.12% and 79.06% on the public and private test sets of the FungiCLEF 2022 dataset, respectively, winning second place in the fungi competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Fine-grained classification</head><p>Existing fine-grained classification methods can be divided into visual-only classification methods and multimodal classification methods. The former relies entirely on visual information to solve the problem of fine-grained classification, while the latter tries to use multimodal data to build a joint representation that merges multimodal information to facilitate fine-grained classification. Fine-grained classification methods that rely solely on vision can be broadly classified into two categories: localization methods <ref type="bibr" coords="2,318.70,652.46,12.84,10.91" target="#b5">[6]</ref> and feature encoding methods <ref type="bibr" coords="2,471.96,652.46,11.43,10.91" target="#b6">[7]</ref>.</p><p>Early work <ref type="bibr" coords="3,153.64,86.97,12.99,10.91" target="#b7">[8]</ref> used partial annotations as supervision to make the network notice subtle differences between certain species and suffer from their expensive annotations. RA-CNN <ref type="bibr" coords="3,493.12,100.52,12.87,10.91" target="#b8">[9]</ref> was proposed to amplify subtle regions to recursively learn to distinguish region attention and region-based feature representations at multiple scales in a mutually reinforcing manner. NTSNet <ref type="bibr" coords="3,127.62,141.16,17.88,10.91" target="#b9">[10]</ref> proposed a self-supervised mechanism to efficiently localize information regions.</p><p>Feature encoding methods are dedicated to enriching feature representation capabilities to improve the performance of fine-grained classification. CAP <ref type="bibr" coords="3,377.19,168.26,18.07,10.91" target="#b10">[11]</ref> designed context-aware attention pools to capture subtle changes in images. TransFG <ref type="bibr" coords="3,370.77,181.81,18.07,10.91" target="#b11">[12]</ref> proposed a part selection module that applies a visual transformer to select discriminative image patches. Compared with localization methods, feature encoding methods are difficult to clearly distinguish the distinguishing regions between different species.</p><p>To distinguish these challenging visual categories, additional information, i.e., geographic location, attributes, and textual descriptions, can be helpfully utilized. Geo-Aware <ref type="bibr" coords="3,438.37,249.56,17.75,10.91" target="#b12">[13]</ref> introduced geographic information prior to fine-grained classification and systematically examined various previous approaches using geographic information, including post-processing, whitelisting, and feature modulation. Presence-only <ref type="bibr" coords="3,243.83,290.20,17.82,10.91" target="#b13">[14]</ref> also introduced a spatio-temporal prior to the network, which was shown to be effective in improving the final classification performance. CVL <ref type="bibr" coords="3,487.94,303.75,18.05,10.91" target="#b14">[15]</ref> proposed a two-branch network in which one branch learned visual features and the other branch learned textual features, and finally combined these two parts to obtain the final latent semantic representation. All the above methods were designed for specific prior information and cannot be flexibly adapted to different auxiliary information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Fungi image classification</head><p>Image analysis tool <ref type="bibr" coords="3,175.57,407.68,17.69,10.91" target="#b15">[16]</ref> was published for mushroom species identification in 1992, analyzing morphological characters such as length, width and other shape descriptors. Computer vision can also be used to classify microscopic images of fungal spores. Microscopic image datasets of fungal infections and classification methods <ref type="bibr" coords="3,274.88,448.32,19.22,10.91" target="#b16">[17,</ref><ref type="bibr" coords="3,294.10,448.32,14.41,10.91" target="#b17">18]</ref> were proposed to speed up medical diagnosis, thus avoiding additional expensive biochemical tests. A visual identification system based on deep CNNs was presented for 1,394 species of fungi and it is used in citizen science projects. The system allowed users to automatically identify observed specimens while providing valuable data to biologists and computer vision researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>As shown in Figure <ref type="figure" coords="3,174.45,561.15,3.66,10.91" target="#fig_0">1</ref>, we first carry out exploratory data analysis on the data, and then introduce PIM <ref type="bibr" coords="3,110.50,574.70,16.33,10.91" target="#b15">[16]</ref>, two-stage training <ref type="bibr" coords="3,219.29,574.70,16.34,10.91" target="#b16">[17]</ref>, Meta Information and other information as auxiliary information. On the basis of the basic image classification model, we use the abundant information to improve the accuracy of model classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fungi challenge Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Exploratory data analysis</head><p>The Fungi dataset contains 295,938 training images including 1,604 species. Dataset Features: Accurate class labels, highly imbalanced long-tailed class distribution. The test set for this competition includes 59,420 objects, 118,676 images, and 3,134 species, covering the full year of 2021, including observations collected across all substrate and habitat types. (Note: For out-of-range classes, use -1 as the predicted label). As shown in Figure <ref type="figure" coords="4,401.97,583.83,3.69,10.91">2</ref>, some images of fungi data set <ref type="bibr" coords="4,126.26,597.38,17.91,10.91" target="#b17">[18]</ref> are shown.</p><p>In addition to the image data information, the challenge also provides metadata, including abundant observational information such as habitat, time, and location. We are conducting a preliminary analysis of this data. There are 33 attributes in the training set, but only 10 attributes in the test set. By comparing one by one, the common features are month, day, countryCode, level0Name, level1Name, level2Name, substrate, habitat. The details are as shown in Table <ref type="table" coords="4,493.28,665.13,3.74,10.91">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Examples of intra-and inter-class similarities and differences for selected species of three taxonomically distinct fungi families <ref type="bibr" coords="5,239.71,258.69,14.83,9.96" target="#b17">[18]</ref>. The similarity holds on the species and the family level. Left: Russulaceae, center: Boletaceae, right: Amanitaceae.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Description of the provided metadata (observation attributes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attribute Description</head><p>Month and day Time information, 12 months, 31 days; CountryCode Country information, covering 30 countries;</p><p>Locality More precise location information;</p><p>Level0Name, Level1Name</p><p>The position information of the observer. and Level2Name</p><p>The three levels correspond to values of 30, 115, and 317, which present position information in refined granularity;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Substrate</head><p>Natural substance on which to live, such as bark, soil, etc.</p><p>There are 32 values in total;</p><p>Habitat Observations of the environment. A total of 32 values including mixed woodland and so on.</p><p>Generally speaking, there are few attributes that can be used. There are mainly time information, more detailed locality information, substrate and habitat information strongly related to the living environment of various fungi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Data Augment</head><p>We do not experiment with subtle data augment combinations on the FungiCLEF 2022 dataset, but use NAS-based or improved data augment methods, namely AutoAugment, RandAugment, and TrivialAugment.</p><p>AutoAugment. AutoAugment <ref type="bibr" coords="5,240.24,641.36,17.87,10.91" target="#b18">[19]</ref> is a simple search-based data augmentation method. Its main idea is to create a search space of data augment strategies and evaluate the quality of a particular strategy directly on some datasets. In the experiments of AutoAugment, the authors design a search space where each strategy consists of many substrategies, and in each batch a substrategy is chosen randomly for each image. The substrategies contain two operations, each of which is an image processing method, such as translation, rotation or clipping, and for each operation there is a set of probabilities and magnitudes to characterize the nature of the use of this operation. Using a search algorithm, search for the best policy that enables the neural network to achieve the highest validation set accuracy on the target dataset. AutoAugment achieves the same performance as the semi-supervised approach without using any unlabeled data. Finally, the strategies learned from one dataset can be directly transferred to other similar datasets and perform equally well. For example, the strategy learned in ImageNet can achieve state-of-the-art accuracy on the FGVC dataset Stanford Cars without training the pre-trained model with new data.</p><p>RandAugment. RandAugment <ref type="bibr" coords="6,249.41,236.01,18.07,10.91" target="#b19">[20]</ref> investigates a data augment strategy based on NAS method search, which provides a significant improvement in search cost compared to earlier NAS search data augment strategies. NAS-method-based data augment strategies (e.g., AA and other methods) suffer from two major drawbacks. First, they use separate search phases, thus increasing the complexity of training and greatly increasing the consumption of computational resources. In addition, since the search phases are separate, these methods cannot adjust the regularization strength according to the model or dataset size. That is, we usually train small models by training them on small datasets and then apply them to train large models. The proposal of RandAugment (later referred to as RA) solves these two problems by significantly reducing the search space and allowing training directly on the target task without a proxy task, and by tailoring the regularization strength of data augment to different datasets and models.</p><p>TrivialAugment. The TrivialAugment <ref type="bibr" coords="6,276.72,385.05,17.82,10.91" target="#b20">[21]</ref> data augment strategy originates from the NAS approach and outperforms NAS, implementing SOTA's data augment strategy in a simpler way. Although the approach of data augment using NAS method for automatic search is effective, the limitation lies in the need to trade-off the search efficiency and the performance of data augmentation. To solve this problem, TrivialAugment data augment strategy (later referred to as TA) is proposed. Compared with previous data augment strategies, TA is parameter-free and uses only one data augmentation method per image, so its search cost is almost free compared to AA and even RA, and it achieves SOTA results. TA uses the same data augment method as RandAugment. Specifically, data augment is defined as consisting of a data augment function a and the corresponding intensity value m (some data augment functions do not use intensity values). For each image, TA samples a data augment function and an intensity value uniformly, and then returns the enhanced image. In addition, while previous methods tend to overlay multiple data augment methods, TA only uses a single data augment method for each image. Using such an approach, the TA-enhanced dataset can be viewed as a single image enhanced separately using all data augment methods, and then uniformly sampled from it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Meta Information</head><p>Appearance information alone is often insufficient to accurately distinguish some fine-grained species. When an image of a species is given, human experts also take advantage of the additional information to help make the final decision. In the related literature, some works <ref type="bibr" coords="6,463.39,651.16,18.07,10.91" target="#b17">[18]</ref> have applied meta data to the post-processing of probability predicted by the image classification model, and some work have integrated meta data and image data as the input of multimodal learning to train a large model which has achieved more excellent results.</p><p>Intuitively, species distribution shows a trend of clustering geographically. Different species have different living habits, and spatiotemporal information can assist the fine-grained task of species classification. When considering latitude and longitude, we firstly need geographic coordinates to circle the earth. To do this, we convert the geographic coordinate system to a Cartesian coordinate system, i.e. When using attributes as meta information, we initialize the attribute list as a vector. For example, for nominal properties such as substrate and habitat, there are 32 different values, so a 32-dimensional feature vector can be generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image feature extraction backbones</head><p>Backbone is crucial for feature extraction of FungiCLEF 2022 dataset. Different bcakbones have different learning ability for features and different focus on the dataset, and the choice of different models can bring us richer data features. In this challenge, we tried the following models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Vision Transformer</head><p>Vision Transformer(viT) <ref type="bibr" coords="7,196.64,362.16,17.76,10.91" target="#b21">[22]</ref> is a model proposed by Google in 2020 to directly apply transformer to image classification. The input of the transformer is a sequence of token embeddings, so the patch embeddings of the image are fed into the transformer and then the features are extracted and classified. Research shows that when enough data is available for pre-training, ViT outperforms CNN and breaks the limitation of the transformer's lack of inductive bias to obtain better migration results in downstream tasks. We loaded ViT pre-trained models on ImageNet dataset for training and achieved good results on our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">SwinTransformer</head><p>There are two main challenges in the application of Transformer to the image field. Visual entities are highly variable, and the visual Transformer performance may not be very good in different scenes Image resolution is high. With many pixel points, Transformer's global self-attention-based computation leads to a large computational effort. To address these two problems, SwinTransformer <ref type="bibr" coords="7,212.59,546.98,12.68,10.91" target="#b1">[2]</ref> proposes a Transformer with a hierarchical design that includes a sliding window operation, which consists of a non-overlapping local window and an overlapping cross-window. Restricting the attention computation to a single window can introduce the localization of CNNs convolution operations on the one hand and save computation on the other. The overall architecture of Swin Transformer is hierarchical, with four stages, each of which reduces the resolution of the input feature map and expands the perceptual field layer by layer like CNNs. There are several places where it is handled differently than ViT. ViT will positionencode the embedding on the input, while Swin-Transformer is here as an option (self.ape). Swin-Transformer does a relative position encoding when calculating Attention. ViT will add a learnable parameter separately as a token for classification, while Swin-Transformer directly averages and outputs classification, which is somewhat similar to the final global average of CNN. pooling layer. On the ImageNet22K dataset, the accuracy rate of Swin Transformer can reach an astonishing 86.4%, which is one of the current SOTA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Efficientnet</head><p>The traditional practice of model scaling is to arbitrarily increase the depth or width of the CNNs, or to use a larger input image resolution for training and evaluation. While these approaches do improve accuracy, they typically require long periods of manual tuning and still often yield suboptimal performance. A new approach to model scaling is to use a simple and efficient composite coefficient to scale CNNs in a more structured way. Unlike traditional methods that arbitrarily scale network dimensions such as width, depth, and resolution, EfficientNets <ref type="bibr" coords="8,439.86,231.14,18.07,10.91" target="#b22">[23]</ref> uniformly scales network dimensions with a fixed set of scale scaling factors. By using this novel scaling method and AutoML techniques, the authors call this model EfficientNets, which is up to 10 times more efficient (smaller and faster). To understand the effect of network scaling, the authors systematically studied the effect of scaling different dimensions on the model. While scaling individual dimensions can improve model performance, the authors observe that balancing all dimensions of the network based on available resources can maximize overall performance. In addition, the effectiveness of model scaling relies heavily on the baseline network. To further improve performance, the authors also develope a new baseline network that optimizes accuracy and efficiency by performing neural structure search using the AutoML MNAS framework. The final architecture uses moving inverse bottleneck convolution (MBConv).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Bidirectional Encoder representation from Image Transformer</head><p>Following the development of BERT in the field of natural language processing, Bao et al. <ref type="bibr" coords="8,488.11,415.96,17.88,10.91" target="#b23">[24]</ref> proposed a masked image modeling task to pretrain visual Transformers(BEiT). Specifically, in image classification field, each image has two views, namely image patches (e.g. 16√ó16 pixels) and visual tokens (i.e. discrete tokens). We first "tokenize" the original image into visual tokens. Then randomly mask some image patches and feed them into the backbone Transformer. The goal of pre-training is to recover original visual tokens from corrupted image patches. After pretraining BEiT, we directly fine-tune model parameters on downstream tasks by appending task layers on the pretrained encoder. Experimental results on image classification and semantic segmentation show that the model achieves better results than previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming DeiT (81.8%) trained from scratch under the same settings. Furthermore, large-size BEiT achieves 86.3% accuracy using only ImageNet-1K, even better than ViT-L (85.2%) with supervised pretraining on ImageNet-22K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss function</head><p>For the task of fungal classification, the loss function we use is a cross-entropy loss function for training. In addition, we use Focal Loss <ref type="bibr" coords="8,268.56,641.83,18.02,10.91" target="#b24">[25]</ref> and Seesaw Loss <ref type="bibr" coords="8,367.17,641.83,18.03,10.91" target="#b25">[26]</ref> to mitigate the problem of long-tailed distribution of the dataset. Where Focal Loss uses a modulating factor to the crossentropy loss to reduce the loss contribution from easy examples and elevate the importance of hard examples, SeesawLoss achieves a relative balance of positive and negative sample gradients by dynamically reducing the weight of the excessive negative sample gradients imposed by the head category on the tail category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Fine-tuning to improve ùêπ ùëö</head><p>1 score</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Attentional mechanism</head><p>We used the fine-grained classification method of PIM <ref type="bibr" coords="9,335.89,184.36,18.02,10.91" target="#b15">[16]</ref> to help us with the task of fungal classification.PIM <ref type="bibr" coords="9,169.97,197.91,17.88,10.91" target="#b15">[16]</ref> is an excellent method for fine-grained classification that automatically finds the most discriminative regions and uses local features to provide features that are more helpful for classification. PIM <ref type="bibr" coords="9,259.73,225.01,18.07,10.91" target="#b15">[16]</ref> is a plug-in module, so it can be integrated into very many common CNN or Transformer-based network backbone, such as swin-transformer, effintnet, etc. The plugin module can output pixel-level feature maps and fuse filtered features to enhance FGVC. It can be briefly explained by selecting appropriate output feature maps from the backbone's blocks to input to a weakly supervised selector to filter out regions with strong discriminative power or regions with little relevance to classification, and finally fusing the features from the selector's output with a combiner to obtain prediction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Two-stage training</head><p>Two-stage training consists of unbalanced training and balanced fine-tuning. In this section, we will focus on different approaches to balance fine-tuning. CNNs trained on imbalanced datasets without any reweighting or resampling methods can learn good feature representations but have poor classification accuracy on underrepresented tail categories. Fine-tuning these networks on balanced subsets makes that features learned from unbalanced datasets are transferred and rebalanced across all classes[31]. These fine-tuning methods <ref type="bibr" coords="9,356.43,423.38,17.78,10.91" target="#b26">[27]</ref> can be divided into two parts: deferred rebalancing via resampling (DRS) and via reweighting (DRW).</p><p>‚Ä¢ DRS first uses a vanilla training schedule and then applies resampling method in the fine-tuning stage. To obtain a balanced subset for fine-tuning, the resampling method described in the related work <ref type="bibr" coords="9,249.39,489.53,17.91,10.91" target="#b27">[28]</ref> on "Resampling Methods"; ‚Ä¢ DRW first uses a vanilla training schedule and then applies reweighting method in the fine-tuning stage. The fine-tuning stage will employ the reweighting method described in the related work <ref type="bibr" coords="9,204.37,531.53,17.91,10.91" target="#b27">[28]</ref> on "Reweighting Methods".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Metadata Use</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">Conditional probability post-processing</head><p>We refer to someone else's method <ref type="bibr" coords="9,244.31,601.83,16.10,10.91" target="#b17">[18]</ref>, a simple way to use metadata to improve classification performance -similar to the spatio-temporal priors used. For metadata (D) and image (I) of a given type, the corresponding conditional probability is calculated, and then the post-processing is carried out for softmax output of the last layer of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2.">Metaformer</head><p>FGVC is the task that requires the classification of objects belonging to multiple subordinate classes of a superclass. Recent state-of-the-art approaches usually design complex learning pipelines to solve this task. However, visual information alone is usually insufficient to accurately distinguish FGVC. Nowadays, meta-information (e.g., spatio-temporal priors, attributes, and textual descriptions) usually appears together with images. Is it possible to use a uniform and simple framework to leverage various meta-information to aid fine-grained classification? To address this question, Metaformer explores a unified and powerful meta-framework for FGVC.</p><p>In practice, MetaFormer provides a simple and effective way to address the joint learning of visual and various meta-information. In addition, MetaFormer provides a powerful baseline for FGVC without the bells and whistles. Numerous experiments have shown that MetaFormer can effectively exploit various meta-information to improve the performance of fine-grained classification. In a fair comparison, MetaFormer can outperform current SOTA methods with only visual information on the iNaturalist2017 and iNaturalist2018 datasets. After adding meta-information, MetaFormer can outperform the current SOTA method by 5.9% and 5.3%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3.">The application of MLP</head><p>Due to the large difference between the meta data and the image data, directly using the multimodal method to learn them at the same time will cause the model to be difficult to converge or to converge too slowly. Therefore, the features and meta information extracted by the image model can be used through MLP. The extracted features are further interacted, which can speed up the convergence speed, and can not waste the information of the meta data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Firstly, we train various well-known architectures such as EfficientNets, SwinTransformer, Vision Transformer, and BEiT on the CNNss and Transformers families respectively. Furthermore, two-stage fine-tuning is introduced and evaluated. Finally, the models of various different architectures are integrated. Additionally, the effects of different metadata and their combinations on the final prediction performance of CNNs and ViT are evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>In this section, we describe the complete training and evaluation procedure, including the training strategy, image augment, and testing procedures. All architectures are initialized with publicly available pre-trained checkpoints and the same policies are trained using the PyTorch framework in a Tesla A100 graphics card. All neural networks are optimized using stochastic gradient descent with momentum set to 0.9. The starting learning rate (LR) is set to 0.01 and is further reduced by Reduce LR On Plateau strategy (the dataset is split 9:1 between training and validation) and the mutilstep adjustment strategy (training with the full dataset). We trained 300 epochs in the training phase of the verification set, and selected checkpoints according to the best results of the divided verification set for fine-tuning in the next stage. In addition, the full training phase trained 21 epochs for fine-tuning. The default parameter settings are used for data augment. In order to speed up the efficiency of the model, the batch size is determined according to the vidio memory during training, which is between 4 and 64. For training, in addition to the conventional data augment methods, we also adopt a more advanced automatic augment technology. More specifically, we use random horizontal flip with 50% probability, random vertical flip with 50% probability, random adjustment crop with 0.8 -1.0 scale, random brightness/contrast adjustment with 40% probability. All images are resized to the desired network input size: in the CNNs performance experiments, the input size is 600 √ó 600. For Swin transformer, the input size is 384 √ó 384, while on BEiT, it is 512 √ó 512. In the testing phase, in addition to resizing and normalizing operations, we also use TTA, specifically 10-Crop verification.</p><p>On the basis of the above training, we replace various loss functions for evaluation. In addition, on the basis of the full amount of extremely unbalanced dataset training used in the first stage, PIM <ref type="bibr" coords="11,111.17,276.66,18.07,10.91" target="#b15">[16]</ref> and two-stage fine-tuning are introduced respectively. Finally, model ensemble is applied, which is fused separately from the result layer and the feature layer. In addition, the impact of meta data on the fungi dataset of this challenge is evaluated by post-processing and MLP, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>According to the requirements of the competition, the evaluation index used is the macroaverage ùêπ 1 . ùêπ ùëö 1 , which is not affected by the class frequency and is more suitable for the long-tailed class distribution observed in nature. Given that the dataset is highly imbalanced and has long-tailed distributions, the learning process may ignore the least present species. So, ùêπ ùëö 1 allows to easily assign a cost value to each label's two error types and measure more task-related performance. Define ùêπ ùëö 1 as the mean of various ùêπ 1 scores:</p><formula xml:id="formula_0" coords="11,257.08,458.93,249.56,33.89">ùêπ ùëö 1 = 1 ùëÅ ùëÅ ‚àëÔ∏Å ùëÜ=1 ùêπ ùëÜ 1 ,<label>(1)</label></formula><p>where ùëÅ represents the number of classes and ùëÜ is the species index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>In this section, we compare the performance of well-known CNN models and Transformer-based models on the ùêπ ùëö 1 metric. Then, two-stage fine-tuning is introduced. Additionally, we discuss the impact of metadata on the performance of image classification models. Finally, the best results are obtained using model ensemble.</p><p>The impact of different data augmentations. We try different data augmentation methods on the basic SwinTransformer, such as RandAugmet <ref type="bibr" coords="11,319.88,621.26,16.08,10.91" target="#b19">[20]</ref>, TrivialAugment <ref type="bibr" coords="11,414.85,621.26,17.76,10.91" target="#b20">[21]</ref> and CutMix and random erasure, as shown in Table <ref type="table" coords="11,247.17,634.81,3.74,10.91" target="#tab_0">2</ref>.</p><p>Two-Stage training Evaluation. We further explore on the basis of the previous model, namely SwinTransformer. The attention mechanism PIM <ref type="bibr" coords="11,354.57,661.91,18.06,10.91" target="#b15">[16]</ref> is introduced, and two-stage fine-tuning using balanced data is attempted. As shown in Table <ref type="table" coords="12,387.14,416.92,3.81,10.91">3</ref>, we can see a significant improvement over the original baseline.</p><p>The impact of different loss function. In addition, the influence of different loss functions on the model accuracy is also discussed. As shown in Table <ref type="table" coords="12,364.97,457.57,3.81,10.91">4</ref>, FocalLoss performs better in fine-grained classification.</p><p>The effect of different backbones. As shown in Table <ref type="table" coords="12,346.09,484.67,3.66,10.91">5</ref>, we use different backbone training on the full data set of this challenge, and we adopt a high-performing data augment approach. In addition, we replace the loss function with FocalLoss, and use two-stage fine-tuning to obtain the results of the online test set evaluation. It is a pity that we do not use PIM <ref type="bibr" coords="12,449.19,525.32,18.07,10.91" target="#b15">[16]</ref> because of computing power, which requires a lot of computing resources and time. It can be seen that the Transformers series is better than the CNNs series as a whole, and BEiT has the best performance. Because the Private Leaderboard's test set accounts for 80%, the evaluation is more authoritative and reliable.</p><p>Use of meta data. First, we try to use the prior probability of meta data as post-processing, but the effect is not ideal as shown in Table <ref type="table" coords="12,285.38,606.61,3.75,10.91">6</ref>. We believe that the poor performance is due to the large difference between the distributions of the training and test sets. In addition, we use MetaFormer and MLP for interactive learning of meta features and image features respectively, from the perspective of the model. Among them, MetaFormer is difficult to converge during training, and it may also lead to exploding gradients. And in MLP, the effect also decreased, as</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,426.10,416.69,9.96;4,89.29,438.05,416.70,9.96;4,89.29,450.01,104.79,9.96;4,89.29,84.19,416.70,335.37"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The framework of the method we used. We adopt models of CNN and Transformer with different architectures to extract image features, and introduce a variety of Tricks to improve the accuracy of classification.</figDesc><graphic coords="4,89.29,84.19,416.70,335.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,235.00,168.26,271.19,10.91;7,89.29,181.81,307.43,10.91;7,397.91,180.25,34.04,6.99;7,410.70,188.31,8.47,6.99;7,433.15,182.54,28.46,9.57;7,462.80,180.25,34.04,6.99;7,475.59,188.31,8.47,6.99;7,498.04,182.54,9.62,9.57"><head>12 )</head><label>12</label><figDesc>[ùëôùëéùë°, ùëôùëúùëõ] ‚Üí [ùë•, ùë¶, ùëß]. Likewise, December to January is closer than October. Therefore, we perform the mapping of [ùëöùëúùëõùë°‚Ñé] ‚Üí [ùë†ùëñùëõ( 2ùúãùëöùëúùëõùë°‚Ñé</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,89.29,84.19,416.70,156.00"><head></head><label></label><figDesc></figDesc><graphic coords="5,89.29,84.19,416.70,156.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="12,88.99,90.49,334.53,299.89"><head>Table 2</head><label>2</label><figDesc>The effect of different data augments</figDesc><table coords="12,88.99,120.26,334.53,270.12"><row><cell>Augment method</cell><cell cols="3">ùêπ ùëö 1 score Score fluctuation</cell></row><row><cell>Baseline</cell><cell></cell><cell>75.01%</cell><cell>-</cell></row><row><cell>RandAugment</cell><cell></cell><cell>75.42%</cell><cell>+0.41%</cell></row><row><cell>TrivialAugment</cell><cell></cell><cell>75.50%</cell><cell>+0.49%</cell></row><row><cell cols="2">Cutmix and Random Erasing</cell><cell>75.59%</cell><cell>+0.58%</cell></row><row><cell>Table 3</cell><cell></cell><cell></cell></row><row><cell>The effect of different Stage2 method</cell><cell></cell><cell></cell></row><row><cell cols="4">Stage2 method ùêπ ùëö 1 score Score fluctuation</cell></row><row><cell>Baseline</cell><cell>75.01%</cell><cell>-</cell></row><row><cell>PIM</cell><cell>75.89%</cell><cell>+0.88%</cell></row><row><cell>DRS</cell><cell>75.82%</cell><cell>+0.81%</cell></row><row><cell>Table 4</cell><cell></cell><cell></cell></row><row><cell>The effect of different loss function</cell><cell></cell><cell></cell></row><row><cell cols="4">loss function ùêπ ùëö 1 score Score fluctuation</cell></row><row><cell>Baseline(CE)</cell><cell>75.01%</cell><cell>-</cell></row><row><cell>FocalLoss</cell><cell>75.89%</cell><cell>+0.88%</cell></row><row><cell>SeesawLoss</cell><cell>75.45%</cell><cell>+0.44%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">Conclusions</head><p>In this work, we conduct a full exploratory analysis of the fungi dataset and experiment with multiple different types of backbones. On the basis of the previous one, a two-stage fine-tuning is introduced, and the model ensemble is carried out to obtain the optimal result. In addition, we delve into additional meta data and try various protocols with poor results. Our methods win second place in the CVPR2022 "Automatic fungi classification as an open-set machine learning problem" challenge.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>shown in Table <ref type="table" coords="13,160.05,506.83,3.74,10.91">6</ref>.</p><p>Of course, meta-information is not useless. We find a value different from the only value in the training set in the substrate attribute of the test set, which is spiders. We find the images where the substrate is spiders, reset their label to -1, and test it online, and the results show that we get a certain improvement.</p><p>Model Ensemble and Challenge Score. In the initial stage, when we divide the dataset 9:1, we perform two levels of fusion, as shown in Table <ref type="table" coords="13,321.02,588.13,3.79,10.91">7</ref>. The output of the Softmax layer uses a simple average fusion, and the feature output layer uses the concat of the features extracted by the pooling of the last layer of each model. Then we train with an MLP to obtain the final result. It can be seen that the effect is not very different, so our final solution selects the first simple and effective fusion method. As shown in Table <ref type="table" coords="13,338.88,642.32,3.79,10.91">8</ref>, we achieve excellent results on the leaderboard and 2nd on the Private and Public leaderboards.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="14,112.66,237.65,394.53,10.91;14,112.66,251.20,393.33,10.91;14,112.66,264.75,383.04,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,167.27,251.20,251.61,10.91">The inaturalist species classification and detection dataset</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,441.39,251.20,64.60,10.91;14,112.66,264.75,285.12,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,278.30,393.33,10.91;14,112.39,291.85,393.60,10.91;14,112.66,305.40,250.30,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,370.89,278.30,135.10,10.91;14,112.39,291.85,181.28,10.91">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,317.36,291.85,188.63,10.91;14,112.66,305.40,142.26,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,318.95,393.33,10.91;14,112.66,332.50,395.17,10.91;14,112.66,346.05,232.29,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,339.05,318.95,110.41,10.91;14,480.07,318.95,25.92,10.91;14,112.66,332.50,223.69,10.91">Fungi recognition as an open set classification problem</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,365.49,332.50,142.34,10.91;14,112.66,346.05,201.59,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of FungiCLEF</note>
</biblStruct>

<biblStruct coords="14,112.66,359.59,394.53,10.91;14,112.66,373.14,393.33,10.91;14,112.66,386.69,393.33,10.91;14,112.66,400.24,168.28,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,185.62,373.14,320.37,10.91;14,112.66,386.69,208.65,10.91">Lifeclef 2022 teaser: An evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,343.19,386.69,162.80,10.91;14,112.66,400.24,38.01,10.91">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,413.79,394.53,10.91;14,112.66,427.34,394.53,10.91;14,112.66,440.89,393.33,10.91;14,112.66,454.44,393.33,10.91;14,112.66,467.99,353.54,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,198.52,440.89,307.47,10.91;14,112.66,454.44,247.50,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,383.00,454.44,122.99,10.91;14,112.66,467.99,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,481.54,393.33,10.91;14,112.66,495.09,393.33,10.91;14,112.66,508.64,283.16,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,209.69,481.54,296.30,10.91;14,112.66,495.09,173.82,10.91">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,309.54,495.09,196.44,10.91;14,112.66,508.64,185.04,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3034" to="3043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,522.18,393.33,10.91;14,112.39,535.73,394.79,10.91;14,112.66,549.28,80.57,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,306.74,522.18,199.25,10.91;14,112.39,535.73,76.08,10.91">Hierarchical bilinear pooling for fine-grained visual recognition</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,209.91,535.73,292.21,10.91">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="574" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,562.83,393.33,10.91;14,112.66,576.38,393.33,10.91;14,112.66,589.93,240.15,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,429.86,562.83,76.12,10.91;14,112.66,576.38,168.53,10.91">Cross-x learning for fine-grained visual categorization</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,308.79,576.38,197.20,10.91;14,112.66,589.93,142.26,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8242" to="8251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,603.48,393.33,10.91;14,112.66,617.03,393.32,10.91;14,112.66,630.58,276.41,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,215.77,603.48,290.22,10.91;14,112.66,617.03,194.88,10.91">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,330.89,617.03,175.09,10.91;14,112.66,630.58,178.50,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,644.13,393.33,10.91;15,112.66,86.97,394.53,10.91;15,112.66,100.52,80.57,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,337.67,644.13,168.31,10.91;15,112.66,86.97,56.26,10.91">Learning to navigate for fine-grained classification</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,191.55,86.97,310.44,10.91">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="420" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,114.06,393.54,10.91;15,112.66,127.61,323.87,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Behera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wharton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hewage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06635</idno>
		<title level="m" coord="15,311.07,114.06,195.13,10.91;15,112.66,127.61,142.15,10.91">Context-aware attentional pooling (cap) for fine-grained visual classification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,141.16,393.71,10.91;15,112.66,154.71,393.33,10.91;15,112.33,168.26,29.19,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07976</idno>
		<title level="m" coord="15,457.57,141.16,48.79,10.91;15,112.66,154.71,239.30,10.91">Transfg: A transformer architecture for fine-grained recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,181.81,393.33,10.91;15,112.66,195.36,393.33,10.91;15,112.66,208.91,262.63,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,458.38,181.81,47.61,10.91;15,112.66,195.36,171.00,10.91">Geo-aware networks for fine-grained recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Potetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,309.82,195.36,196.16,10.91;15,112.66,208.91,194.36,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,222.46,393.33,10.91;15,112.66,236.01,393.53,10.91;15,112.30,249.56,124.54,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,259.01,222.46,246.98,10.91;15,112.66,236.01,57.10,10.91">Presence-only geographical priors for fine-grained image classification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,193.33,236.01,312.85,10.91;15,112.30,249.56,26.65,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9596" to="9606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,263.11,394.62,10.91;15,112.66,276.66,394.53,10.91;15,112.66,290.20,65.30,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,182.67,263.11,304.38,10.91">Fine-grained image classification via combining vision and language</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,112.66,276.66,364.29,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5994" to="6002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,303.75,395.17,10.91;15,112.66,317.30,197.93,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P.-Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-C</forename><surname>Kao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03822</idno>
		<title level="m" coord="15,258.65,303.75,249.17,10.91;15,112.66,317.30,16.17,10.91">A novel plug-in module for fine-grained visual classification</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,330.85,393.33,10.91;15,112.66,344.40,393.33,10.91;15,112.66,357.95,198.44,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="15,275.85,330.85,230.14,10.91;15,112.66,344.40,156.17,10.91">Bag of tricks for long-tailed visual recognition with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,292.03,344.40,213.96,10.91;15,112.66,357.95,49.84,10.91">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3447" to="3455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,371.50,394.53,10.91;15,112.66,385.05,393.33,10.91;15,112.66,398.60,392.48,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="15,112.66,385.05,281.26,10.91">Danish fungi 2020-not just another image recognition dataset</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Laess√∏e</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Fr√∏slev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,420.58,385.05,85.40,10.91;15,112.66,398.60,294.59,10.91">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1525" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,412.15,395.17,10.91;15,112.66,425.70,393.53,10.91;15,112.30,439.25,225.28,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="15,358.94,412.15,148.89,10.91;15,112.66,425.70,118.26,10.91">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,255.11,425.70,251.08,10.91;15,112.30,439.25,137.31,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,452.79,395.17,10.91;15,112.66,466.34,393.32,10.91;15,112.66,479.89,325.92,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="15,302.02,452.79,205.81,10.91;15,112.66,466.34,171.71,10.91">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,307.43,466.34,198.55,10.91;15,112.66,479.89,237.36,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,493.44,394.53,10.91;15,112.66,506.99,395.00,10.91;15,112.41,520.54,38.81,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="15,211.54,493.44,290.80,10.91">Trivialaugment: Tuning-free yet state-of-the-art data augmentation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">G</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,127.24,506.99,334.38,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="774" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,534.09,395.16,10.91;15,112.66,547.64,395.17,10.91;15,112.66,561.19,349.55,10.91" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m" coord="15,330.08,547.64,177.76,10.91;15,112.66,561.19,167.84,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,574.74,394.53,10.91;15,112.66,588.29,346.82,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="15,178.42,574.74,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,127.29,588.29,202.02,10.91">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,601.84,393.33,10.91;15,112.66,615.39,107.17,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m" coord="15,226.97,601.84,203.67,10.91">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,628.93,394.61,10.91;15,112.66,642.48,395.01,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="15,327.53,628.93,159.84,10.91">Focal loss for dense object detection</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,112.66,642.48,299.48,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,656.03,394.53,10.91;15,112.66,669.58,393.58,10.91;16,112.66,86.97,341.92,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="15,112.66,669.58,228.32,10.91">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,370.86,669.58,135.39,10.91;16,112.66,86.97,244.01,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9695" to="9704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,100.52,395.17,10.91;16,112.66,114.06,393.33,10.91;16,112.33,127.61,29.19,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="16,323.58,100.52,184.25,10.91;16,112.66,114.06,138.15,10.91">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,258.65,114.06,234.28,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,141.16,393.53,10.91;16,112.66,154.71,393.33,10.91;16,112.66,168.26,147.08,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="16,305.50,141.16,200.69,10.91;16,112.66,154.71,44.86,10.91">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,180.82,154.71,325.16,10.91;16,112.66,168.26,49.16,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
