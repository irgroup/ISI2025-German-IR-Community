<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,403.01,15.42;1,89.29,106.66,276.48,15.42">Overview of FungiCLEF 2022: Fungi Recognition as an Open Set Classification Problem</title>
				<funder ref="#_exaGYYp">
					<orgName type="full">UWB</orgName>
				</funder>
				<funder ref="#_pzcpxNA">
					<orgName type="full">Technology Agency of the Czech Republic</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,134.97,58.19,11.96"><forename type="first">Lukáš</forename><surname>Picek</surname></persName>
							<email>picekl@kky.zcu.cz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Cybernetics</orgName>
								<orgName type="department" key="dep2">Faculty of Applied Sciences</orgName>
								<orgName type="institution">University of West Bohemia</orgName>
								<address>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,160.12,134.97,51.78,11.96"><forename type="first">Milan</forename><surname>Šulc</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Rossum.ai</orgName>
								<address>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.54,134.97,47.16,11.96"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">The Center for Machine Perception Dept. of Cybernetics</orgName>
								<orgName type="department" key="dep2">FEE</orgName>
								<orgName type="institution">Czech Technical University in</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.70,134.97,120.89,11.96"><forename type="first">Jacob</forename><surname>Heilmann-Clausen</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Center for Macroecology, Evolution and Climate</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,403.01,15.42;1,89.29,106.66,276.48,15.42">Overview of FungiCLEF 2022: Fungi Recognition as an Open Set Classification Problem</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">A3EFCEC51CCDF4EC790F5B9A428DE2A0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>FungiCLEF</term>
					<term>fine grained visual categorization</term>
					<term>metadata</term>
					<term>open-set recognition</term>
					<term>fungi</term>
					<term>species identification</term>
					<term>machine learning</term>
					<term>computer vision</term>
					<term>classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The main goal of the new LifeCLEF challenge, FungiCLEF 2022: Fungi Recognition as an Open Set Classification Problem, was to provide an evaluation ground for end-to-end fungi species recognition in an open class set scenario. An AI-based fungi species recognition system deployed in the Atlas of Danish Fungi helps mycologists to collect valuable data and allows users to learn about fungi species identification. Advances in fungi recognition from images and metadata will allow continuous improvement of the system deployed in this citizen science project. The training set is based on the Danish Fungi 2020 dataset and contains 295,938 photographs of 1,604 species. For testing, we provided a collection of 59,420 expert-approved observations collected in 2021. The test set includes 1,165 species from the training set and 1,969 unknown species, leading to an open-set recognition problem. This paper provides (i) a description of the challenge task and datasets, (ii) a summary of the evaluation methodology, (iii) a review of the systems submitted by the participating teams, and (iv) a discussion of the challenge results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatic recognition of fungi species assists mycologists, citizen scientists and nature enthusiasts in species identification in the wild <ref type="bibr" coords="1,263.15,472.80,11.23,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,277.02,472.80,7.49,10.91" target="#b1">2]</ref>. Its availability supports the collection of valuable biodiversity data. In practice, species identification typically does not depend solely on the visual observation of the specimen but also on other information available to the observer -such as habitat, substrate, location and time. The main goal for the new FungiCLEF competition was to provide an evaluation ground for automatic methods for fungi recognition in an open class set scenario, i.e, the submitted methods have to handle images of unknown species. Similarly to previous LifeCLEF competitions, The competition was hosted on Kaggle primarily to attract machine learning experts to participate and present their ideas. Thanks to rich metadata, precise annotations, and baselines available to all competitors, the challenge provides a benchmark for image recognition with the use of additional information.  <ref type="bibr" coords="2,429.62,410.52,10.37,8.87" target="#b3">[4]</ref>. Atlas of Danish Fungi: ©Bedřiška Picková and ©Jan Riis-Hansen and ©Arne Pedersen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Challenge description</head><p>The new FungiCLEF 2022 challenge: Fungi Recognition as an Open Set Classification Problem, was organized in conjunction with the Conference and Labs of the Evaluation Forum (CLEF <ref type="foot" coords="2,499.16,496.16,3.71,7.97" target="#foot_0">1</ref> ) and LifeCLEF<ref type="foot" coords="2,149.82,509.71,3.71,7.97" target="#foot_1">2</ref> research platform <ref type="bibr" coords="2,237.63,511.47,11.46,10.91" target="#b2">[3]</ref>, and FGVC9 Workshop<ref type="foot" coords="2,355.46,509.71,3.71,7.97" target="#foot_2">3</ref> -The Ninth Workshop on Fine-Grained Visual Categorization organized within the CVPR conference.</p><p>The main goal for this challenge was to return the species with the highest likelihood (or "unknown") for each given test observation, consisting of a set of images and metadatathe information about habitat, substrate, location, and more is provided for each observation. Photographs of unknown fungi species had to be classified into an "unknown" class with label id -1. The baseline procedure to include metadata in the decision problem and baseline pretrained image classifiers were provided as part of the task description to all participants. Sample observations are visualized in Figure <ref type="figure" coords="2,254.12,619.86,3.74,10.91" target="#fig_0">1</ref>. Each row represents one observation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Metadata</head><p>The visual data is accompanied by metadata for approximately 99% of the image observations and includes information about attributes related to the environment, place, time and taxonomy. The provided metadata is acquired by citizen scientists and enables research directions on combining visual data with metadata. We include 21 frequently filled-in attributes. The most important attributes are listed and described below.</p><p>Substrate: Substrates on which fungi live and fruit are an essential source of information that helps differentiate similarly-looking species. Each species or genus has its preferable substrate, and it is rare to find it on other substrates. We provide one of 32 substrate types for more than 99% of images. We differentiate wood of living trees, dead wood, soil, bark, stone, fruits and others. Habitat: While substrate denotes the spots, the habitat indicates the overall environment where fungi grow, which is vital for fungal recognition. We include the information about the habitat for 99.5% of observations.</p><p>Location: Fungi are highly location-dependent. We include multi-level location information. Starting from GPS coordinates with included uncertainty, we further extracted information about the country, region and district.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time-Stamp:</head><p>Observation time is essential for fungi classification in the wild as fruitbodies' presence depends on seasonality or even the time in a day. Figure <ref type="figure" coords="4,410.45,356.95,5.17,10.91" target="#fig_1">2</ref> shows the monthly observation frequency for three genera.</p><p>EXIF data: Since the camera device and its settings affect the resulting image, the image classification models may be biased towards specific device attributes. To allow a deeper study of such phenomena, we include the EXIF data for approximately 84% of images. We included attributes such as White Balance, Color Space, Metering Mode, Aperture, Device, Exposure Time and Shutter Speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Timeline</head><p>The competition and data were published in February 2022 through the LifeCLEF, Kaggle, and FGVC challenge pages allowing anyone with research ambitions to register and participate in the competition. The test data were provided jointly with the training data allowing continuous evaluation. Each team could submit up to 2 submissions a day. The deadline for challenge submissions was May 16, setting the competition for roughly three months. Participants submitted CSV files containing the Top1 prediction for each fungi observation. Once the submission phase was closed (mid-May), the participants were allowed to submit post-competition submissions to evaluate any exciting findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Evaluation Protocol</head><p>The evaluation process consisted of two stages: (i) a public evaluation on the public subset (20%) of the test set, which was available during the whole competition with a limit of two submissions a day, and (ii) a final evaluation on the private test set (80%) after the challenge deadline. The main evaluation metric for the competition was the F 𝑚 1 , defined as the mean of class-wise F 1 scores:</p><formula xml:id="formula_0" coords="5,260.63,185.46,245.35,30.20">F 𝑚 1 = 1 𝑁 𝑁 ∑︁ 𝑠=1 𝐹 1𝑠 ,<label>(1)</label></formula><p>where 𝑁 represents the number of classes -in case of the Kaggle evaluation, 𝑁 = 1, 165 (#classes in the test set) -and 𝑠 is the species index. The F 1 score for each class is calculated as a harmonic mean of the class precision 𝑃 𝑆 and recall 𝑅 𝑆 :</p><formula xml:id="formula_1" coords="5,180.96,271.58,325.03,24.56">𝐹 1𝑠 = 2 × 𝑃 𝑠 × 𝑅 𝑠 𝑃 𝑠 + 𝑅 𝑠 , 𝑃 𝑠 = tp 𝑠 tp 𝑠 + fp 𝑠 , 𝑅 𝑠 = tp 𝑠 tp 𝑠 + fn 𝑠<label>(2)</label></formula><p>In single-label multi-class classification, the True Positives (tp) of a species represents the number of correct Top1 predictions of that species, False Positive (fp) denotes how many times was different species predicted instead of the (tp), and False Negatives (fn) indicates how many images of species 𝑠 have been wrongly classified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Working Notes</head><p>All participants with valid submissions were asked to provide a Working Note paper -a technical report with information needed to reproduce the results of all submissions. All submitted Working Notes were reviewed by 2-3 reviewers. The review process was single-blind and offered up to two rebuttals. The acceptance rate was 75%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Challenge Results</head><p>The official challenge results, based on the F 𝑚 1 score, are displayed in Figure <ref type="figure" coords="5,451.45,490.88,3.81,10.91">3</ref>. The best performing teamxiong -achieved F 𝑚 1 of 80.43% on the private test set and an accuracy of 65.69% on the complete test set. We note that the order would be different in terms of accuracy, as shown in Figure <ref type="figure" coords="5,178.23,531.53,3.81,10.91" target="#fig_2">4</ref>, where the best accuracy of 67.08% on the full rest set was achieved by team GG, primarily due to a high number of correctly identified out-of-scope observations. In the case of the out-of-scope (OoS) identification performance, i.e. what proportion of out-ofscope observations has been correctly classified as OoS, the best performing team with 44.55% correctly categorized observations was one of the worst-performing teams in terms of F 𝑚 1 . As also displayed in Figure <ref type="figure" coords="5,197.96,599.28,5.11,10.91" target="#fig_2">4</ref> participants identified less than 5% OoS observations and only four teams achieved accuracy over 10% on out-of-scope observations. In Figure <ref type="figure" coords="5,417.34,612.83,4.99,10.91" target="#fig_3">5</ref> we have evaluated the species toxicity confusion on the full test set for all the participants, i.e., how often poisonous species are confused for edible ones and vice versa. Interestingly, the more critical confusion where poisonous fungi were misclassified as edible is relatively high even for the best scoring models -5.70% and 6.63% for team GG and team xiong, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Participants and Methods</head><p>In total, 38 teams contributed with 701 valid submissions to the challenge evaluation on Kaggle.</p><p>The results on the public and private test sets (leaderboards) are displayed in Figure <ref type="figure" coords="6,467.64,619.57,3.77,10.91">3</ref>. Below we summarize the approach of teams with published working notes. More details can be found in the individual working notes of participants <ref type="bibr" coords="6,293.38,646.67,11.23,10.91" target="#b4">[5,</ref><ref type="bibr" coords="6,307.07,646.67,7.42,10.91" target="#b5">6,</ref><ref type="bibr" coords="6,316.95,646.67,7.42,10.91" target="#b6">7,</ref><ref type="bibr" coords="6,326.83,646.67,7.42,10.91" target="#b7">8,</ref><ref type="bibr" coords="6,336.72,646.67,7.42,10.91" target="#b8">9,</ref><ref type="bibr" coords="6,346.60,646.67,13.95,10.91" target="#b9">10]</ref> which passed the review process, ensuring a sufficient level of reproducibility and quality.  xiong <ref type="bibr" coords="7,118.35,355.24,11.42,10.91" target="#b5">[6]</ref>: The winning submission by Xiong et al., achieving an impressive F 𝑚 1 score of 80.43% on the private test set, used an ensemble of MetaFormer <ref type="bibr" coords="7,351.65,368.79,18.06,10.91" target="#b10">[11]</ref> and ConvNext <ref type="bibr" coords="7,441.17,368.79,18.07,10.91" target="#b11">[12]</ref> networks. The provided metadata were utilized as inputs to the MetaFormer architecture. To battle the long-tailed distribution of species, the authors used the Seasaw loss <ref type="bibr" coords="7,393.24,395.89,16.30,10.91" target="#b12">[13]</ref>. Additional improvements were achieved by test-time augmentation, adding a model trained with pseudo-labels to the ensemble, and adding a thresholding post-processing to deal with out-of-scope observations. <ref type="bibr" coords="7,176.90,450.09,11.41,10.91" target="#b6">[7]</ref>: The submission by Yu et al. used an ensemble of several CNN and Transformer architectures: Metaformer <ref type="bibr" coords="7,245.82,463.64,16.41,10.91" target="#b10">[11]</ref>, SwinTransformer <ref type="bibr" coords="7,344.71,463.64,20.08,10.91" target="#b13">[14]</ref>, EfficientNet <ref type="bibr" coords="7,429.88,463.64,16.42,10.91" target="#b14">[15]</ref>, ViT (Vision Transformer) <ref type="bibr" coords="7,148.65,477.18,16.09,10.91" target="#b15">[16]</ref>, BEiT <ref type="bibr" coords="7,195.02,477.18,16.09,10.91" target="#b16">[17]</ref>. The team scored 3 rd with 79.06% of F 𝑚 1 score on the private test set. In their working notes, the authors explore the impact of different data augmentation techniques, model architectures, loss functions, and attention mechanisms on the classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USTC-IAT-United</head><p>GG <ref type="bibr" coords="7,107.40,531.38,11.72,10.91" target="#b7">[8]</ref>: Shen et al. introduced a novel architecture CoLKANet based on VAN (Visual Attention Network) <ref type="bibr" coords="7,166.67,544.93,17.75,10.91" target="#b17">[18]</ref> and CoAtNet <ref type="bibr" coords="7,245.81,544.93,16.09,10.91" target="#b18">[19]</ref>. It is a combination of large kernel attention and vision transformer. The proposed CoLKANet outperforms Swin <ref type="bibr" coords="7,344.71,558.48,17.91,10.91" target="#b13">[14]</ref> and VOLO <ref type="bibr" coords="7,414.10,558.48,17.91,10.91" target="#b19">[20]</ref> models in terms of F 𝑚 1 by 2.3 and 1.9 percentage points, respectively. ConvNeXt <ref type="bibr" coords="7,381.73,572.03,18.07,10.91" target="#b11">[12]</ref> performed similarly to the proposed CoLKANet architecture. Furthermore, the team used techniques such as Label Aware Smoothing <ref type="bibr" coords="7,171.42,599.13,16.30,10.91" target="#b20">[21]</ref>, Pseudo labelling for tail classes and various augmentation techniques. When TrivialAugment <ref type="bibr" coords="7,189.62,612.68,17.79,10.91" target="#b21">[22]</ref> was deployed during the middle stage of experimentation, the team observed a rise in F 𝑚 1 of around 0.5%. Progressively, Random Erasing <ref type="bibr" coords="7,407.90,626.23,16.42,10.91" target="#b22">[23]</ref>, CutMix <ref type="bibr" coords="7,467.97,626.23,18.07,10.91" target="#b23">[24]</ref> and Mixup <ref type="bibr" coords="7,120.59,639.78,18.07,10.91" target="#b24">[25]</ref> were added, which helped with regularization. The final submission score was achieved by an ensemble of five models: 2× ConvNeXt, VOLO, Swin, and CoLKANet. The novel CoLKANet is an interesting contribution with potential outside this competition's scope.</p><p>TeamSpirit <ref type="bibr" coords="8,144.68,100.52,11.47,10.91" target="#b4">[5]</ref>: Fan et al., who scored sixth in the challenge with 77.58% F 𝑚 1 score, propose an image classification method called Class-wise Weighted Prototype Classifier. CWPC decouples closed-set training and open-set inference by constructing class centers from the training set features and their prediction scores. A hard classes mining strategy and the LDAM loss <ref type="bibr" coords="8,487.91,141.16,18.07,10.91" target="#b25">[26]</ref> were used to cope with the long-tailed distribution of species. This team encoded the metadata using a multilingual BERT model <ref type="bibr" coords="8,238.81,168.26,17.91,10.91" target="#b26">[27]</ref> with RoBERTa <ref type="bibr" coords="8,326.17,168.26,16.25,10.91" target="#b27">[28]</ref>.</p><p>Stefan <ref type="bibr" coords="8,122.98,195.36,16.56,10.91" target="#b9">[10]</ref>: Wolf and Beyerer refrained from using ensembles of multiple models, andfor the sake of model simplicity -focused on developing a strong single-model submission. The method is based on a Swin Transformer Large backbone <ref type="bibr" coords="8,366.67,222.46,16.41,10.91" target="#b13">[14]</ref>, a class-balancing training scheme <ref type="bibr" coords="8,125.35,236.01,16.29,10.91" target="#b28">[29]</ref>, heavy data augmentation <ref type="bibr" coords="8,264.91,236.01,17.95,10.91" target="#b29">[30]</ref> and thresholding the softmax scores to cope with out-of-scope observations. The team scored 7 th , in the challenge with 77.54% F 𝑚 1 score.</p><p>SSN <ref type="bibr" coords="8,110.22,276.66,13.15,10.91" target="#b8">[9]</ref>:This team experimented with several ResNet <ref type="bibr" coords="8,327.27,276.66,16.26,10.91" target="#b30">[31]</ref>, ResNeXt <ref type="bibr" coords="8,390.93,276.66,16.26,10.91" target="#b31">[32]</ref>, and EfficientNet <ref type="bibr" coords="8,488.05,276.66,17.93,10.91" target="#b32">[33]</ref> architectures. For their best submission, feature vectors from two selected architectures, Effi-cientNetB4 and ResNeXt101, were concatenated with a categorical representation of metadata. The resulted features were later used for training the XGBoost Ensemble Classifier <ref type="bibr" coords="8,467.84,317.30,16.42,10.91" target="#b33">[34]</ref>. An interesting benefit of the XGBoost algorithm is that the relative importance of the ensembled features is computed; thus, each feature might be observed and studied. With an absolute F 𝑚 1 performance of 48.96%, the XGBoost algorithm with two CNN backbones poses a unique approach for the classification, even though performing worst compared to other participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper presents an overview and results of the first edition of the FungiCLEF challenge organized in conjunction with the Conference and Labs of the Evaluation Forum (CLEF <ref type="foot" coords="8,497.22,455.47,3.71,7.97" target="#foot_6">7</ref> ), LifeCLEF<ref type="foot" coords="8,130.44,469.02,3.71,7.97" target="#foot_7">8</ref> research platform <ref type="bibr" coords="8,217.92,470.77,17.91,10.91" target="#b34">[35]</ref> and FGVC.</p><p>All submissions with working notes were based on modern Convolutional Neural Network (CNN) or transformer-inspired architectures, such as Metaformer <ref type="bibr" coords="8,381.10,497.87,16.24,10.91" target="#b10">[11]</ref>, Swin Transformer <ref type="bibr" coords="8,486.88,497.87,16.24,10.91" target="#b13">[14]</ref>, and BEiT <ref type="bibr" coords="8,133.28,511.42,16.41,10.91" target="#b16">[17]</ref>. The best performing teams used ensembles of both CNNs and Transformers. The winning team <ref type="bibr" coords="8,170.78,524.97,12.69,10.91" target="#b5">[6]</ref> achieved 80.43% accuracy with a combination of ConvNext-large <ref type="bibr" coords="8,469.59,524.97,17.76,10.91" target="#b11">[12]</ref> and MetaFormer <ref type="bibr" coords="8,145.92,538.52,16.24,10.91" target="#b10">[11]</ref>. The results were often improved by combining predictions belonging to the same observation and by both training-time and test-time data augmentations.</p><p>Participants experimented with a number of different training losses to battle the long tail distribution and fine-grained classification with small inter-class differences and large intraclass differences: besides the standard Cross Entropy loss function, we have seen successful applications of the Seesaw loss <ref type="bibr" coords="8,228.08,606.27,16.23,10.91" target="#b12">[13]</ref>, Focal loss <ref type="bibr" coords="8,295.84,606.27,16.23,10.91" target="#b35">[36]</ref>, Arcface loss <ref type="bibr" coords="8,374.31,606.27,16.23,10.91" target="#b36">[37]</ref>, Sub-Center loss <ref type="bibr" coords="8,468.98,606.27,17.90,10.91" target="#b37">[38]</ref> and Adaptive Margin <ref type="bibr" coords="8,166.21,619.81,16.25,10.91" target="#b38">[39]</ref>.</p><p>We were happy to see the participants experimented with different use of the provided observation metadata, which often lead to improvements in the recognition scores. Besides the probabilistic baseline published with the dataset <ref type="bibr" coords="9,301.33,100.52,11.28,10.91" target="#b3">[4]</ref>, we have seen hand-crafted encoding of the metadata into feature vectors, as well as encoding of the metadata with a multilingual BERT model <ref type="bibr" coords="9,118.03,127.61,17.76,10.91" target="#b26">[27]</ref> and RoBERTa <ref type="bibr" coords="9,199.78,127.61,16.08,10.91" target="#b27">[28]</ref>. The metadata were then combined with image features extracted from a CNN or Transformer image classifier, or directly used as an input to Metaformer <ref type="bibr" coords="9,481.19,141.16,16.25,10.91" target="#b10">[11]</ref>.</p><p>The results of participants' comprehensive experiments with model architectures, loss functions and usage of metadata in fine-grained image-classification will help to improve species recognition services that aid researchers, citizen-science communities and nature enthusiasts. As discussed in Section 3, there is still a great space for improvement in the recognition of out-of-scope classes. Our evaluation of classification errors identified that confusion of poisonous mushrooms for edible is much more common than confusion of edible mushrooms for poisonous. This could be critical in applications that may affect the decision to consume a mushroom, and presents an important aspect to address in the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,409.58,416.70,9.96;2,89.29,421.54,273.48,9.96;2,93.18,290.96,134.64,100.98"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three fungi specimen observations from the Atlas of Danish Fungi dataset [4]. Atlas of Danish Fungi: ©Bedřiška Picková and ©Jan Riis-Hansen and ©Arne Pedersen.</figDesc><graphic coords="2,93.18,290.96,134.64,100.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,171.87,417.78,8.93;4,89.29,182.94,206.10,9.96;4,89.29,84.19,416.68,69.15"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Monthly observations distribution in the FungiCLEF 2022 training dataset. Three genera: Mycena, Boletus, and Exidia. Image taken from [4].</figDesc><graphic coords="4,89.29,84.19,416.68,69.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,525.25,418.35,8.93;6,89.29,537.25,416.70,8.87;6,89.29,549.21,54.01,8.87"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Out-of-scope identification performance on the full test set, i.e. what proportion of out-ofscope observations has been correctly classified as out-of-scope, compared to the accuracy over all observations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,89.29,292.14,416.69,8.93;7,88.93,304.14,414.58,8.87"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Species toxicity confusion on the full test set: Poisonous -&gt; Edible denotes poisonous fungi that were misclassified as edible, and Edible -&gt; Poisonous denotes edible fungi misclassified as poisonous.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.08,90.49,419.75,367.32"><head>Table 1</head><label>1</label><figDesc>FungiCLEF 2022 dataset statistics for each subset.The FungiCLEF 2022 dataset is based on data collected through the Atlas of Danish Fungi Web4  and mobile (iOS5 and Android 6 ) applications. The Atlas of Danish Fungi is a citizen science platform with more than 4,000 actively contributing volunteers and with more than 1 million content-checked observations of approximately 8,650 fungi species.</figDesc><table coords="3,88.08,122.10,419.75,335.71"><row><cell>Subset</cell><cell cols="5">Species Known Species Unknown Species Images Observations</cell></row><row><cell>Training</cell><cell>1,604</cell><cell>1,604</cell><cell>0</cell><cell>266,344</cell><cell>×</cell></row><row><cell>Validation</cell><cell>1,604</cell><cell>1,604</cell><cell>0</cell><cell>29,594</cell><cell>×</cell></row><row><cell>Test</cell><cell>3,134</cell><cell>1,165</cell><cell>1,969</cell><cell>118,676</cell><cell>59,420</cell></row><row><cell>2.1. Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Development set: For training, the competitors were provided with the DanishFungi 2020</cell></row><row><cell cols="6">(DF20) dataset [4]. DF20 contains 295,938 images -266,344 for training and 29,594 for validation</cell></row><row><cell cols="6">-belonging to 1,604 species. All training samples passed an expert validation process, guar-</cell></row><row><cell cols="6">anteeing high quality labels. Furthermore, rich observation metadata about habitat, substrate,</cell></row><row><cell cols="3">time, location, EXIF etc. are provided.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Test set: The test dataset is constructed from all observations submitted in 2021, for which</cell></row><row><cell cols="6">expert-verified species labels are available. It includes observations collected across all substrate</cell></row><row><cell cols="6">and habitat types. The test set contains 59,420 observations with 118,676 images belonging</cell></row><row><cell cols="6">to 3,134 species: 1,165 known from the training set and 1,969 unknown species covering ap-</cell></row><row><cell cols="6">proximately 30% of the test observations. The test set was further split into public (20%) and</cell></row><row><cell cols="6">private (80%) subsets -a common practice for Kaggle competitions to prevent participants</cell></row><row><cell cols="3">from overfitting to the leaderboard.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.63,86.27,411.35,416.11"><head></head><label></label><figDesc>Official FungiCLEF 2022 competition results, sorted by performance on the private set.</figDesc><table coords="6,88.63,86.27,411.35,416.11"><row><cell>Macro averaged F1 [%]</cell><cell>40 60 80</cell><cell cols="30">35.28 31.73 34.43 31.46 47.38 50.23 52.23 55.29 56.44 59.66 58.06 61.09 58.5 62.25 61.97 63.76 63.49 66.65 63.84 66.36 66.37 69.38 67.86 71.49 68.15 71.81 68.89 71.77 71.91 75.16 74.03 78.46 74.16 78.26 75.56 80.77 76.02 80.21 76.34 79.57 76.43 80.64 77.42 81.44 77.44 80.35 77.54 80.62 77.58 80.54 77.91 80.84 78.91 82.21 79.06 82.98 79.76 83.46 80.43 83.78 Public leaderboard Private Leaderboard</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>xiong</cell><cell>base</cell><cell>USTC-IAT-United</cell><cell>GG</cell><cell>LOL</cell><cell>TeamSpirit</cell><cell>Stefan</cell><cell>bearw</cell><cell>angzhe</cell><cell>Klawens</cell><cell>Harry Potterhehe</cell><cell>HaHaWork</cell><cell>X</cell><cell>Rziting</cell><cell>withHaiHaiLife</cell><cell>Goweild</cell><cell>philomel</cell><cell>Mingle1994</cell><cell>mengyao01</cell><cell>liuaihong</cell><cell>here weli</cell><cell>RongRongXue</cell><cell>MAGUS_YWX</cell><cell>KDELab</cell><cell>asdaas1</cell><cell>YHT_MT1</cell><cell>cmj</cell><cell>Gaurav</cell><cell>Duong Anh Kiet</cell><cell>arcsin2</cell></row><row><cell cols="32">base 5.07 Figure 3: xiong 0 10 20 6.85 30 40 50 60 70 80 Accuracy [%] 65.69 65.08 63.57 USTC-IAT-United 0.04 67.08 GG 16.06 63.57 LOL 0 65.1 TeamSpirit 11.13 64.78 Stefan 6.14 63.34 bearw 0 63.11 angzhe 0 62.92 Klawens 0 64.63 Harry Potterhehe 10.45 62.98 HaHaWork 0 62.57 X 0 62.34 Rziting 0 62.21 withHaiHaiLife 0 61.4 Goweild 0 60.12 philomel 0 60.96 Mingle1994 1.86 60.19 mengyao01 0 58.32 liuaihong 0 57.11 here weli 0 57.03 RongRongXue 0 57.7 MAGUS_YWX 5.5 54.52 KDELab 0 53.8 asdaas1 0.27 52.97 YHT_MT1 0.04 51.19 cmj 0 46.8 Gaurav 0 38.98 Duong Anh Kiet 0 44.02 arcsin2 44.55 Full Test Set Out-of-the Scope [Binary]</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,110.42,649.10,107.67,8.97"><p>http://www.clef-initiative.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,110.42,660.06,86.71,8.97"><p>http://www.lifeclef.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,110.42,671.02,152.86,8.97"><p>https://sites.google.com/view/fgvc9/home</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,108.93,649.12,109.95,8.97"><p>https://svampe.databasen.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,108.93,660.08,242.77,8.97"><p>https://apps.apple.com/us/app/atlas-of-danish-fungi/id1467728588</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,108.93,671.03,255.54,8.97"><p>https://play.google.com/store/apps/details?id=com.noque.svampeatlas</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="8,110.42,660.07,107.67,8.97"><p>http://www.clef-initiative.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="8,110.42,671.03,86.71,8.97"><p>http://www.lifeclef.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>LP was supported by the <rs type="funder">UWB</rs> grant, project No. <rs type="grantNumber">SGS-2022-017</rs>. LP was supported by the <rs type="funder">Technology Agency of the Czech Republic</rs>, project No. <rs type="grantNumber">SS05010008</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_exaGYYp">
					<idno type="grant-number">SGS-2022-017</idno>
				</org>
				<org type="funding" xml:id="_pzcpxNA">
					<idno type="grant-number">SS05010008</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,380.36,393.33,10.91;9,112.66,393.91,393.53,10.91;9,112.30,407.46,124.54,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,375.95,380.36,130.03,10.91;9,112.66,393.91,33.31,10.91">Fungi recognition: A practical use case</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,167.21,393.91,338.98,10.91;9,112.30,407.46,26.65,10.91">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2316" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,421.01,393.33,10.91;9,112.66,434.55,297.83,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,433.00,421.01,72.99,10.91;9,112.66,434.55,191.96,10.91">Automatic fungi recognition: Deep learning meets mycology</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Lind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,313.60,434.55,34.15,10.91">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">633</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,448.10,394.53,10.91;9,112.66,461.65,394.53,10.91;9,112.66,475.20,393.33,10.91;9,112.66,488.75,393.33,10.91;9,112.66,502.30,353.54,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,198.52,475.20,307.47,10.91;9,112.66,488.75,247.50,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,383.00,488.75,122.99,10.91;9,112.66,502.30,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,515.85,394.53,10.91;9,112.66,529.40,393.33,10.91;9,112.66,542.95,392.48,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,112.66,529.40,281.26,10.91">Danish fungi 2020-not just another image recognition dataset</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Laessøe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Frøslev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,420.58,529.40,85.40,10.91;9,112.66,542.95,294.59,10.91">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1525" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,556.50,393.33,10.91;9,112.66,570.05,393.33,10.91;9,112.66,583.60,178.54,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,434.88,556.50,71.10,10.91;9,112.66,570.05,187.77,10.91">Does closed-set training generalize to open-set recognition?</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zining</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weiqiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhicheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,322.40,570.05,183.58,10.91;9,112.66,583.60,147.84,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,597.15,393.53,10.91;9,112.66,610.69,393.59,10.91;9,112.66,624.24,261.72,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,407.33,597.15,98.86,10.91;9,112.66,610.69,265.79,10.91">An empirical study for fine-grained fungi recognition with transformer and convnet</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,401.56,610.69,104.69,10.91;9,112.66,624.24,231.02,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,637.79,394.53,10.91;9,112.66,651.34,393.33,10.91;9,112.66,664.89,178.54,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,112.66,651.34,185.19,10.91">Bag of tricks and a strong baseline for fgvc</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Shuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,320.36,651.34,185.63,10.91;9,112.66,664.89,147.84,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,86.97,393.53,10.91;10,112.66,100.52,393.33,10.91;10,112.66,114.06,107.76,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,229.73,86.97,276.46,10.91;10,112.66,100.52,95.23,10.91">When large kernel meets vision transformer: A solution for snakeclef &amp; fungiclef</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,234.83,100.52,271.16,10.91;10,112.66,114.06,77.06,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,127.61,393.33,10.91;10,112.66,141.16,393.33,10.91;10,112.66,154.71,393.33,10.91;10,112.66,168.26,107.76,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,444.83,127.61,61.16,10.91;10,112.66,141.16,393.33,10.91;10,112.66,154.71,86.06,10.91">Classification of fungi species: A deep learning based image feature extraction and gradient boosting ensemble approach</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Desingu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palaniappan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Chodisetty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bharathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,229.50,154.71,276.49,10.91;10,112.66,168.26,77.06,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,181.81,393.33,10.91;10,112.66,195.36,394.53,10.91;10,112.66,208.91,22.69,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,203.02,181.81,302.96,10.91;10,112.66,195.36,34.41,10.91">Transformer-based fine-grained fungi classification in an open-set scenario</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beyerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,169.51,195.36,332.50,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,222.46,393.53,10.91;10,112.66,236.01,288.50,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02751</idno>
		<title level="m" coord="10,307.65,222.46,198.53,10.91;10,112.66,236.01,106.31,10.91">Metaformer: A unified meta framework for fine-grained recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,249.56,394.61,10.91;10,112.66,263.11,394.53,10.91;10,112.66,276.66,100.87,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,383.60,249.56,103.81,10.91">A convnet for the 2020s</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,263.11,389.80,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,290.20,394.53,10.91;10,112.66,303.75,393.58,10.91;10,112.66,317.30,351.04,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,112.66,303.75,228.32,10.91">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,370.86,303.75,135.39,10.91;10,112.66,317.30,252.92,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9695" to="9704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,330.85,393.33,10.91;10,112.39,344.40,393.60,10.91;10,112.66,357.95,250.30,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,370.89,330.85,135.10,10.91;10,112.39,344.40,181.28,10.91">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,317.36,344.40,188.63,10.91;10,112.66,357.95,142.26,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,371.50,394.53,10.91;10,112.66,385.05,346.82,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,178.42,371.50,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,127.29,385.05,202.02,10.91">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,398.60,395.16,10.91;10,112.66,412.15,395.17,10.91;10,112.66,425.70,349.55,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m" coord="10,330.08,412.15,177.76,10.91;10,112.66,425.70,167.84,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,439.25,393.33,10.91;10,112.66,452.79,107.17,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m" coord="10,226.97,439.25,203.67,10.91">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,466.34,393.60,10.91;10,112.66,479.89,146.44,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09741</idno>
		<title level="m" coord="10,362.79,466.34,109.69,10.91">Visual attention network</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,493.44,393.33,10.91;10,112.66,506.99,354.88,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,254.27,493.44,251.72,10.91;10,112.66,506.99,19.47,10.91">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,139.95,506.99,233.51,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3965" to="3977" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,520.54,394.53,10.91;10,112.66,534.09,173.79,10.91" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13112</idno>
		<title level="m" coord="10,301.13,520.54,201.50,10.91">Volo: Vision outlooker for visual recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,547.64,394.62,10.91;10,112.66,561.19,394.53,10.91;10,112.66,574.74,100.87,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,259.21,547.64,223.84,10.91">Improving calibration for long-tailed recognition</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,561.19,389.80,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16489" to="16498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,588.29,394.53,10.91;10,112.66,601.84,395.00,10.91;10,112.41,615.39,38.81,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,211.54,588.29,290.80,10.91">Trivialaugment: Tuning-free yet state-of-the-art data augmentation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">G</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,127.24,601.84,334.38,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="774" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,628.93,394.62,10.91;10,112.66,642.48,394.53,10.91;10,112.41,656.03,27.76,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,322.30,628.93,161.24,10.91">Random erasing data augmentation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,642.48,265.36,10.91">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,669.58,393.32,10.91;11,112.66,86.97,393.32,10.91;11,112.66,100.52,233.71,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,329.08,669.58,176.90,10.91;11,112.66,86.97,182.01,10.91">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,317.28,86.97,188.70,10.91;11,112.66,100.52,136.06,10.91">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,114.06,395.17,10.91;11,112.66,127.61,197.93,10.91" xml:id="b24">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m" coord="11,331.02,114.06,176.81,10.91;11,112.66,127.61,16.17,10.91">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,141.16,395.17,10.91;11,112.66,154.71,393.33,10.91;11,112.33,168.26,29.19,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="11,323.58,141.16,184.25,10.91;11,112.66,154.71,138.15,10.91">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,258.65,154.71,234.28,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,181.81,393.33,10.91;11,112.66,195.36,363.59,10.91" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="11,353.43,181.81,152.55,10.91;11,112.66,195.36,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,208.91,394.53,10.91;11,112.30,222.46,393.68,10.91;11,112.66,236.01,107.17,10.91" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="11,173.53,222.46,256.77,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,249.56,394.53,10.91;11,112.66,263.11,394.53,10.91;11,112.66,276.66,90.72,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="11,251.23,249.56,251.23,10.91">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,127.24,263.11,375.49,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,290.20,395.17,10.91;11,112.66,303.75,393.33,10.91;11,112.66,317.30,317.74,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="11,302.02,290.20,205.81,10.91;11,112.66,303.75,172.74,10.91">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,308.62,303.75,197.37,10.91;11,112.66,317.30,229.32,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,330.85,394.62,10.91;11,112.66,344.40,394.53,10.91;11,112.66,357.95,22.69,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="11,267.98,330.85,214.89,10.91">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,344.40,389.59,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,371.50,393.33,10.91;11,112.66,385.05,393.33,10.91;11,112.66,398.60,117.06,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="11,298.43,371.50,207.56,10.91;11,112.66,385.05,71.84,10.91">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,206.94,385.05,299.05,10.91;11,112.66,398.60,86.61,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,412.15,394.53,10.91;11,112.66,425.70,352.61,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="11,178.42,412.15,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,127.29,425.70,207.49,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,439.25,393.33,10.91;11,112.66,452.79,394.52,10.91;11,112.66,466.34,394.04,10.91;11,112.66,479.89,233.99,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="11,211.45,439.25,186.67,10.91">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="http://doi.acm.org/10.1145/2939672.2939785.doi:10.1145/2939672.2939785" />
	</analytic>
	<monogr>
		<title level="m" coord="11,421.80,439.25,84.19,10.91;11,112.66,452.79,394.52,10.91;11,112.66,466.34,36.69,10.91">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,493.44,393.33,10.91;11,112.66,506.99,394.52,10.91;11,112.66,520.54,393.32,10.91;11,112.66,534.09,393.32,10.91;11,112.66,547.64,360.25,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="11,247.73,520.54,258.25,10.91;11,112.66,534.09,303.29,10.91">Overview of lifeclef 2021: a system-oriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Castañeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dorso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,439.18,534.09,66.80,10.91;11,112.66,547.64,330.51,10.91">Proceedings of the Twelfth International Conference of the CLEF Association (CLEF 2021)</title>
		<meeting>the Twelfth International Conference of the CLEF Association (CLEF 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,561.19,394.61,10.91;11,112.66,574.74,395.01,10.91" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="11,327.53,561.19,159.84,10.91">Focal loss for dense object detection</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,574.74,299.48,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,588.29,393.33,10.91;11,112.66,601.84,393.33,10.91;11,112.66,615.39,147.08,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="11,276.47,588.29,229.52,10.91;11,112.66,601.84,48.67,10.91">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,184.24,601.84,321.74,10.91;11,112.66,615.39,49.16,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,628.93,393.33,10.91;11,112.66,642.48,394.52,10.91;11,112.66,656.03,80.57,10.91" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="11,308.67,628.93,197.32,10.91;11,112.66,642.48,134.00,10.91">Sub-center arcface: Boosting face recognition by large-scale noisy web faces</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,269.49,642.48,189.95,10.91">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="741" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,669.58,393.33,10.91;12,112.66,86.97,393.33,10.91;12,112.66,100.52,159.65,10.91" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="11,259.22,669.58,246.77,10.91;12,112.66,86.97,48.18,10.91">Adaptiveface: Adaptive margin and sampling for face recognition</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,183.01,86.97,322.98,10.91;12,112.66,100.52,51.39,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11947" to="11956" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
