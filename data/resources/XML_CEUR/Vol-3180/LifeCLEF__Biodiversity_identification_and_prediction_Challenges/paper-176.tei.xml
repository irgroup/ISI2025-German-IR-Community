<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,397.68,15.42;1,89.29,106.66,338.35,15.42">Convolution Neural Network Fine-tuning for Plant and Animal Species Distribution Modelling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,76.94,11.96"><forename type="first">Mélisande</forename><surname>Teng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">AI institute</orgName>
								<address>
									<settlement>Mila</settlement>
									<region>Quebec</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,184.16,134.97,70.82,11.96"><forename type="first">Sal</forename><surname>Elkafrawy</surname></persName>
							<email>sal.elkafrawy@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">AI institute</orgName>
								<address>
									<settlement>Mila</settlement>
									<region>Quebec</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,397.68,15.42;1,89.29,106.66,338.35,15.42">Convolution Neural Network Fine-tuning for Plant and Animal Species Distribution Modelling</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">DD15B8D664DC0226863BE6D3A1827CBB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Representation learning</term>
					<term>Transfer learning</term>
					<term>Self Supervision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Biodiversity is declining at an unprecedented rate, and understanding the current state of biodiversity is a first step towards tackling this challenge. In particular, it is important to understand which species can be found in a given location for biodiversity management and conservation. In these working notes, we present our submission to the 2022 edition of the GeoLifeCLEF challenge which aims at predicting species from remote sensing data, as well as the avenues we explored. Using altitude and land cover data along with remote sensing image RGB patches as input, our model performs better than the baselines and was ranked 5th in the challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Biodiversity is declining at an unprecedented rate, threatening the achievement of the Sustainable Development Goals (SDGs) worldwide. Understanding the current state of biodiversity and where species can be found is an important first step towards biodiversity conservation, and a way to do so is through species distribution modelling. Traditional methods in ecology leverage environmental data to predict species range in space and time but recent advances in computer vision and remote sensing offers new perspectives. Indeed, information derived from remote sensing, such as the NDVI (Normalized Difference Vegetation Index) which is computed using the Red and Near-Infrared bands of a remote sensing image, has been shown to be relevant variables to include in ecology species distribution models <ref type="bibr" coords="1,357.63,489.97,13.00,10.91" target="#b0">[1]</ref> to improve performance, and there is growing interest in aligning remote sensing products and essential biodiversity variables to establish frameworks for priority biodiversity metrics to observe from space <ref type="bibr" coords="1,442.75,517.07,11.43,10.91" target="#b1">[2]</ref>. In this context, the GeoLifeCLEF challenge <ref type="bibr" coords="1,275.02,530.62,13.56,10.91" target="#b2">[3]</ref> was set up and aims at predicting the list of plant and animal species that are the most likely to be observed at a given location from remote sensing data, as one of the components of the LifeCLEF challenge <ref type="bibr" coords="1,379.71,557.71,13.35,10.91" target="#b3">[4]</ref>. The organizers propose three baseline models : a random forest on environmental variables, and convolution neural networks on remote sensing imagery (R, G, B or R, G, near IR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data Description</head><p>The dataset is a cleaned-up version of the previous year's challenge dataset <ref type="bibr" coords="2,434.30,111.28,11.59,10.91" target="#b4">[5]</ref>. The dataset contains 1.6M observations of locations in the US and France. Each observation has the following information:</p><p>• Remote sensing imagery: 256mx256m RGB-IR patches centered at each observation with 1m pixel resolution • Land cover data: 256mx256m patches centered at each observation with 1m pixel resolution • Altitude data: 256mx256m patches centered at each observation with 1m pixel resolution • Environmental variables: 19 low-resolution rasters of Bioclimatic data and 8 lowresolution rasters of Pedologic data Each location is associated with one of 17037 possible observed species, and in addition to the previous editions of the challenge, information about the genus and family of each species is provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Building on the challenge baseline</head><p>The challenge organizers propose two CNN baseline models using a pre-trained ResNet50 architecture, with either R,G,B bands or R,G and near-IR bands as inputs, the latter performing better than the former. Thus, after a first phase of replicating the challenge baselines, we decided to include the near-IR band in all the methods we considered.</p><p>Unless otherwise stated, we normalized the R,G,B and near-IR patches channel-wise using the means and standard deviations computed over the whole training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adding input information</head><p>While the challenge baseline was using a pre-trained ResNet50 on R,G,near-IR bands, we used all four R, G, B and near-IR bands and initialize the R, G, B channels with the pretrained weights, and randomly initialize the input weights of the near-IR band. We trained the model with cross entropy loss, stochastic gradient descent optimizer without momentum and initial learning rate of 0.01, and used random horizontal and vertical flipping transformations for data augmentation. We will refer to this model as Baseline++.</p><p>We also added land cover and altitude information in the input by stacking extracted patches to the RGB-IR patches as two extra channels, matching the resolution to that of the aerial data. We also tried adding land cover only (and not altitude) but the model did not perform as well as the one having both land cover and altitude information. We also tried training a model from scratch but the one using pre-trained weights (on ImageNet) performed better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Satellite Imagery Pretrained Model</head><p>Instead of using a ResNet-50 pretrained on the ImageNet dataset we used a recently publicised pretrained model on satellite image dataset that was trained with self-supervision using MoCov2 <ref type="bibr" coords="3,89.29,134.63,13.00,10.91" target="#b5">[6]</ref> method called SeCo <ref type="bibr" coords="3,201.68,134.63,13.00,10.91" target="#b6">[7]</ref> <ref type="foot" coords="3,214.67,132.88,3.71,7.97" target="#foot_0">1</ref> . The dataset in SeCo has 1M images (to be comparable with ImageNet). We used the same setting as in our baseline with RGB-IR as input: batch size is 32, number of finetuning epochs is 66, start learning rate is 0.01 with SGD optimizer without momentum. we used a 'reduce on plateau' learning rate scheduler with patience value of 5. The top-k error rate on the validation set was close but not better than the baseline's top-k error rate. The model's name is SeCo in table <ref type="bibr" coords="3,263.33,202.38,11.85,10.91" target="#b0">(1)</ref>.</p><p>One reason why SeCo is not better than a Resnet50 pretrained on ImageNet could be the difference in their type which led to different resolutions; SeCo is a satellite imagery dataset obtained from publicly available satellites, it has resolution of 10m, 20m and 60m. However, GeoLifeCLEF dataset is aerial imagery dataset which has 1m resolution as aerial images tend to have more details than satellite images. Also the heuristic involved to collect SeCo's dataset could be another reason; as the authors randomly sampled locations around the cities (within 50km radius).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multimodal with Environmental variables</head><p>We also tried to incorporate the environmental variables as an additional learning signal to the extended baseline Resnet50 model. The environmental variables are the tabular data that were extracted at the point of occurrence. We passed the environmental variables tabular data through a small scale of a Resnet block and concatenated both outputs (from the RGB-IR patches and the environmental variables Resnet) in a multimodal way to predict the final target of 17k species. Figure <ref type="bibr" coords="3,151.93,414.70,13.04,10.91" target="#b0">(1)</ref> shows the model architecture. We used ResNet model, one of few models that are reported as SOTA methods for deep learning with tabular data <ref type="bibr" coords="3,387.34,428.25,13.79,10.91" target="#b7">[8]</ref> <ref type="foot" coords="3,401.14,426.49,3.71,7.97" target="#foot_1">2</ref> . The best model has a top-k error rate higher than the organizers baseline by a margin of 0.04 on the validation set. The model was trained for 70 epochs with no indication of overfitting in earlier epochs. The model's other hyperparameters are: batch size is 32, number of finetuning epochs is 66, start learning rate is 0.01 with SGD optimizer without momentum. we used a 'reduce on plateau' learning rate scheduler with patience value of 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multitask learning</head><p>We explored a multitask learning setting where the tasks are :</p><p>• Species prediction (task S) • Land cover classification (semantic segmentation task) (task LC) • Country prediction (task C) All tasks share the same encoder and we use different decoders for each of the tasks. For the encoder, we tried two architectures: Deeplabv2 <ref type="bibr" coords="3,294.59,634.11,14.78,10.91" target="#b8">[9]</ref> and ResNet50 (up to the classification layer).  We used a Deeplabv2 decoder for LC, a multi-layer perceptron for S and a linear layer for C. In both cases we used pretrained ResnNet-50 and Deeplabv2 on ImageNet weights for the encoder. The loss is the sum of the cross-entropy losses for the LC and S tasks and the binary cross-entropy for the C task. However, our initial trials took too long to train, and due to time constraints, and need for optimization for faster training, we could not carry out these experiments fully. Hence we do not report our results in Table <ref type="table" coords="4,225.89,625.82,3.74,10.91" target="#tab_1">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Submission and discussion</head><p>Our best model, which was chosen for submission to the challenge was the one built up on the baseline as described in 3.1 a ResNet50 with RGB, near-IR, altitude and landcover patches as input trained for 25 epochs. Its performance reported on the private leaderboard 0.7013 top-30 error, and ranked 5th among the submissions to the challenge. In Figure <ref type="figure" coords="5,410.26,277.81,3.71,10.91" target="#fig_3">3</ref>, we compare it with the same model using only RGB and near-IR as input and notice that adding altitude and land cover only reduces the validation top-k error by 0.01. We also tried the same setting with a pretrained ResNet18 and Inceptionv3 architecture but the ResNet50 performed the best.</p><p>We were surprised that the other methods we tried did not improve much the challenge baseline, but we only did minimal hyperparameter tuning for each of the methods we tried. A main limitation we faced was the slow training of bigger models than ResNet50, especially in the multitask setting, as we trained our models on one GPU, but there is room for improvement of training speed. In the future, we would like to explore further the multitask learning setting, and adding data augmentations. We also would like to continue exploring models leveraging self-supervised learning as learning a good representation of remote sensing imagery could be meaningful for more tasks than the one defined by the challenge.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,289.12,173.89,8.93;4,151.80,84.19,291.69,192.37"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multi-modal model architecture</figDesc><graphic coords="4,151.80,84.19,291.69,192.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,506.92,396.64,8.93;4,151.80,315.97,291.68,178.38"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Multitask model architecture : all tasks share the same encoder and are trained jointly</figDesc><graphic coords="4,151.80,315.97,291.68,178.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,262.37,346.30,70.24,9.96;6,246.98,573.58,101.01,9.96"><head></head><label></label><figDesc>(a) Loss vs Epoch (b) Top-K error vs Epoch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,89.29,592.06,416.69,8.93;6,89.29,604.07,416.69,8.87;6,89.29,616.02,154.74,8.87;6,89.29,358.36,416.69,208.34"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of ResNet50 with RGBNIR and RGBNIR+landcover+altitude as input. Training is in full lines and Validation in dotted lines. The addition of the landcover and altitude informations improves very slightly the top-k error.</figDesc><graphic coords="6,89.29,358.36,416.69,208.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,119.90,86.37,355.49,58.33"><head>Model name Validation top-30 error Public test top-30 error</head><label></label><figDesc></figDesc><table coords="5,119.90,98.77,276.69,45.93"><row><cell cols="2">Challenge organizers Baseline -</cell><cell>0.7059</cell></row><row><cell>Baseline++</cell><cell>0.7052</cell><cell>0.6889</cell></row><row><cell>SeCo</cell><cell>0.7191</cell><cell>0.7116</cell></row><row><cell>multimodal</cell><cell>0.7465</cell><cell>0.7416</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,153.65,416.99,32.83"><head>Table 1</head><label>1</label><figDesc>Top-30 error values on the validation set and the public test set (computed over 10% of the entire test data)</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,108.93,660.07,215.10,8.97"><p>https://github.com/ElementAI/seasonal-contrast/tree/main</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,108.93,671.03,189.88,8.97"><p>https://github.com/Yura52/rtdl#papers-and-projects</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,107.59,111.28,398.68,10.91;7,107.59,124.83,400.08,10.91;7,107.59,138.38,174.74,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,285.26,111.28,221.01,10.91;7,107.59,124.83,238.94,10.91">Predicting species distributions and community composition using satellite remote sensing predictors</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pinto-Ledezma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cavender-Bares</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-021-96047-7</idno>
	</analytic>
	<monogr>
		<title level="j" coord="7,354.82,124.83,79.01,10.91">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">16448</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.59,151.93,399.60,10.91;7,107.59,165.48,398.40,10.91;7,107.59,179.03,259.17,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,203.86,165.48,156.01,10.91">Priorities for big biodiversity data</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Butchart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fegraus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Geller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hoft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hutton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kingston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mcrae</surname></persName>
		</author>
		<idno type="DOI">10.1002/fee.1473</idno>
	</analytic>
	<monogr>
		<title level="j" coord="7,372.67,165.48,133.32,10.91;7,107.59,179.03,58.60,10.91">Frontiers in Ecology and the Environment</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="124" to="125" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.59,192.57,399.68,10.91;7,107.59,206.12,398.40,10.91;7,107.59,219.67,400.08,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,358.26,192.57,121.70,10.91;7,107.59,206.12,398.40,10.91;7,107.59,219.67,16.73,10.91">Predicting species presence from multi-modal remote sensing, bioclimatic and pedologic data</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,146.43,219.67,331.25,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of GeoLifeCLEF</note>
</biblStruct>

<biblStruct coords="7,107.59,233.22,399.60,10.91;7,107.59,246.77,399.60,10.91;7,107.59,260.32,398.40,10.91;7,107.59,273.87,398.39,10.91;7,107.59,287.42,353.54,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,195.69,260.32,310.30,10.91;7,107.59,273.87,250.75,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,381.37,273.87,124.61,10.91;7,107.59,287.42,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.59,300.97,399.59,10.91;7,107.26,314.52,335.60,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<idno>arXiv: Arxiv-2004.04192</idno>
		<title level="m" coord="7,107.26,314.52,121.29,10.91">The geolifeclef 2020 dataset</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,107.59,328.07,398.40,10.91;7,107.59,341.62,217.16,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m" coord="7,282.79,328.07,223.20,10.91;7,107.59,341.62,35.05,10.91">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,107.59,355.17,400.25,10.91;7,107.59,368.71,398.65,10.91;7,107.26,382.26,29.19,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mañas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16607</idno>
		<title level="m" coord="7,396.37,355.17,111.46,10.91;7,107.59,368.71,251.61,10.91">Seasonal contrast: Unsupervised pre-training from uncurated remote sensing data</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,107.59,395.81,398.60,10.91;7,107.59,409.36,337.82,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,344.65,395.81,161.54,10.91;7,107.59,409.36,51.32,10.91">Revisiting deep learning models for tabular data</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gorishniy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Rubachev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,167.11,409.36,233.51,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.59,422.91,400.24,10.91;7,107.59,436.46,398.39,10.91;7,107.26,450.01,29.19,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m" coord="7,390.44,422.91,117.38,10.91;7,107.59,436.46,255.53,10.91">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
