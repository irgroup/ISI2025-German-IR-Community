<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.78,84.74,417.09,15.42;1,89.29,106.66,188.72,15.42">Transformer-based Fine-Grained Fungi Classification in an Open-Set Scenario</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,134.97,56.86,11.96"><forename type="first">Stefan</forename><surname>Wolf</surname></persName>
							<email>stefan.wolf@iosb.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Vision and Fusion Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology KIT</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Fraunhofer IOSB</orgName>
								<orgName type="department" key="dep2">Institute of Optronics</orgName>
								<orgName type="department" key="dep3">System Technologies and Image Exploitation</orgName>
								<address>
									<addrLine>Fraunhoferstrasse 1</addrLine>
									<postCode>76131</postCode>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Fraunhofer Center for Machine Learning</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,169.36,134.97,73.27,11.96"><forename type="first">Jürgen</forename><surname>Beyerer</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Vision and Fusion Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology KIT</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Fraunhofer IOSB</orgName>
								<orgName type="department" key="dep2">Institute of Optronics</orgName>
								<orgName type="department" key="dep3">System Technologies and Image Exploitation</orgName>
								<address>
									<addrLine>Fraunhoferstrasse 1</addrLine>
									<postCode>76131</postCode>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Fraunhofer Center for Machine Learning</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Technologiefabrik</orgName>
								<address>
									<addrLine>Haid-und-Neu-Str. 7</addrLine>
									<postCode>76131</postCode>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.78,84.74,417.09,15.42;1,89.29,106.66,188.72,15.42">Transformer-based Fine-Grained Fungi Classification in an Open-Set Scenario</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">3E5746848292BDEE010A070D473CFFA2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fungi classification</term>
					<term>Open-set classification</term>
					<term>FungiCLEF</term>
					<term>Vision transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained fungi classification describes the task of estimating the species of a fungus. The FungiCLEF 2022 challenge started a competition for the best solution to solve this task in an open-set scenario. For our solution, we employ a modern transformer-based classification architecture, use a class-balanced training scheme to handle the class-imbalance and apply heavy data augmentation. We approach the open-set scenario by using the final confidence scores as an indicator for unknown species. With this classification model, we were able to achieve an F1 score of 80.6 and 77.5 on the challenge's public and private test set, respectively. This resulted in achieving the 7 th place in the FungiCLEF 2022 challenge. We provide code at https://github.com/wolfstefan/fungi-classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accurately estimating the species of fungi is an important task for mushroom picker in order to identify toxic fungi. However, this task is difficult to learn due to the fine details distinguishing different species and the high number of species. Thus, an automatic system for classification of fungi species can save lives. Nonetheless, a classifier has to cope with similar challenges like the small differences distinguishing classes. Therefore, the classification model requires careful design choices. We design a classification model that is targeted towards this use case while supporting open-set classification tasks by recognizing images showing unknown fungi species. We design the classification model with simplicity in mind and thus, refrain from using ensembles of multiple models. Our model is based on a Swin Transformer Large <ref type="bibr" coords="1,447.13,535.80,12.81,10.91" target="#b0">[1]</ref> backbone, a class-balancing training scheme <ref type="bibr" coords="1,240.08,549.34,11.33,10.91" target="#b1">[2]</ref>, heavy data augmentation and a recognition of unknown species based on softmax scores.</p><p>With this classification model, we participated in the FungiCLEF 2022 <ref type="bibr" coords="1,410.04,576.44,12.75,10.91" target="#b2">[3]</ref> challenge -part of the LifeCLEF <ref type="bibr" coords="1,149.04,589.99,11.24,10.91" target="#b3">[4,</ref><ref type="bibr" coords="1,163.01,589.99,8.88,10.91" target="#b4">5]</ref> lab -and reached the 7 th place. The task of the FungiCLEF 2022 challenge is to classify the species of fungi based on the Danish Fungi 2020 <ref type="bibr" coords="1,364.82,603.54,12.69,10.91" target="#b5">[6]</ref> datset as training set and the Danish Fungi 2021 <ref type="bibr" coords="2,175.86,86.97,12.95,10.91" target="#b2">[3]</ref> dataset as test set in an open-set scenario. Thus, the classifier must be additionally able to recognize species not occurring in the Danish Fungi 2020 dataset.</p><p>In Section 2, we provide an overview over existing literature in the field of fine-grained fungi classification. In Section 3, we describe the method we use for this classification task. In Section 4, the experiments leading to several design decisions are shown. In Section 5, we conclude our contributions and indicate what future research directions might be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Image-based classification of fungi species has been studied in a small number of publications which are described in this section. Zieliński et al. <ref type="bibr" coords="2,307.88,226.89,12.68,10.91" target="#b6">[7]</ref> explored the classification of fungi species using microscopic images with an approach based on deep learning and bag-of-words. Sulc et al. <ref type="bibr" coords="2,102.19,253.99,12.77,10.91" target="#b7">[8]</ref> proposed a system for fungi classification based on an ensemble of convolutional neural networks. Picek et al. <ref type="bibr" coords="2,190.62,267.54,12.99,10.91" target="#b5">[6]</ref> published the fine-grained fungi dataset called Danish Fungi 2020 which contains metadata like location, habitat, substrate and date of the observation as possible inputs for classification systems in addition to images. Moreover, the authors evaluated multiple classification architectures based on convolutional neural networks and vision transformers with latter have proven to be superior. Additionally, the benefit of the metadata for the classification is highlighted with a significant increase in accuracy. Kiss and Czùni <ref type="bibr" coords="2,392.65,335.28,12.69,10.91" target="#b8">[9]</ref> proposed a mushroom dataset and explored different strategies of learning convolutional neural networks for an optimal classification accuracy of mushroom types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, the base model used for classification and the different adjustments to improve the accuracy of fine-grained fungi classification are described.</p><p>Deep learning model. We employ a Swin Transformer Large <ref type="bibr" coords="2,372.69,458.07,12.91,10.91" target="#b0">[1]</ref> model as the classification backbone to extract features. Swin Transformer is based on multi-head self-attention <ref type="bibr" coords="2,465.56,471.62,16.15,10.91" target="#b9">[10]</ref>. The input is grouped as windows and the multi-head self-attention is applied separately for each window to appropriately exploit the local relations of pixels in images. These windows are shifted after each layer to enable the use of cross-window relations. Similar to ResNet-like <ref type="bibr" coords="2,488.18,512.26,17.80,10.91" target="#b10">[11]</ref> architectures, the model is partitioned in stages with each stage reducing the resolution to create hierarchical feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class-balancing.</head><p>To cope with the imbalanced nature of the Danish Fungi 2020 <ref type="bibr" coords="2,442.12,562.87,12.69,10.91" target="#b5">[6]</ref> dataset, we apply the data resampling scheme proposed by Gupta et al. <ref type="bibr" coords="2,345.03,576.42,11.27,10.91" target="#b1">[2]</ref>. While this scheme was originally proposed for object detection, the application for single-label classification is straightforward. In the original implementation, a single image can increase the counter for multiple categories since multiple categories can occur in a single image in object detection tasks. For single-label image classification, the category-level counter is increased only for the single category of an image. We use a value of 10 -2 as oversample threshold.</p><p>Training strategy. The weights of the model are initialized from a model pretrained on ImageNet-21K <ref type="bibr" coords="3,154.54,86.97,16.09,10.91" target="#b11">[12]</ref>. AdamW <ref type="bibr" coords="3,216.64,86.97,17.75,10.91" target="#b12">[13]</ref> is employed as optimizer with a base learning rate of 7 • 10 -5 , a weight decay of 0.05 and betas of 0.9 and 0.999. A cosine annealing schedule <ref type="bibr" coords="3,432.42,100.52,17.76,10.91" target="#b13">[14]</ref> is applied to reduce the learning rate over the course of the training. A learning rate warm-up is applied for the first 4200 iterations with the initial learning rate being 1% of the base learning rate. Due to the class-balancing heavily increasing the size of a single epoch, the total length of the training is just 6 epochs. The batch size per GPU is 12 with 6 GPUs being used for the training resulting in a total batch size of 48. A label smoothing loss is applied <ref type="bibr" coords="3,354.80,168.26,16.25,10.91" target="#b14">[15]</ref>. Data augmentation. We use a broad set of augmentation techniques to improve the results. First, a crop is applied that extracts a slice of the image with the size of the slice randomly chosen between 8% and 100% of the original image. Afterwards, the slice is resized to 384×384. The is image is flipped horizontally with a probability of 50%. Moreover, RandAugment <ref type="bibr" coords="3,478.22,232.42,17.87,10.91" target="#b15">[16]</ref> is used with the default settings of MMClassification <ref type="bibr" coords="3,321.43,245.97,18.07,10.91" target="#b16">[17]</ref> for training Swin Transformer. This contains geometrical transformations like adjustments of brightness and geometrical operations like shearing. The last augmentation used is a random erase <ref type="bibr" coords="3,360.59,273.07,17.96,10.91" target="#b17">[18]</ref> that masks out a region of a size between 2% and 1 / 3 of the cropped image with a probability of 25%. The last step of the pre-processing pipeline is an image normalization. In contrast to the default data augmentation settings of Swin Transformer, we do not apply Mixup <ref type="bibr" coords="3,336.09,313.72,18.07,10.91" target="#b18">[19]</ref> or Cutmix <ref type="bibr" coords="3,406.56,313.72,18.07,10.91" target="#b19">[20]</ref> since they do not seem appropriate to us for fine-grained classification and preliminary experiments showed a negative impact.</p><p>During inference, the image is scaled with the smaller side being 438 pixels and the resolution of the larger side being chosen so that the original ratio is kept. Afterwards, a crop with the size of 384×384 pixels is taken from the center of the image and an image normalization is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data.</head><p>The training for the final challenge model is performed on the training and the validation set of the Danish Fungi 2020 <ref type="bibr" coords="3,218.63,418.52,12.84,10.91" target="#b5">[6]</ref> dataset.</p><p>Multi-view classification. The Danish Fungi 2020 <ref type="bibr" coords="3,318.09,442.04,12.70,10.91" target="#b5">[6]</ref> dataset used for training and validation and the Danish Fungi 2021 set used as test set for the FungiCLEF 2022 challenge contain multiple images per observation for many observations. For the sake of simplicity, we train the image in a single-view manner with each image treated as an individual sample. During inference on the test set of the challenge, the features of all images for a single observation are averaged channel-wise before the application of the final fully-connected classification layer. Additionally, we create a new image for each existing image by flipping it horizontally. These images are treated as additional views and the results are aggregated in the described way.</p><p>Open-set classification. The FungiCLEF 2022 challenge is using an open-set task with the Danish Fungi 2021 <ref type="bibr" coords="3,176.65,573.94,12.99,10.91" target="#b2">[3]</ref> set containing observations of fungi species without any observation in the the Danish Fungi 2020 <ref type="bibr" coords="3,223.62,587.49,13.00,10.91" target="#b5">[6]</ref> dataset. Thus, observations of unknown classes have to be marked as such. We mark all observations whose highest predicted score for a class after the application of the softmax is below 0.1 as an observation with an unknown class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our final model described in Section 3 is the result of multi experiments which are described in this section. For our ablation studies, the training is performed only on the training subset of the Danish Fungi 2020 <ref type="bibr" coords="4,205.89,138.38,12.99,10.91" target="#b5">[6]</ref> dataset. Four metrics are reported as results. The val top-1 and the val F1 are the top-1 accuracy and the macro F1 score on the validation subset of the Danish Fungi 2020 <ref type="bibr" coords="4,139.73,165.48,12.72,10.91" target="#b5">[6]</ref> dataset, respectively. The results on the validation set are per image and not per observation. Thus, no multi-view consensus is applied for these results. The test public F1 and the test private F1 are the official challenge results on the Danish Fungi 2021 <ref type="bibr" coords="4,430.81,192.57,12.87,10.91" target="#b2">[3]</ref> set which are reported as public and private score by Kaggle, respectively. These are macro F1 scores on a per observation base with the consensus method described in Section 3.</p><p>All models are trained and evaluated with MMClassification <ref type="bibr" coords="4,375.73,233.22,18.07,10.91" target="#b16">[17]</ref> (version 0.23) which is a classification framework based on PyTorch <ref type="bibr" coords="4,288.68,246.77,18.06,10.91" target="#b20">[21]</ref> (version 1.8.2). To reduce the training time required, we train the models with FP16 and dynamic loss scaling enabled.</p><p>Contrary to the final model, some experiments use a target resolution of 224×224 pixels. In these cases, the resolution prior to the cropping in the inference pipelines is 256 for the shorter edge. All experiments until Recognizing unknown species ignore the open-set scenario and predict one of the known classes for every observation.</p><p>Datasets. The Danish Fungi 2020 <ref type="bibr" coords="4,250.13,338.03,13.00,10.91" target="#b5">[6]</ref> dataset contains a total of 295,938 images. Of these, 266,344 are used for training and 29,594 are used for validation. The images are taken from 1,604 different fungi species. The Danish Fungi 2021 <ref type="bibr" coords="4,334.05,365.13,12.99,10.91" target="#b2">[3]</ref> test set contains a total of 118,675 images from 59,420 different observations. While both datasets provide metadata as e.g. time and location, our solution does not make use of them to keep the complexity of the approach low.</p><p>Multi-view consensus. In case of post-classifier-consensus, the whole network is executed for each image independently and the per-class scores of the final fully-connected layer are aggregated by multiplication. The pre-classifier-consensus executes the network up to the last layer before the final fully-connected layer and averages the feature vectors of all images for a single observation. Afterwards, the average feature vector is fed into the final fully-connected layer to predict the class scores of the observation. The experiment is done with a ResNet-50 <ref type="bibr" coords="4,488.22,497.03,17.76,10.91" target="#b10">[11]</ref> model, a target resolution of 224 pixels, no label-smoothing, no class-balanced training, a total batch size of 256 spread across 2 GPUs and a learning rate of 10 -4 . The results are shown in Table <ref type="table" coords="4,115.39,537.68,3.68,10.91" target="#tab_0">1</ref>. No results are reported for the validation set since we performed the evaluation on the validation set image-wise instead of observation-wise. The results show a slight advantage for the pre-classifier-consensus mode. The pre-classifier-consensus mode is used for every further experiment.</p><p>Class-balanced training. We evaluate the class-balanced training strategy as described in Section 3. The number of epochs is reduced to compensate the higher number of iterations per epoch due to the class-balanced training. The evaluation is done with a ResNet-50 backbone trained on two GPUs with a batch size of 128 per GPU, a base learning rate of 10 -4 and without label smoothing. The target resolution is 224×224 for these experiments. A lower oversample thresholds leads to a stronger oversampling of underrepresented classes. According to the results, the impact on the F1 score is not significant and the top-1 score is reduced since the model cannot exploited the class-imbalance anymore. However, the results on the challenge test set showed an improvement which lead to the decision to apply the class-balancing with an oversample threshold of 10 -2 on the final model.   flipped image.</p><p>Recognizing unknown species. As described in Section 3, we mark an observation as one with an unknown species if the highest score is below a certain threshold. In Table <ref type="table" coords="6,480.18,549.63,3.81,10.91" target="#tab_5">5</ref>, the results for different thresholds are shown. The best F1 score was reached with a threshold of 0.2. However, due to a limited number of evaluations possible prior to the end of the challenge, we found this threshold to better only after the end of the challenge and chose a threshold of 0.1 for the final model. With this threshold, 1308 of the 59420 observations are marked as ones with an unknown species.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,439.67,416.69,10.91;5,89.29,453.22,417.89,10.91;5,89.29,466.77,416.94,10.91;5,89.29,480.32,416.69,10.91;5,89.29,493.87,418.37,10.91;5,88.96,507.42,128.28,10.91;5,217.25,506.20,10.82,6.99;5,228.57,507.42,110.99,10.91"><head>Choosing the base model</head><label></label><figDesc>and the resolution. We evaluate multiple deep learning backbones and two different resolutions for the classification. The models evaluated are ResNet-50<ref type="bibr" coords="5,486.72,453.22,16.37,10.91" target="#b10">[11]</ref>, Swin Base and Swin Large. The experiments are either done with a target resolution of 224×224 pixels or with the final resolution of 384×384 pixels as described in Section 3. The experiments are run with label smoothing, class-balanced training, a batch size of 16 per GPU and 4 GPUs. The base learning rate is 6•10 -5 . The results are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,418.65,80.91"><head>Table 1</head><label>1</label><figDesc>Results of two different multi-view consensus modes. Due to the slight advantage of the pre-classifierconsensus mode, we choose this one for the final model.</figDesc><table coords="5,175.39,133.14,242.00,38.25"><row><cell>Consensus type</cell><cell cols="2">Test public F1 Test private F1</cell></row><row><cell>Post-classifier-consensus</cell><cell>66.6</cell><cell>63.6</cell></row><row><cell>Pre-classifier-consensus</cell><cell>66.7</cell><cell>63.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.93,193.65,418.58,141.60"><head>Table 2</head><label>2</label><figDesc>Results of class-balanced training with different oversample thresholds. Since a smaller oversample thresholds results in larger number of iterations per epoch, we reduce the number of epochs. The results indicate no significant difference for the F1 score and a reduced top-1 score on the validation set. However, results on both parts of test set show an improvement. Thus, we apply the class-balancing with an increased oversample threshold of 10 -2 for the final model.</figDesc><table coords="5,100.96,273.09,391.41,62.16"><row><cell>Class-balancing</cell><cell>Oversample threshold</cell><cell>Iterations per epoch</cell><cell>Epochs</cell><cell>Val top-1</cell><cell>Val F1</cell><cell>Test public F1</cell><cell>Test private F1</cell></row><row><cell>No</cell><cell>-</cell><cell>1041</cell><cell>100</cell><cell>75.0</cell><cell>67.3</cell><cell>66.7</cell><cell>63.6</cell></row><row><cell>Yes</cell><cell>10 -3</cell><cell>1626</cell><cell>100</cell><cell>74.8</cell><cell>67.4</cell><cell>66.5</cell><cell>62.9</cell></row><row><cell>Yes</cell><cell>10 -2</cell><cell>4215</cell><cell>25</cell><cell>74.5</cell><cell>67.3</cell><cell>68.1</cell><cell>63.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.93,507.42,417.06,169.91"><head>Table 3 .</head><label>3</label><figDesc>The results show a significant improvement from ResNet-50 to Swin Base and also a slight improvement from Swin Base to Swin Large. The largest improvement can be gained by switching from a resolution of 224 to a resolution of 384. We reduced the number of epochs for most experiments to save compute resources. While this leads to a significantly reduced accuracy for ResNet-50, a longer training schedule provides only a slight advantage for the Swin Large model. Since this advantage is nonexistent for the results on the test set, we apply the shorter schedule with 6 epochs for the final model. To evaluate the test-time augmentation using horizontal flip, we train a Swin Large model on the training and validation set with the final settings as described in Section 3 and only provide the results for the test set. The results are shown in Table4. The results indicate a slight benefit for the combination of processing the original image and the</figDesc><table coords="5,88.93,626.71,123.34,9.76"><row><cell>Test-time augmentation.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,88.99,90.49,417.00,153.56"><head>Table 3</head><label>3</label><figDesc>Results of different backbones and different resolutions. To reduce the training time, we decrease the number of epochs for the ablation studies and increase it again for the final model. The results indicate a clear advantage of the newer Swin Transformer models over the ResNet-50 model and also an advantage of the higher resolution.</figDesc><table coords="6,99.09,157.97,394.61,86.08"><row><cell cols="7">Backbone Target resolution Epochs Val top-1 Val F1 Test public F1 Test private F1</cell></row><row><cell>ResNet-50</cell><cell>224</cell><cell>25</cell><cell>72.3</cell><cell>64.4</cell><cell>68.4</cell><cell>64.5</cell></row><row><cell>ResNet-50</cell><cell>224</cell><cell>6</cell><cell>62.7</cell><cell>53.5</cell><cell>61.8</cell><cell>57.3</cell></row><row><cell>Swin Base</cell><cell>224</cell><cell>6</cell><cell>79.5</cell><cell>73.3</cell><cell>75.2</cell><cell>71.7</cell></row><row><cell>Swin Base</cell><cell>384</cell><cell>6</cell><cell>85.0</cell><cell>79.7</cell><cell>79.2</cell><cell>75.5</cell></row><row><cell>Swin Large</cell><cell>384</cell><cell>6</cell><cell>85.9</cell><cell>81.3</cell><cell>79.2</cell><cell>75.8</cell></row><row><cell>Swin Large</cell><cell>384</cell><cell>12</cell><cell>87.0</cell><cell>82.3</cell><cell>79.1</cell><cell>76.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,88.99,266.30,417.00,81.83"><head>Table 4</head><label>4</label><figDesc>Results of horizontal flip as test-time augmentation. Processing both the original image and the horizontal image and combining both results lead to a slight increase in accuracy for the test set.</figDesc><table coords="6,177.17,309.87,238.44,38.25"><row><cell cols="3">Test-time augmentation Test public F1 Test private F1</cell></row><row><cell>None</cell><cell>79.6</cell><cell>76.5</cell></row><row><cell>Horizontal flip</cell><cell>79.8</cell><cell>76.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,88.99,370.39,417.00,115.47"><head>Table 5</head><label>5</label><figDesc>Results of different score thresholds. All observations below the threshold are marked as ones with an unknown species. For comparison, the total number of observations in both sets is 59420. While a threshold of 0.2 has shown the best accuracy after the end of the challenge, we chose 0.1 for our final model.</figDesc><table coords="6,108.46,435.65,375.86,50.21"><row><cell cols="4">Threshold for unknown species # unknown observation Test public F1 Test private F1</cell></row><row><cell>0.05</cell><cell>554</cell><cell>80.3</cell><cell>77.1</cell></row><row><cell>0.1</cell><cell>1308</cell><cell>80.6</cell><cell>77.5</cell></row><row><cell>0.2</cell><cell>2651</cell><cell>80.7</cell><cell>77.7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">Conclusion</head><p>We proposed a classifier targeted towards fine-grained fungi classification with the support of open-set scenarios. The classifier is based on a modern Swin Transformer Large backbone generating highly expressive features and a multi-view aggregation step to exploit the availability of multiple images per observation. To improve the accuracy, we make use of heavy data augmentation and apply a class-balancing training scheme. Future directions for research might be the incorporation of metadata to aid the classification or a more advanced aggregation of the results from the different images of an observation.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,251.20,393.33,10.91;7,112.39,264.75,393.60,10.91;7,112.66,278.30,250.30,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,370.89,251.20,135.10,10.91;7,112.39,264.75,181.28,10.91">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,317.36,264.75,188.63,10.91;7,112.66,278.30,142.26,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,291.85,394.53,10.91;7,112.66,305.40,394.53,10.91;7,112.66,318.95,90.72,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,251.23,291.85,251.23,10.91">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,127.24,305.40,375.49,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,332.50,393.33,10.91;7,112.66,346.05,395.17,10.91;7,112.66,359.59,232.29,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,339.05,332.50,110.41,10.91;7,480.07,332.50,25.92,10.91;7,112.66,346.05,223.69,10.91">Fungi recognition as an open set classification problem</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,365.49,346.05,142.34,10.91;7,112.66,359.59,201.59,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of FungiCLEF</note>
</biblStruct>

<biblStruct coords="7,112.66,373.14,394.53,10.91;7,112.66,386.69,393.33,10.91;7,112.66,400.24,393.33,10.91;7,112.66,413.79,168.28,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,185.62,386.69,320.37,10.91;7,112.66,400.24,208.65,10.91">Lifeclef 2022 teaser: An evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,343.19,400.24,162.80,10.91;7,112.66,413.79,38.01,10.91">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,427.34,394.53,10.91;7,112.66,440.89,394.53,10.91;7,112.66,454.44,393.33,10.91;7,112.66,467.99,393.33,10.91;7,112.66,481.54,353.54,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,198.52,454.44,307.47,10.91;7,112.66,467.99,247.50,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,383.00,467.99,122.99,10.91;7,112.66,481.54,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,495.09,394.53,10.91;7,112.66,508.64,393.33,10.91;7,112.66,522.18,392.48,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,112.66,508.64,281.26,10.91">Danish fungi 2020-not just another image recognition dataset</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Laessøe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Frøslev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,420.58,508.64,85.40,10.91;7,112.66,522.18,294.59,10.91">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1525" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,535.73,393.33,10.91;7,112.66,549.28,393.98,10.91;7,112.66,562.83,42.79,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,483.13,535.73,22.86,10.91;7,112.66,549.28,303.43,10.91">Deep learning approach to describe and classify fungi microscopic images</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zieliński</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sroka-Oleksiak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rymarczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Piekarczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brzychczy-Włoch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,424.87,549.28,39.04,10.91">PloS one</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">234806</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,576.38,393.33,10.91;7,112.66,589.93,393.53,10.91;7,112.30,603.48,124.54,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,375.95,576.38,130.03,10.91;7,112.66,589.93,33.31,10.91">Fungi recognition: A practical use case</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,167.21,589.93,338.98,10.91;7,112.30,603.48,26.65,10.91">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2316" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,617.03,393.33,10.91;7,112.66,630.58,393.33,10.91;7,112.66,644.13,200.36,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,196.45,617.03,309.54,10.91;7,112.66,630.58,78.68,10.91">Mushroom image classification with cnns: A case-study of different learning strategies</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Czùni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,212.74,630.58,293.25,10.91;7,112.66,644.13,86.15,10.91">2021 12th International Symposium on Image and Signal Processing and Analysis (ISPA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="165" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,657.68,395.17,10.91;8,112.66,86.97,393.33,10.91;8,112.33,100.52,29.19,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,484.04,657.68,23.79,10.91;8,112.66,86.97,143.41,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,264.71,86.97,228.49,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Polo-sukhin</note>
</biblStruct>

<biblStruct coords="8,112.66,114.06,395.17,10.91;8,112.66,127.61,395.01,10.91;8,112.41,141.16,38.81,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,259.74,114.06,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,488.38,114.06,19.45,10.91;8,112.66,127.61,347.24,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,154.71,393.33,10.91;8,112.66,168.26,394.53,10.91;8,112.66,181.81,103.61,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,346.64,154.71,159.35,10.91;8,112.66,168.26,67.28,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,228.08,168.26,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,195.36,393.33,10.91;8,112.66,208.91,107.17,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m" coord="8,238.15,195.36,182.94,10.91">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,222.46,393.59,10.91;8,112.66,236.01,146.44,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m" coord="8,226.91,222.46,243.29,10.91">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,249.56,393.33,10.91;8,112.66,263.11,393.33,10.91;8,112.66,276.66,147.08,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,344.36,249.56,161.63,10.91;8,112.66,263.11,83.10,10.91">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,216.60,263.11,289.39,10.91;8,112.66,276.66,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,290.20,395.17,10.91;8,112.66,303.75,393.32,10.91;8,112.66,317.30,325.92,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,302.02,290.20,205.81,10.91;8,112.66,303.75,171.71,10.91">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,307.43,303.75,198.55,10.91;8,112.66,317.30,237.36,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,330.85,395.01,10.91;8,112.66,344.40,185.16,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmclassification" />
		<title level="m" coord="8,187.41,330.85,250.53,10.91">Openmmlab&apos;s image classification toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,357.95,394.62,10.91;8,112.66,371.50,394.53,10.91;8,112.41,385.05,27.76,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,322.30,357.95,161.24,10.91">Random erasing data augmentation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,112.66,371.50,265.36,10.91">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,398.60,395.17,10.91;8,112.66,412.15,309.29,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,331.02,398.60,176.81,10.91;8,112.66,412.15,16.17,10.91">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,151.43,412.15,240.50,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,425.70,393.32,10.91;8,112.66,439.25,393.32,10.91;8,112.66,452.79,233.71,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="8,329.08,425.70,176.90,10.91;8,112.66,439.25,182.01,10.91">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,317.28,439.25,188.70,10.91;8,112.66,452.79,136.06,10.91">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,466.34,394.53,10.91;8,112.66,479.89,393.33,10.91;8,112.66,493.44,350.57,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="8,261.26,479.89,244.72,10.91;8,112.66,493.44,67.64,10.91">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,188.20,493.44,230.24,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
