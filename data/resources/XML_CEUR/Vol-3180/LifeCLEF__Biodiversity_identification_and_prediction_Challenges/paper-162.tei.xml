<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,379.69,15.42;1,89.29,106.66,356.39,15.42;1,89.29,128.58,230.94,15.43">Classification of Fungi Species: A Deep Learning Based Image Feature Extraction and Gradient Boosting Ensemble Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,80.34,11.96"><forename type="first">Karthik</forename><surname>Desingu</surname></persName>
							<email>karthik19047@cse.ssn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,182.27,156.89,83.91,11.96"><forename type="first">Anirudh</forename><surname>Bhaskar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.83,156.89,115.20,11.96"><forename type="first">Mirunalini</forename><surname>Palaniappan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,170.84,137.29,11.96"><forename type="first">Eeswara</forename><forename type="middle">Anvesh</forename><surname>Chodisetty</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.58,170.84,100.55,11.96"><forename type="first">Haricharan</forename><surname>Bharathi</surname></persName>
							<email>haricharan2010267@ssn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,379.69,15.42;1,89.29,106.66,356.39,15.42;1,89.29,128.58,230.94,15.43">Classification of Fungi Species: A Deep Learning Based Image Feature Extraction and Gradient Boosting Ensemble Approach</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">EB39518DCEB131EFBB391DFB1D1F4911</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ensemble Learning</term>
					<term>Convolutional Neural Networks</term>
					<term>Gradient Boosting Ensemble</term>
					<term>Metadata-aided Classification</term>
					<term>Image Classification</term>
					<term>Transfer Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has been successful in a variety of challenging image classification tasks, characterized by complex and large datasets. Ensemble learning further improves model performance by inferring compound decision boundaries in the feature space, and assorting importance to the most representative features to effectively discriminate between image classes. This paper details a deep learning based feature extraction and subsequent boosting ensemble approach for fungi species classification. The proposed workflow leverages state-of-the-art deep learning architectures such as ResNeXt and Efficient-Net among others, trains them by transfer learning onto a fungi image dataset for feature extraction, and finally integrates the output representation vectors with geographic metadata to train a gradient boosting ensemble classifier that predicts the fungi species. The authors trained multiple deep learning architectures, assessed their individual performance and selected effective feature extraction models. Multiple experiments were performed in choosing these models, and to subsequently perform hyperparameter tuning to train the boosting classifier. The approach attained a maximum macro-averaged F1-Score of 48.96% on the test data. The corresponding validation F1-Score and Accuracy scores were 50.22% and 85.11%, respectively. Furthermore, the best model exhibited more confident predictions than its inferior counterparts, indicating improved inter-class discerning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper presents the participation of Sri Sivasubramaniya Nadar College of Engineering, India for the FungiCLEF-2022 <ref type="bibr" coords="1,229.87,516.67,12.99,10.91" target="#b0">[1]</ref> challenge for fungi species identification held jointly by LifeCLEF-2022 <ref type="bibr" coords="1,157.87,530.22,11.42,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,172.03,530.22,9.00,10.91" target="#b2">3]</ref> lab of the CLEF 2022 conference and the FGVC9 workshop organized in conjunction with CVPR 2022 conference.</p><p>In India, the infection rate of fungi-based diseases such as Mucormycosis was 45374, with a mortality rate of over 4300. Fungi are also essential to the survival of many organisms and sustaining the food cycle as they attract attention as predators of invertebrate animals, act as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The usage of convolutional neural networks in the classification and identification of fungi species is a concept that has been floating around for quite a while. The integration of computer vision and machine learning with the development of more efficient algorithms will undoubtedly be a hotspot for future studies in the context of the mushroom industry <ref type="bibr" coords="2,410.32,240.44,11.43,10.91" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">FungiCLEF</head><p>A study in 2021 in the Turkish Journal of Computer and Mathematics Education by Sukanya S. Gaikwad <ref type="bibr" coords="2,130.33,303.71,12.69,10.91" target="#b4">[5]</ref> used a convolutional neural network to classify various species of fungi, specifically on fungi affected apple leaf diseases. They obtained an accuracy of 88.90% in the classification of fungi.</p><p>In a study by M.E. Mital, pre-trained deep learning models were employed in classifying 9 kinds of Aspergillus. The methodology comprised of preprocessing, deep-learning and performance evaluation. This study achieved a 93.33% testing accuracy proving that the transferred knowledge is accurate, compatible and reliable <ref type="bibr" coords="2,299.59,385.01,11.43,10.91" target="#b5">[6]</ref>.</p><p>A submission to the Danish Fungi 2020 by LukÃ¡Å¡ Picek showed that experiments using convolutional neural networks and the recent Vision Transformers showed that ViT achieves results superior to convolutional neural network baselines with 80.45% accuracy and 0.74 macro F1 score, reducing the convolutional neural network error by 9% and 12% respectively. By incorporating metadata into the decision tree process, the error rate came down significantly by 15% <ref type="bibr" coords="2,122.84,466.30,11.43,10.91" target="#b6">[7]</ref>.</p><p>A study by Krzysztof PrzybyÅ‚ to evaluate rapeseed samples obtained in the process of storage experiments with different humidity and temperature conditions, the classification was carried out based on the different levels of contamination with filamentous fungi. The classifiers that were compared were devised on the basis of the environments TensorFlow (deep learning) and Statistics (machine learning). The lowest classification error of 14% for the test set, 18% classification error for Multi-Layer Perceptron Networks (MLPN), and 21% classification error for Radial Basis Function Networks (RBFN), in the process of recognizing mold in rapeseed with the use of convolutional neural networks <ref type="bibr" coords="2,275.12,574.70,11.43,10.91" target="#b7">[8]</ref>.</p><p>Finally, a paper written by Pranjal Maurya focused on developing a method for classification of mushroom using its texture feature, which is based on the machine learning approach. The performance published was 76.60% by using support vector machine classifier, which was found better with respect to the other classifiers like k-nearest neighbors, Logistic Regression, Linear Discriminant, Decision Tree, and Ensemble classifiers <ref type="bibr" coords="2,330.25,642.44,11.43,10.91" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Convolutional neural networks are deep learning algorithms that use 3 major types of layers, namely: convolutional layer, pooling layer, and fully connected layer.</p><p>In our approach to this problem, we have used different convolutional neural network-based approaches like ResNet101, ResNet50, and EfficientNetB0. The workflow was implemented using Python v3.9.0 and primarily uses TensorFlow V.2.4.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The training dataset of the FungiCLEF-2022 challenge consists of 295938 training images spread over 99% fungi and 1% protozoa belonging to 1604 species observed mostly around Denmark. This challenge, being a new entrant into the foray of LifeCLEF challenges, unfortunately cannot be compared to previous year's datasets. The dataset includes 30 different country codes 5 localities with just over 1% distribution among Store Hareskov and Hegedal and the remaining 98% over the other localities. All the methods used involve convolutional neural networks. The models used are ResNet101 and EfficientNetB0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image Preprocessing</head><p>The photos were first placed into an input sequencer, where the outlier images were found. The images were analyzed, and it was discovered that they were of varying sizes and scales. As a result, image augmentation was used to convert all of the photographs to RGB and scale them to a standard size of 800 by 600 pixels 224 Ã— 224 Ã— 3 dimensions <ref type="bibr" coords="3,380.43,386.87,16.25,10.91" target="#b9">[10]</ref>.</p><p>The images were linearly normalized to values between 0 and 1 to reduce the effect of irrelevant characteristics in the context of the needed task, such as variance in lighting conditions among the shots. To make the model more general, immune to the impact of positional and orientation-based bias, and prevent memory by improving image diversity, transformations such as scale and rotation, as well as contrast and saturation variations, were induced on the model inputs. The aforementioned modifications were utilized to augment the input photos with RandAugment <ref type="bibr" coords="3,181.89,481.72,16.41,10.91" target="#b10">[11]</ref>. RandAugment has two parameters: the number of augmentation transformations to apply in order (N) and the magnitude of all modifications (M). N=3 and M=4 were chosen as the values for the ResNet model by experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Extraction</head><p>Feature extraction is a method prescribed to transform raw data into numerical features. It is preferred as applying machine learning algorithms directly on the raw data yields poorer results. High data rates and information redundancy can be cut down using feature extraction. Since the task this time involved a large data set of over 110GB of training images along with geographic metadata, the data rate is very high. Thus, redundant features are an obvious by-product. Feature extraction thus helped extract only the unique features that describe the images, such as shapes and edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Deep Learning Models Considered</head><p>The feature extraction models for fungi image was selected after experimenting with multiple deep-learning architectures.</p><p>ResNet101 <ref type="bibr" coords="4,149.11,134.63,17.83,10.91" target="#b11">[12]</ref> is a well-known convolutional neural network model that was introduced in 2015. This model addresses the degradation problem, which asserts that as network depth grows, accuracy becomes saturated and subsequently rapidly declines. ResNet solves the degradation problem by using shortcut connections that bypass one or more layers, which was inspired by the Highway network <ref type="bibr" coords="4,189.09,188.83,16.17,10.91" target="#b12">[13]</ref>, which employed gated shortcut connections to manage the flow of information in the shortcut.</p><p>EfficientNet(s) <ref type="bibr" coords="4,165.18,215.93,17.76,10.91" target="#b13">[14]</ref> are a class of convolutional neural networks that were built in 2019. It's a small-scale architecture with about 11 million trainable parameters. It was created with the help of a multi-objective neural network that prioritized precision and floating point operations. It supports compound scaling while maintaining network balance across all dimensions. It employs an inverted bottleneck as well as a depth-wise convolutional network that includes squeeze and excitation operations. It employs MBConv blocks <ref type="bibr" coords="4,361.68,283.68,17.76,10.91" target="#b14">[15]</ref> that serve as Inverted Linear BottleNeck layers. These layers use Depth-Wise Separable Convolution operations. The model complexity of the variants increases from B0 to B7. The authors experimented with B0, B4 and B6 variants to scale the model complexity and find the best suited intricacy for the fungi dataset.</p><p>Another popular convolutional neural network model, ResNeXt101 <ref type="bibr" coords="4,394.30,337.87,16.08,10.91" target="#b15">[16]</ref>, is very similar to the ResNet101 model. ResNet101 features a lot of sequential layers, while ResNeXt101 contains a lot of parallel stacking layers. Like the Inception module <ref type="bibr" coords="4,329.65,364.97,16.45,10.91" target="#b16">[17,</ref><ref type="bibr" coords="4,348.84,364.97,12.34,10.91" target="#b17">18]</ref>, it uses a split-transform-merge technique. ResNeXt shares hyperparameters for all the blocks, unlike the Inception module which has different filters and sizes for each block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Gradient Boosting Ensemble Classifier</head><p>Ensemble methods are techniques used to counter the high variance produced by a single neural network by adding an inherent bias that is obtained from multiple models. They tend to give higher accuracy than their resident models. Boosting is an ensemble technique in which the new models are sequentially added to the existing features to correct the errors.</p><p>In our method, an ensemble of the models trained off the aforementioned architectures were conjoined with metadata features -country, three-level-precise location information, substrate and habitat. These metadata features along with the 4096 features extracted from each of the two models, was concatenated and fed to a gradient boosting classifier.</p><p>The team used the XGBoost library package <ref type="bibr" coords="4,303.40,550.19,18.07,10.91" target="#b18">[19]</ref> for implementing the boosting classifier. XGBoost is a high-speed and high-performance implementation of gradient boosted decision trees. XGBoost's superior execution speed was factored-in while choosing from all the available implementation libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Prediction of Out-of-the-Scope Classes</head><p>The FungiCLEF-2022 test dataset is known to contain images of Fungi that belong to classes, not exposed to the model as part of the training set. From a practical perspective, predicting a previously unseen observation as an unidentified class is as salient as assorting a seen class into its corresponding class. Failure to do either, is misleading. It is, therefore, imperative that such out-of-the-scope observations are identified and flagged.</p><p>To manage such observations during prediction, the authors have adopted a prediction confidence thresholding strategy. The gradient boosting classifier employs the softmax activation function in its classification layer. Consequently, it outputs a discrete probability distribution -ğ‘ƒ , over all the 1604 seen classes, such that Equation 1 is satisfied.</p><formula xml:id="formula_0" coords="5,256.57,178.22,249.42,33.71">1603 âˆ‘ï¸ ğ‘–=0 ğ‘ƒ (ğ‘‹ = ğ‘–) = 1<label>(1)</label></formula><p>where, ğ‘– denotes the class id of the known classes of fungi species, and ğ‘ƒ (ğ‘‹ = ğ‘–) denotes the probability that a specific observation belongs to class ğ‘–.</p><p>The most predominantly adopted strategy to predict the observation's class from this distribution is the Maximum Likelihood Estimate (MLE) <ref type="bibr" coords="5,322.15,258.90,18.06,10.91" target="#b19">[20]</ref> of this probability distribution, with respect to the training data -predict that the observation belongs to class ğ‘˜, such that ğ‘ƒ (ğ‘‹) has a global maxima at ğ‘‹ = ğ‘˜. The same strategy is adopted by the authors, with one nuance.</p><p>The prediction probability associated with a class represents the model's confidence about an observation belonging to that class. If the value at the global maxima of ğ‘ƒ (ğ‘‹) is not strikingly higher than the value of ğ‘ƒ (ğ‘‹) at other points, it suggests that the model is not very confident about its prediction. Extending this idea, the proposed prediction confidence thresholding method classifies an observation as out-of-scope if the maximum prediction confidence for the observation falls below a specific threshold value. One such threshold value is arrived at for each trained ensemble model, by adopting a qualitative, experiment-based method. A histogram of maximum prediction probabilities of the model, for each observation in the test set, is plotted. Subsequently, multiple confidence values are sampled from lower end of the x-axis of the histogram, and tried for prediction to choose an optimal value. The method is described in greater detail, under the subsequent section on Experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Transfer learning from the weights obtained during training with the ImageNet data set <ref type="bibr" coords="5,467.57,493.44,16.09,10.91" target="#b20">[21]</ref>, and fine-tuning on the FungiCLEF-2022 training data were used to train the model. The prediction accuracy of the models was tracked throughout training in order to select the collection of feature extractors to employ for ensembling later. ResNeXt101 and EfficientNetB4 were chosen as feature extractors for ensembling based on observed network performance. The ensembling was performed using the XGBoost gradient boosting library package.</p><p>The forward propagation is described during training and prediction. Each observation is made up of numerous fungus photos, as well as contextual geographic information like nation and exact area where the photograph was taken on four layers, as well as specific attributes like substrate and habitat. Each image in an observation is preprocessed before being fed through the two feature extraction networks to generate two 4096-element-long representation vectors. These vectors are combined with numeric encoded nation, location at three-level precision, substrate, and habitat metadata for the image to produce a final vector with a size of 8198. The numeric encoding was achieved with the help of Label Encoders from the scikit-learn [22] library wherein, the categorical country codes available as training data are converted to integral class labels that can be fed as input to the feature extraction neural network. The boosting ensemble classifier is fed these 8198 features to generate a probability distribution over all potential fungi species classes. This workflow is depicted in Figure <ref type="figure" coords="6,418.86,352.86,3.67,10.91" target="#fig_0">1</ref>. Categories in any metadata feature that are present in the testing data, but not encountered in the training set are encoded as 0, a commonly adopted strategy to deal with unseen categorical data in deep learning methods.</p><p>To obtain a single aggregate distribution of probabilities over all classes, the corresponding class probability values collected for each image in an observation are averaged. As a result, each observation has only one unique probability distribution. The classification label is assigned to the class that has the highest aggregate probability value.</p><p>The subsequent sections describe the model training experiments and parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Training</head><p>The details of the model training process, performed through transfer-learning is presented in this section. A summary of the parameters used for model training is tabulated in Table <ref type="table" coords="6,482.04,524.53,3.74,10.91" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">ResNet101</head><p>The feature extraction layers of ResNet101 were trained with a two-step classification block, comprising two dense blocks with 4096 and 1604 neurons respectively. The extracted features were percolated through a flatten layer to obtain, before feeding to the classification block. In addition, a dropout layer was added after the dense layer to avoid overfitting. Dropout rates between 0.30 and 0.70 were experimented and set to 0.55 in the final version of the model. The model was trained with the Adam optimizer at an initial learning rate of 3ğ‘’-5. It was back-propagated using the Categorical Cross-Entropy (CCE) loss. For feature extraction, the output of the first dense layer was used to produce a feature vector of 4096 elements. During training, the model's prediction accuracy was tracked to later choose the feature extractor to use for ensembling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">EfficientNetB0</head><p>EfficientNetB0 was also trained with a two-step classification block, comprising two dense blocks with 4096 and 1604 neurons, respectively. The extracted features were percolated through a flatten layer to obtain, before feeding to the classification block. The dropout layer added after the dense layer for this network was experimented between 0.30 and 0.70 and fixed at 0.30. The model was trained using the Adam optimizer at an initial learning rate of 3ğ‘’-2. It was back-propagated using the Categorical Cross-Entropy (CCE) loss. For feature extraction, the output of the first dense layer was used to produce a feature vector of 4096 elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">EfficientNetB4</head><p>EfficientNetB4 was trained with a two-step classification block, also comprising two dense blocks with 4096 and 1604 neurons, respectively. While the final layers of the architecture are the same as the other EfficientNet models, the dropout layer added after the dense layer for this network was experimented between 0.30 and 0.70 and fixed at 0.35 for this model. The model was trained using the Adam optimizer at an initial learning rate of 1ğ‘’-3. A Categorical Cross-Entropy (CCE) loss was used to back-propagate, and a 4096-sized vector was extracted as a feature-representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">EfficientNetB6</head><p>EfficientNetB6 followed the same approach -two dense blocks with 4096 and 1604 neurons, dropout layers, and CCE loss for back-propagation, and optimized with Adam -with differences only in the hyperparameters. The dropout layer after the dense layer was fixed at a dropout rate of 0.45 after experiments. The learning rate was initially set at 3ğ‘’-3. A 4096-sized vector was extracted as feature-representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5.">ResNeXt101</head><p>The ResNeXt101 architecture was augmented, following the same strategy as the aforementioned models i.e. by adding a two-step classification block of 4096 and 1604 neurons, respectively.</p><p>Here, the dropout after experimenting, was set at 0.60 -a significantly high rate due to excessive early-onset overfitting. Adam optimizer, along with CCE loss was used for training, with an initial learning rate of 3ğ‘’-3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Loss, Metrics, Activation and Optimizer Used</head><p>The loss functions, activation functions, optimizers, and evaluation metrics used in the model training experiments are listed under this section, along with their parameterizations and equations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Adam Optimizer</head><p>Adam <ref type="bibr" coords="8,117.49,226.67,17.76,10.91" target="#b22">[23]</ref> is a stochastic optimization method which is used on gradient descent and maintains a single learning rate (alpha) throughout training. Adam combines the advantages of the Adaptive Gradient Algorithm and Root Mean Square Propagation. Unlike the Root Mean Square Propagation, in which the first moment about the mean is used, Adam uses the average of the second moments about the mean too. In effect, Adam provides an optimization algorithm that can handle sparse gradients on noisy problems, by maintaining a per-parameter learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Categorical Cross Entropy Loss</head><p>The categorical cross entropy is a measure of the difference between two discrete probability distributions. It is calculated using the formula in Equation <ref type="formula" coords="8,354.61,357.29,3.74,10.91" target="#formula_1">2</ref>.</p><formula xml:id="formula_1" coords="8,248.24,379.10,257.74,33.71">ğ¿ğ‘œğ‘ ğ‘  = - ğ‘› âˆ‘ï¸ ğ‘–=1 ğ‘¦ ğ‘– ğ‘™ğ‘œğ‘”ğ‘¡ ğ‘– ,<label>(2)</label></formula><p>where, ğ‘¦ ğ‘– represents the corresponding target value for ğ‘¡ ğ‘– the scalar model output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Softmax Activation</head><p>The Softmax activation function is used at the end of the output layer to produce the posterior probability distribution over all classes, based on equation 3. Softmax is essentially a mathematical function that converts a vector of numbers into a vector of probabilities, where the probabilities of each value are proportional to the relative scale of each value in the vector. In effect, it normalizes the outputs, converting them from weighted sum values into probabilities that sum to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘ ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘§</head><formula xml:id="formula_2" coords="8,287.73,559.83,218.25,25.50">ğ‘– ) = ğ‘’ğ‘¥ğ‘(ğ‘§ ğ‘– ) Î£ ğ‘— ğ‘’ğ‘¥ğ‘(ğ‘§ ğ‘— )<label>(3)</label></formula><p>where, ğ‘§ represents the values from the neurons of the output layer. The exponential acts as the non-linear function. These values are divided by the sum of exponential values to normalize and convert them into probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">F1-Score Metric</head><p>The F1-Score is usually calculated as the harmonic mean of precision and recall. This is concretely expressed in the equation 4.</p><formula xml:id="formula_3" coords="9,265.50,132.42,240.49,25.50">ğ¹ 1 = 2ğ‘ ğ‘  ğ‘Ÿ ğ‘  ğ‘ ğ‘  + ğ‘Ÿ ğ‘  ,<label>(4)</label></formula><formula xml:id="formula_4" coords="9,266.10,161.59,236.03,25.56">ğ‘ ğ‘  = ğ‘‡ ğ‘ ğ‘‡ ğ‘ + ğ¹ ğ‘ (<label>5</label></formula><formula xml:id="formula_5" coords="9,502.13,168.30,3.86,10.91">)</formula><formula xml:id="formula_6" coords="9,265.94,191.89,240.04,25.56">ğ‘Ÿ ğ‘  = ğ‘‡ ğ‘ ğ‘‡ ğ‘ + ğ¹ ğ‘›<label>(6)</label></formula><p>where, ğ¹ 1 represents the F1-score, ğ‘ ğ‘  represents precision, ğ‘Ÿ ğ‘  represents recall, ğ‘‡ ğ‘ represents truepositive, ğ¹ ğ‘ represents false-positive and ğ¹ ğ‘› represents false-negative. The contest prescribed macro-averaged F1-Score as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5.">Accuracy Metric</head><p>The accuracy score (Acc) is computed as the ratio of correct predictions to the total number samples. This is expressed in the equation 7.</p><formula xml:id="formula_7" coords="9,236.57,333.51,269.41,25.56">ğ´ğ‘ğ‘ = ğ‘‡ ğ‘ + ğ‘‡ ğ‘› ğ‘‡ ğ‘ + ğ‘‡ ğ‘› + ğ¹ ğ‘ + ğ¹ ğ‘› (7)</formula><p>where, ğ´ğ‘ğ‘ represents the accuracy score, ğ‘‡ ğ‘ represents true-positive, ğ‘‡ ğ‘› represents truenegative, ğ¹ ğ‘ represents false-positive and ğ¹ ğ‘› represents false-negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">XGBoost Ensemble Classifier</head><p>Ensembling was performed with the XGBoost classifier. A grid-search strategy6 was adopted and hyperparameters were fine-tuned <ref type="bibr" coords="9,256.72,439.25,17.76,10.91" target="#b23">[24]</ref> for optimal performance when training the XGBoost classifier. The tree's maximum depth was set to 32. The model would become more sophisticated and prone to overfitting if this parameter was increased. Because increasing this value will consume too much memory during training the deep tree, a low value of 16 was chosen.</p><p>Learning rates greater than 0.03 were found to cause rapid divergence, therefore values in the 10ğ‘’-3 to 10ğ‘’-5 range were utilized. Grid-search was carried out by altering the learning rates in this range and using decision trees ranging from 100 to 1000. To fine-tune the tree-level parameters, the combinations with the best accuracy scores were chosen.</p><p>The maximum depth of the tree is left to be selected according to the classifier's training progress and is not fixed in stone. This causes the depth to increase until the leaves are pure (i.e., all samples belong to the same class) or the minimum number of samples required to divide further has been reached. Some classes may require deeper branches to gather more information from the features due to the data set's long-tailed distribution. To prevent overfitting, a grid search over values in the range of 32 to 256 was used to set an upper limit on the number of leaves.</p><p>The learning task and accompanying learning target are then specified using objective parameters. The Softmax objective function was chosen to set up the classifier for multiclass classification, and the number of classes was set at 1604, explicitly.  <ref type="table" coords="10,251.71,351.37,3.40,8.87" target="#tab_2">3</ref>. The orange-colored line-graph depicts cumulative frequency. The red-colored dotted line-graph depicts the 5% quantile of frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Threshold Value for Out-of-the-Scope Classes</head><p>The authors adopted a prediction confidence thresholding method to handle out-of-the-scope classes. A threshold value is arrived at for each trained ensemble model by adopting a qualitative, trial-based method. First, a histogram of maximum prediction probabilities of the model for each observation in the test set is plotted. The histograms for the best and the worst among the top-5 post-competition submissions are presented in Figures <ref type="figure" coords="10,360.89,475.50,5.07,10.91" target="#fig_1">2</ref> and<ref type="figure" coords="10,387.84,475.50,3.74,10.91" target="#fig_2">3</ref>, respectively.</p><p>The x-axis represents the maximum confidence values for predictions on the observations, while the y-axis tracks the frequency of these maximum confidence values. Subsequently, the x-axis point of 5% cumulative frequency is identified (denoted by the red-colored dotted-line in Figure <ref type="figure" coords="10,132.80,529.70,3.65,10.91" target="#fig_1">2</ref>). Since the proportion of out-of-the-scope classes is not known, multiple pointstypically, 2-4 points were chosen during experiments -on the left-hand side of this 5% quantile line are chosen as threshold values, and predictions are made based on each of these threshold values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Conclusion</head><p>The data provided with the metadata -namely country, location at three-levels of precision, substrate, and habitat metadata -were employed as categorical features for the gradient boosting classifier. The inclusion of contextual information showed a strong impact on the classification results -the testing F1-score of the best submission improved from 15.71% to 16.28%. Likewise, the cross-validation accuracy and F1-score for the same model improved from 61.72% and 41.95% to 63.88% and 42.74%, upon inclusion of the geographic data.</p><p>Figure <ref type="figure" coords="11,130.99,427.98,5.02,10.91" target="#fig_3">4</ref> represents the relative importance of the 16 most effective image features extracted using the trained neural networks, along with the six metadata features used to train the ensemble classifier. The feature importance values were normalized and scaled between 0 and 100 to realize the relative impacts. Features named as f1, f2, etc. denote features extracted from the neural networks. It is worth mentioning that f0 through f4095 denote features extracted using EfficientNetB0, while f4096 through f8191 represent the ResNeXt101-extracted features. It is evident that the country, level-0 location, and habitat information have a significant influence on classification. Further, the more specific location information -level-1 and level-2 -have not contributed as much to the classification. The substrate metadata also serves a significant impact.</p><p>After deciding the baseline architectures for ensembling -namely, EfficientNetB4 and ResNeXt101 -based on individual prediction performance, the baseline models were trained towards convergence. Following this, the multiple ensemble models were trained using the gradient boosting classifier using the two baseline models and contextual data, with different hyperparameter settings. The ensemble classifier's performance was improved over several runs, by tuning the hyperparameters of the gradient boosting classifier. The contest prescribed F1-scores macro-averaged across all classes as the evaluation metric. Model runs were evaluated on the given stratum of validation set using this metric. The metrics were evaluated as an average over the five iterations (for 5-fold cross validation) performed in each run during training.</p><p>Subsequently, the experiment described in Section 4.4 was applied on the models with top scores on the validation set to sample multiple threshold values for each model, and class predictions were made based on each threshold value. The top-5 results from this pool of predictions is summarized in Table <ref type="table" coords="12,253.89,428.56,5.17,10.91" target="#tab_1">2</ref> for competition submissions, and in Table <ref type="table" coords="12,459.45,428.56,5.17,10.91" target="#tab_2">3</ref> for postcompetition evaluations.</p><p>The histograms used in the experiment for sampling threshold values for the best and worst post-competition performance are presented in Figure <ref type="figure" coords="12,332.48,469.21,5.06,10.91" target="#fig_1">2</ref> and<ref type="figure" coords="12,359.38,469.21,3.73,10.91" target="#fig_2">3</ref>, respectively. It is worth noting that the 5% quantile -represented using the red-colored dotted-line -occurred at a confidence value of 0.53 for the best submission, as opposed to 0.24 for the worst model. Further, it can be observed that a majority of the confidence values occur between 0.4 and 0.6 for the best model, and between 0.2 and 0.4 for the worst -indicating a rightward shift. These observations signify that the best-performing model not only predicted a larger proportion of classes correctly, but did so with a higher prediction confidence. This is of practical importance in situations where uncertainty in predictions has serious repercussions.</p><p>Our team achieved a training accuracy of 67.12%, validation accuracy of 63.88%. The corresponding model secured an F1-score of 16.28% on the competition's test data. Our team placed 29 ğ‘ ğ‘¡ among 40 participating teams. A complete summary of the model performance is listed in Table <ref type="table" coords="12,115.79,618.25,3.74,10.91" target="#tab_1">2</ref>.</p><p>Based on the results, it is apparent that the inclusion of contextual geographic data for fungi species classification has had a contributing effect. The best post-competition results are on par with the 27 ğ‘¡â„ best submission. Furthermore, the ensembling of features extracted using multiple neural architectures, and adopting transfer learning to adapt the pretrained models to the specific data domain, looks promising. Several existing approaches have introduced metadata such as population counts of various species, more location-specific geographic data such as city, state, and climatic features such as temperature and humidity. An interesting approach is to employ class-wise probability priors to the neural networks based on such metadata <ref type="bibr" coords="13,478.24,313.96,16.25,10.91" target="#b24">[25]</ref>.</p><p>On account of insufficient computing resources to complete all model training experiments in time for the large dataset of fungi images, the run submissions had to be generated before complete model convergence. Significant improvements were observed in classification accuracy after the submission deadline in the validation, as well as testing performance (through the late submission option). Submission number 3 (refer to Table <ref type="table" coords="13,370.60,381.71,3.65,10.91" target="#tab_1">2</ref>), in particular, showed good improvements when the boosting classifier was trained further with the same hyperparameter settings. A summary of post-competition improvements in prediction results during the working notes submission phase of FungiCLEF-2022 is tabulated in Table <ref type="table" coords="13,377.36,422.35,3.74,10.91" target="#tab_2">3</ref>. It is evident that subsequent hyperparameter tuning and training have been effectiveboth in terms of the proportion of classes predicted correctly, as well as the confidence of these predictions. Hence, the team believes and suggests that the ensembling approach is an effective option for applying to data-intensive and high-complexity image classification tasks that are commonly released as a LifeCLEF task. We further conjecture that training the individual models to convergence, and subsequently applying the boosting ensembler with hyperparameter tuning will culminate in a superior prediction performance, that exhausts the proposed architectures' and methodology's potential. In addition, approaches involving input image resolution variations, usage of alternative pre-trained weights <ref type="bibr" coords="14,423.40,141.16,16.25,10.91" target="#b25">[26]</ref>, as well as the inclusion of custom training layers to the frozen base model when transfer learning <ref type="bibr" coords="14,469.42,154.71,18.03,10.91" target="#b26">[27]</ref> can greatly improve the quality of feature extraction. Finally, the application of image preprocessing techniques can be of significance in improving the overall model performance, particularly that of the neural networks used for feature extraction <ref type="bibr" coords="14,339.18,195.36,16.42,10.91" target="#b27">[28]</ref>. However, the authors could not expend sufficient time to systematically and exhaustively experiment with image preprocessing techniques to analyze their impact on classification performance, due to the aforementioned time and computational constraints. Further experiments will also explore these directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,89.29,250.70,416.69,8.93;6,89.29,262.66,416.69,8.96;6,89.29,274.66,401.75,8.87;6,89.29,84.19,416.70,159.09"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Prediction workflow used in the proposed work for the classification of fungi species from their images and metadata using an ensembling gradient boosting classifier. Note: Model architectures depicted are illustrative only and NOT accurate representations of the underlying network design.</figDesc><graphic coords="6,89.29,84.19,416.70,159.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,89.29,339.37,416.70,8.96;10,89.29,351.32,418.23,8.96;10,89.02,363.06,290.99,9.18;10,89.29,84.19,416.71,247.75"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Histogram of maximum prediction confidence over the test set for the ensemble model corresponding to Submission 1 in Table3. The orange-colored line-graph depicts cumulative frequency. The red-colored dotted line-graph depicts the 5% quantile of frequency.</figDesc><graphic coords="10,89.29,84.19,416.71,247.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,89.29,339.37,416.70,8.96;11,89.29,351.32,418.23,8.96;11,89.02,363.06,290.99,9.18;11,89.29,84.19,416.71,247.75"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Histogram of maximum prediction confidence over the test set for the ensemble model corresponding to Submission 5 in Table 3. The orange-colored line-graph depicts cumulative frequency. The red-colored dotted line-graph depicts the 5% quantile of frequency.</figDesc><graphic coords="11,89.29,84.19,416.71,247.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,89.29,311.26,416.69,8.93;12,89.29,323.26,353.75,8.87;12,89.29,84.19,416.70,220.48"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Relative importance on a scale of 0-100 of the 14 most impactful features extracted from the deep-learning models, along with the six metadata features used to train the classifier.</figDesc><graphic coords="12,89.29,84.19,416.70,220.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.99,553.54,417.00,115.47"><head>Table 1</head><label>1</label><figDesc>Model training parameters used to train each of the convolutional neural networks used for this classification task.</figDesc><table coords="6,159.32,594.89,276.63,74.12"><row><cell>Parameter</cell><cell cols="4">Optimizer Learning rate Batch Size Epochs</cell></row><row><cell>ResNet101</cell><cell>Adam</cell><cell>3ğ‘’-5</cell><cell>32</cell><cell>45</cell></row><row><cell>EfficientNetB0</cell><cell>Adam</cell><cell>3ğ‘’-2</cell><cell>32</cell><cell>50</cell></row><row><cell>EfficientNetB4</cell><cell>Adam</cell><cell>1ğ‘’-3</cell><cell>32</cell><cell>60</cell></row><row><cell>EfficientNetB6</cell><cell>Adam</cell><cell>3ğ‘’-3</cell><cell>32</cell><cell>45</cell></row><row><cell>ResNeXt101</cell><cell>Adam</cell><cell>3ğ‘’-3</cell><cell>32</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="13,88.99,90.49,417.00,115.47"><head>Table 2</head><label>2</label><figDesc>Performance metrics of the 5 best submissions, ordered best to worst. F1-Scores are macro-averaged across classes.</figDesc><table coords="13,101.31,131.84,390.15,74.12"><row><cell cols="5">Submission# Training Accuracy Validation Accuracy Validation F1-Score Test F1-Score</cell></row><row><cell>1</cell><cell>67.12</cell><cell>63.88</cell><cell>42.74</cell><cell>16.28</cell></row><row><cell>2</cell><cell>63.76</cell><cell>60.79</cell><cell>38.88</cell><cell>16.23</cell></row><row><cell>3</cell><cell>60.11</cell><cell>58.21</cell><cell>38.42</cell><cell>15.64</cell></row><row><cell>4</cell><cell>60.14</cell><cell>57.13</cell><cell>37.21</cell><cell>14.17</cell></row><row><cell>5</cell><cell>53.42</cell><cell>49.22</cell><cell>30.97</cell><cell>12.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="13,88.99,463.32,416.99,117.69"><head>Table 3</head><label>3</label><figDesc>Performance metrics of the 5 best submissions post-competition, ordered best to worst. F1-Scores are macro-averaged across classes.</figDesc><table coords="13,101.31,506.89,390.15,74.12"><row><cell cols="5">Submission# Training Accuracy Validation Accuracy Validation F1-Score Test F1-Score</cell></row><row><cell>1</cell><cell>86.78</cell><cell>85.11</cell><cell>50.22</cell><cell>48.96</cell></row><row><cell>2</cell><cell>83.37</cell><cell>80.56</cell><cell>46.78</cell><cell>41.54</cell></row><row><cell>3</cell><cell>78.43</cell><cell>77.10</cell><cell>40.14</cell><cell>37.52</cell></row><row><cell>4</cell><cell>74.81</cell><cell>71.82</cell><cell>37.13</cell><cell>34.63</cell></row><row><cell>5</cell><cell>73.16</cell><cell>70.84</cell><cell>34.76</cell><cell>33.29</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="14,112.66,294.63,393.33,10.91;14,112.66,308.18,395.17,10.91;14,112.66,321.73,232.29,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,339.05,294.63,110.41,10.91;14,480.07,294.63,25.92,10.91;14,112.66,308.18,223.69,10.91">Fungi recognition as an open set classification problem</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,365.49,308.18,142.34,10.91;14,112.66,321.73,201.59,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of FungiCLEF</note>
</biblStruct>

<biblStruct coords="14,112.66,335.28,394.53,10.91;14,112.66,348.83,394.53,10.91;14,112.66,362.38,393.33,10.91;14,112.66,375.93,393.33,10.91;14,112.66,389.48,353.54,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,198.52,362.38,307.47,10.91;14,112.66,375.93,247.50,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,383.00,375.93,122.99,10.91;14,112.66,389.48,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,403.03,394.53,10.91;14,112.66,416.58,393.33,10.91;14,112.66,430.13,393.33,10.91;14,112.66,443.67,168.28,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,185.62,416.58,320.37,10.91;14,112.66,430.13,208.65,10.91">Lifeclef 2022 teaser: An evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,343.19,430.13,162.80,10.91;14,112.66,443.67,38.01,10.91">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,457.22,393.33,10.91;14,112.66,470.77,392.04,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,210.42,457.22,295.56,10.91;14,112.66,470.77,114.44,10.91">Computer vision and machine learning applied in the mushroom industry: A critical review</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,235.58,470.77,186.09,10.91">Computers and Electronics in Agriculture</title>
		<imprint>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="page">107015</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,484.32,393.33,10.91;14,112.48,497.87,379.80,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,213.19,484.32,245.75,10.91">Fungi classification using convolution neural network</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Gaikwad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,470.68,484.32,35.31,10.91;14,112.48,497.87,285.72,10.91">Turkish Journal of Computer and Mathematics Education (TURCOMAT)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="4563" to="4569" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,511.42,395.01,10.91;14,112.30,524.97,393.68,10.91;14,112.66,538.52,393.32,10.91;14,112.41,552.07,397.73,10.91;14,112.66,568.06,43.94,7.90" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,248.32,524.97,257.66,10.91;14,112.66,538.52,283.02,10.91">Transfer learning approach for the classification of conidial fungi (genus aspergillus) thru pre-trained deep learning models</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Mital</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Villaruel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Maningo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kerwin Billones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Vicerra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bandala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dadios</surname></persName>
		</author>
		<idno type="DOI">10.1109/TENCON50793.2020.9293803</idno>
	</analytic>
	<monogr>
		<title level="m" coord="14,442.59,538.52,63.39,10.91;14,112.41,552.07,129.70,10.91">IEEE REGION 10 CONFERENCE (TENCON)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1069" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,579.17,394.53,10.91;14,112.66,592.72,393.33,10.91;14,112.66,606.27,392.48,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,112.66,592.72,281.26,10.91">Danish fungi 2020-not just another image recognition dataset</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>LaessÃ¸e</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>FrÃ¸slev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,420.58,592.72,85.40,10.91;14,112.66,606.27,294.59,10.91">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1525" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,619.81,393.32,10.91;14,112.66,633.36,393.33,10.91;14,112.66,646.91,147.62,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,453.56,619.81,52.42,10.91;14,112.66,633.36,393.33,10.91;14,112.66,646.91,37.10,10.91">Application of deep and machine learning using image analysis to detect fungal contamination of rapeseed</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>PrzybyÅ‚</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wawrzyniak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Koszela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Adamski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gawrysiak-Witulska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,158.32,646.91,34.15,10.91">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">7305</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,660.46,393.33,10.91;15,112.66,86.97,393.33,10.91;15,112.66,100.52,176.28,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,220.99,660.46,284.99,10.91;15,112.66,86.97,38.07,10.91">Mushroom classification using feature-based machine learning approach</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Maurya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,173.53,86.97,332.46,10.91;15,112.66,100.52,45.66,10.91">Proceedings of 3rd International Conference on Computer Vision and Image Processing</title>
		<meeting>3rd International Conference on Computer Vision and Image Processing</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,114.06,374.73,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,155.93,114.06,169.95,10.91">Bilinear interpolation of digital images</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,334.60,114.06,73.92,10.91">Ultramicroscopy</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="201" to="204" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,127.61,395.17,10.91;15,112.66,141.16,393.32,10.91;15,112.66,154.71,325.92,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,302.02,127.61,205.81,10.91;15,112.66,141.16,171.71,10.91">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,307.43,141.16,198.55,10.91;15,112.66,154.71,237.36,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,168.26,394.61,10.91;15,112.66,181.81,314.10,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="15,253.11,168.26,199.28,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1512.03385</idno>
		<ptr target="https://arxiv.org/abs/1512.03385.doi:10.48550/ARXIV.1512.03385" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,195.36,393.33,10.91;15,112.66,208.91,217.61,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,308.43,195.36,130.70,10.91">Training very deep networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,449.80,195.36,56.18,10.91;15,112.66,208.91,172.82,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,222.46,393.33,10.91;15,112.33,236.01,370.70,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="15,187.42,222.46,318.57,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1905.11946</idno>
		<ptr target="https://arxiv.org/abs/1905.11946.doi:10.48550/ARXIV.1905.11946" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,249.56,393.33,10.91;15,112.66,263.11,287.29,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,187.31,249.56,229.74,10.91">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,446.71,249.56,59.28,10.91;15,112.66,263.11,146.65,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10096" to="10106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,276.66,395.17,10.91;15,112.66,290.20,397.48,10.91;15,112.66,306.20,32.07,7.90" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="15,289.75,276.66,218.07,10.91;15,112.66,290.20,54.60,10.91">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1611.05431</idno>
		<ptr target="https://arxiv.org/abs/1611.05431.doi:10.48550/ARXIV.1611.05431" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,317.30,394.53,10.91;15,112.28,330.85,395.39,10.91;15,112.66,344.40,161.38,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1409.4842</idno>
		<ptr target="https://arxiv.org/abs/1409.4842.doi:10.48550/ARXIV.1409.4842" />
		<title level="m" coord="15,178.24,330.85,137.69,10.91">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,357.95,393.33,10.91;15,112.66,371.50,393.33,10.91;15,112.66,385.05,147.08,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="15,344.36,357.95,161.63,10.91;15,112.66,371.50,83.10,10.91">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,216.60,371.50,289.39,10.91;15,112.66,385.05,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,398.60,393.33,10.91;15,112.66,412.15,394.03,10.91;15,112.41,425.70,284.14,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Xgboost</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="https://doi.org/10.1145%2F2939672.2939785.doi:10.1145/2939672.2939785" />
		<title level="m" coord="15,270.61,398.60,235.38,10.91;15,112.66,412.15,240.76,10.91">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,439.25,395.17,10.91;15,112.66,452.79,95.43,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="15,166.45,439.25,188.24,10.91">Tutorial on maximum likelihood estimation</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">J</forename><surname>Myung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,362.76,439.25,145.07,10.91;15,112.66,452.79,16.57,10.91">Journal of mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,466.34,393.33,10.91;15,112.66,479.89,394.53,10.91;15,112.66,493.44,103.61,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="15,346.64,466.34,159.35,10.91;15,112.66,479.89,67.28,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,228.08,479.89,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,506.99,394.53,10.91;15,112.66,520.54,393.33,10.91;15,112.48,534.09,261.79,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="15,307.23,520.54,176.03,10.91">Scikit-learn: Machine learning in python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,492.05,520.54,13.94,10.91;15,112.48,534.09,167.70,10.91">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,547.64,395.01,10.91;15,112.66,561.19,246.16,10.91" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="15,232.29,547.64,162.24,10.91">A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1412.6980</idno>
		<ptr target="https://arxiv.org/abs/1412.6980.doi:10.48550/ARXIV.1412.6980" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,574.74,393.33,10.91;15,112.66,588.29,393.33,10.91;15,112.33,601.84,29.19,10.91" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="15,418.54,574.74,87.45,10.91;15,112.66,588.29,248.34,10.91">Benchmarking and optimization of gradient boosting decision tree algorithms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Anghel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Parnell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">De</forename><surname>Palma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Pozidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04559</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,615.39,393.61,10.91;15,112.66,628.93,393.53,10.91;15,112.39,642.48,231.07,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="15,364.18,615.39,142.08,10.91;15,112.66,628.93,152.79,10.91">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,288.06,628.93,218.14,10.91;15,112.39,642.48,133.15,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,656.03,394.52,10.91;15,112.66,669.58,393.33,10.91;16,112.66,86.97,393.33,10.91;16,112.66,100.52,394.53,10.91;16,112.66,114.06,80.57,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="15,236.07,669.58,269.91,10.91;16,112.66,86.97,307.48,10.91">Overview of lifeclef 2020: a system-oriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,446.71,86.97,59.28,10.91;16,112.66,100.52,346.66,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="342" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,127.61,394.53,10.91;16,112.66,141.16,393.33,10.91;16,112.66,154.71,308.59,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="16,112.66,141.16,393.33,10.91;16,112.66,154.71,136.12,10.91">Multispecies bioacoustic classification using transfer learning of deep convolutional neural networks with pseudo-labeling</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lebien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Campos-Cerqueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Dodhia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Ferres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Velev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Aide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,257.28,154.71,80.93,10.91">Applied Acoustics</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page">107375</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,168.26,395.17,10.91;16,112.26,181.81,393.73,10.91;16,112.66,195.36,304.70,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="16,207.01,168.26,300.82,10.91;16,112.26,181.81,24.38,10.91">Preprocessing for image classification by convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sudeep</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,179.83,181.81,326.15,10.91;16,112.66,195.36,179.69,10.91">IEEE International Conference on Recent Trends in Electronics, Information &amp; Communication Technology (RTEICT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1778" to="1781" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
