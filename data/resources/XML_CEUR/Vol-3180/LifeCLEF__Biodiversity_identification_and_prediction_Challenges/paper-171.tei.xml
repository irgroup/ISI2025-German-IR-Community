<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,373.81,15.42;1,89.29,106.66,218.60,15.42">Motif Mining and Unsupervised Representation Learning for BirdCLEF 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,134.97,98.65,11.96"><forename type="first">Anthony</forename><surname>Miyaguchi</surname></persName>
							<email>acmiyaguchi@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<addrLine>North Ave NW</addrLine>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.17,134.97,58.60,11.96"><forename type="first">Jiangyue</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<addrLine>North Ave NW</addrLine>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.42,134.97,117.34,11.96"><forename type="first">Bryan</forename><surname>Cheungvivatpant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<addrLine>North Ave NW</addrLine>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.40,134.97,73.57,11.96"><forename type="first">Dakota</forename><surname>Dudley</surname></persName>
							<email>dakotadudley13@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<addrLine>North Ave NW</addrLine>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.87,148.92,72.60,11.96"><forename type="first">Aniketh</forename><surname>Swain</surname></persName>
							<email>aswain9@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<addrLine>North Ave NW</addrLine>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,373.81,15.42;1,89.29,106.66,218.60,15.42">Motif Mining and Unsupervised Representation Learning for BirdCLEF 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">9F97427DCA93F2DA64DE4B240371F92D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>motif mining</term>
					<term>matrix profile</term>
					<term>unsupervised representation learning</term>
					<term>embedding</term>
					<term>CEUR-WS</term>
					<term>BirdCLEF 2022</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We build a classification model for the BirdCLEF 2022 challenge using unsupervised methods. We implement an unsupervised representation of the training dataset using a triplet loss on spectrogram representation of audio motifs. Our best model performs with a score of 0.48 on the public leaderboard.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The BirdCLEF 2022 challenge <ref type="bibr" coords="1,220.38,347.68,12.68,10.91" target="#b0">[1]</ref> [2] involves identifying species of native Hawaiian birds from soundscape recordings. The training dataset comprises 14.8 thousand recordings totaling over 190 hours from Xeno-canto <ref type="bibr" coords="1,217.99,374.78,13.00,10.91" target="#b2">[3]</ref> of varying length and quality. The task involves predicting the presence of 29 bird species in non-overlapping 5-second windows of 1-minute soundscape recordings. The recordings are Ogg Vorbis audio files encoded at a 32khz sample rate. The Xeno-canto training examples are labeled by species but not at 5-second intervals per the challenge's task. Therefore, they may contain both ambient noise and birdcalls from other species.</p><p>We focus our efforts on unsupervised methods to address the lack of concrete labels for the multi-label classification problem. First, we experiment with motif mining algorithms to identify windows of audio that contain birdcalls as an unsupervised process for generating labels. The motif mining process also provides metadata used to train downstream models. Additionally, instead of training a classification model directly from training examples, we choose to build an embedding that captures similarities between birdcalls across all training examples. We finally train classification models using the embedding model to reduce the dimensionality of the original data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motif Mining</head><p>We utilize a motif mining algorithm called SiMPle <ref type="bibr" coords="2,312.79,111.28,12.79,10.91" target="#b3">[4]</ref> to extract the segments of the audio clip that best represent common patterns in the clip. SiMPle provides two primary operations to compute summary data structures called the matrix profile and profile index: a self-join that computes the similarity within a track and a join that computes the similarity between two tracks. The procedure involves converting the raw audio track into a spectrogram to capture the frequency components of the audio over a sliding window in time. Then we apply SiMPle to compute the matrix profile and profile index, providing the distance to the nearest neighbor and the index of the nearest neighbor in the set of windows in the track. A motif is a window of an audio track with the lowest distance to all other windows in the join operation as given by the matrix profile. The indices of the matrix profile's minimum and maximum distance values are the motifs and discord, respectively. We hypothesize that the motifs and discords capture the audio clip's salient features, i.e., bird calls. Much like how SiMPle can identify the chorus of a song via the motif, it may be possible to extract birdcalls as a motif from the training examples. We transform the training dataset by extracting the matrix profile and profile index information from each track via a self-join and using the 5-second motifs as a soft label for birdcalls in each species. We utilize the self-join index profile during the training procedure of the birdcall embedding. We also test the feasibility of using a matrix profile join as a feature in the classifier for the main BirdCLEF task by directly utilizing the resulting join against a random set of motifs extracted from the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Birdcall Embedding</head><p>An embedding maps data from one space into a lower-dimensional space while maintaining relative distances between mapped data. We experiment with creating a birdcall embedding using a modified Tile2Vec model <ref type="bibr" coords="3,242.28,226.89,11.59,10.91" target="#b4">[5]</ref>. The Tile2Vec model utilizes a sampling procedure on spatially distributed data and a triplet loss to learn a lower-dimensional representation of Earth imagery tiles that retains semantic similarity between tiles. The triplet loss takes advantage of the triangle inequality by forming triplets between anchor, neighbor, and distant tiles. We borrow the loss for triplets (ğ‘¡ ğ‘ , ğ‘¡ ğ‘› , ğ‘¡ ğ‘‘ ) with a margin ğ‘š where ğ‘“ ğœƒ maps audio data to a ğ‘‘-dimensional vector of real numbers using a model with parameters ğœƒ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ¿(ğ‘¡</head><formula xml:id="formula_0" coords="3,169.49,321.73,336.49,12.95">ğ‘ , ğ‘¡ ğ‘› , ğ‘¡ ğ‘‘ ) = [||ğ‘“ ğœƒ (ğ‘¡ ğ‘ ) -ğ‘“ ğœƒ (ğ‘¡ ğ‘› )|| 2 -||ğ‘“ ğœƒ (ğ‘¡ ğ‘ ) -ğ‘“ ğœƒ (ğ‘¡ ğ‘‘ )|| 2 + ğ‘š] +<label>(1)</label></formula><p>We hypothesize that we can learn an embedding by forming triplets using windows of audio with similar spectral qualities. We utilize the SiMPLe index of the audio spectrogram to determine the neighborhood of any given window of audio within a track. The Tile2Vec model is a modified ResNet-18 convolutional neural network -we make further modifications to generate spectrograms in the correct shape as the first layer in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Motif Mining Details</head><p>We begin by extracting the motifs and discords of the entire training dataset. However, this process is computationally intensive because it requires streaming the entire audio dataset, generating the spectrogram via STFT, and computing the matrix profile. Therefore, we precompute the matrix profile and index profile for offline processing. We use librosa <ref type="bibr" coords="3,450.79,516.19,12.70,10.91" target="#b5">[6]</ref> for audio loading and transformation and implement a NumPy implementation of the SiMPle algorithm to facilitate usage in our processing pipeline. <ref type="foot" coords="3,291.68,541.54,3.71,7.97" target="#foot_0">1</ref>We use chroma energy normalized (CEN) and Mel-scaled spectrograms. We generate CEN spectrograms at ten samples per second using the default parameters in librosa, with a 50 sample window for motif mining purposes and the Mel-scaled spectrograms using an FFT window of 2048, a hop length of 80 samples, and 16 Mel bands with a 400 sample window.</p><p>As per figure <ref type="figure" coords="3,160.59,611.04,3.71,10.91" target="#fig_0">1</ref>, we note that the observed quality of a spectrogram and the resulting matrix profile varies depending on the algorithm and parameters used. We can clearly distinguish the chirps in the high-frequency ranges in the Mel spectrogram, with obvious discords occurring in the same time intervals. We are presented with a lower resolution spectrogram with CEN because it is aligned by chroma instead, which does not delineate between the bird call and the background noise as clearly.</p><p>We initially performed our motif mining using the CEN parameters, following closely with the SiMPLe cover song extraction experiment. We found that the lower resolution of the CEN spectrogram representation allowed us to process all training examples and store the primary representation on disk, as per table 1. Although noisy, the representation did extract bird calls as the primary motif, although lack of domain knowledge prevented us from quantifying the quality of these labels. As a result, some extracted motifs are background noise or human voices. We reuse the open-sourced Tile2Vec model and add a Mel spectrogram layer via nnAudio <ref type="bibr" coords="4,492.63,528.20,11.27,10.91" target="#b6">[7]</ref>. We use an FFT window of 4096 with a calculated hop length matching the height of 128 Mel bands. We choose these parameters to mimic the 128x128 pixel images used to train the source model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Embedding Clustering Quality</head><p>We built two separate data loaders for this task -the first data loader pre-computed triplets from the original Ogg Vorbis files into NumPy array files. We only use the extracted motifs during the data loading process. In addition, we attempt to adjust for the skewed species distribution by oversampling underrepresented species. A second data loader streamed all audio computing triplets using the pre-computed SiMPle indices for each track. We break each track into windows of 5-seconds and assign the nearest neighbor using the profile index. Next, we create several queues, each containing window pairs from tracks of different species of birds.</p><p>The data loader pops pairs from each queue to form a mini-batch, after which triplets are formed by randomly assigning the third element from an element within the mini-batch. Finally, we augment the audio tracks once formed into triplets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Sampling triplets from audio using precomputed SiMPle index</head><formula xml:id="formula_1" coords="5,317.37,329.53,63.52,9.57">) âˆˆ ğ‘ƒ, ğ‘¥ Ì¸ = ğ‘¦}</formula><p>The first approach is untenable for an online training procedure because it naively generated motif triplets using upwards of 3ğ‘ disk reads. In addition, we noticed that running the data loading procedure online would cause underutilization between the CPU and GPU due to the loading bottleneck. By storing 5ğ‘’5 NumPy array triplets to disk, we could fully utilize the GPU at the cost of disk space, going from the original 6GB of audio data to 135GB for the pre-computed triplet data. The second approach required no additional memory because it took on an iterable approach to creating triplets. It has a different distributional semantic from the first since it includes all audio windows instead of just the track motifs. We also do not address class imbalances because online undersampling is difficult. We need further preprocessing to determine the total number and locations of birdcalls per class.</p><p>We apply a random gain of [-20, 20] dB, Gaussian random noise between [-5, 40] dB, and pitch shift between [-4, 4] semitones using the audiomentations library <ref type="bibr" coords="5,424.93,506.99,11.58,10.91" target="#b7">[8]</ref>. We note that augmentation can become a bottleneck in the data loading process. Instead of applying it to the entire triplet, it suffices to apply it to just the motif pairs before triplet formation. We consider torch-audiomentations but had poor performance due to difficulties applying the correct device to torch tensors inside the data loader.</p><p>We validate the embedding learned on the iterable data loader by training a classifier on motifs transformed into the embedding space. We chose a subset of three species: brnowl, skylar, and houfin. These are common species with several hundred audio tracks available. We generate a training dataset comprised of the motif of each track and cross-validate a logistic regression model using model accuracy on species accuracy. We sample ğ‘˜ = 300 motifs for ten models and report the mean and standard deviation of the accuracy on embedding models that vary in the output size.</p><p>We note a slight difference between accuracy in models of equivalent parameterization when </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Accuracy table of a logistic regression model trained on the embedding using motif mined from three species of birds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Classifier Performance</head><p>We train a classifier using the embedding model as a preprocessing step. The first classifier trains a gradient-boosted decision tree (GBDT) via LightGBM <ref type="bibr" coords="6,357.84,568.64,11.28,10.91" target="#b8">[9]</ref>. It uses a MultiOutputClassifer to create a model per task to suit the multi-label classification task using the default parameters of the LightGBM classifier. Next, we apply a similar set of augmentation to the embedding model, with a random gain of <ref type="bibr" coords="6,226.76,610.01,41.21,9.57">[-20, 20]</ref> dB, pitch shift between <ref type="bibr" coords="6,378.50,610.01,30.30,9.57">[-4, 4]</ref> semitones, time shift between [-10, 10]% of the window's length, and colored noise between [-3, 30] dB and [-2, 2] frequency power decay using the PyTorch-audiomentations library <ref type="bibr" coords="6,388.53,636.38,16.19,10.91" target="#b9">[10]</ref>. We also train a multilayer perceptron (MLP) to perform multi-label classification directly using a binary cross-entropy loss. The loss function allows us to implement mixup <ref type="bibr" coords="6,327.15,663.48,17.84,10.91" target="#b10">[11]</ref> during data loading mini-batches as an augmentation process. We experimented with using matrix profile join data as a feature in our classification model. We draw 64 motif samples from a set of training motifs and run each audio window in the testing task through a matrix profile against the training sample. We use the concatenation of the matrix profile join's min, median, and max as an additional feature in the model. Unfortunately, we found that the additional CPU overhead to run SiMPle far outweighed any marginal performance benefit to the model and was omitted in subsequent model iterations.</p><p>classifier macro F1-score GBDT 0.0177 MLP 0.0151</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>A comparison of the two classifiers' performance on a subset of the training audio data, using macro F1-score as metric. We limit the scored data to the first ten tracks of each species in the task.</p><p>We compute the macro F1-score in table <ref type="table" coords="7,278.80,287.29,4.98,10.91">3</ref> to compare our best GBDT model against our best MLP model. We note that the GBDT edges slightly over the MLP, but performance for both classifiers is poor. We often see a lack of positive predictions for a class, which leads to a score of 0. We also assume that all audio windows contain birdcalls to simplify the calculations, so these scores understate the actual performance of the models. However, we found that the general performance is still poor in the BirdCLEF task, hovering around 0.48 on the public leaderboard during the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In a small set of shorter tracks (30 seconds or less), we consistently found that the discord tends to be a birdcall instead of the motif. It might be more productive to standardize a mining procedure on shorter tracks instead of using the whole, variable-length tracks to take advantage of this observation. We can build a no-call detector using the matrix profile as a feature if we can consistently detect positions of bird calls through discords or motifs. In addition, we can achieve higher throughput on parallelization by reducing data skew in the distribution of training example lengths. Finally, both spectrogram transformations via nnAudio and the SiMPle algorithm can be written in PyTorch to increase the performance of the motif mining algorithm.</p><p>We found that the birdcall motif triplet embedding performed poorly for downstream prediction tasks. We hypothesize that the spectrogram parameters during motif mining cause the embedding to rest on a representation ill-suited for classification. One modification to the embedding triplet procedure would be to form pairs of anchors and neighbors from overlapping audio windows. These pairs would be similar due to their distance in time. As the experimental procedure before, triplets are formed by randomization in mini-batches. A sliding window triplet procedure would enable a comparison of the validity of the learned motif triplet embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We implemented and experimented with the SiMPle motif mining algorithm, a modified Tile2Vec embedding model, and several multi-label classification models. <ref type="foot" coords="8,383.41,123.07,3.71,7.97" target="#foot_1">2</ref> Our performance on the leaderboard was underwhelming, but we solved many engineering problems throughout the challenge with potential improvements for future BirdCLEF challenges.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,552.65,416.69,8.93;2,89.29,564.66,416.70,8.87;2,89.29,576.61,416.69,8.87;2,89.29,588.57,417.78,8.87;2,88.93,600.52,327.13,8.87;2,89.29,268.29,416.71,264.98"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Spectrograms show frequency components of audio transformed via STFT. We apply SiMPLe to obtain a matrix profile that summarizes the distance to the nearest neighbor for all time-slices in the spectrogram. Spectrogram parameterization affects the quality of the matrix profile summary. The Mel-frequency spectogram renders log-scaled filter banks to approximate human hearing response, while the CENS spectrogram renders filter banks that map to a chromatic scale.</figDesc><graphic coords="2,89.29,268.29,416.71,264.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,498.78,311.50,8.93;4,89.29,367.93,416.69,106.33"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Flow diagram of the constituent pieces of the birdcall embedding.</figDesc><graphic coords="4,89.29,367.93,416.69,106.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,326.01,416.69,8.93;6,89.29,338.02,416.69,8.87;6,89.29,349.97,112.56,8.87;6,140.13,84.19,312.53,234.40"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A scatter plot of a random set of motifs (n=300) drawn from three species of birds. The motifs are truncated, padded, and transformed into the embedding space. We plot the top two principle components found by PCA.</figDesc><graphic coords="6,140.13,84.19,312.53,234.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.81,86.42,418.71,87.30"><head>Table 1</head><label>1</label><figDesc>Statistics from processing track XC144892 which is 28.8 seconds long. Each operation is run 10 times. We note that our parameterization of the Mel-scaled spectrogram results in data that takes several orders of magnitude longer to process than the CEN spectrograms.</figDesc><table coords="4,112.08,86.42,368.63,33.57"><row><cell cols="3">spectrogram format transform time shape</cell><cell>size</cell><cell cols="2">SiMPle window self-join</cell></row><row><cell>Mel-scaled</cell><cell>131ms</cell><cell cols="3">(16, 7932) 126912 400</cell><cell>3.3s</cell></row><row><cell>CEN</cell><cell>287ms</cell><cell>(12, 292)</cell><cell>3504</cell><cell>50</cell><cell>8.85ms</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,89.29,152.66,416.90,187.50"><head></head><label></label><figDesc>Require: ğ· is the set of audio tracks as raw samples Require: ğ‘“ ğ‘  is the sampling rate of the audio in Hz Require: ğ‘  is the size of the audio window in seconds Require: ğ¼ is the SiMPle index Ensure: ğ‘‡ is the set of triplets formed by all windows Ensure: |ğ‘‡ | = |ğ‘ƒ | Initialize pairs ğ‘ƒ = {} for ğ‘¦ âˆˆ ğ· do ğ‘¤ â† window(ğ‘¦, ğ‘“ ğ‘  , ğ‘ ) â— ğ‘¤ is an array of audio windows each of length ğ‘  Ã— ğ‘“ ğ‘  for ğ‘– â† 1, length(ğ‘¤) do ğ‘ƒ â† ğ‘ƒ âˆª (ğ‘¦, ğ‘¤[ğ‘–], ğ‘¤[ğ¼[ğ‘–]]) â— Use the SiMPle index to get the nearest neighbor end for end for ğ‘‡ â† {(ğ‘¥ ğ‘ , ğ‘¥ ğ‘› , ğ‘¦ ğ‘ )| âˆ€(ğ‘¥, ğ‘¥ ğ‘ , ğ‘¥ ğ‘› ) âˆˆ ğ‘ƒ, âˆƒ((ğ‘¦, ğ‘¦ ğ‘ , ğ‘¦ ğ‘›</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,108.93,671.02,234.78,8.97"><p>Implementation at github.com/acmiyaguchi/simple-fast-python</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="8,108.93,671.03,212.58,8.97"><p>Implementation at github.com/acmiyaguchi/birdclef-2022</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Thanks to the <rs type="institution">Data Science @ Georgia Tech (DS@GT)</rs> officers for organizing and publicizing recruitment for the DS@GT Kaggle competition team, in particular <rs type="person">Pulak Agarwal</rs> and <rs type="person">Krishi Manek</rs>.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,296.28,394.53,10.91;8,112.66,309.83,393.33,10.91;8,112.66,323.38,393.33,10.91;8,112.66,336.93,111.86,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,210.09,309.83,295.90,10.91;8,112.66,323.38,113.91,10.91">Overview of birdclef 2022: Endangered bird species recognition in soundscape recordings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,236.87,323.38,269.11,10.91;8,112.66,336.93,79.94,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,350.47,394.53,10.91;8,112.66,364.02,394.53,10.91;8,112.66,377.57,395.17,10.91;8,112.66,391.12,393.32,10.91;8,112.66,404.67,394.53,10.91;8,112.66,418.22,22.69,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,260.93,377.57,246.90,10.91;8,112.66,391.12,313.05,10.91">Overview of lifeclef 2022: an evaluation of machinelearning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,448.45,391.12,57.53,10.91;8,112.66,404.67,346.66,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,431.77,394.62,10.91;8,112.31,445.32,77.08,10.91" xml:id="b2">
	<monogr>
		<ptr target="https://xeno-canto.org" />
		<title level="m" coord="8,112.66,431.77,310.32,10.91">Xeno-canto, Xeno-canto: Sharing bird sounds from around the world</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,458.87,393.53,10.91;8,112.66,472.42,367.45,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,362.99,458.87,143.21,10.91;8,112.66,472.42,135.73,10.91">Fast similarity matrix profile for music analysis and exploration</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-C</forename><forename type="middle">M</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,256.77,472.42,149.55,10.91">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="29" to="38" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,485.97,395.17,10.91;8,112.66,499.52,393.33,10.91;8,112.66,513.06,255.23,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,374.32,485.97,133.51,10.91;8,112.66,499.52,206.57,10.91">Tile2vec: Unsupervised representation learning for spatially distributed data</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Samar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Azzari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lobell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,341.94,499.52,164.04,10.91;8,112.66,513.06,106.61,10.91">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3967" to="3974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,526.61,394.62,10.91;8,112.28,540.16,393.70,10.91;8,112.66,553.71,122.28,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,474.12,526.61,33.16,10.91;8,112.28,540.16,185.47,10.91">librosa: Audio and music signal analysis in python</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,321.11,540.16,184.87,10.91;8,112.66,553.71,46.30,10.91">Proceedings of the 14th python in science conference</title>
		<meeting>the 14th python in science conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,567.26,393.33,10.91;8,112.66,580.81,393.32,10.91;8,112.33,594.36,280.68,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,345.31,567.26,160.68,10.91;8,112.66,580.81,322.16,10.91">nnaudio: An on-the-fly gpu audio to spectrogram conversion toolbox using 1d convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Cheuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Agres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Herremans</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.3019084</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,443.65,580.81,54.52,10.91">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="161981" to="162003" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,607.91,395.17,10.91;8,112.66,621.46,394.53,10.91;8,112.33,635.01,395.33,10.91;8,112.66,648.56,190.14,10.91" xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Jordal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tamazian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">T</forename><surname>Chourdakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Angonin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Karpov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Sarioglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">B</forename><surname>Ã‡oban</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Mirus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Marvinlvn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solomidhero</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>AlumÃ¤e</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.6594177</idno>
		<ptr target="https://doi.org/10.5281/zenodo.6594177.doi:10.5281/zenodo.6594177" />
	</analytic>
	<monogr>
		<title level="j" coord="8,164.65,635.01,28.77,10.91">iver</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">0</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,86.97,393.61,10.91;9,112.66,100.52,393.33,10.91;9,112.66,114.06,129.40,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,418.47,86.97,87.80,10.91;9,112.66,100.52,181.97,10.91">Lightgbm: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,306.31,100.52,199.68,10.91;9,112.66,114.06,35.32,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3146" to="3154" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,127.61,394.53,10.91;9,112.66,141.16,395.17,10.91;9,112.66,154.71,397.48,10.91;9,112.66,170.70,115.16,7.90" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Jordal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nishi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bredin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">C</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Å»elasko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Quatra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Schmidbauer</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.6381721</idno>
		<idno>0.10.1</idno>
		<ptr target="https://doi.org/10.5281/zenodo.6381721.doi:10.5281/zenodo.6381721" />
		<title level="m" coord="9,355.11,141.16,152.72,10.91;9,112.66,154.71,73.86,10.91">FrenchKrab, asteroid-team/torchaudiomentations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,181.81,395.17,10.91;9,112.66,195.36,197.93,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m" coord="9,331.02,181.81,176.81,10.91;9,112.66,195.36,16.17,10.91">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
