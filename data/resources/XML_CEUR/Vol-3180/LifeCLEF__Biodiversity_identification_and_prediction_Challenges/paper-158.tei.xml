<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,101.58,399.21,15.42;1,89.29,123.50,353.47,15.42;1,89.29,145.41,106.62,15.43">Combination of Object Detection, Geospatial Data, and Feature Concatenation for Snake Species Identification</title>
				<funder>
					<orgName type="full">University of Applied Sciences and Arts Dortmund, Dortmund, Germany</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,173.73,62.76,11.96"><forename type="first">Louise</forename><surname>Bloch</surname></persName>
							<email>louise.bloch@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Stra√üe 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<settlement>Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,169.99,173.73,120.96,11.96"><forename type="first">Jan-Frederick</forename><surname>B√∂ckmann</surname></persName>
							<email>jan-frederick.boeckmann005@stud.fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Stra√üe 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.59,173.73,83.24,11.96"><forename type="first">Benjamin</forename><surname>Bracke</surname></persName>
							<email>benjamin.bracke002@stud.fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Stra√üe 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,187.68,111.78,11.96"><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
							<email>christoph.friedrich@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Stra√üe 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<settlement>Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,101.58,399.21,15.42;1,89.29,123.50,353.47,15.42;1,89.29,145.41,106.62,15.43">Combination of Object Detection, Geospatial Data, and Feature Concatenation for Snake Species Identification</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">B41BA641F4F58E73DD5D38942F465166</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Snake species identification</term>
					<term>YOLOv5</term>
					<term>Feature concatenation</term>
					<term>Geospatial data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic snake species identification based on non-standardized photographs is an important task to improve the medical treatment of snake bites in developing countries. To overcome this problem, the SnakeCLEF 2022 challenge provides a large data set containing photographs and geospatial data for 1,572 snake species. This paper describes the participation of the FHDO Biomedical Computer Science Group (BCSG) in this challenge. The presented experiments included object detection with You Only Look Once (YOLO) v5, feature concatenation, and multiplication with prior probabilities of regional metadata. The experiments showed that object detection, geospatial feature concatenation, Test Time Augmentation (TTA), and multiplication with regional prior probabilities can improve the detection task. The best results in the challenge were achieved by an ensemble model combining three EfficientNet-B4, two EfficientNet-v2-M, one EfficientNet-B5, and one ConvNeXt model. The ensemble reached an ùêπ ùëùùë¢ùëè 1 score of 75.426 % and an ùêπ ùëùùëüùëñùë£ 1 score of 70.798 %.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="841.89" lry="595.276"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper presents the participation of University of Applied Sciences and Arts Dortmund (FHDO) Biomedical Computer Science Group (BCSG) at the Conference of Labs of the Evaluation Forum (CLEF) 2022 1 SnakeCLEF <ref type="bibr" coords="1,234.55,516.17,12.80,10.91" target="#b0">[1]</ref> challenge 2 for snake species identification. This challenge is part of the LifeCLEF 2022 <ref type="bibr" coords="1,212.58,529.72,11.23,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,226.41,529.72,8.88,10.91" target="#b2">3]</ref> research platform focusing on the automated identification of species. The LifeCLEF platform consists of five data-driven challenges. The code to reproduce the participation is available online <ref type="foot" coords="2,246.19,102.05,3.71,7.97" target="#foot_0">3</ref> .</p><p>The annual mortality of snakebites is between 81,000 and 138,000 people <ref type="bibr" coords="2,435.37,117.35,11.58,10.91" target="#b3">[4]</ref>. In addition, 400,000 victims of snakebites suffer from incurable physical and psychological disabilities each year <ref type="bibr" coords="2,111.89,144.45,11.58,10.91" target="#b3">[4]</ref>. Identifying the snake species might help to administer the right antivenom <ref type="bibr" coords="2,473.49,144.45,12.99,10.91" target="#b4">[5]</ref> and thus reduce the number of victims. Additionally, the protection of harmful snakes could be improved using snake species identification, by reducing the number of snakes that were killed out of people's fear <ref type="bibr" coords="2,177.64,185.10,11.43,10.91" target="#b5">[6]</ref>.</p><p>The SnakeCLEF challenge aims to feature data-driven analysis and thus improve the identification of snake species based on non-standardized photographs. This paper summarizes the experiments and results of FHDO BCSG for the SnakeCLEF 2022 challenge. The presented approach expands the FHDO BCSG submissions <ref type="bibr" coords="2,304.47,239.30,11.29,10.91" target="#b6">[7,</ref><ref type="bibr" coords="2,318.48,239.30,8.92,10.91" target="#b7">8]</ref> for SnakeCLEF 2020 <ref type="bibr" coords="2,421.48,239.30,12.76,10.91" target="#b8">[9]</ref> and SnakeCLEF 2021 <ref type="bibr" coords="2,112.31,252.85,16.25,10.91" target="#b9">[10]</ref>.</p><p>In comparison to the participation in SnakeCLEF 2021 <ref type="bibr" coords="2,350.52,266.40,11.58,10.91" target="#b7">[8]</ref>, the classification models were expanded using ConvNeXt <ref type="bibr" coords="2,209.74,279.94,17.79,10.91" target="#b10">[11]</ref> and EfficientNet-v2 <ref type="bibr" coords="2,317.91,279.94,17.79,10.91" target="#b11">[12]</ref> models. For object detection, You only look once (YOLO) v5 <ref type="bibr" coords="2,185.18,293.49,17.97,10.91" target="#b12">[13]</ref> was introduced and compared to the previously used Mask Region-Based Convolutional Neural Network (Mask-RCNN) <ref type="bibr" coords="2,319.66,307.04,16.08,10.91" target="#b13">[14]</ref>. Geospatial feature concatenation was implemented to combine image and location information. Additionally, the effect of different optimizers, including Sharpness Aware Minimization (SAM) <ref type="bibr" coords="2,360.44,334.14,17.93,10.91" target="#b14">[15]</ref> and AdamW <ref type="bibr" coords="2,440.13,334.14,16.27,10.91" target="#b15">[16]</ref>, as well as learning rate schedulers, including Cosine Annealing with Warm Restarts (CAWR) <ref type="bibr" coords="2,467.37,347.69,18.07,10.91" target="#b16">[17]</ref> was investigated.</p><p>The article is structured as follows: In Section 2, the related work in this field of research is described. Section 3 summarizes the SnakeCLEF 2022 data set and Section 4 describes the Machine Learning (ML) workflow and the methods used for implementation. Section 5 shows the results achieved using this workflow. Finally, the results are summarized and concluded in Section 6 which also mentions limitations and gives an outlook on future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In early research <ref type="bibr" coords="2,164.55,487.61,16.09,10.91" target="#b17">[18]</ref>, classical ML requiring manual extraction of features, was used to identify snake species. For example, taxonomic features of 1,299 images were extracted to differentiate six species in a semiautomatic approach <ref type="bibr" coords="2,269.91,514.71,16.24,10.91" target="#b17">[18]</ref>. However, manual feature extraction is a tedious task. To omit manual feature extraction, field-based approaches were developed which collect unstructured photographs and extract textural or deep learning features from snake images. Unfortunately, those images often suffered from poor image quality.</p><p>Color and Edge Directivity Descriptors (CEDDs) <ref type="bibr" coords="2,312.37,568.91,17.76,10.91" target="#b18">[19]</ref> are extracted as textural features in <ref type="bibr" coords="2,487.55,568.91,16.09,10.91" target="#b19">[20]</ref>. The data set included 349 images of 22 Malaysian snake species recorded at the Perlis Snake Park, Malaysia. The rarest species in this data set included three images. For the final classification, five classical ML models were applied. The best accuracy of 89.22 % was obtained for the nearest neighbor classifier.</p><p>In recently published studies <ref type="bibr" coords="2,229.20,636.65,16.34,10.91" target="#b20">[21,</ref><ref type="bibr" coords="2,248.27,636.65,12.51,10.91" target="#b21">22,</ref><ref type="bibr" coords="2,263.52,636.65,12.51,10.91" target="#b22">23,</ref><ref type="bibr" coords="2,278.77,636.65,12.51,10.91" target="#b23">24,</ref><ref type="bibr" coords="2,294.01,636.65,13.97,10.91" target="#b24">25]</ref> deep learning-based approaches are used for snake species identification. Some of these studies were designed as object detection tasks.</p><p>For example, different deep learning-based object detection methods were compared to each other in <ref type="bibr" coords="3,128.23,117.35,16.41,10.91" target="#b20">[21]</ref>. The data set which was extracted from ImageNet-1k <ref type="bibr" coords="3,393.00,117.35,18.07,10.91" target="#b25">[26]</ref> and augmented by a Google Image search <ref type="foot" coords="3,183.71,129.15,3.71,7.97" target="#foot_1">4</ref> included 1,027 images of eleven Australian species. The least frequent class contained 60 images. The best mean Average Precision (mAP) was achieved for a Faster Region-Based Convolutional Neural Network (Faster RCNN) <ref type="bibr" coords="3,358.12,158.00,17.81,10.91" target="#b26">[27]</ref> with a ResNet <ref type="bibr" coords="3,442.92,158.00,17.81,10.91" target="#b27">[28]</ref> backbone.</p><p>A similar approach <ref type="bibr" coords="3,185.48,171.55,17.76,10.91" target="#b21">[22]</ref> used Faster RCNN with different detection layers. The data set which was collected using three data sources contained 250 images of nine species occurring on the Gal√°pagos Islands, Ecuador. To collect the data set, two internet searches performed on the platforms Google and Flickr and an image data set provided by the Ecuadorian Institution of Tropical Herping <ref type="foot" coords="3,179.20,223.99,3.71,7.97" target="#foot_2">5</ref> were accessed. Similar to the previously described method, the ResNet backbone achieved the best accuracy of 75 %.</p><p>Further studies performed deep learning-based classification tasks. For example, the performances of three deep learning networks, namely VGG16 <ref type="bibr" coords="3,360.72,266.40,16.39,10.91" target="#b28">[29]</ref>, DenseNet161 <ref type="bibr" coords="3,446.48,266.40,16.40,10.91" target="#b29">[30]</ref>, and Mo-bileNetV2 <ref type="bibr" coords="3,137.13,279.94,18.07,10.91" target="#b30">[31]</ref> are compared in <ref type="bibr" coords="3,234.00,279.94,16.41,10.91" target="#b22">[23]</ref>. The data set contained 3,050 images of 28 species. As a pre-processing step, the GrabCut <ref type="bibr" coords="3,243.03,293.49,18.07,10.91" target="#b31">[32]</ref> algorithm was applied to extract the snakes from the image background. An accuracy of 72 % was reached for the test data set and the DenseNet161 architecture.</p><p>A deep Siamese network <ref type="bibr" coords="3,211.43,334.14,17.79,10.91" target="#b32">[33]</ref> for one-shot learning was developed in <ref type="bibr" coords="3,406.21,334.14,16.13,10.91" target="#b33">[34]</ref>. The network was trained on 200 images of the World Health Organization (WHO) venomous snake database. This data set included three to 16 images per class.</p><p>Although the previously described methods each investigated less than 30 distinguishable species, more than 600 out of 3,700 snake species worldwide are medically relevant <ref type="bibr" coords="3,461.60,388.34,16.25,10.91" target="#b34">[35]</ref>.</p><p>The SnakeCLEF challenge <ref type="bibr" coords="3,217.53,401.89,16.35,10.91" target="#b9">[10,</ref><ref type="bibr" coords="3,236.60,401.89,13.98,10.91" target="#b34">35]</ref> overcomes this disadvantage by providing a more diverse data set containing images of more than 1,000 species. Multiple deep learning approaches were successfully submitted in previous rounds of this challenge.</p><p>In SnakeCLEF 2020 <ref type="bibr" coords="3,187.43,442.54,12.72,10.91" target="#b8">[9]</ref> the winning approach <ref type="bibr" coords="3,302.40,442.54,17.80,10.91" target="#b35">[36]</ref> used a ResNet architecture pre-trained on ImageNet-21k <ref type="bibr" coords="3,154.69,456.08,17.93,10.91" target="#b36">[37]</ref> and reached a macro-averaging ùêπ 1 score of 62.54 %. The FHDO BCSG <ref type="bibr" coords="3,493.13,456.08,12.85,10.91" target="#b6">[7]</ref> combined object detection and image classification using a Mask-RCNN <ref type="bibr" coords="3,406.06,469.63,17.76,10.91" target="#b13">[14]</ref> instance detection framework and an EfficientNet-B4 <ref type="bibr" coords="3,249.47,483.18,18.07,10.91" target="#b37">[38]</ref> classification model. This method reached a macroaveraging ùêπ 1 score of 40.35 %. In post competition experiments, the score could be optimized to 59.4 %.</p><p>The winning approach <ref type="bibr" coords="3,199.57,523.83,17.76,10.91" target="#b38">[39]</ref> of SnakeCLEF 2021 combined object detection with an EfficientDet-D1 <ref type="bibr" coords="3,104.83,537.38,17.97,10.91" target="#b39">[40]</ref> model, and an EfficientNet-B0 classifier as well as likelihood weighting to fuse image and location information. The best model reached a macro-averaging ùêπ 1 score of 90.30 %.</p><p>Experiments with multiple Convolutional Neural Network (CNN) architectures were presented in <ref type="bibr" coords="3,134.22,578.03,16.41,10.91" target="#b40">[41]</ref>. The best ùêπ 1 score of 83.0 % was reached for an ensemble model combining two ResNeSt <ref type="bibr" coords="3,150.05,591.58,18.07,10.91" target="#b41">[42]</ref> models with a ResNet <ref type="bibr" coords="3,273.25,591.58,16.41,10.91" target="#b27">[28]</ref>, and a ResNeXt <ref type="bibr" coords="3,366.91,591.58,18.07,10.91" target="#b42">[43]</ref> model. The ensemble was generated using a majority voting of the top-1 predictions of the individual models.</p><p>The FHDO BCSG <ref type="bibr" coords="3,178.61,618.67,12.69,10.91" target="#b7">[8]</ref> expanded the SnakeCLEF 2020 workflow by combining object detection with EfficientNets and Vision Transformers (ViT) <ref type="bibr" coords="3,323.06,632.22,16.41,10.91" target="#b43">[44]</ref>. The best model was an ensemble averaging the model predictions of an EfficientNet-B4 model and ViTs. This submission reached a macro-averaging ùêπ 1 score of 78.75 %.</p><p>This research expands the ML workflow developed from FHDO BCSG <ref type="bibr" coords="4,417.30,130.90,11.42,10.91" target="#b6">[7,</ref><ref type="bibr" coords="4,431.45,130.90,9.00,10.91" target="#b7">8]</ref> in SnakeCLEF 2020 and SnakeCLEF 2021 by adding a new geospatial feature concatenation strategy, using YOLOv5 for object detection, and implementing diverse classification model architectures, learning rate schedulers as well as optimizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Set</head><p>The SnakeCLEF 2022 data set included 318,531 images of 187,129 observations. The training data set contained 270,251 (84.84 %) images belonging to 158,698 (84.81 %) observations. The remaining 48,280 (15.16 %) images pertaining to 28,431 (15.19 %) observations were used as a test set. The data set contains images of 1,572 different snake species and originated from the iNaturalist platform <ref type="foot" coords="4,177.94,282.62,3.71,7.97" target="#foot_3">6</ref> .</p><p>The distribution of images per snake species is highly imbalanced. The most frequent species was the Natrix natrix containing 5,518 images, for 20 species only three images were collected.</p><p>In addition to the photographs, metadata that provides information about the region (country), the country code (code), and if the species is endemic (endemic) is available. Most images (ùëõ = 63, 194; 23.38 %) are taken in the US. The region with the most images was Texas (ùëõ = 15, 138; 5.60 %). For 10,980 images (4.06 %) no information about the region was available. No country code was available for 8,487 training images (3.14 %). Of the 1,572 species, 267 (16.98 %) were endemic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head><p>In the following, the methods and the ML workflow, which was visualized in Figure <ref type="figure" coords="4,483.15,464.94,5.17,10.91" target="#fig_0">1</ref> are elaborated. The workflow was implemented in a modular way, thus, different submissions can investigate the effect of different parts and their interactions. The programming language Python v3.6.9 <ref type="bibr" coords="4,152.43,505.59,17.91,10.91" target="#b44">[45]</ref> was used for implementation.</p><p>The workflow divides into two stages, the pre-processing and the classification stage. The pre-processing stage starts with loading the image data set, followed by an optional object detection step. Object detection was successfully applied as a pre-processing step in previous snake species classification tasks <ref type="bibr" coords="4,237.59,559.79,11.36,10.91" target="#b6">[7,</ref><ref type="bibr" coords="4,251.66,559.79,7.47,10.91" target="#b7">8,</ref><ref type="bibr" coords="4,261.85,559.79,12.55,10.91" target="#b38">39,</ref><ref type="bibr" coords="4,277.12,559.79,12.32,10.91" target="#b45">46]</ref>. The object detection was followed by an image pre-processing step which scales the images to a uniform, quadratic size. The classification stage starts with image augmentation to make the subsequent deep learning models more robust <ref type="bibr" coords="4,121.53,600.43,16.41,10.91" target="#b46">[47]</ref>. EfficientNets, EfficientNet-v2 <ref type="bibr" coords="4,283.06,600.43,16.42,10.91" target="#b11">[12]</ref>, and ConvNeXt <ref type="bibr" coords="4,378.44,600.43,18.06,10.91" target="#b10">[11]</ref> models were trained to distinguish between snake species. As the data set included multiple images per observation, multi-instance learning summarized the predictions for each observation. Finally, optional metadata multiplication was implemented. In this step, the model's prediction probabilities and  the a priori probability distribution of the snake species given the regional information were implemented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Object Detection</head><p>The idea of using object detection as a pre-processing step for species classification originated from the winning team <ref type="bibr" coords="5,200.55,472.48,18.07,10.91" target="#b45">[46]</ref> of round 2 of the AICrowd Snake Species Identification Challenge <ref type="bibr" coords="5,116.02,486.03,16.25,10.91" target="#b34">[35]</ref>. The aim is to focus the classification model on the object that should be classified. The starting point for the training of the YOLOv5, as well as the Mask-RCNN object detection model, were 1,800 manually annotated snake images taken from last year's SnakeCLEF 2021 <ref type="foot" coords="5,501.96,511.37,3.71,7.97" target="#foot_4">7</ref>data set. Those were randomly split into a training (ùëõ = 1, 440, 80.0 %) and a validation (ùëõ = 360, 20.0 %) set. The object detection results are summarized in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">YOLOv5</head><p>YOLOv5<ref type="foot" coords="5,125.77,587.44,3.71,7.97" target="#foot_5">8</ref> , as a state-of-the-art object detection framework, is used in this work to detect snakes in non-standardized photographs and differentiate them from the background.</p><p>Unlike the object detectors before, which examine many potential regions in an image to find present objects, YOLO <ref type="bibr" coords="5,187.96,629.84,17.76,10.91" target="#b12">[13]</ref> directly works on the whole image. For this, YOLO divides the whole image into N√óN grid cells, and with only one forward pass through a trained YOLOv5 model, several bounding boxes are predicted for each cell. The cell which contains the object's center points is mainly responsible for detecting the object. The metric Intersection over Union (IoU) is used to evaluate object localization. In the case that multiple bounding boxes for different objects are in an image, non-max suppression (NMS) <ref type="bibr" coords="6,334.63,158.00,18.07,10.91" target="#b47">[48]</ref> is used to make sure that objects are only detected once. NMS works in that way that first, it looks for class probabilities (ùëÉ ùëê ) associated with each of these detections for particular objects. Secondly, the largest ùëÉ ùëê , which is the most confident detection for the object, is taken. Next, NMS is analyzing the remaining bounding boxes and selects all bounding boxes which have a high IoU with the bounding box of highest ùëÉ ùëê and suppresses them. This procedure is repeated on the remaining bounding boxes. Multiple anchor boxes are used if more than one central point of an object or an overlapping of a different object in the same cell. These anchor box shapes are slightly different, which can also help capture various shapes of objects.</p><p>First of all, a YOLOv5l model (later referred to as baseline) was trained for 200 epochs with a batch size of 14 and an image input size of 640 on 1,800 manually annotated snake images, taken from the SnakeCLEF 2021 data set randomly split into a training (ùëõ = 1, 440, 80.0 %) and a validation (ùëõ = 360, 20.0 %) set. Additionally, the pre-trained weights and the default hyperparameters for the YOLOv5l model were used.</p><p>This baseline model was used to predict bounding boxes on all images from the SnakeCLEF 2022 <ref type="foot" coords="6,109.99,359.49,3.71,7.97" target="#foot_6">9</ref> data set. Only the bounding boxes with a minimum confidence p of at least 85.0 % were selected. The images found with their corresponding bounding boxes were then split without further evaluation of the bounding box quality into training or validation sets based on the observation_id from the given metadata. The 1,800 manually annotated images of the SnakeCLEF 2021 data set were no longer used for training or validation.</p><p>This training and validation set, which only contains bounding boxes with at least a confidence of 85.0 %, was used to train another YOLOv5l model (later referenced to as YOLOv5l_basic) for 50 epochs, with a batch size of 24 and an image input size of 640. In addition, the pre-trained weights and the default hyperparameters for the YOLOv5l model were used. This trained model was then used for object detection.</p><p>Self-training <ref type="bibr" coords="6,159.92,496.73,18.07,10.91" target="#b48">[49]</ref> with a feature query on the confidence value of each bounding box was carried out in a higher number of iterations in the following experiment by starting with the best weights of the baseline model and default hyperparameters of YOLOv5, predicting bounding boxes for all snake images in the SnakeCLEF 2022 data set. Furthermore, the minimum confidence p of 70.0 % was specified for this first run to save only well-recognizable snakes and their bounding box for the next run. These bounding boxes were split without further evaluation of the bounding box quality together with the corresponding snake image using the observation_id in the metadata, either for training or validation. The enlarged data set consisting of snake images and corresponding bounding boxes was used to train another YOLOv5l model for 30 epochs with the same image input and batch size. This model was used for further self-training as already described, with the difference that this time only those images were viewed which did not yet have a bounding box with a confidence greater than p. The procedure was carried out three times, and in the last run, p was set to 55.0 % in order to include the remaining challenging images.</p><p>With the larger data set generated by the self-training approach above, a YOLOv5l model (referred to as YOLOv5l_adv) and a YOLOv5x6 model (referred to as YOLOv5lx6_adv) were created. Both models were examined in the following two experiments to see how well they could improve the snake classification model. The settings of YOLOv5 parameters in the experiments are listed in Table <ref type="table" coords="7,228.62,185.10,3.74,10.91" target="#tab_1">1</ref>.</p><p>1. In this experiment, the YOLOv5l model was selected as a compromise between model size and performance (mAP) among all YOLOv5 models <ref type="foot" coords="7,365.44,220.21,7.41,7.97" target="#foot_7">10</ref>  To obtain information about the bounding boxes generated by YOLOv5, Figure <ref type="figure" coords="7,444.68,485.61,4.97,10.91" target="#fig_1">2</ref> and Figure 3 can be used. Both were generated with the available training and validation data. Figure <ref type="figure" coords="7,471.39,499.16,4.97,10.91" target="#fig_1">2</ref> shows a 2D plot of the relative coordinates of the central point of all bounding boxes against each other to get information on where the bounding box center points are located in the original images. The plot shows, that most snakes are found in the middle of the images. No distortion into a specific direction was observed for the center points of the bounding boxes. Figure <ref type="figure" coords="7,477.28,553.35,4.97,10.91" target="#fig_2">3</ref> plots the relative width and height of all bounding boxes against each other to obtain information about the sizes of the bounding boxes. The plot shows no clear conspicuous observations. Some bounding boxes are extremely narrow or high. This might be caused by the anatomy of snakes but might lead to strong distortions if the images are scaled up. Additionally, there are accumulated observations on the diagonal line of the image. This observation suggests that the proportion of width and height of the bounding boxes correspond to the proportion of the images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Mask-RCNN</head><p>As a comparison, a Mask-RCNN <ref type="bibr" coords="8,242.19,503.02,18.06,10.91" target="#b13">[14]</ref> was trained similarly to previous work <ref type="bibr" coords="8,447.36,503.02,11.49,10.91" target="#b6">[7,</ref><ref type="bibr" coords="8,462.41,503.02,7.65,10.91" target="#b7">8]</ref>. Mask-RCNN is an extension of Faster RCNN <ref type="bibr" coords="8,265.12,516.57,18.03,10.91" target="#b26">[27]</ref> and implements instance segmentation. Instance segmentation is a combination of object detection and semantic segmentation. This means, that bounding boxes are identified for each object of interest, and each pixel in a bounding box was segmented into a predefined range of given classes. Accordingly, the Mask-RCNN architecture consists of two phases, the first phase is identical to Faster RCNN. This phase includes a Region Proposal Network to identify candidate bounding boxes and non-maximum suppression <ref type="bibr" coords="8,488.00,584.32,17.99,10.91" target="#b47">[48]</ref> to focus on the most promising candidates. In the second stage, a Region Of Interest (ROI) Align Network was applied to the remaining candidate bounding boxes followed by a parallel implementation of fully connected networks to identify the object class and the offset of the bounding boxes. Finally, a CNN was trained for the semantic segmentation task.</p><p>In this work, Mask-RCNN was trained to differentiate between snakes and background. For this reason, only the object detection part of the method was applied. The training of the Mask-RCNN was split into two phases. In the first warm-up phase, newly added layers were trained for 20 epochs. Afterwards, the weights of the entire model were fine-tuned for another 30 epochs.</p><p>An adaption 12 to Tensorflow 2.1.0 of the implementation of the Mask-RCNN model implemented by Abdulla 13 has been used to implement the Mask-RCNN. No data augmentation was implemented during training and a threshold of 0.3 was used for the minimum detection confidence. Stochastic Gradient Descent (SGD) <ref type="bibr" coords="9,303.34,519.55,18.02,10.91" target="#b49">[50]</ref> was used to optimize the model weights, with a momentum value of 0.9, a weight decay of 10 -4 , and a mini-batch size of 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Augmentations</head><p>The classification models were trained using six different image augmentation pipelines. The basic pipeline includes random cropping of approximately 10 % of the image length and width, as well as random horizontal and vertical flipping each with a probability of ùëù = 0.5. The images are normalized with a mean and standard deviation of 0.5 for each dimension. This pipeline was implemented using the Python package torchvision v0.11.2+cu111 <ref type="bibr" coords="9,450.99,637.03,16.40,10.91" target="#b50">[51]</ref>. In post competition experiments, a slightly different pipeline called base was used, differing mainly in cropping and normalization with ImageNet default values for scale (0.08 to 1.0), ratios ( <ref type="formula" coords="10,477.10,102.25,4.23,6.99">3</ref>4 to 4 3 ) as well as channel means (0.485, 0.456, 0.406) and standard deviations (0.229, 0.224, 0.225).</p><p>A comparison pipeline, including random horizontal and vertical flipping, Random erasing <ref type="bibr" coords="10,106.81,144.45,16.42,10.91" target="#b51">[52]</ref>, and normalization with the predefined ImageNet-1k values was implemented. The flipping transforms were performed with a probability of ùëù = 0.5 each. During random erasing, a rectangular region is randomly selected in the image. In this work, the pixel values in this region are replaced with 0. Random erasing was executed with a probability of ùëù = 0.5. In each image, between 2 % and 33 % of the original image area was erased. The aspect ratio of the erased area ranged between 0.3 and 3.3. Random erasing was implemented using the Python package timm v0.4.11 <ref type="bibr" coords="10,187.84,225.75,16.25,10.91" target="#b52">[53]</ref>. This pipeline is called the Era pipeline in the following.</p><p>RandAugment <ref type="bibr" coords="10,165.32,239.30,17.76,10.91" target="#b53">[54]</ref> was used in two augmentation pipelines. First, the Rand pipeline included random cropping of approximately 10 % of the image length and width, as well as RandAugment augmentations. The RandAugment algorithm randomly selects ùëõ out of 14 basic augmentations for each image and applies those with a magnitude of ùëö. The selected augmentations were applied sequentially. In the implementation, 31 magnitude steps were introduced. The Rand pipeline in this paper selected two augmentation transforms (ùëõ = 2) and a magnitude step of 9 (ùëö = 9). The images are normalized with a mean and standard deviation of 0.5 for each dimension. RandAugment was implemented using the Python package torchvision v0.11.2+cu111 <ref type="bibr" coords="10,153.94,347.69,16.25,10.91" target="#b50">[51]</ref>.</p><p>The fourth pipeline, namely RandEra, included the basic augmentations, RandAugment with ùëõ = 3 and ùëö = 10, as well as Random erasing with the previously defined hyperparameters. In this pipeline, Random erasing was implemented using the Python package torchvision v0.11.2+cu111 <ref type="bibr" coords="10,153.94,401.89,16.25,10.91" target="#b50">[51]</ref>.</p><p>The AutoEra pipeline combines the previously defined random erasing strategy and the AutoAugment <ref type="bibr" coords="10,157.08,428.99,18.07,10.91" target="#b54">[55]</ref> strategy. AutoAugment uses a reinforcement learning search strategy to optimize the image augmentation pipeline. The search space included 16 augmentation strategies, 10 augmentation magnitudes controlling the intensity of the augmentations as well as 11 probability steps. The AutoAugment strategy was implemented using the Python package timm v0.4.11 <ref type="bibr" coords="10,148.90,483.18,16.25,10.91" target="#b52">[53]</ref>.</p><p>The last pipeline is named AutoEraCut and includes, the previously described AutoAugment, and Random Erasing modules, as well as the CutMix <ref type="bibr" coords="10,320.57,510.28,17.75,10.91" target="#b55">[56]</ref> augmentation method. CutMix <ref type="bibr" coords="10,478.55,510.28,17.76,10.91" target="#b55">[56]</ref> is an augmentation strategy combining CutOut <ref type="bibr" coords="10,290.94,523.83,17.85,10.91" target="#b56">[57]</ref> and MixUp <ref type="bibr" coords="10,363.37,523.83,17.85,10.91" target="#b57">[58]</ref> augmentations. Parts of the original image are replaced by patches of different images in the training data set. In addition, label smoothing was applied by adapting the ground truth labels in proportion to the area of the patches. In this work, CutMix was applied with a probability of ùëù = 0.5, and the Python library timm v0.4.11 <ref type="bibr" coords="10,182.34,578.03,17.96,10.91" target="#b52">[53]</ref> was used for implementation with default values. The results of the different augmentation pipelines are summarized in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model Training</head><p>Three model architectures were implemented during model training. Those are described in Section 4.3.1, Section 4.3.2, and Section 4.3.3. All models and pre-trained weights were loaded using the Python library timm v0.4.11 <ref type="bibr" coords="10,269.15,668.40,16.41,10.91" target="#b52">[53]</ref>. PyTorch v1.10.1+cu111 <ref type="bibr" coords="10,403.88,668.40,18.07,10.91" target="#b50">[51]</ref> was used to train the models. The results achieved using the model architectures are presented in Section 5.3.</p><p>In addition, the effects of multiple optimizers, namely Adam <ref type="bibr" coords="11,370.08,103.81,16.41,10.91" target="#b58">[59]</ref>, SGD <ref type="bibr" coords="11,418.16,103.81,16.41,10.91" target="#b49">[50]</ref>, and SAM <ref type="bibr" coords="11,487.92,103.81,18.07,10.91" target="#b14">[15]</ref> with an SGD base optimizer were investigated. The parameters of the Adam optimizer were ùõΩ 1 = 0.9, ùõΩ 2 = 0.999 and ùúñ = 10 -8 . The Python library sam-pytorch v0.0.1 <ref type="bibr" coords="11,429.06,130.90,16.48,10.91" target="#b59">[60,</ref><ref type="bibr" coords="11,448.27,130.90,14.06,10.91" target="#b60">61]</ref> was used to implement the SAM optimizer. In comparison to the models trained using the Adam and SGD optimizers, no mixed precision <ref type="bibr" coords="11,257.31,158.00,18.06,10.91" target="#b61">[62]</ref> training was implemented for SAM. Some models were trained using CAWR <ref type="bibr" coords="11,204.82,171.55,17.76,10.91" target="#b16">[17]</ref> as a learning rate scheduler. For all optimizers, different learning rates and mini-batch sizes were used as is documented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">EfficientNets</head><p>The base architecture of EfficientNets <ref type="bibr" coords="11,262.62,234.43,18.06,10.91" target="#b37">[38]</ref> resulted from an architecture search. This search optimizes for both, accuracy and Floating-Point Operations Per Second (FLOPS). EfficientNets mainly consisted of Mobile Inverted Bottleneck Convolutional (MBConv) layers. To increase model performances, the base model is successively scaled up using a uniform balance between model depth, model width, and image resolution. While EfficientNets are smaller and faster than many of the compared models, they achieved state-of-the-art performances on the ImageNet classification task <ref type="bibr" coords="11,170.60,315.72,16.25,10.91" target="#b37">[38]</ref>.</p><p>In this work, EfficientNet-B4 and EfficientNet-B5 models were trained for snake species detection. For the EfficientNet-B4 models, all images were scaled to an image size of 380 √ó 380 pixels. An image size of 456 √ó 456 pixels was used for the EfficientNet-B5 models. The model weights were initialized using a model pre-trained on the ImageNet-1k data set <ref type="bibr" coords="11,434.36,369.92,16.08,10.91" target="#b25">[26]</ref>. The output layer of the model included 1,572 neurons corresponding to the number of snake species in the data set. The resulting EfficientNet-B4 model contained 20,367,212 parameters, whereas the EfficientNet-B5 model contained 31,561,812 parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">EfficientNet-v2</head><p>EfficientNet-v2 <ref type="bibr" coords="11,157.41,459.90,17.76,10.91" target="#b11">[12]</ref> is an advanced version of EfficientNets, intended to counteract the observed bottlenecks. Therefore, the original EfficientNet search space used for the architecture search was enriched. For example, later MBConv blocks of EfficientNets were gradually replaced by Fused-MBConv blocks. The architecture search optimizes for accuracy, parameter efficiency, and training efficiency. To avoid excessive memory consumption, the uniform training strategy of EfficientNets was replaced by a non-uniform strategy. The resulting EfficientNet-v2 models train up to four times faster than the original EfficientNet models, and the number of parameters was reduced with a factor of 6.8 <ref type="bibr" coords="11,235.01,554.74,16.25,10.91" target="#b11">[12]</ref>.</p><p>This work uses the EfficientNet-v2-M model with an image size of 384 √ó 384 pixels. The model weights were initialized using a model pre-trained on the ImageNet-1k data set <ref type="bibr" coords="11,476.02,581.84,17.95,10.91" target="#b25">[26]</ref> as well as on the ImageNet-21k data set <ref type="bibr" coords="11,250.35,595.39,16.09,10.91" target="#b36">[37]</ref>. The output layer of the model included 1,572 neurons. The resulting EfficientNet-v2-M model contained 54,872,088 parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">ConvNeXt</head><p>Vision Transformers (ViTs) <ref type="bibr" coords="11,213.78,658.27,17.97,10.91" target="#b43">[44]</ref> and its further developments have quickly gained popularity and superseded CNNs as state-of-the-art models for image classification. The ConvNeXt archi-tecture <ref type="bibr" coords="12,123.14,103.81,17.90,10.91" target="#b10">[11]</ref> is an approach to modernize the standard CNN architecture (ResNet50) regarding the design choices of ViT. Therefore, the authors of ConvNeXt conducted several experiments to discover the key components that lead to the performance differences. A key component was changing the multi-stage macro design of the architecture to reduce the stage computation ratio and changing the stem to a simpler "patchify" layer similar to ViT <ref type="bibr" coords="12,415.86,158.00,16.41,10.91" target="#b10">[11]</ref>. Other changes included the use of inverted bottleneck blocks with depth-wise convolution, a larger kernel size, and an increased network width to the same number of channels as the Swin-Transformer <ref type="bibr" coords="12,487.54,185.10,16.10,10.91" target="#b10">[11]</ref>. ConvNeXt also adopted some features of the micro-scale architecture of transformers, such as replacing the Rectified Linear Units (ReLU) activation function with its smoother Gaussian Error Linear Unit (GELU) <ref type="bibr" coords="12,179.29,225.75,17.99,10.91" target="#b62">[63]</ref> variant, using fewer normalization layers, and replacing BatchNorm layers with simple Layer-Normalization <ref type="bibr" coords="12,275.27,239.30,16.41,10.91" target="#b10">[11]</ref>. Other performance differences resulted from similar training techniques as for ViT, e.g., the use of the AdamW <ref type="bibr" coords="12,394.84,252.85,18.07,10.91" target="#b15">[16]</ref> optimizer, extended training epochs, heavy data augmentation including CutMix, RandAugment, Random erasing, and label smoothing <ref type="bibr" coords="12,182.59,279.94,16.25,10.91" target="#b10">[11]</ref>.</p><p>In this work, ImageNet-21k <ref type="bibr" coords="12,222.24,293.49,17.76,10.91" target="#b36">[37]</ref> pre-trained ConvNeXt-L models were trained with an image size of 384 √ó 384 pixels and an adjusted output layer size of 1,572 neurons. The resulting ConvNeXt-L models contained a total of 198,619,428 parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Using Geospatial Feature Concatenation</head><p>As mentioned in Section 3, the data set contains additional metadata including region information for most of the images. The idea is to train a model that uses the region information as additional features to the image data to improve the classification performance of the snake species.</p><p>Unfortunately, the metadata only contains nominal region information in ISO-3166-ALHPA2 encoded form, which makes it difficult to interpret geographical relationships such as location and distance between region information. A more practical way of representing region information for deep learning, which also allows interpretation of geographic relationships, is the use of numerical geospatial data such as latitude and longitude coordinates. For this reason, a workflow was implemented to convert the ISO encoded region information of the metadata to geospatial data with latitude and longitude coordinates. Therefore, the ALPHA2-ISO codes were translated into country names using the Python package pycountry <ref type="foot" coords="12,408.41,517.20,7.41,7.97" target="#foot_8">14</ref> , and then converted to geospatial data with latitude and longitude coordinates in decimal notation (-90¬∞to +90¬∞f or latitude and -180¬∞to +180¬∞for longitude) using the Python package GeoPy <ref type="foot" coords="12,459.80,544.30,7.41,7.97" target="#foot_9">15</ref> and the OpenStreetMap <ref type="foot" coords="12,159.82,557.85,7.41,7.97" target="#foot_10">16</ref> services. A drawback of this method is that only the longitude and latitude coordinates of the center of a region are considered as well as the fact that the earth is a sphere and coordinates of e.g. -179¬∞and 179¬∞longitude are far away by notation, although they are actually very close to each other on the sphere.</p><p>To use geospatial data as additional features to the image data, the model architecture must be adapted. For this purpose, a feature concatenation approach inspired by the Wide&amp;Deep architectures <ref type="bibr" coords="12,151.27,640.90,18.07,10.91" target="#b63">[64]</ref>   which are then represented as a flattened vector using global average pooling. The normalized geospatial data of longitude and latitude coordinates (-1 to 1) are then concatenated with this flattened image feature vector and passed to the fully connected classifier. To avoid linear decision boundaries by multiplying the geospatial data with the image features, the classifier contains a hidden fully connected layer with normalization and activation function. Empirical experiments with different normalization methods such as BatchNorm and LayerNorm as well as the activation functions tanh, Sigmoid Linear Units (SiLU) showed that BatchNorm <ref type="bibr" coords="13,469.22,386.33,17.82,10.91" target="#b64">[65]</ref> and SiLU <ref type="bibr" coords="13,114.09,399.88,18.07,10.91" target="#b65">[66]</ref> provided the best results. To regulate early overfitting, dropout (ùëù = 0.5) is used before the output layer. In Figure <ref type="figure" coords="13,237.72,413.43,3.68,10.91" target="#fig_3">4</ref>, a schematic representation of the model architecture using an EfficientNet-B4 as the CNN backbone is shown.</p><p>It should be noted that some images did not contain any region information in the data set metadata and instead were marked as "unknown". To pass geospatial data for these images to the model during training and testing, random values for the longitude and latitude coordinates were determined. Random values from a uniform distribution have the disadvantage that they could fall in regions that are not included in the data set, such as oceans. To prevent this, the random values were drawn from a density function that was previously determined from the frequencies of all known longitude and latitude coordinates in the data set using kernel density estimation of scikit-learn 17 package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Test Time Augmentation (TTA)</head><p>Test Time Augmentation (TTA) <ref type="bibr" coords="13,235.18,585.09,16.55,10.91" target="#b66">[67,</ref><ref type="bibr" coords="13,254.48,585.09,14.11,10.91" target="#b67">68]</ref> is a method to make model predictions more robust. During model inference, multiple augmented versions of an image are presented to the model. Similar to multi-instance learning methods, the model predictions of one image are summarized. In this work, the basic augmentation pipeline was used to generate ten augmented versions of each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Multi-Instance Learning</head><p>As was described in Section 3, some of the observations in the data set contained more than one image. However, one snake species per observation should be predicted. To summarize the model's prediction probabilities per class, those are averaged across all images of an observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Multiplication with Regional Prior Probabilities</head><p>Regional information was optionally added to the probability predictions of the classification models. In this work, the prior probabilities were estimated by the relative frequency distribution of observations per snake species and region in the training data set. Two strategies were applied to combine the region and the image information. First, the raw regional prior probabilities were multiplied with the prediction probabilities of the observation. The second strategy used a binarized version of the raw probabilities with a cut-off value of 0. Prior probabilities with nonzero values were transformed to one, whereas those with a value of zero were kept unchanged. For images with missing regional information, as well as for regions not available in the training data set, the prior probability of the "unknown" class was used. The results achieved using multiplication with regional prior probabilities are summarized in Section 5.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In the following section, the classification results of the ML workflow, including multiple ablation studies to investigate the effects of the modules, are described. The macro-averaging ùêπ 1 scores of the classification models for the private (ùêπ ùëùùëüùëñùë£</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>) and the public (ùêπ ùëùùë¢ùëè 1 ) test data set are summarized in Table <ref type="table" coords="14,199.92,422.42,3.68,10.91" target="#tab_3">2</ref>. Additional information about the models is given in Table <ref type="table" coords="14,467.81,422.42,10.00,10.91" target="#tab_1">10</ref> in the appendix. For better readability, ùêπ 1 scores, as well as improvements in ùêπ 1 scores are given as percentage values in this work. It can be shown, that the results achieved for the public and the private data set showed reasonable coherence. The best results during the challenge are achieved for model 32, which was an ensemble of seven different models, namely model 1, model 6, model 16, model 24, model 25, model 29, and model 30. Those models are three EfficientNet-B4 models, one EfficientNet-B5 model, one ConvNeXt-L model, and two EfficientNet-v2-M models. In the ensemble, the raw predictions of all models were averaged without weighting. Afterwards, the metadata multiplication was applied using the regional prior probabilities. This ensemble reached an ùêπ ùëùùë¢ùëè 1 score of 75.426 % and an ùêπ ùëùùëüùëñùë£ 1 score of 70.798 %. Some post competition experiments were performed after the SnakeCLEF 2022 deadline. During this phase, the previous results were outperformed by model 34, which is the same ensemble model as model 32 but was multiplied with the prior probabilities of the country code. This multiplication results in an ùêπ ùëùùë¢ùëè 1 score of 78.085 % and an ùêπ ùëùùëüùëñùë£ 1 of 73.900 %. The ablation studies investigated the use of object detection (introduced in Sec. 5.1), image augmentation strategies (introduced in Sec. 5.2), deep learning classifiers (introduced in Sec. 5.3), feature concatenation using geospatial data (introduced in Sec. 5.4), transfer learning (introduced in Sec. 5.5), optimizers and learning rate schedulers (introduced in Sec. 5.6), TTA (introduced in Sec. 5.7), and the multiplication with prior probabilities of regional information available for the snake images (introduced in Sec. 5.8). All models were trained on an Ubuntu server using NVIDIA Tesla V100 16GB GPUs. Training runtimes of a classification model on a single GPU, e.g. for the model with ID 35, took about 17h for 30 epochs. Due to these long training runtimes, some models were trained on up to four GPUs, which reduced the training runtime, e.g. for model ID 11 to about 6h for 30 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Object Detection</head><p>This evaluation makes a distinction between the results with the multiplication of regional prior probabilities and the results without it.</p><p>The basis for the comparison of the improvement of the ùêπ 1 scores is the classification model with ID 1. This model used neither regional information nor object detection. On average, the YOLOv5 trials could improve the public ùêπ 1 score by about +15.97 % and the private ùêπ 1 score by around 13.01 %. The object detection model with Mask-RCNN could improve the private ùêπ 1 by 11.33 % and the public ùêπ 1 by 7.32 %. The exact values for the individual approaches are summarized in Table <ref type="table" coords="16,184.82,248.38,3.74,10.91" target="#tab_4">3</ref>.</p><p>The results of the classification model with ID 2 are used to compare the results that were generated, taking into account the multiplication with regional prior probabilities. Here, the YOLOv5 models improved on average, the ùêπ ùëùùëüùëñùë£  <ref type="table" coords="16,365.89,318.68,3.74,10.91" target="#tab_5">4</ref>. However, the performance of both frameworks cannot be compared fairly, as the YOLOv5 models have been trained using active learning and active learning generated a large non- In these experiments, the object detection approaches, mainly increased the public and private ùêπ 1 scores. However, for the comparison between model 27 (trained without object detection) and model 28 (trained with object detection) a more complex behaviour was observed. For the ùêπ ùëùùë¢ùëè 1 score, model 27 performed 0.456 % points (0.617 %) better than model 28. On the other hand, model 28 reached an ùêπ ùëùùëüùëñùë£ 1 score of 69.733 % which was 0.241 % points (0.347 %) better than model 27. Thus, the snake classification model could be positively influenced by object detection. Without multiplying the model prediction with the regional prior probabilities, the influence of object detection is higher, but still, the ùêπ 1 score is lower than the ùêπ 1 score of the results that consider the multiplication with the regional prior probabilities. In these results, the influence of object recognition is less pronounced, but there is still a positive influence. Due to time constraints, further experiments to improve object detection were not conducted for this competition. The individual object detection models using YOLOv5 required some computation time due to the large image data set. For the YOLOv5l_adv model, an approximate time of 75 minutes per epoch can be given, and for the YOLOv5x6_adv model, an approximate time of 630 minutes per epoch was observed. The latter model required an exceptionally long time for training due to the small batch size (4) and the large image input (1280 px) and was therefore stopped manually after the 16th epoch (after about seven days of training).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Image Augmentations</head><p>As previously described in Section 4.2, six augmentation pipelines were trained and compared to each other, those comparisons are summarized in Table <ref type="table" coords="17,344.33,412.11,3.66,10.91" target="#tab_6">5</ref>. The hyperparameters not included in this table are equals for all models. The comparison of model 1 and model 5 shows, that the RandAugment pipeline (model 5) outperformed the basic pipeline (model 1) by 2.131 % points (4.294 %) on the public data set. Both architectures are EfficientNet-B4 which were trained without object detection and with an Adam optimizer. However, for the private data set both models reached a similar performance. The ùêπ ùëùùëüùëñùë£ 1 score for model 1 was 0.180 % points (0.396 %) better than the ùêπ ùëùùëüùëñùë£ 1 score for model 5. Model 2 is the same model as model 1 but the model's prediction probabilities are multiplied with the prior probabilities of the region. The same applies for model 5 and model 6. In the comparison between model 2 and model 6, the latter, trained with the Rand augmentation pipeline slightly outperforms model 2 by 0.383 % points (0.571 %) for the ùêπ ùëùùë¢ùëè 1 score and by 0.121 % points (0.196 %) for the ùêπ ùëùùëüùëñùë£ 1 . In summary, the basic and Rand pipelines achieved similar results in the experiments. Slight advantages are observed for the RandAugment pipeline.</p><p>Additional experiments on data augmentation were conducted after the challenge to test base, Rand, RandomErasing, CutMix, and combinations of them under more comparable conditions. For this purpose, an EfficientNet-B4 pre-trained on ImageNet-1k was trained with images without object detection for 45 epochs. The results are shown in Table <ref type="table" coords="17,448.98,644.75,5.17,10.91" target="#tab_6">5</ref> under post competition experiments. As described in section 4.2, augmentation base differs slightly from basic in terms of cropping and normalization, making the results of the post competition experiments not directly comparable to the previous experiments. The key findings of these experiments were that much better results were obtained with the augmentation methods base + Rand or CutMix as well as combinations of them. In addition, it was noticeable that RandomErasing + base performed slightly better than base by itself, but in combination with Rand and CutMix the results were much worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Classification Model</head><p>This section compares the results of different classification models. A summary of those results can be found in Table <ref type="table" coords="18,187.93,554.77,3.74,10.91">6</ref>.</p><p>Regarding those comparisons, it has to be noted, that image and batch sizes differed across models which might lead to biases in the comparison.</p><p>The comparison between model 1 and model 15, as well as between model 2 and model 16 shows, that the EfficientNet-B5 model outperforms the EfficientNet-B4 model for the ùêπ ùëùùë¢ùëè  ) achieved a marginal improvement of about 0.8 % points ùêπ ùëùùë¢ùëè 1 and 3.0 % points ùêπ ùëùùëüùëñùë£ 1 for snake species classification. Comparing the results of the models after multiplying the prediction probabilities by regional prior probabilities, as mentioned in Section 4.7, the experiments with IDs 4 and 36 are relevant. Multiplying the prediction probabilities by regional prior probabilities and without geospatial feature concatenation, the EfficientNet-B4 achieved an ùêπ ùëùùë¢ùëè 1 of 71.130 % and an ùêπ ùëùùëüùëñùë£ 1 of 65.761 %. In contrast, the EfficientNet-B4 with geospatial feature concatenation and multiplication of the prediction probabilities by regional prior probabilities only reached an ùêπ ùëùùë¢ùëè 1 of about 64.048 %, and an ùêπ ùëùùëüùëñùë£ 1 of about 61.439 %. This is an improvement for the snake species classification compared to the model with geospatial feature concatenation and without multiplying the prediction probabilities by regional prior probabilities. However, it is also a degradation compared to the model without geospatial feature concatenation and multiplication of prediction probabilities by regional prior probabilities.</p><p>Similar effects were obtained for models with ConvNeXt-L CNN backbones in the experiments with <ref type="bibr" coords="20,112.88,588.80,27.06,10.91">ID 21,</ref><ref type="bibr" coords="20,143.21,588.80,12.80,10.91" target="#b21">22,</ref><ref type="bibr" coords="20,159.28,588.80,12.80,10.91" target="#b22">23,</ref><ref type="bibr" coords="20,175.35,588.80,32.70,10.91" target="#b23">and 24.</ref> In general, the classification results for snake species performed better than with EfficientNet-B4. It should be noted that this comparisons should be made with caution because different hyperparameters were used in the experiments of EfficientNet-B4 and ConvNeXt-L. Comparing the classification result of the ConvNeXt-L model (ID 21) with geospatial feature concatenation, the ùêπ ùëùùë¢ùëè 1 of about 68.129 % and ùêπ ùëùùëüùëñùë£ 1 of about 64.987 % are higher than for the ConvNeXt-L model (ID 23) without geospatial feature concatenation with ùêπ ùëùùë¢ùëè 1 of about 62.499 % and ùêπ ùëùùëüùëñùë£ 1 of about 59.009 %. However, it can also be seen that the ConvNeXt-L model (ID 22) with geospatial feature concatenation and multiplication of the prediction probabilities by binarized regional prior probabilities achieved a lower classification result than the model (ID 24) without geospatial feature concatenation and multiplication of the prediction probabilities by binarized regional prior probabilities.</p><p>Based on these results, it can be assumed that the concatenation of geospatial features has only a minor effect on the classification result, while the simple multiplication of the prediction probabilities by the regional prior probabilities has a more positive effect on the classification of the snake species. Due to time constraints, no further experiments with geospatial feature concatenation models were performed during the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Transfer Learning</head><p>The SnakeCLEF 2022 data set contains 1,572 snake species and thus more than the 1,000 classes of the ImageNet-1k data set. It was assumed that a model pre-trained for a data set with a larger number of classes could improve the model performances. For this reason, one experiment was carried out to compare EfficientNet-v2-M models pre-trained with the ImageNet-1k (model 33) and the ImageNet-21k (model 27) data set. Model 27 which was pre-trained with the ImageNet-21k data set slightly outperformed the model 33 for the ùêπ ùëùùë¢ùëè 1 score by 1.245 % points (1.705 %). However, model 33, pre-trained for the ImageNet-1k data set slightly outperformed model 27 for the ùêπ ùëùùëüùëñùë£ 1 score by 0.739 % points (1.063 %). Overall, no clear advantage was observed comparing the ImageNet-1k and the ImageNet-21k data sets for transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Optimizers and LR Schedulers</head><p>The effect of CAWR as a LR Scheduler for snake species identification was investigated using ablation studies. The results are summarized in Table <ref type="table" coords="21,322.92,433.60,3.66,10.91" target="#tab_8">8</ref>. The CAWR scheduler was implemented using two hyperparameter settings. In both settings, the number of iterations for the first restart was set to 5 epochs. The factor increasing the number of epochs between two subsequent restarts was set to 1 in the first setting and to 2 in the second one. The comparisons between model 9 and model 38, as well as between model 10 and model 39 used the first setting. For the ùêπ ùëùùë¢ùëè 1 score, both models showed decreased results using the CAWR. The ùêπ ùëùùëüùëñùë£ 1 score also decreased. A slight improvement of 0.012 % points (0.019 %) in the ùêπ ùëùùëüùëñùë£ 1 score was achieved using the CAWR with the first setting for model 39. Model 27 was trained using CAWR with the second setting. The results of this model outperformed the results of model 40 by 2.879 % points (4.034 %) for the ùêπ ùëùùë¢ùëè  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Test Time Augmentation</head><p>As mentioned in Section 4.5, the effect of TTA on the classification result of the snake species was evaluated. The experiments with IDs 12, 13, and 14 of . It should be noted that the prediction probabilities of both models were multiplied by the raw regional prior probabilities during the experiments, as mentioned in Section 4.7. In the experiment with ID 13, the prediction probabilities were multiplied by the regional binarized prior probabilities and results of 67.502 % ùêπ ùëùùë¢ùëè 1 and 65.083 % ùêπ ùëùùëüùëñùë£ 1 were achieved. Comparing these results with the TTA experiment of ID 14, the effect of TTA is minimal. This is one of the reasons, besides the fact that TTA requires much longer inference times and only a limited number of submissions were allowed during the challenge, why TTA was not applied in further experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">Multiplication with Regional Prior Probabilities</head><p>The results of the ablation study executed to identify the effects of multiplication with regional prior probabilities are summarized in Table <ref type="table" coords="22,289.02,594.71,3.81,10.91" target="#tab_11">9</ref>. In all experiments, the results achieved using multiplication with regional prior probabilities outperformed the raw predictions. The mean improvement achieved by multiplying the raw regional prior probabilities to the model predictions is 14.725 % points <ref type="bibr" coords="22,212.68,635.36,19.18,10.91">(27.</ref>   score. Similar to previous work <ref type="bibr" coords="23,210.57,347.53,11.23,10.91" target="#b6">[7,</ref><ref type="bibr" coords="23,224.35,347.53,7.49,10.91" target="#b7">8]</ref>, the comparison between model 12 which was multiplied with raw regional prior probabilities of the and model 13, multiplied with the binarized regional prior probabilities shows improved ùêπ ùëùùë¢ùëè 1 (0.293 % points, 0.435 %) and ùêπ ùëùùëüùëñùë£ 1 scores (1.417 % points, 2.226 %) for the binarized variant.</p><p>After the deadline of the challenge expired, another variant of the best performing model was submitted. Instead of multiplying model predictions with the regional binarized prior probabilities, those were multiplied with the binarized prior probabilities of the country code. The comparison of model 32 and model 34 visualized in Table <ref type="table" coords="23,358.35,442.38,4.97,10.91" target="#tab_11">9</ref> shows a further improvement of 2.659 % points (3.525 %) for the ùêπ ùëùùë¢ùëè Those improvements might result from less images with unknown values in the country codes in comparison to the country information (described in Section 3) as well as less restrictive prior probability distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, a deep learning based workflow was implemented to identify snake species in photographs. Additional metadata available in the training and test data set include the region and the country code. The workflow included object detection, image augmentation, classification model training, feature concatenation, multi-instance learning, TTA, and multiplication with regional prior probabilities. The effects of interesting modules are investigated using ablation studies. Due to limitations in time and number of submissions during the challenge, for some experiments, no ablation studies were executed, those were added using post competition experiments.</p><p>In comparison to the participation in SnakeCLEF 2021, the classification models were expanded using ConvNeXt and EfficientNet-v2 models. To improve the results of the object detection as a pre-processing step, Mask-RCNN was replaced by YOLOv5. Additionally, geospatial feature concatenation was implemented to combine image and location information.</p><p>Except for one experiment, the results of the ablation studies showed improved results for the YOLOv5 object detection in comparison to the training without object detection and to the Mask-RCNN model implemented in prior work. The different image augmentation pipelines showed only slight performance differences in the snake classification. For the classification models, EfficientNet-B5 models as well as EfficientNet-v2-M models outperformed EfficientNet-B4 models. ConvNeXt-L models reached similar performances as the EfficientNet-v2-M models. However, these comparisons were not totally fair, because image sizes, and batch sizes differed between model architectures. Improved results were achieved using the SAM instead of the SGD optimizer and CAWR as a learning rate scheduler.</p><p>Geospatial feature concatenation positively affects the classification results, but simple multiplication of the model prediction with the prior probabilities of the regional metadata had a more positive effect on the snake species classification. The multiplication with prior probabilities extracted from the country codes outperformed the multiplication with prior probabilities of the regions. The best results during the challenge were achieved for an ensemble model consisting of multiple architectures. Using TTA, slight improvements were achieved.</p><p>Future work will address transfer learning with classifiers pre-trained with biodiversity data. More recently developed deep learning architectures like MetaFormers <ref type="bibr" coords="24,412.34,347.69,18.05,10.91" target="#b68">[69]</ref> show promising results <ref type="bibr" coords="24,123.19,361.24,18.06,10.91" target="#b69">[70]</ref> in Fine-Grained Visual Classification (FGVC) and should thus be examined in future work. Additionally, the impact of different mini-batch sizes and learning rates should be investigated more systematically. The data set is imbalanced with only three observations in the training data set for some species. To overcome this problem, future work should address oversampling to improve the influence of underrepresented species. The problem of highly imbalanced class distributions can be also addressed by using specific loss functions including ArcFace <ref type="bibr" coords="24,126.80,442.54,17.76,10.91" target="#b70">[71]</ref> or SeeSaw <ref type="bibr" coords="24,194.51,442.54,16.09,10.91" target="#b71">[72]</ref>. Most of the presented classifiers were trained without a validation data set, this procedure might lead to model overfitting. To avoid overfitting and thus make the prediction models more robust, future work should monitor the model training process using a validation data set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,359.94,279.87,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: ML workflow used to differentiate between snake species.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,89.29,432.68,416.69,9.65;8,89.29,444.90,258.95,8.87;8,126.31,101.03,340.15,325.28"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison between all relative coordinates ùëã 0 and ùëå 0 of bounding box center points in the created training (n=234,886) and validation (n=37,582) datasets.</figDesc><graphic coords="8,126.31,101.03,340.15,325.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,89.29,402.57,416.69,8.93;9,88.99,414.58,191.07,8.87;9,140.49,101.03,311.80,294.79"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison between all relative widths and heights of bounding boxes in the created training (n=234,886) and validation (n=37,582) datasets.</figDesc><graphic coords="9,140.49,101.03,311.80,294.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="13,89.29,255.47,416.69,8.93;13,89.29,267.48,263.00,8.87"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Schematic representation of an EfficientNet-B4 model architecture using feature concatenation of image and geospatial features for snake species classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="16,281.62,294.89,4.23,6.99;16,301.73,289.02,131.09,11.36;16,435.80,289.02,71.86,10.91;16,88.97,303.72,291.12,10.91;16,381.60,301.15,15.33,6.99;16,380.09,309.58,4.23,6.99;16,400.42,303.72,105.57,10.91;16,89.29,319.41,7.02,9.57;16,97.82,316.11,12.79,6.99;16,96.31,324.55,4.23,6.99;16,113.84,318.68,249.33,10.91"><head>1 score by 5 . 53 %</head><label>1553</label><figDesc>and public ùêπ 1 score by 6.33 %. The object detection model with Mask-RCNN could improve the ùêπ ùëùùëüùëñùë£ 1 score by 4.48 % and the ùêπ ùëùùë¢ùëè 1 score by 2.15 %. The exact values can be found in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="18,491.18,614.83,4.23,6.99;18,89.29,623.66,27.13,10.91;18,117.94,621.09,15.33,6.99;18,116.43,629.52,4.23,6.99;18,137.42,623.66,368.77,10.91;18,89.29,637.21,416.94,10.91;18,89.29,650.76,207.54,10.91;18,298.35,648.19,12.79,6.99;18,296.83,656.62,4.23,6.99;18,314.36,650.76,191.62,10.91;18,89.29,666.19,7.02,9.57;18,97.82,662.89,15.33,6.99;18,96.31,671.32,4.23,6.99;18,116.66,665.46,25.56,10.91;18,100.20,679.01,405.78,10.91"><head>1 and ùêπ ùëùùëüùëñùë£ 1 scores.</head><label>1</label><figDesc>Model 1 and model 15 did not use the multiplication with regional prior probabilities. In this comparison, the EfficientNet-B5 model outperformed the EfficientNet-B4 model for 5.048 % points (10.172 %) for the ùêπ ùëùùë¢ùëè 1 score and 4.021 % points (8.841 %) for the ùêπ ùëùùëüùëñùë£ 1 score. Smaller differences were reached for model 2 and model 16, which used the multiplication</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="21,205.32,562.55,4.23,6.99;21,222.85,556.69,174.57,10.91;21,398.94,554.12,15.33,6.99;21,397.42,562.55,4.23,6.99;21,415.05,556.69,2.40,10.91;21,100.20,570.23,405.78,10.91;21,89.29,583.78,416.69,10.91;21,89.29,597.33,177.35,10.91;21,266.64,596.11,10.82,6.99;21,277.96,597.33,228.23,10.91;21,88.89,610.88,417.30,10.91;21,89.29,624.43,270.13,10.91;21,360.94,621.86,12.79,6.99;21,359.42,630.29,4.23,6.99;21,376.70,624.43,129.94,10.91;21,89.29,639.13,38.92,10.91;21,129.73,636.56,15.33,6.99;21,128.22,644.99,4.23,6.99;21,148.50,639.13,357.48,10.91;21,89.29,652.68,416.69,10.91;21,89.29,666.23,121.00,10.91"><head>1 and 2 . 1 .</head><label>121</label><figDesc>822 % points (4.233 %) for the ùêπ ùëùùëüùëñùë£ The comparison of model 25 and model 41 investigated, whether the SAM optimizer improves the snake species detection. Model 25 was trained using the SAM optimizer with an SGD base classifier and a learning rate of 10 -1 . The comparison model 41 with an SGD classifier with the same learning rate. The results show, that model 25, which used the SAM optimizer outperformed model 41 by 4.472 % points (6.982 %) for the ùêπ ùëùùë¢ùëè 1 and 3.499 % points (5.695 %) for the ùêπ ùëùùëüùëñùë£ 1 score. However, no further investigations with the SAM classifier were performed because the implementation of the SAM classifier did not support mixed precision, which increases the training time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="23,179.92,339.84,4.23,6.99"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="23,244.19,461.79,4.23,6.99;23,261.71,455.92,200.14,10.91;23,463.37,453.36,15.33,6.99;23,461.86,461.79,4.23,6.99;23,482.21,455.92,25.46,10.91"><head>1 score and 3 .</head><label>13</label><figDesc>102 % points (4.381 %) for the ùêπ ùëùùëüùëñùë£ 1 score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,221.96,418.19,246.37"><head>Table 1</head><label>1</label><figDesc>Settings used for the YOLOv5 models as well as the size of the data set on which they were trained (#images ùë°ùëüùëéùëñùëõ ) and validated (#images ùë£ùëéùëô ) are presented below. Abbreviations: OD: Object detection, Img: Image., train: training dataset, val: validation dataset.</figDesc><table coords="7,373.35,221.96,132.64,10.91"><row><cell>. It was trained for 50 epochs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,172.30,640.90,334.89,10.91"><head></head><label></label><figDesc>was chosen. A CNN backbone first extracts features from the image data,</figDesc><table coords="13,133.46,99.61,325.52,149.28"><row><cell></cell><cell>1. Extract image</cell><cell cols="3">2. Concatenation of</cell><cell></cell><cell></cell><cell cols="4">3. Fully connected</cell><cell></cell></row><row><cell></cell><cell>features with CNN</cell><cell cols="3">image features with</cell><cell></cell><cell></cell><cell></cell><cell cols="2">classifier</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">geospatial features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Image Data Geospatial Data: Longitude Latitude 3x380x380</cell><cell>1x1 EfficientNetB4 Backbone 1x1</cell><cell>1792x12x12</cell><cell>Global Avg. Pooling</cell><cell>1794x1</cell><cell>Fully Connected Layer</cell><cell>Batch Norm</cell><cell>Dropout p=0.5 SiLU AcÔøΩvaÔøΩon</cell><cell>1572x1</cell><cell>Fully Connected Layer</cell><cell>1572x1</cell><cell>Snake species predictions</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="15,88.99,107.32,418.65,534.52"><head>Table 2</head><label>2</label><figDesc>Macro-averaging ùêπ 1 scores achieved for the private (ùêπ ùëùùëüùëñùë£ Model 25 was trained without mixed precision. ùëê Model 29 was trained using cross-entropy with label smoothing. ùëë Model 41 was trained without mixed precision.</figDesc><table coords="15,315.63,116.31,190.35,13.56"><row><cell>1</cell><cell>) and public (ùêπ ùëùùë¢ùëè 1 ) test data set. The best</cell></row></table><note coords="15,92.58,622.79,1.92,3.36"><p>ùëè </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="16,88.99,382.76,418.52,139.08"><head>Table 3</head><label>3</label><figDesc>Comparison of the improvement of ùêπ ùëùùëüùëñùë£ 1 and ùêπ ùëùùë¢ùëè 1 without the inclusion of the country distribution, in terms of the influence of object detection before classification. The comparison is based on ùêπ ùëùùëüùëñùë£</figDesc><table coords="16,489.03,412.25,3.97,6.12"><row><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="16,88.99,541.74,418.09,138.89"><head>Table 4</head><label>4</label><figDesc>Comparison of the improvement of ùêπ ùëùùëüùëñùë£ Nevertheless, the training of the classification model and the generation of the snake class predictions remained consistent throughout the object detection experiments.</figDesc><table coords="16,89.29,550.73,417.79,129.90"><row><cell cols="5">1 in terms of the influence of object detection before classification. The comparison is based on ùêπ ùëùùëüùëñùë£ and ùêπ ùëùùë¢ùëè with the multiplication of regional prior probabilities, 1 and 1 ùêπ ùëùùë¢ùëè 1 of the first model (ID 2), which did not use object detection. The best results are highlighted in</cell></row><row><cell cols="3">bold. Abbreviations: OD: object detection.</cell><cell></cell><cell></cell></row><row><cell>ID</cell><cell>OD</cell><cell>model</cell><cell>Improvement ùêπ ùëùùë¢ùëè 1</cell><cell>Improvement ùêπ ùëùùëüùëñùë£ 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(in %)</cell><cell>(in %)</cell></row><row><cell>4</cell><cell>YOLOv5l_basic</cell><cell>YOLOv5l</cell><cell>+ 5.97</cell><cell>+ 6.27</cell></row><row><cell>8</cell><cell>YOLOv5l_adv</cell><cell>YOLOv5l</cell><cell>+ 6.18</cell><cell>+ 5.85</cell></row><row><cell cols="2">20 YOLOv5x6_adv</cell><cell>YOLOv5x6</cell><cell>+ 5.71</cell><cell>+ 6.88</cell></row><row><cell>18</cell><cell>Mask-RCNN</cell><cell>Mask-RCNN</cell><cell>+ 2.15</cell><cell>+ 4.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="18,88.98,107.32,418.09,305.62"><head>Table 5</head><label>5</label><figDesc>Ablation study to compare the image augmentation pipelines. The hyperparameters not included in this table are equals for all models. The best results are highlighted in bold. Abbreviations: concat.: concatenation, Multip.: Multiplication.</figDesc><table coords="18,101.78,157.57,391.71,255.37"><row><cell cols="3">ID Epochs Multip. metadata</cell><cell>Augment.</cell><cell>ùêπ ùëùùë¢ùëè 1</cell><cell>ùêπ ùëùùëüùëñùë£ 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(in %)</cell><cell>(in %)</cell></row><row><cell>1</cell><cell>30</cell><cell>-</cell><cell>basic</cell><cell cols="2">49.627 45.479</cell></row><row><cell>5</cell><cell>30</cell><cell>-</cell><cell>Rand</cell><cell>51.758</cell><cell>45.299</cell></row><row><cell>2</cell><cell>30</cell><cell>yes</cell><cell>basic</cell><cell>67.126</cell><cell>61.879</cell></row><row><cell>6</cell><cell>30</cell><cell>yes</cell><cell>Rand</cell><cell cols="2">67.509 62.000</cell></row><row><cell></cell><cell></cell><cell cols="2">Post competition experiments</cell><cell></cell><cell></cell></row><row><cell>42</cell><cell>45</cell><cell>-</cell><cell>base</cell><cell>53.014</cell><cell>48.710</cell></row><row><cell>43</cell><cell>45</cell><cell>-</cell><cell>base + Rand</cell><cell>55.104</cell><cell>49.675</cell></row><row><cell>44</cell><cell>45</cell><cell>-</cell><cell>base + RandEra</cell><cell>53.138</cell><cell>49.756</cell></row><row><cell>45</cell><cell>45</cell><cell>-</cell><cell>base + Rand + RandEra</cell><cell>54.991</cell><cell>49.923</cell></row><row><cell>46</cell><cell>45</cell><cell>-</cell><cell>base + CutMix</cell><cell>55.527</cell><cell>50.743</cell></row><row><cell>47</cell><cell>45</cell><cell>-</cell><cell>base + Rand + CutMix</cell><cell cols="2">57.099 52.733</cell></row><row><cell>48</cell><cell>45</cell><cell>-</cell><cell>base + Rand + RandEra + CutMix</cell><cell>54.873</cell><cell>50.442</cell></row><row><cell>49</cell><cell>45</cell><cell>binary code</cell><cell>base</cell><cell>65.780</cell><cell>61.595</cell></row><row><cell>50</cell><cell>45</cell><cell>binary code</cell><cell>base + Rand</cell><cell>67.738</cell><cell>62.411</cell></row><row><cell>51</cell><cell>45</cell><cell>binary code</cell><cell>base + RandEra</cell><cell>66.010</cell><cell>61.334</cell></row><row><cell>52</cell><cell>45</cell><cell>binary code</cell><cell>base + Rand + RandEra</cell><cell>65.819</cell><cell>61.825</cell></row><row><cell>53</cell><cell>45</cell><cell>binary code</cell><cell>base + CutMix</cell><cell>68.016</cell><cell>64.004</cell></row><row><cell>54</cell><cell>45</cell><cell>binary code</cell><cell>base + Rand + CutMix</cell><cell cols="2">69.777 64.191</cell></row><row><cell>55</cell><cell>45</cell><cell>binary code</cell><cell>base + Rand + RandEra + CutMix</cell><cell>67.886</cell><cell>63.516</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="20,88.99,107.32,418.84,302.07"><head>Table 7</head><label>7</label><figDesc>Comparison of EfficientNet-B4 and ConvNeXt-L model architectures with geospatial feature concatenation. For all models, macro-averaging ùêπ 1 scores achieved for the private (ùêπ ùëùùëüùëñùë£ B4 or ConvNeXt-L as the CNN backbone. The results of the EfficientNet-B4 and ConvNeXt-L model architectures with geospatial feature concatenation are summarized in Table7.To evaluate the influence of geospatial feature concatenation for the EfficientNet-B4 model, the experiment with ID 11 is relevant as well as the experiment with ID 3 without geospatial feature concatenation as reference. Without geospatial feature concatenation, the EfficientNet-B4 achieved an ùêπ ùëùùë¢ùëè 1 of 57.937 % and an ùêπ ùëùùëüùëñùë£ 1 of 50.990 %. With geospatial feature concatenation the EfficientNet-B4 (ID 37: 58.780 % ùêπ ùëùùë¢ùëè</figDesc><table coords="20,393.97,129.28,112.02,13.56"><row><cell>1</cell><cell>) and public (ùêπ ùëùùë¢ùëè 1 ) test</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="22,88.99,107.32,417.17,186.67"><head>Table 8</head><label>8</label><figDesc>Comparison of optimizers and LR schedulers. For all models, macro-averaging ùêπ 1 scores achieved for the private (ùêπ ùëùùëüùëñùë£ 1 ) and public (ùêπ ùëùùë¢ùëè 1 ) test data set are given. The hyperparameters not included in this table are equals for all models. The best results are highlighted in bold. Abbreviations: LR: Learning rate, Multip.: Multiplication.</figDesc><table coords="22,121.35,170.53,352.58,123.46"><row><cell cols="3">ID Model Optimizer (LR)</cell><cell>Scheduler</cell><cell>Multip. metadata.</cell><cell>ùêπ ùëùùë¢ùëè 1</cell><cell>ùêπ ùëùùëüùëñùë£ 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(in %)</cell><cell>(in %)</cell></row><row><cell>9</cell><cell>E-B4</cell><cell>SGD (10 -1 )</cell><cell>CAWR (5,1)</cell><cell>-</cell><cell>49.489</cell><cell>45.554</cell></row><row><cell>38</cell><cell>E-B4</cell><cell>SGD (10 -1 )</cell><cell>-</cell><cell>-</cell><cell cols="2">54.261 48.276</cell></row><row><cell>10</cell><cell>E-B4</cell><cell>SGD (10 -1 )</cell><cell>CAWR (5,1)</cell><cell>binary</cell><cell cols="2">66.937 64.579</cell></row><row><cell>39</cell><cell>E-B4</cell><cell>SGD (10 -1 )</cell><cell>-</cell><cell>binary</cell><cell>68.530</cell><cell>64.567</cell></row><row><cell cols="2">27 E-v2-M</cell><cell>SGD (10 -1 )</cell><cell>CAWR (5,2)</cell><cell>binary</cell><cell cols="2">74.251 69.492</cell></row><row><cell cols="2">40 E-v2-M</cell><cell>SGD (10 -1 )</cell><cell>-</cell><cell>binary</cell><cell>71.372</cell><cell>66.670</cell></row><row><cell>25</cell><cell>E-B4</cell><cell>SAM (10 -1 )</cell><cell>-</cell><cell>binary</cell><cell cols="2">68.520 64.938</cell></row><row><cell>41</cell><cell>E-B4</cell><cell>SGD (10 -1 )</cell><cell>-</cell><cell>binary</cell><cell>64.048</cell><cell>61.439</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="22,88.75,352.47,418.44,83.42"><head></head><label></label><figDesc>Table 10 are relevant for this evaluation, which represent the classification results of the same model with different inference conditions. The experiment with ID 14 represents the classification results influenced by TTA, which achieved an ùêπ ùëùùë¢ùëè Comparing the result with the experiment of ID 12 without TTA (ùêπ ùëùùë¢ùëè 1 : 67.209 % and ùêπ ùëùùëüùëñùë£</figDesc><table /><note coords="22,180.28,398.98,4.23,6.99;22,197.81,393.12,95.65,10.91;22,294.97,390.55,15.33,6.99;22,293.46,398.98,4.23,6.99;22,313.81,393.12,55.94,10.91;22,345.99,413.94,4.23,6.99;22,363.62,408.08,142.36,10.91;22,88.75,423.05,200.44,10.91;22,290.70,420.48,12.79,6.99;22,289.18,428.91,4.23,6.99;22,303.98,423.05,162.78,10.91;22,468.28,420.48,15.33,6.99;22,466.76,428.91,4.23,6.99"><p>1 of 68.781 % and an ùêπ ùëùùëüùëñùë£ 1 of 65.986 %. 1 : 63.666 %), an improvement of 1.572 % points (2.339 %) was achieved for ùêπ ùëùùë¢ùëè 1 , and 2.320 % points (3.644 %) for ùêπ ùëùùëüùëñùë£ 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="22,89.29,632.79,416.90,56.69"><head></head><label></label><figDesc>333 %) for the ùêπ ùëùùë¢ùëè</figDesc><table coords="22,89.29,635.36,416.90,54.12"><row><cell>score and 15.287 % points (31.569 %) for score. The highest improvement for the ùêπ ùëùùë¢ùëè 1 score was achieved for the comparison 1 between model 1 and model 2 (17.499 % points, 35.261 %). For the ùêπ ùëùùëüùëñùë£ the ùêπ ùëùùëüùëñùë£ 1 1 score, the highest</cell></row><row><cell>improvement of 16.701 % points (36.868 %) was reached for the comparison of model 5 and</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="23,88.97,107.32,417.02,237.57"><head>Table 9</head><label>9</label><figDesc>Ablation study to identify the effect of the multiplication with regional prior probabilities. The best results are highlighted in bold. Abbreviations: Multip.: Multiplication. Only one comparison was performed to compare the multiplication with binarized regional prior probabilities to the model probabilities. The comparison between model 9 and model 10 shows an improvement of 17.448 % points (35.256 %) for the ùêπ ùëùùë¢ùëè 1 score and 19.025 % points (41.764 %) for the ùêπ ùëùùëüùëñùë£</figDesc><table coords="23,89.29,146.79,411.98,142.76"><row><cell cols="3">First comparison model</cell><cell></cell><cell></cell><cell cols="2">Second comparison model</cell><cell></cell><cell></cell><cell cols="2">Improvement</cell></row><row><cell cols="2">ID Multip. metadata</cell><cell>ùêπ ùëùùë¢ùëè 1</cell><cell>ùêπ ùëùùëüùëñùë£ 1</cell><cell cols="2">ID Multip. metadata</cell><cell>ùêπ ùëùùë¢ùëè 1</cell><cell>ùêπ ùëùùëüùëñùë£ 1</cell><cell>ùêπ ùëùùë¢ùëè 1</cell><cell>ùêπ ùëùùë¢ùëè 1</cell><cell>ùêπ ùëùùëüùëñùë£ 1</cell><cell>ùêπ ùëùùëüùëñùë£ 1</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell></cell><cell></cell><cell>(%)</cell><cell cols="2">(%) (% points)</cell><cell cols="2">(%) (% points)</cell><cell>(%)</cell></row><row><cell>1</cell><cell>-</cell><cell>49.627</cell><cell>45.479</cell><cell>2</cell><cell>yes</cell><cell>67.126</cell><cell>61.879</cell><cell cols="2">17.499 35.261</cell><cell>16.400</cell><cell>36.061</cell></row><row><cell>5</cell><cell>-</cell><cell>51.758</cell><cell>45.299</cell><cell>6</cell><cell>yes</cell><cell>67.509</cell><cell>62.000</cell><cell>15.751</cell><cell>30.432</cell><cell cols="2">16.701 36.868</cell></row><row><cell>7</cell><cell>-</cell><cell cols="2">58.780 51.773</cell><cell>8</cell><cell>yes</cell><cell>71.277</cell><cell>65.497</cell><cell>12.497</cell><cell>21.261</cell><cell>13.724</cell><cell>26.508</cell></row><row><cell>15</cell><cell>-</cell><cell>54.675</cell><cell cols="2">49.500 16</cell><cell>yes</cell><cell>68.947</cell><cell>63.953</cell><cell>14.272</cell><cell>26.103</cell><cell>14.453</cell><cell>29.198</cell></row><row><cell>19</cell><cell>-</cell><cell>55.251</cell><cell cols="2">48.827 20</cell><cell>yes</cell><cell>68.567</cell><cell>64.652</cell><cell>13.316</cell><cell>24.101</cell><cell>15.825</cell><cell>32.410</cell></row><row><cell>21</cell><cell>-</cell><cell>55.944</cell><cell cols="2">51.521 22</cell><cell>yes</cell><cell cols="2">70.958 66.137</cell><cell>15.014</cell><cell>26.838</cell><cell>14.616</cell><cell>28.369</cell></row><row><cell>9</cell><cell>-</cell><cell>49.489</cell><cell cols="2">45.554 10</cell><cell>binary</cell><cell>66.937</cell><cell>64.579</cell><cell>17.448</cell><cell>35.256</cell><cell>19.025</cell><cell>41.764</cell></row><row><cell>12</cell><cell>yes</cell><cell>67.209</cell><cell cols="2">63.666 13</cell><cell>binary</cell><cell>67.502</cell><cell>65.083</cell><cell>0.293</cell><cell>0.435</cell><cell>1.417</cell><cell>2.226</cell></row><row><cell>32</cell><cell>binary</cell><cell>75.426</cell><cell cols="2">70.798 34</cell><cell>binary code</cell><cell>78.085</cell><cell>73.900</cell><cell>2.659</cell><cell>3.525</cell><cell>3.102</cell><cell>4.381</cell></row><row><cell>model 6.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,108.93,672.66,397.50,8.97;2,89.29,683.62,125.09,8.97"><p>Participation of FHDO BCSG at SnakeCLEF 2022: https://github.com/DiffPro-ML/SnakeCLEF_2022_FHDO_ BCSG [Last accessed: 2022-06-30].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="3,108.93,661.69,398.57,8.97;3,89.29,672.65,13.50,8.97"><p>Google Image Search: https://images.google.com/imghp?hl=de&amp;gl=de&amp;gws_rd=ssl, [Last accessed: 2022-06-30].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="3,105.65,682.04,2.78,5.98;3,108.93,683.61,298.39,8.97"><p><ref type="bibr" coords="3,105.65,682.04,2.78,5.98" target="#b4">5</ref> Tropical Herping: https://www.tropicalherping.com/, [Last accessed: 2022-06-30].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="4,108.93,683.58,252.29,8.97"><p>iNaturalist: https://www.inaturalist.org/, [Last accessed: 2022-06-30].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="5,108.93,650.75,398.43,8.97;5,89.29,661.71,214.35,8.97"><p>Imageclef.org. 2022. SnakeCLEF 2021 | ImageCLEF / LifeCLEF -Multimedia Retrieval in CLEF. https://www. imageclef.org/SnakeCLEF2021 [Last accessed: 2022-06-30].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="5,108.93,672.66,397.05,8.97;5,89.29,683.62,283.60,8.97"><p>Glenn Jocher et al., 2022. ultralytics/yolov5: v6.1 -TensorRT, TensorFlow Edge TPU and OpenVINO Export and Inference. https://zenodo.org/record/6222936 [Last accessed: 2022-06-30].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6" coords="6,108.93,672.65,398.44,8.97;6,89.29,683.61,214.35,8.97"><p>Imageclef.org. 2022. SnakeCLEF2022 | ImageCLEF / LifeCLEF -Multimedia Retrieval in CLEF. https://www. imageclef.org/SnakeCLEF2022 [Last accessed: 2022-06-30].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7" coords="7,108.93,672.66,363.29,8.97"><p>Pytorch.org. 2022. YOLOv5. https://pytorch.org/hub/ultralytics_yolov5/ [Last accessed 2022-06-30].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_8" coords="12,108.93,661.71,341.45,8.97"><p>pycountry package: https://github.com/flyingcircusio/pycountry, [Last accessed: 2022-06-30].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_9" coords="12,108.93,672.66,284.33,8.97"><p>GeoPy package: https://github.com/geopy/geopy, [Last accessed: 2022-06-30].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_10" coords="12,108.93,683.62,283.87,8.97"><p>OpenStreetMap: https://www.openstreetmap.de/, [Last accessed: 2022-06-30].</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The work of <rs type="person">Louise Bloch</rs> was partially funded by a PhD grant from <rs type="funder">University of Applied Sciences and Arts Dortmund, Dortmund, Germany</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>Ablation study to compare the different classification models. For all models, macro-averaging ùêπ 1 scores achieved for the private (ùêπ ùëùùëüùëñùë£ 1 ) and public (ùêπ ùëùùë¢ùëè 1 ) test data set are given. The hyperparameters not included in this table are equals for the compared models. The best results are highlighted in bold. Abbreviations: E-B4: EfficientNet-B4, E-B5: EfficientNet-B5, E-v2-M: EfficientNet-v2-M, C-NeXt-L: ConvNeXt-L, LR: learning rate, Multip.: Multiplication. score. The EfficientNet-B4 model was also compared to an EfficientNet-v2-M model, as can be seen in Table <ref type="table" coords="19,129.08,406.36,3.81,10.91">6</ref>. The EfficientNet-v2-M model (model 33) outperforms the EfficientNet-B4 model (model 9) by 6.069 % points (9.067 %) for the ùêπ ùëùùë¢ùëè Additional experiments on model architectures were conducted after the challenge to test EfficientNet-B4, EfficientNet-v2-M as well as ConvNeXt-L under more comparable conditions. Using the knowledge from the previous experiments, the pre-trained ImageNet models were trained over 30 epochs on snake images without object detection at the same resolution as for pre-training as well as basic + Rand + Mixup augmentations. The results are shown in Table <ref type="table" coords="19,89.29,514.76,5.07,10.91">6</ref> under post competition experiments. These results reveal once again that EfficientNet-v2-M outperforms the less complex architecture EfficientNet-B4 of about 6.9 % points ùêπ ùëùùë¢ùëè 1 and of about 4.8 % points ùêπ ùëùùëüùëñùë£ 1 . Comparing the EfficientNet-v2-M model with the much more complex ConvNeXt-L architecture, only marginal differences were observed. While the EfficientNet-v2-M model achieved a slightly higher ùêπ ùëùùë¢ùëè 1 of about 1.2 % points, the ConvNeXt-L model achieved a slightly higher ùêπ ùëùùëüùëñùë£ 1 of about 0.6 % points. Multiplication with binary regional (code) prior probabilities improved the results of all model architectures, although the relative proportions of the architectures did not change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Geospatial Feature Concatenation</head><p>As mentioned in Section 4.4, a model architecture that uses geospatial data in addition to image data was tested. More specifically, two model architectures were developed, using EfficientNet- </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="24,112.66,613.98,393.33,10.91;24,112.66,627.53,393.33,10.91;24,112.66,641.08,394.87,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="24,292.93,613.98,213.05,10.91;24,112.66,627.53,172.54,10.91">Overview of SnakeCLEF 2022: Automated snake species identification on a global scale</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hr√∫z</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,309.75,627.53,196.24,10.91;24,112.66,641.08,185.93,10.91">Working Notes of the 13th Conference and Labs of the Evaluation Forum (CLEF 2022)</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2022" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,654.63,394.53,10.91;24,112.66,668.18,394.53,10.91;24,112.66,681.73,393.33,10.91;25,112.66,103.81,393.33,10.91;25,112.66,117.35,393.33,10.91;25,112.66,130.90,394.52,10.91;25,112.66,144.45,65.44,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="24,193.74,681.73,312.25,10.91;25,112.66,103.81,261.31,10.91">Overview of LifeCLEF 2022: an evaluation of Machine-Learning based Species Identification and Species Distribution Prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,401.17,103.81,104.82,10.91;25,112.66,117.35,393.33,10.91;25,112.66,130.90,213.05,10.91">Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<meeting>the 13th International Conference of the CLEF Association (CLEF 2022)<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2022" to="2029" />
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="25,112.66,158.00,394.53,10.91;25,112.66,171.55,394.53,10.91;25,112.66,185.10,393.33,10.91;25,112.66,198.65,394.53,10.91;25,112.66,212.20,393.33,10.91;25,112.66,225.75,395.17,10.91;25,112.66,239.30,395.01,10.91;25,112.66,252.85,187.11,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="25,195.27,185.10,310.72,10.91;25,112.66,198.65,208.84,10.91">LifeCLEF 2022 teaser: An evaluation of Machine-Learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-99739-7_49</idno>
	</analytic>
	<monogr>
		<title level="m" coord="25,313.94,212.20,192.05,10.91;25,112.66,225.75,305.31,10.91">Proceedings of the European Conference on Information Retrieval (ECIR 2022): Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hagen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Verberne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>N√∏rv√•g</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Setty</surname></persName>
		</editor>
		<meeting>the European Conference on Information Retrieval (ECIR 2022): Advances in Information Retrieval<address><addrLine>Stavanger, Norway; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,266.40,394.53,10.91;25,112.66,279.94,397.48,10.91;25,112.66,295.94,43.94,7.90" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="25,112.66,279.94,100.79,10.91">Snakebite envenoming</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Guti√©rrez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Calvete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Warrell</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrdp.2017.63</idno>
	</analytic>
	<monogr>
		<title level="j" coord="25,224.02,279.94,147.96,10.91">Nature Reviews Disease Primers</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,307.04,394.52,10.91;25,112.33,320.59,394.85,10.91;25,112.33,334.14,226.75,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="25,112.33,320.59,390.47,10.91">The urgent need to develop novel strategies for the diagnosis and treatment of snakebites</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">F</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Layfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vallance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Bicknell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Trim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vaiyapuri</surname></persName>
		</author>
		<idno type="DOI">10.3390/toxins11060363</idno>
	</analytic>
	<monogr>
		<title level="j" coord="25,112.33,334.14,29.72,10.91">Toxins</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,347.69,395.16,10.91;25,112.66,361.24,393.32,10.91;25,112.66,374.79,393.33,10.91;25,112.28,388.34,394.90,10.91;25,112.66,401.89,264.21,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="25,289.49,347.69,218.33,10.91;25,112.66,361.24,393.32,10.91;25,112.66,374.79,117.59,10.91">No more fear of every snake: Applying chatbotbased learning system for snake knowledge promotion improvement: A regional snake knowledge learning system</title>
		<author>
			<persName coords=""><forename type="first">H.-T</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-C</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICALT49669.2020.00029</idno>
	</analytic>
	<monogr>
		<title level="m" coord="25,253.19,374.79,252.80,10.91;25,112.28,388.34,209.52,10.91">Proceedings of the IEEE 20th International Conference on Advanced Learning Technologies (ICALT 2020)</title>
		<meeting>the IEEE 20th International Conference on Advanced Learning Technologies (ICALT 2020)<address><addrLine>Tartu, Estonia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="72" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,415.44,394.53,10.91;25,112.66,428.99,393.32,10.91;25,112.66,442.54,393.33,10.91;25,112.41,456.08,394.77,10.91;25,112.33,469.63,372.49,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="25,246.68,428.99,259.30,10.91;25,112.66,442.54,273.70,10.91">Combination of image and location information for snake species identification using object detection and EfficientNets</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Boketta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Keibel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mense</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Michailutschenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Willemeit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_201.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="25,409.33,442.54,96.66,10.91;25,112.41,456.08,278.59,10.91">Working Notes of the 11th Conference and Labs of the Evaluation Forum (CLEF 2020)</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,483.18,395.17,10.91;25,112.66,496.73,393.33,10.91;25,112.66,510.28,394.52,10.91;25,112.66,523.83,310.80,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="25,225.36,483.18,282.47,10.91;25,112.66,496.73,188.60,10.91">EfficientNets and Vision Transformers for snake species identification using image and location information</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-126.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="25,322.64,496.73,183.35,10.91;25,112.66,510.28,184.84,10.91">Working Notes of the 12th Conference and Labs of the Evaluation Forum (CLEF 2020)</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1477" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,537.38,393.59,10.91;25,112.66,550.93,393.33,10.91;25,112.66,564.48,394.53,10.91;25,112.33,578.03,372.49,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="25,380.27,537.38,125.97,10.91;25,112.66,550.93,252.34,10.91">Overview of the SnakeCLEF 2020: Automatic snake species identification challenge</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Casta√±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M</forename><surname>Sharada</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_258.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="25,395.55,550.93,110.44,10.91;25,112.66,564.48,271.29,10.91">Proceedings of the 11th Conference and Labs of the Evaluation Forum (CLEF 2020)</title>
		<meeting>the 11th Conference and Labs of the Evaluation Forum (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">258</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,591.58,394.61,10.91;25,112.28,605.13,393.71,10.91;25,112.41,618.67,395.42,10.91;25,112.66,632.22,394.67,10.91;25,112.41,645.77,35.59,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="25,368.26,591.58,139.00,10.91;25,112.28,605.13,276.73,10.91">Overview of SnakeCLEF 2021: Automatic snake species identification with country-level focus</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Casta√±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-125.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="25,411.61,605.13,94.38,10.91;25,112.41,618.67,395.42,10.91;25,112.66,632.22,21.81,10.91">Working Notes of the 12th International Conference of the CLEF Association (CLEF 2021): 2021-09-21 -2021-09-24</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1463" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,659.32,394.53,10.91;25,112.66,672.87,393.33,10.91;26,112.66,103.81,222.92,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2201.03545</idno>
		<title level="m" coord="25,391.49,659.32,111.12,10.91;25,379.65,672.87,126.34,10.91;26,112.66,103.81,197.50,10.91">IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>A ConvNet for the 2020s</note>
</biblStruct>

<biblStruct coords="26,112.66,117.35,393.33,10.91;26,112.33,130.90,394.94,10.91;26,112.66,144.45,393.33,10.91;26,112.66,158.00,394.03,10.91;26,112.66,171.55,82.84,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="26,178.19,117.35,221.10,10.91">EfficientNetV2: smaller models and faster training</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/tan21a/tan21a.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="26,141.32,130.90,361.69,10.91;26,403.69,145.47,102.30,9.72;26,112.66,159.02,76.47,9.72">Proceedings of the 38th International Conference on Machine Learning (ICML 2021)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning (ICML 2021)<address><addrLine>PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10096" to="10106" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="26,112.66,185.10,393.32,10.91;26,112.66,198.65,393.33,10.91;26,112.66,212.20,395.01,10.91;26,112.41,225.75,179.18,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="26,322.64,185.10,183.34,10.91;26,112.66,198.65,71.31,10.91">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.91</idno>
	</analytic>
	<monogr>
		<title level="m" coord="26,206.11,198.65,299.88,10.91;26,112.66,212.20,114.71,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)<address><addrLine>Las Vegas, Nevada, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,239.30,393.33,10.91;26,112.66,252.85,394.53,10.91;26,112.30,266.40,395.36,10.91;26,112.66,279.94,143.58,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R-Cnn</forename><surname>Mask</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.322</idno>
		<title level="m" coord="26,395.24,239.30,110.75,10.91;26,112.66,252.85,271.02,10.91">Proceedings of the IEEE International Conference on Computer Vision (ICCV 2017)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV 2017)<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,293.49,393.61,10.91;26,112.66,307.04,393.33,10.91;26,112.66,320.59,395.00,10.91;26,112.41,334.14,269.17,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="26,310.67,293.49,195.60,10.91;26,112.66,307.04,112.27,10.91">Sharpness-aware Minimization for efficiently improving generalization</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=6Tm1mposlrM" />
	</analytic>
	<monogr>
		<title level="m" coord="26,248.15,307.04,257.84,10.91;26,112.66,320.59,124.74,10.91;26,357.87,320.59,103.23,10.91">Proceedings of the International Conference on Learning Representations (ICLR 2021)</title>
		<meeting>the International Conference on Learning Representations (ICLR 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
	<note>Online-only conference</note>
</biblStruct>

<biblStruct coords="26,112.66,347.69,393.33,10.91;26,112.66,361.24,395.16,10.91;26,112.66,374.79,393.33,10.91;26,112.66,388.34,58.17,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="26,221.94,347.69,176.13,10.91">Decoupled weight decay regularization</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m" coord="26,421.66,347.69,84.32,10.91;26,112.66,361.24,395.16,10.91;26,112.66,374.79,8.50,10.91">Proceedings of the International Conference on Learning Representations (ICLR 2019) 2019-05-06 -2019-05-09</title>
		<meeting>the International Conference on Learning Representations (ICLR 2019) 2019-05-06 -2019-05-09<address><addrLine>New Orleans, Louisiana, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,401.89,394.61,10.91;26,112.66,415.44,394.61,10.91;26,112.66,428.99,394.04,10.91;26,112.66,442.54,96.24,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="26,230.71,401.89,251.88,10.91">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Skq89Scxx" />
	</analytic>
	<monogr>
		<title level="m" coord="26,112.66,415.44,390.17,10.91">Proceedings of the International Conference on Learning Representations (ICLR 2017)</title>
		<meeting>the International Conference on Learning Representations (ICLR 2017)<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,456.08,393.33,10.91;26,112.66,469.63,393.58,10.91;26,112.33,483.18,200.73,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="26,311.94,456.08,194.05,10.91;26,112.66,469.63,137.07,10.91">Discriminative histogram taxonomy features for snake species identification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sugathan</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13673-014-0003-0</idno>
	</analytic>
	<monogr>
		<title level="j" coord="26,257.77,469.63,240.64,10.91">Human-Centric Computing and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,496.73,393.71,10.91;26,112.66,510.28,395.01,10.91;26,112.33,523.83,394.94,10.91;26,112.66,537.38,394.53,10.91;26,112.66,550.93,270.41,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="26,282.37,496.73,223.99,10.91;26,112.66,510.28,235.19,10.91">CEDD: Color and Edge Directivity Descriptor: A compact descriptor for image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-79547-6_30</idno>
	</analytic>
	<monogr>
		<title level="m" coord="26,179.94,523.83,322.89,10.91">Proceedings of the International Computer Vision Systems (ICVS 2008)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gasteratos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vincze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</editor>
		<meeting>the International Computer Vision Systems (ICVS 2008)<address><addrLine>Santorini, Greece; Berlin Heidelberg, Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="312" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,564.48,393.33,10.91;26,112.66,578.03,395.17,10.91;26,112.66,591.58,394.61,10.91;26,112.66,605.13,394.52,10.91;26,112.66,618.67,285.98,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="26,336.92,564.48,169.07,10.91;26,112.66,578.03,153.29,10.91">Image classification for snake species using Machine Learning techniques</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A H</forename><surname>Zahri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yaakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Ahmad</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-48517-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="26,489.15,578.03,18.68,10.91;26,112.66,591.58,390.35,10.91">Proceedings of the Computational Intelligence in Information Systems Conference (CIIS 2016)</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Phon-Amnuaisuk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T.-W</forename><surname>Au</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Omar</surname></persName>
		</editor>
		<meeting>the Computational Intelligence in Information Systems Conference (CIIS 2016)<address><addrLine>Brunei; Brunei Darussalam; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,632.22,393.33,10.91;26,112.66,645.77,393.33,10.91;26,111.46,659.32,393.45,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="26,197.03,632.22,234.53,10.91">Snake detection and classification using Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sinnott</surname></persName>
		</author>
		<idno type="DOI">10.24251/hicss.2021.148</idno>
	</analytic>
	<monogr>
		<title level="m" coord="26,452.94,632.22,53.05,10.91;26,112.66,645.77,338.90,10.91;26,120.16,659.32,45.78,10.91">Proceedings of the 54th Hawaii International Conference on System Sciences (HICSS 2021)</title>
		<meeting>the 54th Hawaii International Conference on System Sciences (HICSS 2021)<address><addrLine>Maui, Hawaii, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1212" to="1221" />
		</imprint>
	</monogr>
	<note>2021-01-08</note>
</biblStruct>

<biblStruct coords="26,112.66,672.87,393.33,10.91;27,112.66,103.81,394.53,10.91;27,112.28,117.35,234.36,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="26,461.50,672.87,44.49,10.91;27,112.66,103.81,389.73,10.91">Revealing the unknown: Real-time recognition of Gal√°pagos snake species using Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Khatod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Matijosaitiene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Gilkey</surname></persName>
		</author>
		<idno type="DOI">10.3390/ani10050806</idno>
	</analytic>
	<monogr>
		<title level="j" coord="27,112.28,117.35,37.19,10.91">Animals</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">806</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,130.90,393.33,10.91;27,112.66,144.45,393.33,10.91;27,112.66,158.00,397.48,10.91;27,112.66,173.99,73.61,7.90" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="27,367.69,130.90,138.30,10.91;27,112.66,144.45,50.14,10.91">Snake species identification and recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vasmatkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Zare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kumbla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pimpalkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="DOI">10.1109/IBSSC51096.2020.9332218</idno>
	</analytic>
	<monogr>
		<title level="m" coord="27,186.15,144.45,285.95,10.91">Proceedings of the IEEE Bombay Section Signature Conference</title>
		<meeting>the IEEE Bombay Section Signature Conference<address><addrLine>IBSSC; Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,185.10,394.52,10.91;27,112.66,198.65,393.33,10.91;27,112.33,212.20,393.65,10.91;27,112.66,225.75,363.70,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="27,279.77,185.10,222.55,10.91">Snake image classification using Siamese networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Abeysinghe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Welivita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Perera</surname></persName>
		</author>
		<idno type="DOI">10.1145/3338472.3338476</idno>
	</analytic>
	<monogr>
		<title level="m" coord="27,127.57,198.65,378.42,10.91;27,112.33,212.20,54.73,10.91">Proceedings of the 3rd International Conference on Graphics and Signal Processing (ICGSP 2019)</title>
		<meeting>the 3rd International Conference on Graphics and Signal Processing (ICGSP 2019)<address><addrLine>Hong Kong, Hong Kong; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,239.30,393.33,10.91;27,112.66,252.85,393.33,10.91;27,112.66,266.40,394.52,10.91;27,112.66,279.94,394.53,10.91;27,112.66,293.49,287.08,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="27,310.02,239.30,195.96,10.91;27,112.66,252.85,166.44,10.91">Image-based classification of snake species using Convolutional Neural Network</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">S</forename><surname>Abdurrazaq</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Suyanto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Utama</surname></persName>
		</author>
		<idno type="DOI">10.1109/isriti48646.2019.9034633</idno>
	</analytic>
	<monogr>
		<title level="m" coord="27,304.32,252.85,201.67,10.91;27,112.66,266.40,330.00,10.91">Proceedings of the International Seminar on Research of Information Technology and Intelligent Systems (ISRITI 2019)</title>
		<meeting>the International Seminar on Research of Information Technology and Intelligent Systems (ISRITI 2019)<address><addrLine>Yogyakarta, Indonesia</addrLine></address></meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,307.04,393.32,10.91;27,112.66,320.59,393.33,10.91;27,112.66,334.14,393.33,10.91;27,112.66,347.69,397.48,10.91;27,112.66,363.68,43.94,7.90" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="27,341.73,307.04,164.25,10.91;27,112.66,320.59,65.21,10.91">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m" coord="27,200.77,320.59,305.21,10.91;27,112.66,334.14,109.45,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2009)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2009)<address><addrLine>Miami Beach, Florida, US</addrLine></address></meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,374.79,393.32,10.91;27,112.66,388.34,393.33,10.91;27,112.66,401.89,267.33,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="27,261.44,374.79,244.54,10.91;27,112.66,388.34,108.39,10.91">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2016.2577031</idno>
	</analytic>
	<monogr>
		<title level="j" coord="27,228.73,388.34,277.26,10.91">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,415.44,395.16,10.91;27,112.66,428.99,394.61,10.91;27,112.66,442.54,393.33,10.91;27,112.66,456.08,301.22,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="27,259.80,415.44,203.34,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="27,488.38,415.44,19.44,10.91;27,112.66,428.99,390.35,10.91">Proceesings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)</title>
		<meeting>eesings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)<address><addrLine>Las Vegas, Nevada, US</addrLine></address></meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,469.63,393.33,10.91;27,112.66,483.18,393.33,10.91;27,112.66,496.73,395.17,10.91;27,112.66,510.28,317.89,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="27,247.99,469.63,258.00,10.91;27,112.66,483.18,50.14,10.91">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
	</analytic>
	<monogr>
		<title level="m" coord="27,316.83,483.18,189.15,10.91;27,112.66,496.73,291.76,10.91">Conference Track Proceedings of the 3rd International Conference on Learning Representations (ICLR 2015)</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, California, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2015" to="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,523.83,393.33,10.91;27,112.66,537.38,393.33,10.91;27,112.66,550.93,395.01,10.91;27,112.66,564.48,143.58,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="27,359.69,523.83,146.30,10.91;27,112.66,537.38,39.98,10.91">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="27,182.56,537.38,323.43,10.91;27,112.66,550.93,107.75,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,578.03,393.33,10.91;27,112.66,591.58,393.53,10.91;27,112.30,605.13,394.88,10.91;27,112.66,618.67,266.52,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="27,399.80,578.03,106.19,10.91;27,112.66,591.58,141.79,10.91">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00474</idno>
	</analytic>
	<monogr>
		<title level="m" coord="27,278.69,591.58,227.50,10.91;27,112.30,605.13,193.13,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2018)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2018)<address><addrLine>Salt Lake City, Utah, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,632.22,395.16,10.91;27,112.66,645.77,394.61,10.91;27,112.66,659.32,397.48,10.91;27,112.36,675.31,43.94,7.90" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="27,181.03,632.22,283.00,10.91">GrabCut color image segmentation based on region of interest</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CISP.2014.7003812</idno>
	</analytic>
	<monogr>
		<title level="m" coord="27,488.38,632.22,19.44,10.91;27,112.66,645.77,390.24,10.91">Proceedings of the 7th International Congress on Image and Signal Processing (ICISP 2014)</title>
		<meeting>the 7th International Congress on Image and Signal Processing (ICISP 2014)<address><addrLine>Cherburg, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="392" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,103.81,393.33,10.91;28,112.66,117.35,393.33,10.91;28,112.66,130.90,394.53,10.91;28,112.66,144.45,394.03,10.91;28,112.66,158.00,224.56,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="28,341.94,103.81,164.05,10.91;28,112.66,117.35,119.27,10.91">Signature verification using a Siamese time delay neural network</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>S√§ckinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/1993/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="28,449.66,117.35,56.33,10.91;28,112.66,130.90,236.09,10.91">Advances in Neural Information Processing Systems (NIPS 1993)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Cowan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Alspector</surname></persName>
		</editor>
		<meeting><address><addrLine>Denver, Colorado, US</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,171.55,395.17,10.91;28,112.66,185.10,393.32,10.91;28,112.66,198.65,395.00,10.91;28,112.66,212.20,276.83,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="28,277.31,171.55,230.52,10.91;28,112.66,185.10,25.23,10.91">Siamese Neural Networks for one-shot image recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="28,160.76,185.10,345.22,10.91;28,112.66,198.65,149.47,10.91">Proceedings of the Deep Learning workshop of the International Conference on Machine Learning (ICML 2015)</title>
		<meeting>the Deep Learning workshop of the International Conference on Machine Learning (ICML 2015)<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2015" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,225.75,394.53,10.91;28,112.66,239.30,393.33,10.91;28,112.66,252.85,393.33,10.91;28,112.66,266.40,268.68,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="28,112.66,239.30,393.33,10.91;28,112.66,252.85,281.21,10.91">Supervised learning computer vision benchmark for snake species identification from photographs: Implications for herpetology and global health</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Salath√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Casta√±eda</surname></persName>
		</author>
		<idno type="DOI">10.3389/frai.2021.582110</idno>
	</analytic>
	<monogr>
		<title level="j" coord="28,407.34,252.85,98.64,10.91;28,112.66,266.40,51.98,10.91">Frontiers in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,279.94,394.61,10.91;28,112.66,293.49,395.17,10.91;28,112.66,307.04,394.03,10.91;28,112.66,320.59,66.21,10.91" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="28,195.64,279.94,286.96,10.91">Impact of pretrained networks for snake species classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_194.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="28,112.66,293.49,363.88,10.91">Proceedings of the 11th Conference and Labs of the Evaluation Forum (CLEF 2020)</title>
		<meeting>the 11th Conference and Labs of the Evaluation Forum (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">194</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,334.14,393.33,10.91;28,112.66,347.69,393.33,10.91;28,112.33,361.24,395.50,10.91;28,112.66,374.79,375.74,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="28,352.11,334.14,153.88,10.91;28,112.66,347.69,28.26,10.91">ImageNet-21K pretraining for the masses</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Zkj_VcZ6ol" />
	</analytic>
	<monogr>
		<title level="m" coord="28,163.39,347.69,342.60,10.91;28,112.33,361.24,247.57,10.91;28,474.63,361.24,33.21,10.91;28,112.66,374.79,71.10,10.91">Proceedings of the 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Datasets and Benchmarks Track (Round 1)</title>
		<meeting>the 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Datasets and Benchmarks Track (Round 1)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Onlineonly Conference</note>
</biblStruct>

<biblStruct coords="28,112.66,388.34,394.52,10.91;28,112.66,401.89,393.33,10.91;28,112.66,415.44,394.53,10.91;28,112.39,428.99,371.49,10.91" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="28,178.09,388.34,324.20,10.91">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/tan19a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="28,293.89,401.89,212.10,10.91;28,112.66,415.44,151.51,10.91;28,322.62,415.44,54.73,10.91">Proceedings of the 36th International Conference on Machine Learning (ICML 2019)</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning (ICML 2019)<address><addrLine>Long Beach, California, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
	<note>-2019-06-15</note>
</biblStruct>

<biblStruct coords="28,112.66,442.54,393.33,10.91;28,112.66,456.08,393.33,10.91;28,112.66,469.63,395.01,10.91;28,112.66,483.18,217.35,10.91" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="28,202.64,442.54,303.34,10.91;28,112.66,456.08,88.24,10.91">Incorporation of object detection models and location data into snake species classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Borsodi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Papp</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-127.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="28,222.23,456.08,283.76,10.91;28,112.66,469.63,199.82,10.91">Working Notes of the 12th Conference and Labs of the Evaluation Forum (CLEF 2021): 2021-09-21 -2021-09-24</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1499" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,496.73,394.61,10.91;28,112.66,510.28,395.16,10.91;28,112.66,523.83,394.53,10.91;28,112.41,537.38,394.11,10.91;28,112.66,550.93,290.74,10.91" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="28,245.07,496.73,236.65,10.91">EfficientDet: Scalable and efficient object detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="28,112.66,510.28,395.16,10.91;28,112.66,523.83,309.77,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR 2020): 2020-06-14 -2020-06-19</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition (CVPR 2020): 2020-06-14 -2020-06-19</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10781" to="10790" />
		</imprint>
	</monogr>
	<note>Online-only conference</note>
</biblStruct>

<biblStruct coords="28,112.66,564.48,393.33,10.91;28,112.66,578.03,393.33,10.91;28,112.66,591.58,395.01,10.91;28,112.66,605.13,217.35,10.91" xml:id="b40">
	<analytic>
		<title level="a" type="main" coord="28,302.26,564.48,203.73,10.91;28,112.66,578.03,71.13,10.91">A deep learning method for visual recognition of snake species</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-128.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="28,207.06,578.03,298.92,10.91;28,112.66,591.58,199.82,10.91">Working Notes of the 12th Conference and Labs of the Evaluation Forum (CLEF 2021): 2021-09-21 -2021-09-24</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1512" to="1525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,618.67,394.53,10.91;28,112.66,632.22,394.62,10.91;28,112.66,645.77,144.07,10.91" xml:id="b41">
	<monogr>
		<title level="m" type="main" coord="28,196.33,632.22,150.02,10.91">ResNeSt: Split-attention networks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno>CoRR abs/2004.08955</idno>
		<ptr target="https://arxiv.org/abs/2004.08955" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,659.32,393.32,10.91;28,112.66,672.87,393.33,10.91;29,112.66,103.81,395.01,10.91;29,112.66,117.35,143.58,10.91" xml:id="b42">
	<analytic>
		<title level="a" type="main" coord="28,301.73,659.32,204.25,10.91;28,112.66,672.87,68.94,10.91">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.634</idno>
	</analytic>
	<monogr>
		<title level="m" coord="28,204.45,672.87,301.54,10.91;29,112.66,103.81,107.67,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,130.90,395.16,10.91;29,112.66,144.45,393.33,10.91;29,112.26,158.00,393.72,10.91;29,112.66,171.55,395.17,10.91;29,112.66,185.10,378.80,10.91" xml:id="b43">
	<analytic>
		<title level="a" type="main" coord="29,399.68,144.45,106.31,10.91;29,112.26,158.00,218.31,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m" coord="29,351.27,158.00,154.71,10.91;29,112.66,171.55,238.73,10.91;29,473.33,171.55,34.49,10.91;29,112.66,185.10,68.94,10.91">Proceedings of the 9th International Conference on Learning Representations (ICLR 2021)</title>
		<meeting>the 9th International Conference on Learning Representations (ICLR 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
	<note>Onlineonly conference</note>
</biblStruct>

<biblStruct coords="29,112.66,198.65,393.32,10.91;29,112.28,212.20,159.11,10.91" xml:id="b44">
	<monogr>
		<title level="m" type="main" coord="29,250.13,198.65,67.52,10.91">Python tutorial</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Van Rossum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">L</forename><surname>Drake</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>The Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Centrum voor Wiskunde en Informatica Amsterdam</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,225.75,395.16,10.91;29,112.66,239.30,394.53,10.91;29,112.66,252.85,394.67,10.91;29,112.66,266.40,395.17,10.91;29,112.66,279.94,16.43,10.91" xml:id="b45">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Gokula</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Krishnan</surname></persName>
		</author>
		<ptr target="https://medium.com/@Stormblessed/diving-into-deep-learning-part-3-a-deep-learning-practitioners-attempt-to-build-state-of-the-2460292bcfb" />
		<title level="m" coord="29,245.89,225.75,261.94,10.91;29,112.66,239.30,390.74,10.91">Diving into deep learning -Part 3 -A Deep Learning practitioner&apos;s attempt to build state of the art snake-species image classifier</title>
		<imprint>
			<date type="published" when="2019">2019. 2022-05-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,293.49,393.33,10.91;29,112.66,307.04,393.33,10.91;29,112.66,320.59,394.52,10.91;29,112.66,334.14,319.17,10.91" xml:id="b46">
	<analytic>
		<title level="a" type="main" coord="29,255.37,293.49,250.62,10.91;29,112.66,307.04,126.79,10.91">Best practices for Convolutional Neural Networks applied to visual document analysis</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2003.1227801</idno>
	</analytic>
	<monogr>
		<title level="m" coord="29,267.86,307.04,238.12,10.91;29,112.66,320.59,225.18,10.91">Proceedings of the 7th International Conference on Document Analysis and Recognition (ICDAR 2003)</title>
		<meeting>the 7th International Conference on Document Analysis and Recognition (ICDAR 2003)<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="958" to="963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,347.69,393.33,10.91;29,112.66,361.24,393.58,10.91;29,111.46,374.79,383.28,10.91" xml:id="b47">
	<analytic>
		<title level="a" type="main" coord="29,263.04,347.69,157.71,10.91">Learning non-maximum suppression</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.685</idno>
	</analytic>
	<monogr>
		<title level="m" coord="29,442.19,347.69,63.80,10.91;29,112.66,361.24,339.49,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6469" to="6477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,388.34,393.33,10.91;29,112.66,401.89,393.32,10.91;29,112.33,415.44,395.33,10.91;29,112.66,428.99,155.45,10.91" xml:id="b48">
	<analytic>
		<title level="a" type="main" coord="29,320.94,388.34,185.05,10.91;29,112.66,401.89,74.68,10.91">Semi-Supervised Self-Training of Object Detection Models</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schneiderman</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACVMOT.2005.107</idno>
	</analytic>
	<monogr>
		<title level="m" coord="29,233.34,401.89,272.65,10.91;29,112.33,415.44,93.47,10.91">Seventh IEEE Workshops on Applications of Computer Vision (WACV/MOTION&apos;05)</title>
		<meeting><address><addrLine>Breckenridge, CO, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,442.54,394.53,10.91;29,112.33,456.08,240.22,10.91" xml:id="b49">
	<analytic>
		<title level="a" type="main" coord="29,217.72,442.54,284.94,10.91">Stochastic estimation of the maximum of a regression function</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wolfowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="29,112.33,456.08,169.16,10.91">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="page" from="462" to="466" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,469.63,394.53,10.91;29,112.66,483.18,394.52,10.91;29,112.66,496.73,394.53,10.91;29,112.66,510.28,394.53,10.91;29,112.66,523.83,395.17,10.91;29,112.66,537.38,394.53,10.91;29,112.66,550.93,394.67,10.91;29,112.66,564.48,226.69,10.91" xml:id="b50">
	<analytic>
		<title level="a" type="main" coord="29,369.79,496.73,137.40,10.91;29,112.66,510.28,178.35,10.91">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="29,292.30,523.83,215.52,10.91;29,112.66,537.38,85.14,10.91">Advances in Neural Information Processing Systems (Neurips 2019)</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch√©-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,578.03,395.17,10.91;29,112.66,591.58,394.52,10.91;29,112.66,605.13,378.97,10.91" xml:id="b51">
	<analytic>
		<title level="a" type="main" coord="29,305.97,578.03,154.26,10.91">Random erasing data augmentation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i07.7000</idno>
	</analytic>
	<monogr>
		<title level="m" coord="29,469.02,578.03,38.81,10.91;29,112.66,591.58,278.40,10.91">Proceedings of the 34 Conference on Artificial Intelligence (AAAI 2020)</title>
		<meeting>the 34 Conference on Artificial Intelligence (AAAI 2020)<address><addrLine>New York, New York, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,618.67,346.15,10.91" xml:id="b52">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4414861</idno>
	</analytic>
	<monogr>
		<title level="j" coord="29,176.94,618.67,99.26,10.91">PyTorch Image Models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,632.22,395.16,10.91;29,112.66,645.77,393.32,10.91;29,112.66,659.32,395.17,10.91;29,112.66,672.87,360.80,10.91" xml:id="b53">
	<analytic>
		<title level="a" type="main" coord="29,300.35,632.22,207.47,10.91;29,112.66,645.77,172.80,10.91">RandAugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW50498.2020.00359</idno>
	</analytic>
	<monogr>
		<title level="m" coord="29,308.67,645.77,197.31,10.91;29,112.66,659.32,239.75,10.91;29,473.58,659.32,34.26,10.91;29,112.66,672.87,68.94,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR 2020)</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition (CVPR 2020)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3008" to="3017" />
		</imprint>
	</monogr>
	<note>Onlineonly conference</note>
</biblStruct>

<biblStruct coords="30,112.66,103.81,395.17,10.91;30,112.66,117.35,393.33,10.91;30,112.66,130.90,394.52,10.91;30,112.66,144.45,395.00,10.91" xml:id="b54">
	<analytic>
		<title level="a" type="main" coord="30,352.23,103.81,155.60,10.91;30,112.66,117.35,104.63,10.91">AutoAugment: Learning augmentation strategies from data</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Man√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00020</idno>
	</analytic>
	<monogr>
		<title level="m" coord="30,239.41,117.35,266.58,10.91;30,112.66,130.90,148.40,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019)<address><addrLine>Long Beach, California, US</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,112.66,158.00,393.33,10.91;30,112.66,171.55,393.32,10.91;30,112.66,185.10,394.53,10.91;30,112.66,198.65,248.89,10.91" xml:id="b55">
	<analytic>
		<title level="a" type="main" coord="30,328.85,158.00,177.14,10.91;30,112.66,171.55,181.87,10.91">CutMix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00612</idno>
	</analytic>
	<monogr>
		<title level="m" coord="30,317.15,171.55,188.84,10.91;30,112.66,185.10,205.78,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV 2019)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV 2019)<address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6022" to="6031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,112.66,212.20,393.33,10.91;30,112.66,225.75,165.41,10.91" xml:id="b56">
	<monogr>
		<title level="m" type="main" coord="30,222.82,212.20,283.17,10.91;30,112.66,225.75,28.65,10.91">Improved regularization of Convolutional Neural Networks with Cutout</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>ArXiv abs/1708.04552</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,112.66,239.30,395.16,10.91;30,112.66,252.85,395.16,10.91;30,112.66,266.40,394.61,10.91;30,112.66,279.94,201.92,10.91" xml:id="b57">
	<analytic>
		<title level="a" type="main" coord="30,346.19,239.30,161.64,10.91;30,112.66,252.85,40.72,10.91">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb" />
	</analytic>
	<monogr>
		<title level="m" coord="30,183.83,252.85,323.99,10.91;30,112.66,266.40,78.22,10.91">Proceedings of the International Conference on Learning Representations (ICLR 2018)</title>
		<meeting>the International Conference on Learning Representations (ICLR 2018)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,112.66,293.49,393.33,10.91;30,112.66,307.04,394.53,10.91;30,112.66,320.59,352.19,10.91" xml:id="b58">
	<analytic>
		<title level="a" type="main" coord="30,242.45,293.49,169.23,10.91">A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m" coord="30,438.43,293.49,67.55,10.91;30,112.66,307.04,330.49,10.91">Proceedings of the 3rd International Conference for Learning Representations (ICLR 2014)</title>
		<meeting>the 3rd International Conference for Learning Representations (ICLR 2014)<address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,112.66,334.14,356.93,10.91" xml:id="b59">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hataya</surname></persName>
		</author>
		<ptr target="https://github.com/moskomule/sam.pytorch" />
		<title level="m" coord="30,161.30,334.14,52.54,10.91">sam.pytorch</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,112.66,347.69,394.96,10.91" xml:id="b60">
	<monogr>
		<title level="m" type="main" coord="30,157.36,347.69,143.45,10.91">SAM implementation in PyTorch</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Balloli</surname></persName>
		</author>
		<ptr target="https://github.com/tourdeml/sam" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,112.66,361.24,394.53,10.91;30,112.66,374.79,393.33,10.91;30,112.66,388.34,395.17,10.91;30,112.66,401.89,395.00,10.91" xml:id="b61">
	<analytic>
		<title level="a" type="main" coord="30,280.58,374.79,111.74,10.91">Mixed precision training</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1gs9JgRZ" />
	</analytic>
	<monogr>
		<title level="m" coord="30,420.18,374.79,85.80,10.91;30,112.66,388.34,291.76,10.91">Proceedings of the International Conference on Learning Representations (ICLR 2018)</title>
		<meeting>the International Conference on Learning Representations (ICLR 2018)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,112.66,415.44,393.33,10.91;30,112.66,428.99,379.99,10.91" xml:id="b62">
	<monogr>
		<title level="m" type="main" coord="30,224.60,415.44,281.39,10.91;30,112.66,428.99,78.28,10.91">Bridging nonlinearities and stochastic regularizers with Gaussian error Linear Units</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>CoRR abs/1606.08415</idno>
		<ptr target="http://arxiv.org/abs/1606.08415" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,112.66,442.54,395.17,10.91;30,112.66,456.08,393.33,10.91;30,112.66,469.63,393.32,10.91;30,112.66,483.18,394.52,10.91;30,112.66,496.73,395.01,10.91;30,112.66,510.28,155.45,10.91" xml:id="b63">
	<analytic>
		<title level="a" type="main" coord="30,470.41,456.08,35.57,10.91;30,112.66,469.63,179.64,10.91">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName coords=""><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1145/2988450.2988454</idno>
	</analytic>
	<monogr>
		<title level="m" coord="30,315.90,469.63,190.08,10.91;30,112.66,483.18,267.91,10.91">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (DLRS 2016): 2016-09-15</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems (DLRS 2016): 2016-09-15<address><addrLine>Boston, Massachusetts, US, DLRS; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,112.66,523.83,393.33,10.91;30,112.66,537.38,395.16,10.91;30,112.66,550.93,394.53,10.91;30,112.66,564.48,386.21,10.91" xml:id="b64">
	<analytic>
		<title level="a" type="main" coord="30,200.82,523.83,305.16,10.91;30,112.66,537.38,102.09,10.91">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v37/ioffe15.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="30,238.23,537.38,269.59,10.91;30,112.66,550.93,249.43,10.91">Proceedings of the 32nd International Conference on International Conference on Machine Learning (ICML 2015)</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning (ICML 2015)<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,112.66,578.03,393.33,10.91;30,112.66,591.58,394.51,10.91;30,112.66,607.57,121.09,7.90" xml:id="b65">
	<analytic>
		<title level="a" type="main" coord="30,247.71,578.03,258.28,10.91;30,112.66,591.58,176.16,10.91">Sigmoid-weighted linear units for Neural Network function approximation in reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Elfwing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2017.12.012</idno>
	</analytic>
	<monogr>
		<title level="j" coord="30,296.55,591.58,72.90,10.91">Neural networks</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,112.66,618.67,393.33,10.91;30,112.66,632.22,393.33,10.91;30,112.66,645.77,393.33,10.91;30,112.33,659.32,394.36,10.91;30,112.66,672.87,387.22,10.91" xml:id="b66">
	<analytic>
		<title level="a" type="main" coord="30,293.89,618.67,212.10,10.91;30,112.66,632.22,75.56,10.91">ImageNet classification with deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="30,462.03,632.22,43.95,10.91;30,112.66,645.77,247.26,10.91">Advances in Neural Information Processing Systems (NIPS 2012)</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting><address><addrLine>Lake Tahoe, Nevada, US</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,112.66,103.81,394.53,10.91;31,112.28,117.35,393.70,10.91;31,112.66,130.90,394.53,10.91;31,112.66,144.45,317.33,10.91" xml:id="b67">
	<analytic>
		<title level="a" type="main" coord="31,178.57,117.35,137.02,10.91">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m" coord="31,337.26,117.35,168.72,10.91;31,112.66,130.90,242.50,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)<address><addrLine>Boston, Massachusetts, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,112.66,158.00,393.33,10.91;31,112.66,171.55,393.33,10.91;31,112.66,185.10,394.53,10.91;31,112.66,198.65,118.49,10.91" xml:id="b68">
	<analytic>
		<title level="a" type="main" coord="31,383.97,158.00,122.01,10.91;31,112.66,171.55,86.43,10.91">Metaformer is actually what you need for vision</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="31,222.99,171.55,283.00,10.91;31,112.66,185.10,165.40,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)<address><addrLine>New Orleans, Louisiana, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10819" to="10829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,112.66,212.20,395.17,10.91;31,112.66,225.75,397.48,10.91;31,112.66,241.74,61.75,7.90" xml:id="b69">
	<monogr>
		<title level="m" type="main" coord="31,294.96,212.20,212.87,10.91;31,112.66,225.75,86.32,10.91">Metaformer: A unified meta framework for finegrained recognition</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2203.02751</idno>
		<ptr target="https://arxiv.org/abs/2203.02751.doi:10.48550/ARXIV.2203.02751" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,112.66,252.85,393.33,10.91;31,112.66,266.40,393.33,10.91;31,112.66,279.94,395.00,10.91;31,112.41,293.49,207.13,10.91" xml:id="b70">
	<analytic>
		<title level="a" type="main" coord="31,276.47,252.85,229.52,10.91;31,112.66,266.40,48.18,10.91">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00482</idno>
	</analytic>
	<monogr>
		<title level="m" coord="31,183.01,266.40,322.98,10.91;31,112.66,279.94,110.75,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2019)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2019)<address><addrLine>Long Beach, California, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4685" to="4694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,112.66,307.04,394.53,10.91;31,112.66,320.59,393.58,10.91;31,112.66,334.14,395.17,10.91;31,112.66,347.69,397.48,10.91;31,112.66,363.68,32.07,7.90;31,89.29,406.21,126.82,12.85" xml:id="b71">
	<analytic>
		<title level="a" type="main" coord="31,112.66,320.59,228.32,10.91">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR46437.2021.00957</idno>
	</analytic>
	<monogr>
		<title level="m" coord="31,370.86,320.59,135.39,10.91;31,112.66,334.14,305.98,10.91;31,143.15,347.69,107.72,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2021)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9690" to="9699" />
		</imprint>
	</monogr>
	<note>Online-only Conference. Detailed Results</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
