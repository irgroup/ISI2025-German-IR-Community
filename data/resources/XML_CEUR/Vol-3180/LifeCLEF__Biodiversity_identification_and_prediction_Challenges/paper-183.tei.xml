<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,385.57,15.42;1,89.29,106.66,310.96,15.42">Solutions for Fine-grained and Long-tailed Snake Species Recognition in SnakeCLEF 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,54.54,11.96"><forename type="first">Cheng</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ant Group</orgName>
								<address>
									<addrLine>Yuan Space 556 Xixi Road</addrLine>
									<postCode>310013</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,156.48,134.97,52.21,11.96"><forename type="first">Furong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ant Group</orgName>
								<address>
									<addrLine>Yuan Space 556 Xixi Road</addrLine>
									<postCode>310013</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,221.33,134.97,59.39,11.96"><forename type="first">Meng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ant Group</orgName>
								<address>
									<addrLine>Yuan Space 556 Xixi Road</addrLine>
									<postCode>310013</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.37,134.97,34.78,11.96"><forename type="first">Wen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ant Group</orgName>
								<address>
									<addrLine>Yuan Space 556 Xixi Road</addrLine>
									<postCode>310013</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,359.14,134.97,59.64,11.96"><forename type="first">Yuan</forename><surname>Cheng</surname></persName>
							<email>chengyuan.c@antgroup.com</email>
							<affiliation key="aff0">
								<orgName type="department">Ant Group</orgName>
								<address>
									<addrLine>Yuan Space 556 Xixi Road</addrLine>
									<postCode>310013</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,385.57,15.42;1,89.29,106.66,310.96,15.42">Solutions for Fine-grained and Long-tailed Snake Species Recognition in SnakeCLEF 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">64A0FC40BD7349D10E2180DB4764A420</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Snake Species Classification</term>
					<term>Fine-grained Classification</term>
					<term>Long-tailed Class Distribution</term>
					<term>Self-supervised Pretraining</term>
					<term>SnakeCLEF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic snake species recognition is important because it has vast potential to help lower deaths and disabilities caused by snakebites. We introduce our solution in SnakeCLEF 2022 for fine-grained snake species recognition on a heavy long-tailed class distribution. First, a network architecture is designed to extract and fuse features from multiple modalities, i.e. photograph from visual modality and geographic locality information from language modality. Then, logit adjustment based methods are studied to relieve the impact caused by the severe class imbalance. Next, a combination of supervised and self-supervised learning method is proposed to make full use of the dataset, including both labeled training data and unlabeled testing data. Finally, post processing strategies, such as multi-scale and multi-crop test-timeaugmentation, location filtering and model ensemble, are employed for better performance. With an ensemble of several different models, a private score 82.65%, ranking the 3rd, is achieved on the final leaderboard.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Snakebite is a global health problem, especially in remote geographic areas and developing countries. According to <ref type="bibr" coords="1,193.29,436.97,11.28,10.91" target="#b0">[1]</ref>, in Asia, up to two million people are envenomed by snakes each year, while in Africa, there are about 435,000 to 580,000 snakebites annually that need treatment, for they can cause permanent disability and disfigurement. Taxonomic knowledge about snakes is crucial in diagnosis and medical response to snakebites, and accurate identification of the snake species is important for the appropriate treatment of snakebite victims since specific antivenoms are effective against specific venomous snakes <ref type="bibr" coords="1,297.52,504.71,11.41,10.91" target="#b1">[2]</ref>. Manual identification, e.g. training doctors on each species, is no easy feat, because there are more than 3,500 species of snakes, 600 of which are venomous <ref type="bibr" coords="1,185.02,531.81,11.51,10.91" target="#b2">[3]</ref>. So, building an automatic and robust image-based system for snake species identification has the greatest potential to save lives <ref type="bibr" coords="1,357.84,545.36,11.43,10.91" target="#b3">[4]</ref>.</p><p>The difficulty of snake species identification, from both a human and a machine perspective, lies in the high intra-class and low inter-class variance in appearance, which may depend on geographic location, colour morph, sex, or age <ref type="bibr" coords="1,316.69,586.01,11.58,10.91" target="#b3">[4]</ref>. Sometimes, having the image alone is not enough, because many species are visually similar to other species, while introducing the geographic origin of an observation can help to recognize considerably. The task of SnakeCLEF 2022 challenge <ref type="bibr" coords="2,159.27,100.52,11.58,10.91" target="#b4">[5]</ref>, as part of the LifeCLEF 2022 <ref type="bibr" coords="2,310.04,100.52,11.48,10.91" target="#b5">[6,</ref><ref type="bibr" coords="2,324.54,100.52,7.65,10.91" target="#b6">7]</ref>, aims to recognize a snake species ID given multiple photographs of the same individual and its corresponding geographic locality information.</p><p>In this paper, we introduce the solution of team "SAI" in SnakeCLEF 2022 for fine-grained snake species recognition on a severe long-tailed class distribution. First, as discussed above, using the image data alone is not enough, more cues are required for prediction, so a network architecture is designed to extract and fuse features from multiple modalities, i.e. photograph from visual modality and geographic locality information from language modality. Then, because of the heavy long-tailed class distribution, some logit adjustment based methods are studied to relieve the impact caused by the severe class imbalance, which significantly improves the performance. Next, to make full use of the dataset, including both labeled training data and unlabeled testing data, a combination of supervised and self-supervised learning method is utilized for pretraining. Finally, some post processing strategies are employed for better performance, including multi-scale and multi-crop test-time-augmentation, location filtering and model ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Fine-grained Vision Classification</head><p>Snake species recognition is basically a task of fine-grained vision classification (FGVC). Modern fine-grained image classification methods can be divided into two parts, use image data only <ref type="bibr" coords="2,495.95,383.35,11.23,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,89.29,396.90,7.52,10.91" target="#b8">9,</ref><ref type="bibr" coords="2,99.65,396.90,12.59,10.91" target="#b9">10,</ref><ref type="bibr" coords="2,115.08,396.90,12.42,10.91" target="#b10">11]</ref>, and use image data as well as extra data from other modalities <ref type="bibr" coords="2,420.72,396.90,16.55,10.91" target="#b11">[12,</ref><ref type="bibr" coords="2,440.11,396.90,12.59,10.91" target="#b12">13,</ref><ref type="bibr" coords="2,455.55,396.90,12.59,10.91" target="#b13">14,</ref><ref type="bibr" coords="2,470.98,396.90,12.42,10.91" target="#b14">15]</ref>. For the task of snake species recognition, one can solve it by using image data only, but a better choice is to use both image data and geographic locality information. In such studies with extra modalities, <ref type="bibr" coords="2,166.94,437.54,17.76,10.91" target="#b11">[12]</ref> is a classic method to introduce geographic information, <ref type="bibr" coords="2,438.23,437.54,17.76,10.91" target="#b12">[13]</ref> introduced spatio-temporal information into the network. MetaFormer <ref type="bibr" coords="2,351.20,451.09,17.76,10.91" target="#b14">[15]</ref> proposed a unified and flexibly framework to joint the visual appearance and various meta-information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Long-tailed Distribution</head><p>Snake species recognition in real world is also a task of long-tailed image classification. Recently, long-tailed learning has received plenty of research interests, and among which there are mainly two kinds of solutions, one is post-hoc normalisation of the classification weights <ref type="bibr" coords="2,442.76,541.47,16.31,10.91" target="#b15">[16,</ref><ref type="bibr" coords="2,461.34,541.47,12.50,10.91" target="#b16">17,</ref><ref type="bibr" coords="2,476.11,541.47,12.50,10.91" target="#b17">18,</ref><ref type="bibr" coords="2,490.88,541.47,12.23,10.91" target="#b18">19]</ref>, and the other is modification of the loss to account for varying class penalties <ref type="bibr" coords="2,454.92,555.02,16.56,10.91" target="#b19">[20,</ref><ref type="bibr" coords="2,475.00,555.02,12.59,10.91" target="#b20">21,</ref><ref type="bibr" coords="2,491.11,555.02,12.42,10.91" target="#b21">22]</ref>. <ref type="bibr" coords="2,89.29,568.57,17.87,10.91" target="#b22">[23]</ref> revisited the classic idea of logit adjustment based on the label frequencies, either applied post-hoc to a trained model, or enforced in the loss during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Snake Species Classification</head><p>Before us, there have been a few works tried to build automatic snake species recognition systems. In <ref type="bibr" coords="2,140.64,645.39,11.23,10.91" target="#b0">[1,</ref><ref type="bibr" coords="2,154.22,645.39,12.23,10.91" target="#b23">24]</ref>, object detectors were first trained to reduce clutter and drop the unnecessary background for preprocessing, and then the detected snakes were classified by trained deep models. <ref type="bibr" coords="3,127.42,354.94,18.07,10.91" target="#b24">[25]</ref> extracted feature for each image with Inception ResNet V2 and concatenated it with geographic feature, then the concatenated features were classified using a lightweight gradient boost classifier. In <ref type="bibr" coords="3,215.91,382.04,16.41,10.91" target="#b23">[24]</ref>, EfficientNet and Vision Transformer were trained, and the prior probabilities of location information were multiplied with the model predictions in a subsequent step. Besides, <ref type="bibr" coords="3,204.16,409.14,11.36,10.91" target="#b1">[2,</ref><ref type="bibr" coords="3,218.25,409.14,7.47,10.91" target="#b2">3,</ref><ref type="bibr" coords="3,228.45,409.14,12.55,10.91" target="#b23">24,</ref><ref type="bibr" coords="3,243.72,409.14,14.03,10.91" target="#b25">26]</ref> used multiple models to improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The proposed solution for fine-grained snake species recognition on a long-tailed class distribution mainly consists of four parts: 1) a network architecture to extract and fuse features from multiple modalities, 2) logit adjustment to relieve the impact caused by the severe class imbalance. 3) a combination of supervised and self-supervised learning method to make full use of both labeled training data and unlabeled testing data, 4) post processing strategies such as multi-scale and multi-crop test-time-augmentation, location filtering and model ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>The network design follows MetaFormer <ref type="bibr" coords="3,270.48,606.20,16.10,10.91" target="#b14">[15]</ref>. It has a hybrid framework, where on one branch CNN is used to extract vision features, on the other branch MLP is used to encode meta data, and transformer is later used to fuse vision features and meta information. The output feature map of CNN branch is transformed to a series of patch embeddings (denoted as patch tokens), along with the output embedding of MLP branch (denoted as meta token), along with the class token, are fed into the transformer layers for prediction. In the task of snake species recognition, the meta data available consists of location code, country and the tag of endemic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Logit Adjustment for Long-tailed Learning</head><p>According to statistics, the dataset has a heavy long-tailed class distribution, where the most frequent species is represented by 6,472 images and the least frequent species by just 5 samples. It is also noteworthy that the evaluation metric, the Mean (Macro) F1-Score, weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favoured over extremely good performance on one and poor performance on the other. If there is no action to deal with long-tailed distribution problem, the recall for rare classes would be very low, thus the overall performance could not be high.</p><p>Logit adjustment <ref type="bibr" coords="4,178.31,258.24,17.88,10.91" target="#b22">[23]</ref> is adopted to relieve the long-tailed distribution problem. In post-hoc adjustment, we predict the following instead of the original one,</p><formula xml:id="formula_0" coords="4,158.33,291.97,347.65,14.19">argmax 𝑦∈[𝐿] exp(𝑓 (𝑥) 𝑦 )/𝜋 𝜏 𝑦 = argmax 𝑦∈[𝐿] 𝑓 (𝑥) 𝑦 -𝜏 • log 𝜋 𝑦<label>(1)</label></formula><p>In logit adjusted loss, it is combined with the soft target cross entropy loss,</p><formula xml:id="formula_1" coords="4,181.69,337.63,324.30,11.36">𝐿(𝑓 (𝑥) 𝑦 , 𝑦) = -𝑦 • log softmax(𝑓 (𝑥) 𝑦 + 𝜏 • log 𝜋 𝑦 )<label>(2)</label></formula><p>where 𝜏 &gt; 0 is a hyper-parameter, 𝑓 (𝑥) 𝑦 is the output logits of the neural network given input 𝑥, and 𝜋 𝑦 is the estimate of the class prior, e.g., the empirical class frequency on the training data. In practice, one can use either post-hoc logit adjustment or logit adjusted loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Combination of Supervised and Self-supervised Learning</head><p>In order to make full use of the dataset, including both labeled training data and unlabeled testing data, a combination of supervised and self-supervised training framework is proposed for pretraining. Specifically, we perform supervised learning on labelled training data and selfsupervised learning (SSL) on all the data to obtain a set of task-related parameter initialization. Since the task provides images and meta information, we do pretraining for MetaFormer <ref type="bibr" coords="4,488.03,490.20,17.96,10.91" target="#b14">[15]</ref> with meta data, which can jointly take advantage of vision and meta-information.</p><p>To obtain a task-relevant initialization instead of imagenet initialization, we combine the self-supervised method SimCLR <ref type="bibr" coords="4,230.76,530.84,17.75,10.91" target="#b26">[27]</ref> with MetaFormer. Specifically, for each input pair of image and meta data, we randomly perform two data augmentations (Fig. <ref type="figure" coords="4,398.36,544.39,4.25,10.91" target="#fig_2">2</ref>) for the image, but no augmentation on meta data, then a classification loss SoftTargetCE <ref type="bibr" coords="4,382.52,557.94,16.31,10.91" target="#b27">[28,</ref><ref type="bibr" coords="4,401.39,557.94,13.95,10.91" target="#b28">29]</ref> (short for soft target cross entropy) is applied to the labeled data only, and a contrastive loss InfoNCE <ref type="bibr" coords="4,443.68,571.49,17.76,10.91" target="#b29">[30]</ref> is applied to all the data. Thus, the loss function for pretraining is,</p><formula xml:id="formula_2" coords="4,117.43,604.96,388.55,14.37">𝐿 𝑝𝑟𝑒𝑡𝑟𝑎𝑖𝑛 (𝑋, 𝑌 ) = SoftTargetCE 𝑌 ̸ =-1 (𝑓 (𝑋), 𝑌 ) + 𝛼 • InfoNCE(𝑔([𝑋; 𝑋 ¯]), 𝑌 )<label>(3)</label></formula><p>where 𝑓 (𝑋) stands for logits, 𝑔(𝑋) stands for extracted features for a given batch 𝑋, and 𝑌 stands for its corresponding label, 𝑌 = -1 means unlabeled testing data. 𝑋 ¯is an augmented version of 𝑋. 𝛼 is a hyper-parameter to balance the relative importance between supervised loss and self-supervised loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Post Processing</head><p>Two kinds of post processing strategies are mainly used, one is multi-scale, multi-crop and multi-model ensemble, the other is location filtering. For model ensemble, average logits is first calculated based on the output logits of different models, then the mean logits is adjusted to deal with long-tailed distribution problem. Specifically, for each single scale/crop input 𝑥 𝑖 , the 𝑗-th model outputs logits 𝑓 𝑗 (𝑥 𝑖 ), then the final logits used for prediction is,</p><formula xml:id="formula_3" coords="5,181.45,351.84,324.53,25.43">logits_adjusted = mean( ∑︁ 𝑗 ∑︁ 𝑖 𝑓 𝑗 (𝑥 𝑖 )) -𝜏 • log 𝜋<label>(4)</label></formula><p>Location Filtering. During training, it can be found that top-5 accuracy is much higher than that of top-1, which implies that if properly chosen from top predictions, the result could be better than the naive argmax one. A locations-to-species mapping is used to heuristically choose the best candidate prediction. For simplicity, we iterate through a sorted logits list until the first certain species appears, whose species name co-occurs with its location code. Algorithm 1 shows the numpy style pseudo code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Dataset: The dataset is based on 187,129 snake observations with 318,532 photographs belonging to 1,572 snake species and observed in 208 countries. The data were gathered from the online biodiversity platform iNaturalist. The provided dataset has a heavy long-tailed class distribution, where the most frequent species (Natrix natrix) is represented by 6,472 images and the least frequent species by just 5 samples. Evaluation Metric: The evaluation metric for this competition is macro 𝐹 1 -Score. The 𝐹 1 score for the 𝑖-th species is computed as, The macro 𝐹 1 is calculated by averaging the 𝐹 1 scores over all the species <ref type="bibr" coords="6,431.93,373.15,11.43,10.91" target="#b0">[1]</ref>,</p><formula xml:id="formula_4" coords="5,230.25,641.33,275.73,25.50">𝐹 𝑖 1 = 2 • 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑖 • 𝑟𝑒𝑐𝑎𝑙𝑙 𝑖 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑖 + 𝑟𝑒𝑐𝑎𝑙𝑙 𝑖<label>(5)</label></formula><formula xml:id="formula_5" coords="6,252.63,396.95,253.36,33.71">macro 𝐹 1 = 𝑁 ∑︁ 𝑖=1 𝐹 𝑖 1 𝑁<label>(6)</label></formula><p>where 𝑁 is the number of species. The macro 𝐹 1 score is not biased by class frequencies and is more suitable for the long-tailed class distributions. The 𝐹 1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favoured over extremely good performance on one and poor performance on the other. Implementation Details: MetaFormer <ref type="bibr" coords="6,268.17,509.38,17.81,10.91" target="#b14">[15]</ref> is selected as our base network. More specifically, we use MetaFormer-2 with extra meta information as input for both pretraining and finetuning. The meta data available for snake species consists of location code, country and the tag of endemic. Thus we construct a 2438-d vector to encode all of the above meta data, and then send it to an MLP to generate meta token. Hyper-parameter 𝜏 in Eq. 1 is set to 0.55, 𝛼 in Eq. 3 is set to 0.001. In supervised and self-supervised pretraining, ImageNet-21k pretrained model is loaded as initialization, and the learning rate is initialized as 5 × 10 -5 . In later finetuning, the learning rate is initialized as 5 × 10 -6 . The models are trained on 8-NVIDIA A100-GPU machines for 300 epochs with a per GPU batch size of 72 for size 384 and 32 for size 512. The augmentations for self-supervised learning is illustrated in Fig. <ref type="figure" coords="6,376.03,631.33,3.80,10.91" target="#fig_2">2</ref>, while those for supervised learning and finetuning still follows MetaFormer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,293.27,416.69,8.93;3,89.06,305.28,416.92,8.87;3,89.29,316.97,400.43,9.14;3,89.29,84.19,416.70,189.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall network architecture. It has a hybrid framework, where CNN branch is used to extract vision features, MLP branch is used to encode meta data, and transformer is used to fuse vision features and meta information. 𝑔(𝑥) stands for extracted features and 𝑓 (𝑥) stands for the predicted logits.</figDesc><graphic coords="3,89.29,84.19,416.70,189.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,94.31,89.50,143.10,10.91;5,100.14,104.54,344.15,10.91;5,100.14,118.08,143.88,10.91;5,90.62,136.31,198.52,10.91;5,90.62,149.86,106.58,10.91;5,91.02,166.30,4.10,7.13;5,116.81,163.41,144.98,10.91;5,91.02,179.85,4.10,7.13;5,116.81,176.96,191.05,10.91;5,91.42,193.40,4.10,7.13;5,133.48,190.51,24.80,10.91;5,90.62,212.37,101.70,10.91"><head>Algorithm 1 :] 2 for idx in idx_sort do 3 species_name = species_list[idx] 4 if 5 break 6</head><label>13456</label><figDesc>Location filtering Input : logits_adjusted, locations-to-species mapping 𝐿2𝑆, meta data 𝑀 𝑒𝑡𝑎 Output : predicted species name 1 idx_sort = np.argsort(logits_adjusted)[::-1species_name in L2S[Meta['code']] then return species_name</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,311.64,416.70,8.93;6,88.93,323.64,418.15,8.87;6,89.29,335.60,266.97,8.87;6,89.29,84.19,416.69,208.06"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Augmentations for self-supervised learning. From left to right, top to bottom: original image without augmentation, color jittering, gray scale, horizontal flip, random resized crop, Gaussian blur, random erasing, a composed random augmentation of the above.</figDesc><graphic coords="6,89.29,84.19,416.69,208.06" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Representative experimental results about the importance of: 1) meta information, 2) different pretrained models, 3) solving long-tailed class distribution problem, 4) location filtering, <ref type="bibr" coords="7,408.20,114.45,7.70,8.87" target="#b4">5)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>In this part, we report some representative experimental results, including: 1) the importance of meta information, 2) the importance of pretrained models, 3) the importance of solving long-tailed class distribution, 4) the importance of location filtering, 5) the importance of larger input size, 6) the importance of model ensemble.</p><p>The experimental results are summarized in Tab. 1. Academically, these comparisons here are not strict ablation studies, because they are obtained by few limited submissions during the competition. However, these results provided a meaningful and effective path to optimize the solution, which did improve the online judge performance.</p><p>As shown in Tab. 1, training the model with additional meta information significantly improves the performance from 66.64% to 74.19%, which indicates the importance of data from multiple modalities. For pretrained models, the proposed task-related supervised+SSL pretraining for Snake performs better than those commonly used ones, which indicates the importance of using unlabeled testing data. In long-tailed learning, logit adjustment is proved to be more effective, which improves the score from 74.19% to 78.57%. Location filtering, a task-specific post processing, using the statistics prior from the whole dataset to remove illegal predictions, improves the score from 64.32% to 69.09%. Also, training with larger input size 512 improves the performance from 81.18% to 82.03%.</p><p>With an ensemble of seven different models, we got private score 82.65% on the final leaderboard. The improvement is marginal compared to a single model, because the differences among these models are small, i.e., different epochs, different hyper-parameters. Interestingly, in late submission, we find it inferior to 82.72%, the ensemble of only two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we introduce our solution in SnakeCLEF 2022 for fine-grained snake species recognition on a severe long-tailed class distribution. Attentions have been mainly focused on four parts: 1) fusion of vision features and meta information, 2) solving long-tailed class distribution problem, 3) making full use of both the labeled training data and unlabeled testing data via supervised and self-supervised pretraining, 4) post processing strategies such as location filtering and model ensemble.</p><p>Though great improvements have been made, there still exist some actions of great potential for future work: 1) hard example mining for fine-grained and long-tailed dataset, 2) treating it as a retrieval task not a classification task, 3) using a snake detection model to get more precise bounding box for data pre-processing. We have tried some of the above but none of them contributed to the final performance during the competition, but they have great potential if further studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Online Resources</head><p>The code and models are available at Google Drive.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,448.10,393.33,10.91;8,112.66,461.65,312.01,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,202.64,448.10,303.34,10.91;8,112.66,461.65,89.01,10.91">Incorporation of object detection models and location data into snake species classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Borsodi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Papp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,228.02,461.65,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1499" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,475.20,393.33,10.91;8,112.66,488.75,103.57,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,302.26,475.20,203.73,10.91;8,112.66,488.75,71.65,10.91">A deep learning method for visual recognition of snake species</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,502.30,394.62,10.91;8,112.66,515.85,395.01,10.91;8,112.41,529.40,48.96,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,391.31,502.30,115.97,10.91;8,112.66,515.85,224.64,10.91">Uaic-ai at snakeclef 2021: Impact of convolutions in snake species recognition</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">G</forename><surname>Coca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">T</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">P</forename><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,364.29,515.85,97.61,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1540" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,542.95,393.33,10.91;8,112.66,556.50,265.83,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>De Castañeda</surname></persName>
		</author>
		<title level="m" coord="8,334.93,542.95,171.06,10.91;8,112.66,556.50,233.91,10.91">Overview of snakeclef 2021: Automatic snake species identification with country-level focus</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,570.05,393.33,10.91;8,112.66,583.60,393.33,10.91;8,112.66,597.15,159.39,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,292.93,570.05,107.55,10.91;8,429.01,570.05,76.98,10.91;8,112.66,583.60,165.68,10.91">Automated snake species identification on a global scale</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hrúz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,300.64,583.60,205.35,10.91;8,112.66,597.15,128.70,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of SnakeCLEF</note>
</biblStruct>

<biblStruct coords="8,112.66,610.69,394.53,10.91;8,112.66,624.24,394.53,10.91;8,112.66,637.79,393.33,10.91;8,112.66,651.34,393.33,10.91;8,112.66,664.89,353.54,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,198.52,637.79,307.47,10.91;8,112.66,651.34,247.50,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,383.00,651.34,122.99,10.91;8,112.66,664.89,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,86.97,394.53,10.91;9,112.66,100.52,393.33,10.91;9,112.66,114.06,393.33,10.91;9,112.66,127.61,168.28,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,185.62,100.52,320.37,10.91;9,112.66,114.06,208.65,10.91">Lifeclef 2022 teaser: An evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,343.19,114.06,162.80,10.91;9,112.66,127.61,38.01,10.91">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,141.16,395.17,10.91;9,112.66,154.71,395.01,10.91;9,112.41,168.26,48.96,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,278.75,141.16,229.08,10.91;9,112.66,154.71,16.35,10.91">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.83,154.71,309.40,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,181.81,393.33,10.91;9,112.66,195.36,378.89,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,261.21,181.81,244.78,10.91;9,112.66,195.36,92.20,10.91">Learning deep bilinear transformation for fine-grained image representation</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,213.25,195.36,233.51,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,208.91,393.71,10.91;9,112.66,222.46,393.33,10.91;9,112.33,236.01,29.19,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07976</idno>
		<title level="m" coord="9,457.57,208.91,48.79,10.91;9,112.66,222.46,239.30,10.91">Transfg: A transformer architecture for fine-grained recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,249.56,393.33,10.91;9,112.66,263.11,393.33,10.91;9,112.66,276.66,283.16,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,209.69,249.56,296.30,10.91;9,112.66,263.11,173.82,10.91">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,309.54,263.11,196.44,10.91;9,112.66,276.66,185.04,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3034" to="3043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,290.20,393.33,10.91;9,112.66,303.75,393.33,10.91;9,112.66,317.30,262.63,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,458.38,290.20,47.61,10.91;9,112.66,303.75,171.00,10.91">Geo-aware networks for fine-grained recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Potetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,309.82,303.75,196.16,10.91;9,112.66,317.30,194.36,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,330.85,393.33,10.91;9,112.66,344.40,393.53,10.91;9,112.30,357.95,124.54,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,259.01,330.85,246.98,10.91;9,112.66,344.40,57.10,10.91">Presence-only geographical priors for fine-grained image classification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,193.33,344.40,312.85,10.91;9,112.30,357.95,26.65,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9596" to="9606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,371.50,394.62,10.91;9,112.66,385.05,394.53,10.91;9,112.66,398.60,65.30,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,182.67,371.50,304.38,10.91">Fine-grained image classification via combining vision and language</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,112.66,385.05,364.29,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5994" to="6002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,412.15,393.53,10.91;9,112.66,425.70,288.50,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02751</idno>
		<title level="m" coord="9,307.65,412.15,198.53,10.91;9,112.66,425.70,106.31,10.91">Metaformer: A unified meta framework for fine-grained recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,439.25,393.33,10.91;9,112.66,452.79,393.33,10.91;9,112.33,466.34,29.19,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04486</idno>
		<title level="m" coord="9,271.57,439.25,234.42,10.91;9,112.66,452.79,236.14,10.91">To balance or not to balance: A simple-yet-effective approach for learning with long-tailed distributions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,479.89,393.32,10.91;9,112.33,493.44,88.63,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,180.50,479.89,255.94,10.91">Adjusting decision boundary for class imbalanced learning</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,444.71,479.89,53.54,10.91">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="81674" to="81685" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,506.99,395.17,10.91;9,112.66,520.54,393.57,10.91;9,112.33,534.09,29.19,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09217</idno>
		<title level="m" coord="9,433.71,506.99,74.12,10.91;9,112.66,520.54,236.88,10.91">Decoupling representation and classifier for long-tailed recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,547.64,393.33,10.91;9,112.66,561.19,350.30,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H.-J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-C</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01385</idno>
		<title level="m" coord="9,318.93,547.64,187.06,10.91;9,112.66,561.19,168.20,10.91">Identifying and compensating for feature deviation in imbalanced deep learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,574.74,393.33,10.91;9,112.66,588.29,393.32,10.91;9,112.66,601.84,192.34,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,361.53,574.74,144.46,10.91;9,112.66,588.29,79.19,10.91">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,215.02,588.29,290.96,10.91;9,112.66,601.84,84.28,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11662" to="11671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,615.39,395.17,10.91;9,112.66,628.93,393.33,10.91;9,112.33,642.48,29.19,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,323.58,615.39,184.25,10.91;9,112.66,628.93,138.15,10.91">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,258.65,628.93,234.28,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,656.03,393.53,10.91;9,112.66,669.58,393.33,10.91;10,112.66,86.97,147.08,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="9,305.50,656.03,200.69,10.91;9,112.66,669.58,44.86,10.91">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,180.82,669.58,325.16,10.91;10,112.66,86.97,49.16,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,100.52,393.33,10.91;10,112.66,114.06,252.90,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07314</idno>
		<title level="m" coord="10,411.38,100.52,94.60,10.91;10,112.66,114.06,70.44,10.91">Long-tail learning via logit adjustment</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,127.61,395.17,10.91;10,112.66,141.16,395.01,10.91;10,112.41,154.71,48.96,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,231.91,127.61,275.92,10.91;10,112.66,141.16,213.18,10.91">Efficientnets and vision transformers for snake species identification using image and location information</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,357.29,141.16,101.81,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1477" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,168.26,393.33,10.91;10,112.33,181.81,205.09,10.91" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="10,277.52,168.26,228.47,10.91;10,112.33,181.81,43.77,10.91">Snake Species Classification Using Transfer Learning Technique</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Desingu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palaniappan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>EasyChair</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="10,112.66,195.36,393.33,10.91;10,112.66,208.91,394.52,10.91;10,112.41,222.46,22.69,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="10,431.02,195.36,74.97,10.91;10,112.66,208.91,191.90,10.91">Automatic snake classification using deep learning algorithm</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kalinathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Balasundaram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Bathala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Mukesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,332.09,208.91,99.38,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,236.01,393.33,10.91;10,112.66,249.56,183.50,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="10,311.65,236.01,194.34,10.91;10,112.66,249.56,107.47,10.91">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,243.14,249.56,22.08,10.91">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,263.11,393.33,10.91;10,112.66,276.66,217.61,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="10,286.37,263.11,153.18,10.91">When does label smoothing help?</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,449.77,263.11,56.21,10.91;10,112.66,276.66,172.82,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,290.20,393.33,10.91;10,112.66,303.75,347.85,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="10,424.98,290.20,81.01,10.91;10,112.66,303.75,68.82,10.91">Delving deep into label smoothing</title>
		<author>
			<persName coords=""><forename type="first">C.-B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P.-T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,190.41,303.75,176.02,10.91">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5984" to="5996" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,317.30,394.53,10.91;10,112.66,330.85,173.79,10.91" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="10,248.09,317.30,254.55,10.91">Representation learning with contrastive predictive coding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,344.40,394.53,10.91;10,112.66,357.95,393.58,10.91;10,112.66,371.50,351.04,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="10,112.66,357.95,228.32,10.91">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,370.86,357.95,135.39,10.91;10,112.66,371.50,252.92,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9695" to="9704" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
