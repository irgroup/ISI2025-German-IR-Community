<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,412.82,15.42;1,89.29,106.66,240.40,15.42">Deep Learning and Gradient Boosting Ensembles for Classification of Snake Species</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,115.20,11.96"><forename type="first">Mirunalini</forename><surname>Palaniappan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,217.14,134.97,80.34,11.96"><forename type="first">Karthik</forename><surname>Desingu</surname></persName>
							<email>karthik19047@cse.ssn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.12,134.97,100.56,11.96"><forename type="first">Haricharan</forename><surname>Bharathi</surname></persName>
							<email>haricharan2010267@ssn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,137.29,11.96"><forename type="first">Eeswara</forename><forename type="middle">Anvesh</forename><surname>Chodisetty</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.58,148.92,83.91,11.96"><forename type="first">Anirudh</forename><surname>Bhaskar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,412.82,15.42;1,89.29,106.66,240.40,15.42">Deep Learning and Gradient Boosting Ensembles for Classification of Snake Species</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">9C8C9364DDC0FBE5E7B97EC3E80E4AB5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ensemble Learning</term>
					<term>Convolutional Neural Networks</term>
					<term>Gradient Boosting Ensemble</term>
					<term>Feature Extraction</term>
					<term>Metadata-aided Classification</term>
					<term>Image Classification</term>
					<term>Transfer Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes an deep-learning based ensembling approach for snake species classification. The proposed method employs state-of-the-art models such as ResNet and EfficientNet among others, applies transfer learning and fine-tunes them to the target data domain -snake images, uses them as feature extractors, and finally conjoins the produced representation vectors along with geographic metadata information to train a gradient boosting ensemble classifier to predict the snake species. The authors performed multiple experiments to train individual deep-learning architectures, select effective feature extraction models, and train a gradient boosting classifier using their ensembled features. The approach attained a maximum macro-averaged F1-Score of 51.39% on the test data. The corresponding validation F1-Score and Accuracy scores were 52.04% and 87.13%, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The SnakeCLEF challenge 2022 <ref type="bibr" coords="1,233.17,415.45,12.99,10.91" target="#b0">[1]</ref> held as a part of the LifeCLEF-2022 <ref type="bibr" coords="1,410.42,415.45,11.47,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,424.63,415.45,9.03,10.91" target="#b2">3]</ref> lab of the CLEF 2022 conference and the FGVC9 workshop organized in conjunction with CVPR 2022 conference aims to use image recognition to identify snake species.</p><p>The human population has been growing at an alarming rate from just over 1 billion in 1800 to 7.9 billion in 2000. This exponential increase in population has led to a demand for urban and rural dwellings, which has resulted in degradation of animal habitats. One of the species affected by this is snakes. India averages about 58000 snake bite deaths a year <ref type="bibr" coords="1,406.35,496.74,13.00,10.91" target="#b3">[4]</ref> which has become the result of frequent wildlife encounters. Snake bites can range from minor symptoms like nausea and breathlessness to major symptoms such as amputations and permanent disability. To address such problems, identifying the type of snake would help administer a precise antidote. Taxonomic identification of the species helps healthcare providers to articulate the symptoms, responses of the treatment and antivenom efficacy and also aids in clinical management <ref type="bibr" coords="1,483.08,564.49,11.43,10.91" target="#b4">[5]</ref>.</p><p>Furthermore, snake species identification is crucial for biodiversity, conservation, and global health. Millions of snake bites occur globally every year, which often leads to snakebite envenoming, killing, and disabling humans across the globe <ref type="bibr" coords="2,353.76,86.97,11.30,10.91" target="#b5">[6]</ref>. Because of the high intra-class and low inter-class variance, situational stress, and dread of potential danger, identifying the snake species from both a manual standpoint and a machine perspective is imperative. An accurate identification may also depend on various other factors, such as geographical location, morph, color, sex or age. Knowing the geographic location can also contribute more towards an accurate identification. An automatic system that helps in recognizing the snake species from the photographic image and geographic information can be paramount in overcoming the above problems. Hence, we propose an automated system that helps in identifying the snake species from the given input images.</p><p>The data provided in the task includes standardized images obtained from the online Biodiversity platform iNaturalist and photographed images. The dataset has a very long-tailed classification varying from about 6500 images to 5 samples. Different training set images such as large, medium and small sized images along with the metadata information were provided. We proposed a deep learning-based ensembling method using the large training set of images and the metadata information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep learning-based models such as EfficientNets and Vision Transformer (ViT) models were used and the prior probabilities of the location information were multiplied with the model predictions was proposed in <ref type="bibr" coords="2,221.96,362.38,13.00,10.91" target="#b6">[7]</ref> to classify the snake species. A deep learning architecture ResNeXt50-V2 was proposed for identifying 772 snake species <ref type="bibr" coords="2,372.16,375.93,11.54,10.91" target="#b7">[8]</ref>. Taxonomy based features were used in classification of snakes in <ref type="bibr" coords="2,261.19,389.48,12.69,10.91" target="#b8">[9]</ref> which has used similarity nearest neighbor classifier. The holistic methods such as k-nearest neighbors (kNN), support vector machine (SVM) and logistic regression (LR) are used in combination of dimension reduction approach PCA and LDA and the work was compared with CNN was proposed <ref type="bibr" coords="2,323.76,430.13,16.09,10.91" target="#b9">[10]</ref>. Study of deep learning, its strategies, comparison of frameworks, and algorithms were presented <ref type="bibr" coords="2,354.77,443.67,16.25,10.91" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">SnakeCLEF</head><p>The best scoring team's paper in SnakeCLEF-2021 written by RegÅ‘ Borsodi <ref type="bibr" coords="2,445.20,493.40,16.41,10.91" target="#b11">[12]</ref>, used an EfficientNetB0 base model while incorporating object detection and preprocessing using an EfficientNetD1 object detector. They got the highest F1 score of 0.90 with close to 95% classification accuracy. The 3rd best scoring team's paper in SnakeCLEF-2021 <ref type="bibr" coords="2,436.28,534.05,18.07,10.91" target="#b12">[13]</ref> written by Rail Chamidullin, went ahead with an ensemble of 4 models choosing to include ResNeSt50, ResNeSt101, ResNeSt200, and ResNet101. A majority voting strategy was employed ResNeSt200 was chosen since the model achieved an F1 score of 0.83 and a 91.60 classification accuracy.</p><p>The paper submitted by Lucia Georgiana Coca for SnakeCLEF-2021 <ref type="bibr" coords="2,413.40,588.25,18.07,10.91" target="#b13">[14]</ref> used 3 different models such as GoogleLeNet, VGG16 as well as ResNet. Vision transformers were used and an ensemble ResNet model were employed and achieved F1 score of 0.79. Finally, the paper published by Karthik Desingu et al. <ref type="bibr" coords="2,244.14,628.89,17.76,10.91" target="#b14">[15]</ref> employed a transfer learning method by ensembling the features extracted from Inception-ResNet-v2 with the metadata information after preprocessing the images.</p><p>Based on the results from last year's submissions, our team observed that 2 architectures stood out among the rest -ResNet101 and EfficientNetB0, producing very high individual F1 scores with base models. Also, most of the teams reported that using an ensemble model with gradient boosting and the usage of transfer learning algorithms gave very high F1 score and classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Transfer Learning <ref type="bibr" coords="3,173.93,199.79,18.07,10.91" target="#b15">[16]</ref> is a technique where a neural network is trained on a given problem domain, and then used on another similar problem by adding one or more layers to the trained model to fine-tune the model to solve the new problem at hand. This technique is generally used to reduce the training time for a neural network model and also results in a lower generalization error. The weights in the old model are typically used as a starting point in the fine-tuning process. Ensembling <ref type="bibr" coords="3,188.13,267.54,18.07,10.91" target="#b16">[17]</ref> is another popular computational learning technique, where the predictions or extracted features from multiple machine learning models are employed for classification or regression, aiming to achieve improved performance than that of a single model.</p><p>Our team applied transfer learning to calibrate state-of-the-art networks like ResNet101, ResNeXt101 and EfficientNets for snake species classification. Our proposed method extracts representative features from input snake images. The feature vectors were then fed to a gradient boosting ensemble classifier, along with contextual categorical features to predict the class probabilities for a given observation, thereby inferring its most probable class label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The given dataset consists of 187129 snake observations made with 318532 images belonging to 1572 unique species. A set of 208 countries are also recorded. The given metadata gives us useful geographical information such as the country where the snake was spotted. This can be used to supplement the visual information during training. The current dataset is vast compared to last year's challenge dataset which had only 772 distinct snake species on the contrary having a 414424 images. The number of images per species has drastically reduced from 536 last year to 212 this year.</p><p>While last year SnakeCLEF2021 <ref type="bibr" coords="3,243.97,520.50,17.90,10.91" target="#b17">[18]</ref> has 414424 photographs of 772 snake species collected in 188 countries where present in the dataset, this year featured a significantly lower number of photographs bringing the total down to 318532. The number of snake species incorporated this year was 1572, increasing the number of species by more than a factor of 2 compared to the previous years. The photographs were captured from across 208 countries, showing an increase of 10.60% compared to the previous year. The dataset is particularly challenging due to its long-tailed distribution -it has a heavy long-tailed class distribution, where the most frequent species -Natrix natrix is represented by 6472 images and the least frequent species by just 5 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Input Image Preprocessing</head><p>The given images were first fed into an input sequencer where we noticed a lot of noisy data. Analysis of the images presented the fact that they differed in size and were of different scales. We thus employed a fold-down strategy to convert all images to the RGB format and resize them to a standard size of 224 Ã— 224 Ã— 3 using bi-linear interpolation <ref type="bibr" coords="4,402.57,148.18,16.25,10.91" target="#b18">[19]</ref>.</p><p>To eliminate the effect of irrelevant factors in the context of the required task such as variation in lighting conditions among the photographs, the images were linearly normalized to values between 0 and 1 Transformations such as scale and rotation, as well as contrast and saturation variations were induced on the model inputs to make the model more generic, immune to the impact of positional and orientation-based bias and prevent memorization by enhancing image diversity. RandAugment <ref type="bibr" coords="4,199.17,229.48,17.85,10.91" target="#b19">[20]</ref> was used to augment the input images using the aforementioned transformations. RandAugment is parameterized by two values -the number of augmentation transformations to apply sequentially (N), and the magnitude for all the transformations (M). The values used in <ref type="bibr" coords="4,174.73,270.13,17.91,10.91" target="#b19">[20]</ref> for the ResNet model i.e N=3 and M=4 were chosen, by example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Extraction</head><p>Reducing the number of resources to train on without losing relevant information is the basis on which feature extraction is built. The dimensions are reduced such that the raw data is made into chunks of manageable groups for processing. This method proved useful as the given large dataset had over 110 GB of training images and thus the amount of features that actually represent the dataset are shadowed by widely unrepresentative features.</p><p>Concretely, state-of-the-art neural networks were used as feature extractors that output representation vectors for the input images. The feature extraction process is tuned to extract representative and class-discriminate features through supervised learning, wherein class labels are used as ground truth to backpropagate <ref type="bibr" coords="4,280.84,428.25,17.91,10.91" target="#b20">[21]</ref> and tune the model weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Deep Learning Architectures Considered</head><p>Multiple deep-learning architectures were considered for this classification task.</p><p>ResNet101 <ref type="bibr" coords="4,150.80,491.52,18.06,10.91" target="#b21">[22]</ref> is a popular convolutional neural network model developed in 2015. This model solves the degradation problem which states that as the network depth increases accuracy gets saturated and then degrades rapidly. ResNet101 uses shortcut connections that skip one or more layers to solve the degradation problem which was inspired from the Highway network <ref type="bibr" coords="4,89.29,545.72,17.91,10.91" target="#b22">[23]</ref> which used gated shortcut connections to control the flow of information in the shortcut.</p><p>EfficientNet(s) <ref type="bibr" coords="4,165.18,559.27,17.76,10.91" target="#b23">[24]</ref> are a class of convolutional neural networks that were built in 2019. It's a small-scale architecture with about 11 million trainable parameters. It was created with the help of a multi-objective neural network that prioritized precision and floating point operations. It supports compound scaling while maintaining network balance across all dimensions. It employs an inverted bottleneck, as well as a depth-wise convolutional network that includes squeeze and excitation operations. It employs MBConv blocks <ref type="bibr" coords="4,361.68,627.01,17.76,10.91" target="#b24">[25]</ref> that serve as Inverted Linear BottleNeck layers. These layers use Depth-Wise Separable Convolution operations. The model complexities of the variants increase from B0 to B7. The authors experimented with B0, B4 and B6 variants to scale the model complexity and find the best suited intricacy for the dataset.</p><p>ResNeXt101 <ref type="bibr" coords="5,158.94,86.97,18.07,10.91" target="#b25">[26]</ref> is another popular convolutional neural network model which is very similar to the ResNet101 model. ResNet101 has many sequential layers whereas ResNeXt101 has parallel stacking layers instead. It follows a split-transform-merge strategy like the Inception module <ref type="bibr" coords="5,125.58,127.61,16.32,10.91" target="#b26">[27]</ref>. Unlike the Inception module which has different filters and sizes for each block, ResNeXt shares these hyper-parameters for all the blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Gradient Boosting Ensemble Classifier</head><p>Ensemble methods <ref type="bibr" coords="5,178.84,190.89,18.06,10.91" target="#b16">[17]</ref> are techniques employed to combine multiple model(s) to produce improved results. They boast higher accuracy scores than the individual models themselves. Boosting <ref type="bibr" coords="5,132.74,217.99,18.07,10.91" target="#b27">[28]</ref> is a prominent ensembling technique used wherein new models are added to the existing features of the model to correct errors. Our solution adopted a gradient-boosting ensemble approach to classify images into their corresponding snake species.</p><p>XGBoost <ref type="bibr" coords="5,143.56,258.64,18.07,10.91" target="#b28">[29]</ref> is an implementation of gradient boosted decision trees designed for speed and performance. The XGBoost library package was chosen by the authors for implementation among all the other available boosters for its superior execution speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Model training was performed through transfer-learning from the weights obtained upon training with the ImageNet data set <ref type="bibr" coords="5,248.19,357.91,16.12,10.91" target="#b29">[30]</ref>, and fine-tuning on the SnakeCLEF-2022 training data. During model training, the models' prediction accuracy were tracked to later choose the set of feature extractors to use for ensembling. Based on the observed performance of each network, ResNeXt101 and EfficientNetB6 were chosen as feature extractors for ensembling. The XGBoost gradient boosting ensemble classifier was used for ensembling.</p><p>The forward propagation during training and prediction is described. Each observation is composed of multiple snake images along with its contextual geographic information -the country where the species was observed. Each image in an observation is first preprocessed, then passed through the two feature extraction networks to obtain two representation vectors, each of size 4096. These vectors are merged together, along with numeric encoded country metadata for the image, to obtain a final vector of size 8193. The numeric encoding was achieved with the help of Label Encoders from the scikit-learn <ref type="bibr" coords="5,338.98,506.95,18.07,10.91" target="#b30">[31]</ref> library wherein, the categorical country codes available as training data are converted to integral class labels that can be fed as input to the feature extraction neural network. These 8193 features are fed to the boosting ensemble classifier to obtain a probability distribution over all possible snake species classes. This workflow is depicted in Figure <ref type="figure" coords="5,252.90,561.15,3.81,10.91" target="#fig_0">1</ref>. Countries that are present in the testing data, but not encountered in the training set are encoded as 0, a commonly adopted strategy to deal with unseen categorical data in deep learning methods.</p><p>The corresponding class probability values obtained for each image in an observation are averaged to obtain a single aggregate distribution of probabilities over all classes. Consequently, there is one single probability distribution for each observation. The class that is attributed with the highest aggregate probability value is output as the classification label. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Training</head><p>The details of the model training process, performed through transfer-learning is presented in this section. A summary of the parameters used for model training is tabulated in Table <ref type="table" coords="6,482.04,338.64,3.74,10.91" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">ResNet101</head><p>The feature extraction layers of ResNet101 were trained with a two-step classification block, comprising two dense blocks with 4096 and 1572 neurons respectively. The extracted features were percolated through a flatten layer to obtain, before feeding to the classification block. In addition, a dropout layer was added after the dense layer to avoid overfitting. Dropout rates between 0.30 and 0.70 were experimented and set to 0.40 in the final version of the model. The model was trained with the Adam optimizer at an initial learning rate of 3ğ‘’-4. It was backpropagated using the Categorical Cross-Entropy (CCE) loss. For feature extraction, the output of the first dense layer was used to produce a feature vector of 4096 elements. During training, the model's prediction accuracy was tracked to later choose the feature extractor to use for ensembling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">EfficientNetB0</head><p>EfficientNetB0 was also trained with a two-step classification block, comprising two dense blocks with 4096 and 1572 neurons respectively. The extracted features were percolated through a flatten layer to obtain, before feeding to the classification block. The dropout layer added after the dense layer for this network was experimented between 0.30 and 0.70 and fixed at 0.45. The model was trained with the using the Adam optimizer at an initial learning rate of 1ğ‘’-5. It was back propagated using the Categorical Cross-Entropy (CCE) loss. For feature extraction, the output of the first dense layer was used to produce a feature vector of 4096 elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">EfficientNetB4</head><p>EfficientNetB4 was trained with a two-step classification block, also comprising two dense blocks with 4096 and 1572 neurons respectively. While the final layers of the architecture are same as the other EfficientNet models, the dropout layer added after the dense layer for this network was experimented between 0.30 and 0.70 and fixed at 0.35 for this model. The model was trained using the Adam optimizer at an initial learning rate of 1ğ‘’-4. Categorical Cross-Entropy (CCE) loss was used to back-propagate, and a 4096-sized vector was extracted as feature-representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">EfficientNetB6</head><p>EfficientNetB6 followed the same approach -two dense blocks with 4096 and 1572 neurons, dropout layers, and CCE loss for back-propagation, and optimized with Adam -with differences only in the hyperparameters. The dropout layer after the dense layer was fixed at a dropout rate of 0.45 after experiments. The initial learning rate was set at 3ğ‘’-2. A 4096-sized vector was extracted as feature-representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5.">ResNeXt101</head><p>The ResNeXt101 architecture was augmented following the same strategy as the aforementioned two i.e. by adding a two-step classification block of 4096 and 1572 neurons respectively. Here, the dropout after experimenting, was set at 0.30. An Adam optimizer along with CCE loss was used for training, with an initial learning rate of 1ğ‘’-3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Loss, Metrics, Activation and Optimizer Used</head><p>The specific details of the loss functions, activation functions, optimizer, and evaluation metrics used during the experiments are laid out in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Adam Optimizer</head><p>Adam <ref type="bibr" coords="7,117.49,625.56,17.76,10.91" target="#b31">[32]</ref> is a stochastic optimization method which is used on gradient descent and maintains a single learning rate (alpha) throughout training. Adam combines the advantages of the Adaptive Gradient Algorithm and Root Mean Square Propagation. Unlike the Root Mean Square Propagation in which the first moment about the mean is used, Adam uses the average of the second moments about the mean too. In effect, Adam provides an optimization algorithm that can handle sparse gradients on noisy problems, by maintaining a per-parameter learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Categorical Cross Entropy Loss</head><p>The categorical cross entropy is a measure of the difference between two discrete probability distributions. It is calculated using the formula in Equation <ref type="formula" coords="8,354.61,163.39,3.74,10.91" target="#formula_0">1</ref>.</p><formula xml:id="formula_0" coords="8,248.24,185.20,257.74,33.71">ğ¿ğ‘œğ‘ ğ‘  = - ğ‘› âˆ‘ï¸ ğ‘–=1 ğ‘¦ ğ‘– ğ‘™ğ‘œğ‘”ğ‘¡ ğ‘– ,<label>(1)</label></formula><p>where, ğ‘¦ ğ‘– represents the corresponding target value for ğ‘¡ ğ‘– the scalar model output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Softmax Activation</head><p>The Softmax activation function is used at the end of the output layer to produce the posterior probability distribution over all classes, based on equation 2. Softmax is essentially a mathematical function that converts a vector of numbers into a vector of probabilities, where the probabilities of each value are proportional to the relative scale of each value in the vector. In effect, it normalizes the outputs, converting them from weighted sum values into probabilities that sum to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘ ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘§</head><formula xml:id="formula_1" coords="8,287.73,365.93,218.25,25.50">ğ‘– ) = ğ‘’ğ‘¥ğ‘(ğ‘§ ğ‘– ) Î£ ğ‘— ğ‘’ğ‘¥ğ‘(ğ‘§ ğ‘— )<label>(2)</label></formula><p>where, ğ‘§ represents the values from the neurons of the output layer. The exponential acts as the non-linear function. These values are divided by the sum of exponential values to normalize and convert them into probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">F1-Score Metric</head><p>The F1-Score is usually calculated as the harmonic mean of precision and recall. This is concretely expressed in Equation <ref type="formula" coords="8,190.13,488.35,3.74,10.91" target="#formula_2">3</ref>.</p><formula xml:id="formula_2" coords="8,265.50,499.68,240.49,25.50">ğ¹ 1 = 2ğ‘ ğ‘  ğ‘Ÿ ğ‘  ğ‘ ğ‘  + ğ‘Ÿ ğ‘  ,<label>(3)</label></formula><formula xml:id="formula_3" coords="8,266.10,531.59,239.89,25.56">ğ‘ ğ‘  = ğ‘‡ ğ‘ ğ‘‡ ğ‘ + ğ¹ ğ‘<label>(4)</label></formula><formula xml:id="formula_4" coords="8,265.94,564.63,240.04,25.56">ğ‘Ÿ ğ‘  = ğ‘‡ ğ‘ ğ‘‡ ğ‘ + ğ¹ ğ‘›<label>(5)</label></formula><p>where, ğ¹ 1 represents the F1-score, ğ‘ ğ‘  represents precision, ğ‘Ÿ ğ‘  represents recall, ğ‘‡ ğ‘ represents true-positive, ğ¹ ğ‘ represents false-positive and ğ¹ ğ‘› represents false-negative. The contest prescribed macro-averaged F1-Score as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5.">Accuracy Metric</head><p>The accuracy score (Acc) is computed as the ratio of correct predictions to the total number samples. This is expressed in Equation <ref type="formula" coords="9,263.51,121.08,3.74,10.91" target="#formula_5">6</ref>.</p><formula xml:id="formula_5" coords="9,236.57,143.82,269.41,25.56">ğ´ğ‘ğ‘ = ğ‘‡ ğ‘ + ğ‘‡ ğ‘› ğ‘‡ ğ‘ + ğ‘‡ ğ‘› + ğ¹ ğ‘ + ğ¹ ğ‘›<label>(6)</label></formula><p>where, ğ´ğ‘ğ‘ represents the accuracy score, ğ‘‡ ğ‘ represents true-positive, ğ‘‡ ğ‘› represents truenegative, ğ¹ ğ‘ represents false-positive and ğ¹ ğ‘› represents false-negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">XGBoost Ensemble Classifier</head><p>The XGBoost classifier was used for ensembling. While using XGBoost, hyperparameters were tuned <ref type="bibr" coords="9,118.50,253.14,18.07,10.91" target="#b32">[33]</ref> for optimal performance by trial and error. The maximum depth of the tree was set to 32. Increasing this value would make the model more complex and prone to overfitting. Increasing this value would also aggressively consume memory while training the deep tree and thus a lower value of 32 was set.</p><p>It was observed that learning rates higher than 0.10 lead to quick divergence, hence values in the range of 10ğ‘’-3 to 10ğ‘’-5 were used. Grid-search was performed by varying the learning rates in this range, with decision trees in the range of 100 to 1000. Combinations having the least losses were chosen to further tune the tree-level parameters. The maximum depth for the tree is left to be determined based on the training progress of the classifier and is not set strictly. This causes the depth to expand until the leaves are pure (has all samples belonging to the same class) or has reached the threshold of minimum number of samples required to split further. Due to the long-tailed distribution of the data set, some classes may require deeper branches to capture more information from the features. To control overfitting, an upper limit was set on the number of leaves by performing a grid search over values in range of 32 to 256.</p><p>Then, objective parameters helps specify the learning task and corresponding learning objective. Softmax was chosen as the objective function. The classifier was configured for multiclass classification and the number of classes was explicitly set to 1572.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Conclusion</head><p>The country metadata was used as a categorical feature in the ensemble classifier. It showed a strong impact on the classification result. With inclusion of this categorical country metadata, the testing F1-score of the best submission improved from 2.68% to 3.64%. Likewise, the cross-validation accuracy and F1-score for the same model improved from 37.62% and 26.55% to 44.91% and 29.31%, upon inclusion of the geographic data.</p><p>Figure <ref type="figure" coords="9,130.13,596.30,4.97,10.91" target="#fig_1">2</ref> represents the relative importance of the 20 most effective image or metadata features of the 20 most impactful features used to train the ensemble classifier. The feature importance values were normalized and scaled between 0 and 100 to realize the relative impacts. Features named as f1, f2, etc. denote features extracted from the neural networks It is worth mentioning that f0 through f4095 denote features extracted using EfficientNetB0, while f4096 through f8191 represent the ResNet101-extracted features. It is evident that country information has a signification influence on classification.</p><p>After deciding the baseline architectures for ensembling -namely, EfficientNetB6 and ResNeXt101 -based on individual prediction performance, the baseline models were trained towards convergence. Following this, the multiple ensemble models were trained using the gradient boosting classifier using the two baseline models and contextual data, with different hyperparameter settings. The ensemble classifier's performance was improved over several runs, by tuning the hyperparameters of the gradient boosting classifier. The contest prescribed F1-scores macro-averaged across all classes as the evaluation metric. Model runs were evaluated on the given stratum of validation set using this metric. The metrics were evaluated as an average over the five iterations (for 5-fold cross validation) performed in each run during training.</p><p>The top scoring models were then used to perform prediction on the test data and evaluated on the contest website. The top-5 results from this pool of predictions is summarized in Table <ref type="table" coords="10,501.01,539.44,4.98,10.91" target="#tab_1">2</ref> for competition submissions, and in Table <ref type="table" coords="10,278.13,552.99,5.07,10.91" target="#tab_2">3</ref> for post-competition evaluations.</p><p>Our team achieved a training accuracy of 45.78%, validation accuracy of 44.91%. The corresponding model secured an F1-score of 3.64% on the competition's test data. Our team placed 38 ğ‘¡â„ among 51 participating teams. These results are summarized in Table <ref type="table" coords="10,454.65,593.64,37.19,10.91" target="#tab_1">2 below:</ref> The results depict the integration of contextual geographic data for snake species classification in a positive light. Furthermore, the fine-tuning and ensembling of features extracted using multiple neural architectures, and merging contextual data looks promising. Several existing approaches have introduced metadata such as population counts of various species, more location-specific geographic data such as city, state and climatic features such as temperature and humidity. An interesting approach is to employ class-wise probability priors to the neural networks based on such metadata <ref type="bibr" coords="11,242.57,263.55,16.25,10.91" target="#b33">[34]</ref>.</p><p>On account of insufficient computing resources to complete all model training experiments in time for the SnakeCLEF's large snake dataset, results were submitted before complete model convergence. Post the deadline, significant improvements were observed in classification accuracy, particularly with Submission Numbers 1 and 4 (refer to Table <ref type="table" coords="11,405.64,317.75,4.12,10.91" target="#tab_1">2</ref>) were observed upon further training the baseline neural networks, and tuning hyperparameters of the gradient boosting ensemble classifier. A summary of post-competition improvements in prediction results during the working notes submission phase of SnakeCLEF-2022 is tabulated in Table <ref type="table" coords="11,500.02,358.39,3.73,10.91" target="#tab_2">3</ref>. Our best post-competition result is at par with the 11 ğ‘¡â„ best of the 51 contesting teams. It is apparent that subsequent hyperparameter tuning and training of the baseline networks, as well as the boosting classifier, have been effective in improving the model performance. Hence, the team believes and strongly advocates that for data-intensive and high-complexity image classification tasks that are commonly released as LifeCLEF tasks, the adopted ensembling approach using gradient boosting is an impactful option. We further conjecture that training the individual models to convergence, and subsequently applying the boosting ensembler with hyperparameter tuning will culminate in a far more superior classifier performance, that exhausts the enitrety of the proposed architectures' and methodology's potential. Furthermore, approaches involving input image resolution variations, usage of alternative pre-trained weights <ref type="bibr" coords="12,89.29,100.52,16.23,10.91" target="#b34">[35]</ref>, as well as the inclusion of custom training layers to the frozen base model when transfer learning <ref type="bibr" coords="12,130.05,114.06,18.07,10.91" target="#b35">[36]</ref> can greatly improve the quality of feature extraction. In this participation, for reasons of time constraints already described, the application of image preprocessing techniques could not be experimented with exhaustively and systematically. The authors further speculate that such an exercise would be of significance in improving the overall model performance, particularly that of the baseline neural networks used for feature extraction <ref type="bibr" coords="12,428.33,168.26,16.25,10.91" target="#b36">[37]</ref>.</p><p>Other approaches, such as varying input image resolutions and employing alternative pretrained weights <ref type="bibr" coords="12,164.77,195.36,18.07,10.91" target="#b34">[35]</ref> as well as including custom training layers to the frozen base model, in addition to the classification block <ref type="bibr" coords="12,262.72,208.91,16.41,10.91" target="#b35">[36]</ref>, can improve classifier performance. Finally, the application of image preprocessing techniques can play a significant role in improving the feature extraction ability of convolutional neural networks <ref type="bibr" coords="12,353.40,236.01,16.25,10.91" target="#b36">[37]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,89.29,243.01,416.69,8.93;6,89.29,254.97,416.69,8.96;6,89.29,266.97,204.67,8.87;6,89.29,84.19,416.70,151.40"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Prediction workflow used for the classification of snake species using an gradient boosted ensembling classifier. Note: Model architectures depicted are illustrative only and NOT accurate representations of the underlying network design.</figDesc><graphic coords="6,89.29,84.19,416.70,151.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,89.29,314.47,416.69,8.93;10,89.29,326.47,295.21,8.87;10,89.29,84.19,416.68,223.69"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Relative importance on a scale of 0-100 of the 20 most impactful features used to train the classifier. The first bar represents feature importance of country feature.</figDesc><graphic coords="10,89.29,84.19,416.68,223.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.99,367.65,417.00,115.47"><head>Table 1</head><label>1</label><figDesc>Model training parameters used to train each of the convolutional neural networks used for this classification task.</figDesc><table coords="6,159.87,409.00,275.53,74.12"><row><cell>Parameter</cell><cell cols="4">Optimizer Learning rate Batch size Epochs</cell></row><row><cell>ResNet101</cell><cell>Adam</cell><cell>3ğ‘’-4</cell><cell>32</cell><cell>40</cell></row><row><cell>EfficientNetB0</cell><cell>Adam</cell><cell>1ğ‘’-5</cell><cell>32</cell><cell>40</cell></row><row><cell>EfficientNetB4</cell><cell>Adam</cell><cell>1ğ‘’-4</cell><cell>32</cell><cell>40</cell></row><row><cell>EfficientNetB6</cell><cell>Adam</cell><cell>3ğ‘’-2</cell><cell>32</cell><cell>40</cell></row><row><cell>ResNeXt101</cell><cell>Adam</cell><cell>1ğ‘’-3</cell><cell>32</cell><cell>30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,88.99,90.49,378.96,105.74"><head>Table 2</head><label>2</label><figDesc>Performance metrics of the 5 best submissions. F1-Scores are macro-averaged across classes.</figDesc><table coords="11,144.58,122.10,303.62,74.12"><row><cell cols="4">Submission# Validation Accuracy Validation F1-Score Test F1-Score</cell></row><row><cell>1</cell><cell>44.91</cell><cell>29.31</cell><cell>3.64</cell></row><row><cell>2</cell><cell>40.72</cell><cell>26.13</cell><cell>3.38</cell></row><row><cell>3</cell><cell>35.11</cell><cell>25.27</cell><cell>2.53</cell></row><row><cell>4</cell><cell>35.18</cell><cell>25.46</cell><cell>2.15</cell></row><row><cell>5</cell><cell>34.97</cell><cell>22.18</cell><cell>2.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,88.99,399.30,416.99,117.69"><head>Table 3</head><label>3</label><figDesc>Performance metrics of the 5 best submissions post-competition, ordered best to worst. F1-Scores are macro-averaged across classes.</figDesc><table coords="11,101.31,442.87,390.15,74.12"><row><cell cols="5">Submission# Training Accuracy Validation Accuracy Validation F1-Score Test F1-Score</cell></row><row><cell>1</cell><cell>89.11</cell><cell>87.13</cell><cell>52.04</cell><cell>51.39</cell></row><row><cell>2</cell><cell>86.01</cell><cell>85.39</cell><cell>51.42</cell><cell>49.94</cell></row><row><cell>3</cell><cell>80.86</cell><cell>80.44</cell><cell>46.13</cell><cell>46.11</cell></row><row><cell>4</cell><cell>79.81</cell><cell>78.92</cell><cell>47.19</cell><cell>45.52</cell></row><row><cell>5</cell><cell>78.64</cell><cell>77.14</cell><cell>44.11</cell><cell>42.18</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to express their gratitude to the <rs type="institution">Machine Learning Research Group (MLRG)</rs>, <rs type="institution">Department of Computer Science and Engineering, Sri Sivasubramaniya Nadar College of Engineering, Chennai, India</rs> (https://www.ssn.edu.in/) for providing the GPU resources for model training and testing.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="12,112.66,393.91,393.33,10.91;12,112.66,407.46,393.33,10.91;12,112.66,421.01,159.39,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,292.93,393.91,107.55,10.91;12,429.01,393.91,76.98,10.91;12,112.66,407.46,165.68,10.91">Automated snake species identification on a global scale</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>HrÃºz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,300.64,407.46,205.35,10.91;12,112.66,421.01,128.70,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of SnakeCLEF</note>
</biblStruct>

<biblStruct coords="12,112.66,434.55,394.53,10.91;12,112.66,448.10,394.53,10.91;12,112.66,461.65,393.33,10.91;12,112.66,475.20,393.33,10.91;12,112.66,488.75,353.54,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,198.52,461.65,307.47,10.91;12,112.66,475.20,247.50,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,383.00,475.20,122.99,10.91;12,112.66,488.75,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,502.30,394.53,10.91;12,112.66,515.85,393.33,10.91;12,112.66,529.40,393.33,10.91;12,112.66,542.95,168.28,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,185.62,515.85,320.37,10.91;12,112.66,529.40,208.65,10.91">Lifeclef 2022 teaser: An evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,343.19,529.40,162.80,10.91;12,112.66,542.95,38.01,10.91">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,556.50,395.01,10.91;12,112.66,570.05,394.53,10.91;12,112.66,583.60,393.33,10.91;12,112.33,597.15,397.81,10.91;12,112.36,613.14,150.76,7.90" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,310.65,570.05,196.54,10.91;12,112.66,583.60,315.92,10.91">for the Million Death Study Collaborators, Snakebite mortality in india: A nationally representative mortality survey</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mohapatra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Suraweera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Jotkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">S</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jha</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pntd.0001018</idno>
		<ptr target="https://doi.org/10.1371/journal.pntd.0001018.doi:10.1371/journal.pntd.0001018" />
	</analytic>
	<monogr>
		<title level="j" coord="12,436.14,583.60,69.84,10.91;12,112.33,597.15,77.36,10.91">PLOS Neglected Tropical Diseases</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,624.24,361.77,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="12,201.80,624.24,240.71,10.91">Snake detection and classification using deep learning</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sinnott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,637.79,394.53,10.91;12,112.66,651.34,312.93,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,112.66,651.34,98.54,10.91">Snakebite envenoming</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>GutiÃ©rrez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Calvete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Warrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,220.38,651.34,141.57,10.91">Nature reviews Disease primers</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,86.97,395.17,10.91;13,112.66,100.52,394.52,10.91;13,112.66,114.06,393.33,10.91;13,112.66,127.61,393.33,10.91;13,112.66,141.16,299.78,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,227.05,86.97,280.78,10.91;13,112.66,100.52,193.36,10.91">Efficientnets and vision transformers for snake species identification using image and location information</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,176.85,114.06,329.13,10.91;13,112.66,127.61,93.10,10.91">Proceedings of the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="13,124.27,141.16,151.15,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum<address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">September 21st -to -24th, 2021. 2936. 2021</date>
			<biblScope unit="page" from="1477" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,154.71,393.33,10.91;13,112.66,168.26,267.36,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="13,431.02,154.71,74.97,10.91;13,112.66,168.26,192.46,10.91">Automatic snake classification using deep learning algorithm</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kalinathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Balasundaram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Bathala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Mukesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,181.81,397.48,10.91;13,112.66,197.80,26.14,7.90" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="13,163.34,181.81,148.17,10.91">Snake classification from images</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<idno type="DOI">10.7287/peerj.preprints.2867</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,208.91,393.33,10.91;13,112.66,222.46,255.25,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,242.41,208.91,263.57,10.91;13,112.66,222.46,102.77,10.91">A comparative study on image-based snake identification using machine learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rajabizadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rezghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,223.74,222.46,75.46,10.91">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,236.01,393.32,10.91;13,112.66,249.56,295.27,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,198.80,236.01,253.04,10.91">A review: deep learning technique for image classification</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,459.67,236.01,46.32,10.91;13,112.66,249.56,242.67,10.91">ACCENTS transactions on image processing and computer vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,263.11,393.33,10.91;13,112.66,276.66,123.93,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Borsodi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Papp</surname></persName>
		</author>
		<title level="m" coord="13,202.64,263.11,303.34,10.91;13,112.66,276.66,92.01,10.91">Incorporation of object detection models and location data into snake species classification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,290.20,393.33,10.91;13,112.66,303.75,103.57,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="13,302.26,290.20,203.73,10.91;13,112.66,303.75,71.65,10.91">A deep learning method for visual recognition of snake species</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,317.30,394.62,10.91;13,112.66,330.85,262.85,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">G</forename><surname>Coca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">T</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">P</forename><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<title level="m" coord="13,391.31,317.30,115.97,10.91;13,112.66,330.85,230.93,10.91">Uaic-ai at snakeclef 2021: Impact of convolutions in snake species recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,344.40,393.33,10.91;13,112.33,357.95,29.19,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Desingu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palaniappan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<title level="m" coord="13,283.29,344.40,222.70,10.91">Snake species classification using transfer learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,371.50,393.33,10.91;13,112.66,385.05,395.01,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="13,208.09,371.50,77.17,10.91;13,309.07,371.50,196.92,10.91;13,112.66,385.05,263.15,10.91">Handbook of research on machine learning applications and trends: algorithms, methods, and techniques</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Torrey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shavlik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>IGI global</publisher>
			<biblScope unit="page" from="242" to="264" />
		</imprint>
	</monogr>
	<note>Transfer learning</note>
</biblStruct>

<biblStruct coords="13,112.66,398.60,394.53,10.91;13,112.28,412.15,183.08,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,240.23,398.60,263.67,10.91">Ensembling neural networks: many could be better than all</title>
		<author>
			<persName coords=""><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,112.28,412.15,94.07,10.91">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="239" to="263" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,425.70,393.33,10.91;13,112.66,439.25,265.83,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>De CastaÃ±eda</surname></persName>
		</author>
		<title level="m" coord="13,334.93,425.70,171.06,10.91;13,112.66,439.25,233.91,10.91">Overview of snakeclef 2021: Automatic snake species identification with country-level focus</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,452.79,374.73,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,155.93,452.79,169.95,10.91">Bilinear interpolation of digital images</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,334.60,452.79,73.92,10.91">Ultramicroscopy</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="201" to="204" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,466.34,395.17,10.91;13,112.66,479.89,393.32,10.91;13,112.66,493.44,325.92,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,302.02,466.34,205.81,10.91;13,112.66,479.89,171.71,10.91">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,307.43,479.89,198.55,10.91;13,112.66,493.44,237.36,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,506.99,394.53,10.91;13,112.66,520.54,308.05,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,346.06,506.99,156.55,10.91">Backpropagation: The basic theory</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Golden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,112.66,520.54,252.21,10.91">Backpropagation: Theory, architectures and applications</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,534.09,394.61,10.91;13,112.66,547.64,314.10,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="13,253.11,534.09,199.28,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1512.03385</idno>
		<ptr target="https://arxiv.org/abs/1512.03385.doi:10.48550/ARXIV.1512.03385" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,561.19,393.33,10.91;13,112.66,574.74,217.61,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="13,308.43,561.19,130.70,10.91">Training very deep networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,449.80,561.19,56.18,10.91;13,112.66,574.74,172.82,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,588.29,393.33,10.91;13,112.33,601.84,370.70,10.91" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="13,187.42,588.29,318.57,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1905.11946</idno>
		<ptr target="https://arxiv.org/abs/1905.11946.doi:10.48550/ARXIV.1905.11946" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,615.39,393.33,10.91;13,112.66,628.93,287.29,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="13,187.31,615.39,229.74,10.91">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,446.71,615.39,59.28,10.91;13,112.66,628.93,146.65,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10096" to="10106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,642.48,395.17,10.91;13,112.66,656.03,397.48,10.91;13,112.66,672.02,32.07,7.90" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="13,289.75,642.48,218.07,10.91;13,112.66,656.03,54.60,10.91">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1611.05431</idno>
		<ptr target="https://arxiv.org/abs/1611.05431.doi:10.48550/ARXIV.1611.05431" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,86.97,394.53,10.91;14,112.28,100.52,395.39,10.91;14,112.66,114.06,161.38,10.91" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1409.4842</idno>
		<ptr target="https://arxiv.org/abs/1409.4842.doi:10.48550/ARXIV.1409.4842" />
		<title level="m" coord="14,178.24,100.52,137.69,10.91">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,127.61,393.58,10.91;14,112.33,141.16,42.07,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="14,207.96,127.61,167.73,10.91">Gradient boosting machines, a tutorial</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Natekin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,383.52,127.61,114.97,10.91">Frontiers in neurorobotics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,154.71,393.33,10.91;14,112.66,168.26,394.03,10.91;14,112.41,181.81,284.14,10.91" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Xgboost</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="https://doi.org/10.1145%2F2939672.2939785.doi:10.1145/2939672.2939785" />
		<title level="m" coord="14,270.61,154.71,235.38,10.91;14,112.66,168.26,240.76,10.91">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,195.36,393.33,10.91;14,112.66,208.91,394.53,10.91;14,112.66,222.46,103.61,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="14,346.64,195.36,159.35,10.91;14,112.66,208.91,67.28,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,228.08,208.91,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,236.01,394.53,10.91;14,112.66,249.56,393.33,10.91;14,112.48,263.11,261.79,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="14,307.23,249.56,176.03,10.91">Scikit-learn: Machine learning in python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,492.05,249.56,13.94,10.91;14,112.48,263.11,167.70,10.91">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,276.66,395.01,10.91;14,112.66,290.20,246.16,10.91" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="14,232.29,276.66,162.24,10.91">A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1412.6980</idno>
		<ptr target="https://arxiv.org/abs/1412.6980.doi:10.48550/ARXIV.1412.6980" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,303.75,393.33,10.91;14,112.66,317.30,393.33,10.91;14,112.33,330.85,29.19,10.91" xml:id="b32">
	<monogr>
		<title level="m" type="main" coord="14,418.54,303.75,87.45,10.91;14,112.66,317.30,248.34,10.91">Benchmarking and optimization of gradient boosting decision tree algorithms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Anghel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Parnell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">De</forename><surname>Palma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Pozidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04559</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,344.40,393.61,10.91;14,112.66,357.95,393.53,10.91;14,112.39,371.50,231.07,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="14,364.18,344.40,142.08,10.91;14,112.66,357.95,152.79,10.91">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,288.06,357.95,218.14,10.91;14,112.39,371.50,133.15,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,385.05,394.52,10.91;14,112.66,398.60,393.33,10.91;14,112.66,412.15,393.33,10.91;14,112.66,425.70,394.53,10.91;14,112.66,439.25,80.57,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="14,236.07,398.60,269.91,10.91;14,112.66,412.15,307.48,10.91">Overview of lifeclef 2020: a system-oriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,446.71,412.15,59.28,10.91;14,112.66,425.70,346.66,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="342" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,452.79,394.53,10.91;14,112.66,466.34,393.33,10.91;14,112.66,479.89,308.59,10.91" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="14,112.66,466.34,393.33,10.91;14,112.66,479.89,136.12,10.91">Multispecies bioacoustic classification using transfer learning of deep convolutional neural networks with pseudo-labeling</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lebien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Campos-Cerqueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Dodhia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Ferres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Velev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Aide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,257.28,479.89,80.93,10.91">Applied Acoustics</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page">107375</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,493.44,395.17,10.91;14,112.26,506.99,393.73,10.91;14,112.66,520.54,304.70,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="14,207.01,493.44,300.82,10.91;14,112.26,506.99,24.38,10.91">Preprocessing for image classification by convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sudeep</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,179.83,506.99,326.15,10.91;14,112.66,520.54,179.69,10.91">IEEE International Conference on Recent Trends in Electronics, Information &amp; Communication Technology (RTEICT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1778" to="1781" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
