<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.81,84.32,419.92,16.17;1,89.29,106.24,338.61,16.17">st Place Solution for FungiCLEF 2022 Competition: Fine-grained Open-set Fungi Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,136.79,58.70,10.37"><forename type="first">Zihua</forename><surname>Xiong</surname></persName>
							<email>xiongzihua.xzh@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">MYbank</orgName>
								<orgName type="laboratory" key="lab2">Ant Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,159.53,136.79,65.81,10.37"><forename type="first">Yumeng</forename><surname>Ruan</surname></persName>
							<email>ruanyumeng.rym@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">MYbank</orgName>
								<orgName type="laboratory" key="lab2">Ant Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.06,136.79,38.97,10.37"><forename type="first">Yifei</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">MYbank</orgName>
								<orgName type="laboratory" key="lab2">Ant Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.03,136.79,49.94,10.37"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">MYbank</orgName>
								<orgName type="laboratory" key="lab2">Ant Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.63,136.79,44.49,10.37"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">MYbank</orgName>
								<orgName type="laboratory" key="lab2">Ant Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,405.78,136.79,53.46,10.37"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">MYbank</orgName>
								<orgName type="laboratory" key="lab2">Ant Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,150.73,46.16,10.37"><forename type="first">Bing</forename><surname>Han</surname></persName>
							<email>hanbing@antgroup.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">MYbank</orgName>
								<orgName type="laboratory" key="lab2">Ant Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.81,84.32,419.92,16.17;1,89.29,106.24,338.61,16.17">st Place Solution for FungiCLEF 2022 Competition: Fine-grained Open-set Fungi Recognition</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">56AA3A7C37579CC3AF6CC354932118D5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>FungiCLEF</term>
					<term>fungi recognition</term>
					<term>long tail</term>
					<term>open-set</term>
					<term>fine-grained</term>
					<term>classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our method for Fine-Grained Fungi Recognition at FungiCLEF 2022, which aims to recognize the fungi belonging to 1,604 known species and many other unknown species, termed as a fine-grained, open-set machine learning problem. For the purpose of building a strong close-set classifier, we taken MetaFormer [1] and ConvNext [2] as our strong baseline, then we applied hyper-parameter tuning and some modern training techniques to improve it. To deal with long tailed class distribution problem, we adapt the Seesaw Loss [3] to balance the training process between head classes and tail classes. Furthermore, to avoid tail categories being misclassified as open-set categories, we intuitively design a post process to alleviate the confusion. As a common practice, test time augmentations and model ensemble are used. With all these techniques together, our method achieves superior mean ğ‘“ 1 score on test set, that is 83.78% on public leaderboard, and 80.43% on private leaderboard which is the 1st place among the participators. The code will be made available at https://github.com/guoshengcv/fgvc9_fungiclef.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>FungiCLEF 2022 <ref type="bibr" coords="1,168.53,429.96,12.71,9.46" target="#b3">[4]</ref> is a competition held jointly by CLEF 2022 conference <ref type="bibr" coords="1,426.75,429.96,11.80,9.46" target="#b4">[5,</ref><ref type="bibr" coords="1,441.27,429.96,9.08,9.46" target="#b5">6]</ref> and FGVC9 workshop at CVPR 2022 conference. The competition release the train data based on Danish Fungi 2020 <ref type="bibr" coords="1,145.96,457.05,11.74,9.46" target="#b6">[7]</ref>, which aims at fine-grained fungi recognition. It includes both image and meta-information such as habitat, substrate, time, longitude, latitude etc, and contains 295,938 samples belonging to 1,604 species. The competition also release test data which contains 59,420 observations with 118,676 images and 3,134 species, it includes meta-information as train data but miss some attributes such as longitude and latitude. After data analyze, we found the category distribution of the train dataset is long-tailed, as a result, in this work we tackle the competition as a fine-grained, long-tailed, open-set classification task.</p><p>Different from common classification tasks that try to distinguish objects with large inter-class variations, Fine-Grained Visual Classification (FGVC) aims at capturing the subtle difference within similar categories, such as differentiating bird species, car types, etc. It is acknowledged that FGVC is a challenging task due to small inter-class variations and large intra-class variations.</p><p>Numerous methods for FGVC are mainly focused on modeling discriminative regions, such as part-based model <ref type="bibr" coords="2,182.89,101.60,11.94,9.46" target="#b7">[8,</ref><ref type="bibr" coords="2,198.22,101.60,8.24,9.46" target="#b8">9,</ref><ref type="bibr" coords="2,209.85,101.60,14.62,9.46" target="#b9">10]</ref> and attention-based model <ref type="bibr" coords="2,349.89,101.60,17.40,9.46" target="#b10">[11,</ref><ref type="bibr" coords="2,370.68,101.60,13.05,9.46" target="#b11">12]</ref>. Recently, inspired by the fact that human experts use meta-information to distinguish visually similar species, there are many works <ref type="bibr" coords="2,145.88,128.69,17.33,9.46" target="#b12">[13,</ref><ref type="bibr" coords="2,165.93,128.69,13.66,9.46" target="#b13">14,</ref><ref type="bibr" coords="2,182.32,128.69,8.21,9.46" target="#b0">1,</ref><ref type="bibr" coords="2,193.25,128.69,14.58,9.46" target="#b14">15]</ref> utilize additional information to enhance fine-grained classification performance. Among them, Metaformer <ref type="bibr" coords="2,275.04,142.24,12.87,9.46" target="#b0">[1]</ref> is the state-of-the-art work proposed recently, it is a hybrid framework that convolution and transformer are both used. In this work, we taken Metaformer as a strong baseline and improve it progressively.</p><p>Recently, transformers have leading the research in the filed of computer vision, starting from Vision Transformer <ref type="bibr" coords="2,178.96,196.44,16.88,9.46" target="#b15">[16]</ref>, there are many various transformer backbones achieve SoTA performance in a wild range of vision tasks, such as Swin Transformer <ref type="bibr" coords="2,373.49,209.99,16.72,9.46" target="#b16">[17]</ref>, CSwin Transformer <ref type="bibr" coords="2,486.45,209.99,16.72,9.46" target="#b17">[18]</ref>, etc. On the other side, ConvNext <ref type="bibr" coords="2,235.96,223.54,12.75,9.46" target="#b1">[2]</ref> is a pure convolution backbone, it applies modern training techniques, macro and micro design of the network architecture, achieves comparable results with transformers. In this work, in order to obtain models with distinct difference and enhance the performance of model ensemble, we taken ConvNext as another baseline backbone.</p><p>In real world scenarios, the distribution of the categories is often long-tailed. It is well known that major class will dominate the training process and suppress the performance of tail class. Many works designed loss function to deal with the problem of long-tail classification, such as Adaptive Class Suppression Loss <ref type="bibr" coords="2,253.10,318.38,16.88,9.46" target="#b18">[19]</ref>, Equalization loss <ref type="bibr" coords="2,356.60,318.38,16.88,9.46" target="#b19">[20]</ref>, Seesaw Loss <ref type="bibr" coords="2,440.77,318.38,11.73,9.46" target="#b2">[3]</ref>, etc. In our method, we utilize Seesaw Loss to dynamically balance the training process between head classes and tail classes.</p><p>For practical application, it usually faces with open-set recognition challenge, the classifier should not only recognize the classes which have been seen during training, but also notice that if a instance comes from unknown classes. Motivated by <ref type="bibr" coords="2,324.10,386.13,16.57,9.46" target="#b20">[21]</ref>, in this work, we trained the model on known classes to obtain a good close-set classifier, and determine whether a instance belonging to open-set based on the maximum value of it's logit score vector. Furthermore, to avoid tail categories being misclassified as open-set categories, we intuitively design a post process to alleviate the confusion.</p><p>Our main contributions in FungiCLEF 2022 competition can be summarized as follows:</p><p>â€¢ We take Metaformer and ConvNext as our strong baseline, then we apply hyper-parameter tuning and some modern training techniques to improve it's performance on fungi dataset. â€¢ We find category distribution of the fungi dataset is long-tailed, thus we adapt the Seesaw Loss to balance the training process between head classes and tail classes, which lift up the baseline model performance. â€¢ To avoid tail categories being misclassified as open-set categories, we intuitively design a post process to alleviate the confusion. â€¢ Detailed ablation experiments have been done. With the techniques above, we achieve superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach</head><p>Motivated by <ref type="bibr" coords="2,148.06,651.79,16.57,9.46" target="#b20">[21]</ref>, we divide the fine-grained, open-set recognition problem into two parts. Firstly we are attended to lift up the close-set recognition performance, including network architecture, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview of the Approach</head><p>As shown in Figure <ref type="figure" coords="3,184.48,455.80,4.17,9.46" target="#fig_0">1</ref>, we taken MetaFormer <ref type="bibr" coords="3,302.35,455.80,12.86,9.46" target="#b0">[1]</ref> and ConvNext <ref type="bibr" coords="3,388.98,455.80,12.86,9.46" target="#b1">[2]</ref> as our initial baseline.</p><p>MetaFormer is a hybrid framework that combines convolution and vision transformer, it also proposes a simple and effective solution for adding meta-information using the transformer layer.</p><p>In our approach, we directly use MetaFormer and modify the input of meta-information. We perform the mapping</p><formula xml:id="formula_0" coords="3,187.97,507.36,137.25,15.05">[ğ‘šğ‘œğ‘›ğ‘¡â„, ğ‘‘ğ‘ğ‘¦] â†’ [ğ‘ ğ‘–ğ‘›( 2ğœ‹ğ‘šğ‘œğ‘›ğ‘¡â„<label>12</label></formula><formula xml:id="formula_1" coords="3,326.41,507.36,63.69,15.05">), ğ‘ğ‘œğ‘ ( 2ğœ‹ğ‘šğ‘œğ‘›ğ‘¡â„<label>12</label></formula><p>), ğ‘ ğ‘–ğ‘›( 2ğœ‹ğ‘‘ğ‘ğ‘¦ 31 ), ğ‘ğ‘œğ‘ ( 2ğœ‹ğ‘‘ğ‘ğ‘¦ 31 )] to encode temporal information. We use one-hot encoding to encode category meta-information such as countryCode, Substrate and Habitat. To enhance the model diversity for later model ensemble, we use ConvNext as another network architecture. We apply hyper-parameter tuning to improve their performance, and we will illustrate the ablation studies in Sec 3.2 to show the progressive process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Loss for Long Tail Classification</head><p>It is known that instances from head categories dominate the training process, the biased learning lead to misclassification for tail categories. In this work, we borrow the idea from Seesaw Loss <ref type="bibr" coords="3,493.41,642.11,12.58,9.46" target="#b2">[3]</ref> to alleviate this problem. During training process, Seesaw Loss dynamically balances positive and negative gradients for each category with a dynamic factor, it reformulate the Cross Entropy loss as</p><formula xml:id="formula_2" coords="4,229.70,97.35,272.78,66.06">ğ¿ ğ‘ ğ‘’ğ‘’ğ‘ ğ‘ğ‘¤ (ğ‘§) = - ğ¶ âˆ‘ï¸ ğ‘–=1 ğ‘¦ ğ‘– log(Ì‚ï¸€ ğ‘ ğ‘– ), with Ì‚ï¸€ ğ‘ ğ‘– = ğ‘’ ğ‘§ ğ‘– âˆ‘ï¸€ ğ¶ ğ‘—Ì¸ =ğ‘– ğ‘† ğ‘–ğ‘— ğ‘’ ğ‘§ ğ‘— + ğ‘’ ğ‘§ ğ‘– . (<label>1</label></formula><formula xml:id="formula_3" coords="4,502.48,125.46,4.24,9.46">)</formula><p>where ğ‘¦ is the category label, usually represented by one-hot, ğ‘§ is the outputs of model, Ì‚ï¸€ ğ‘ is the probability calculated by ğ‘†ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘§) with a dynamic factor ğ‘†. For more detail, please refer to Seesaw Loss <ref type="bibr" coords="4,159.12,198.76,11.59,9.46" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Post Process</head><p>The post process is intuitively designed based on several observations, and applied on final ensemble results. In order to be as clear as possible the process, we write the post process in python-style pseudocode, refer to the Algorithm 1. We detailed it in the following.</p><p>Threshold for selecting open-set samples. It is acknowledged that the predict confidence score of open-set samples are relatively low. This phenomenon can be used as the criterion for their recognition. Specifically, we draw the logit (the direct output of the model) frequency distribution of both validation set and test set, shown in Figure <ref type="figure" coords="4,370.39,330.87,4.17,9.46" target="#fig_1">2</ref>. As the open set samples are only contained in the test set, we can compare the low confidence areas of the two distributions to approximately get the logit threshold for open set samples. For example, we can set threshold to 5 as an approximate for the Alleviate the influence of microscopy images. In the test set, we find that one test sample may contains several images as shown in Figure <ref type="figure" coords="4,288.84,507.01,4.17,9.46" target="#fig_3">3</ref>. For such case, we average the model outputs of them to get the confidence and use argmax to get predicted category. During the above process, we also find that there are images showing huge visual discrepancy with the majority. Specifically, we find that some test samples contain microscopy images such as sample_c shown in Figure <ref type="figure" coords="4,131.03,561.20,4.01,9.46" target="#fig_3">3</ref>, these microscopy images tend to produce low confidence due to little training data, it will influence the naive average strategy. To cope with this problem, we delicately design the post process. As shown in Algorithm 1, from line 29 to line 35, if the maximum logit of averaged outputs is lower than a certain threshold, we will look into the maximum logit of all images from a test sample, if it is greater than a certain threshold, we will get the corresponding category as the prediction. In this case, we think that the test samples with low averaged outputs may be caused by containing too many microscopy images, the high confidence prediction of one image from test sample is sufficient to infer the category, and the test sample should not be considered as open-set categories.  We found many test samples contain several images, we predict the category of the test sample by averaging the model outputs of them. We also notice that some test samples such as sample_c contains one image pictured from natural environment, and the other images come from microscope view, it will disturb the average results.</p><p>Distinguish tail categories and open-set categories. We put the tail categories that are never been predicted by the model into hard tail categories, we argue that there are many hard tail categories misclassified as open-set categories. To deal with the problem above, we design the post process, refer to the Algorithm 1. From line 18 to line 27, to avoid the misclassification, we mining hard tail categories from top-3 predictions with low threshold filtering.</p><p>Algorithm 1 Pseudocode of Post Process in a python-like style. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section, we first elaborate on the implementation and training details. Then we introduce ablation studies on loss functions and bag of training settings. Then we list some other attempts and it's results. Finally we study on different test time augmentations, and show the effectiveness of post process for tail categories recognition and open-set categories recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation Details</head><p>We trained the model on Danish Fungi 2020 dataset <ref type="bibr" coords="6,317.02,531.24,12.69,9.46" target="#b6">[7]</ref> which contains 295,938 training images belonging to 1,604 species observed mostly in Denmark, the dataset has been divided into train and validation set. We use both train and validation for training in most settings. We report the results on test set which contains 59,420 observations with 118,676 images and 3,134 species. The test set is divided into 2 parts, the public set contains 20% of the data, the private set contains 80% of the data. As the performance of open-set recognition affects the mean ğ‘“ 1 score, to make relative fair comparison in ablation studies, based on the observation illustrated in Sec 2.3, we utilize threshold to select âˆ¼1000 samples which have low confidence score as open-set samples for most experiments. We conduct all the experiments with Tesla V100 (32G). We use AdamW optimizer with cosine learning scheduler, initialize the learning rate to 5ğ‘’ -5 and scale it by batch size, we follow most of the augmentation and regularization strategies of <ref type="bibr" coords="6,409.24,666.73,18.17,9.46" target="#b16">[17]</ref> in training.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ablation Studies</head><p>As shown in Table <ref type="table" coords="7,171.18,394.38,4.01,9.46" target="#tab_1">1</ref>, we train MetaFormer-0 for 100 epochs, with Soft Target Cross Entropy loss and mixup <ref type="bibr" coords="7,138.50,407.93,18.20,9.46" target="#b21">[22]</ref> augmentation to build our baseline. For ablation studies, it should be noted that except the parameter to be compared, there are little other not consistent parameters, such as the accumulate steps in last row in Table <ref type="table" coords="7,253.28,435.03,4.09,9.46">5</ref>, we argue that it will not affects the conclusion largely.</p><p>Losses. As shown in Table <ref type="table" coords="7,210.96,448.58,4.15,9.46" target="#tab_2">2</ref>, we compare different losses with several common augmentation techniques. Specifically, we compare cross entropy loss with either mixup or label smoothing <ref type="bibr" coords="7,487.96,462.13,18.03,9.46" target="#b22">[23]</ref> and Seesaw loss. They are all devoted to alleviate the long-tail problem in training. It is found that label smooth converges faster than mixup in our experiments. The best performance is achieved when Seesaw loss is adopted. Batch size. Training epochs. We found the longer training epochs will not definitely improve the performance. As shown in Table <ref type="table" coords="7,209.37,597.62,4.16,9.46">5</ref>, for MetaFormer-0 and MetaFormer-2, it is consistent that proper epochs is essential for better result. Following this line, we did not train models with dozens of epochs such as 100 epochs and above. Image size. Usually, training with larger image size improves the overall performance, especially for fine-grained tasks. We use the 384 as the baseline image size and try several other larger settings. As shown in Table <ref type="table" coords="7,209.04,665.37,4.01,9.46">6</ref>, the larger image size 448 does not consistently bring improvements   Pseudo label. After training models with various settings, we use model ensemble to get the best model currently, and take the model predictions on test samples as their label. We select top âˆ¼ 50% test samples by their confidence score. We trained MetaFormer-2 and ConvNext-large with train+val+pseudo, the results are listed in Table <ref type="table" coords="9,325.00,460.34,4.09,9.46" target="#tab_6">9</ref>. For open-set recognition, we intuitively select samples with lower confidence score as open-set samples. As shown in Table <ref type="table" coords="9,220.73,646.64,9.27,9.46" target="#tab_9">12</ref>, by increasing open-set sample from âˆ¼ 1000 to âˆ¼ 1500, we improve mean ğ‘“ 1 score from 83.26% to 83.50% on public test set, from 79.38% to 79.60% on private test set. It demonstrates the effectiveness to select a proper open-set threshold. Compare the average ensemble (v3) with average ensemble (v2), v3 improves the ensemble performance, the only difference between them is that v3 contains the models trained with pseudo label. It is acknowledged that the tail categories tend to have lower confidence score compared to head categories, so the tail categories are easier to be misclassified as head categories or wrongly identified as open-set categories. As shown in Table <ref type="table" coords="10,318.11,155.79,9.07,9.46" target="#tab_9">12</ref>, with our post process for tail categories applied on average ensemble (v3), which termed as average ensemble (v4), the mean ğ‘“ 1 score improves a lot on both public test set and private test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Test Time Augmentation and Post Process</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we introduce our solution for FungiCLEF 2022 competition. To solve this challenging fine-grained, open-set problem, we try a bunch of techniques, such as different network baseline, hyper-parameters tuning, modern training techniques, loss for long tail recognition and specially designed post process. With these endeavours we achieved 1st place among the participators. The experimental results show the progressive process for single model, and the effectiveness of test time augmentation and post process for tail categories. For future work, it is valuable to study the method that fuse meta-information and visual information for Fine-Grained Visual Classification, and the problem of distinguish between tail categories and open-set categories is also worth exploring.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,326.76,416.69,9.35;3,89.29,338.82,337.98,9.22;3,89.29,84.19,417.48,232.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall of our apporach. We trained MetaFormer and Convnext with various settings, during testing process, model snsemble and post process are used.</figDesc><graphic coords="3,89.29,84.19,417.48,232.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,227.05,371.51,278.94,9.46;4,89.29,384.71,416.70,9.81;4,89.29,398.61,417.08,9.46;4,89.29,412.16,416.70,9.46;4,89.29,425.36,416.69,9.81;4,89.29,439.26,418.60,9.46;4,89.29,452.81,416.88,9.46;4,89.29,466.01,416.69,9.81;4,89.29,479.91,199.96,9.46"><head>Figure 2 .</head><label>2</label><figDesc>On this basis, we have draw a rough conclusion that the test set contains approximate 1000 âˆ¼ 2000 open set samples. It should be noted that this rough conclusion may be wrong, since we have no information about the reality that how many open-set samples in test set. Despite of it, in the rest of the experiments, we set the samples with top-k lowest confidence as the open set, the value of ğ‘˜ is set based on this rough conclusion. As shown from line 7 to line 9 in Algorithm 1, we adjust the threshold to obtain open-set samples. For different experiment setting and model, it is hard and needless to have exactly same number of open-set samples, experimentally, we set k to âˆ¼ 1000 at first, and adjust it to âˆ¼ 1500 at the final based on the public test set performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,244.23,416.95,9.35;5,89.29,256.28,360.21,9.22;5,120.07,283.53,352.37,194.69"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Logit score frequency distribution of val and test. We draw the Logit score frequency distribution on both validation and test set, with the output logits of a single model.</figDesc><graphic coords="5,120.07,283.53,352.37,194.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,89.29,499.50,416.69,9.35;5,89.29,511.55,416.70,9.22;5,89.29,523.50,418.08,9.22;5,89.29,535.46,368.36,9.22"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Selected test samples. We found many test samples contain several images, we predict the category of the test sample by averaging the model outputs of them. We also notice that some test samples such as sample_c contains one image pictured from natural environment, and the other images come from microscope view, it will disturb the average results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.98,90.03,410.04,43.79"><head>Table 1</head><label>1</label><figDesc>MetaFormer-0 baseline. loss batch size accumulate steps epochs mixup train+val public mean ğ‘“ 1 private mean ğ‘“ 1</figDesc><table coords="7,96.25,126.48,370.78,7.34"><row><cell>Soft Target CE 32</cell><cell>1</cell><cell>100</cell><cell>yes</cell><cell>no</cell><cell>78.76%</cell><cell>74.26%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.98,142.81,415.57,72.24"><head>Table 2</head><label>2</label><figDesc>Mean ğ‘“ 1 score on public/private test set with different losses and MetaFormer-0 as backbone.</figDesc><table coords="7,95.92,170.76,403.44,44.29"><row><cell>loss</cell><cell cols="7">batch size accumulate steps epochs mixup train+val public mean ğ‘“ 1 private mean ğ‘“ 1</cell></row><row><cell>Soft Target CE</cell><cell>32</cell><cell>1</cell><cell>32</cell><cell>yes</cell><cell>no</cell><cell>71.49%</cell><cell>67.6%</cell></row><row><cell cols="2">Label Smoothing CE 32</cell><cell>1</cell><cell>32</cell><cell>no</cell><cell>no</cell><cell>76.9%</cell><cell>72.48%</cell></row><row><cell cols="2">Label Smoothing CE 64</cell><cell>3</cell><cell>32</cell><cell>no</cell><cell>yes</cell><cell>79.45%</cell><cell>75.67%</cell></row><row><cell>Seesaw Loss</cell><cell>64</cell><cell>3</cell><cell>32</cell><cell>no</cell><cell>yes</cell><cell>79.79%</cell><cell>76.15%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.98,223.96,418.94,53.82"><head>Table 3</head><label>3</label><figDesc>Mean ğ‘“ 1 score on public/private test set with different batch size and MetaFormer-0 as backbone.</figDesc><table coords="7,95.92,251.90,403.44,25.88"><row><cell>loss</cell><cell cols="7">batch size accumulate steps epochs mixup train+val public mean ğ‘“ 1 private mean ğ‘“ 1</cell></row><row><cell cols="2">Label Smoothing CE 32</cell><cell>1</cell><cell>32</cell><cell>no</cell><cell>no</cell><cell>76.90%</cell><cell>72.48%</cell></row><row><cell cols="2">Label Smoothing CE 64</cell><cell>1</cell><cell>32</cell><cell>no</cell><cell>no</cell><cell>77.49%</cell><cell>74.27%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,88.98,286.69,410.44,71.89"><head>Table 4</head><label>4</label><figDesc>Mean ğ‘“ 1 score on public/private test set with different accumulate steps.</figDesc><table coords="7,95.87,314.62,403.56,43.96"><row><cell>loss</cell><cell cols="4">batch size accumulate steps epochs backbone</cell><cell cols="2">train+val public mean ğ‘“ 1 private mean ğ‘“ 1</cell></row><row><cell cols="2">Seesaw Loss 64</cell><cell>3</cell><cell>32</cell><cell cols="2">MetaFormer-0 yes</cell><cell>79.79%</cell><cell>76.15%</cell></row><row><cell cols="2">Seesaw Loss 64</cell><cell>6</cell><cell>32</cell><cell cols="2">MetaFormer-0 yes</cell><cell>80.22%</cell><cell>76.90%</cell></row><row><cell cols="2">Seesaw Loss 32</cell><cell>3</cell><cell>64</cell><cell cols="2">MetaFormer-1 yes</cell><cell>81.67%</cell><cell>77.62%</cell></row><row><cell cols="2">Seesaw Loss 32</cell><cell>6</cell><cell>64</cell><cell cols="2">MetaFormer-1 yes</cell><cell>81.66%</cell><cell>77.94%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,89.29,516.33,418.05,63.66"><head></head><label></label><figDesc>Table 3 illustrates that larger batch size improves the performance. in detail, by increasing batch size from 32 to 64, we improved ğ‘“ 1 score from 76.90% to 77.49% on public set, consistently improve ğ‘“ 1 score from 72.48% to 74.27% on private set. Similar techniques is to increase the accumulate steps. As shown in Table 4, enlarging accumulate steps improves mean ğ‘“ 1 score in private test set consistently with MetaFormer-0 and MetaFormer-1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,88.98,90.03,417.00,70.71"><head>Table 9</head><label>9</label><figDesc>Mean ğ‘“ 1 score on public/private test set with pseudo label, ConvNext-large and MetaFormer-2 as backbone.</figDesc><table coords="9,97.59,128.38,400.12,32.36"><row><cell>backbone</cell><cell cols="5">batch size accumulate steps epochs public mean ğ‘“ 1 private mean ğ‘“ 1</cell></row><row><cell cols="2">ConvNext-large 24</cell><cell>6</cell><cell>80</cell><cell>80.65%</cell><cell>76.61%</cell></row><row><cell>MetaFormer-2</cell><cell>24</cell><cell>8</cell><cell>80</cell><cell>82.45%</cell><cell>77.93%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,88.98,169.96,408.27,66.29"><head>Table 10</head><label>10</label><figDesc>Single model's mean ğ‘“ 1 score on public/private test set with different test time augmentation.</figDesc><table coords="9,169.43,198.18,256.42,38.07"><row><cell>test time augmentation</cell><cell>public mean ğ‘“ 1</cell><cell>private mean ğ‘“ 1</cell></row><row><cell>center crop / five crop</cell><cell cols="2">81.66% / 81.76% 77.94% / 78.25%</cell></row><row><cell>center crop / five crop</cell><cell cols="2">81.67% / 81.63% 77.62% / 77.69%</cell></row><row><cell cols="3">five crop / multi scale &amp; ten crop 80.46% / 80.20% 77.02% / 77.31%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="9,88.98,255.22,418.94,47.01"><head>Table 11</head><label>11</label><figDesc>Ensemble model's mean ğ‘“ 1 score on public/private test set with different test time augmentation.</figDesc><table coords="9,159.18,283.49,276.92,18.75"><row><cell>test time augmentation</cell><cell>public mean ğ‘“ 1</cell><cell>private mean ğ‘“ 1</cell></row><row><cell cols="3">center crop / multi scale &amp; ten crops 83.20% / 83.26% 79.51% / 79.38%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="9,88.98,311.27,418.65,93.26"><head>Table 12</head><label>12</label><figDesc>The effectiveness of post process for tail categories recognition and open-set categories recognition. ensemble and post process number of open-set samples public mean ğ‘“ 1 private mean ğ‘“ 1</figDesc><table coords="9,97.56,361.86,364.28,42.66"><row><cell>average ensemble (v1)</cell><cell>âˆ¼1000</cell><cell>83.26%</cell><cell>79.38%</cell></row><row><cell>average ensemble (v2)</cell><cell>âˆ¼1500</cell><cell>83.50%</cell><cell>79.60%</cell></row><row><cell>average ensemble (v3)</cell><cell>âˆ¼1500</cell><cell>83.65%</cell><cell>79.79%</cell></row><row><cell>average ensemble (v4)</cell><cell>âˆ¼1500</cell><cell>83.78%</cell><cell>80.43%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="9,88.93,510.73,417.43,118.28"><head></head><label></label><figDesc>Test Time Augmentation. For test time augmentation, we use center crop, five crop and multi scale &amp; ten crop during test phase. The effects of test time augmentation(TTA) are shown in Table10and Table11. It should be noted that the mean ğ‘“ 1 score on public test set is out of accord with private test set in some experiments, and it is hard to decide which TTA is better only based on public score, in consideration of robustness, we have chosen multi scale &amp; ten crop based on the public mean ğ‘“ 1 in Table11. Post Process. For short, we name the different version of ensemble and post process as v1 (initial version), v2 (v1 + proper open-set threshold), v3 (v2 + models trained with pseudo label) and v4 (v3 + post process for tail categories).</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>on public test set. We blame it to the coupled training schedules with the image size, which we do not investigate into it. Finally, we adopt the image size 384 in all settings. Nevertheless, the performance with this baseline is satisfactory enough. Pretrain dataset. We transfer MetaFormer-2 pretrained on different dataset such as herbarium, imagenet22k and inaturalist21. The results are shown in Table <ref type="table" coords="8,373.89,519.16,4.17,9.46">7</ref>. Experimentally, we do not directly choose the best-performed pre-training model. Instead, we use ensemble techniques to combine them. We find that ensemble will produce a consistent improvement compared with single model. Even combining the best-performed single model with other slightly poorerperformed models will not affect the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Other Attempts</head><p>ConvNext. In addition to MetaFormer, We also train ConvNext. The results are listed in Table <ref type="table" coords="8,499.81,624.17,4.04,9.46">8</ref>.</p><p>The experiments in ConvNext is not fully explored compared to MetaFormer. Although the results of ConvNext are inferior to MetaFormer, we still add it to the model ensemble process and the performance is also improved.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,111.34,407.75,394.64,8.64;10,111.34,419.70,177.54,8.64" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02751</idno>
		<title level="m" coord="10,284.82,407.75,221.16,8.64;10,111.34,419.70,45.38,8.64">Metaformer: A unified meta framework for fine-grained recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.34,431.66,394.64,8.64;10,111.34,443.61,366.17,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,358.84,431.66,91.79,8.64">A convnet for the 2020s</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,458.27,431.66,47.72,8.64;10,111.34,443.61,334.63,8.64">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.34,455.57,394.64,8.64;10,111.34,467.52,394.64,8.64;10,111.34,479.48,160.47,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,459.62,455.57,46.37,8.64;10,111.34,467.52,143.30,8.64">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,275.80,467.52,230.19,8.64;10,111.34,479.48,66.74,8.64">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9695" to="9704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.34,491.43,394.64,8.64;10,111.34,503.39,394.64,8.64;10,111.34,515.34,113.89,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,310.39,491.43,97.95,8.64;10,436.17,491.43,69.81,8.64;10,111.34,503.39,145.71,8.64">Fungi recognition as an open set classification problem</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,278.96,503.39,227.02,8.64;10,111.34,515.34,84.17,8.64">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of FungiCLEF</note>
</biblStruct>

<biblStruct coords="10,111.34,527.30,395.88,8.64;10,111.34,539.25,394.63,8.64;10,111.34,551.21,395.88,8.64;10,111.34,563.16,52.30,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,139.96,539.25,366.02,8.64;10,111.34,551.21,119.19,8.64">Lifeclef 2022 teaser: An evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,251.79,551.21,187.38,8.64">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.34,575.12,395.89,8.64;10,111.34,587.07,395.88,8.64;10,111.34,599.03,394.64,8.64;10,111.34,610.98,394.64,8.64;10,111.34,622.94,193.13,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,151.04,599.03,354.94,8.64;10,111.34,610.98,135.45,8.64">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,267.86,610.98,238.12,8.64;10,111.34,622.94,124.77,8.64">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.34,634.89,394.64,8.64;10,111.34,646.85,350.76,8.64" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>LaessÃ¸e</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>FrÃ¸slev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10107</idno>
		<title level="m" coord="10,477.19,634.89,28.79,8.64;10,111.34,646.85,218.60,8.64">Danish fungi 2020 -not just another image recognition dataset</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.34,658.80,394.64,8.64;11,111.34,88.66,394.64,8.64;11,111.34,100.62,188.30,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,209.93,658.80,296.05,8.64;11,111.34,88.66,127.25,8.64">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,259.39,88.66,246.60,8.64;11,111.34,100.62,94.37,8.64">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3034" to="3043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.34,112.57,394.64,8.64;11,111.34,124.53,394.64,8.64;11,111.34,136.48,196.27,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,324.81,112.57,181.17,8.64;11,111.34,124.53,178.78,8.64">Filtration and distillation: Enhancing region attention for fine-grained visual categorization</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,310.33,124.53,195.66,8.64;11,111.34,136.48,45.71,8.64">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11555" to="11562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.34,148.44,395.88,8.64;11,111.34,160.39,377.98,8.64" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,309.35,148.44,194.31,8.64">Learning to navigate for fine-grained classification</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,124.95,160.39,279.57,8.64">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="420" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.34,172.35,394.81,8.64;11,111.34,184.30,291.79,8.64" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07976</idno>
		<title level="m" coord="11,415.91,172.35,90.24,8.64;11,111.34,184.30,159.63,8.64">Transfg: A transformer architecture for fine-grained recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.34,196.26,394.64,8.64;11,111.34,208.21,395.88,8.64;11,111.34,220.17,72.23,8.64" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,300.36,196.26,205.62,8.64;11,111.34,208.21,54.09,8.64">Channel interaction networks for fine-grained image categorization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,186.40,208.21,245.91,8.64">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10818" to="10825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.34,232.12,394.64,8.64;11,111.34,244.08,369.49,8.64" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,173.83,232.12,264.32,8.64">Fine-grained image classification via combining vision and language</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,458.27,232.12,47.72,8.64;11,111.34,244.08,275.56,8.64">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5994" to="6002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.34,256.03,394.63,8.64;11,111.34,267.99,394.64,8.64;11,111.34,279.95,190.11,8.64" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,461.66,256.03,44.32,8.64;11,111.34,267.99,148.96,8.64">Geo-aware networks for fine-grained recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Potetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,281.68,267.99,224.31,8.64;11,111.34,279.95,125.62,8.64">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.34,291.90,394.64,8.64;11,111.34,303.86,395.88,8.64;11,111.34,315.81,62.27,8.64" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,267.90,291.90,238.08,8.64;11,111.34,303.86,49.86,8.64">Presence-only geographical priors for fine-grained image classification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,181.50,303.86,297.36,8.64">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9596" to="9606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.34,327.77,395.88,8.64;11,111.34,339.72,396.03,8.64;11,111.04,351.68,396.61,8.64;11,111.34,363.63,396.03,8.64;11,111.34,375.59,187.97,8.64" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,370.80,339.72,136.57,8.64;11,111.04,351.68,178.91,8.64">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m" coord="11,314.12,351.68,193.52,8.64;11,111.34,363.63,99.59,8.64">9th International Conference on Learning Representations, ICLR 2021</title>
		<meeting><address><addrLine>Virtual Event, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.34,387.54,394.63,8.64;11,111.10,399.50,394.89,8.64;11,111.34,411.45,141.09,8.64" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,378.41,387.54,127.57,8.64;11,111.10,399.50,158.26,8.64">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,290.03,399.50,215.96,8.64;11,111.34,411.45,111.48,8.64">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.34,423.41,395.00,8.64;11,111.34,435.36,386.19,8.64" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00652</idno>
		<title level="m" coord="11,415.39,423.41,90.96,8.64;11,111.34,435.36,255.88,8.64">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.34,447.32,394.64,8.64;11,111.34,459.27,91.85,8.64" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="11,331.89,447.32,174.09,8.64;11,111.34,459.27,61.95,8.64">Adaptive class suppression loss for long-tail object detection</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.34,471.23,394.64,8.64;11,111.34,483.18,394.64,8.64;11,111.34,495.14,396.13,8.64;11,110.60,507.09,395.88,8.64;11,111.34,519.05,398.82,8.64;11,111.34,531.94,32.38,7.01" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,347.82,471.23,158.16,8.64;11,111.34,483.18,44.14,8.64">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01168</idno>
		<ptr target="https://openaccess.thecvf.com/content_CVPR_2020/html/Tan_Equalization_Loss_for_Long-Tailed_Object_Recognition_CVPR_2020_paper.html.doi:10.1109/CVPR42600.2020.01168" />
	</analytic>
	<monogr>
		<title level="m" coord="11,199.28,483.18,306.70,8.64;11,111.34,495.14,17.93,8.64">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2020-06-13">2020. June 13-19, 2020. 2020</date>
			<biblScope unit="page" from="11659" to="11668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.34,542.96,394.64,8.64;11,111.10,554.91,304.11,8.64" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,287.44,542.96,218.55,8.64;11,111.10,554.91,38.87,8.64">Open-set recognition: a good closed-set classifier is all you need?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vaze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,171.45,554.91,214.72,8.64">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.34,566.87,395.88,8.64;11,111.34,578.82,395.88,8.64;11,110.99,590.78,395.56,8.64;11,111.34,602.73,153.27,8.64" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="11,324.60,566.87,178.26,8.64">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>CissÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb" />
	</analytic>
	<monogr>
		<title level="m" coord="11,124.92,578.82,278.20,8.64">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s" coord="11,217.79,590.78,125.42,8.64">Conference Track Proceedings</title>
		<meeting><address><addrLine>Vancouver, BC, Canada; OpenReview</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.34,614.69,394.64,8.64;11,111.34,626.65,394.64,8.64;11,111.34,638.60,92.52,8.64" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="11,255.17,614.69,134.15,8.64">When does label smoothing help?</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,410.63,614.69,95.36,8.64;11,111.34,626.65,274.60,8.64">Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
