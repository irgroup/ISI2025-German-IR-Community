<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,416.70,15.42;1,89.29,106.66,373.83,15.42;1,89.29,128.58,133.23,15.43">Species Distribution Modeling based on aerial images and environmental features with Convolutional Neural Networks</title>
				<funder ref="#_rqEHp9x">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_cxpR5f9">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_w5DPGC2">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,68.40,11.96"><forename type="first">C√©sar</forename><surname>Leblanc</surname></persName>
							<email>cesar.leblanc@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,170.33,156.89,52.66,11.96"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.64,156.89,76.33,11.96"><forename type="first">Titouan</forename><surname>Lorieul</surname></persName>
							<email>titouan.lorieul@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.62,156.89,106.20,11.96"><forename type="first">Maximilien</forename><surname>Servajean</surname></persName>
							<email>maximilien.servajean@lirmm.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Universit√© Paul Val√©ry</orgName>
								<orgName type="institution" key="instit2">University of Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,170.84,67.32,11.96"><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<email>pierre.bonnet@cirad.fr</email>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">AMAP</orgName>
								<orgName type="laboratory" key="lab2">CIRAD</orgName>
								<orgName type="institution" key="instit1">Univ Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">INRAE</orgName>
								<orgName type="institution" key="instit4">IRD</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,416.70,15.42;1,89.29,106.66,373.83,15.42;1,89.29,128.58,133.23,15.43">Species Distribution Modeling based on aerial images and environmental features with Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">3AB6D6122FAB8D031BBEF1F675AD190D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Species distribution modelling</term>
					<term>Aerial images</term>
					<term>Environmental features</term>
					<term>Biodiversity</term>
					<term>LifeCLEF</term>
					<term>Presenceonly data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting which species are likely to be observed at a given location is an important issue both from a scientific point of view and for citizens interested in biodiversity. The aim of the GeoLifeCLEF challenge is to predict the presence of around 17K plant and animal species using 1.6M geo-localized observations from France and the US. Beyond GPS coordinates, additional covariates are provided for each observation: remote sensing imagery, land cover data, altitude data, bioclimatic data and pedologic data. We tested two simple approaches making use of every covariate available: (i) training separate models from them, testing them separately, and, (ii) combining those models together in the simplest manner, i.e., averaging their predicted scores. These simple methods allowed us to reach the fourth position on the private leaderboard of the competition. We also managed to improve our score after the end of the challenge by implementing a multi-modal convolutional neural network with separated features extractors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The aim of the GeoLifeCLEF 2022 challenge <ref type="bibr" coords="1,289.62,474.18,11.55,10.91" target="#b0">[1]</ref>, held jointly as part of the LifeCLEF 2022 lab <ref type="bibr" coords="1,89.29,487.73,13.00,10.91" target="#b1">[2]</ref> and of the FGVC9 workshop, is to predict a list of species most likely to be observed at a given location. It could be useful for many scenarios related to biodiversity management and conservation. For example, it could improve species identification tools by reducing the list of candidate species observable at a given site. Moreover, it could also facilitate biodiversity inventories through the development of location-based recommendation services, encourage the involvement of citizen scientist observers, and accelerate the annotation and validation of species observations to produce large, high-quality data sets. Last but not least, it could be used for educational purposes through biodiversity discovery applications with features such as contextualized educational pathways.</p><p>The GeoLifeCLEF 2022 challenge relies on a collection of observations of plants and animals in the US and France. Each observation consists of a species name with the GPS coordinates where it was observed. In addition, observations are paired with a set of covariates characterizing the landscape and environment around them. There are 17K species in the dataset (9K plant species and 8K animal species).</p><p>In a nutshell, we submitted the predictions of two types of methods: (i) a Convolutional Neural Network (CNN) <ref type="bibr" coords="2,161.18,168.26,12.68,10.91" target="#b2">[3]</ref> based on remote sensing imagery (RGB-patch to be more precise) and (ii), a fusion of the predictions of multiple CNNs based on remote sensing imagery (RGB-patch and IR-patch), land cover data, altitude data, bioclimatic data, pedologic data and GPS coordinates. Other models, including a multi-modal convolutional neural network with separated features extractors, were still running when the challenge closed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data and Evaluation Metric</head><p>In this section, we briefly present the data and the evaluation metric used for the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data</head><p>This paragraph is simply a description of the standard GeoLifeCLEF 2022 dataset, don't hesitate to skip it if you already know about the data. The dataset contains exactly 1,663,896 occurrences of species (see Table <ref type="table" coords="2,182.08,357.25,4.16,10.91">1</ref>) with the latitude and longitude of their location. observations were assigned into a grid of 5km√ó5km quadrats. 2.5% of these quadrats were randomly sampled for the test set, another 2.5% for the validation set, while the remaining ones were assigned to the training set.</p><p>Each observation is paired with the following covariates:</p><p>‚Ä¢ Remote sensing imagery: 256m√ó256m RGB-NIR patches centered at each observation, at a resolution of 1 meter per pixel; ‚Ä¢ Land cover data: 256m√ó256m patches centered at each observation, at a resolution of 1 meter per pixel; ‚Ä¢ Altitude data: 256m√ó256m patches centered at each observation, at a resolution of 1 meter per pixel; ‚Ä¢ Bioclimatic data: 19 low-resolution rasters, at a resolution of 30 arcsec per pixel, i.e., around 1 kilometer per pixel; ‚Ä¢ Pedologic data: 8 low-resolution rasters, at a resolution of 250 meters per pixel.</p><p>A complete description of how the original GeoLifeCLEF 2020 dataset was built can be found in the associated paper <ref type="bibr" coords="3,197.74,510.48,11.59,10.91" target="#b3">[4]</ref>, and the 2022 edition of GeoLifeCLEF uses a cleaned-up version (changelog available on Kaggle <ref type="foot" coords="3,227.01,522.28,3.71,7.97" target="#foot_0">1</ref> ).</p><p>An example of remote sensing imagery, land cover data and altitude data can be seen in Figure <ref type="figure" coords="3,121.30,551.13,10.35,10.91" target="#fig_7">16</ref> in Section 5.2 (Appendix), while bioclimatic data for the same observation can be seen in Figure <ref type="figure" coords="3,154.97,564.68,10.20,10.91" target="#fig_9">17</ref> and pedologic data can be seen in Figure <ref type="figure" coords="3,352.20,564.68,8.41,10.91" target="#fig_10">18</ref>. We detail the complete list and resolutions of environmental variables in Table <ref type="table" coords="3,301.91,578.23,3.74,10.91" target="#tab_6">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Evaluation Metric</head><p>The evaluation metric for the GeoLifeCLEF 2022 competition is the top-30 error rate. Each observation ùëñ is associated with a single ground-truth label ùë¶ ùëñ corresponding to the observed  species. For each observation, the submissions provide 30 candidate labels ùë¶ ÀÜùëñ, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This section describes the methods we tried during the competition. Our first intention was, inspired by the winning solution of GeoLifeCLEF 2021 competition <ref type="bibr" coords="4,406.89,628.54,11.58,10.91" target="#b4">[5]</ref>, to create a multimodal convolutional neural network with separated features extractors consisting of several convolutional neural networks combined to return a single prediction value in order to take advantage of every modality (RGB images, near-infrared images, altitude data, landcover data, pedologic rasters, bioclimatic rasters and GPS locations), as detailed in Section 4. However, the model we tried was too large to fit in our GPUs (GeForce RTX 2080 Ti with 11GB of RAM capacity per card) and we did not solve this problem in time to include this type of models in our submissions. Thus, we decided to train 8 separate models and to concatenate their predictions to eventually get a better final prediction than by simply using individual models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data exploration</head><p>To better interpret the effect of the different covariates on the prediction, we decided to split the five main modalities (remote sensing imagery, land cover data, altitude data, bioclimatic data and pedologic data) into 8 different covariates: The biggest change compared to the initial formatting of the input data is that we split the bioclimatic data into two different covariates: temperature and precipitation. Indeed, a look at the description of bioclimatic data (Table <ref type="table" coords="5,276.00,401.47,4.21,10.91" target="#tab_6">2</ref>) shows that around half of them (bio_1 to bio_11) are measures of temperature (yearly mean temperature, min temperature, max temperature, etc.) while the other half (bio_12 to bio_19) are measures of precipitation (yearly precipitation, precipitation of wettest month, precipitation of driest month, etc.).</p><p>Figure <ref type="figure" coords="5,130.74,455.66,4.98,10.91" target="#fig_3">3</ref> shows a correlation heatmap of these variables using Pearson correlation coefficient (a measure of the linear correlation between the variables) <ref type="bibr" coords="5,346.90,469.21,12.70,10.91" target="#b5">[6]</ref> computed using the central pixel of each extracted patch. Two clear blocks appear on that heatmap: variables bio_1 to bio_11 are strongly correlated, so are variables bio_12 to bio_19, while these two blocks of variables are not much correlated. It thus seems natural to treat these two blocks of variables separately.</p><p>In order to keep the same number of input variables for every model (i.e., 3 variables passed as images with 3 channels), we decided to separately pick the 3 most important variables for (i) temperature, (ii) precipitation, and (iii) soil. To select them, we ran different random forest classifiers <ref type="bibr" coords="5,135.74,564.06,12.92,10.91" target="#b6">[7]</ref> on environmental vectors and then computed feature importances taken as the mean and standard deviation of accumulation of the impurity decrease within each tree <ref type="bibr" coords="5,492.22,577.61,11.58,10.91" target="#b7">[8]</ref>. Each random forest was grown with 16 trees having each a maximum depth of 10. The missing values were set to the minimum value of float32. In total, four random forests were fitted from different sets of input variables:</p><p>‚Ä¢ every environmental variable, with results shown on Figure <ref type="figure" coords="5,385.59,640.13,12.63,10.91" target="#fig_5">4a;</ref><ref type="figure" coords=""></ref> ‚Ä¢ only temperature variables, with results shown on Figure <ref type="figure" coords="5,374.43,654.86,13.03,10.91" target="#fig_5">4b;</ref><ref type="figure" coords=""></ref> ‚Ä¢ only precipitation variables, with results shown on Figure <ref type="figure" coords="5,376.82,669.58,8.21,10.91" target="#fig_5">4c</ref>; ‚Ä¢ only soil variables, with results shown on Figure <ref type="figure" coords="6,335.34,383.81,8.66,10.91" target="#fig_5">4d</ref>.</p><p>From this analysis, we decided to keep the features bio_1, bio_2 and bio_7 for the temperature model, features bio_12, bio_14 and bio_15 for the precipitation model and orcdrc, phihox and sltppt for the soil model. Some of the features that seem to be more important than others were not taken into account because they probably repeat the same information than other features (for example, looking at Figure <ref type="figure" coords="6,227.22,460.52,10.32,10.91" target="#fig_5">4b</ref> it seems that bio_3 is the most important, but in fact it is the result of bio_2 divided by bio_7 and then multiplied by 100).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Patch preparation</head><p>In this subsection, we detail how, for each covariate, we built the 256x256x3 image patches fed into the CNN. An illustration of the resulting patches for observation n¬∞10010000 in provided in Figure <ref type="figure" coords="6,120.36,550.90,3.74,10.91" target="#fig_6">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB images</head><p>The first type of data are the RGB patches centered at each observation. The image size was already 256x256x3 so we did not need to adapt them, see Figure <ref type="figure" coords="6,427.70,593.20,8.14,10.91" target="#fig_6">5a</ref>. The resolution of these patches is 1 meter per pixel.</p><p>Near-infrared images For the near-infrared images, we decided to simply replicate the 256x256 patch three times. The result is shown in Figure <ref type="figure" coords="6,352.87,649.06,8.74,10.91" target="#fig_6">5b</ref>. Again, the resolution of these patches is 1 meter per pixel.  Landcover data Each patch is also a 256x256 patch centered at each observation, but each pixel takes a value between 0 and 33 (see Table <ref type="table" coords="7,299.72,460.42,3.55,10.91">3</ref>). We decided to simply scale each point from 0-33 to 0-255 and then replicate the channel three times. The result is shown in Figure <ref type="figure" coords="7,478.72,473.97,8.17,10.91" target="#fig_6">5c</ref>. As the two patches above, the resolution of these patches is 1 meter per pixel.</p><p>Altitude data For altitude data, several methods were tested to check what is the best way to preprocess the patches. The best results were obtained by scaling each patch one by one, going from each patch's minimum and maximum to 0 and 255. This means for example that our model won't see the difference between a flat plain at the level of the sea and a flat plain at an altitude of 4,000m. We then again replicated the channel three times to imitate an RGB patch. The result is shown in Figure <ref type="figure" coords="7,251.54,584.03,8.61,10.91" target="#fig_6">5d</ref>. Like RGB images, near-infrared images and landcover data, the resolution of these patches is 1 meter per pixel.</p><p>Temperature rasters Each patch was extracted around each location with a size of 20x20 pixels (=20x20km) and then resized to a 256x256 pixels image. The missing values (e.g., near the coasts) were replaced by the minimum of all the data from the patch minus one. The patches are then scaled between 0 and 255, with respect to the global extremes of the whole raster, and then combined into one single patch. The result is shown in Figure <ref type="figure" coords="8,363.79,369.58,8.12,10.91" target="#fig_6">5e</ref>. The resolution of bioclimatic data (and thus temperature rasters) is 30 arcsec per pixel (‚àº 1 kilometer per pixel).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precipitation rasters</head><p>We also extracted each patch with a size of 20x20 pixels and then resized it to a 256x256 image patch. We replaced the missing values of each patch by the minimum of the given patch minus one, scaled the patchs between 0 and 255 and then combined the three patches to have one final patch of size 256x256x3. The result is shown in Figure <ref type="figure" coords="8,496.59,452.53,7.38,10.91" target="#fig_6">5f</ref>. Again, the resolution of temperature rasters is 30 arcsec per pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pedologic rasters</head><p>The exact same operations were done here with patches orcdrc, phihox and sltppt, even if the resolution of these rasters are different from the resolution of the biologic rasters (i.e., we still extract 20x20 pixels patches that we resize to 256x256 pixels patches and we use the same method to replace missing values, to scale the data and to combine the patches into one 256x256x3 image). The result is shown in Figure <ref type="figure" coords="8,345.93,549.04,8.60,10.91" target="#fig_6">5g</ref>. The resolution of pedologic data is 250 meters per pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPS locations</head><p>For the last covariate that we used, we combined three constant patches of size 256x256. The first patch was filled with the latitude, the second patch was filled with the longitude and the last patch was filled with the country (0 if France, 255 if USA). Then we scaled the first and second patches between 0 and 255 according to the minimum and maximum values of the country the observation is in, and we combined the three patches to have an input of size 256x256x3. The result is shown in Figure <ref type="figure" coords="8,273.95,659.09,8.89,10.91" target="#fig_6">5h</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training procedure</head><p>Before being fed into the CNN, each patch goes through a transformation stage. The patch is (i) divided by 255 (so it was scaled between 0 and 1), (ii) rotated by a random angle in the range of -45¬∞to +45¬∞(with the pixel fill value for the area outside the rotated image fixed to 1), (iii) randomly cropped to a size of 224x224x3, (iv) randomly flipped (horizontally and/or vertically with a probability of 0.5 each), and, (v) normalized using the mean and standard deviation of a random subset of the train split of ImageNet<ref type="foot" coords="9,351.43,173.53,3.71,7.97" target="#foot_1">2</ref> . The cropping and normalization stages were employed because we used pre-trained models (on the ImageNet2012 dataset <ref type="bibr" coords="9,89.29,202.38,11.26,10.91" target="#b8">[9]</ref>), and according to PyTorch<ref type="foot" coords="9,229.76,200.63,3.71,7.97" target="#foot_2">3</ref> documentation all pre-trained models expect input images normalized in the same way, i.e., mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224 (the images have to be loaded in to a range of [0, 1] and then normalized with the values we used). For all of our models, we used a ResNet-50 architecture <ref type="bibr" coords="9,359.08,256.58,18.07,10.91" target="#b9">[10]</ref> as implemented in PyTorch <ref type="bibr" coords="9,89.29,270.13,18.04,10.91" target="#b10">[11]</ref> with the provided pre-trained weights. To accommodate to the number of classes of the competition, we changed the size of the output of the last linear layer to 17,037 (instead of 1,000 for ImageNet). They were trained for 10 epochs using stochastic gradient descent (SGD) with a learning rate of 0.01, a Nesterov momentum of 0.9, a batch size of 32. For each training, we save the best intermediate parameters measured as the ones providing the lowest top-30 error rate value on the validation set computed at the end of each epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results</head><p>The evolution of the top-30 error rate on the validation set during the training of each individual model is shown in Figure <ref type="figure" coords="9,200.35,401.15,3.66,10.91" target="#fig_7">6</ref>. It was not necessary to train the models during more than 10 epochs as, most of the time, the best validation error is obtained around epoch 6-7 and then it starts to slowly overfit (except for the PREC patch that achieves its best validation Top-30 Error on the last epoch). As, for each type of patches, the minimum value is achieved at a different epoch, we thus keep the best parameters for each one of them (as explained in previous Section 3.3). Their individual performance is provided in Figure <ref type="figure" coords="9,286.17,468.89,9.86,10.91" target="#fig_9">7a</ref> which shows that the most informative patches, when used individually, are RGB, then NIR, followed by TEMP, PREC, COOR and LAND (which are very close) and finally PEDO and ALT. It's coherent that altitude data is not very good because of the way we process the data (each patch is scaled individually). We tried different techniques, like scaling the data according to the global extremes or applying the logarithmic function to support the fact that a small change of altitude at the level of the sea is probably more important than the same small change but at a very high altitude, but each time the Top-30 Error was higher. The same remark can be made for landcover data because we didn't take into account the categories. Indeed, handling categorical inputs with a CNN is tedious, and processing the data using one-hot-encoding, which for each patch will create a vector of size 34 (the number of different labels) filled with 0s, except for a 1 at the position associated with the current label, (or others methods less high-dimensional and thus less vulnerable to overfitting and less computationally expensive) could be interesting for future works. Moreover, with one-hot-encoding, all categories would be equally different from each other (the inner product between any two one-hot vectors is zero and as a consequence, we cannot generalize), but we can see on Table <ref type="table" coords="10,165.11,406.38,5.05,10.91">3</ref> that some categories seem close. Interestingly, the model using only COOR patches has a performance fairly close to an K-nearest neighbor algorithm fitted directly on GPS coordinates (using a Haversine distance <ref type="bibr" coords="10,290.80,433.48,17.93,10.91" target="#b11">[12]</ref> and K=1000, validated on the validation set) which achieves a validation top-30 error rate of 79.9%. This suggests that this model is able to retrieve the localisation information properly.</p><p>To enhance the results and to study the complementarity of the different sources of data, we decided to combine the predictions of all individual models into a single prediction by averaging the outputs of each model. Figure <ref type="figure" coords="10,289.15,501.22,10.49,10.91" target="#fig_9">7b</ref> shows the effect of gradually adding one type of patch to the ensemble (first RGB alone, then RGB+NIR, then RGB+NIR+LAND, ..., until having all types of patches). As can be seen, the error rate decreases smoothly to reach a minima for RGB+NIR+LAND+ALT+TEMP+PREC. It then increases when adding pedological and coordinates data. It could have been interesting to try different orders to check how it affects the error rate and what is the best combination of patches (note that, however, there are in total 2 8 = 256 possible combinations). But, by lack of time, we only added them in the order they appear on the description of the challenge.</p><p>In the end, we decided to submit the whole ensemble (with all the patches) which resulted in a top-30 error rate of 0.69779 on the official test set of the competition (the one used for the private leaderboard on Kaggle). This led us to the fourth position among the 52 participants.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Post-challenge works 4.1. New multi-modal approach</head><p>During the few weeks between the final run submission deadline and the deadline for working note paper submission to LifeCLEF lab, we had the time to enhance our work on this challenge. Since we add more time and resources (we used Nvidia Tesla T4 GPUs cards which each has 16GB of RAM capacity, thus allowing to train bigger models), we managed to test other methods.</p><p>In particular, we tried other ways to aggregate the different sources of data, for example, a multi-modal convolutional neural network with separated features extractors, illustrated in Figure <ref type="figure" coords="11,120.36,630.16,3.74,10.91" target="#fig_10">8</ref>, that we thought could potentially improve the accuracy of predictions. We thought about two ways of testing it:</p><p>1. Creating the eight models and then training them for scratch in order to start everything  Training the models from scratch was infeasible with our GPUs without parallel processing, which we didn't implement in time for this challenge. We thus had to create 8 "blank" ResNet-50 models, change the size of the output of the last linear layer to 17,037, take the state_dicts (i.e. a dictionary of the learnable parameters) of the best epoch of each model that has been trained separately (i.e. the epoch at which they achieve their best Top-30 Error), load them into the models and freeze the models so that no change happens to its parameters. Then we removed the last classification layer of each model and we added a classifier head (having an input size of 2,048 √ó 8, because the last 2D convolution layer in ResNet-50 produces 2,048 channels, and an output size of 17,037, because it is the number of classes in our dataset) on top of their concatenated outputs.</p><p>However, the results were quite disappointing (see solid black lines in Figure <ref type="figure" coords="12,458.24,553.54,3.65,10.91" target="#fig_12">9</ref>), and we couldn't beat our simple ResNet-50 model trained on the RGB patches. We could see that the multi-modal convolutional neural network with separated features extractors was clearly overfitting, so we tried to add weight decays (see Figure <ref type="figure" coords="12,354.09,594.19,9.05,10.91" target="#fig_12">9a</ref>) and learning rate decays (see Figure <ref type="figure" coords="12,120.30,607.74,8.03,10.91" target="#fig_12">9b</ref>). It improved the validation Top-30 Error of our model a little bit (enough to beat by a really small margin the model trained on the RGB patches, which is our best model when it comes to models with only one feature extractor), but it was still a poor upgrade, and the overfitting was obvious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Enhancement of the multi-modal model and of the features extractors</head><p>After wondering what was possible in order to enhance the multi-modal convolutional neural network with separated features extractors, we found three inconsistencies that were probably degrading the performances of the models:</p><p>1. First of all, even though we were disabling gradient calculation (which allowed us to reduce memory consumption), we forgot to set dropout and batch normalization layers to evaluation, which yielded inconsistent inference results. 2. Secondly, we loaded the wrong models' parameter dictionaries for the PREC and PEDO patches. Indeed, when training simple ResNet-50 on these features extractors alone, we tried to extract them with different sizes before realizing that we obtained our best results with a size of 20x20 pixels around each location. But in the multi-modal convolutional neural network with separated features extractors, we were loading the parameters of the models that we trained on rasters extracted with a size of 10x10 pixels for the PREC and the PEDO patches (even though we were extracting them with a size of 20x20 on the new model). 3. Last but not least, which seems to be the worst mistake in our model was the fact that we were passing two image tensors to the same feature extractor in our forward method (the method that computes output tensors from input tensors). Indeed, both the image tensor of the PEDO patch and the image tensor of the COOR patch were passed to the model in which we loaded the parameter dictionary of the PEDO patch. Thus, the model in which we loaded the parameter dictionary of the COOR patch was unused during the whole process.</p><p>We obviously corrected these mistakes quite easily (by setting dropout and batch normalization layers to evaluation mode, by loading the correct learnable parameters for the models trained on the PREC and the PEDO patches and by modifying the typo in the code), but we also wanted to improve the features extractors with which we weren't fully satisfied. Furthermore, we also lost some of the models we trained after the end of the challenge due to disk storage cleaning (like the models trained on the bioclimatic data, on the pedologic data and on the coordinates), and even though we re-trained them in the same way, the stochasticity of the convolutional neural networks means that the results we show here may slightly differ from the previous sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Altitude data</head><p>First of all, we wanted to try some other things with the altitude patch. We developed the bad habit of converting each patch to integers (unsigned integer 8bits to be more specific) after scaling them between 0-255 because this was how the RGB patch was, but we wanted to try to leave floating values (occupying 32 bits in computer memory) in order to lose less information. For each patch, we tried to apply a logarithmic function and then scale the patch according to the global extremes, we tried to simply scale the patch according to the global extremes, we tried to scale the patch according to the patch's own extremes (for this three methods, the patch was then replicated on three channels to have a 256x256x3 patch at the end) and we tried to have three channels, and apply one method on each of the channel (so we would still have a 256x256x3 patch, but this time each channel would be different). And  we tried that with and without converting to integers during the scaling. As we can see in Figure <ref type="figure" coords="14,121.35,389.68,8.53,10.91" target="#fig_13">10</ref>, what worked best was to work with a patch having three different channels and without converting the data to integers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Landcover data</head><p>We also wanted to try different methods to deal with the landcover data. First of all, we tried to feed to the model each patch replicated three times without any modification (a 256x256x3 matrix of integers between 0 and 33). We also tried to scale the data from 0-33 to 0-255 (and replicate the patch along three channels) and to use the suggested labels (Table <ref type="table" coords="14,498.18,472.63,4.23,10.91">4</ref>) to go from 34 categories to 14 and then scale the data from 0-14 to 0-255 (and again, replicate this patch along three channels). And finally, we tried (since we achieved our best results with this method for the altitude data) to use the three methods and to pile up the three obtained patches to have a 256x256x3 patch, every channel being different. As we can see in Figure <ref type="figure" coords="14,494.54,526.83,8.43,10.91" target="#fig_14">11</ref>, what worked best for us was simply to scale the data from 0-33 to 0-255.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bioclimatic and pedologic data</head><p>For this kind of data, we just wanted to test if the action of casting the data (originally 32-bit-precision floating-point numbers) to 8-bit unsigned integers was worth it or not. So we ran each model twice as we described it Section 3.2, once with changing the matrix type and once without. As we can see in Figure <ref type="figure" coords="14,395.75,609.79,8.36,10.91" target="#fig_16">12</ref>, what worked best for us was different according to each patch. For the TEMP and PREC patches, it was better to convert the data to uint8 during its scaling, but for the PEDO patch it was not. However, we can see that even if the conversion to 8-bit unsigned integers is often better for environmental features, the learning curve has a lot of spikes and it less stable than when using 32-bit-precision  floating-point numbers.</p><p>NDVI We wanted to try a new type of patch, using the Normalized Difference Vegetation Index (NDVI), which is often a good indicator whether a high-resolution remote sensing imagery contains live green vegetation or not <ref type="bibr" coords="15,255.71,402.42,16.25,10.91" target="#b12">[13]</ref>. Written mathematically, the formula is simply:</p><formula xml:id="formula_0" coords="15,252.42,422.86,89.25,25.77">NDVI = NIR -Red NIR + Red</formula><p>We decided to create a new patch and to train a model using only this patch. As above, we tried dealing with 32-bit-precision floating-point numbers and with 8-bit unsigned integers (because the RGB images and the near-infrared images are filled with integers in the range of 0 to 255). With the uint8 values we tried:</p><p>‚Ä¢ to replace divisions by 0 (when the near-infrared pixel and the red pixels are both equal to 0) by divisions by 1. Then we scaled each patch from its extremes to 0-255 and we replicated the channel three times. ‚Ä¢ to replace divisions by 0 directly by the result 0 (which should be equivalent to the first method because if NIR + Red = 0 then NIR -Red = 0). Then we scaled each patch from its extremes to 0-255 and we replicated the channel three times. ‚Ä¢ to replace the divisions by 0 by the maximum pixel value of the matrix plus one. If every pixel in the the near-infrared image and in the red channel of the image is 0 (thus we only have divisions by 0) we put the value 1 in every pixel of the normalized difference vegetation index matrix. Then we scaled each patch from its extremes to 0-255 and we replicated the channel three times. ‚Ä¢ to do all of the methods above and stack them to have a 256x256x3 patch.  With the floats values we tried to replace divisions by 0 (when the near-infrared pixel and the red pixels are both equal to 0) by the value 0, by the value -1 and by the value 1 (since the values taken by the NDVI matrix are between -1 and 1), scale each patch from -1 and 1 to 0 and 255 and replicate the channel three times. As always, the last method consisted of having one channel with each different manipulation. As we can see in Figure <ref type="figure" coords="16,391.02,423.99,13.35,10.91" target="#fig_18">13a</ref>, it was really worth it to convert the data to floats instead of keeping the ints values of the original images. What worked best was to replace divisions by 0 by the value 0 (while working with floats) and to replicate the same channel three times (see in Appendix in Figure <ref type="figure" coords="16,383.87,464.64,10.15,10.91" target="#fig_23">19</ref> what this image without the 3-channels replication looks like). Now that we had the 9 features extractors trained (see Figure <ref type="figure" coords="16,379.91,491.74,8.05,10.91" target="#fig_20">14</ref>), we could launch for the last time the new multi-modal convolutional neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Final models (post-challenge evaluation)</head><p>Before training the multi-modal convolutional neural network with the 9 separated features extractors (RGB, NIR, LAND, ALT, TEMP, PREC, PEDO, COOR &amp; NDVI), we wanted to get a little overview with only two features. We selected the RGB and TEMP patches because intuitively they seemed to be less correlated than some other pairs of features (like RGB &amp; NIR or TEMP &amp; PREC). We tried the following configurations:</p><p>1. training the bi-modal model from scratch (without loading any parameter, all of them had to be learned). It was impossible on all modalities because of memory limitations but we could do it with only two modalities.  2. training the bi-modal model from scratch but replacing the flat classifier of the previous configuration (a linear layer of input size 2,048 √ó 2 and output size 17,037) by a sequential container where the input will first be passed to a linear layer of input size 2,048 √ó 2 and output size 2,048, then the output of this layer will be used as the input to a ReLU layer (which applies the rectified linear unit function element-wise <ref type="bibr" coords="17,387.02,445.20,16.78,10.91" target="#b13">[14]</ref>) and finally the output of the ReLU will become the input of the final linear layer of input size 2,048 and output size 17,037. This method creates a bottleneck layer. 3. loading each learnable parameters for the two separate features extractors and freezing their weights to only train the final classifier (the flat one, as in configuration 1). 4. loading each learnable parameters for the two separate features extractors and freezing their weights to only train the final classifier (the one with the bottleneck layer, as in configuration 2). 5. loading each learnable parameters for the two separate features extractors and only freezing the weights of the first 7 layers to only train the last two layers of each ResNet-50 (the sequential container that consists of 3 bottlenecks and the 2D adaptive average pooling) and the final classifier (the flat one, as in configuration 1 and 3). 6. loading each learnable parameters for the two separate features extractors and only freezing the weights of the first 7 layers to only train the last two layers of each ResNet-50 and the final classifier (the one with the bottleneck layer, as in configuration 2 and 4).</p><p>The results are shown in Figure <ref type="figure" coords="17,245.84,656.03,15.36,10.91" target="#fig_22">15a</ref> and Figure <ref type="figure" coords="17,314.80,656.03,13.64,10.91" target="#fig_22">15b</ref>. We can see that we truly improve our previous best score. On Kaggle, we submitted a solution which gave us 70.1% on the validation   set (see Figure <ref type="figure" coords="18,156.49,328.19,8.17,10.91" target="#fig_9">7b</ref>), which was taking into account the prediction of 8 features extractors (we didn't use NDVI at the time). Here, simply with the RGB and the TEMP patch, we obtain 69.9% Top-30 Error on the validation set. Several conclusions could be made from this test on the RGB+TEMP multi-modal CNN. First of all, it's better to load the learnable parameters into each feature extractor and freeze the weights than to start the training from scratch. Secondly, it's even better to unfreeze the last layers to re-train the features extractors. Moreover, having a bottleneck at the end of the model instead of a simple linear layer is always better, except when we start the training from scratch. Last but not least, it's useless to train for a lot of epochs when using the pre-trained feature extractors, because the Top-30 Error of the model starts very quickly to increase.</p><p>We were then able to launch our multi-modal convolutional neural network with the 9 separated features extractors. We launched three versions of the model:</p><p>1. one where we loaded each learnable parameters for the nine separate features extractors and froze their weights to only train the final classifier (the flat one, with a single linear layer), 2. one where we did the same thing but adding the bottleneck layer described above, 3. and one where we did the same thing but we unfroze the two last layers of each feature extractor.</p><p>This last option was the model with which we obtained our best score: a Top-30 Error on the validation set of 69.2%, beating our best Kaggle submission by almost 1%.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Perspectives for future work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Convolutional Neural Networks</head><p>With more time, it would have been interesting to try other options on our multi-modal convolutional neural network. For example, since unfreezing the last two layers was beneficial, we could have tried to unfreeze more layers (it would obviously have been interesting to also train the whole model from scratch, but we didn't have enough resources on our GPUs and we should have tried to implement parallel processing or to maybe work with a subset of the dataset). We could also have selected a part of the features extractors (like we did for RGB+TEMP but with others features) and have modified the bottleneck (by adding other linear layers and/or other activation functions than the ReLU).</p><p>It would have also been interesting to better optimize the hyper-parameters. Indeed, we kept a fixed learning-rate at 0.01 and we didn't add any weight decay, learning-rate decay, gradient value clipping, gradient norm clipping nor anything else.</p><p>It would also have been worth to try a different method of selection for the 3 temperature, precipitation and pedologic variables that we were keeping, because the MDI has several flaws including the fact that it is biased in presence of correlated features <ref type="bibr" coords="20,396.64,236.01,16.41,10.91" target="#b14">[15]</ref>. For example, using permutation importance <ref type="bibr" coords="20,202.51,249.56,16.38,10.91" target="#b15">[16]</ref>, defined as the decrease in a model score when a single feature value is randomly shuffled, would have maybe improved the variables selection.</p><p>We would also have liked to try to change the first 2D convolution layer of a ResNet-50 to change the number of channels in the input image from 3 to 37 (4 for remote sensing imagery, i.e. RGB-IR patches, 1 for land cover data, 1 for altitude data, 19 for bioclimatic data, 8 for pedologic data, 3 for GPS coordinates and 1 for NDVI patches) in order to directly pass a tensor that stacks every patch on top of each other (because we felt like stacking three times the same patch in order to have an image with three channels like we did for landcover data was not optimal). Moreover, especially for the non-RGB inputs, it would probably make more sense to normalize by the dataset statistics instead of ImageNet statistics (because the goal is to get to zero mean and unit standard deviation and the values we use to normalize our tensor images are referring to natural images).</p><p>Furthermore, implementing macro-average Top-K Error could maybe help us understand why the model trained only on altitude data achieves the worst accuracy among all other models trained on only one feature (it could be different with macro-average since species present at high altitude tend to be less common).</p><p>Last but not least, we stayed with ResNet-50 during the whole challenge, but maybe switching to other models adressing the task of image classification that are known to have better top-1 and top-5 accuracies on a dataset such as ImageNet-1K (like EfficientNet B7 <ref type="bibr" coords="20,441.28,506.99,18.07,10.91" target="#b16">[17]</ref> or simply ResNet-152) would have improved our overall score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Other models</head><p>We also wanted to find other ways to incorporate the localization information that could be also worth investigating, but lacked time to finish this task. Although, as shown in Section 3.4, the way we build COOR patches to fed this data to a CNN model provides comparable results to more direct approaches such as K-Nearest Neighbors, simpler and less costly models such as Multi-Layer Perceptron (MLP) might be sufficient to extract this information.</p><p>Finally, leveraging co-occurrences of species would also be an important next step. One way, for instance, to incorporate this information would be for each observation to build a vector having for dimension the number of different species (17,037) which components contain the distance from the given observation to the closest occurrence of each species.</p><p>We would have also liked to try the VisionTransformer model <ref type="bibr" coords="21,378.76,100.52,17.91,10.91" target="#b17">[18]</ref> which seems promising.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tables</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,258.96,392.38,9.96;3,141.38,84.19,312.53,163.09"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution of the occurrences of the species in both the training and validation sets.</figDesc><graphic coords="3,141.38,84.19,312.53,163.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,194.45,231.37,206.09,9.96;4,88.99,422.89,416.99,9.96;4,102.50,434.84,132.79,9.96"><head></head><label></label><figDesc>(a) Observations distribution in France and the US. (b) The test data (in red) is drawn from different spatial blocks than the training data (in blue). The validation data appears in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,452.71,417.79,9.96;4,89.29,464.67,29.60,9.96;4,193.47,243.42,208.35,172.59"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Observations distribution on the dataset and close-up view on the region around Montpellier, France.</figDesc><graphic coords="4,193.47,243.42,208.35,172.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,89.29,345.32,290.91,9.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pearson correlation coefficients of the bioclimatic covariates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,103.02,225.90,176.43,9.96;7,313.09,225.90,181.30,9.96;7,99.96,379.67,182.54,9.96;7,331.03,379.67,145.43,9.96"><head></head><label></label><figDesc>(a) Fitted using all environmental variables. (b) Fitted using solely temperature variables. (c) Fitted using solely precipitation variables.(d) Fitted using solely soil variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,89.29,396.43,416.69,9.96;7,89.29,408.38,271.38,9.96;7,89.29,237.96,204.19,134.84"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Feature importances computed as the Mean Decrease in Impurity (MDI) after running a Random Forest with 16 trees having each a maximum depth of 10.</figDesc><graphic coords="7,89.29,237.96,204.19,134.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,89.29,319.14,417.29,9.96;8,89.29,331.09,252.20,9.96;8,89.29,199.31,100.01,96.19"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of the patches used for observation n¬∞10010000. They all have a size of (256x256x3) even though they are not all showing the same spatial extent.</figDesc><graphic coords="8,89.29,199.31,100.01,96.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,89.29,340.79,354.20,9.96"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Evolution of the Top-30 Error of every individual patch on the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="11,125.13,231.83,155.58,6.72;11,191.03,237.97,21.79,7.48;11,99.91,226.53,10.93,6.72;11,99.91,210.21,10.93,6.72;11,99.91,193.88,10.93,6.72;11,99.91,177.55,10.93,6.72;11,99.91,161.23,10.93,6.72;11,99.91,144.90,10.93,6.72;11,99.91,128.57,10.93,6.72;11,99.91,112.24,10.93,6.72;11,99.91,95.92,10.93,6.72;11,91.32,190.81,7.48,12.34;11,91.32,161.50,7.48,27.60;11,91.32,141.36,7.48,18.43;11,91.32,125.67,7.48,13.99;11,122.62,199.64,15.59,6.72;11,143.05,187.89,15.59,6.72;11,163.48,150.66,15.59,6.72;11,183.91,110.83,15.59,6.72;11,204.34,162.42,15.59,6.72;11,224.77,156.54,15.59,6.72;11,245.20,137.60,15.59,6.72;11,265.63,151.97,15.59,6.72;11,135.54,89.41,132.78,8.98;11,88.99,255.01,204.48,9.96;11,102.15,266.97,24.69,9.96;11,333.60,231.43,161.51,6.88;11,401.13,238.31,22.32,6.87;11,312.57,212.62,6.40,6.88;11,312.57,185.85,6.40,6.88;11,312.57,159.09,6.40,6.88;11,312.57,132.32,6.40,6.88;11,312.57,105.55,6.40,6.88;11,304.36,166.38,6.87,28.64;11,304.36,145.75,6.87,18.88;11,304.36,129.67,6.87,14.33;11,331.04,109.45,15.98,6.88;11,351.97,134.88,15.98,6.88;11,372.90,142.91,36.91,6.88;11,414.77,157.63,36.91,9.56;11,456.63,152.28,36.91,8.22;11,352.69,86.22,119.20,8.25;11,301.50,255.01,204.48,9.96;11,315.26,266.97,63.23,9.96"><head></head><label></label><figDesc>validation Top-30 Error vs. Patches (a) Best validation Top-30 Error for every single patch. RGB +NIR +LAND +ALT +TEMP +PREC +PEDO +COOR 69.5% 70.0% 70.1% Validation Top-30 Error vs. Patches (b) Evolution of the validation Top-30 Error while adding patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="11,89.29,289.71,271.79,9.96"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performances on the validation set of the different runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="11,89.29,441.18,418.22,9.96;11,89.02,453.14,416.97,9.96;11,89.29,465.09,353.36,9.96;11,193.47,317.44,208.35,117.20"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Architecture of multi-modal convolutional neural network with separated features extractors. The individual patches are trained with ResNet-50 models whose last linear layers have been removed and replaced by a single new classifier having input samples of size 2,048 √ó 8 = 16,384.</figDesc><graphic coords="11,193.47,317.44,208.35,117.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="12,136.03,230.51,3.17,6.13;12,171.96,230.51,3.17,6.13;12,207.89,230.51,3.17,6.13;12,243.82,230.51,3.17,6.13;12,278.16,230.51,6.34,6.13;12,190.56,237.48,19.85,5.90;12,100.16,216.30,7.92,6.13;12,100.16,198.63,7.92,6.13;12,100.16,180.96,7.92,6.13;12,100.16,163.29,7.92,6.13;12,100.16,145.62,7.92,6.13;12,100.16,127.95,7.92,6.13;12,100.16,110.28,7.92,6.13;12,100.16,92.61,7.92,6.13;12,92.70,160.19,5.90,18.70;12,92.70,144.27,5.90,14.19;12,160.85,87.01,79.26,7.09;12,129.99,203.11,3.17,6.13;12,129.99,210.42,11.20,6.13;12,129.99,217.72,11.20,6.13;12,165.09,203.11,11.20,6.13;12,165.09,210.42,11.20,6.13;12,200.19,203.11,11.20,6.13;12,200.19,210.42,11.20,6.13;12,235.30,203.11,11.20,6.13;12,235.30,210.42,3.17,6.13;12,270.40,203.11,6.34,6.13;12,270.40,210.42,9.51,6.13;12,88.99,253.41,204.48,9.96;12,102.15,265.37,191.32,9.96;12,102.15,277.32,191.58,9.96;12,101.90,289.28,27.96,9.96;12,348.54,232.41,3.17,3.59;12,384.47,232.41,3.17,3.59;12,420.40,232.41,3.17,3.59;12,456.33,232.41,3.17,3.59;12,490.67,232.41,6.34,3.59;12,403.07,237.48,19.85,5.90;12,312.67,218.20,7.92,3.59;12,312.67,200.53,7.92,3.59;12,312.67,182.86,7.92,3.59;12,312.67,165.19,7.92,3.59;12,312.67,147.52,7.92,3.59;12,312.67,129.85,7.92,3.59;12,312.67,112.18,7.92,3.59;12,312.67,94.51,7.92,3.59;12,305.21,160.19,5.90,18.70;12,305.21,144.27,5.90,14.19;12,373.36,87.01,79.26,7.09;12,378.89,100.88,3.17,3.59;12,405.96,100.88,17.83,3.59;12,447.70,100.88,17.83,3.59;12,489.43,100.88,8.01,3.59;12,301.50,253.41,204.48,9.96;12,315.26,265.37,190.72,9.96;12,315.26,277.32,190.72,9.96;12,315.26,289.28,53.96,9.96"><head></head><label></label><figDesc>Evolution of the Top-30 Error of the multi-modal convolutional neural network with separated features extractors with different weight decay values. Evolution of the Top-30 Error of the multi-modal convolutional neural network with separated features extractors with different learning rate decay values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="12,89.29,306.88,416.69,9.96;12,89.29,318.83,416.70,9.96;12,89.29,330.79,383.12,9.96"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Evolution of the Top-30 Error of the multi-modal convolutional neural network with separated features extractors with different weight decay and learning rate decay values. The training Top-30 Errors are shown with dotted line and the validation Top-30 Errors are shown with solid lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="14,89.29,337.64,314.76,9.96"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Results of working with the altitude data with different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="15,89.29,309.50,322.67,9.96"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Results of working with the landcover data with different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="16,137.80,225.74,3.08,6.76;16,172.71,225.74,3.08,6.76;16,207.61,225.74,3.08,6.76;16,242.52,225.74,3.08,6.76;16,275.89,225.74,6.16,6.76;16,190.78,232.47,19.29,6.61;16,99.87,208.82,10.77,6.76;16,99.87,193.21,10.77,6.76;16,99.87,177.60,10.77,6.76;16,99.87,162.00,10.77,6.76;16,99.87,146.39,10.77,6.76;16,99.87,130.78,10.77,6.76;16,99.87,115.17,10.77,6.76;16,99.87,99.57,10.77,6.76;16,91.97,178.62,6.61,39.72;16,91.97,164.63,6.61,12.31;16,91.97,135.74,6.61,27.21;16,91.97,115.89,6.61,18.16;16,91.97,100.42,6.61,13.79;16,111.35,86.14,178.15,7.93;16,196.69,97.96,13.11,6.76;16,233.02,97.96,12.72,6.76;16,268.96,97.96,13.51,6.76;16,88.99,248.80,204.48,9.96;16,102.15,260.75,193.00,9.96;16,102.15,272.71,191.32,9.96;16,102.15,284.66,191.32,9.96;16,102.15,296.62,191.32,9.96;16,102.15,308.57,21.26,9.96;16,333.25,226.12,145.28,6.60;16,395.30,232.13,20.98,7.21;16,312.08,221.02,6.01,6.60;16,312.08,200.05,6.01,6.60;16,312.08,179.09,6.01,6.60;16,312.08,158.13,6.01,6.60;16,312.08,137.17,6.01,6.60;16,312.08,116.20,6.01,6.60;16,312.08,95.24,6.01,6.60;16,303.80,213.76,7.21,11.88;16,303.80,173.34,7.21,38.78;16,303.80,159.67,7.21,12.02;16,303.80,131.45,7.21,26.58;16,303.80,112.06,7.21,17.74;16,303.80,96.95,7.21,13.47;16,332.14,121.54,15.02,6.60;16,358.60,114.21,15.02,6.60;16,385.06,110.02,15.02,6.60;16,411.51,119.45,15.02,6.60;16,437.97,112.11,41.47,8.70;16,309.41,89.08,192.77,8.65;16,301.50,248.80,204.48,9.96;16,315.26,260.75,192.25,9.96;16,315.26,272.71,190.72,9.96;16,314.89,284.66,191.09,9.96;16,315.26,296.62,190.72,9.96;16,315.26,308.57,37.96,9.96"><head></head><label></label><figDesc>Evolution of the Top-30 Error of the models trained on environmental features with different patches. Patches using the conversion to ints are shown with dotted line and patches with no conversion (keeping floats) are shown with solid lines. validation Top-30 Error vs. Patches (b) Best Top-30 Error of the models trained on environmental features with different patches. Patches using the conversion to ints are shown with diagonal hatching and patches with no conversion (keeping floats) are shown without hatching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16" coords="16,89.29,331.31,353.19,9.96"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Results of working with the environmental features with different patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17" coords="17,138.39,231.89,3.12,7.56;17,173.80,231.89,3.12,7.56;17,209.21,231.89,3.12,7.56;17,244.62,231.89,3.12,7.56;17,278.47,231.89,6.25,7.56;17,192.14,249.14,19.56,8.15;17,99.91,226.59,10.93,7.56;17,99.91,204.82,10.93,7.56;17,99.91,183.05,10.93,7.56;17,99.91,161.28,10.93,7.56;17,99.91,139.51,10.93,7.56;17,99.91,117.74,10.93,7.56;17,99.91,95.97,10.93,7.56;17,90.82,190.76,8.15,13.81;17,90.82,161.45,8.15,27.60;17,90.82,141.31,8.15,18.43;17,90.82,125.62,8.15,13.99;17,135.96,89.50,131.91,9.78;17,132.44,212.09,31.71,7.56;17,132.44,219.29,25.75,7.56;17,187.73,212.09,18.17,7.56;17,187.73,219.29,19.95,7.56;17,231.25,212.09,18.17,7.56;17,231.25,219.29,44.38,7.56;17,88.99,267.08,204.48,9.96;17,102.15,279.04,191.32,9.96;17,102.15,290.99,191.32,9.96;17,102.15,302.95,191.32,9.96;17,102.15,314.90,191.92,9.96;17,102.15,326.86,108.82,9.96;17,329.05,231.19,19.93,7.75;17,335.94,236.82,6.17,7.75;17,337.43,242.45,3.20,7.75;17,353.04,231.19,13.82,7.75;17,358.36,236.82,3.20,7.75;17,373.98,231.19,13.82,7.75;17,375.41,236.82,10.96,7.75;17,396.70,231.19,10.24,7.75;17,391.86,236.82,19.90,7.75;17,395.75,242.45,12.13,7.75;17,415.84,231.19,13.82,7.75;17,421.16,236.82,3.20,7.75;17,436.77,231.19,13.82,7.75;17,441.18,236.82,5.01,7.75;17,457.71,231.19,13.82,7.75;17,463.02,236.82,3.20,7.75;17,480.43,231.19,10.24,7.75;17,475.60,236.82,19.90,7.75;17,479.49,242.45,12.13,7.75;17,400.06,249.38,24.45,7.67;17,312.57,225.76,6.40,7.75;17,312.57,198.99,6.40,7.75;17,312.57,172.23,6.40,7.75;17,312.57,145.46,6.40,7.75;17,312.57,118.70,6.40,7.75;17,312.57,91.93,6.40,7.75;17,303.77,197.75,7.67,12.64;17,303.77,181.85,7.67,14.15;17,303.77,151.82,7.67,28.28;17,303.77,131.19,7.67,18.88;17,303.77,115.11,7.67,14.33;17,331.04,120.19,15.98,7.75;17,351.97,101.99,15.98,7.75;17,372.90,161.41,15.98,7.75;17,393.84,119.65,15.98,7.75;17,414.77,188.71,78.77,10.42;17,333.43,85.91,157.71,9.20;17,301.50,267.08,204.48,9.96;17,315.26,279.04,192.41,9.96;17,315.26,290.99,190.73,9.96;17,315.26,302.95,190.72,9.96;17,314.89,314.90,191.09,9.96;17,314.89,326.86,72.56,9.96"><head></head><label></label><figDesc>Evolution of the Top-30 Error of the models trained on ndvi patches with different methods for handling divisions by 0. Methods using the conversion to ints are shown with dotted line and methods with no conversion (keeping floats) are shown with solid lines. 75.7% 75.9% 75.4% Best NDVI validation Top-30 Error vs. Methods (b) Best Top-30 Error of the models trained on ndvi patches with different methods for handling divisions by 0. Methods using the conversion to ints are shown with diagonal hatching and methods with no conversion (keeping floats) are shown without hatching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18" coords="17,89.29,344.73,416.94,9.96;17,89.29,356.68,6.82,9.96"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Results of working with the NDVI patches with different methods for handling divisions by 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19" coords="18,138.39,231.83,3.12,6.72;18,173.80,231.83,3.12,6.72;18,209.21,231.83,3.12,6.72;18,244.62,231.83,3.12,6.72;18,278.47,231.83,6.25,6.72;18,192.14,238.55,19.56,6.71;18,99.91,226.53,10.93,6.72;18,99.91,207.87,10.93,6.72;18,99.91,189.21,10.93,6.72;18,99.91,170.56,10.93,6.72;18,99.91,151.90,10.93,6.72;18,99.91,133.24,10.93,6.72;18,99.91,114.58,10.93,6.72;18,99.91,95.92,10.93,6.72;18,91.90,190.99,6.71,11.97;18,91.90,161.68,6.71,27.60;18,91.90,141.54,6.71,18.43;18,91.90,125.85,6.71,13.99;18,137.06,90.11,129.71,8.05;18,159.23,102.20,10.58,6.72;18,159.23,109.40,8.53,6.72;18,159.23,116.60,13.67,6.72;18,196.46,102.20,8.42,6.72;18,196.46,109.40,13.29,6.72;18,233.31,102.20,12.90,6.72;18,233.31,109.40,13.70,6.72;18,270.57,102.20,14.56,6.72;18,270.57,109.40,12.20,6.72;18,88.99,255.01,204.48,9.96;18,102.15,266.97,110.22,9.96"><head></head><label></label><figDesc>Evolution of the Top-30 Error of every individual patch on the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20" coords="18,89.29,289.71,381.15,9.96"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Performances on the validation set of the models trained on a single type of data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21" coords="19,301.50,497.54,204.48,9.96;19,315.10,509.50,190.88,9.96;19,314.85,521.45,56.87,9.96"><head></head><label></label><figDesc>(d) Best Top-30 Error of the multi-modal CNN with the nine separated features extractors on the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22" coords="19,89.29,538.21,416.69,9.96;19,89.29,550.17,146.17,9.96"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Performances on the validation set of the multi-modal convolutional neural network with 2 and 9 separated features extractors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23" coords="25,89.29,236.15,306.88,9.96;25,224.72,84.19,145.85,140.29"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: NDVI image of observation n¬∞10171444 with a size of (256x256).</figDesc><graphic coords="25,224.72,84.19,145.85,140.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="23,89.29,146.85,416.69,214.71"><head></head><label></label><figDesc></figDesc><graphic coords="23,89.29,146.85,416.69,214.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="24,89.29,84.19,416.70,290.86"><head></head><label></label><figDesc></figDesc><graphic coords="24,89.29,84.19,416.70,290.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="24,89.29,419.34,416.66,146.50"><head></head><label></label><figDesc></figDesc><graphic coords="24,89.29,419.34,416.66,146.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="26,88.99,198.86,338.40,366.69"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="26,89.29,209.93,338.10,355.62"><row><cell>Environmental variables</cell><cell></cell><cell></cell></row><row><cell>Name</cell><cell>Description</cell><cell>Resolution</cell></row><row><cell>bio_1</cell><cell>Annual Mean Temperature</cell><cell>30 arcsec</cell></row><row><cell>bio_2</cell><cell>Mean Diurnal Range</cell><cell>30 arcsec</cell></row><row><cell>bio_3</cell><cell>Isothermality</cell><cell>30 arcsec</cell></row><row><cell>bio_4</cell><cell>Temperature Seasonality</cell><cell>30 arcsec</cell></row><row><cell>bio_5</cell><cell>Max Temperature of Warmest Month</cell><cell>30 arcsec</cell></row><row><cell>bio_6</cell><cell>Min Temperature of Coldest Month</cell><cell>30 arcsec</cell></row><row><cell>bio_7</cell><cell>Temperature Annual Range</cell><cell>30 arcsec</cell></row><row><cell>bio_8</cell><cell cols="2">Mean Temperature of Wettest Quarter 30 arcsec</cell></row><row><cell>bio_9</cell><cell>Mean Temperature of Driest Quarter</cell><cell>30 arcsec</cell></row><row><cell cols="3">bio_10 Mean Temperature of Warmest Quarter 30 arcsec</cell></row><row><cell>bio_11</cell><cell cols="2">Mean Temperature of Coldest Quarter 30 arcsec</cell></row><row><cell>bio_12</cell><cell>Annual Precipitation</cell><cell>30 arcsec</cell></row><row><cell>bio_13</cell><cell>Precipitation of Wettest Month</cell><cell>30 arcsec</cell></row><row><cell>bio_14</cell><cell>Precipitation of Driest Month</cell><cell>30 arcsec</cell></row><row><cell>bio_15</cell><cell>Precipitation Seasonality</cell><cell>30 arcsec</cell></row><row><cell>bio_16</cell><cell>Precipitation of Wettest Quarter</cell><cell>30 arcsec</cell></row><row><cell>bio_17</cell><cell>Precipitation of Driest Quarter</cell><cell>30 arcsec</cell></row><row><cell>bio_18</cell><cell>Precipitation of Warmest Quarter</cell><cell>30 arcsec</cell></row><row><cell>bio_19</cell><cell>Precipitation of Coldest Quarter</cell><cell>30 arcsec</cell></row><row><cell>orcdrc</cell><cell>Soil Organic Carbon Content</cell><cell>250 m</cell></row><row><cell>phihox</cell><cell>Ph x 10 in H20</cell><cell>250 m</cell></row><row><cell>cecsol</cell><cell>Cation Exchange Capacity of Soil</cell><cell>250 m</cell></row><row><cell>bdticm</cell><cell>Absolute Depth to Bedrock</cell><cell>250 m</cell></row><row><cell>clyppt</cell><cell>Clay Mass Fraction</cell><cell>250 m</cell></row><row><cell>sltppt</cell><cell>Silt Mass Fraction</cell><cell>250 m</cell></row><row><cell>sndppt</cell><cell>Sand Mass Fraction</cell><cell>250 m</cell></row><row><cell>bldfie</cell><cell>Bulk Density</cell><cell>250 m</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,108.93,671.02,291.00,8.97"><p>https://www.kaggle.com/competitions/geolifeclef-2022-lifeclef-2022-fgvc9/data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="9,108.93,660.07,102.90,8.97"><p>https://www.image-net.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="9,108.93,671.02,110.94,8.97"><p>https://pytorch.org/docs/1.8.1/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Thanks to the <rs type="funder">French National Research Agency</rs> (under the <rs type="programName">Investments for the Future Program</rs>, referred as <rs type="grantNumber">ANR-16-CONV-0004</rs>) and to the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation program</rs> (under grant agreement No <rs type="grantNumber">863463</rs> known as <rs type="projectName">Cos4Cloud</rs> project) for funding the <rs type="grantNumber">GeoLifeCLEF2022</rs> project. The authors are grateful to the OPAL infrastructure from <rs type="institution">Universit√© C√¥te d'Azur</rs> for providing resources and support.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_cxpR5f9">
					<idno type="grant-number">ANR-16-CONV-0004</idno>
					<orgName type="program" subtype="full">Investments for the Future Program</orgName>
				</org>
				<org type="funded-project" xml:id="_rqEHp9x">
					<idno type="grant-number">863463</idno>
					<orgName type="project" subtype="full">Cos4Cloud</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation program</orgName>
				</org>
				<org type="funding" xml:id="_w5DPGC2">
					<idno type="grant-number">GeoLifeCLEF2022</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Visualization</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="21,112.66,271.96,394.62,10.91;21,112.66,285.51,393.33,10.91;21,112.66,299.06,394.53,10.91;21,112.66,312.61,22.69,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="21,359.16,271.96,121.10,10.91;21,112.66,285.51,393.33,10.91;21,112.66,299.06,17.41,10.91">Predicting species presence from multi-modal remote sensing, bioclimatic and pedologic data</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,154.12,299.06,347.69,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of GeoLifeCLEF</note>
</biblStruct>

<biblStruct coords="21,112.66,326.16,394.53,10.91;21,112.66,339.71,394.53,10.91;21,112.66,353.26,393.33,10.91;21,112.66,366.81,393.33,10.91;21,112.66,380.36,353.54,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="21,198.52,353.26,307.47,10.91;21,112.66,366.81,247.50,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,383.00,366.81,122.99,10.91;21,112.66,380.36,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,393.91,394.53,10.91;21,112.66,407.46,393.98,10.91;21,112.66,421.01,38.81,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="21,112.66,407.46,263.64,10.91">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,383.78,407.46,87.18,10.91">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,434.55,394.53,10.91;21,112.33,448.10,316.44,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04192</idno>
		<title level="m" coord="21,112.33,448.10,134.48,10.91">The GeoLifeCLEF 2020 dataset</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="21,112.66,461.65,393.33,10.91;21,112.66,475.20,255.52,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="21,183.25,461.65,322.74,10.91;21,112.66,475.20,124.27,10.91">Contrastive representation learning for natural world imagery: Habitat prediction for 30,000 species</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Seneviratne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,245.12,475.20,91.15,10.91">CLEF working notes</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,488.75,395.00,10.91;21,112.66,502.30,395.17,10.91;21,112.66,515.85,203.73,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pisani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Purves</surname></persName>
		</author>
		<ptr target="https://www.amazon.com/Statistics-Fourth-International-Student-Freedman/dp/0393930432" />
		<title level="m" coord="21,267.33,488.75,205.63,10.91">Statistics: Fourth international student edition</title>
		<imprint>
			<publisher>W. W. Norton &amp; Company</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,529.40,277.53,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="21,167.61,529.40,67.74,10.91">Random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,243.28,529.40,78.19,10.91">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,542.95,393.33,10.91;21,112.66,556.50,311.68,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="21,165.35,542.95,294.92,10.91">Manual on setting up, using, and understanding random forests v3. 1</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="42" />
			<pubPlace>CA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Statistics Department University of California Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,570.05,394.53,10.91;21,112.28,583.60,395.55,10.91;21,112.66,597.15,236.19,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="21,248.54,583.60,215.36,10.91">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,472.25,583.60,35.58,10.91;21,112.66,597.15,147.19,10.91">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,610.69,395.17,10.91;21,112.66,624.24,395.01,10.91;21,112.41,637.79,38.81,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="21,259.74,610.69,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,488.38,610.69,19.45,10.91;21,112.66,624.24,347.24,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,651.34,394.53,10.91;22,112.66,86.97,393.33,10.91;22,112.66,100.52,350.57,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="22,260.06,86.97,245.92,10.91;22,112.66,100.52,67.64,10.91">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,188.20,100.52,230.24,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,114.06,394.53,10.91;22,112.66,127.61,146.39,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Van Brummelen</surname></persName>
		</author>
		<title level="m" coord="22,202.89,114.06,299.56,10.91">Heavenly mathematics: The forgotten art of spherical trigonometry</title>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,141.16,393.33,10.91;22,112.66,154.71,394.53,10.91;22,112.66,168.26,148.80,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="22,489.04,141.16,16.95,10.91;22,112.66,154.71,389.89,10.91">The normalized difference vegetation index (ndvi): unforeseen successes in animal ecology</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Pettorelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Bunnefeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Jƒôdrzejewska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kausrud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,112.66,168.26,75.01,10.91">Climate research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="15" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,181.81,393.33,10.91;22,112.66,195.36,107.17,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m" coord="22,191.43,181.81,228.34,10.91">Deep learning using rectified linear units (relu)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="22,112.66,208.91,394.61,10.91;22,112.66,222.46,311.02,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="22,247.28,208.91,259.99,10.91;22,112.66,222.46,143.22,10.91">Predictor correlation impacts machine learning algorithms: implications for genomic studies</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Nicodemus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Malley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,263.99,222.46,65.61,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1884" to="1890" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,236.01,393.33,10.91;22,112.66,249.56,256.77,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="22,312.54,236.01,193.45,10.91;22,112.66,249.56,88.19,10.91">Permutation importance: a corrected feature importance measure</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Altmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tolo≈üi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lengauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,209.74,249.56,65.61,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1340" to="1347" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,263.11,394.53,10.91;22,112.66,276.66,346.82,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="22,178.42,263.11,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,127.29,276.66,202.02,10.91">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,290.20,395.16,10.91;22,112.66,303.75,395.17,10.91;22,112.66,317.30,349.55,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m" coord="22,330.08,303.75,177.76,10.91;22,112.66,317.30,167.84,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
