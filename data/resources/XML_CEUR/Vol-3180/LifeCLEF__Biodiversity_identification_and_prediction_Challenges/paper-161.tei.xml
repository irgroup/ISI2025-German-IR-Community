<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,358.41,15.42">Few-shot Long-Tailed Bird Audio Recognition</title>
				<funder>
					<orgName type="full">Humboldt Foundation (JMU Würzburg)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,113.06,82.62,11.96"><forename type="first">Marcos</forename><forename type="middle">V</forename><surname>Conde</surname></persName>
							<email>marcos.conde-osorio@uni-wuerzburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<orgName type="institution">University of Würzburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,195.36,113.06,55.16,11.96"><forename type="first">Ui-Jin</forename><surname>Choi</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>MegaStudyEdu</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,92.44,139.05,24.54,7.99"><forename type="first">H2o</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<orgName type="institution">University of Würzburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,358.41,15.42">Few-shot Long-Tailed Bird Audio Recognition</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">3353D4A583E22583A6A2CFADF6231390</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>BirdCLEF2022</term>
					<term>LifeCLEF2022</term>
					<term>Deep Learning</term>
					<term>Sound Event Detection</term>
					<term>Audio Recognition</term>
					<term>CNN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is easier to hear birds than see them. However, they still play an essential role in nature and are excellent indicators of deteriorating environmental quality and pollution. Recent advances in Deep Neural Networks allow us to process audio data to detect and classify birds. This technology can assist researchers in monitoring bird populations and biodiversity. We propose a sound detection and classification pipeline to analyze complex soundscape recordings and identify birdcalls in the background. Our method learns from weak labels and few data and acoustically recognizes the bird species. Our solution achieved 18th place of 807 teams at the BirdCLEF 2022 Challenge hosted on Kaggle. Code and models will be open-sourced at https://github.com/Choiuijin1125/bclef2022.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The BirdCLEF 2022 Challenge <ref type="bibr" coords="1,223.50,383.57,11.27,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,237.51,383.57,8.90,10.91" target="#b1">2]</ref> proposes to identify which birds are calling in long recordings given quite limited training data. This is the exact challenge faced by scientists trying to monitor rare birds in Hawaii. However, we propose a novel machine learning solution to help advance the science of bioacoustics and support ongoing research to protect endangered Hawaiian birds.</p><p>The motivation behind this challenge and our solution is the fact that Hawaii has lost 68% of its bird species <ref type="bibr" coords="1,169.27,464.86,11.43,10.91" target="#b2">[3]</ref>.</p><p>Researchers use population bioacoustic monitoring to understand how native birds react to changes in the environment and conservation efforts. This approach could provide passive, low labor, and cost-effective strategy for studying endangered bird populations. Current methods for processing large bioacoustic datasets involve manual annotation of each recording. This is an expensive process that requires specialized training and large amounts of time. For this reason, we propose a Machine Learning solution to automatically identify bird species in long audio recordings via birdcall detection and classification within the audio. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Recent advances in Machine Learning (ML) have made it possible to automatically identify bird songs for common species using annotated soundscapes as training data.</p><p>The main challenges from the machine learning point of view are:</p><p>1. Weak labels. Training data consists of soundscapes of variable duration, recorded in the wild. Therefore, we find substantial noise in the recordings (other birds besides the target, rain, wind, planes, etc). 2. Long-tailed distribution. Rare and endangered species (such as those in Hawaii) are less represented in the training data, and therefore, the model struggles to learn their features and generalize for those classes. 3. Few-shot training is required. We find less than four recordings for some endemic bird species (crehon, hawhaw, maupar, etc). The most represented bird is "skylar" with 500 recordings, which is not a tremendous amount of training data in the context of ML.</p><p>Previous years BirdCLEF challenges <ref type="bibr" coords="2,270.77,439.25,11.48,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,286.13,439.25,9.03,10.91" target="#b4">5]</ref> proposed different problems related to largescale bird recognition in soundscapes or complex acoustic environments. Sprengel et.al. <ref type="bibr" coords="2,492.99,452.79,13.00,10.91" target="#b5">[6]</ref> and Lasseck <ref type="bibr" coords="2,146.40,466.34,11.45,10.91" target="#b6">[7,</ref><ref type="bibr" coords="2,160.59,466.34,9.01,10.91" target="#b7">8]</ref> introduced deep learning techniques for the "Bird species identification in soundscapes" problem. State-of-the-art (SOTA) solutions are based on Deep Convolutional Neural Networks (CNNs) <ref type="bibr" coords="2,200.67,493.44,11.24,10.91" target="#b8">[9,</ref><ref type="bibr" coords="2,214.22,493.44,12.50,10.91" target="#b9">10,</ref><ref type="bibr" coords="2,229.03,493.44,12.23,10.91" target="#b10">11]</ref>, usually, deep CNNs with attention mechanisms are selected as backbone in these experiments <ref type="bibr" coords="2,247.81,506.99,16.55,10.91" target="#b11">[12,</ref><ref type="bibr" coords="2,267.77,506.99,12.59,10.91" target="#b12">13,</ref><ref type="bibr" coords="2,283.77,506.99,12.59,10.91" target="#b13">14,</ref><ref type="bibr" coords="2,299.77,506.99,12.42,10.91" target="#b14">15]</ref>, or suitable for fine-grained classification tasks <ref type="bibr" coords="2,114.26,520.54,16.15,10.91" target="#b15">[16]</ref>. Pretrained audio neural networks (PANNs) <ref type="bibr" coords="2,329.36,520.54,17.82,10.91" target="#b13">[14]</ref> provide a multi-task SOTA baseline for audio related tasks, showing great generalization capability. Other approaches are focused on Sound Event Detection (SED) <ref type="bibr" coords="2,242.31,547.64,16.55,10.91" target="#b16">[17,</ref><ref type="bibr" coords="2,262.16,547.64,12.59,10.91" target="#b17">18,</ref><ref type="bibr" coords="2,278.04,547.64,12.59,10.91" target="#b13">14,</ref><ref type="bibr" coords="2,293.92,547.64,12.42,10.91" target="#b18">19]</ref>, similar to video understanding <ref type="bibr" coords="2,458.25,547.64,16.41,10.91" target="#b19">[20]</ref>, these approaches usually employ 2D CNNs to extract useful features from the input audio signal (log-melspectrogram), these features still contain information about frequency and time, then recurrent neural networks (RNNs) are used to model longer temporal context from the extracted features or use the feature map directly to predict since it preserves time segment information.</p><p>Solutions for the BirdCLEF 2021 Challenge follow these directions, moreover, they propose additional post-processing techniques to eliminate false detections (FPs) <ref type="bibr" coords="2,425.78,628.93,16.41,10.91" target="#b20">[21]</ref>, divers CNNbased ensembles <ref type="bibr" coords="2,165.42,642.48,16.39,10.91" target="#b21">[22,</ref><ref type="bibr" coords="2,184.55,642.48,12.53,10.91" target="#b22">23,</ref><ref type="bibr" coords="2,199.82,642.48,12.53,10.91" target="#b23">24,</ref><ref type="bibr" coords="2,215.08,642.48,12.53,10.91" target="#b24">25,</ref><ref type="bibr" coords="2,230.35,642.48,12.53,10.91" target="#b25">26,</ref><ref type="bibr" coords="2,245.62,642.48,12.30,10.91" target="#b26">27]</ref>, and transformer-based solutions like STFT <ref type="bibr" coords="2,456.27,642.48,16.21,10.91" target="#b27">[28]</ref>. These solutions can identify birds in long audio recordings, at different locations (Colombia, USA, and Costa Rica), with ≈ 70% accuracy. In this challenge, we focus only on Hawaiian Bird Species.</p><p>We define some terms related to this challenge referring last year competition solution <ref type="bibr" coords="3,473.62,86.97,16.30,10.91" target="#b22">[23,</ref><ref type="bibr" coords="3,492.03,86.97,13.95,10.91" target="#b21">22]</ref> that we will use in the description of our method in Section 2:</p><p>• Leaderboard denoted as LB (including its two variants, public and private) • Cross-Validation denoted as CV.</p><p>• We define "nocall" as the class corresponding to the events (a.k.a segments or clips) in audio where birdcalls are not detected. • We refer to the "BirdCLEF 2021 Birdcall Identification Challenge (Kaggle)" as the "previous or last competition". • We define "weakly labeled" as the labels that do not contain time-wise information about bird species in audio clips (i.e., not specific information about in which 5s segment in the audio, the bird calls). • We define "strongly labeled" as the labels that contain time-wise information about bird species in audio clips (i.e., approximate second within the audio, where the bird calls). • BirdCLEF 2021 train soundscapes audios denoted as "train soundscapes" which are 20 audio clips strongly labeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Dataset</head><p>The training set consists of short audio recordings of 152 bird species, and only 21 bird species of interest are scored. These bird species inhabit Hawaii. However, many of the remaining birds across the islands are isolated in difficult-to-access, high-elevation habitats. Therefore, physical monitoring is difficult, and scientists have turned to sound recordings. As we show in Figure <ref type="figure" coords="3,499.80,387.11,3.69,10.91" target="#fig_1">2</ref>, the distribution of the "interesting" bird species is very long-tailed <ref type="bibr" coords="3,390.25,400.66,17.41,10.91" target="#b28">[29]</ref>, making it necessary to deal with extreme class imbalance. As we introduced, in this competition, our challenge is to develop ML models to identify bird species using sounds. Such models have to deal with real-world problems such as long-tailed rare birds and weak-noisy labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Evaluation</head><p>The performance is measured using a custom metric that is most similar to the "macro F1 score". The test set consists of approximately 5500 recordings. Participants submit code and models and never have access to the test audios. There is a public LB that shows the score corresponding to 16% of the test (880 audios), and a private or final LB with the scores over the rest 84%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preprocessing</head><p>Previous BirdCLEF challenges <ref type="bibr" coords="4,226.12,227.78,11.37,10.91" target="#b3">[4,</ref><ref type="bibr" coords="4,240.21,227.78,8.97,10.91" target="#b4">5]</ref> showed that long audio clips for training improve performance. For this reason, we randomly cropped a 30-second time window of each audio, next, we split the 30s audio clips into 5s and 6-parts chucks as proposed by Henkel et al. <ref type="bibr" coords="4,439.51,254.88,16.15,10.91" target="#b21">[22]</ref>, finally we transformed the such chunks to Mel Spectrogram using torchaudio library. The spectrograms were generated using the following parameters: sample_rate=32000, n_mels=128, fmax=14000, fmin=50 hop_size=512, hop_size=512,top_db=None.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Augmentations</head><p>After splitting audios, we applied 3 types of augmentations to handle robustness and the long-tailed distribution problem. First, we used three external datasets freefield1010 <ref type="bibr" coords="4,483.35,359.74,19.07,9.76" target="#b29">[30]</ref>, BirdVox-DCASE-20k <ref type="bibr" coords="4,183.65,373.29,22.20,9.76" target="#b30">[31]</ref>, train_soundscapes (from 2021 Challenge) for background noise. Second, to handle class imbalance, we used selective mixup <ref type="bibr" coords="4,357.48,385.90,17.76,10.91" target="#b31">[32]</ref> which only uses the 21 scored birds of interest in the audio clip. In every training batch, we fed randomly cropped scored birds Mel Spectrograms and used mixup with training data. Next, we applied spec-augmentations <ref type="bibr" coords="4,484.23,413.00,18.74,10.91" target="#b32">[33]</ref>. This method showed good performance in our local validation (CV), especially selective mixup boosted + 0.03 our score. However, we also observed an overfitting behavior in some classes. because some scored birds (21 classes as in Figure <ref type="figure" coords="4,217.63,467.19,4.16,10.91" target="#fig_1">2</ref>) have a long-tail distribution, model tends to predict more high confidence scores for some birds like a "skylar" and "houfin" which has a large distribution. Figure <ref type="figure" coords="4,120.36,494.29,5.07,10.91" target="#fig_2">3</ref> shows the pre-processing pipeline and augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Modeling</head><p>We used 9 different backbones and 22 models. Our models are a combination of top solutions from previous competition <ref type="bibr" coords="4,209.77,557.57,11.35,10.91" target="#b3">[4]</ref>. The main architectures are CNNs backbones with Sound Event Detection heads <ref type="bibr" coords="4,162.67,571.12,16.14,10.91" target="#b18">[19]</ref>, which showed good performance in the previous challenges <ref type="bibr" coords="4,452.28,571.12,11.32,10.91" target="#b3">[4]</ref>. We feed 5-second, 6-part Mel Spectrograms into the network as <ref type="bibr" coords="4,332.97,584.67,16.12,10.91" target="#b21">[22]</ref>. Figure <ref type="figure" coords="4,386.94,584.67,4.99,10.91" target="#fig_3">4</ref> shows the pipeline of our models. We also tried ConformerSED <ref type="bibr" coords="4,246.21,598.21,20.40,10.91" target="#b33">[34]</ref>, FDY-SED <ref type="bibr" coords="4,309.38,598.21,20.09,10.91" target="#b34">[35]</ref>, HTS-AT <ref type="bibr" coords="4,369.31,598.21,21.55,10.91" target="#b35">[36]</ref> but the results were much worse than using well-known CNN approaches. We show the different backbones in Table <ref type="table" coords="4,500.09,611.76,3.79,10.91">1</ref>. In particular we focus on tf_efficientnet_b0_ns since it is light-weight and suitable for smartphone devices, and its performance is consistently competitive.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Training</head><p>We trained our models using focal binary cross-entropy loss, AdamW optimizer, and cosine annealing scheduler with batch size 24. We also used the quality rating, which is meta information on audio quality. While computing the loss, we weight it using the normalized quality rating, and we used one-sided label smoothing, adding 0.01 across all negative labels. Both methods were proposed by Henkel et al. <ref type="bibr" coords="5,229.54,543.45,17.91,10.91" target="#b21">[22]</ref> and improved our performance consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Post-Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1.">Penalization</head><p>We observed that our models are biased, and tend to predict with high confidence scores the most represented birds (i.e. skylar and houfin). This implies a large number of False Positives (FP) and misclassified clips. We give a penalty score depending on the distribution of the birds, such that most represented birds are more penalized. Penalization (PN) can be explained as in Eq.1 where x is the distribution of each bird. We used penalty factor=0.8. Penalization is not a realistic technique, the model should not filter out bird species in that way, yet, in this scenario it boosted our score on the public LB. As a better alternative, we aim to make our method less sensitive to the data distribution and robust against background birdcalls from non-interest birds, we tried to find class-wise thresholds of each bird species.</p><formula xml:id="formula_0" coords="6,222.68,148.06,283.30,27.09">p 𝑖 = p 𝑖 -penalty factor × 𝑥 𝑖 ∑︀ 𝑛 𝑖 𝑥 𝑖<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2.">Class-Wise Thresholds</head><p>We observed that if there is a birdcall in the audio clips, regardless of the bird species, our models show higher confidence scores. We used train_soundscape audio clips to validate nocall thresholds for bird species using AUC score per each bird (as a binary classification problem call/nocall). Even though there is no label for scored birds in train_soundscape, we can estimate the appropriate nocall/birdcall thresholds for each bird. We used a grid search method to find the best nocall quantile-based threshold of each bird, such that we achieve the maximum AUC score per each bird, or in other words, such that we can distinguish better birdcalls from noise or background, independent from the bird species present in the audio. Figure <ref type="figure" coords="6,451.55,304.60,5.17,10.91" target="#fig_4">5</ref> shows the distribution of probabilities using train_soundscapes. This Class-wise (CW) post-processing method further boosted our score in comparison to Penalization and is more robust. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results and discussion</head><p>Table <ref type="table" coords="7,115.53,111.28,5.02,10.91">1</ref> summarizes our experiments. We tested 9 different CNN backbones. We found difficult to calibrate thresholds using an ensemble of models, yet, we used quantile-based thresholds on the ensemble predictions. Penalization (PN) showed good performance in Public LB. However, penalizing common birds as skylar or houfin that most probably appear in most of the audios is not realistic. On the other hand, the Class-wise (CW) method showed better performance in general, and it is robust to background birds. We find that calibration of thresholds is very sensitive because there are very few rare birds in audio clips and the real world. Our results imply that we can find proper thresholds for each rare bird using nocall/birdcall validation and a quantile-based approach without strongly labeled data, as we show in Figure <ref type="figure" coords="7,441.62,219.67,3.74,10.91" target="#fig_4">5</ref>.</p><p>We also provide qualitative Grad-CAM <ref type="bibr" coords="7,271.73,233.22,17.76,10.91" target="#b36">[37]</ref> results of our model tf_efficientnet_b0_ns in Figure <ref type="figure" coords="7,132.73,246.77,3.81,10.91" target="#fig_5">6</ref>, which shows how our model is able to learn and focus on particular frequencies and segments within the audio, and it is robust against background noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Experiments result of models. For local validation, we used "micro F1-score" and train soundscapes. We highlight in blue our top-3 models, in yellow the results of our final submission ensemble, and in green the top solutions in the challenge LB. We also distinguish two post-processing methods: PN and CW. Contemporary results from other competitors can be found at <ref type="bibr" coords="7,345.94,336.75,15.05,8.87" target="#b37">[38,</ref><ref type="bibr" coords="7,363.48,336.75,11.46,8.87" target="#b38">39,</ref><ref type="bibr" coords="7,377.43,336.75,11.46,8.87" target="#b39">40,</ref><ref type="bibr" coords="7,391.38,336.75,11.29,8.87" target="#b40">41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>CV Public LB Private LB Post-Proc. tf_efficientnet_b0_ns <ref type="bibr" coords="7,236.27,364.78,16.46,8.87" target="#b12">[13]</ref> 0.8745 0.7922 0.7240 PN tf_efficientnet_b0_ns <ref type="bibr" coords="7,236.27,376.74,16.46,8.87" target="#b12">[13]</ref> 0.8745 0.7817 0.7548 CW eca_nfnet_l0 <ref type="bibr" coords="7,215.01,388.70,16.46,8.87" target="#b41">[42]</ref> 0.8761 0.7510 0.7387 CW resnest50d <ref type="bibr" coords="7,210.73,400.65,16.46,8.87" target="#b11">[12]</ref> 0.8822 0.7550 0.7372 CW tf_efficientnet_b1_ns <ref type="bibr" coords="7,232.10,412.61,16.46,8.87" target="#b12">[13]</ref> 0.7843 0.7395 0.6946 CW tf_efficientnet_b2_ns <ref type="bibr" coords="7,232.10,424.56,16.46,8.87" target="#b12">[13]</ref> 0.8848 0.7277 0.7046 CW tf_efficientnet_b3_ns <ref type="bibr" coords="7,232.10,436.52,16.46,8.87" target="#b12">[13]</ref> 0.8561 0.7262 0.6640 CW tf_efficientnetv2_s_in21k <ref type="bibr" coords="7,240.32,448.47,16.46,8.87" target="#b42">[43]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We hope our work can help researchers and conservation practitioners accurately survey population trends, so they can regularly and more effectively evaluate threats. We present a sound detection and classification pipeline for analyzing soundscape recordings. Our models learn from few data and weak labels; they can accurately classify fine-grained bird vocalizations in 0.04s using a single GPU. Moreover, they show robustness against noisy sounds (e.g., rain, cars). We aim to improve the model's efficiency for smartphone devices applications. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,196.61,416.69,8.93;2,89.29,208.61,230.07,8.87;2,91.28,91.22,104.16,70.31"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Photographs of some Hawai'i endemic bird species studied in this work. Photo credit: Amanda K. Navine, Alexander Wang and Ann Tanimoto-Johnson.</figDesc><graphic coords="2,91.28,91.22,104.16,70.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,601.48,418.22,8.93;3,89.29,613.48,418.22,8.87;3,89.02,625.44,416.96,8.87;3,88.99,637.39,279.38,8.87;3,89.29,464.02,416.69,118.07"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of bird species in the training set. We can see a notable long-tailed distribution. Many bird species are represented with less than 10 audio clips (i.e., maupar, crehon, hawhaw, puaioh). The red line indicates that most of the birds appear less than 50 times in the training set. Only two birds (houfin and skylar) appear more than 100 times in the training data.</figDesc><graphic coords="3,89.29,464.02,416.69,118.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,260.12,416.69,8.93;5,89.29,272.13,372.18,8.87;5,89.29,298.93,416.69,107.63"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Illustration of our pre-processing pipeline. In every training batch, we used mixup of "scored" birds. We can expect that selective mixup can make very robust training for rare birds<ref type="bibr" coords="5,442.82,272.13,14.92,8.87" target="#b21">[22]</ref>.</figDesc><graphic coords="5,89.29,298.93,416.69,107.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,89.29,431.09,287.65,8.93"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of our Bird Classification model inspired in [22].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,89.29,622.96,416.70,8.93;6,89.29,634.97,280.76,8.87;6,89.29,354.41,416.68,250.01"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distributions of nocall probability validated using train_soundscapes. We show the class-wise best quantile thresholds to obtain the maximum AUC score per bird.</figDesc><graphic coords="6,89.29,354.41,416.68,250.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,89.29,608.59,416.69,8.93;8,89.29,620.60,416.70,8.87;8,89.29,632.55,286.20,8.87;8,91.28,503.05,416.69,93.54"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Grad-CAM [37] activations from our model tf_efficientnet_b0_ns on different validation audio spectrograms. These qualitative results show how our model focuses on particular frequencies through time to recognize the birds. Best viewed in electronic version.</figDesc><graphic coords="8,91.28,503.05,416.69,93.54" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p><rs type="person">Marcos Conde</rs> is supported by H2O.ai and by <rs type="funder">Humboldt Foundation (JMU Würzburg)</rs>. We would like to thank <rs type="person">Kaggle</rs> and <rs type="person">Dr. Stefan Kahl</rs> for hosting the BirdCLEF 2022 Challenge. We also want to thank the contributions from: <rs type="person">Amanda K. Navine</rs>, <rs type="person">Ann Tanimoto-Johnson</rs>, <rs type="person">Hidehisa Arai</rs>, <rs type="person">Christof Henkel</rs>, <rs type="person">Pascal Pfeiffer</rs>, and <rs type="person">Philipp Singer</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,210.55,394.53,10.91;9,112.66,224.10,393.33,10.91;9,112.66,237.65,393.33,10.91;9,112.66,251.20,111.86,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,210.09,224.10,295.90,10.91;9,112.66,237.65,113.91,10.91">Overview of birdclef 2022: Endangered bird species recognition in soundscape recordings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,236.87,237.65,269.11,10.91;9,112.66,251.20,79.94,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,264.75,394.53,10.91;9,112.66,278.30,394.53,10.91;9,112.66,291.85,395.17,10.91;9,112.66,305.40,393.32,10.91;9,112.66,318.95,394.53,10.91;9,112.66,332.50,22.69,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,260.93,291.85,246.90,10.91;9,112.66,305.40,313.05,10.91">Overview of lifeclef 2022: an evaluation of machinelearning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,448.45,305.40,57.53,10.91;9,112.66,318.95,346.66,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,346.05,395.00,10.91;9,112.28,359.59,96.76,10.91" xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/competitions/birdclef-2022/" />
	</analytic>
	<monogr>
		<title level="j" coord="9,148.45,346.05,27.86,10.91">Kaggle</title>
		<imprint>
			<date type="published" when="2022-06-03">2022. 2022. 2022-06-03</date>
			<pubPlace>Birdclef</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,373.14,394.53,10.91;9,112.66,386.69,393.33,10.91;9,112.66,400.24,394.89,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,112.66,386.69,98.33,10.91;9,238.81,386.69,206.97,10.91">Bird call identification in soundscape recordings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,467.81,386.69,38.18,10.91;9,112.66,400.24,296.63,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1437" to="1450" />
		</imprint>
	</monogr>
	<note>Overview of BirdCLEF</note>
</biblStruct>

<biblStruct coords="9,112.66,413.79,394.53,10.91;9,112.66,427.34,394.52,10.91;9,112.66,440.89,384.98,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,112.66,427.34,99.64,10.91;9,240.77,427.34,261.46,10.91">Bird Sound Recognition in Complex Acoustic Environments</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Clapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hopping</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,127.29,440.89,339.66,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Overview of BirdCLEF</note>
</biblStruct>

<biblStruct coords="9,112.66,454.44,393.33,10.91;9,112.66,467.99,184.41,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,311.76,454.44,194.23,10.91;9,112.66,467.99,109.52,10.91">Audio based bird species identification using deep learning techniques</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sprengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,481.54,313.10,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,167.73,481.54,182.90,10.91">Bird species identification in soundscapes</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,374.03,481.54,21.05,10.91">CLEF</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,495.09,395.17,10.91;9,112.26,508.64,100.14,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,168.33,495.09,339.50,10.91;9,112.26,508.64,24.88,10.91">Audio-based bird species identification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,522.18,394.52,10.91;9,112.66,535.73,22.69,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,164.21,522.18,293.26,10.91">Bird identification from timestamped, geotagged audio recordings</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,549.28,394.53,10.91;9,112.66,562.83,66.36,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,217.13,549.28,265.34,10.91">Xception based method for bird sound recognition of birdclef</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,576.38,393.33,10.91;9,112.66,589.93,157.65,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="9,344.79,576.38,161.20,10.91;9,112.66,589.93,82.93,10.91">Bird species recognition via neural architecture search</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mühling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Korfhage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Freisleben</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,603.48,394.53,10.91;9,112.66,617.03,349.74,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m" coord="9,185.89,617.03,146.12,10.91">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,630.58,394.53,10.91;9,112.66,644.13,122.77,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="9,186.32,630.58,316.07,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,86.97,393.33,10.91;10,112.66,100.52,360.52,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,376.83,86.97,129.16,10.91;10,112.66,100.52,230.56,10.91">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10211</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,114.06,395.01,10.91;10,112.66,130.06,97.35,7.90" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m" coord="10,266.99,114.06,208.54,10.91">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,141.16,394.53,10.91;10,112.66,154.71,173.79,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">V</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Turgutlu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10587</idno>
		<title level="m" coord="10,232.48,141.16,270.63,10.91">Exploring vision transformers for fine-grained classification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,168.26,393.33,10.91;10,112.66,181.81,393.32,10.91;10,112.66,195.36,206.64,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,378.71,168.26,127.27,10.91;10,112.66,181.81,198.75,10.91">Sound event detection with depthwise separable and dilated convolutions</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Drossos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gharib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,334.49,181.81,171.49,10.91;10,112.66,195.36,112.29,10.91">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,208.91,393.33,10.91;10,112.66,222.46,322.12,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="10,406.83,208.91,99.15,10.91;10,112.66,222.46,192.66,10.91">Learning sound event classifiers from web audio with noisy labels</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01189</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,236.01,393.33,10.91;10,112.66,249.56,397.48,10.91;10,112.66,265.55,73.62,7.90" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,375.77,236.01,130.21,10.91;10,112.66,249.56,136.44,10.91">Robust sound event detection in bioacoustic sensor networks</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lostanlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farnsworth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kelling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bello</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0214168</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,257.94,249.56,49.15,10.91">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">214168</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,276.66,393.53,10.91;10,112.66,290.20,365.55,10.91" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="10,378.79,276.66,127.40,10.91;10,112.66,290.20,183.87,10.91">Multi-attention networks for temporal localization of video-level labels</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nizampatnam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gangopadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">V</forename><surname>Conde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06866</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,303.75,393.33,10.91;10,112.66,317.30,393.33,10.91;10,112.66,330.85,394.53,10.91;10,112.66,344.40,394.53,10.91;10,112.66,357.95,310.80,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,295.40,303.75,210.59,10.91;10,112.66,317.30,247.49,10.91">Birdcall identification using CNN and gradient boosting decision trees with weak and noisy supervision</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Murakami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nishimori</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-136.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,383.04,317.30,122.95,10.91;10,112.66,330.85,296.02,10.91">Proceedings of the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="10,318.03,344.40,146.46,10.91">CEUR Workshop Proceedings, CEUR</title>
		<meeting>the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum<address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">September 21st -to -24th, 2021. 2936. 2021</date>
			<biblScope unit="page" from="1597" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,371.50,393.53,10.91;10,112.26,385.05,397.88,10.91;10,112.66,401.04,61.74,7.90" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="10,257.24,371.50,248.95,10.91;10,112.26,385.05,78.70,10.91">Recognizing bird species in diverse soundscapes under weak supervision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Henkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Singer</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2107.07728</idno>
		<ptr target="https://arxiv.org/abs/2107.07728.doi:10.48550/ARXIV.2107.07728" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,412.15,393.33,10.91;10,112.66,425.70,394.53,10.91;10,112.66,439.25,378.89,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,419.15,412.15,86.84,10.91;10,112.66,425.70,345.00,10.91">Weakly-supervised classification and detection of bird sounds in the wild. a birdclef 2021 solution</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">V</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shubham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Agnihotri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">D</forename><surname>Movva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bessenyei</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-131.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,480.53,425.70,26.65,10.91;10,112.66,439.25,59.92,10.91">CLEF, CEUR-WS.org</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1547" to="1558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,452.79,394.62,10.91;10,112.66,466.34,242.12,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,211.69,452.79,274.43,10.91">Bird-species audio identification, ensembling 1d + 2d signals</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,466.34,212.16,10.91">Proceedings of the Working Notes of CLEF 2021</title>
		<meeting>the Working Notes of CLEF 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,479.89,393.33,10.91;10,112.66,493.44,395.17,10.91;10,112.66,506.99,394.52,10.91;10,112.66,520.54,378.89,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="10,234.83,479.89,271.16,10.91;10,112.66,493.44,353.61,10.91">Tuc media computing at birdclef 2021: Noise augmentation strategies in bird sound classification in combination with densenets and resnets</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-138.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,488.92,493.44,18.91,10.91;10,112.66,506.99,193.59,10.91">Proceedings of the Working Notes of CLEF 2021</title>
		<title level="s" coord="10,382.72,508.01,124.46,9.72;10,112.66,520.54,21.79,10.91">CEUR Workshop Proceedings, CEUR</title>
		<meeting>the Working Notes of CLEF 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2936</biblScope>
			<biblScope unit="page" from="1617" to="1626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,534.09,393.33,10.91;10,112.66,547.64,393.58,10.91;10,112.66,561.19,48.11,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="10,354.23,534.09,35.36,10.91;10,421.91,534.09,84.08,10.91;10,112.66,547.64,182.60,10.91">building a birdcall segmentation model based on weak labels</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">V</forename><surname>Shugaev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tanahashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,317.58,547.64,188.67,10.91">Proceedings of the Working Notes of CLEF</title>
		<meeting>the Working Notes of CLEF</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021. 2021</date>
		</imprint>
	</monogr>
	<note>Birdclef</note>
</biblStruct>

<biblStruct coords="10,112.66,574.74,394.62,10.91;10,112.66,588.29,393.33,10.91;10,112.66,601.84,395.01,10.91;10,112.41,615.39,269.04,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="10,166.97,574.74,318.35,10.91">Learning to monitor birdcalls from weakly-labeled focused recordings</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-139.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,376.30,588.29,129.69,10.91;10,112.66,601.84,84.21,10.91">Proceedings of the Working Notes of CLEF 2021</title>
		<title level="s" coord="10,273.70,601.84,149.53,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2021</meeting>
		<imprint>
			<date type="published" when="2021">2936. 2021</date>
			<biblScope unit="page" from="1627" to="1638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,628.93,395.01,10.91;10,112.41,642.48,271.84,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="10,163.76,628.93,185.90,10.91">Stft transformers for bird song recognition</title>
		<author>
			<persName coords=""><forename type="first">J.-F</forename><surname>Puget</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-137.pdff" />
	</analytic>
	<monogr>
		<title level="m" coord="10,372.53,628.93,88.35,10.91">CLEF, CEUR-WS.org</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1609" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,656.03,395.17,10.91;10,112.66,669.58,395.01,10.91;11,112.66,86.97,197.80,10.91" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="10,278.69,656.03,229.14,10.91;10,112.66,669.58,218.69,10.91">Google landmarks dataset v2 -a large-scale benchmark for instance-level recognition and retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2004.01804</idno>
		<ptr target="https://arxiv.org/abs/2004.01804.doi:10.48550/ARXIV.2004.01804" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,100.52,394.61,10.91;11,112.66,114.06,397.48,10.91;11,112.66,130.06,26.14,7.90" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="11,233.54,100.52,273.73,10.91;11,112.66,114.06,56.13,10.91">An open dataset for research on audio field recording archives: freefield1010</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1309.5275</idno>
		<ptr target="https://arxiv.org/abs/1309.5275.doi:10.48550/ARXIV.1309.5275" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,141.16,393.33,10.91;11,112.66,154.71,394.04,10.91;11,112.66,168.26,224.44,10.91" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="11,399.53,141.16,106.46,10.91;11,112.66,154.71,228.62,10.91">BirdVox-DCASE-20k: a dataset for bird audio detection in 10-second clips</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lostanlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farnsworth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kelling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1208080</idno>
		<ptr target="https://doi.org/10.5281/zenodo.1208080.doi:10.5281/zenodo.1208080" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,181.81,395.17,10.91;11,112.66,195.36,387.14,10.91" xml:id="b31">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1710.09412</idno>
		<ptr target="https://arxiv.org/abs/1710.09412.doi:10.48550/ARXIV.1710.09412" />
		<title level="m" coord="11,330.41,181.81,177.42,10.91;11,112.66,195.36,16.17,10.91">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,208.91,395.17,10.91;11,112.66,222.46,395.01,10.91;11,112.66,236.01,371.05,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="11,464.17,208.91,43.66,10.91;11,112.66,222.46,349.37,10.91">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-2680</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,485.22,222.46,22.45,10.91;11,112.66,236.01,72.85,10.91">Proc. Interspeech 2019</title>
		<meeting>Interspeech 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,249.56,393.33,10.91;11,112.66,263.11,393.33,10.91;11,112.14,276.66,77.85,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="11,428.02,249.56,77.96,10.91;11,112.66,263.11,331.85,10.91">Conformer-based sound event detection with semi-supervised learning and data augmentation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Komatsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,453.13,263.11,52.86,10.91;11,112.14,276.66,45.93,10.91">DCASE2020 Workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,290.20,395.17,10.91;11,112.66,303.75,394.03,10.91;11,112.66,317.30,220.49,10.91" xml:id="b34">
	<monogr>
		<title level="m" type="main" coord="11,301.17,290.20,206.67,10.91;11,112.66,303.75,242.50,10.91">Frequency dynamic convolution: Frequencyadaptive pattern recognition for sound event detection</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B.-Y</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-H</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2203.15296</idno>
		<ptr target="https://arxiv.org/abs/2203.15296.doi:10.48550/ARXIV.2203.15296" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,330.85,393.33,10.91;11,112.66,344.40,394.61,10.91;11,112.31,357.95,288.85,10.91" xml:id="b35">
	<monogr>
		<title level="m" type="main" coord="11,405.71,330.85,100.28,10.91;11,112.66,344.40,313.51,10.91">Hts-at: A hierarchical token-semantic audio transformer for sound classification and detection</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dubnov</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2202.00874</idno>
		<ptr target="https://arxiv.org/abs/2202.00874.doi:10.48550/ARXIV.2202.00874" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,371.50,393.33,10.91;11,112.66,385.05,393.32,10.91;11,112.66,398.60,308.31,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="11,428.24,371.50,77.75,10.91;11,112.66,385.05,287.52,10.91">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,422.95,385.05,83.03,10.91;11,112.66,398.60,220.81,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,412.15,394.61,10.91;11,112.66,425.70,394.53,10.91;11,112.66,439.25,89.12,10.91" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="11,279.47,412.15,207.17,10.91">Bird Species Classification: One Step at a Time</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,425.70,344.36,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09">Sep. 2022. 2022</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2022</note>
</biblStruct>

<biblStruct coords="11,112.66,452.79,394.61,10.91;11,112.66,466.34,394.53,10.91;11,112.66,479.89,89.12,10.91" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="11,229.33,452.79,258.32,10.91">Dealing with Class Imbalance in Bird Sound Classification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Martynov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Uematsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,466.34,344.36,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09">Sep. 2022. 2022</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2022</note>
</biblStruct>

<biblStruct coords="11,112.66,493.44,395.17,10.91;11,112.39,506.99,394.88,10.91;11,112.66,520.54,344.98,10.91" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="11,387.32,493.44,120.51,10.91;11,112.39,506.99,199.42,10.91">Motif Mining and Unsupervised Representation Learning for BirdCLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miyaguchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Cheungvivatpant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dudley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Swain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,356.26,506.99,151.01,10.91;11,112.66,520.54,201.59,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09">2022. Sep. 2022. 2022</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2022</note>
</biblStruct>

<biblStruct coords="11,112.66,534.09,393.33,10.91;11,112.66,547.64,394.53,10.91;11,112.66,561.19,374.45,10.91" xml:id="b40">
	<analytic>
		<title level="a" type="main" coord="11,258.07,534.09,163.11,10.91;11,450.18,534.09,55.81,10.91;11,112.66,547.64,254.23,10.91">Strategies in identifying bird sounds in a complex acoustic environment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sampathkumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,389.94,547.64,112.80,10.91;11,112.66,561.19,231.07,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09">2022. Sep. 2022. 2022</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2022</note>
</biblStruct>

<biblStruct coords="11,112.66,574.74,393.33,10.91;11,112.26,588.29,394.92,10.91;11,112.66,601.84,65.30,10.91" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="11,294.53,574.74,211.46,10.91;11,112.26,588.29,98.97,10.91">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,234.62,588.29,209.50,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1059" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,615.39,393.33,10.91;11,112.66,628.93,287.29,10.91" xml:id="b42">
	<analytic>
		<title level="a" type="main" coord="11,187.31,615.39,229.74,10.91">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,446.71,615.39,59.28,10.91;11,112.66,628.93,146.65,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10096" to="10106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,642.48,393.33,10.91;11,112.66,656.03,275.66,10.91" xml:id="b43">
	<analytic>
		<title level="a" type="main" coord="11,305.12,642.48,200.87,10.91;11,112.66,656.03,89.67,10.91">Birdnet: A deep learning solution for avian diversity monitoring</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,211.05,656.03,99.32,10.91">Ecological Informatics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">101236</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
